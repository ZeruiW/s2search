{"id": 3279, "s2_id": "dc928e21f7559d97600b3121c5defb4cea0d6e48", "title": "Memory Vulnerability: A Case for Delaying Error Reporting", "abstract": "To face future reliability challenges, it is necessary to quantify the risk of error in any part of a computing system. To this goal, the Architectural Vulnerability Factor (AVF) has long been used for chips. However, this metric is used for offline characterisation, which is inappropriate for memory. We survey the literature and formalise one of the metrics used, the Memory Vulnerability Factor, and extend it to take into account false errors. These are reported errors which would have no impact on the program if they were ignored. We measure the False Error Aware MVF (FEA) and related metrics precisely in a cycle-accurate simulator, and compare them with the effects of injecting faults in a program's data, in native parallel runs. Our findings show that MVF and FEA are the only two metrics that are safe to use at runtime, as they both consistently give an upper bound on the probability of incorrect program outcome. FEA gives a tighter bound than MVF, and is the metric that correlates best with the incorrect outcome probability of all considered metrics.", "venue": "ArXiv", "authors": ["Luc  Jaulmes", "Miquel  Moret\u00f3", "Mateo  Valero", "Marc  Casas"], "year": 2018, "n_citations": 0}
{"id": 7070, "s2_id": "34671db66bfea865f18db7edf9090ff26e2b86ba", "title": "A survey of techniques for dynamic branch prediction", "abstract": "Branch predictor (BP) is an essential component in modern processors since high BP accuracy can improve performance and reduce energy by decreasing the number of instructions executed on wrong\u2010path. However, reducing the latency and storage overhead of BP while maintaining high accuracy presents significant challenges. In this paper, we present a survey of dynamic branch prediction techniques. We classify the works based on key features to underscore their differences and similarities. We believe this paper will spark further research in this area and will be useful for computer architects, processor designers, and researchers.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Sparsh  Mittal"], "year": 2019, "n_citations": 29}
{"id": 8278, "s2_id": "f25c697e926eacb1127c00899d4f3b77770a06d2", "title": "Enabling High-Capacity, Latency-Tolerant, and Highly-Concurrent GPU Register Files via Software/Hardware Cooperation", "abstract": "Graphics Processing Units (GPUs) employ large register files to accommodate all active threads and accelerate context switching. Unfortunately, register files are a scalability bottleneck for future GPUs due to long access latency, high power consumption, and large silicon area provisioning. Prior work proposes hierarchical register file to reduce the register file power consumption by caching registers in a smaller register file cache. Unfortunately, this approach does not improve register access latency due to the low hit rate in the register file cache. \nIn this paper, we propose the Latency-Tolerant Register File (LTRF) architecture to achieve low latency in a two-level hierarchical structure while keeping power consumption low. We observe that compile-time interval analysis enables us to divide GPU program execution into intervals with an accurate estimate of a warp's aggregate register working-set within each interval. The key idea of LTRF is to prefetch the estimated register working-set from the main register file to the register file cache under software control, at the beginning of each interval, and overlap the prefetch latency with the execution of other warps. We observe that register bank conflicts while prefetching the registers could greatly reduce the effectiveness of LTRF. Therefore, we devise a compile-time register renumbering technique to reduce the likelihood of register bank conflicts. Our experimental results show that LTRF enables high-capacity yet long-latency main GPU register files, paving the way for various optimizations. As an example optimization, we implement the main register file with emerging high-density high-latency memory technologies, enabling 8X larger capacity and improving overall GPU performance by 34%.", "venue": "ArXiv", "authors": ["Mohammad  Sadrosadati", "Amirhossein  Mirhosseini", "Ali  Hajiabadi", "Seyed Borna Ehsani", "Hajar  Falahati", "Hamid  Sarbazi-Azad", "Mario  Drumond", "Babak  Falsafi", "Rachata  Ausavarungnirun", "Onur  Mutlu"], "year": 2020, "n_citations": 0}
{"id": 8930, "s2_id": "780653f2b60e428cedaf0e6bd9f626a6ee474023", "title": "ECMO: Peripheral Transplantation to Rehost Embedded Linux Kernels", "abstract": "Dynamic analysis based on the full-system emulator QEMU is widely used for various purposes.However, it is challenging to run firmware images of embedded devices in QEMU, especially the process to boot the Linux kernel (we call this process rehosting the Linux kernel in this paper). That's because embedded devices usually use different system-on-chips (SoCs) from multiple vendors and only a limited number of SoCs are currently supported in QEMU. In this work, we propose a technique called peripheral transplantation. The main idea is to transplant the device drivers of designated peripherals into the Linux kernel binary. By doing so, it can replace the peripherals in the kernel that are currently unsupported in QEMU with supported ones, thus making the Linux kernel rehostable. After that, various applications can be built. We implemented this technique inside a prototype system called ECMO and applied it to 815 firmware images, which consist of 20 kernel versions and 37 device models. The result shows that ECMO can successfully transplant peripherals for all the 815 Linux kernels. Among them, 710 kernels can be successfully rehosted, i.e., launching a user-space shell (87.1% success rate). The failed cases are mainly because the root file system format (ramfs) is not supported by the kernel. Meanwhile, we are able to inject rather complex drivers (i.e., NIC driver) for all the rehosted Linux kernels by installing kernel modules. We further build three applications, i.e., kernel crash analysis, rootkit forensic analysis, and kernel fuzzing, based on the rehosted kernels to demonstrate the usage scenarios of ECMO.", "venue": "CCS", "authors": ["Muhui  Jiang", "Lin  Ma", "Yajin  Zhou", "Qiang  Liu", "Cen  Zhang", "Zhi  Wang", "Xiapu  Luo", "Lei  Wu", "Kui  Ren"], "year": 2021, "n_citations": 0}
{"id": 11074, "s2_id": "2ec71f3d831cd750ee91fe46aafaa89f5efb598b", "title": "Wafer-level Variation Modeling for Multi-site RF IC Testing via Hierarchical Gaussian Process", "abstract": "Wafer-level performance prediction has been attracting attention to reduce measurement costs without compromising test quality in production tests. Although several efficient methods have been proposed, the site-to-site variation, which is often observed in multi-site testing for radio frequency circuits, has not yet been sufficiently addressed. In this paper, we propose a wafer-level performance prediction method for multi-site testing that can consider the site-to-site variation. The proposed method is based on the Gaussian process, which is widely used for wafer-level spatial correlation modeling, improving the prediction accuracy by extending hierarchical modeling to exploit the test site information provided by test engineers. In addition, we propose an active test-site sampling method to maximize measurement cost reduction. Through experiments using industrial production test data, we demonstrate that the proposed method can reduce the estimation error to 1/19 of that obtained using a conventional method. Moreover, we demonstrate that the proposed sampling method can reduce the number of the measurements by 97% while achieving sufficient estimation accuracy.", "venue": "2021 IEEE International Test Conference (ITC)", "authors": ["Michihiro  Shintani", "Riaz-Ul-Haque  Mian", "Tomoki  Nakamura", "Masuo  Kajiyama", "Makoto  Eiki", "Michiko  Inoue"], "year": 2021, "n_citations": 0}
{"id": 11455, "s2_id": "1bca29fb47c59b47f21f7993643838113aa7de77", "title": "A Microprocessor implemented in 65nm CMOS with Configurable and Bit-scalable Accelerator for Programmable In-memory Computing", "abstract": "This paper presents a programmable in-memory-computing processor, demonstrated in a 65nm CMOS technology. For data-centric workloads, such as deep neural networks, data movement often dominates when implemented with today's computing architectures. This has motivated spatial architectures, where the arrangement of data-storage and compute hardware is distributed and explicitly aligned to the computation dataflow, most notably for matrix-vector multiplication. In-memory computing is a spatial architecture where processing elements correspond to dense bit cells, providing local storage and compute, typically employing analog operation. Though this raises the potential for high energy efficiency and throughput, analog operation has significantly limited robustness, scale, and programmability. This paper describes a 590kb in-memory-computing accelerator integrated in a programmable processor architecture, by exploiting recent approaches to charge-domain in-memory computing. The architecture takes the approach of tight coupling with an embedded CPU, through accelerator interfaces enabling integration in the standard processor memory space. Additionally, a near-memory-computing datapath both enables diverse computations locally, to address operations required across applications, and enables bit-precision scalability for matrix/input-vector elements, through a bit-parallel/bit-serial (BP/BS) scheme. Chip measurements show an energy efficiency of 152/297 1b-TOPS/W and throughput of 4.7/1.9 1b-TOPS (scaling linearly with the matrix/input-vector element precisions) at VDD of 1.2/0.85V. Neural network demonstrations with 1-b/4-b weights and activations for CIFAR-10 classification consume 5.3/105.2 $\\mu$J/image at 176/23 fps, with accuracy at the level of digital/software implementation (89.3/92.4 $\\%$ accuracy).", "venue": "ArXiv", "authors": ["Hongyang  Jia", "Yinqi  Tang", "Hossein  Valavi", "Jintao  Zhang", "Naveen  Verma"], "year": 2018, "n_citations": 18}
{"id": 13424, "s2_id": "5849dc26f5f10c53a9f169c8be88a531c398561f", "title": "Single Event Transient Fault Analysis of ELEPHANT cipher", "abstract": "In this paper, we propose a novel fault attack termed as Single Event Transient Fault Analysis (SETFA) attack, which is well suited for hardware implementations. The proposed approach pinpoints hotspots in the cipher\u2019s Sbox combinational logic circuit that significantly reduce the key entropy when subjected to faults. ELEPHANT is a parallel authenticated encryption and associated data (AEAD) scheme targeted to hardware implementations, a finalist in the Lightweight cryptography (LWC) competition launched by NIST. In this work, we investigate vulnerabilities of ELEPHANT against fault analysis. We observe that the use of 128-bit random nonce makes it resistant against many cryptanalysis techniques like differential, linear, etc., and their variants. However, the relaxed nature of Statistical Fault Analysis (SFA) methods makes them widely applicable in restrictive environments. We propose an SETFA-based key recovery attack on Elephant. We performed Single experiments with random plaintexts and keys, on Dumbo, a Spongent-based instance of Elephant-AEAD scheme. Our proposed approach could recover the secret key in 85\u2212 250 ciphertexts. In essence, this work investigates new vulnerabilities towards fault analysis that may require to be addressed to ensure secure computations and communications in IoT scenarios.", "venue": "ArXiv", "authors": ["Priyanka  Joshi", "Bodhistwa  Mazumdar"], "year": 2021, "n_citations": 0}
{"id": 13899, "s2_id": "1f1e4b0459eb45ab8016cd2b4d6bb8d05228706f", "title": "Gemini: Reducing DRAM Cache Hit Latency by Hybrid Mappings", "abstract": "Die-stacked DRAM caches are increasingly advocated to bridge the performance gap between on-chip Cache and main memory. It is essential to improve DRAM cache hit rate and lower cache hit latency simultaneously. Prior DRAM cache designs fall into two categories according to the data mapping polices: set-associative and direct-mapped, achieving either one. In this paper, we propose a partial direct-mapped die-stacked DRAM cache to achieve the both objectives simultaneously, called Gemini, which is motivated by the following observations: applying unified mapping policy to different blocks cannot achieve high cache hit rate and low hit latency in terms of mapping structure. Gemini cache classifies data into leading blocks and following blocks, and places them with static mapping and dynamic mapping respectively in a unified set-associative structure. Gemini also designs a replacement policy to balance the different blocks miss penalty and the recency, and provides strategies to mitigate cache thrashing due to block type transitions. Experimental results demonstrate that Gemini cache can narrow the hit latency gap with direct-mapped cache significantly, from 1.75X to 1.22X on average, and can achieve comparable hit rate with set-associative cache. Compared with the state-of-the-art baselines, i.e., enhanced Loh-Hill cache, Gemini improves the IPC by up to 20% respectively.", "venue": "ArXiv", "authors": ["Ye  Chi"], "year": 2018, "n_citations": 0}
{"id": 18191, "s2_id": "11f32270570a5c385910896213efd8389e26d7b4", "title": "Universal Numeric Segmented Display", "abstract": "Segmentation display plays a vital role to display numerals. But in today's world matrix display is also used in displaying numerals. Because numerals has lots of curve edges which is better supported by matrix display. But as matrix display is costly and complex to implement and also needs more memory, segment display is generally used to display numerals. But as there is yet no proposed compact display architecture to display multiple language numerals at a time, this paper proposes uniform display architecture to display multiple language digits and general mathematical expressions with higher accuracy and simplicity by using a 18-segment display, which is an improvement over the 16 segment display.", "venue": "ArXiv", "authors": ["Muhammad Abul Kalam Azad", "Rezwana  Sharmeen", "S. M. Kamruzzaman"], "year": 2010, "n_citations": 2}
{"id": 20211, "s2_id": "6a1db9edcf82f889864fa9d1be9b07a97e7ac7e9", "title": "Fundamental Limits on Energy-Delay-Accuracy of In-memory Architectures in Inference Applications", "abstract": "This paper obtains fundamental limits on the computational precision of in-memory computing architectures (IMCs). An IMC noise model and associated SNR metrics are defined and their interrelationships analyzed to show that the accuracy of IMCs is fundamentally limited by the compute SNR (SNRa) of its analog core, and that activation, weight and output precision needs to be assigned appropriately for the final output SNR SNRT \u2192 SNRa. The minimum precision criterion (MPC) is proposed to minimize the ADC precision. Three in-memory compute models charge summing (QS), current summing (IS) and charge redistribution (QR) are shown to underlie most known IMCs. Noise, energy and delay expressions for the compute models are developed and employed to derive expressions for the SNR, ADC precision, energy, and latency of IMCs. The compute SNR expressions are validated via Monte Carlo simulations in a 65 nm CMOS process. For a 512 row SRAM array, it is shown that: 1) IMCs have an upper bound on their maximum achievable SNRa due to constraints on energy, area and voltage swing, and this upper bound reduces with technology scaling for QS-based architectures; 2) MPC enables SNRT \u2192 SNRa to be realized with minimal ADC precision; 3) QS-based (QR-based) architectures are preferred for low (high) compute SNR scenarios.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Sujan Kumar Gonugondla", "Charbel  Sakr", "Hassan  Dbouk", "Naresh R. Shanbhag"], "year": 2021, "n_citations": 0}
{"id": 21557, "s2_id": "902127e4852c32b3f2226b899fee80eb83787e2a", "title": "A New MRAM-Based Process In-Memory Accelerator for Efficient Neural Network Training with Floating Point Precision", "abstract": "The excellent performance of modern deep neural networks (DNNs) comes at an often prohibitive training cost, limiting the rapid development of DNN innovations and raising various environmental concerns. To reduce the dominant data movement cost of training, process in-memory (PIM) has emerged as a promising solution as it alleviates the need to access DNN weights. However, state-of-the-art PIM DNN training accelerators employ either analog/mixed signal computing which has limited precision or digital computing based on a memory technology that supports limited logic functions and thus requires complicated procedure to realize floating point computation. In this paper, we propose a spin orbit torque magnetic random access memory (SOT-MRAM) based digital PIM accelerator that supports floating point precision. Specifically, this new accelerator features an innovative (1) SOT-MRAM cell, (2) full addition design, and (3) floating point computation. Experiment results show that the proposed SOT-MRAM PIM based DNN training accelerator can achieve 3.3\u00d7, 1.8\u00d7, and 2.5\u00d7 improvement in terms of energy, latency, and area, respectively, compared with a state-of-the-art PIM based DNN training accelerator.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Hongjie  Wang", "Yang  Zhao", "Chaojian  Li", "Yue  Wang", "Yingyan  Lin"], "year": 2020, "n_citations": 4}
{"id": 22061, "s2_id": "49ed24db50243c0987de19a542dd48c809dabe3b", "title": "Hardware-accelerated Simulation-based Inference of Stochastic Epidemiology Models for COVID-19", "abstract": "Epidemiology models are central in understanding and controlling large scale pandemics. Several epidemiology models require simulation-based inference such as Approximate Bayesian Computation (ABC) to fit their parameters to observations. ABC inference is highly amenable to efficient hardware acceleration. In this work, we develop parallel ABC inference of a stochastic epidemiology model for COVID-19. The statistical inference framework is implemented and compared on Intel Xeon CPU, NVIDIA Tesla V100 GPU and the Graphcore Mk1 IPU, and the results are discussed in the context of their computational architectures. Results show that GPUs are 4x and IPUs are 30x faster than Xeon CPUs. Extensive performance analysis indicates that the difference between IPU and GPU can be attributed to higher communication bandwidth, closeness of memory to compute, and higher compute power in the IPU. The proposed framework scales across 16 IPUs, with scaling overhead not exceeding 8% for the experiments performed. We present an example of our framework in practice, performing inference on the epidemiology model across three countries, and giving a brief overview of the results.", "venue": "ArXiv", "authors": ["Sourabh  Kulkarni", "Mario Michael Krell", "Seth  Nabarro", "Csaba Andras Moritz"], "year": 2020, "n_citations": 2}
{"id": 22117, "s2_id": "c0a476ce62fe15446782c7a210f73b783e2adb94", "title": "High Level Synthesis Implementation of a Three-dimensional Systolic Array Architecture for Matrix Multiplications on Intel Stratix 10 FPGAs", "abstract": "In this paper, we consider the HLS implementation of a three-dimensional systolic array architecture for matrix multiplication that targets specific characteristics of Intel Stratix 10 FPGAs in order to produce designs that achieve a high floatingpoint throughput using most of the DSPs at high frequencies in a way that avoids the congestion of the routing fabric. The investigated three-dimensional systolic array architecture is able to produce hardware designs that use 99% of the available DSPs with maximum frequencies that let us achieve performance above 3 TFLOPS.", "venue": "ArXiv", "authors": ["Paolo  Gorlani", "Christian  Plessl"], "year": 2021, "n_citations": 0}
{"id": 22122, "s2_id": "472f91342cbe3aa1cc0f2d6addb23538ade297ae", "title": "Low Power Artificial Neural Network Architecture", "abstract": "Recent artificial neural network architectures improve performance and power dissipation by leveraging resistive devices to store and multiply synaptic weights with input data. Negative and positive synaptic weights are stored on the memristors of a reconfigurable crossbar array (MCA). Existing MCA-based neural network architectures use high power consuming voltage converters or operational amplifiers to generate the total synaptic current through each column of the crossbar array. This paper presents a low power MCA-based feedforward neural network architecture that uses a spintronic device per pair of columns to generate the synaptic current for each neuron. It is shown experimentally that the proposed architecture dissipates significantly less power compared to existing feedforward memristive neural network architectures.", "venue": "ArXiv", "authors": ["Krishna Prasad Gnawali", "Seyed Nima Mozaffari", "Spyros  Tragoudas"], "year": 2019, "n_citations": 2}
{"id": 33343, "s2_id": "51fc8321e4d2dd0ba90231d77bb6eac7773de0fe", "title": "Best-Effort FPGA Programming: A Few Steps Can Go a Long Way", "abstract": "FPGA-based heterogeneous architectures provide programmers with the ability to customize their hardware accelerators for flexible acceleration of many workloads. Nonetheless, such advantages come at the cost of sacrificing programmability. FPGA vendors and researchers attempt to improve the programmability through high-level synthesis (HLS) technologies that can directly generate hardware circuits from high-level language descriptions. However, reading through recent publications on FPGA designs using HLS, one often gets the impression that FPGA programming is still hard in that it leaves programmers to explore a very large design space with many possible combinations of HLS optimization strategies. \nIn this paper we make two important observations and contributions. First, we demonstrate a rather surprising result: FPGA programming can be made easy by following a simple best-effort guideline of five refinement steps using HLS. We show that for a broad class of accelerator benchmarks from MachSuite, the proposed best-effort guideline improves the FPGA accelerator performance by 42-29,030x. Compared to the baseline CPU performance, the FPGA accelerator performance is improved from an average 292.5x slowdown to an average 34.4x speedup. Moreover, we show that the refinement steps in the best-effort guideline, consisting of explicit data caching, customized pipelining, processing element duplication, computation/communication overlapping and scratchpad reorganization, correspond well to the best practice guidelines for multicore CPU programming. Although our best-effort guideline may not always lead to the optimal solution, it substantially simplifies the FPGA programming effort, and will greatly support the wide adoption of FPGA-based acceleration by the software programming community.", "venue": "ArXiv", "authors": ["Jason  Cong", "Zhenman  Fang", "Yuchen  Hao", "Peng  Wei", "Cody Hao Yu", "Chen  Zhang", "Peipei  Zhou"], "year": 2018, "n_citations": 12}
{"id": 33783, "s2_id": "1f07c63a21c9e200bf7a75626e9469cd751a9c9f", "title": "Analytical Inverter Delay Modeling Using Matlab's Curve Fitting Toolbox", "abstract": "This paper presents a new analytical propagation delay model for deep submicron CMOS inverters. The model is inspired by the key observation that the inverter delay is a complicated function of several process parameters as well as load capacitance. These relationships are considered by fitting functions for each parameter derived from the Curve Fitting Toolbox in Matlab. Compared to SPICE simulations based on the BSIM4 transistor model, the analytical delay model shows very good accuracy with an average error less than 2% over a wide range of process parameters and output loads. Hence, the proposed model can be efficiently used for different technology nodes as well as statistical gate delay characterisation.", "venue": "ArXiv", "authors": ["Walter  Schneider"], "year": 2018, "n_citations": 1}
{"id": 36821, "s2_id": "092d0ba602a8275254c0ac5718bf176526b12eaa", "title": "LSTM-Sharp: An Adaptable, Energy-Efficient Hardware Accelerator for Long Short-Term Memory", "abstract": "The effectiveness of LSTM neural networks for popular tasks such as Automatic Speech Recognition has fostered an increasing interest in LSTM inference acceleration. Due to the recurrent nature and data dependencies of LSTM computations, designing a customized architecture specifically tailored to its computation pattern is crucial for efficiency. Since LSTMs are used for a variety of tasks, generalizing this efficiency to diverse configurations, i.e., adaptiveness, is another key feature of these accelerators. In this work, we first show the problem of low resource-utilization and adaptiveness for the state-of-the-art LSTM implementations on GPU, FPGA and ASIC architectures. To solve these issues, we propose an intelligent tiled-based dispatching mechanism that efficiently handles the data dependencies and increases the adaptiveness of LSTM computation. To do so, we propose LSTM-Sharp as a hardware accelerator, which pipelines LSTM computation using an effective scheduling scheme to hide most of the dependent serialization. Furthermore, LSTM-Sharp employs dynamic reconfigurable architecture to adapt to the model's characteristics. LSTM-Sharp achieves 1.5x, 2.86x, and 82x speedups on average over the state-of-the-art ASIC, FPGA, and GPU implementations respectively, for different LSTM models and resource budgets. Furthermore, we provide significant energy-reduction with respect to the previous solutions, due to the low power dissipation of LSTM-Sharp (383 GFLOPs/Watt).", "venue": "ArXiv", "authors": ["Reza  Yazdani", "Olatunji  Ruwase", "Minjia  Zhang", "Yuxiong  He", "Jose-Maria  Arnau", "Antonio  Gonzalez"], "year": 2019, "n_citations": 7}
{"id": 39690, "s2_id": "565f07bd560116720a2589e05c38a21c1061f907", "title": "Phism: Polyhedral High-Level Synthesis in MLIR", "abstract": "Polyhedral optimisation, a methodology that views nested loops as polyhedra and searches for their optimal transformation regarding specific objectives (parallelism, locality, etc.), sounds promising for mitigating difficulties in automatically optimising hardware designs described by high-level synthesis (HLS), which are typically software programs with nested loops. Nevertheless, existing polyhedral tools cannot meet the requirements from HLS developers for platform-specific customisation and software/hardware co-optimisation. This paper proposes \u03c6sm (Phism), a polyhedral HLS framework built on MLIR, to address these challenges through progressive lowering multi-level intermediate representations (IRs) from polyhedra to HLS designs.", "venue": "ArXiv", "authors": ["Ruizhe  Zhao", "Jianyi  Cheng"], "year": 2021, "n_citations": 0}
{"id": 40004, "s2_id": "8d4b8bb90ed3fd462519c39b1d1957f1ca0524e2", "title": "Dominant block guided optimal cache size estimation to maximize IPC of embedded software", "abstract": "Embedded system software is highly constrained from performance, memory footprint, energy consumption and implementing cost view point. It is always desirable to obtain better Instructions per Cycle. Instruction cache has major contribution in improving IPC. Cache memories are realized on the same chip where the processor is running. This considerably increases the system cost as well. Hence, it is required to maintain a trade off between cache sizes and performance improvement offered. Determining the number of cache lines and size of cache line are important parameters for cache designing. The design space for cache is quite large. It is time taking to execute the given application with different cache sizes on an instruction set simulator to figure out the optimal cache size. In this paper, a technique is proposed to identify a number of cache lines and cache line size for the L1 instruction cache that will offer best or nearly best IPC. Cache size is derived, at a higher abstraction level, from basic block analysis in the Low Level Virtual Machine environment. The cache size estimated is cross validated by simulating the set of benchmark applications with different cache sizes in simple scalar simulator. The proposed method seems to be superior in terms of estimation accuracy and estimation time as compared to the existing methods for estimation of optimal cache size parameters like cache line size, number of cache lines.", "venue": "ArXiv", "authors": ["Rajendra  Patel", "Arvind  Rajawat"], "year": 2013, "n_citations": 1}
{"id": 43299, "s2_id": "36c950b1d440b620f598492f1098b6c86b2d7e3a", "title": "Modern Multicore CPUs are not Energy Proportional: Opportunity for Bi-objective Optimization for Performance and Energy", "abstract": "Energy proportionality is the key design goal followed by architects of modern multicore CPUs. One of its implications is that optimization of an application for performance will also optimize it for energy. In this work, we show that energy proportionality does not hold true for multicore CPUs. This finding creates the opportunity for bi-objective optimization of applications for performance and energy. We propose and study the first application-level method for bi-objective optimization of multithreaded data-parallel applications for performance and energy. The method uses two decision variables, the number of identical multithreaded kernels (threadgroups) executing the application and the number of threads in each threadgroup, with the workload always partitioned equally between the threadgroups. We experimentally demonstrate the efficiency of the method using four highly optimized multithreaded data-parallel applications, 2D fast Fourier transform based on FFTW and Intel MKL, and dense matrix-matrix multiplication using OpenBLAS and Intel MKL. Four modern multicore CPUs are used in the experiments. The experiments show that optimization for performance alone results in the increase in dynamic energy consumption by up to 89% and optimization for dynamic energy alone degrades the performance by up to 49%. By solving the bi-objective optimization problem, the method determines up to 11 globally Pareto-optimal solutions. Finally, we propose a qualitative dynamic energy model employing performance monitoring counters as parameters, which we use to explain the discovered energy nonproportionality and the Pareto-optimal solutions determined by our method. The model shows that the energy nonproportionality in our case is due to the activity of the data translation lookaside buffer (dTLB), which is disproportionately energy expensive.", "venue": "ArXiv", "authors": ["Semyon  Khokhriakov", "Ravi  Reddy", "Alexey L. Lastovetsky"], "year": 2019, "n_citations": 0}
{"id": 44834, "s2_id": "fc49c5fb52ac462fd9efda60b263e5b38c468215", "title": "Stochastic Rounding: Algorithms and Hardware Accelerator", "abstract": "We present algorithms and a hardware accelerator for performing stochastic rounding (SR). Our main goal is to augment the ARM M4F-based multi-core processor SpiNNaker2 with a more flexible rounding functionality than is available in the ARM processor itself. The motivation of adding such functionality in hardware is based on our previous results showing improvements in numerical accuracy of ODE solvers in fixed-point arithmetic with SR, compared to a standard round to nearest mode (RN) or bit truncation. Performing SR purely in software can be expensive due to requirement of multiple masking and shifting instructions, and an addition operation per each rounding. Also, saturation of values is included since it is required on overflows, which is common in fixed-point arithmetic due to a narrow dynamic range. The main intended use of the accelerator is to round fixed-point multiplier outputs, which are returned unrounded by the ARM processor in a wider fixed-point format than the arguments. The proposed accelerator is not specific to SpiNNaker, and is a generally applicable rounding unit provided a pseudorandom number generator is available that can supply random bits to it. Additionally, to the best of our knowledge, this is a first exploration of a stochastic rounding accelerator with a programmable bit position and multiple data type support.", "venue": "2021 International Joint Conference on Neural Networks (IJCNN)", "authors": ["Mantas  Mikaitis"], "year": 2021, "n_citations": 4}
{"id": 45382, "s2_id": "339662789be4812752ac2709e2b1a58573466d95", "title": "Chain-NN: An energy-efficient 1D chain architecture for accelerating deep convolutional neural networks", "abstract": "Deep convolutional neural networks (CNN) have shown their good performances in many computer vision tasks. However, the high computational complexity of CNN involves a huge amount of data movements between the computational processor core and memory hierarchy which occupies the major of the power consumption. This paper presents Chain-NN, a novel energy-efficient 1D chain architecture for accelerating deep CNNs. Chain-NN consists of the dedicated dual-channel process engines (PE). In Chain-NN, convolutions are done by the 1D systolic primitives composed of a group of adjacent PEs. These systolic primitives, together with the proposed column-wise scan input pattern, can fully reuse input operand to reduce the memory bandwidth requirement for energy saving. Moreover, the 1D chain architecture allows the systolic primitives to be easily reconfigured according to specific CNN parameters with fewer design complexity. The synthesis and layout of Chain-NN is under TSMC 28nm process. It costs 3751k logic gates and 352KB on-chip memory. The results show a 576-PE Chain-NN can be scaled up to 700MHz. This achieves a peak throughput of 806.4GOPS with 567.5mW and is able to accelerate the five convolutional layers in AlexNet at a frame rate of 326.2fps. 1421.0GOPS/W power efficiency is at least 2.5 to 4.1x times better than the state-of-the-art works.", "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017", "authors": ["Shihao  Wang", "Dajiang  Zhou", "Xushen  Han", "Takeshi  Yoshimura"], "year": 2017, "n_citations": 36}
{"id": 45529, "s2_id": "03fd3c8dd91fb8a73f90e557607050dc7244d9f9", "title": "ClepsydraCache - Preventing Cache Attacks with Time-Based Evictions", "abstract": "Both the shift towards attacks on the microarchitectural CPU level and the ongoing transition towards cloud computing and shared VM hosts have increasingly drawn attention towards cache attacks. In these fields of application, cache sidechannels lay the cornerstone that is leveraged by attackers to exfiltrate secret information from the CPU microarchitecture. We build upon the observation that current cache side-channel attacks mostly exploit the architectural visibility of conflicting cache addresses. With CLEPSYDRACACHE, we break away this foundation by unraveling the linkage between cache evictions and accesses to conflicting addresses. Our solution takes a new approach that assigns each cache entry a random time-to-live to reduce the amount of cache conflicts. By making those conflicts unobservable to an attacker, CLEPSYDRACACHE efficiently protects against attacks like PRIME+PROBE and FLUSH+RELOAD. Furthermore, our solution is applicable to large last-level caches which are the most common targets for cache attacks. We implement CLEPSYDRACACHE using the Gem5 simulator and provide a proof-of-concept hardware design and simulation using 65-nm CMOS technology. CLEPSYDRACACHE matches the performance of traditional cache architectures while improving the system security against cache attacks. A Clepsydra is an ancient time-measuring device worked by a flow of water.", "venue": "ArXiv", "authors": ["Jan Philipp Thoma", "Christian  Niesler", "Dominic A. Funke", "Gregor  Leander", "Pierre  Mayr", "Nils  Pohl", "Lucas  Davi", "Tim  G\u00fcneysu"], "year": 2021, "n_citations": 1}
{"id": 45984, "s2_id": "e983eab5dbef64581d96789c88208a697e854a9b", "title": "Benchmarking Quantized Neural Networks on FPGAs with FINN", "abstract": "The ever-growing cost of both training and inference for state-of-the-art neural networks has brought literature to look upon ways to cut off resources used with a minimal impact on accuracy. Using lower precision comes at the cost of negligible loss in accuracy. While training neural networks may require a powerful setup, deploying a network must be possible on low-power and lowresource hardware architectures. Reconfigurable architectures have proven to be more powerful and flexible than GPUs when looking at a specific application. This article aims to assess the impact of mixed-precision when applied to neural networks deployed on FPGAs. While several frameworks exist that create tools to deploy neural networks using reduced-precision, few of them assess the importance of quantization and the framework quality. FINN and Brevitas, two frameworks from Xilinx labs, are used to assess the impact of quantization on neural networks using 2 to 8 bit precisions and weights with several parallelization configurations. Equivalent accuracy can be obtained using lower-precision representation and enough training. However, the compressed network can be better parallelized allowing the deployed network throughput to be 62 times faster. The benchmark set up in this work is available in a public repository (https://github.com/QDucasse/nn benchmark).", "venue": "ArXiv", "authors": ["Quentin  Ducasse", "Pascal  Cotret", "Loic  Lagadec", "Robert  Stewart"], "year": 2021, "n_citations": 0}
{"id": 52568, "s2_id": "c567056bdd280d372c375c923b45ccd0fe90d215", "title": "A Flexible High-Bandwidth Low-Latency Multi-Port Memory Controller", "abstract": "Multi-port memory controllers (MPMCs) have become increasingly important in many modern applications due to the tremendous growth in bandwidth requirement. Many approaches so far have focused on improving either the memory access latency or the bandwidth utilization for specific applications. Moreover, the application systems are likely to require certain adjustments to connect with an MPMC, since the MPMC interface is limited to a single-clock and single-data-width domain. In this paper, we propose efficient techniques to improve the flexibility, latency, and bandwidth of an MPMC. Firstly, MPMC interfaces employ a pair of dual-clock dualport FIFOs at each port, so any multi-clock multi-data-width application system can connect to an MPMC without requiring extra resources. Secondly, memory access latency is significantly reduced because parallel FIFOs temporarily keep the data transfer between the application system and memory. Lastly, a proposed arbitration scheme, namely window-based first-come-first-serve, considerably enhances the bandwidth utilization. Depending on the applications, MPMC can be properly configured by updating several internal configuration registers. The experimental results in an Altera Cyclone V FPGA prove that MPMC is fully operational at 150 MHz and supports up to 32 concurrent connections at various clocks and data widths. More significantly, achieved bandwidth utilization is approximately 93.2% of the theoretical bandwidth, and the access latency is minimized as compared to previous designs.", "venue": "Vietnam Journal of Science and Technology", "authors": ["Xuan-Thuan  Nguyen", "Trong-Tu  Bui", "Huu-Thuan  Huynh", "Duc-Hung  Le", "Cong-Kha  Pham"], "year": 2018, "n_citations": 0}
{"id": 55119, "s2_id": "b04bf010f505a19f2031c45816dc738d03361978", "title": "ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Neural Network Accelerators", "abstract": "Hardware accelerators are being increasingly deployed to boost the performance and energy efficiency of deep neural network (DNN) inference. In this paper we propose Thundervolt, a new framework that enables aggressive voltage underscaling of high-performance DNN accelerators without compromising classification accuracy even in the presence of high timing error rates. Using post-synthesis timing simulations of a DNN accelerator modeled on the Google TPU, we show that Thundervolt enables between 34%-57% energy savings on state-of-the-art speech and image recognition benchmarks with less than 1% loss in classification accuracy and no performance loss. Further, we show that Thundervolt is synergistic with and can further increase the energy efficiency of commonly used run-time DNN pruning techniques like Zero-Skip.", "venue": "ArXiv", "authors": ["Jeff  Zhang", "Kartheek  Rangineni", "Zahra  Ghodsi", "Siddharth  Garg"], "year": 2018, "n_citations": 13}
{"id": 58025, "s2_id": "0f01db7f1bfd06f6ac3418b93d9b0bd514c9994a", "title": "Benchmarking Processor Performance by Multi-Threaded Machine Learning Algorithms", "abstract": "Machine learning algorithms have enabled computers to predict things by learning from previous data. The data storage and processing power are increasing rapidly, thus increasing machine learning and Artificial intelligence applications. Much of the work is done to improve the accuracy of the models built in the past, with little research done to determine the computational costs of machine learning acquisitions. In this paper, I will proceed with this later research work and will make a performance comparison of multi-threaded machine learning clustering algorithms. I will be working on Linear Regression, Random Forest, and K-Nearest Neighbors to determine the performance characteristics of the algorithms as well as the computation costs to the obtained results. I will be benchmarking system hardware performance by running these multi-threaded algorithms to train and test the models on a dataset to note the differences in performance matrices of the algorithms. In the end, I will state the best performing algorithms concerning the performance efficiency of these algorithms on my system.", "venue": "ArXiv", "authors": ["Muhammad Fahad Saleem"], "year": 2021, "n_citations": 0}
{"id": 59743, "s2_id": "9992f6c13542a173639349590a27eb4416c68a96", "title": "A Brief Review on Some Architectures Providing Support for DIFT", "abstract": "Dynamic Information Flow Tracking (DIFT) is a technique to track potential security vulnerabilities in software and hardware systems at run time. The last fifteen years have seen a lot of research work on DIFT, including both hardware-based and software-based implementations for different types of processor architectures. This survey briefly reviews some hardware architectures that provide DIFT support. Starting from introducing different approaches for hardware based DIFT, this survey focuses on integrated/in-core architectures. Protection schemes, including tagging system, tag propagation, and tag checking for each architecture will be discussed. The survey is organized in such a way that it illustrates the evolution of integrated DIFT architectures, each architecture tries to improve the precious proposed architectures generality/versatility weaknesses. However, improving security while providing generality and versatility is kind of trade-offs. This survey compares the architectures from different aspects to show the trade-offs clearer.", "venue": "ArXiv", "authors": ["Ali  Jahanshahi"], "year": 2019, "n_citations": 1}
{"id": 61322, "s2_id": "256299c8912b1cc250f3ac2839d58e885a1af96e", "title": "Virtual Coset Coding for Encrypted Non-Volatile Memories with Multi-Level Cells", "abstract": "Recently, Phase-Change Memory (PCM) has become a popular commercialized non-volatile memory (NVM), which has been deployed as a backing memory for DRAM main memory, secondary storage, or even as a DRAM main memory replacement. Like other NVMs, PCM has asymmetric access energy; writes dominate reads. When considering multilevel cells (MLC), this asymmetry can vary by an order of magnitude. Many schemes have been developed to take advantage of the asymmetric patterns of \u20180\u2019s and \u20181\u2019s in the data to reduce write energy. Because the memory is non-volatile, data can be recovered via physical attack or across system reboot cycles. To protect information stored in PCM against these attacks requires encryption. Unfortunately, most encryption algorithms scramble \u20180\u2019s and \u20181\u2019s in the data, effectively removing any patterns and negatively impacting schemes that leverage data bias and similarity to reduce write energy. In this paper, we introduce Virtual Coset Coding (VCC) as a workload-independent approach that reduces costly symbol transitions for storing encrypted data. VCC is based on two ideas. First, using coset encoding with random coset candidates, it is possible to effectively reduce the frequency of costly bit/symbol transitions when writing encrypted data. Second, a small set of random substrings can be used to achieve the same encoding efficiency as a large number of random coset candidates, but at a much lower encoding/decoding cost. Additionally, we demonstrate how VCC can be leveraged for energy reduction in combination with fault-mitigation and faulttolerance to dramatically increase the lifetimes of endurancelimited NVMs, such as PCM. We evaluate the design of VCC and demonstrate that it can be implemented on-chip with only a nominal area overhead. VCC reduces dynamic energy by 22-28% while maintaining the same performance. Using our multi-objective optimization approach achieves at least a 36% improvement in lifetime over the state-of-the-art and at least a 50% improvement in lifetime vs. an unencoded memory, while maintaining its energy savings and system performance.", "venue": "ArXiv", "authors": ["Stephen  Longofono", "Seyed Mohammad Seyedzadeh", "Alex K. Jones"], "year": 2021, "n_citations": 0}
{"id": 63742, "s2_id": "703a603c4922a717125d5435ac2b75bee9370071", "title": "High-Bandwidth Spatial Equalization for mmWave Massive MU-MIMO With Processing-in-Memory", "abstract": "All-digital basestation (BS) architectures enable superior spectral efficiency compared to hybrid solutions in massive multi-user MIMO systems. However, supporting large bandwidths with all-digital architectures at mmWave frequencies is challenging as traditional baseband processing would result in excessively high power consumption and large silicon area. The recently-proposed concept of finite-alphabet equalization is able to address both of these issues by using equalization matrices that contain low-resolution entries to lower the power and complexity of high-throughput matrix-vector products in hardware. In this brief, we explore two different finite-alphabet equalization hardware implementations that tightly integrate the memory and processing elements: (i) a parallel array of multiply-accumulate (MAC) units and (ii) a bit-serial processing-in-memory (PIM) architecture. Our all-digital VLSI implementation results in 28nm CMOS show that the bit-serial PIM architecture reduces the area and power consumption up to a factor of <inline-formula> <tex-math notation=\"LaTeX\">$ 2\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$3\\times $ </tex-math></inline-formula>, respectively, when compared to a parallel MAC array that operates at the same throughput.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Oscar  Casta\u00f1eda", "Sven  Jacobsson", "Giuseppe  Durisi", "Tom  Goldstein", "Christoph  Studer"], "year": 2020, "n_citations": 2}
{"id": 65696, "s2_id": "453155b2214d030a251a5c3170e51466bcb2408e", "title": "Approximate ripple carry and carry lookahead adders \u2014 A comparative analysis", "abstract": "Approximate ripple carry adders (RCAs) and carry lookahead adders (CLAs) are presented which are compared with accurate RCAs and CLAs for performing a 32-bit addition. The accurate and approximate RCAs and CLAs are implemented using a 32/28nm CMOS process. Approximations ranging from 4- to 20-bits are considered for the less significant adder bit positions. The simulation results show that approximate RCAs report reductions in the power-delay product (PDP) ranging from 19.5% to 82% than the accurate RCA for approximation sizes varying from 4- to 20-bits. Also, approximate CLAs report reductions in PDP ranging from 16.7% to 74.2% than the accurate CLA for approximation sizes varying from 4- to 20-bits. On average, for the approximation sizes considered, it is observed that approximate CLAs achieve a 46.5% reduction in PDP compared to the approximate RCAs. Hence, approximate CLAs are preferable over approximate RCAs for the low power implementation of approximate computer arithmetic.", "venue": "2017 IEEE 30th International Conference on Microelectronics (MIEL)", "authors": ["P.  Balasubramanian", "C.  Dang", "Douglas L. Maskell", "K.  Prasad"], "year": 2017, "n_citations": 7}
{"id": 69508, "s2_id": "f12759fe62dc442fe547a699ee9043a4631b1ed2", "title": "Constructing Depth-Optimum Circuits for Adders and AND-OR Paths", "abstract": "We examine the fundamental problem of constructing depth-optimum circuits for binary addition. More precisely, as in literature, we consider the following problem: Given auxiliary inputs $t_0, \\dotsc, t_{m-1}$, so-called generate and propagate signals, construct a depth-optimum circuit over the basis {AND2, OR2} computing all $n$ carry bits of an $n$-bit adder, where $m=2n-1$. In fact, carry bits are AND-OR paths, i.e., Boolean functions of the form $t_0 \\lor ( t_1 \\land (t_2 \\lor ( \\dots t_{m-1}) \\dots ))$. Classical approaches construct so-called prefix circuits which do not achieve a competitive depth. For instance, the popular construction by Kogge and Stone is only a $2$-approximation. A lower bound on the depth of any prefix circuit is $1.44 \\log_2 m$ + const, while recent non-prefix circuits have a depth of $\\log_2 m$ + $\\log_2 \\log_2 m$ + const. However, it is unknown whether any of these polynomial-time approaches achieves the optimum depth for all $m$. \nWe present a new exponential-time algorithm solving the problem optimally. The previously best exact algorithm with a running time of $\\mathcal O(2.45^m)$ is viable only for $m \\leq 29$. Our algorithm is significantly faster: We achieve a running time of $\\mathcal O(2.02^m)$ and apply sophisticated pruning strategies to improve practical running times dramatically. This allows us to compute optimum circuits for all $m \\leq 64$. Combining these computational results with new theoretical insights, we derive the optimum depths of $2^k$-bit adder circuits for all $k \\leq 13$, previously known only for $k \\leq 4$. \nIn fact, we solve a more general problem occurring in VLSI design: $delay$ optimization of a $generalization$ of AND-OR paths where AND and OR do not necessarily alternate. Our algorithm arises from our new structure theorem which characterizes delay-optimum generalized AND-OR path circuits.", "venue": "ArXiv", "authors": ["Ulrich  Brenner", "Anna  Hermann", "Jannik  Silvanus"], "year": 2020, "n_citations": 0}
{"id": 70555, "s2_id": "be28ed1be084385f5d389db25fd7f56cd2d7f7bf", "title": "Exploring computation-communication tradeoffs in camera systems", "abstract": "Cameras are the defacto sensor. The growing demand for real-time and low-power computer vision, coupled with trends towards high-efficiency heterogeneous systems, has given rise to a wide range of image processing acceleration techniques at the camera node and in the cloud. In this paper, we characterize two novel camera systems that use acceleration techniques to push the extremes of energy and performance scaling, and explore the computation-communication tradeoffs in their design. The first case study targets a camera system designed to detect and authenticate individual faces, running solely on energy harvested from RFID readers. We design a multi-accelerator SoC design operating in the sub-mW range, and evaluate it with real-world workloads to show performance and energy efficiency improvements over a general purpose microprocessor. The second camera system supports a 16-camera rig processing over 32 Gb/s of data to produce real-time 3D-360\u00b0 virtual reality video. We design a multi-FPGA processing pipeline that outperforms CPU and GPU configurations by up to 10\u00d7 in computation time, producing panoramic stereo video directly from the camera rig at 30 frames per second. We find that an early data reduction step, either before complex processing or offloading, is the most critical optimization for in-camera systems.", "venue": "2017 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Amrita  Mazumdar", "Thierry  Moreau", "Sung  Kim", "Meghan  Cowan", "Armin  Alaghi", "Luis  Ceze", "Mark  Oskin", "Visvesh  Sathe"], "year": 2017, "n_citations": 11}
{"id": 76439, "s2_id": "7f9cba895a8dfb2d15536a45a1f6c41abb21b5ca", "title": "Efficient reconfigurable regions management method for adaptive and dynamic FPGA based systems", "abstract": "Adaptive systems based on field programmable gate array (FPGA) architectures can greatly benefi t fro m th e high degree of flexibility offered by dynamic partial reconfiguration (DPR). By using this technique, hardware tasks can be loaded and reloaded on demand depending on the system requirements. In this paper, we propose to use the DPR for dynamic and adaptive implementation of a video cut detection application based on the MPEG-7 color structure descriptor (CSD). In the proposed implementation, different scenarios have been tested. Depending on the application and the system requirements, the CSD module can be loaded at any time with variable module size (corresponding to different version of the CSD) and allocated in different possible reconfigurable regions. Such implementation entails many problems related to communication, relocation and reconfigurable region management. We will demonstrate how we have made this implementation successful through the use of an appropriate design method. This method was proposed to support the management of variable-size hardware tasks on DPR FPGAs based adaptive systems. It permits to efficiently handle the reconfigurable area and to relocate the reconfigurable modules in different possible regions. The implementation results for the considered application show an important optimization in terms of configuration time (until 66 %) and memory storage (until 87 %) and an efficient hardware resources utilization rate (until 90%).", "venue": "ArXiv", "authors": ["Marwa  Hannachi", "Abdesslam Ben Abdelali", "Hassan  Rabah", "Abdellatif  Mtibaa"], "year": 2018, "n_citations": 3}
{"id": 77572, "s2_id": "5c9430311adb49a8e78846c9b55e079505244670", "title": "BRISC-V: An Open-Source Architecture Design Space Exploration Toolbox", "abstract": "In this work, we introduce the BRISC-V Toolbox, a register-transfer level (RTL) tool for architecture design space exploration. The BRISC-V Toolbox is an open-source, parameterized, synthesizable set of RTL modules for designing RISC-V based single and multi-core architecture systems. The toolbox is designed with a high degree of modularity. It provides highly parameterized, composable RTL modules for fast and accurate exploration of different RISC-V based core complexities, multi-level caching and memory organizations, system topologies, router architectures and routing schemes. BRISC-V can be used for both RTL simulation and FPGA based emulation. The hardware modules are implemented in synthesizable Verilog using no vendor-specific blocks. The toolbox includes a GCC RISC-V compiler tool-chain to assist in developing software for the cores and a web based system configuration graphical user interface (GUI). The BRISC-V Toolbox supports a myriad of RISC-V architectures, ranging from a simple single cycle processor to a multi-core SoC with a complex memory hierarchy and a network-on-chip. The modules are designed to support incremental additions and modifications. The module interfaces are carefully designed to enable an arbitrary pipeline depth and changes to a single module without impacting the rest of the system. The BRISC-V platform allows researchers to quickly instantiate complete working RISC-V multicore systems with synthesizable RTL correctness and make targeted modifications to fit their needs.", "venue": "FPGA", "authors": ["Sahan  Bandara", "Alan  Ehret", "Donato  Kava", "Michel A. Kinsy"], "year": 2019, "n_citations": 10}
{"id": 79776, "s2_id": "20107caad4438aa3ecdc164698e4bbf69fc5c9f7", "title": "Heterogeneous Dual-Core Overlay Processor for Light-Weight CNNs", "abstract": "Convolutional neural networks (CNNs) have achieved extensive success on miscellaneous artificial intelligence applications such as image classification and object detection. A plethora of models emerge with different operators and architectures, gradually shifting attention from accuracy to efficiency in terms of speed and power, since VGG-like architecture from early stage has significant redundancy. Light-weight CNNs are proposed to reduce computation complexity and parameter amount. MobileNets, one typical example of light-weight CNNs, adopt depthwise separable convolution, while others such as SqueezeNet alter model topology to spare computation power.", "venue": "2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Tiandong  Zhao", "Yunxuan  Yu", "Kun  Wang", "Lei  He"], "year": 2021, "n_citations": 0}
{"id": 93104, "s2_id": "fd571eee278f30a818ccf985cd4713beb48d097a", "title": "Asynchronous early output section-carry based carry lookahead adder with alias carry logic", "abstract": "A new asynchronous early output section-carry based carry lookahead adder (SCBCLA) with alias carry output logic is presented in this paper. To evaluate the proposed SCBCLA with alias carry logic and to make a comparison with other CLAs, a 32-bit addition operation is considered. Compared to the weak-indication SCBCLA with alias logic, the proposed early output SCBCLA with alias logic reports a 13% reduction in area without any increases in latency and power dissipation. On the other hand, in comparison with the early output recursive CLA (RCLA), the proposed early output SCBCLA with alias logic reports a 16% reduction in latency while occupying almost the same area and dissipating almost the same average power. All the asynchronous CLAs are quasi-delay-insensitive designs which incorporate the delay-insensitive dual-rail data encoding and adhere to the 4-phase return-to-zero handshaking. The adders were realized and the simulations were performed based on a 32/28nm CMOS process.", "venue": "2017 IEEE 30th International Conference on Microelectronics (MIEL)", "authors": ["P.  Balasubramanian", "C.  Dang", "Douglas L. Maskell", "K.  Prasad"], "year": 2017, "n_citations": 5}
{"id": 97599, "s2_id": "5edbccff8abdf1c4509e27cf9c626815b6ec75f0", "title": "FPGA-based CNN inference accelerator synthesized from multi-threaded C software", "abstract": "A deep-learning inference accelerator is synthesized from a C-language software program parallelized with Pthreads. The software implementation uses the well-known producer/consumer model with parallel threads interconnected by FIFO queues. The LegUp high-level synthesis (HLS) [1] tool synthesizes threads into parallel FPGA hardware, translating software parallelism into spatial parallelism. A complete system is generated where convolution, pooling and padding are realized in the synthesized accelerator, with remaining tasks executing on an embedded ARM processor. The accelerator incorporates reduced precision, and a novel approach for zero-weight-skipping in convolution. On a mid-sized Intel Arria 10 SoC FPGA, peak performance on VGG-16 is 138 effective GOPS.", "venue": "2017 30th IEEE International System-on-Chip Conference (SOCC)", "authors": ["Jin Hee Kim", "Brett  Grady", "Ruolong  Lian", "John  Brothers", "Jason Helge Anderson"], "year": 2017, "n_citations": 28}
{"id": 100858, "s2_id": "ecac0a8b0833e345adf024f0222371c5aec0ab8e", "title": "Hardware Translation Coherence for Virtualized Systems", "abstract": "To improve system performance, operating systems (OSes) often undertake activities that require modification of virtual-to-physical address translations. For example, the OS may migrate data between physical pages to manage heterogeneous memory devices. We refer to such activities as page remappings. Unfortunately, page remappings are expensive. We show that a big part of this cost arises from address translation coherence, particularly on systems employing virtualization. In response, we propose hardware translation invalidation and coherence or HATRIC, a readily implementable hardware mechanism to piggyback translation coherence atop existing cache coherence protocols. We perform detailed studies using KVM-based virtualization, showing that HATRIC achieves up to 30% performance and 10% energy benefits, for per-CPU area overheads of 0.2%. We also quantify HATRIC's benefits on systems running Xen and find up to 33% performance improvements.", "venue": "OPSR", "authors": ["Zi  Yan", "J\u00e1n  Vesel\u00fd", "Guilherme  Cox", "Abhishek  Bhattacharjee"], "year": 2018, "n_citations": 5}
{"id": 101915, "s2_id": "7bedde631566a1a527bc2c47c04434c34e6e5eac", "title": "PANDA: Processing-in-MRAM Accelerated De Bruijn Graph based DNA Assembly", "abstract": "Spurred by widening gap between data processing speed and data communication speed in Von-Neumann computing architectures, some bioinformatic applications have harnessed the computational power of Processing-in-Memory (PIM) platforms. However, the performance of PIMs unavoidably diminishes when dealing with such complex applications seeking bulk bit-wise comparison or addition operations. In this work, we present an efficient Processing-in-MRAM Accelerated De Bruijn Graph based DNA Assembly platform named PANDA based on an optimized and hardware-friendly genome assembly algorithm. PANDA is able to assemble large-scale DNA sequence data-set from all-pair overlaps. We first design PANDA platform that exploits MRAM as a computational memory and converts it to a potent processing unit for genome assembly. PANDA can execute not only efficient bulk bit-wise X(N)OR-based comparison/addition operations heavily required for the genome assembly task but a full-set of 2-/3-input logic operations inside MRAM chip. We then develop a highly parallel and step-by-step hardware-friendly DNA assembly algorithm for PANDA that only requires the developed in-memory logic operations. The platform is then configured with a novel data partitioning and mapping technique that provides local storage and processing to fully utilize the algorithm-level's parallelism. The cross-layer simulation results demonstrate that PANDA reduces the run time and power, respectively, by a factor of 18 and 11 compared with CPU. Besides, speed-ups of up-to 2-4x can be obtained over recent processing-in-MRAM platforms to perform the same task.", "venue": "ArXiv", "authors": ["Shaahin  Angizi", "Naima Ahmed Fahmi", "Wei  Zhang", "Deliang  Fan"], "year": 2020, "n_citations": 0}
{"id": 103080, "s2_id": "d346b633e208c16b42cc03922e8a0f0d3c0b4331", "title": "Refresh Triggered Computation: Improving the Energy Efficiency of Convolutional Neural Network Accelerators", "abstract": "Recently, many studies proposed CNN accelerator architectures with custom computation units that try to improve energy-efficiency and performance of CNN by minimizing data transfers from DRAM-based main memory. However, in these architectures, DRAM still contributes half of the overall energy consumption of the system, on average. A key factor of the high energy consumption of DRAM is the refresh overhead, which is estimated to consume 40% of the total DRAM energy. \nWe propose a new mechanism, Refresh Triggered Computation (RTC), that exploits the memory access patterns of CNN applications to reduce the number of refresh operations. RTC mainly uses two techniques to mitigate the refresh overhead. First, Refresh Triggered Transfer (RTT) is based on our new observation that a CNN application accesses a large portion of the DRAM in a predictable and recurring manner. Thus, the read/write accesses inherently refresh the DRAM, and therefore a significant fraction of refresh operations can be skipped. Second, Partial Array Auto-Refresh (PAAR) eliminates the refresh operations to DRAM regions that do not store any data. \nWe propose three RTC designs, each of which requires a different level of aggressiveness in terms of customization to the DRAM subsystem. All of our designs have small overhead. Even the most aggressive design of RTC imposes an area overhead of only 0.18% in a 2Gb DRAM chip and can have less overhead for denser chips. Our experimental evaluation on three well-known CNNs (i.e., AlexNet, LeNet, and GoogleNet) show that RTC can reduce the DRAM refresh energy from 25% to 96%, for the least aggressive and the most aggressive designs, respectively. Although we mainly use CNNs in our evaluations, we believe RTC can be applied to a wide range of applications, whose memory access patterns remain predictable for sufficiently long time.", "venue": "ArXiv", "authors": ["Syed M. A. H. Jafri", "Hasan  Hassan", "Ahmed  Hemani", "Onur  Mutlu"], "year": 2019, "n_citations": 2}
{"id": 103238, "s2_id": "f2dd73ae127c5ee3713a92e1057eddea92fbf207", "title": "A 0.3\u20132.6 TOPS/W precision-scalable processor for real-time large-scale ConvNets", "abstract": "A low-power precision-scalable processor for ConvNets or convolutional neural networks (CNN) is implemented in a 40nm technology. Its 256 parallel processing units achieve a peak 102GOPS running at 204MHz. To minimize energy consumption while maintaining throughput, this works is the first to both exploit the sparsity of convolutions and to implement dynamic precision-scalability enabling supply- and energy scaling. The processor is fully C-programmable, consumes 25-288mW at 204 MHz and scales efficiency from 0.3-2.6 real TOPS/W. This system hereby outperforms the state-of-the-art up to 3.9\u00d7 in energy efficiency.", "venue": "2016 IEEE Symposium on VLSI Circuits (VLSI-Circuits)", "authors": ["Bert  Moons", "Marian  Verhelst"], "year": 2016, "n_citations": 134}
{"id": 105354, "s2_id": "62008bd1a19b9fddafc815ac10c20b2384d5453a", "title": "SpecBox: A Label-Based Transparent Speculation Scheme Against Transient Execution Attacks", "abstract": "Speculative execution techniques have been a cornerstone of modern processors to improve instruction-level parallelism. However, recent studies showed that this kind of techniques could be exploited by attackers to leak secret data via transient execution attacks, such as Spectre. Many defenses are proposed to address this problem, but they all face various challenges: (1) Tracking data flow in the instruction pipeline could comprehensively address this problem, but it could cause pipeline stalls and incur high performance overhead; (2) Making side effect of speculative execution imperceptible to attackers, but it often needs additional storage components and complicated data movement operations. In this paper, we propose a label-based transparent speculation scheme called SPECBOX. It dynamically partitions the cache system to isolate speculative data and nonspeculative data, which can prevent transient execution from being observed by subsequent execution. Moreover, it uses thread ownership semaphores to prevent speculative data from being accessed across cores. In addition, SPECBOX also enhances the auxiliary components in the cache system against transient execution attacks, such as hardware prefetcher. Our security analysis shows that SPECBOX is secure and the performance evaluation shows that the performance overhead on SPEC CPU 2006 and PARSEC-3.0 benchmarks is small.", "venue": "ArXiv", "authors": ["Bowen  Tang", "Chenggang  Wu", "Zhe  Wang", "Lichen  Jia", "Pen-Chung  Yew", "Yueqiang  Cheng", "Yinqian  Zhang", "Chenxi  Wang", "Guoqing Harry Xu"], "year": 2021, "n_citations": 0}
{"id": 106429, "s2_id": "496aa8ad9d651038e2c825e05714d16132b715db", "title": "Producing NLP-based On-line Contentware", "abstract": "For its internal needs as well as for commercial purposes, CDC Group has produced several NLP-based on-line contentware applications for years. The development process of such applications is subject to numerous constraints such as quality of service, integration of new advances in NLP, direct reactions from users, continuous versioning, short delivery deadlines and cost control. Following this industrial and commercial experience, malleability of the applications, their openness towards foreign components, efficiency of applications and their ease of exploitation have appeared to be key points. In this paper, we describe TalLab, a powerful architecture for on-line contentware which fulfils these requirements.", "venue": "ArXiv", "authors": ["Francis  Wolinski", "Frantz  Vichot", "Olivier  Gremont"], "year": 1998, "n_citations": 13}
{"id": 109044, "s2_id": "ecb72580979ce5b776ece8410d3317ea6466db50", "title": "C based hardware design for wireless applications", "abstract": "The algorithms used in wireless applications are increasingly more sophisticated and consequently more challenging to implement in hardware. Traditional design flows require developing the micro architecture, coding the RTL, and verifying the generated RTL against the original functional C or MATLAB specification. This paper describes a C-based design flow that is well suited for the hardware implementation of DSP algorithms commonly found in wireless applications. The C design low relies on guided synthesis to generate the RTL directly from the untimed C algorithm. The specifics of the C-based design flow are described using a simple DSP filtering algorithm consisting of a forward adaptive equalizer, a 64-QAM slicer and an adaptive decision feedback equalizer. The example illustrates some of the capabilities and advantages offered by this flow.", "venue": "Design, Automation and Test in Europe", "authors": ["Andr\u00e9s  Takach", "Bryan  Bowyer", "Thomas  Bollaert"], "year": 2005, "n_citations": 28}
{"id": 110504, "s2_id": "8f24ad970bf520486d151c6f8c3aba09cc9badfa", "title": "ATRIA: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-DRAM CNN Processing", "abstract": "With the rapidly growing use of Convolutional Neural Networks (CNNs) in real-world applications related to machine learning and Artificial Intelligence (Al), several hardware accelerator designs for CNN inference and training have been proposed recently. In this paper, we present ATRIA, a novel bit-pArallel sTochastic aRithmetic based In-DRAM Accelerator for energy-efficient and high-speed inference of CNNs. ATRIA employs light-weight modifications in DRAM cell arrays to implement bit-parallel stochastic arithmetic based acceleration of multiply-accumulate (MAC) operations inside DRAM. ATRIA significantly improves the latency, throughput, and efficiency of processing CNN inferences by performing 16 MAC operations in only five consecutive memory operation cycles. We mapped the inference tasks of four benchmark CNNs on ATRIA to compare its performance with five state-of-the-art in-DRAM CNN accelerators from prior work. The results of our analysis show that ATRIA exhibits only 3.5% drop in CNN inference accuracy and still achieves improvements of up to 3.2\u00d7 in frames-per-second (FPS) and up to 10\u00d7 in efficiency (FPS/W/mm2), compared to the best-performing in-DRAM accelerator from prior work.", "venue": "2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Supreeth Mysore Shivanandamurthy", "Ishan. G. Thakkar", "Sayed Ahmad Salehi"], "year": 2021, "n_citations": 0}
{"id": 116414, "s2_id": "28e153b0fc53b92d44874274c91856e14428a7d2", "title": "DRACO: Co-Optimizing Hardware Utilization, and Performance of DNNs on Systolic Accelerator", "abstract": "The number of processing elements (PEs) in a fixed-sized systolic accelerator is well matched for large and compute-bound DNNs; whereas, memory-bound DNNs suffer from PE underutilization and fail to achieve peak performance and energy efficiency. To mitigate this, specialized dataflow and/or micro-architectural techniques have been proposed. However, due to the longer development cycle and the rapid pace of evolution in the deep learning fields, these hardware-based solutions can be obsolete and ineffective in dealing with PE underutilization for state-of-the-art DNNs. In this work, we address the challenge of PE underutilization at the algorithm front and propose data reuse aware co-optimization (DRACO). This improves the PE utilization of memory-bound DNNs without any additional need for dataflow/micro-architecture modifications. Furthermore, unlike the previous co-optimization methods, DRACO not only maximizes performance and energy efficiency but also improves the predictive performance of DNNs. To the best of our knowledge, DRACO is the first work that resolves the resource underutilization challenge at the algorithm level and demonstrates a trade-off between computational efficiency, PE utilization, and predictive performance of DNN. Compared to the state-of-the-art row stationary dataflow, DRACO achieves 41.8% and 42.6% improvement in average PE utilization and inference latency (respectively) with negligible loss in predictive performance in MobileNetV1 on a 64\u00d764 systolic array. DRACO provides seminal insights for utilization-aware DNN design methodologies that can fully leverage the computation power of systolic array-based hardware accelerators.", "venue": "2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Nandan Kumar Jha", "Shreyas  Ravishankar", "Sparsh  Mittal", "Arvind  Kaushik", "Dipan  Mandal", "Mahesh  Chandra"], "year": 2020, "n_citations": 3}
{"id": 117624, "s2_id": "ac9b21d8408c19eef33778e05ba908ba969f29ed", "title": "NAAS: Neural Accelerator Architecture Search", "abstract": "Data-driven, automatic design space exploration of neural accelerator architecture is desirable for specialization and productivity. Previous frameworks focus on sizing the numerical architectural hyper-parameters while neglect searching the PE connectivities and compiler mappings. To tackle this challenge, we propose Neural Accelerator Architecture Search (NAAS) that holistically searches the neural network architecture, accelerator architecture and compiler mapping in one optimization loop. NAAS composes highly matched architectures together with efficient mapping. As a data-driven approach, NAAS rivals the human design Eyeriss by $4.4 \\times$ EDP reduction with 2.7% accuracy improvement on ImageNet under the same computation resource, and offers $1.4 \\times$ to $3.5 \\times$ EDP reduction than only sizing the architectural hyper-parameters.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Yujun  Lin", "Mengtian  Yang", "Song  Han"], "year": 2021, "n_citations": 1}
{"id": 119856, "s2_id": "dffb2c249542492e9872a156b723d3be98e24a92", "title": "LazyFP: Leaking FPU Register State using Microarchitectural Side-Channels", "abstract": "Modern processors utilize an increasingly large register set to facilitate efficient floating point and SIMD computation. This large register set is a burden for operating systems, as its content needs to be saved and restored when the operating system context switches between tasks. As an optimization, the operating system can defer the context switch of the FPU and SIMD register set until the first instruction is executed that needs access to these registers. Meanwhile, the old content is left in place with the hope that the current task might not use these registers at all. This optimization is commonly called lazy FPU context switching. To make it possible, a processor offers the ability to toggle the availability of instructions utilizing floating point and SIMD registers. If the instructions are turned off, any attempt of executing them will generate a fault. \nIn this paper, we present an attack that exploits lazy FPU context switching and allows an adversary to recover the FPU and SIMD register set of arbitrary processes or VMs. The attack works on processors that transiently execute FPU or SIMD instructions that follow an instruction generating the fault indicating the first use of FPU or SIMD instructions. On operating systems using lazy FPU context switching, the FPU and SIMD register content of other processes or virtual machines can then be reconstructed via cache side effects. \nWith SIMD registers not only being used for cryptographic computation, but also increasingly for simple operations, such as copying memory, we argue that lazy FPU context switching is a dangerous optimization that needs to be turned off in all operating systems, if there is a chance that they run on affected processors.", "venue": "ArXiv", "authors": ["Julian  Stecklina", "Thomas  Prescher"], "year": 2018, "n_citations": 66}
{"id": 122937, "s2_id": "14d8d2588cb3e58ce1f04ee4cbe35710e73cd01e", "title": "Low Overhead Online Data Flow Tracking for Intermittently Powered Non-Volatile FPGAs", "abstract": "Energy harvesting is an attractive way to power future Internet of Things (IoT) devices since it can eliminate the need for battery or power cables. However, harvested energy is intrinsically unstable. While Field-programmable Gate Array (FPGAs) have been widely adopted in various embedded systems, it is hard to survive unstable power since all the memory components in FPGA are based on volatile Static Random-access Memory (SRAMs). The emerging non-volatile memory-based FPGAs provide promising potentials to keep configuration data on the chip during power outages. Few works have considered implementing efficient runtime intermediate data checkpoint on non-volatile FPGAs. To realize accumulative computation under intermittent power on FPGA, this article proposes a low-cost design framework, Data-Flow-Tracking FPGA (DFT-FPGA), which utilizes binary counters to track intermediate data flow. Instead of keeping all on-chip intermediate data, DFT-FPGA only targets on necessary data that is labeled by off-line analysis and identified by an online tracking system. The evaluation shows that compared with state-of-the-art techniques, DFT-FPGA can realize accumulative computing with less off-line workload and significantly reduce online roll-back time and resource utilization.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Xinyi  Zhang", "Clay  Patterson", "Yongpan  Liu", "Chengmo  Yang", "Chun Jason Xue", "Jingtong  Hu"], "year": 2020, "n_citations": 2}
{"id": 124191, "s2_id": "2c960ab43265fb5b909d27ee98a6adcdc69ee970", "title": "Compiler Directed Speculative Intermittent Computation", "abstract": "This paper presents CoSpec, a new architecture/compiler co-design scheme that works for commodity in-order processors used in energy-harvesting systems. To achieve crash consistency without requiring unconventional architectural support, CoSpec leverages speculation assuming that power failure is not going to occur and thus holds all committed stores in a store buffer (SB), as if they were speculative, in case of mispeculation. CoSpec compiler first partitions a given program into a series of recoverable code regions with the SB size in mind, so that no region overflows the SB. When the program control reaches the end of each region, the speculation turns out to be successful, thus releasing all the buffered stores of the region to NVM. If power failure occurs during the execution of a region, all its speculative stores disappear in the volatile SB, i.e., they never affect program states in NVM. Consequently, the interrupted region can be restarted with consistent program states in the wake of power failure. To hide the latency of the SB release, i.e., essentially NVM writes, at each region boundary, CoSpec overlaps the NVM writes of the current region with the speculative execution of the next region. Such instruction level parallelism gives an illusion of out-of-order execution on top of the in-order processor, achieving a speedup of more than 1.2X when there is no power outage. Our experiments on a set of real energy harvesting traces with frequent outages demonstrate that CoSpec outperforms the state-of-the-art scheme by 1.8~3X on average.", "venue": "ArXiv", "authors": ["Jongouk  Choi", "Qingrui  Liu", "Changhee  Jung"], "year": 2020, "n_citations": 1}
{"id": 128766, "s2_id": "7547b514498917dc53e701f2240b88e863ec0c9b", "title": "FuSeConv: Fully Separable Convolutions for Fast Inference on Systolic Arrays", "abstract": "Both efficient neural networks and hardware accelerators are being explored to speed up DNN inference on edge devices. For example, MobileNet uses depthwise separable convolution to achieve much lower latency, while systolic arrays provide much higher performance per watt. Interestingly however, the combination of these two ideas is inefficient: The computational patterns of depth-wise separable convolution are not systolic and lack data reuse to saturate the systolic array's constrained dataflow. In this paper, we propose FuSeConv (Fully-Separable Convolution) as a drop-in replacement for depth-wise separable convolution. FuSeConv generalizes the decomposition of convolutions fully to separable 1D convolutions along spatial and depth dimensions. The resultant computation is systolic and efficiently utilizes the systolic array with a slightly modified dataflow. With FuSeConv, we achieve a significant speed-up of 3x-7x with the MobileNet family of networks on a systolic array of size 64x64, with comparable accuracy on the ImageNet dataset. The high speed-up motivates exploration of hardware-aware Neural Operator Search (NOS) in complement to ongoing efforts on Neural Architecture Search (NAS).", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Surya  Selvam", "Vinod  Ganesan", "Pratyush  Kumar"], "year": 2021, "n_citations": 1}
{"id": 130436, "s2_id": "eb980880600b557204aa7f324cc062762a94ca73", "title": "Automatic generation of high-coverage tests for RTL designs using software techniques and tools", "abstract": "Register Transfer Level (RTL) design validation is a crucial stage in the hardware design process. We present a new approach to enhancing RTL design validation using available software techniques and tools. Our approach converts the source code of a RTL design into a C++ software program. Then a powerful symbolic execution engine is employed to execute the converted C++ program symbolically to generate test cases. To better generate efficient test cases' we limit the number of cycles to guide symbolic execution. Moreover' we add bit-level symbolic variable support into the symbolic execution engine. Generated test cases are further evaluated by simulating the RTL design to get accurate coverage. We have evaluated the approach on a floating point unit (FPU) design. The preliminary results show that our approach can deliver high-quality tests to achieve high coverage.", "venue": "2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)", "authors": ["Yu  Zhang", "Wenlong  Feng", "Mengxing  Huang"], "year": 2016, "n_citations": 1}
{"id": 134852, "s2_id": "a9aa44e7462718dd7333df239953815721d81fd7", "title": "Wishbone bus Architecture - A Survey and Comparison", "abstract": "The performance of an on-chip interconnection architecture used for communication between IP cores depends on the efficiency of its bus architecture. Any bus architecture having advantages of faster bus clock speed, extra data transfer cycle, improved bus width and throughput is highly desirable for a low cost, reduced time-to-market and efficient System-on-Chip (SoC). This paper presents a survey of WISHBONE bus architecture and its comparison with three other on-chip bus architectures viz. Advanced Microcontroller Bus Architecture (AMBA) by ARM, CoreConnect by IBM and Avalon by Altera. The WISHBONE Bus Architecture by Silicore Corporation appears to be gaining an upper edge over the other three bus architecture types because of its special performance parameters like the use of flexible arbitration scheme and additional data transfer cycle (Read-Modify-Write cycle). Moreover, its IP Cores are available free for use requiring neither any registration nor any agreement or license.", "venue": "VLSIC 2012", "authors": ["Mohandeep  Sharma", "Dilip  Kumar"], "year": 2012, "n_citations": 15}
{"id": 136581, "s2_id": "1ceea275e329546e837ae307d9b1ca29cba8ad59", "title": "The Memory Controller Wall: Benchmarking the Intel FPGA SDK for OpenCL Memory Interface", "abstract": "Supported by their high power efficiency and recent advancements in High Level Synthesis (HLS), FPGAs are quickly finding their way into HPC and cloud systems. Large amounts of work have been done so far on loop and area optimizations for different applications on FPGAs using HLS. However, a comprehensive analysis of the behavior and efficiency of the memory controller of FPGAs is missing in literature, which becomes even more crucial when the limited memory bandwidth of modern FPGAs compared to their GPU counterparts is taken into account. In this work, we will analyze the memory interface generated by Intel FPGA SDK for OpenCL with different configurations for input/output arrays, vector size, interleaving, kernel programming model, on-chip channels, operating frequency, padding, and multiple types of overlapped blocking. Our results point to multiple shortcomings in the memory controller of Intel FPGAs, especially with respect to memory access alignment, that can hinder the programmer\u2019s ability in maximizing memory performance in their design. For some of these cases, we will provide work-arounds to improve memory bandwidth efficiency; however, a general solution will require major changes in the memory controller itself.", "venue": "2019 IEEE/ACM International Workshop on Heterogeneous High-performance Reconfigurable Computing (H2RC)", "authors": ["Hamid Reza Zohouri", "Satoshi  Matsuoka"], "year": 2019, "n_citations": 4}
{"id": 136843, "s2_id": "7a06bef512dd7646d8b868f51d5272a584ba3c65", "title": "Energy Efficiency Aspects of the AMD Zen 2 Architecture", "abstract": "In High Performance Computing, systems are evaluated based on their computational throughput. However, performance in contemporary server processors is primarily limited by power and thermal constraints. Ensuring operation within a given power envelope requires a wide range of sophisticated control mechanisms. While some of these are handled transparently by hardware control loops, others are controlled by the operating system. A lack of publicly disclosed implementation details further complicates this topic. However, understanding these mechanisms is a prerequisite for any effort to exploit the full computing capability and to minimize the energy consumption of today\u2019s server systems. This paper highlights the various energy efficiency aspects of the AMD Zen 2 microarchitecture to facilitate system understanding and optimization. Key findings include qualitative and quantitative descriptions regarding core frequency transition delays, workload-based frequency limitations, effects of I/O die P-states on memory performance as well as discussion on the built-in power monitoring capabilities and its limitations. Moreover, we present specifics and caveats of idle states, wakeup times as well as the impact of idling and inactive hardware threads and cores on the performance of active resources such as other cores.", "venue": "2021 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Robert  Sch\u00f6ne", "Thomas  Ilsche", "Mario  Bielert", "Markus  Velten", "Markus  Schmidl", "Daniel  Hackenberg"], "year": 2021, "n_citations": 1}
{"id": 137042, "s2_id": "6049a4ea931693a133b0ac3aaa835809697e3b7e", "title": "A Power-Efficient Binary-Weight Spiking Neural Network Architecture for Real-Time Object Classification", "abstract": "Neural network hardware is considered an essential part of future edge devices. In this paper, we propose a binary-weight spiking neural network (BW-SNN) hardware architecture for low-power real-time object classification on edge platforms. This design stores a full neural network on-chip, and hence requires no off-chip bandwidth. The proposed systolic array maximizes data reuse for a typical convolutional layer. A 5-layer convolutional BW-SNN hardware is implemented in 90nm CMOS. Compared with state-of-the-art designs, the area cost and energy per classification are reduced by 7$\\times$ and 23$\\times$, respectively, while also achieving a higher accuracy on the MNIST benchmark. This is also a pioneering SNN hardware architecture that supports advanced CNN architectures.", "venue": "ArXiv", "authors": ["Pai-Yu  Tan", "Po-Yao  Chuang", "Yen-Ting  Lin", "Cheng-Wen  Wu", "Juin-Ming  Lu"], "year": 2020, "n_citations": 3}
{"id": 146175, "s2_id": "d42923c818a7822a809aa33aa036ce43c5e7c560", "title": "A simple 1-byte 1-clock RC4 design and its efficient implementation in FPGA coprocessor for secured ethernet communication", "abstract": "In the field of cryptography till date the 1-byte in 1-clock is the best known RC4 hardware design [1], while the 1-byte in 3clocks is the best known implementation [2,3]. The design algorithm in [1] considers two consecutive bytes together and processes them in 2 clocks. The design of 1-byte in 3-clocks is too much modular and clock hungry. In this paper considering the RC4 algorithm, as it is, a simpler RC4 hardware design providing higher throughput is proposed in which 1-byte is processed in 1-clock. In the design two sequential tasks are executed as two independent events during rising and falling edges of the same clock and the swapping is directly executed using a MUX-DEMUX combination. The power consumed in behavioral and structural designs of RC4 are estimated and a power optimization technique is proposed. The NIST statistical test suite is run on RC4 key streams in order to know its randomness property. The encryption and decryption designs are respectively embedded on two FPGA boards with RC4 in a custom coprocessor followed by Ethernet communication.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Sangeet  Saha", "J. K. M. Sadique Uz Zaman", "Suman  Das", "Amlan  Chakrabarti", "Ranjan  Ghosh"], "year": 2012, "n_citations": 10}
{"id": 152675, "s2_id": "3fa31cc248e3c45a8fb909f5030c1cb9a33152c6", "title": "Taming Process Variations in CNFET for Efficient Last Level Cache Design", "abstract": "Carbon nanotube field-effect transistors (CNFET) emerge as a promising alternative to CMOS transistors for the much higher speed and energy efficiency, which makes the technology particularly suitable for building the energy-hungry last level cache (LLC). However, the process variations (PVs) in CNFET caused by the imperfect fabrication lead to large timing variation and the worst-case timing dramatically limits the LLC operation speed. Particularly, we observe that the CNFETbased cache latency distribution is closely related to the LLC layouts. For the two typical LLC layouts that have the CNT growth direction aligned to the cache way direction and cache set direction respectively, we proposed variation-aware set aligned (VASA) cache and variation-aware way aligned (VAWA) cache in combination with corresponding cache optimizations such as data shuffling and page mapping to enable low-latency cache for frequently used data. According to our experiments, the optimized LLC reduces the average access latency by 32% and 45% compared to the baseline designs on the two different CNFET layouts respectively while it improves the overall performance by 6% and 9% and reduces the energy consumption by 4% and 8% respectively. In addition, with both the architecture induced latency variation and PV incurred latency variation considered in a unified model, we extended the VAWA and VASA cache design for the CNFET-based NUCA and the proposed NUCA achieves both significant performance improvement and energy saving compared to the straightforward variation-aware NUCA.", "venue": "ArXiv", "authors": ["Dawen  Xu", "Zhuangyu  Feng", "Cheng  Liu", "Li  Li", "Ying  Wang", "Yuanqing  Cheng", "Huawei  Li", "Xiaowei  Li"], "year": 2021, "n_citations": 0}
{"id": 157474, "s2_id": "a0dd5427964c8099b4a24fcec660cf5c9374889f", "title": "Energy-Efficient Hardware-Accelerated Synchronization for Shared-L1-Memory Multiprocessor Clusters", "abstract": "The steeply growing performance demands for highly power- and energy-constrained processing systems such as end-nodes of the Internet-of-Things (IoT) have led to parallel near-threshold computing (NTC), joining the energy-efficiency benefits of low-voltage operation with the performance typical of parallel systems. Shared-L1-memory multiprocessor clusters are a promising architecture, delivering performance in the order of GOPS and over 100 GOPS/W of energy-efficiency. However, this level of computational efficiency can only be reached by maximizing the effective utilization of the processing elements (PEs) available in the clusters. Along with this effort, the optimization of PE-to-PE synchronization and communication is a critical factor for performance. In this article, we describe a light-weight hardware-accelerated synchronization and communication unit (SCU) for tightly-coupled clusters of processors. We detail the architecture, which enables fine-grain per-PE power management, and its integration into an eight-core cluster of RISC-V processors. To validate the effectiveness of the proposed solution, we implemented the eight-core cluster in advanced 22 nm FDX technology and evaluated performance and energy-efficiency with tunable microbenchmarks and a set of real-life applications and kernels. The proposed solution allows synchronization-free regions as small as 42 cycles, over 41\u00d7 smaller than the baseline implementation based on fast test-and-set access to L1 memory when constraining the microbenchmarks to 10 percent synchronization overhead. When evaluated on the real-life DSP-applications, the proposed SCU improves performance by up to 92 and 23 percent on average and energy efficiency by up to 98 and 39 percent on average.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Florian  Glaser", "Giuseppe  Tagliavini", "Davide  Rossi", "Germain  Haugou", "Qiuting  Huang", "Luca  Benini"], "year": 2021, "n_citations": 1}
{"id": 166368, "s2_id": "e011c0b27a1cbc903e118be5236f6be11ca5a056", "title": "Hardware support for QoS-based function allocation in reconfigurable systems", "abstract": "This paper presents a new approach for allocating suitable function-implementation variants depending on given quality-of-service function requirements for run-time reconfigurable multi-device systems. Our approach adapts methodologies from the domain of knowledge-based systems which can be used for doing run-time hardware/software resource usage optimizations.", "venue": "Design, Automation and Test in Europe", "authors": ["Michael  Ullmann", "Wansheng  Jin", "J\u00fcrgen  Becker"], "year": 2005, "n_citations": 4}
{"id": 171121, "s2_id": "fef10221ba1d1cd3abc7999372da4f430a15a090", "title": "Design Of A Reconfigurable DSP Processor With Bit Efficient Residue Number System", "abstract": "Residue Number System (RNS), which originates from the Chinese Remainder Theorem, offers a promising future in VLSI because of its carry-free operations in addition, subtraction and multiplication. This property of RNS is very helpful to reduce the complexity of calculation in many applications. A residue number system represents a large integer using a set of smaller integers, called residues. But the area overhead, cost and speed not only depend on this word length, but also the selection of moduli, which is a very crucial step for residue system. This parameter determines bit efficiency, area, frequency etc. In this paper a new moduli set selection technique is proposed to improve bit efficiency which can be used to construct a residue system for digital signal processing environment. Subsequently, it is theoretically proved and illustrated using examples, that the proposed solution gives better results than the schemes reported in the literature. The novelty of the architecture is shown by comparison the different schemes reported in the literature. Using the novel moduli set, a guideline for a Reconfigurable Processor is presented here that can process some predefined functions. As RNS minimizes the carry propagation, the scheme can be implemented in Real Time Signal Processing & other fields where high speed computations are required.", "venue": "VLSIC 2012", "authors": ["Chaitali Biswas Dutta", "Partha  Garai", "Amitabha  Sinha"], "year": 2012, "n_citations": 13}
{"id": 180147, "s2_id": "63868b0f1fd6cef60c0016e52f10e8e95db8c6db", "title": "Energon: Towards Efficient Acceleration of Transformers Using Dynamic Sparse Attention", "abstract": "In recent years, transformer models have revolutionized Natural Language Processing (NLP) and also show promising performance on Computer Vision (CV) tasks. Despite their effectiveness, transformers\u2019 attention operations are hard to accelerate due to complicated data movement and quadratic computational complexity, prohibiting the real-time inference on resource-constrained edge-computing platforms. To tackle this challenge, we propose Energon, an algorithmarchitecture co-design approach that accelerates various transformers using dynamic sparse attention. With the observation that attention results only depend on a few important query-key pairs, we propose a multi-round filtering algorithm to dynamically identify such pairs at runtime. We adopt low bitwidth in each filtering round and only use high-precision tensors in the attention stage to reduce overall complexity. By this means, we significantly mitigate the computational cost with negligible accuracy loss. To enable such an algorithm with lower latency and better energyefficiency, we also propose an Energon co-processor architecture. Elaborated pipelines and specialized optimizations jointly boost the performance and reduce power consumption. Extensive experiments on both NLP and CV benchmarks demonstrate that Energon achieves 161\u00d7 and 8.4\u00d7 geo-mean speedup and up to 104\u00d7 and 103\u00d7 energy reduction compared with Intel Xeon 5220 CPU and NVIDIA V100 GPU. Compared to state-of-the-art attention accelerators SpAtten and A3, Energon also achieves 1.7\u00d7,1.25\u00d7 speedup and 1.6\u00d7,1.5\u00d7 higher energy efficiency.", "venue": "ArXiv", "authors": ["Zhe  Zhou", "Junlin  Liu", "Zhenyu  Gu", "Guangyu  Sun"], "year": 2021, "n_citations": 0}
{"id": 180184, "s2_id": "85bb64526b2edf69999cd26c4759d9081518609f", "title": "Development of a Predictive Process Design kit for15-nm FinFETs: FreePDK15", "abstract": "FinFETs are predicted to advance semiconductorscaling for sub-20nm devices. In order to support their intro-duction into research and universities it is crucial to develop anopen source predictive process design kit. This paper discussesin detail the design process for such a kit for 15nm FinFETdevices, called the FreePDK15. The kit consists of a layerstack with thirteen-metal layers based on hierarchical-scalingused in ASIC architecture, Middle-of-Line local interconnectlayers and a set of Front-End-of-Line layers. The physical andgeometrical properties of these layers are defined and theseproperties determine the density and parasitics of the design. Thedesign rules are laid down considering additional guidelines forprocess variability, challenges involved in FinFET fabrication anda unique set of design rules are developed for critical dimensions.Layout extraction including modified rules for determining thegeometrical characteristics of FinFET layouts are implementedand discussed to obtain successful Layout Versus Schematicchecks for a set of layouts. Moreover, additional parasiticcomponents of a standard FinFET device are analyzed andthe parasitic extraction of sample layouts is performed. Theseextraction results are then compared and assessed against thevalidation models.", "venue": "ArXiv", "authors": ["Kirti  Bhanushali", "Chinmay  Tembe", "W. Rhett Davis"], "year": 2020, "n_citations": 0}
{"id": 182012, "s2_id": "e370ac1adb6d676aeceb2012f4cf2e751e17e377", "title": "TensorLib: A Spatial Accelerator Generation Framework for Tensor Algebra", "abstract": "Tensor algebra finds applications in various domains, and these applications, especially when accelerated on spatial hardware accelerators, can deliver high performance and low power. Spatial hardware accelerator exhibits complex design space. Prior approaches based on manual implementation lead to low programming productivity, rendering thorough design space exploration impossible. In this paper, we propose TensorLib, a framework for generating spatial hardware accelerator for tensor algebra applications. TensorLib is motivated by the observation that, different dataflows share common hardware modules, which can be reused across different designs. To build such a framework, TensorLib first uses Space-Time Transformation to explore different dataflows, which can compactly represent the hardware dataflow using a simple transformation matrix. Next, we identify the common structures of different dataflows and build parameterized hardware module templates with Chisel. Our generation framework can select the needed hardware modules for each dataflow, connect the modules using a specified interconnection pattern, and automatically generate the complete hardware accelerator design. TensorLib remarkably improves the productivity for the development and optimization of spatial hardware architecture, providing a rich design space with tradeoffs in performance, area, and power. Experiments show that TensorLib can automatically generate hardware designs with different dataflows and achieve 21% performance improvement on FPGA compared to the state-of-the-arts.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Liancheng  Jia", "Zizhang  Luo", "Liqiang  Lu", "Yun  Liang"], "year": 2021, "n_citations": 2}
{"id": 183406, "s2_id": "0a9ad37a0d8765a525705fe7ba5f4fdb0522971b", "title": "Mosaic: An Application-Transparent Hardware-Software Cooperative Memory Manager for GPUs", "abstract": "Modern GPUs face a trade-off on how the page size used for memory management affects address translation and demand paging. Support for multiple page sizes can help relax the page size trade-off so that address translation and demand paging optimizations work together synergistically. However, existing page coalescing and splintering policies require costly base page migrations that undermine the benefits multiple page sizes provide. In this paper, we observe that GPGPU applications present an opportunity to support multiple page sizes without costly data migration, as the applications perform most of their memory allocation en masse (i.e., they allocate a large number of base pages at once). We show that this en masse allocation allows us to create intelligent memory allocation policies which ensure that base pages that are contiguous in virtual memory are allocated to contiguous physical memory pages. As a result, coalescing and splintering operations no longer need to migrate base pages. \nWe introduce Mosaic, a GPU memory manager that provides application-transparent support for multiple page sizes. Mosaic uses base pages to transfer data over the system I/O bus, and allocates physical memory in a way that (1) preserves base page contiguity and (2) ensures that a large page frame contains pages from only a single memory protection domain. This mechanism allows the TLB to use large pages, reducing address translation overhead. During data transfer, this mechanism enables the GPU to transfer only the base pages that are needed by the application over the system I/O bus, keeping demand paging overhead low.", "venue": "ArXiv", "authors": ["Rachata  Ausavarungnirun", "Joshua  Landgraf", "Vance  Miller", "Saugata  Ghose", "Jayneel  Gandhi", "Christopher J. Rossbach", "Onur  Mutlu"], "year": 2018, "n_citations": 2}
{"id": 183644, "s2_id": "c45ea97470f911bd2d64837c80dfd87bb17a366d", "title": "Information Theory as a Means of Determining the Main Factors Affecting the Processors Architecture", "abstract": "In this article we are investigating the computers development process in the past decades in order to identify the factors that influence it the most. We describe such factors and use them to predict the direction of further development. To solve these problems, we use the concept of the Computer Capacity, which allows us to estimate the performance of computers theoretically, relying only on the description of its architecture.", "venue": "ArXiv", "authors": ["Anton  Rakitskiy", "Boris  Ryabko"], "year": 2020, "n_citations": 0}
{"id": 184786, "s2_id": "4b206de446fd85120b034a8ea40a157ad568f2f3", "title": "An Analytical Model for Performance and Lifetime Estimation of Hybrid DRAM-NVM Main Memories", "abstract": "Emerging Non-Volatile Memories (NVMs) have promising advantages (e.g., lower idle power, higher density, and non-volatility) over the existing predominant main memory technology, DRAM. Yet, NVMs also have disadvantages (e.g., longer latencies, higher active power, and limited endurance). System architects are therefore examining hybrid DRAM-NVM main memories to enable the advantages of NVMs while avoiding the disadvantages as much as possible. Unfortunately, the hybrid memory design space is very large and complex due to the existence of very different types of NVMs and their rapidly-changing characteristics. Therefore, optimization of performance and lifetime of hybrid memory based computing platforms and their experimental evaluation using traditional simulation methods can be very time-consuming and sometimes even impractical. As such, it is necessary to develop a fast and flexible analytical model to estimate the performance and lifetime of hybrid memories on various workloads. This paper presents an analytical model for hybrid memories based on Markov decision processes. The proposed model estimates the hit ratio and lifetime for various configurations of DRAM-NVM hybrid main memories. Our model also provides accurate estimation of the effect of data migration policies on the hybrid memory hit ratio (i.e., percentage of accesses supplied by either DRAM or NVM), one of the most important factors in hybrid memory performance and lifetime. Such an analytical model can aid designers to tune hybrid memory configurations to improve performance and/or lifetime. We present several optimizations that make our model more efficient while maintaining its accuracy. Our experimental evaluations conducted using the PARSEC benchmark suite show that the proposed model (a) accurately predicts the hybrid memory hit ratio compared to the state-of-the-art hybrid memory simulators with an average (maximum) error of 4.61 percent (13.6 percent) on a commodity server (equipped with 192 GB main memory and quad-core Xeon processor), (b) accurately estimates the NVM lifetime with an average (maximum) error of 2.93 percent (8.8 percent), and (c) is on average (up to) 4x (10x) faster than conventional state-of-the-art simulation platforms for hybrid memories.", "venue": "IEEE Transactions on Computers", "authors": ["Reza  Salkhordeh", "Onur  Mutlu", "Hossein  Asadi"], "year": 2019, "n_citations": 8}
{"id": 186556, "s2_id": "e7b381fe2532fd7915b9333999395685eeeb013b", "title": "SME: ReRAM-based Sparse-Multiplication-Engine to Squeeze-Out Bit Sparsity of Neural Network", "abstract": "Resistive Random-Access-Memory (ReRAM) cross-bar is a promising technique for deep neural network (DNN) accelerators, thanks to its in-memory and in-situ analog computing abilities for Vector-Matrix Multiplication-and-Accumulations (VMMs). However, it is challenging for crossbar architecture to exploit the sparsity in DNNs. It inevitably causes complex and costly control to exploit fine-grained sparsity due to the limitation of tightly-coupled crossbar structure.As the countermeasure, we develop a novel ReRAM-based DNN accelerator, named Sparse-Multiplication-Engine (SME), based on a hardware and software co-design framework. First, we orchestrate the bit-sparse pattern to increase the density of bit-sparsity based on existing quantization methods. Second, we propose a novel weight mapping mechanism to slice the bits of a weight across the crossbars and splice the activation results in peripheral circuits. This mechanism can decouple the tightly-coupled crossbar structure and cumulate the sparsity in the crossbar. Finally, a superior squeeze-out scheme empties the crossbars mapped with highly-sparse non-zeros from the previous two steps. We design the SME architecture and discuss its use for other quantization methods and different ReRAM cell technologies. Compared with prior state-of-the-art designs, the SME shrinks the use of crossbars up to 8.7\u00d7 and 2.1\u00d7 using ResNet-50 and MobileNet-v2, respectively, with \u2264 0.3% accuracy drop on ImageNet.", "venue": "2021 IEEE 39th International Conference on Computer Design (ICCD)", "authors": ["Fangxin  Liu", "Wenbo  Zhao", "Yilong  Zhao", "Zongwu  Wang", "Tao  Yang", "Zhezhi  He", "Naifeng  Jing", "Xiaoyao  Liang", "Li  Jiang"], "year": 2021, "n_citations": 0}
{"id": 191599, "s2_id": "7a1838f02f571a15cce34f042d33f4320c39079d", "title": "ASAP: Accelerated Short-Read Alignment on Programmable Hardware", "abstract": "The proliferation of high-throughput sequencing machines ensures rapid generation of up to billions of short nucleotide fragments in a short period of time. This massive amount of sequence data can quickly overwhelm today's storage and compute infrastructure. This paper explores the use of hardware acceleration to significantly improve the runtime of short-read alignment, a crucial step in preprocessing sequenced genomes. We focus on the Levenshtein distance (edit-distance) computation kernel and propose the ASAP accelerator, which utilizes the intrinsic delay of circuits for edit-distance computation elements as a proxy for computation. Our design is implemented on an Xilinx Virtex 7 FPGA in an IBM POWER8 system that uses the CAPI interface for cache coherence across the CPU and FPGA. Our design is <inline-formula><tex-math notation=\"LaTeX\">$200\\times$</tex-math><alternatives><inline-graphic xlink:href=\"banerjee-ieq1-2875733.gif\"/></alternatives></inline-formula> faster than an equivalent Smith-Waterman-C implementation of the kernel running on the host processor, <inline-formula><tex-math notation=\"LaTeX\">$40-60\\times$</tex-math><alternatives><inline-graphic xlink:href=\"banerjee-ieq2-2875733.gif\"/></alternatives></inline-formula> faster than an equivalent Landau-Vishkin-C++ implementation of the kernel running on the IBM Power8 host processor, and <inline-formula><tex-math notation=\"LaTeX\">$2\\times$</tex-math><alternatives><inline-graphic xlink:href=\"banerjee-ieq3-2875733.gif\"/></alternatives></inline-formula> faster for an end-to-end alignment tool for 120\u2013150 base-pair short-read sequences. Further the design represents a <inline-formula><tex-math notation=\"LaTeX\">$3760\\times$</tex-math><alternatives><inline-graphic xlink:href=\"banerjee-ieq4-2875733.gif\"/></alternatives></inline-formula> improvement over the CPU in performance/Watt terms.", "venue": "IEEE Transactions on Computers", "authors": ["Subho Sankar Banerjee", "Mohamed  El-Hadedy", "Jong Bin Lim", "Zbigniew T. Kalbarczyk", "Deming  Chen", "Steven S. Lumetta", "Ravishankar K. Iyer"], "year": 2019, "n_citations": 19}
{"id": 191978, "s2_id": "654cda1542d7db6c272ad0cf0a6bb35e5e34a5fd", "title": "Communication Lower Bound in Convolution Accelerators", "abstract": "In current convolutional neural network (CNN) accelerators, communication (i.e., memory access) dominates the energy consumption. This work provides comprehensive analysis and methodologies to minimize the communication for CNN accelerators. For the off-chip communication, we derive the theoretical lower bound for any convolutional layer and propose a dataflow to reach the lower bound. This fundamental problem has never been solved by prior studies. The on-chip communication is minimized based on an elaborate workload and storage mapping scheme. We in addition design a communication-optimal CNN accelerator architecture. Evaluations based on the 65nm technology demonstrate that the proposed architecture nearly reaches the theoretical minimum communication in a three-level memory hierarchy and it is computation dominant. The gap between the energy efficiency of our accelerator and the theoretical best value is only 37-87%.", "venue": "2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Xiaoming  Chen", "Yinhe  Han", "Yu  Wang"], "year": 2020, "n_citations": 6}
{"id": 196942, "s2_id": "540746504cfe51a146762cbbca06cbc03229c778", "title": "NeuroTrainer: An Intelligent Memory Module for Deep Learning Training", "abstract": "This paper presents, NeuroTrainer, an intelligent memory module with in-memory accelerators that forms the building block of a scalable architecture for energy efficient training for deep neural networks. The proposed architecture is based on integration of a homogeneous computing substrate composed of multiple processing engines in the logic layer of a 3D memory module. NeuroTrainer utilizes a programmable data flow based execution model to optimize memory mapping and data re-use during different phases of training operation. A programming model and supporting architecture utilizes the flexible data flow to efficiently accelerate training of various types of DNNs. The cycle level simulation and synthesized design in 15nm FinFET showspower efficiency of 500 GFLOPS/W, and almost similar throughput for a wide range of DNNs including convolutional, recurrent, multi-layer-perceptron, and mixed (CNN+RNN) networks", "venue": "ArXiv", "authors": ["Duckhwan  Kim", "Taesik  Na", "Sudhakar  Yalamanchili", "Saibal  Mukhopadhyay"], "year": 2017, "n_citations": 1}
{"id": 201007, "s2_id": "d785d9b7ca3b67c0c97fcb8ebf0bf2768413c261", "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-Based Image Upsampling", "abstract": "State-of-the-art deep learning solutions for image upsampling are currently trained using either resize or sub-pixel convolution to learn kernels that generate high fidelity images with minimal artifacts. However, performing inference with these learned convolution kernels requires memory-intensive feature map transformations that dominate time and energy costs in real-time applications. To alleviate this pressure on memory bandwidth, we propose a novel energy-efficient edge computing paradigm that confines the use of resize or sub-pixel convolution to training in the cloud by transforming learned convolution kernels to deconvolution kernels before deploying them for inference as a functionally equivalent deconvolution. These kernel transformations, intended as a one-time cost when shifting from training to inference, enable a systems designer to use each algorithm in their optimal context by preserving the image fidelity learned when training in the cloud while minimizing data transfer penalties during inference at the edge. We compare the inference properties of these convolution-based image upsampling algorithms and introduce a novel deconvolution inference algorithm, which we refer to as REVD2. To demonstrate the benefits of our approach, we upsample images selected from the BSD300 dataset using a pre-trained single-image super resolution network provided by the PyTorch model zoo. Using quantitative models of incurred time and energy costs to analyze this deep neural network, we estimate that using REVD2 for inference at the edge improves system latency by <inline-formula> <tex-math notation=\"LaTeX\">$2.1\\times $ </tex-math></inline-formula> or <inline-formula> <tex-math notation=\"LaTeX\">$2.8\\times $ </tex-math></inline-formula> and energy efficiency by <inline-formula> <tex-math notation=\"LaTeX\">$2.1\\times $ </tex-math></inline-formula> or <inline-formula> <tex-math notation=\"LaTeX\">$2.7\\times $ </tex-math></inline-formula> when respectively compared to sub-pixel or resize convolution counterparts.", "venue": "IEEE Access", "authors": ["Ian  Colbert", "Kenneth  Kreutz-Delgado", "Srinjoy  Das"], "year": 2021, "n_citations": 1}
{"id": 202568, "s2_id": "f5863121888ef9e793c2a877221c24d4b78531a9", "title": "Sphinx: A Secure Architecture Based on Binary Code Diversification and Execution Obfuscation", "abstract": "Sphinx, a hardware-software co-design architecture for binary code and runtime obfuscation. The Sphinx architecture uses binary code diversification and self-reconfigurable processing elements to maintain application functionality while obfuscating the binary code and architecture states to attackers. This approach dramatically reduces an attacker's ability to exploit information gained from one deployment to attack another deployment. Our results show that the Sphinx is able to decouple the program's execution time, power and memory and I/O activities from its functionality. It is also practical in the sense that the system (both software and hardware) overheads are minimal.", "venue": "ArXiv", "authors": ["Michel A. Kinsy", "Donato  Kava", "Alan  Ehret", "Miguel  Mark"], "year": 2018, "n_citations": 2}
{"id": 203101, "s2_id": "1ff82aa7706e82d0f4760ea2a236a76b28a6609d", "title": "Capstan: A Vector RDA for Sparsity", "abstract": "This paper proposes Capstan: a scalable, parallel-patterns-based, reconfigurable dataflow accelerator (RDA) for sparse and dense tensor applications. Instead of designing for one application, we start with common sparse data formats, each of which supports multiple applications. Using a declarative programming model, Capstan supports application-independent sparse iteration and memory primitives that can be mapped to vectorized, high-performance hardware. We optimize random-access sparse memories with configurable out-of-order execution to increase SRAM random-access throughput from 32% to 80%. For a variety of sparse applications, Capstan with DDR4 memory is 18\u00d7 faster than a multi-core CPU baseline, while Capstan with HBM2 memory is 16\u00d7 faster than an Nvidia V100 GPU. For sparse applications that can be mapped to Plasticine, a recent dense RDA, Capstan is 7.6\u00d7 to 365\u00d7 faster and only 16% larger.", "venue": "MICRO", "authors": ["Alexander  Rucker", "Matthew  Vilim", "Tian  Zhao", "Yaqi  Zhang", "Raghu  Prabhakar", "Kunle  Olukotun"], "year": 2021, "n_citations": 2}
{"id": 203507, "s2_id": "82ba1445a01c288f1325503b3520b01f083e7414", "title": "A Cache Energy Optimization Technique for STT-RAM Last Level Cache", "abstract": "Last level caches (LLCs) occupy a large chip-area and there size is expected to grow further to offset the limitations of memory bandwidth and speed. Due to high leakage consumption of SRAM device, caches designed with SRAM consume large amount of energy. To address this, use of emerging technologies such as spin torque transfer RAM (STT-RAM) has been investigated which have lower leakage power dissipation. However, the high write latency and power of it may lead to large energy consumption which present challenges in its use. In this report, we propose a cache reconfiguration based technique for improving the energy efficiency of STT-RAM based LLCs. Our technique dynamically adjusts the active cache size to reduce the cache leakage energy consumption with minimum performance loss. We choose a suitable value of STT-RAM retention time for avoiding refresh overhead and gaining performance. Single-core simulations have been performed using SPEC2006 benchmarks and Sniper x86-64 simulator. The results show that while, compared to an STT-RAM LLC of similar area, an SRAM LLC incurs nearly 100% loss in energy and 7.3% loss in performance; our technique using STT-RAM cache saves 21.8% energy and incurs only 1.7% loss in performance.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 1}
{"id": 206114, "s2_id": "825f5b4d191f6e94799c1cf7c3ce47fbd0511d09", "title": "Fast Convolution based on Winograd Minimum Filtering: Introduction and Development", "abstract": "Convolutional Neural Network (CNN) has been widely used in various fields and played an important role. Convolution operators are the fundamental component of convolutional neural networks, and it is also the most time-consuming part of network training and inference. In recent years, researchers have proposed several fast convolution algorithms including FFT and Winograd. Among them, Winograd convolution significantly reduces the multiplication operations in convolution, and it also takes up less memory space than FFT convolution. Therefore, Winograd convolution has quickly become the first choice for fast convolution implementation within a few years. At present, there is no systematic summary of the convolution algorithm. This article aims to fill this gap and provide detailed references for follow-up researchers. This article summarizes the development of Winograd convolution from the three aspects of algorithm expansion, algorithm optimization, implementation, and application, and finally makes a simple outlook on the possible future directions.", "venue": "Computer Science and Information Technology Trends", "authors": ["Gan  Tong", "Libo  Huang"], "year": 2021, "n_citations": 0}
{"id": 207073, "s2_id": "1b781f266fcca0e778ae51e791d5a9d392436e57", "title": "Model Checking for Verification of Quantum Circuits", "abstract": "In this talk, we will describe a framework for assertion-based verification (ABV) of quantum circuits by applying model checking techniques for quantum systems developed in our previous work, in which: \u2013 Noiseless and noisy quantum circuits are modelled as operatorand superoperator-valued transition systems, respectively, both of which can be further represented by tensor networks. \u2013 Quantum assertions are specified by a temporal extension of Birkhoff-von Neumann quantum logic. Their semantics is defined based on the design decision: they will be used in verification of quantum circuits by simulation on classical computers or human reasoning rather than by quantum physics experiments (e.g. testing through measurements); \u2013 Algorithms for reachability analysis and model checking of quantum circuits are developed based on contraction of tensor networks. We observe that many optimisation techniques for computing relational products used in BDD-based model checking algorithms can be generalised for contracting tensor networks of quantum circuits.", "venue": "FM", "authors": ["Mingsheng  Ying"], "year": 2021, "n_citations": 0}
{"id": 209590, "s2_id": "cbfda12bf40f08f61ac03b3e756bf00bd5765a26", "title": "A CNN Accelerator on FPGA Using Depthwise Separable Convolution", "abstract": "Convolutional neural networks (CNNs) have been widely deployed in the fields of computer vision and pattern recognition because of their high accuracy. However, large convolution operations are computing intensive and often require a powerful computing platform such as a graphics processing unit. This makes it difficult to apply CNNs to portable devices. The state-of-the-art CNNs, such as MobileNetV2 and Xception, adopt depthwise separable convolution to replace the standard convolution for embedded platforms, which significantly reduces operations and parameters with only limited loss in accuracy. This highly structured model is very suitable for field-programmable gate array (FPGA) implementation. In this brief, a scalable high performance depthwise separable convolution optimized CNN accelerator is proposed. The accelerator can be fit into an FPGA of different sizes, provided the balancing between hardware resources and processing speed. As an example, MobileNetV2 is implemented on Arria 10 SoC FPGA, and the results show this accelerator can classify each picture from ImageNet in 3.75 ms, which is about 266.6 frames per second. The FPGA design achieves 20x speedup if compared to CPU.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Lin  Bai", "Yiming  Zhao", "Xinming  Huang"], "year": 2018, "n_citations": 51}
{"id": 210926, "s2_id": "fbb32a271ea02f44f57733e28d25f59d40a01012", "title": "Modeling Gate-Level Abstraction Hierarchy Using Graph Convolutional Neural Networks to Predict Functional De-Rating Factors", "abstract": "The paper is proposing a methodology for modeling a gate-level netlist using a Graph Convolutional Network (GCN). The model predicts the overall functional de-rating factors of sequential elements of a given circuit. In the preliminary phase of the work, the important goal is making a GCN which able to take a gate-level netlist as input information after transforming it into the Probabilistic Bayesian Graph in the form of Graph Modeling Language (GML). This part enables the GCN to learn the structural information of netlist in graph domains. In the second phase of the work, the modeled GCN trained with a functional de-rating factor of a very low number of individual sequential elements (flip-flops). The third phase includes the understanding of GCN models accuracy to model an arbitrary circuit netlist. The designed model validated for two circuits. One is the IEEE 754 standard double precision floating point adder and the second one is the 10-Gigabit Ethernet MAC IEEE 802.3 standard. The predicted results compared to the standard fault injection campaign results of the error called Single Event Upset (SEU). The validated results are graphically pictured in the form of the histogram and sorted probabilities and evaluated with the Confidence Interval (CI) metric between the predicted and simulated fault injection results.", "venue": "2019 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)", "authors": ["Aneesh  Balakrishnan", "Thomas  Lange", "Maximilien  Glorieux", "Dan  Alexandrescu", "Maksim  Jenihhin"], "year": 2019, "n_citations": 4}
{"id": 214935, "s2_id": "2dec6b7390da71e51d2140bbd46e642447baab71", "title": "Evaluation of SystemC modelling of reconfigurable embedded systems", "abstract": "This paper evaluates the use of pin and cycle accurate SystemC models for embedded system design exploration and early software development. The target system is the MicroBlaze VanillaNet Platform running MicroBlaze uClinux operating system. The paper compares register transfer level (RTL) hardware description language (HDL) simulation speed to the simulation speed of several different SystemC models. It is shown that simulation speed of pin and cycle accurate models can go up to 150 kHz, compared to the 100 Hz range of HDL simulation. Furthermore, utilising techniques that temporarily compromise cycle accuracy, effective simulation speed of up to 500 kHz can be obtained.", "venue": "Design, Automation and Test in Europe", "authors": ["Tero  Rissa", "Adam  Donlin", "Wayne  Luk"], "year": 2005, "n_citations": 31}
{"id": 218465, "s2_id": "39a196333dd36af7c9ca16b7572565b4baa23643", "title": "The Effect of Temperature on Amdahl Law in 3D Multicore Era", "abstract": "This work studies the influence of temperature on performance and scalability of 3D Chip Multiprocessors (CMP) from Amdahl's law perspective. We find that 3D CMP may reach its thermal limit before reaching its maximum power. We show that a high level of parallelism may lead to high peak temperatures even in small scale 3D CMPs, thus limiting 3D CMP scalability and calling for different, in-memory computing architectures.", "venue": "IEEE Transactions on Computers", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar"], "year": 2016, "n_citations": 3}
{"id": 219220, "s2_id": "93be02a261d53006c3d2b05eef5a6921e3c3b665", "title": "Reducing the Energy Cost of Inference via In-sensor Information Processing", "abstract": "There is much interest in incorporating inference capabilities into sensor-rich embedded platforms such as autonomous vehicles, wearables, and others. A central problem in the design of such systems is the need to extract information locally from sensed data on a severely limited energy budget. This necessitates the design of energy-efficient sensory embedded system. A typical sensory embedded system enforces a physical separation between sensing and computational subsystems - a separation mandated by the differing requirements of the sensing and computational functions. As a consequence, the energy consumption in such systems tends to be dominated by the energy consumed in transferring data over the sensor-processor interface (communication energy) and the energy consumed in processing the data in digital processor (computational energy). In this article, we propose an in-sensor computing architecture which (mostly) eliminates the sensor-processor interface by embedding inference computations in the noisy sensor fabric in analog and retraining the hyperparameters in order to compensate for non-ideal computations. The resulting architecture referred to as the Compute Sensor - a sensor that computes in addition to sensing - represents a radical departure from the conventional. We show that a Compute Sensor for image data can be designed by embedding both feature extraction and classification functions in the analog domain in close proximity to the CMOS active pixel sensor (APS) array. Significant gains in energy-efficiency are demonstrated using behavioral and energy models in a commercial semiconductor process technology. In the process, the Compute Sensor creates a unique opportunity to develop machine learning algorithms for information extraction from data on a noisy underlying computational fabric.", "venue": "ArXiv", "authors": ["Sai  Zhang", "Mingu  Kang", "Charbel  Sakr", "Naresh R. Shanbhag"], "year": 2016, "n_citations": 3}
{"id": 220180, "s2_id": "7764cbec30532468b95536a5a83b189a7ce763a3", "title": "STONNE: A Detailed Architectural Simulator for Flexible Neural Network Accelerators", "abstract": "The design of specialized architectures for accelerating the inference procedure of Deep Neural Networks (DNNs) is a booming area of research nowadays. First-generation rigid proposals have been rapidly replaced by more advanced flexible accelerator architectures able to efficiently support a variety of layer types and dimensions. As the complexity of the designs grows, it is more and more appealing for researchers to have cycle-accurate simulation tools at their disposal to allow for fast and accurate design-space exploration, and rapid quantification of the efficacy of architectural enhancements during the early stages of a design. To this end, we present STONNE (Simulation TOol of Neural Network Engines), a cycle-accurate, highly-modular and highly-extensible simulation framework that enables end-to-end evaluation of flexible accelerator architectures running complete contemporary DNN models. We use STONNE to model the recently proposed MAERI architecture and show how it can closely approach the performance results of the publicly available BSV-coded MAERI implementation. Then, we conduct a comprehensive evaluation and demonstrate that the folding strategy implemented for MAERI results in very low compute unit utilization (25% on average across 5 DNN models) which in the end translates into poor performance.", "venue": "ArXiv", "authors": ["Francisco  Munoz-Mart'inez", "Jos'e L. Abell'an", "Manuel E. Acacio", "Tushar  Krishna"], "year": 2020, "n_citations": 4}
{"id": 220360, "s2_id": "9580bde1a9eefead3204c2862d2388de031177b6", "title": "A memory hierarchical layer assigning and prefetching technique to overcome the memory performance/energy bottleneck", "abstract": "The memory subsystem has always been a bottleneck in performance as well as significant power contributor in memory intensive applications. Many researchers have presented multi-layered memory hierarchies as a means to design energy and performance efficient systems. However, most of the previous work does not explore trade-offs systematically. We fill this gap by proposing a formalized technique that takes into consideration data reuse, limited life-time of the arrays of an application and application specific prefetching opportunities, and performs a thorough tradeoff exploration for different memory layer sizes. This technique has been implemented on a prototype tool, which was tested successfully using nine real-life applications of industrial relevance. Following this approach we have able to reduce execution time up to 60%, and energy consumption up to 70%.", "venue": "Design, Automation and Test in Europe", "authors": ["Minas  Dasygenis", "Erik  Brockmeyer", "Bart  Durinck", "Francky  Catthoor", "Dimitrios  Soudris", "Adonios  Thanailakis"], "year": 2005, "n_citations": 9}
{"id": 225702, "s2_id": "b7f249564f1b6aff24f93898cbda3edfb88e43a3", "title": "Composing Graph Theory and Deep Neural Networks to Evaluate SEU Type Soft Error Effects", "abstract": "Rapidly shrinking technology node and voltage scaling increase the susceptibility of Soft Errors in digital circuits. Soft Errors are radiation-induced effects while the radiation particles such as Alpha, Neutrons or Heavy Ions, interact with sensitive regions of microelectronic devices/circuits. The particle hit could be a glancing blow or a penetrating strike. A well apprehended and characterized way of analyzing soft error effects is the fault-injection campaign, but that typically acknowledged as time and resource-consuming simulation strategy. As an alternative to traditional fault injection-based methodologies and to explore the applicability of modern graph based neural network algorithms in the field of reliability modeling, this paper proposes a systematic framework that explores gate-level abstractions to extract and exploit relevant feature representations at low-dimensional vector space. The framework allows the extensive prediction analysis of SEU type soft error effects in a given circuit. A scalable and inductive type representation learning algorithm on graphs called GraphSAGE has been utilized for efficiently extracting structural features of the gate-level netlist, providing a valuable database to exercise a downstream machine learning or deep learning algorithm aiming at predicting fault propagation metrics. Functional Failure Rate (FFR): the predicted fault propagating metric of SEU type fault within the gate-level circuit abstraction of the 10-Gigabit Ethernet MAC (IEEE 802.3) standard circuit.", "venue": "2020 9th Mediterranean Conference on Embedded Computing (MECO)", "authors": ["Aneesh  Balakrishnan", "Thomas  Lange", "Maximilien  Glorieux", "Dan  Alexandrescu", "Maksim  Jenihhin"], "year": 2020, "n_citations": 3}
{"id": 226318, "s2_id": "1fdbd0c5ef0b0b54c840e180a4730548d100e70f", "title": "Mapping Stencils on Coarse-grained Reconfigurable Spatial Architecture", "abstract": "Stencils represent a class of computational patterns where an output grid point depends on a fixed shape of neighboring points in an input grid. Stencil computations are prevalent in scientific applications engaging a significant portion of supercomputing resources. Therefore, it has been always important to optimize stencil programs for the best performance. A rich body of research has focused on optimizing stencil computations on almost all parallel architectures. Stencil applications have regular dependency patterns, inherent pipeline-parallelism, and plenty of data reuse. This makes these applications a perfect match for a coarse-grained reconfigurable spatial architecture (CGRA). A CGRA consists of many simple, small processing elements (PEs) connected with an on-chip network. Each PE can be configured to execute part of a stencil computation and all PEs run in parallel; the network can also be configured so that data loaded can be passed from a PE to a neighbor PE directly and thus reused by many PEs without register spilling and memory traffic. How to efficiently map a stencil computation to a CGRA is the key to performance. In this paper, we show a few unique and generalizable ways of mapping one- and multidimensional stencil computations to a CGRA, fully exploiting the data reuse opportunities and parallelism. Our simulation experiments demonstrate that these mappings are efficient and enable the CGRA to outperform state-of-the-art GPUs.", "venue": "ArXiv", "authors": ["Jesmin Jahan Tithi", "Fabrizio  Petrini", "Hongbo  Rong", "Andrei  Valentin", "Carl  Ebeling"], "year": 2020, "n_citations": 0}
{"id": 226834, "s2_id": "195eff97a3048c286589188a6d6c254b9623dbf4", "title": "Enabling Privacy-Preserving, Compute- and Data-Intensive Computing using Heterogeneous Trusted Execution Environment", "abstract": "There is an urgent demand for privacy-preserving techniques capable of supporting compute and data intensive (CDI) computing in the era of big data. However, none of existing TEEs can truly support CDI computing tasks, as CDI requires high throughput accelerators like GPU and TPU but TEEs do not offer security protection of such accelerators. This paper present HETEE (Heterogeneous TEE), the first design of TEE capable of strongly protecting heterogeneous computing with unsecure accelerators. HETEE is uniquely constructed to work with today's servers, and does not require any changes for existing commercial CPUs or accelerators. The key idea of our design runs security controller as a stand-alone computing system to dynamically adjust the boundary of between secure and insecure worlds through the PCIe switches, rendering the control of an accelerator to the host OS when it is not needed for secure computing, and shifting it back when it is. The controller is the only trust unit in the system and it runs the custom OS and accelerator runtimes, together with the encryption, authentication and remote attestation components. The host server and other computing systems communicate with controller through an in memory task queue that accommodates the computing tasks offloaded to HETEE, in the form of encrypted and signed code and data. Also, HETEE offers a generic and efficient programming model to the host CPU. We have implemented the HETEE design on a hardware prototype system, and evaluated it with large-scale Neural Networks inference and training tasks. Our evaluations show that HETEE can easily support such secure computing tasks and only incurs a 12.34% throughput overhead for inference and 9.87% overhead for training on average.", "venue": "ArXiv", "authors": ["Jianping  Zhu", "Rui  Hou", "Xiaofeng  Wang", "Wenhao  Wang", "Jiangfeng  Cao", "Lutan  Zhao", "Fengkai  Yuan", "Peinan  Li", "Zhongpu  Wang", "Boyan  Zhao", "Lixin  Zhang", "Dan  Meng"], "year": 2019, "n_citations": 5}
{"id": 228991, "s2_id": "6d9f372fded518c03257b2af616a1e6b19d2bfe6", "title": "Hardware/Software Obfuscation against Timing Side-channel Attack on a GPU", "abstract": "GPUs are increasingly being used in security applications, especially for accelerating encryption/decryption. While GPUs are an attractive platform in terms of performance, the security of these devices raises a number of concerns. One vulnerability is the data-dependent timing information, which can be exploited by adversary to recover the encryption key. Memory system features are frequently exploited since they create detectable timing variations. In this paper, our attack model is a coalescing attack, which leverages a critical GPU microarchitectural feature the coalescing unit. As multiple concurrent GPU memory requests can refer to the same cache block, the coalescing unit collapses them into a single memory transaction. The access time of an encryption kernel is dependent on the number of transactions. Correlation between a guessed key value and the associated timing samples can be exploited to recover the secret key. In this paper, a series of hardware/software countermeasures are proposed to obfuscate the memory timing side channel, making the GPU more resilient without impacting performance. Our hardware-based approach attempts to randomize the width of the coalescing unit to lower the signal-to-noise ratio. We present a hierarchical Miss Status Holding Register (MSHR) design that can merge transactions across different warps. This feature boosts performance, while, at the same time, secures the execution. We also present a software-based approach to permute the organization of critical data structures, significantly changing the coalescing behavior and introducing a high degree of randomness. Equipped with our new protections, the effort to launch a successful attack is increased up to $1433X\\times 178X$, while also improving encryption/decryption performance up to 7%.", "venue": "2020 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)", "authors": ["Elmira  Karimi", "Yunsi  Fei", "David  Kaeli"], "year": 2020, "n_citations": 3}
{"id": 229555, "s2_id": "9f39ced347555286ab9fab69d8f70a3e356bb5fc", "title": "High Speed Multiple Valued Logic Full Adder Using Carbon Nano Tube Field Effect Transistor", "abstract": "High speed Full-Adder (FA) module is a critical element in designing high performance arithmetic circuits. In this paper, we propose a new high speed multiple-valued logic FA module. The proposed FA is constructed by 14 transistors and 3 capacitors, using carbon nano-tube field effect transistor (CNFET) technology. Furthermore, our proposed technique has been examined in different voltages (i.e., 0.65v and 0.9v). The observed results reveal power consumption and power delay product (PDP) improvements compared to existing FA counterparts", "venue": "VLSIC 2011", "authors": ["Ashkan  Khatir", "Shaghayegh  Abdolahzadegan", "Iman  Mahmoudi"], "year": 2011, "n_citations": 22}
{"id": 232246, "s2_id": "269216efe045f85a45c16bc982d978dce042b962", "title": "sBSNN: Stochastic-Bits Enabled Binary Spiking Neural Network With On-Chip Learning for Energy Efficient Neuromorphic Computing at the Edge", "abstract": "In this work, we propose stochastic Binary Spiking Neural Network (sBSNN) composed of stochastic spiking neurons and binary synapses (stochastic only during training) that computes probabilistically with one-bit precision for power-efficient and memory-compressed neuromorphic computing. We present an energy-efficient implementation of the proposed sBSNN using \u2018stochastic bit\u2019 as the core computational primitive to realize the stochastic neurons and synapses, which are fabricated in 90nm CMOS process, to achieve efficient on-chip training and inference for image recognition tasks. The measured data shows that the \u2018stochastic bit\u2019 can be programmed to mimic spiking neurons, and stochastic Spike Timing Dependent Plasticity (or sSTDP) rule for training the binary synaptic weights without expensive random number generators. Our results indicate that the proposed sBSNN realization offers possibility of up to $32\\times $ neuronal and synaptic memory compression compared to full precision (32-bit) SNN and energy efficiency of 89.49 TOPS/Watt for two-layer fully-connected SNN.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Minsuk  Koo", "Gopalakrishnan  Srinivasan", "Yong  Shim", "Kaushik  Roy"], "year": 2020, "n_citations": 6}
{"id": 237479, "s2_id": "2a4ec92284d9d33f634e305c17ecf7d684afba88", "title": "Arnold: An eFPGA-Augmented RISC-V SoC for Flexible and Low-Power IoT End Nodes", "abstract": "A wide range of Internet of Things (IoT) applications require powerful, energy-efficient, and flexible end nodes to acquire data from multiple sources, process and distill the sensed data through near-sensor data analytics algorithms, and transmit it wirelessly. This work presents <italic>Arnold</italic>: a 0.5-to-0.8-V, 46.83-<inline-formula> <tex-math notation=\"LaTeX\">$\\mu \\text{W}$ </tex-math></inline-formula>/MHz, 600-MOPS fully programmable RISC-V microcontroller unit (MCU) fabricated in 22-nm Globalfoundries GF22FDX (GF22FDX) technology, coupled with a state-of-the-art (SoA) microcontroller to an embedded field-programmable gate array (eFPGA). We demonstrate the flexibility of the system-on-chip (SoC) to tackle the challenges of many emerging IoT applications, such as interfacing sensors and accelerators with nonstandard interfaces, performing on-the-fly preprocessing tasks on data streamed from peripherals, and accelerating near-sensor analytics, encryption, and machine learning tasks. A unique feature of the proposed SoC is the exploitation of body-biasing to reduce leakage power of the eFPGA fabric by up to <inline-formula> <tex-math notation=\"LaTeX\">$18\\times $ </tex-math></inline-formula> at 0.5 V, achieving SoA state bitstream-retentive sleep power for the eFPGA fabric, as low as <inline-formula> <tex-math notation=\"LaTeX\">$20.5~\\mu \\text{W}$ </tex-math></inline-formula>. The proposed SoC provides <inline-formula> <tex-math notation=\"LaTeX\">$3.4\\times $ </tex-math></inline-formula> better performance and <inline-formula> <tex-math notation=\"LaTeX\">$2.9\\times $ </tex-math></inline-formula> better energy efficiency than other fabricated heterogeneous reconfigurable SoCs of the same class.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Pasquale Davide Schiavone", "Davide  Rossi", "Alfio  Di Mauro", "Frank K. G\u00fcrkaynak", "Timothy  Saxe", "Mao  Wang", "Ket Chong Yap", "Luca  Benini"], "year": 2021, "n_citations": 4}
{"id": 240296, "s2_id": "8f9a21139dbec1fe16e1733c5e29130d1d0b571a", "title": "Stream Semantic Registers: A Lightweight RISC-V ISA Extension Achieving Full Compute Utilization in Single-Issue Cores", "abstract": "Single-issue processor cores are very energy efficient but suffer from the von Neumann bottleneck, in that they must explicitly fetch and issue the loads/storse necessary to feed their ALU/FPU. Each instruction spent on moving data is a cycle not spent on computation, limiting ALU/FPU utilization to 33 percent on reductions. We propose \u201cStream Semantic Registers\u201d to boost utilization and increase energy efficiency. SSR is a lightweight, non-invasive RISC-V ISA extension which implicitly encodes memory accesses as register reads/writes, eliminating a large number of loads/stores. We implement the proposed extension in the RTL of an existing multi-core cluster and synthesize the design for a modern 22 nm technology. Our extension provides a significant, 2x to 5x, architectural speedup across different kernels at a small 11 percent increase in core area. Sequential code runs 3x faster on a single core, and 3x fewer cores are needed in a cluster to achieve the same performance. The utilization increase to almost 100 percent in leads to a 2x energy efficiency improvement in a multi-core cluster. The extension reduces instruction fetches by up to 3.5x and instruction cache power consumption by up to 5.6x. Compilers can automatically map loop nests to SSRs, making the changes transparent to the programmer.", "venue": "IEEE Transactions on Computers", "authors": ["Fabian  Schuiki", "Florian  Zaruba", "Torsten  Hoefler", "Luca  Benini"], "year": 2021, "n_citations": 7}
{"id": 247877, "s2_id": "124594c66535548a7c1a0bb3a9def3284616e31b", "title": "Hybrid CMOS-CNFET based NP dynamic Carry Look Ahead Adder", "abstract": "Advanced electronic device technologies require a faster operation and smaller average power consumption, which are the most important parameters in very large scale integrated circuit design. The conventional Complementary Metal-Oxide Semiconductor (CMOS) technology is limited by the threshold voltage and subthreshold leakage problems in scaling of devices. This leads to failure in adapting it to sub-micron and nanotechnologies. The carbon nanotube (CNT) technology overcomes the threshold voltage and subthreshold leakage problems despite reduction in size. The CNT based technology develops the most promising devices among emerging technologies because it has most of the desired features. Carbon Nanotube Field Effect Transistors (CNFETs) are the novel devices that are expected to sustain the transistor scalability while increasing its performance. Recently, there have been tremendous advances in CNT technology for nanoelectronics applications. CNFETs avoid most of the fundamental limitations and offer several advantages compared to silicon-based technology. Though CNT evolves as a better option to overcome some of the bulk CMOS problems, the CNT itself still immersed with setbacks. The fabrication of carbon nanotube at very large digital circuits on a single substrate is difficult to achieve. Therefore, a hybrid NP dynamic Carry Look Ahead Adder (CLA) is designed using p-CNFET and n-MOS transistors. Here, the performance of CLA is evaluated in 8-bit, 16-bit, 32-bit and 64-bit stages with the following four different implementations: silicon MOSFET (Si-MOSFET) domino logic, Si-MOSFET NP dynamic CMOS, carbon nanotube MOSFET (CN-MOSFET) domino logic, and CN-MOSFET NP dynamic CMOS. Finally, a Hybrid CMOS-CNFET based 64-bit NP dynamic CLA is evaluated based on HSPICE simulation in 32nm technology, which effectively suppresses power dissipation without an increase in propagation delay.", "venue": "ArXiv", "authors": ["A.  Nagalakshmi", "Ch.  Sirisha", "D. N. Madhusudana Rao"], "year": 2018, "n_citations": 0}
{"id": 248603, "s2_id": "912f93bd04604cba8158e0af5c9d18b26d5727aa", "title": "Energy-Efficient Runtime Adaptable L1 STT-RAM Cache Design", "abstract": "Much research has shown that applications have variable runtime cache requirements. In the context of the increasingly popular spin-transfer torque RAM (STT-RAM) cache, the retention time, which defines how long the cache can retain a cache block in the absence of power, is one of the most important cache requirements that may vary for different applications. In this paper, we propose a logically adaptable retention time STT-RAM (LARS) cache that allows the retention time to be dynamically adapted to applications\u2019 runtime requirements. LARS cache comprises of multiple STT-RAM units with different retention times, with only one unit being used at a given time. LARS dynamically determines which STT-RAM unit to use during runtime, based on executing applications\u2019 needs. As an integral part of LARS, we also explore different algorithms to dynamically determine the best retention time based on different cache design tradeoffs. Our experiments show that by adapting the retention time to different applications\u2019 requirements, LARS cache can reduce the average cache energy by 25.31%, compared to prior work, with minimal overheads.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Kyle  Kuan", "Tosiron  Adegbija"], "year": 2020, "n_citations": 11}
{"id": 248881, "s2_id": "960f04e099afeed7509d66bbdf24b3de0b29adff", "title": "Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices", "abstract": "The prevalence of Internet of things (IoT) devices and abundance of sensor data has created an increase in real-time data processing such as recognition of speech, image, and video. While currently such processes are offloaded to the computationally powerful cloud system, a localized and distributed approach is desirable because (i) it preserves the privacy of users and (ii) it omits the dependency on cloud services. However, IoT networks are usually composed of resource-constrained devices, and a single device is not powerful enough to process real-time data. To overcome this challenge, we examine data and model parallelism for such devices in the context of deep neural networks. We propose Musical Chair to enable efficient, localized, and dynamic real-time recognition by harvesting the aggregated computational power from the resource-constrained devices in the same IoT network as input sensors. Musical chair adapts to the availability of computing devices at runtime and adjusts to the inherit dynamics of IoT networks. To demonstrate Musical Chair, on a network of Raspberry PIs (up to 12) each connected to a camera, we implement a state-of-the-art action recognition model for videos and two recognition models for images. Compared to the Tegra TX2, an embedded low-power platform with a six-core CPU and a GPU, our distributed action recognition system achieves not only similar energy consumption but also twice the performance of the TX2. Furthermore, in image recognition, Musical Chair achieves similar performance and saves dynamic energy.", "venue": "ArXiv", "authors": ["Ramyad  Hadidi", "Jiashen  Cao", "Matthew  Woodward", "Michael S. Ryoo", "Hyesoon  Kim"], "year": 2018, "n_citations": 24}
{"id": 251296, "s2_id": "98089dc874d6aafc3975522f831734f32ee5759f", "title": "High-level synthesis under I/O timing and memory constraints", "abstract": "In the design of complex systems-on-chips, it is necessary to take into account communication and memory access constraints for the integration of a dedicated hardware accelerator. We present a methodology and a tool that allow the high-level synthesis of a DSP algorithm, under both I/O timing and memory constraints. Based on formal models and a generic architecture, this tool helps the designer to find a reasonable trade-off between the required I/O timing behavior and the internal memory access parallelism of the circuit. The interest of our approach is demonstrated by the case study of an FFT algorithm.", "venue": "2005 IEEE International Symposium on Circuits and Systems", "authors": ["Philippe  Coussy", "Gwenol\u00e9  Corre", "Eric  Senn", "Pierre  Bomel", "Eric  Martin"], "year": 2005, "n_citations": 22}
{"id": 257226, "s2_id": "e7c35a183e922c19878bb07ed0b0c28c9de64931", "title": "GRVI Phalanx: A Massively Parallel RISC-V FPGA Accelerator Accelerator", "abstract": "GRVI is an FPGA-efficient RISC-V RV32I soft processor. Phalanx is a parallel processor and accelerator array framework. Groups of processors and accelerators form shared memory clusters. Clusters are interconnected with each other and with extreme bandwidth I/O and memory devices by a Hoplite NOC with 300-bit links. An example Kintex UltraScale 040 system has 400 RISC-V cores, peak throughput of 100,000 MIPS, peak shared memory bandwidth of 600 GB/s, NOC bisection bandwidth of 700 Gb/s, and uses 12-17 W.", "venue": "2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Jan  Gray"], "year": 2016, "n_citations": 50}
{"id": 257423, "s2_id": "f521f388dec8df639f64ab8601870d95d6356132", "title": "Optical Fault Injection Attacks against Radiation-Hard Registers", "abstract": "If devices are physically accessible optical fault injection attacks pose a great threat since the data processed as well as the operation flow can be manipulated. Successful physical attacks may lead not only to leakage of secret information such as cryptographic private keys, but can also cause economic damage especially if as a result of such a manipulation a critical infrastructure is successfully attacked. Laser based attacks exploit the sensitivity of CMOS technologies to electromagnetic radiation in the visible or the infrared spectrum. It can be expected that radiation-hard designs, specially crafted for space applications, are more robust not only against high-energy particles and short electromagnetic waves but also against optical fault injection attacks. In this work we investigated the sensitivity of radiation-hard JICG shift registers to optical fault injection attacks. In our experiments, we were able to trigger bit-set and bit-reset repeatedly changing the data stored in single JICG flip-flops despite their high-radiation fault tolerance.", "venue": "ArXiv", "authors": ["Dmytro  Petryk", "Zoya  Dyka", "Roland  Sorge", "Jan  Schaeffner", "Peter  Langendoerfer"], "year": 2021, "n_citations": 0}
{"id": 264787, "s2_id": "a989718eece1cc804ae89c0b26edf63768e2ae0f", "title": "MaskedNet: The First Hardware Inference Engine Aiming Power Side-Channel Protection", "abstract": "Differential Power Analysis (DPA) has been an active area of research for the past two decades to study the attacks for extracting secret information from cryptographic implementations through power measurements and their defenses. The research on power side-channels have so far predominantly focused on analyzing implementations of ciphers such as AES, DES, RSA, and recently post-quantum cryptography primitives (e.g., lattices). Meanwhile, machine-learning applications are becoming ubiquitous with several scenarios where the Machine Learning Models are Intellectual Properties requiring confidentiality. Expanding side-channel analysis to Machine Learning Model extraction, however, is largely unexplored. This paper expands the DPA framework to neural-network classifiers. First, it shows DPA attacks during inference to extract the secret model parameters such as weights and biases of a neural network. Second, it proposes the first countermeasures against these attacks by augmenting masking. The resulting design uses novel masked components such as masked adder trees for fully-connected layers and masked Rectifier Linear Units for activation functions. On a SAKURA-X FPGA board, experiments show that the first-order DPA attacks on the unprotected implementation can succeed with only 200 traces and our protection respectively increases the latency and area-cost by $ 2.8\\times$ and $2.3\\times$.", "venue": "2020 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)", "authors": ["Anuj  Dubey", "Rosario  Cammarota", "Aydin  Aysu"], "year": 2020, "n_citations": 20}
{"id": 268681, "s2_id": "d7d840201a5b3081cc86a0ed44a9fb051543118e", "title": "Towards a Hardware DSL Ecosystem : RubyRTL and Friends", "abstract": "For several years, hardware design has been undergoing a surprising revival: fueled by open source initiatives, various tools and architectures have recently emerged. This resurgence also involves new hardware description languages. Inspired by the Migen Python community, we present RubyRTL, a novel internal domain-specific language for hardware design embedded in the Ruby language. Ruby -- which is best known in the field of web design -- has proven to be an excellent solution for the design of such DSLs, because of its meta-programming features. This paper presents the main aspects of RubyRTL, along with illustrating examples. We also propose a language-neutral interchange format, named Sexpir, that allows to seamlessly exchange RTL designs between Migen Python DSL and RubyRTL. This paves the way for interactions between various agile communities in the field of open source hardware design.", "venue": "ArXiv", "authors": ["Jean-Christophe Le Lann", "Hannah  Badier", "Florent  Kermarrec"], "year": 2020, "n_citations": 0}
{"id": 278190, "s2_id": "6a72bf14fcdcd0dfb7ef1a7fd1134adfafa1b672", "title": "Predict and Write: Using K-Means Clustering to Extend the Lifetime of NVM Storage", "abstract": "Non-volatile memory (NVM) technologies suffer from limited write endurance. To address this challenge, we propose Predict and Write (PNW), a K/V-store that uses a clustering-based machine learning approach to extend the lifetime of NVMs. PNW decreases the number of bit flips for PUT/UPDATE operations by determining the best memory location an updated value should be written to. PNW leverages the indirection level of K/V-stores to freely choose the target memory location for any given write based on its value. PNW organizes NVM addresses in a dynamic address pool clustered by the similarity of the data values they refer to. We show that, by choosing the right target memory location for a given PUT/UPDATE operation, the number of total bit flips and cache lines can be reduced by up to 85% and 56% over the state of the art.", "venue": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "authors": ["Saeed  Kargar", "Heiner  Litz", "Faisal  Nawab"], "year": 2021, "n_citations": 3}
{"id": 280872, "s2_id": "1927c9e166c00974e5819890debc86a6f24460b0", "title": "ANNETTE: Accurate Neural Network Execution Time Estimation With Stacked Models", "abstract": "With new accelerator hardware for Deep Neural Networks (DNNs), the computing power for Artificial Intelligence (AI) applications has increased rapidly. However, as DNN algorithms become more complex and optimized for specific applications, latency requirements remain challenging, and it is critical to find the optimal points in the design space. To decouple the architectural search from the target hardware, we propose a time estimation framework that allows for modeling the inference latency of DNNs on hardware accelerators based on mapping and layer-wise estimation models. The proposed methodology extracts a set of models from micro-kernel and multi-layer benchmarks and generates a stacked model for mapping and network execution time estimation. We compare estimation accuracy and fidelity of the generated mixed models, statistical models with the roofline model, and a refined roofline model for evaluation. We test the mixed models on the ZCU102 SoC board with Xilinx Deep Neural Network Development Kit (DNNDK) and Intel Neural Compute Stick 2 (NCS2) on a set of 12 state-of-the-art neural networks. It shows an average estimation error of 3.47% for the DNNDK and 7.44% for the NCS2, outperforming the statistical and analytical layer models for almost all selected networks. For a randomly selected subset of 34 networks of the NASBench dataset, the mixed model reaches fidelity of 0.988 in Spearman\u2019s $\\rho $ rank correlation coefficient metric.", "venue": "IEEE Access", "authors": ["Matthias  Wess", "Matvey  Ivanov", "Christoph  Unger", "Anvesh  Nookala", "Alexander  Wendt", "Axel  Jantsch"], "year": 2021, "n_citations": 1}
{"id": 288050, "s2_id": "6a2f8cc8845b3b8228537a84824274f74503e9a8", "title": "Design of SEC-DED and SEC-DED-DAEC Codes of different lengths", "abstract": "Reliability is an important requirement for both communication and storage systems. Due to continuous scale down of technology multiple adjacent bits error probability increases. The data may be corrupted due soft errors. Error correction codes are used to detect and correct the errors. In this paper, design of single error correction-double error detection (SEC-DED) and single error correction-double error detection-double adjacent error correction (SEC-DED-DAEC) codes of different data lengths have been proposed. Proposed SEC-DED and SEC-DED-DAEC codes require lower delay and power compared to existing coding schemes. Area complexity in terms of logic gates of proposed and existing codes have been presented. ASIC-based synthesis results show a notable reduction compared to existing SEC-DED codes. All the codec architectures are synthesized on ASIC platform. Performances of different SEC-DED-DAEC codes are tabulated in terms of area, power and delay.", "venue": "ArXiv", "authors": ["Sayan  Tripathi", "Jhilam  Jana", "Jaydeb  Bhaumik"], "year": 2020, "n_citations": 0}
{"id": 288219, "s2_id": "1c4f763c75aa9336f3f915f93dd320f7872e3903", "title": "MuonTrap: Preventing Cross-Domain Spectre-Like Attacks by Capturing Speculative State", "abstract": "The disclosure of the Spectre speculative-execution attacks in January 2018 has left a severe vulnerability that systems are still struggling with how to patch. The solutions that currently exist tend to have incomplete coverage, perform badly, or have highly undesirable performance edge cases.MuonTrap allows processors to continue to speculate, avoiding significant reductions in performance, without impacting security. We instead prevent the propagation of any state based on speculative execution, by placing the results of speculative cache accesses into a small, fast L0 filter cache, that is non-inclusive, non-exclusive with the rest of the cache hierarchy. This isolates all parts of the system that can\u2019t be quickly cleared on any change in threat domain. MuonTrap uses these speculative filter caches, which are cleared on context and protection-domain switches, along with a series of extensions to the cache coherence protocol and prefetcher. This renders systems immune to cross-domain information leakage via Spectre and a host of similar attacks based on speculative execution, with low performance impact and few changes to the CPU design.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Sam  Ainsworth", "Timothy M. Jones"], "year": 2020, "n_citations": 30}
{"id": 288547, "s2_id": "15f04407906f4111119e289a308be0db33f9ffba", "title": "An Energy-Efficient Accelerator Architecture with Serial Accumulation Dataflow for Deep CNNs", "abstract": "Convolutional Neural Networks (CNNs) have shown outstanding accuracy for many vision tasks during recent years. When deploying CNNs on portable devices and embedded systems, however, the large number of parameters and computations result in long processing time and low battery life. An important factor in designing CNN hardware accelerators is to efficiently map the convolution computation onto hardware resources. In addition, to save battery life and reduce energy consumption, it is essential to reduce the number of DRAM accesses since DRAM consumes orders of magnitude more energy compared to other operations in hardware. In this paper, we propose an energy-efficient architecture which maximally utilizes its computational units for convolution operations while requiring a low number of DRAM accesses. The implementation results show that the proposed architecture performs one image recognition task using the VGGNet model with a latency of 393 ms and only 251.5 MB of DRAM accesses.", "venue": "2020 18th IEEE International New Circuits and Systems Conference (NEWCAS)", "authors": ["Mehdi  Ahmadi", "Shervin  Vakili", "J. M. Pierre Langlois"], "year": 2020, "n_citations": 4}
{"id": 289650, "s2_id": "ee226106b8af135477e8ba3fb3d5aedbeb027555", "title": "A Technique for Efficiently Managing SRAM-NVM Hybrid Cache", "abstract": "In this paper, we present a SRAM-PCM hybrid cache design, along with a cache replacement policy, named dead fast block (DFB) to manage the hybrid cache. This design aims to leverage the best features of both SRAM and PCM devices. Compared to a PCM-only cache, the hybrid cache with DFB policy provides superior results on all relevant evaluation metrics, viz. cache lifetime, performance and energy efficiency. Also, use of DFB policy for managing the hybrid cache provides better results compared to LRU replacement policy on all the evaluation metrics.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 1}
{"id": 291191, "s2_id": "4fd42a1099aca4fef2d4196e227c40906695f418", "title": "Data Conversion in Area-Constrained Applications: the Wireless Network-on-Chip Case", "abstract": "Network-on-Chip (NoC) is currently the paradigm of choice to interconnect the different components of System-on-Chips (SoCs) or Chip Multiprocessors (CMPs). As the levels of integration continue to grow, however, current NoCs face significant scalability limitations and have prompted research in novel interconnect technologies. Among these, wireless intra-chip communications have been under intense scrutiny due to their low latency broadcast and architectural flexibility. Thus far, the practicality of the idea has been studied from the RF front-end and the network interface perspectives, whereas little to no attention has been placed on another essential component: the data converters. This article aims to fill this gap by providing a comprehensive analysis of the requirements of the scenario, as well as of the current performance and cost trends of Analog-to-Digital Converters (ADCs). Based on Murmann\u2019s data, we demonstrate that ADCs will not be a roadblock for the realization of wireless intra-chip communications although current designs do not meet their demands fully.", "venue": "2018 Conference on Design of Circuits and Integrated Systems (DCIS)", "authors": ["Sergi  Abadal", "Eduard  Alarc\u00f3n"], "year": 2018, "n_citations": 0}
{"id": 291340, "s2_id": "caff5f05d94cf411d0572a9e45c9427254dfaec2", "title": "Coordinated Management of DVFS and Cache Partitioning under QoS Constraints to Save Energy in Multi-Core Systems", "abstract": "Reducing the energy expended to carry out a computational task is important. In this work, we explore the prospects of meeting Quality-of-Service requirements of tasks on a multi-core system while adjusting resources to expend a minimum of energy. This paper considers, for the first time, a QoS-driven coordinated resource management algorithm (RMA) that dynamically adjusts the size of the per-core last-level cache partitions and the per-core voltage-frequency settings to save energy while respecting QoS requirements of every application in multi-programmed workloads run on multi-core systems. It does so by doing configuration-space exploration across the spectrum of LLC partition sizes and Dynamic Voltage Frequency Scaling (DVFS) settings at runtime at negligible overhead. We show that the energy of 4-core and 8-core systems can be reduced by up to 18% and 14%, respectively, compared to a baseline with even distribution of cache resources and a fixed mid-range core voltage-frequency setting. The energy savings can potentially reach 29% if the QoS targets are relaxed to 40% longer execution time.", "venue": "J. Parallel Distributed Comput.", "authors": ["Mehrzad  Nejat", "Madhavan  Manivannan", "Miquel  Peric\u00e0s", "Per  Stenstr\u00f6m"], "year": 2020, "n_citations": 2}
{"id": 292565, "s2_id": "c8f66164288b17613338840530552d601bd7334e", "title": "Selective decoding in associative memories based on Sparse-Clustered Networks", "abstract": "Associative memories are structures that can retrieve previously stored information given a partial input pattern instead of an explicit address as in indexed memories. A few hardware approaches have recently been introduced for a new family of associative memories based on Sparse-Clustered Networks (SCN) that show attractive features. These architectures are suitable for implementations with low retrieval latency, but are limited to small networks that store a few hundred data entries. In this paper, a new hardware architecture of SCNs is proposed that features a new data-storage technique as well as a method we refer to as Selective Decoding (SD-SCN). The SD-SCN has been implemented using a similar FPGA used in the previous efforts and achieves two orders of magnitude higher capacity, with no error-performance penalty but with the cost of few extra clock cycles per data access.", "venue": "2013 IEEE Global Conference on Signal and Information Processing", "authors": ["Hooman  Jarollahi", "Naoya  Onizawa", "Warren J. Gross"], "year": 2013, "n_citations": 6}
{"id": 294921, "s2_id": "e2bbb101d19d87a747acc45c4fe57cd9e6f6bcf8", "title": "Best implementations of quaternary adders", "abstract": "The implementation of a quaternary 1-digit adder composed of a 2-bit binary adder, quaternary to binary decoders and binary to quaternary encoders is compared with several recent implementations of quaternary adders. This simple implementation outperforms all other implementations using only one power supply. It is equivalent to the best other implementation using three power supplies. The best quaternary adder using a 2-bit binary adder, the interface circuits between quaternary and binary levels are just overhead compared to the binary adder. This result shows that the quaternary approach for adders use more transistors, more chip area and more power dissipation than the corresponding binary ones.", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2020, "n_citations": 2}
{"id": 298252, "s2_id": "fb527b677b3208bcf91ed8fcee48a21c4af150bf", "title": "Systolic-CNN: An OpenCL-defined Scalable Run-time-flexible FPGA Accelerator Architecture for Accelerating Convolutional Neural Network Inference in Cloud/Edge Computing", "abstract": "This paper presents Systolic-CNN, an OpenCLdefined scalable, run-time-flexible FPGA accelerator architecture, optimized for performing the low-latency, energy-efficient inference of various convolutional neural networks (CNNs) in the context of multi-tenancy cloud/edge computing. Systolic-CNN adopts a highly pipelined and parallelized 1-D systolic array architecture, which efficiently explores both spatial and temporal parallelism for accelerating CNN inference on FPGAs. SystolicCNN is highly scalable and parameterized, which can be easily adapted by users to achieve 100% utilization of the coarsegrained computation resources (i.e., DSP blocks) for a given FPGA. In addition, Systolic-CNN is run-time-flexible, which can be time-shared, in the context of multi-tenancy cloud or edge computing, to accelerate a variety of CNN models at run time without the need of recompiling the FPGA kernel hardware nor reprogramming the FPGA. The experiment results based on an Intel Arria 10 GX FPGA Development board show that Systolic-CNN, when mapped with a single-precision data format, can achieve 100% utilization of the DSP block resource and an average inference latency of 10ms, 84ms, 1615ms, and 990ms per image for accelerating AlexNet, ResNet-50, RetinaNet, and Light-weight RetinaNet, respectively. The peak computational throughput is measured at 80\u2013170 GFLOPS/s across the acceleration of different CNN models. Codes are available at https://github.com/PSCLab-ASU/SystolicCNN.", "venue": "2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Akshay  Dua", "Yixing  Li", "Fengbo  Ren"], "year": 2020, "n_citations": 3}
{"id": 300392, "s2_id": "ab4ce07fe9651e84d822c222344a746ca0eb178b", "title": "LORAX: Loss-Aware Approximations for Energy-Efficient Silicon Photonic Networks-on-Chip", "abstract": "The approximate computing paradigm advocates for relaxing accuracy goals in applications to improve energy-efficiency and performance. Recently, this paradigm has been explored to improve the energy efficiency of silicon photonic networks-on-chip (PNoCs). In this paper, we propose a novel framework (LORAX) to enable more aggressive approximation during communication over silicon photonic links in PNoCs. This is the first work that considers loss-aware laser power management and multilevel signaling to enable effective data approximation and energy-efficiency in PNoCs. Simulation results show that our framework can achieve up to 31.4% lower laser power consumption and up to 12.2% better energy efficiency than the best known prior work on approximate communication in PNoCs, for the same application output quality.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Febin  Sunny", "Asif  Mirza", "Ishan G. Thakkar", "Sudeep  Pasricha", "Mahdi  Nikdast"], "year": 2020, "n_citations": 6}
{"id": 301751, "s2_id": "1b7601652f5b9ce8824eca06da9d2ef246361f8c", "title": "Computing-in-Memory for Performance and Energy-Efficient Homomorphic Encryption", "abstract": "Homomorphic encryption (HE) allows direct computations on encrypted data. Despite numerous research efforts, the practicality of HE schemes remains to be demonstrated. In this regard, the enormous size of ciphertexts involved in HE computations degrades computational efficiency. Near-memory processing (NMP) and computing-in-memory (CiM)\u2014paradigms where computation is done within the memory boundaries\u2014represent architectural solutions for reducing latency and energy associated with data transfers in data-intensive applications, such as HE. This article introduces CiM-HE, a CiM architecture that can support operations for the Brakerski/Fan\u2013Vercauteren (B/FV) scheme, a somewhat HE scheme for general computation. CiM-HE hardware consists of customized peripherals, such as sense amplifiers, adders, bit shifters, and sequencing circuits. The peripherals are based on CMOS technology and could support computations with memory cells of different technologies. Circuit-level simulations are used to evaluate our CiM-HE framework assuming a 6T-SRAM memory. We compare our CiM-HE implementation against: 1) two optimized CPU HE implementations and 2) a field-programmable gate array (FPGA)-based HE accelerator implementation. Compared with a CPU solution, CiM-HE obtains speedups between <inline-formula> <tex-math notation=\"LaTeX\">$4.6\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$9.1\\times $ </tex-math></inline-formula> and energy savings between <inline-formula> <tex-math notation=\"LaTeX\">$266.4\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$532.8\\times $ </tex-math></inline-formula> for homomorphic multiplications (the most expensive HE operation). Also, a set of four end-to-end tasks, i.e., mean, variance, linear regression, and inference, are up to <inline-formula> <tex-math notation=\"LaTeX\">$1.1\\times $ </tex-math></inline-formula>, <inline-formula> <tex-math notation=\"LaTeX\">$7.7\\times $ </tex-math></inline-formula>, <inline-formula> <tex-math notation=\"LaTeX\">$7.1\\times $ </tex-math></inline-formula>, and <inline-formula> <tex-math notation=\"LaTeX\">$7.5\\times $ </tex-math></inline-formula> faster (and <inline-formula> <tex-math notation=\"LaTeX\">$301.1\\times $ </tex-math></inline-formula>, <inline-formula> <tex-math notation=\"LaTeX\">$404.6\\times $ </tex-math></inline-formula>, <inline-formula> <tex-math notation=\"LaTeX\">$532.3\\times $ </tex-math></inline-formula>, and <inline-formula> <tex-math notation=\"LaTeX\">$532.8\\times $ </tex-math></inline-formula> more energy efficient). Compared with CPU-based HE in previous work, CiM-HE obtains <inline-formula> <tex-math notation=\"LaTeX\">$14.3\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$> 2600\\times $ </tex-math></inline-formula> energy savings. Finally, our design offers <inline-formula> <tex-math notation=\"LaTeX\">$2.2\\times $ </tex-math></inline-formula> speedup with <inline-formula> <tex-math notation=\"LaTeX\">$88.1\\times $ </tex-math></inline-formula> energy savings compared with a state-of-the-art FPGA-based accelerator.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Dayane  Reis", "Jonathan  Takeshita", "Taeho  Jung", "Michael  Niemier", "Xiaobo Sharon Hu"], "year": 2020, "n_citations": 6}
{"id": 301960, "s2_id": "b763e0c00e26a805cbaa95847742a20acb1983a6", "title": "Non-volatile Hierarchical Temporal Memory: Hardware for Spatial Pooling", "abstract": "Hierarchical Temporal Memory (HTM) is a biomimetic machine learning algorithm imbibing the structural and algorithmic properties of the neocortex. Two main functional components of HTM that enable spatio-temporal processing are the spatial pooler and temporal memory. In this research, we explore a scalable hardware realization of the spatial pooler closely coupled with the mathematical formulation of spatial pooler. This class of neuromorphic algorithms are advantageous in solving a subset of the future engineering problems by extracting nonintuitive patterns in complex data. The proposed architecture, Non-volatile HTM (NVHTM), leverages large-scale solid state flash memory to realize a optimal memory organization, area and power envelope. A behavioral model of NVHTM is evaluated against the MNIST dataset, yielding 91.98% classification accuracy. A full custom layout is developed to validate the design in a TSMC 180nm process. The area and power profile of the spatial pooler are 30.538mm2 and 64.394mW, respectively. This design is a proof-of-concept that storage processing is a viable platform for large scale HTM network models.", "venue": "ArXiv", "authors": ["Lennard  Streat", "Dhireesha  Kudithipudi", "Kevin  Gomez"], "year": 2016, "n_citations": 12}
{"id": 302323, "s2_id": "09792059ec5c1e0802fe5a28993fa21bd947acb0", "title": "A Customized NoC Architecture to Enable Highly Localized Computing-On-the-Move DNN Dataflow", "abstract": "The ever-increasing computation complexity of fastgrowing Deep Neural Networks (DNNs) has requested new computing paradigms to overcome the memory wall in conventional Von Neumann computing architectures. The emerging Computing-In-Memory (CIM) architecture has been a promising candidate to accelerate neural network computing. However, data movement between CIM arrays may still dominate the total power consumption in conventional designs. This paper proposes a flexible CIM processor architecture named Domino and \u201cComputing-On-the-Move\u201d (COM) dataflow, to enable stream computing and local data access to significantly reduce data movement energy. Meanwhile, Domino employs customized distributed instruction scheduling within Network-on-Chip (NoC) to implement inter-memory computing and attain mapping flexibility. The evaluation with prevailing DNN models shows that Domino achieves 1.77-to-2.37\u00d7 power efficiency over several state-of-the-art CIM accelerators and improves the throughput by 1.28-to-13.16\u00d7.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Kaining  Zhou", "Yangshuo  He", "Rui  Xiao", "Jiayi  Liu", "Kejie  Huang"], "year": 2021, "n_citations": 0}
{"id": 305489, "s2_id": "a87c6d41a5b6d5d75bcb3efbb73bc8ff6d807c50", "title": "Combined Integer and Floating Point Multiplication Architecture(CIFM) for FPGAs and Its Reversible Logic Implementation", "abstract": "In this paper, the authors propose the idea of a combined integer and floating point multiplier (CIFM) for FPGAs. The authors propose the replacement of existing 18times18 dedicated multipliers in FPGAs with dedicated 24times24 multipliers designed with small 4times4 bit multipliers. It is also proposed that for every dedicated 24times24 bit multiplier block designed with 4times4 bit multipliers, four redundant 4times4 multiplier should be provided to enforce the feature of self repairability (to recover from the faults). In the proposed CIFM reconfigurability at run time is also provided resulting in low power. The major source of motivation for providing the dedicated 24times24 bit multiplier stems from the fact that single precision floating point multiplier requires 24times24 bit integer multiplier for mantissa multiplication. A reconfigurable, self-repairable 24times24 bit multiplier (implemented with 4times4 bit multiply modules) will ideally suit this purpose, making FPGAs more suitable for integer as well floating point operations. A dedicated 4times4 bit multiplier is also proposed in this paper. Moreover, in the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. It is not possible to realize quantum computing without reversible logic. Thus, this paper also paper provides the reversible logic implementation of the proposed CIFM. The reversible CIFM designed and proposed here will form the basis of the completely reversible FPGAs.", "venue": "2006 49th IEEE International Midwest Symposium on Circuits and Systems", "authors": ["Himanshu  Thapliyal", "Hamid R. Arabnia", "A. Prasad Vinod"], "year": 2006, "n_citations": 23}
{"id": 307855, "s2_id": "03567bbe6d15144eb16bd21d6fb786fca8b6f77c", "title": "Analysis and Design of a 32nm FinFET Dynamic Latch Comparator", "abstract": "Comparators have multifarious applications in various fields, especially used in analog to digital converters. Over the years, we have seen many different designs of single stage, dynamic latch type and double tail type comparators based on CMOS technology, and all of them had to make the tradeoff between power consumption and delay time. Meanwhile, to mitigate the short channel effects of conventional CMOS based design, FinFET has emerged as the most promising alternative by owning the tremendous gate control feature over the channel region. In this paper, we have analyzed the performance of some recent dynamic latch type comparators and proposed a new structure of dynamic latch comparator; moreover, 32nm FinFET technology has been considered as the common platform for all of the comparators circuit design. The proposed comparator has shown impressive performance in case of power consumption, time delay, power delay product and offset voltage while compared with the other recent comparators through simulations with LTspice.", "venue": "2019 5th International Conference on Advances in Electrical Engineering (ICAEE)", "authors": ["Mir Muntasir Hossain", "Satyendra  Biswas"], "year": 2019, "n_citations": 1}
{"id": 310541, "s2_id": "31a7d6b721abd0d95f9c1d1bc46d813f96cc5aac", "title": "Accelerating Transient Fault Injection Campaigns by using Dynamic HDL Slicing", "abstract": "Along with the complexity of electronic systems for safety-critical applications, the cost of safety mechanisms evaluation by fault injection simulation is rapidly going up. To reduce these efforts, we propose a fault injection methodology where Hardware Description Language (HDL) code slicing is exploited to accelerate transient fault injection campaigns by pruning fault lists and reducing the number of the injections. In particular, the dynamic HDL slicing technique provides for a critical fault list and allows avoiding injections at non-critical time-steps. Experimental results on an industrial core show that the proposed methodology can successfully reduce the number of injections by up to 10 percent and speed-up the fault injection campaigns.", "venue": "2019 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC)", "authors": ["Ahmet Cagri Bagbaba", "Maksim  Jenihhin", "Jaan  Raik", "Christian  Sauer"], "year": 2019, "n_citations": 2}
{"id": 313130, "s2_id": "deaf935195d968ea6996727f42f758e5e28dc196", "title": "Koios: A Deep Learning Benchmark Suite for FPGA Architecture and CAD Research", "abstract": "With the prevalence of deep learning (DL) in many applications, researchers are investigating different ways of optimizing FPGA architecture and CAD to achieve better quality-of-results (QoR) on DL-based workloads. In this optimization process, benchmark circuits are an essential component; the QoR achieved on a set of benchmarks is the main driver for architecture and CAD design choices. However, current academic benchmark suites are inadequate, as they do not capture any designs from the DL domain. This work presents a new suite of DL acceleration benchmark circuits for FPGA architecture and CAD research, called Koios. This suite of 19 circuits covers a wide variety of accelerated neural networks, design sizes, implementation styles, abstraction levels, and numerical precisions. These designs are larger, more data parallel, more heterogeneous, more deeply pipelined, and utilize more FPGA architectural features compared to existing open-source benchmarks. This enables researchers to pin-point architectural inefficiencies for this class of workloads and optimize CAD tools on more realistic benchmarks that stress the CAD algorithms in different ways. In this paper, we describe the designs in our benchmark suite, present results of running them through the Verilog-to-Routing (VTR) flow using a recent FPGA architecture model, and identify key insights from the resulting metrics. On average, our benchmarks have 3.7\u00d7 more netlist primitives, 1.8\u00d7 and 4.7\u00d7 higher DSP and BRAM densities, and 1.7\u00d7 higher frequency with 1.9\u00d7 more near-critical paths compared to the widely-used VTR suite. Finally, we present two example case studies showing how architectural exploration for DL-optimized FPGAs can be performed using our new benchmark suite.", "venue": "2021 31st International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Aman  Arora", "Andrew  Boutros", "Daniel  Rauch", "Aishwarya  Rajen", "Aatman  Borda", "Seyed Alireza Damghani", "Samidh  Mehta", "Sangram  Kate", "Pragnesh  Patel", "Kenneth B. Kent", "Vaughn  Betz", "Lizy K. John"], "year": 2021, "n_citations": 0}
{"id": 313558, "s2_id": "c145303891b9458ec29eba8836d6876f3f152674", "title": "Special Session: Reliability Analysis for ML/AI Hardware", "abstract": "Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive in today\u2019s applications, such as autonomous vehicles, healthcare, aerospace, cybersecurity, and many critical applications. Ensuring the reliability and robustness of the underlying AI/ML hardware becomes our paramount importance. In this paper, we explore and evaluate the reliability of different AI/ML hardware. The first section outlines the reliability issues in a commercial systolic array-based ML accelerator in the presence of faults engendering from devicelevel non-idealities in the DRAM. Next, we quantified the impact of circuit-level faults in the MSB and LSB logic cones of the Multiply and Accumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we present two key reliability issues \u2013 circuit aging and endurance in emerging neuromorphic hardware platforms and present our system-level approach to mitigate them.", "venue": "ArXiv", "authors": ["Shamik  Kundu", "Kanad  Basu", "Mehdi  Sadi", "Twisha  Titirsha", "Shihao  Song", "Anup  Das", "Ujjwal  Guin"], "year": 2021, "n_citations": 8}
{"id": 313696, "s2_id": "85c6ca41d4a7813b6e7e13a0a876a28110e15c79", "title": "Hybrid crossbar architecture for a memristor based memory", "abstract": "This paper describes a new memristor crossbar architecture that is proposed for use in a high density cache design. This design has less than 10% of the write energy consumption than a simple memristor crossbar. Also, it has up to 4 times the bit density of an STT-MRAM system and up to 11 times the bit density of an SRAM architecture. The proposed architecture is analyzed using a detailed SPICE analysis that accounts for the resistance of the wires in the memristor structure. Additionally, the memristor model used in this work has been matched to specific device characterization data to provide accurate results in terms of energy, area, and timing.", "venue": "NAECON 2014 - IEEE National Aerospace and Electronics Conference", "authors": ["Chris  Yakopcic", "Tarek M. Taha", "Raqibul  Hasan"], "year": 2014, "n_citations": 21}
{"id": 314183, "s2_id": "750e2c07bc7b8932156d08a50ac00d15933fd90e", "title": "A Traffic-Aware Medium Access Control Mechanism for Energy-Efficient Wireless Network-on-Chip Architectures", "abstract": "Wireless interconnection has emerged as an energy efficient solution to the challenges of multi-hop communication over the wireline paths in conventional Networks-on-Chips (NoCs). However, to ensure the full benefits of this novel interconnect technology, design of simple, fair and efficient Medium Access Control (MAC) mechanism to grant access to the on-chip wireless communication channel is needed. Moreover, to adapt to the varying traffic demands from the applications running on a multicore environment, MAC mechanisms should dynamically adjust the transmission slots of the wireless interfaces (WIs). Such dynamic adjustment in transmission slots will result in improving the utilization of the wireless medium in a Wireless NoC (WiNoC). In this paper we present the design of two dynamic MAC mechanisms that adjust the transmission slots of the WIs based on predicted traffic demands and allow partial packet transfer. Through system level simulations, we demonstrate that the traffic aware MAC mechanisms are more energy efficient as well as capable of sustaining higher data bandwidth in WiNoCs.", "venue": "ArXiv", "authors": ["Naseef  Mansoor", "Abhishek  Vashist", "M. Meraj Ahmed", "Md Shahriar Shamim", "Syed Ashraf Mamun", "Amlan  Ganguly"], "year": 2018, "n_citations": 2}
{"id": 317817, "s2_id": "96034556f6dfe7918eae1a003dca241a45521fbe", "title": "Testable design of repeaterless low swing on-chip interconnect", "abstract": "Repeaterless low swing interconnects use mixed signal circuits to achieve high performance at low power. When these interconnects are used in large scale and high volume digital systems their testability becomes very important. This paper discusses the testability of low swing repeaterless on-chip interconnects with equalization and clock synchronization. A capacitively coupled transmitter with a weak driver is used as the transmitter. The receiver samples the low swing input data at the center of the data eye and converts it to rail to rail levels and also synchronizes the data to the receiver's clock domain. The system is a mixed signal circuit and the digital components are all scan testable. For the analog section, just a DC test has a fault coverage of 50% of the structural faults. Simple techniques allow integration of the analog components into the digital scan chain increasing the coverage to 74%. Finally, a BIST with low overhead enhances the coverage to 95% of the structural faults. The design and simulations have been done in UMC 130 nm CMOS technology.", "venue": "2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["K.  Naveen", "Dinesh K. Sharma"], "year": 2016, "n_citations": 2}
{"id": 322781, "s2_id": "d6531e7909ce6782b68bc71efb5e70f91893100d", "title": "An Analog Neural Network Computing Engine Using CMOS-Compatible Charge-Trap-Transistor (CTT)", "abstract": "An analog neural network computing engine based on CMOS-compatible charge-trap transistor (CTT) is proposed in this paper. CTT devices are used as analog multipliers. Compared to digital multipliers, CTT-based analog multiplier shows significant area and power reduction. The proposed computing engine is composed of a scalable CTT multiplier array and energy efficient analog\u2013digital interfaces. By implementing the sequential analog fabric, the engine\u2019s mixed-signal interfaces are simplified and hardware overhead remains constant regardless of the size of the array. A proof-of-concept 784 by 784 CTT computing engine is implemented using TSMC 28-nm CMOS technology and occupies 0.68 mm2. The simulated performance achieves 76.8 TOPS (8-bit) with 500 MHz clock frequency and consumes 14.8 mW. As an example, we utilize this computing engine to address a classic pattern recognition problem\u2014classifying handwritten digits on MNIST database and obtained a performance comparable to state-of-the-art fully connected neural networks using 8-bit fixed-point resolution.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Yuan  Du", "Li  Du", "Xuefeng  Gu", "Jieqiong  Du", "X. Shawn Wang", "Boyu  Hu", "Mingzhe  Jiang", "Xiaoliang  Chen", "Subramanian S. Iyer", "Mau-Chung Frank Chang"], "year": 2019, "n_citations": 18}
{"id": 325952, "s2_id": "95b54b0e9fae6fd3c2ff99bde6882cad5cee7299", "title": "S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN Acceleration", "abstract": "Exploiting sparsity is a key technique in accelerating quantized convolutional neural network (CNN) inference on mobile devices. Prior sparse CNN accelerators largely exploit unstructured sparsity and achieve significant speedups. Due to the unbounded, largely unpredictable sparsity patterns, however, exploiting unstructured sparsity requires complicated hardware design with significant energy and area overhead, which is particularly detrimental to mobile/IoT inference scenarios where energy and area efficiency are crucial. We propose to exploit structured sparsity, more specifically, Density Bound Block (DBB) sparsity for both weights and activations. DBB block tensors bound the maximum number of non-zeros per block. DBB thus exposes statically predictable sparsity patterns that enable lean sparsity-exploiting hardware. We propose new hardware primitives to implement DBB sparsity for (static) weights and (dynamic) activations, respectively, with very low overheads. Building on top of the primitives, we describe S2TA, a systolic array-based CNN accelerator that exploits joint weight and activation DBB sparsity and new dimensions of data reuse unavailable on the traditional systolic array. S2TA in 16nm achieves more than 2\u00d7 speedup and energy reduction compared to a strong baseline of a systolic array with zero-value clock gating, over five popular CNN benchmarks. Compared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and SparTen (45nm), S2TA in 65nm uses about 2.2\u00d7 and 3.1\u00d7 less energy per inference, respectively.", "venue": "ArXiv", "authors": ["Zhi-Gang  Liu", "Paul N. Whatmough", "Yuhao  Zhu", "Matthew  Mattina"], "year": 2021, "n_citations": 0}
{"id": 327225, "s2_id": "bb2ddccd0ab767a68292156ed016b37380e93feb", "title": "Exploration of Low Numeric Precision Deep Learning Inference Using Intel\u00ae FPGAs", "abstract": "Convolutional neural networks (CNN) have been shown to maintain reasonable classification accuracy when quantized to lower precisions, however, quantizing to sub 8-bit activations and weights can result in classification accuracy falling below an acceptable threshold. Techniques exist for closing the accuracy gap of limited numeric precision networks typically by means of increasing computation. This results in a trade-off between throughput and accuracy and can be tailored for different networks through various combinations of activation and weight data widths. Customizable hardware architectures like FPGAs provide the opportunity for data width specific computation through unique logic configurations leading to highly optimized processing that is unattainable by full precision networks. Specifically, ternary and binary weighted networks offer an efficient method of inference for 2-bit and 1-bit data respectively. Most hardware architectures can take advantage of the memory storage and bandwidth savings that come along with a smaller datapath, but very few architectures can take full advantage of limited numeric precision at the computation level. In this paper, we present a hardware design for FPGAs that takes advantage of the bandwidth, memory, power, and computation savings of limited numerical precision data. We provide insights into the trade-offs between throughput and accuracy for various networks and how they map to our framework. Further, we show how limited numeric precision computation can be efficiently mapped onto FPGAs for both ternary and binary cases. Starting with Arria 10, we show a 2-bit activation and ternary weighted AlexNet running in hardware that achieves 3,700 images per second on the ImageNet dataset with a top-1 accuracy of 0.49. Using a hardware modeler designed for our low numeric precision framework we project performance most notably for a 55.5 TOPS Stratix 10 device running a modified ResNet-34 with only 3.7% accuracy degradation compared with single precision.", "venue": "FCCM", "authors": ["Philip  Colangelo", "Nasibeh  Nasiri", "Eriko  Nurvitadhi", "Asit K. Mishra", "Martin  Margala", "Kevin  Nealis"], "year": 2018, "n_citations": 12}
{"id": 327903, "s2_id": "767cab306270f08e100ad65ba6e596f9e912116c", "title": "A design methodology for space-time adapter", "abstract": "This paper presents a solution to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall architecture of the system is significantly affected by communication architecture, so the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism&), and formalizes communication constraints through a Resource Constraints Graph (RCG). The RCG properties enable an efficient architecture space exploration in order to synthesize a STAR component. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.", "venue": "GLSVLSI '07", "authors": ["Cyrille  Chavet", "Philippe  Coussy", "Pascal  Urard", "Eric  Martin"], "year": 2007, "n_citations": 8}
{"id": 328996, "s2_id": "b7df68848577e08ce52d7718628628a57c31eaa3", "title": "NoM: Network-on-Memory for Inter-Bank Data Transfer in Highly-Banked Memories", "abstract": "Data copy is a widely-used memory operation in many programs and operating system services. In conventional computers, data copy is often carried out by two separate read and write transactions that pass data back and forth between the DRAM chip and the processor chip. Some prior mechanisms propose to avoid this unnecessary data movement by using the shared internal bus in the DRAM chip to directly copy data within the DRAM chip (e.g., between two DRAM banks). While these methods exhibit superior performance compared to conventional techniques, data copy across different DRAM banks is still greatly slower than data copy within the same DRAM bank. Hence, these techniques have limited benefit for the emerging 3D-stacked memories (e.g., HMC and HBM) that contain hundreds of DRAM banks across multiple memory controllers. In this paper, we present Network-on-Memory (NoM), a lightweight inter-bank data communication scheme that enables direct data copy across both memory banks of a 3D-stacked memory. NoM adopts a TDM-based circuit-switching design, where circuit setup is done by the memory controller. Compared to state-of-the-art approaches, NoM enables both fast data copy between multiple DRAM banks and concurrent data transfer operations. Our evaluation shows that NoM improves the performance of data-intensive workloads by 3.8X and 75 percent, on average, compared to the baseline conventional 3D-stacked DRAM architecture and state-of-the-art techniques, respectively.", "venue": "IEEE Computer Architecture Letters", "authors": ["Seyyed Hossein SeyyedAghaei Rezaei", "Mehdi  Modarressi", "Rachata  Ausavarungnirun", "Mohammad  Sadrosadati", "Onur  Mutlu", "Masoud  Daneshtalab"], "year": 2020, "n_citations": 14}
{"id": 333685, "s2_id": "8f8fd384727dbe66555e5f12877e9d7fe652a1ac", "title": "Non Uniform On Chip Power Delivery Network Synthesis Methodology", "abstract": "In this paper, we proposed a non-uniform power delivery network (PDN) synthesis methodology. It first constructs initial PDN using uniform approach. Then preliminary power integrity analysis is performed to derive IR-safe candidate window. Congestion map is obtained based global route congestion estimation. A self-adaptive non-uniform PDN synthesis is then performed to globally and locally optimize PDN over selected regions. The PDN synthesis is congestion-driven and IR- guarded. Experimental results show significant timing important in trade-off small PDN length reduction with no EM/IR impact. We further explored potential power savings using our non-uniform PDN synthesis methodology.", "venue": "ArXiv", "authors": ["Patrick  Benediktsson", "Jon A. Flandrin", "Chen  Zheng"], "year": 2017, "n_citations": 4}
{"id": 335707, "s2_id": "ea52486d3e77be3b825238c32f5566d6eaa3b598", "title": "A Unique 10 Segment Display for Bengali Numerals", "abstract": "Segmented display is widely used for efficient display of alphanumeric characters. English numerals are displayed by 7 segment and 16 segment display. The segment size is uniform in this two display architecture. Display architecture using 8, 10, 11, 18 segments have been proposed for Bengali numerals 0...9 yet no display architecture is designed using segments of uniform size and uniform power consumption. In this paper we have proposed a uniform 10 segment architecture for Bengali numerals. This segment architecture uses segments of uniform size and no bent segment is used.", "venue": "ArXiv", "authors": ["Muhammad Abul Kalam Azad", "Rezwana  Sharmeen", "Shabbir  Ahmad", "S. M. Kamruzzaman"], "year": 2010, "n_citations": 6}
{"id": 340789, "s2_id": "f55f8542b7712ef386548d9d502dc4a7e93fd96b", "title": "Fault Tolerant Variable Block Carry Skip Logic (VBCSL) using Parity Preserving Reversible Gates", "abstract": "Reversible logic design has become one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient design approaches of fault tolerant carry skip adders (FTCSAs) and compares those designs with the existing ones. Variable block carry skip logic (VBCSL) using the fault tolerant full adders (FTFAs) has also been developed. The designs are minimized in terms of hardware complexity, gate count, constant inputs and garbage outputs. Besides of it, technology independent evaluation of the proposed designs clearly demonstrates its superiority with the existing counterparts.", "venue": "ArXiv", "authors": ["Md. Saiful Islam", "Muhammad Mahbubur Rahman", "Zerina  Begum", "Mohd. Zulfiquar Hafiz"], "year": 2010, "n_citations": 11}
{"id": 340824, "s2_id": "f3dc0da54268a4b558866a1fbc2101791316d029", "title": "Design of an Optoelectronic State Machine with integrated BDD based Optical logic", "abstract": "In this paper I demonstrate a novel design for an optoelectronic State Machine which replaces input/output forming logic found in conventional state machines with BDD based optical logic while still using solid state memory in the form of flip-flops in order to store states. This type of logic makes use of waveguides and ring resonators to create binary switches. These switches in turn can be used to create combinational logic which can be used as input/output forming logic for a state machine. Replacing conventional combinational logic with BDD based optical logic allows for a faster range of state machines that can certainly outperform conventional state machines as propagation delays within the logic described are in the order of picoseconds as opposed to nanoseconds in digital logic.", "venue": "ArXiv", "authors": ["Macauley  Coggins"], "year": 2016, "n_citations": 0}
{"id": 346438, "s2_id": "20b51591d2ac8e2b914a59dff289be9c4139d161", "title": "TrappeD: DRAM Trojan Designs for Information Leakage and Fault Injection Attacks", "abstract": "In this paper, we investigate the advanced circuit features such as wordline- (WL) underdrive (prevents retention failure) and overdrive (assists write) employed in the peripherals of Dynamic RAM (DRAM) memories from a security perspective. In an ideal environment, these features ensure fast and reliable read and write operations. However, an adversary can re-purpose them by inserting Trojans to deliver malicious payloads such as fault injections, Denial-of-Service (DoS), and information leakage attacks when activated by the adversary. Simulation results indicate that wordline voltage can be increased to cause retention failure and thereby launch a DoS attack in DRAM memory. Furthermore, two wordlines or bitlines can be shorted to leak information or inject faults by exploiting the DRAM's refresh operation. We demonstrate an information leakage system exploit by implementing TrappeD on RocketChip SoC.", "venue": "ArXiv", "authors": ["Karthikeyan  Nagarajan", "Asmit  De", "Mohammad Nasim Imtiaz Khan", "Swaroop  Ghosh"], "year": 2020, "n_citations": 1}
{"id": 350010, "s2_id": "ce80e4895a9a697694cad7c7913639ac8f3f5369", "title": "VESPA: VIPT Enhancements for Superpage Accesses", "abstract": "L1 caches are critical to the performance of modern computer systems. Their design involves a delicate balance between fast lookups, high hit rates, low access energy, and simplicity of implementation. Unfortunately, constraints imposed by virtual memory make it difficult to satisfy all these attributes today. Specifically, the modern staple of supporting virtual-indexing and physical-tagging (VIPT) for parallel TLB-L1 lookups means that L1 caches are usually grown with greater associativity rather than sets. This compromises performance -- by degrading access times without significantly boosting hit rates -- and increases access energy. We propose VIPT Enhancements for SuperPage Accesses or VESPA in response. VESPA side-steps the traditional problems of VIPT by leveraging the increasing ubiquity of superpages; since superpages have more page offset bits, they can accommodate L1 cache organizations with more sets than baseline pages can. VESPA dynamically adapts to any OS distribution of page sizes to operate L1 caches with good access times, hit rates, and energy, for both single- and multi-threaded workloads. Since the hardware changes are modest, and there are no OS or application changes, VESPA is readily-implementable. \nBy superpages (also called huge or large pages) we refer to any page sizes supported by the architecture bigger than baseline page size.", "venue": "ArXiv", "authors": ["Mayank  Parasar", "Abhishek  Bhattacharjee", "Tushar  Krishna"], "year": 2017, "n_citations": 0}
{"id": 351774, "s2_id": "a392cccea7ee32de1f9f2d37a1bf2d954212d674", "title": "Concurrent Processing Memory", "abstract": "A theoretical memory with limited processing power and internal connectivity at each element is proposed. This memory carries out parallel processing within itself to solve generic array problems. The applicability of this in-memory finest-grain massive SIMD approach is studied in some details. For an array of N items, it reduces the total instruction cycle count of universal operations such as insertion/deletion and match finding to ~ 1, local operations such as filtering and template matching to ~ local operation size, and global operations such as sum, finding global limit and sorting to ~\u221aN instruction cycles. It eliminates most streaming activities for data processing purpose on the system bus. Yet it remains general-purposed, easy to use, pin compatible with conventional memory, and practical for implementation. Keyword: SIMD processors; Parallel Processors; Memory Structures; Performance evaluation of algorithms and systems;", "venue": "ArXiv", "authors": ["Chengpu  Wang", "Zhen  Wang"], "year": 2006, "n_citations": 1}
{"id": 352279, "s2_id": "f1cc72d3671aa9c579d03d30026d21123dc37d59", "title": "Sidebar: Scratchpad Based Communication Between CPUs and Accelerators", "abstract": "Hardware accelerators for neural networks have shown great promise for both performance and power. These accelerators are at their most efficient when optimized for a fixed functionality. But this inflexibility limits the longevity of the hardware itself as the underlying neural network algorithms and structures undergo improvements and changes. We propose and evaluate a flexible design paradigm for accelerators with a close coordination with host processors. The relatively static matrix operations are implemented in specialized accelerators while fast-evolving functions, such as activations, are computed on the host processor. This architecture is enabled by a low latency shared buffer we call Sidebar. Sidebar memory is shared between the accelerator and host, exists outside of program address space and holds intermediate data only. We show that a generalised DMA dependent flexible accelerator design performs poorly in both perf and energy as compared to an equivalent fixed function accelerator. Sidebar based accelerator design achieves near identical performance and energy to equivalent fixed function accelerator while still providing all the flexibility of computing activations on the host processor.", "venue": "ArXiv", "authors": ["Ayoosh  Bansal", "Chance  Coats", "Evan  Lissoos", "Benjamin  Schreiber"], "year": 2019, "n_citations": 0}
{"id": 353621, "s2_id": "0aeae2d76a3c3b01f6babc7efc3c75382bc3172e", "title": "A First Look at RISC-V Virtualization from an Embedded Systems Perspective", "abstract": "This article describes the first public implementation and evaluation of the latest version of the RISC-V hypervisor extension (H-extension v0.6.1) specification in a Rocket chip core. To perform a meaningful evaluation for modern multi-core embedded and mixedcriticality systems, we have ported Bao, an open-source static partitioning hypervisor, to RISC-V. We have also extended the RISC-V platformlevel interrupt controller (PLIC) to enable direct guest interrupt injection with low and deterministic latency and we have enhanced the timer infrastructure to avoid trap and emulation overheads. Experiments were carried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the system was also successfully deployed and tested in a Zynq UltraScale+ MPSoC ZCU104. Our hardware implementation was opensourced and is currently in use by the RISC-V community towards the ratification of the H-extension specification.", "venue": "IEEE Transactions on Computers", "authors": ["Bruno  S'a", "Jos'e  Martins", "Sandro  Pinto"], "year": 2021, "n_citations": 0}
{"id": 362786, "s2_id": "0892030d495fbfac905abcef85c29d5b18bfae6f", "title": "GateKeeper: a new hardware architecture for accelerating pre\u2010alignment in DNA short read mapping", "abstract": "Motivation High throughput DNA sequencing (HTS) technologies generate an excessive number of small DNA segments \u2010called short reads\u2010 that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and \u2018candidate\u2019 locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: (i) it is implemented using quadratic\u2010time dynamic programming algorithms and (ii) the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment algorithms. Results We propose GateKeeper, a new hardware accelerator that functions as a pre\u2010alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre\u2010alignment using Field\u2010Programmable Gate Arrays (FPGAs), which can perform pre\u2010alignment much faster than software. When implemented on a single FPGA chip, GateKeeper maintains high accuracy (on average >96%) while providing, on average, 90\u2010fold and 130\u2010fold speedup over the state\u2010of\u2010the\u2010art software pre\u2010alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD), respectively. The addition of GateKeeper as a pre\u2010alignment step can reduce the verification time of the mrFAST mapper by a factor of 10. Availability and implementation https://github.com/BilkentCompGen/GateKeeper Contact mohammedalser@bilkent.edu.tr or onur.mutlu@inf.ethz.ch or calkan@cs.bilkent.edu.tr Supplementary information Supplementary data are available at Bioinformatics online.", "venue": "Bioinform.", "authors": ["Mohammed  Alser", "Hasan  Hassan", "Hongyi  Xin", "Oguz  Ergin", "Onur  Mutlu", "Can  Alkan"], "year": 2017, "n_citations": 63}
{"id": 372241, "s2_id": "5fabdfdc62cbbf5266d4355a09779ccb70c153fd", "title": "SoftWear: Software-Only In-Memory Wear-Leveling for Non-Volatile Main Memory", "abstract": "Several emerging technologies for byte-addressable non-volatile memory (NVM) have been considered to replace DRAM as the main memory in computer systems during the last years. The disadvantage of a lower write endurance, compared to DRAM, of NVM technologies like Phase-Change Memory (PCM) or Ferroelectric RAM (FeRAM) has been addressed in the literature. As a solution, in-memory wear-leveling techniques have been proposed, which aim to balance the wear-level over all memory cells to achieve an increased memory lifetime. Generally, to apply such advanced aging-aware wear-leveling techniques proposed in the literature, additional special hardware is introduced into the memory system to provide the necessary information about the cell age and thus enable aging-aware wear-leveling decisions. \nThis paper proposes software-only aging-aware wear-leveling based on common CPU features and does not rely on any additional hardware support from the memory subsystem. Specifically, we exploit the memory management unit (MMU), performance counters, and interrupts to approximate the memory write counts as an aging indicator. Although the software-only approach may lead to slightly worse wear-leveling, it is applicable on commonly available hardware. We achieve page-level coarse-grained wear-leveling by approximating the current cell age through statistical sampling and performing physical memory remapping through the MMU. This method results in non-uniform memory usage patterns within a memory page. Hence, we further propose a fine-grained wear-leveling in the stack region of C / C++ compiled software. \nBy applying both wear-leveling techniques, we achieve up to $78.43\\%$ of the ideal memory lifetime, which is a lifetime improvement of more than a factor of $900$ compared to the lifetime without any wear-leveling.", "venue": "ArXiv", "authors": ["Christian  Hakert", "Kuan-Hsun  Chen", "Paul R. Genssler", "Georg von der Br\u00fcggen", "Lars  Bauer", "Hussam  Amrouch", "Jian-Jia  Chen", "J\u00f6rg  Henkel"], "year": 2020, "n_citations": 4}
{"id": 376515, "s2_id": "dd8394868c251d2872700199073227d0b8e8bead", "title": "Micro BTB: A High Performance and Lightweight Last-Level Branch Target Buffer for Servers", "abstract": "High-performance branch target buffers (BTBs) and the L1I cache are key to high-performance front-end. Modern branch predictors are highly accurate, but with an increase in code footprint in modern-day server workloads, BTB and L1I misses are still frequent. Recent industry trend shows usage of large BTBs (100s of KB per core) that provide performance closer to the ideal BTB along with a decoupled front-end that provides efficient fetch-directed L1I instruction prefetching. On the other hand, techniques proposed by academia, like BTB prefetching and using retire order stream for learning, fail to provide significant performance with modern-day processor cores that are deeper and wider. We solve the problem fundamentally by increasing the storage density of the last-level BTB. We observe that not all branch instructions require a full branch target address. Instead, we can store the branch target as a branch offset, relative to the branch instruction. Using branch offset enables the BTB to store multiple branches per entry. We reduce the BTB storage in half, but we observe that it increases skewness in the BTB. We propose a skewed indexed and compressed last-level BTB design called MicroBTB (MBTB) that stores multiple branches per BTB entry. We evaluate MBTB on 100 industry-provided server workloads. A 4K-entry MBTB provides 17.61% performance improvement compared to an 8K-entry baseline BTB design with a storage savings of 47.5KB per core.", "venue": "ArXiv", "authors": ["Vishal  Gupta", "Biswabandan  Panda"], "year": 2021, "n_citations": 0}
{"id": 377890, "s2_id": "399ca0405094aa2795e889b3167ce37b7ac830fb", "title": "Hyperdimensional Computing Nanosystem", "abstract": "One viable solution for continuous reduction in energy-per-operation is to rethink functionality to cope with uncertainty by adopting computational approaches that are inherently robust to uncertainty. It requires a novel look at data representations, associated operations, and circuits, and at materials and substrates that enable them. 3D integrated nanotechnologies combined with novel brain-inspired computational paradigms that support fast learning and fault tolerance could lead the way. Recognizing the very size of the brain's circuits, hyperdimensional (HD) computing can model neural activity patterns with points in a HD space, that is, with hypervectors as large randomly generated patterns. At its very core, HD computing is about manipulating and comparing these patterns inside memory. Emerging nanotechnologies such as carbon nanotube field effect transistors (CNFETs) and resistive RAM (RRAM), and their monolithic 3D integration offer opportunities for hardware implementations of HD computing through tight integration of logic and memory, energy-efficient computation, and unique device characteristics. We experimentally demonstrate and characterize an end-to-end HD computing nanosystem built using monolithic 3D integration of CNFETs and RRAM. With our nanosystem, we experimentally demonstrate classification of 21 languages with measured accuracy of up to 98% on >20,000 sentences (6.4 million characters), training using one text sample (~100,000 characters) per language, and resilient operation (98% accuracy) despite 78% hardware errors in HD representation (outputs stuck at 0 or 1). By exploiting the unique properties of the underlying nanotechnologies, we show that HD computing, when implemented with monolithic 3D integration, can be up to 420X more energy-efficient while using 25X less area compared to traditional silicon CMOS implementations.", "venue": "ArXiv", "authors": ["Abbas  Rahimi", "Tony F. Wu", "Haitong  Li", "Jan M. Rabaey", "H.-S. Philip Wong", "Max M. Shulaker", "Subhasish  Mitra"], "year": 2018, "n_citations": 3}
{"id": 379436, "s2_id": "979e5c4f7bca05992adb593f244e85edbbac2432", "title": "Particle Mesh Ewald for Molecular Dynamics in OpenCL on an FPGA Cluster", "abstract": "Molecular Dynamics (MD) simulations play a central role in physics-driven drug discovery. MD applications often use the Particle Mesh Ewald (PME) algorithm to accelerate electrostatic force computations, but efficient parallelization has proven difficult due to the high communication requirements of distributed 3D FFTs. In this paper, we present the design and implementation of a scalable PME algorithm that runs on a cluster of Intel Stratix 10 FPGAs and can handle FFT sizes appropriate to address real-world drug discovery projects (grids up to 1283). To our knowledge, this is the first work to fully integrate all aspects of the PME algorithm (charge spreading, 3D FFT/IFFT, and force interpolation) within a distributed FPGA framework. The design is fully implemented with OpenCL for flexibility and ease of development and uses 100 Gbps links for direct FPGA-to-FPGA communications without the need for host interaction. We present experimental data up to 4 FPGAs (e.g., 206 microseconds per timestep for a 65536 atom simulation and 643 3D FFT), outperforming GPUs. Additionally, we discuss design scalability on clusters with differing topologies up to 64 FPGAs (with expected performance greater than all known GPU implementations) and integration with other hardware components to form a complete molecular dynamics application. We predict best-case performance of 6.6 microseconds per timestep on 64 FPGAs.", "venue": "2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Lawrence C. Stewart", "Carlo  Pascoe", "Emery  Davis", "Brian W. Sherman", "Martin C. Herbordt", "Vipin  Sachdeva"], "year": 2021, "n_citations": 0}
{"id": 384094, "s2_id": "6e4a4ac219ba1b26a5026c08c1f36cd20f102067", "title": "SiliFuzz: Fuzzing CPUs by proxy", "abstract": "CPUs are becoming more complex with every generation, at both the logical and the physical levels. This potentially leads to more logic bugs and electrical defects in CPUs being overlooked during testing, which causes data corruption or other undesirable effects when these CPUs are used in production. These ever-present problems may also have simply become more evident as more CPUs are operated and monitored by large cloud providers. If the RTL (\u201csource code\u201d) of a CPU were available, we could apply greybox fuzzing to the CPU model almost as we do to any other software [1]. However our targets are general purpose x86_64 CPUs produced by third parties, where we do not have the RTL design, so in our case CPU implementations are opaque. Moreover, we are more interested in electrical defects as opposed to logic bugs. We present SiliFuzz, a work-in-progress system that finds CPU defects by fuzzing software proxies, like CPU simulators or disassemblers, and then executing the accumulated test inputs (known as the corpus) on actual CPUs on a large scale. The major difference between this work and traditional software fuzzing is that a software bug fixed once will be fixed for all installations of the software, while for CPU defects we have to test every individual core repeatedly over its lifetime due to wear and tear. In this paper we also analyze four groups of CPU defects that SiliFuzz has uncovered and describe patterns shared by other SiliFuzz findings.", "venue": "ArXiv", "authors": ["Kostya  Serebryany", "Maxim  Lifantsev", "Konstantin  Shtoyk", "Doug  Kwan", "Peter  Hochschild"], "year": 2021, "n_citations": 0}
{"id": 385121, "s2_id": "70b203d0547ed4f843c8785ef3bebf36d2c7b2cc", "title": "Run-time Performance Monitoring of Heterogenous Hw/Sw Platforms Using PAPI", "abstract": "In the era of Cyber Physical Systems, designers need to offer support for run-time adaptivity considering different constraints, including the internal status of the system. This work presents a run-time monitoring approach, based on the Performance Application Programming Interface, that offers a unified interface to transparently access both the standard Performance Monitoring Counters (PMCs) in the CPUs and the custom ones integrated into hardware accelerators. Automatic tools offer to Sw programmers the support to design and implement Coarse-Grain Virtual Reconfigurable Circuits, instrumented with custom PMCs. This approach has been validated on a heterogeneous application for image/video processing with an overhead of 6% of the execution time. 1 Context and Objectives Cyber-Physical Systems (CPS) are complex systems, composed of different components characterized by a strong interaction with environment and users. In particular, they need to adapt their behaviour according to the environment, any user requests and also their internal status [1]. The H2020 CERBERO European Project [2, 3] is developing a continuous design environment for CPS, relying on a set of tools developed by project partners. Effective support for run-time adaptation in heterogeneous systems, taking into account a plethora of different internal and external triggers, is among the CERBERO expected outcomes, and a fundamental step is monitoring the hardware (Hw) and software (Sw) elements of the heterogeneous system [4]. This paper focuses on one fundamental step necessary to design self-adaptive systems: the monitoring of heterogeneous architectures, where processing cores are connected to custom hardware accelerators that can be reconfigured at run-time. One of the Hw reconfigurable infrastructures supported in CERBERO is the Coarse-Grain Virtual Reconfigurable Circuits (CG-VRCs) [5]. CG-VRCs offer fast and low power reconfiguration, with a good trade-off between performance and flexibility, being suitable for providing run-time Hw adaptation. In these kinds of systems, all the resources belonging to all the configurations are instantiated in the substrate and different configurations are enabled by multiplexing resources in time [6], they can be implemented on both Field Programmable Gate Array (FPGA) or Application Specific Integrated Circuit (ASIC) systems. These kinds of accelerators are suited to support: 1. Functional oriented adaptivity: the application is able to execute different functionalities over the same substrate (e.g., algorithm changes) [7]. 2. Non-functional oriented adaptivity: the application is able to execute only one functionality, but with different performance (e.g., the precision of a filter could be reduced to save energy) [8]. In CERBERO, the Multi-Dataflow Composer (MDC) [9] tool automates the development of CG-VRCs. Users describe the applications to be accelerated as dataflows and MDC automatically merges them through a datapath merging algorithm, generating a Xilinx-compliant IP with its drivers to delegate computing tasks to the coprocessor [10]. The first step to enable a feedback loop that allows for the design of self-adaptive CPS, consists of instrumenting the system with monitors to capture its internal status changes [4]. The most extended Sw approach for enabling self-awareness is based on accessing the existing Performance Monitoring Counters (PMCs) of modern CPUs. On the other hand, a Hw accelerator can be specialized by the designer to include custom monitors. This second solution is not suitable for Sw developers who may have limited knowledge of the Hw design flow. Furthermore, if these solutions rely on custom methods to read the monitors, the process of reading the monitors in the Hw accelerators and the PMCs already available on the CPU could not be the same, and heterogeneity of solutions, complex to be implemented, may be required. In CERBERO, PAPIFY [11, 12] provides a lightweight monitoring infrastructure by means of an event library aimed at generalizing the Performance Application Programming Interface (PAPI) [13] for embedded heterogeneous architectures. In a previous work [14] we proposed the idea of using PAPIFY in combination with MDC to offer support for the ar X iv :2 10 3. 01 19 5v 1 [ cs .A R ] 1 M ar 2 02 1 design, implementation and monitoring of run-time reconfigurable systems, as the CG-VRCs, using PAPIFY. In that work we presented a PAPI-compliant component that could be automatically configured with events information using an XML file. The work presented in this paper relies on the idea of offering to Sw developers the support to design and implement run-time reconfigurable systems and to monitor both the processor and the Hw accelerator using a unified methodology based on PAPIFY. Being in a heterogeneous-core computing era, a unified methodology allows a fairer comparison of Hw and Sw performance and facilitates the performance analysis in terms of debugging (e.g., monitor the correct execution of internal modules) and optimization (e.g., monitoring of CG-VRC allows for prospectively switching among different configuration if the users require better performance). \u2022 In this work the MDC tool has been extended to provide automatic instrumentation of the CG-VRCs with custom PMCs and to automatically generate the XML file necessary to automatically configure the previous developed PAPI-component. This automatic flow allows Sw programmers to define the applications to be accelerated and instrumented as dataflow descriptions, without the need of any Hw knowledge. \u2022 The Application Programming Interfaces (APIs) provided by MDC, in combination with the Sw libraries provided by PAPIFY, offer the transparent PAPI-compliant access to the Hw PMCs. \u2022 The monitoring of heterogeneous Hw/Sw systems is a mandatory step to allow self-adaptation of CPS. Nevertheless, in this preliminary exploration the design under test is not a CPS one. Assessment on a processorcoprocessor system for image processing, validates the automatic design flow, the monitoring PAPI-based approach and the effectiveness of PAPIFY on heterogeneous Hw/Sw systems. The paper is organized as follows: Section 2 explores the solutions at the state of the art, Section 3 presents the proposed Hw/Sw unified monitoring approach together with the exploited tools, and Section 4 presents a proof of concept evaluation of the effectiveness of the approach. At the end, Section 5 summarizes and concludes the paper with some directions for future works.", "venue": "ArXiv", "authors": ["Tiziana  Fanni", "Daniel  Madronal", "Claudio  Rubattu", "Carlo  Sau", "Francesca  Palumbo", "Eduardo  Juarez", "Maxime  Pelcat", "Cesar  Sanz", "Luigi  Raffo"], "year": 2021, "n_citations": 3}
{"id": 401163, "s2_id": "c7aa9f5aeba24f778180734f0a9bfa24e9c2cf9d", "title": "Utilizing Reconfigurable Hardware Processors via Grid Services", "abstract": "Computational grids typically consist of nodes utilizing ordinary processors such as the Intel Pentium. Field Programmable Gate Arrays (FPGAs) are able to perform certain compute-intensive tasks very well due to their inherent parallel architecture, often resulting in orders of magnitude speedups. This paper explores how FPGAs can be transparently exposed for remote use via grid services, by integrating the Proteus Software Platform with the Globus Toolkit 3.0.", "venue": "ArXiv", "authors": ["Darran  Nathan", "Ralf  Clemens"], "year": 2004, "n_citations": 2}
{"id": 404555, "s2_id": "ab0afae695025583602944d129bbd71b25e6994a", "title": "Time-Based Roofline for Deep Learning Performance Analysis", "abstract": "Deep learning applications based on neural networks are generating considerable interest in various fields due to their high accuracy. Such an application is usually very compute-intensive thus requires a long run time. Researchers and engineers are actively exploring new solutions to this issue from both hardware and software/algorithm sides. However, little previous work has focused on providing a practical methodology to characterize deep learning performance bottlenecks and potentially guide the following optimization efforts. In this paper, we introduce an extension of the Roofline model and use it to analyze two representative computation kernels in deep learning, 2D convolution and long short-term memory, on NVIDIA GPUs. This new time-based Roofline model incorporates both compute/bandwidth complexity and run time in its formulae to demonstrate performance issues that cannot be reflected by the classic Roofline. Factors such as arithmetic intensity, data transfer, kernel launch overhead, and the Tensor Core usage will be examined by varying different parameters such as batch size and feature size, etc. This work helped form a more systematic way to understand the performance issue of deep learning applications. Last but not least, this generic performance model can be applied to a wide category of applications besides deep learning as well.", "venue": "2020 IEEE/ACM Fourth Workshop on Deep Learning on Supercomputers (DLS)", "authors": ["Yunsong  Wang", "Charlene  Yang", "Steven  Farrell", "Yan  Zhang", "Thorsten  Kurth", "Samuel  Williams"], "year": 2020, "n_citations": 3}
{"id": 413507, "s2_id": "6de492d5d6869504ad3b6d80efbcf618e8760c7d", "title": "MicroGrad: A Centralized Framework for Workload Cloning and Stress Testing", "abstract": "We present MicroGrad, a centralized automated framework that is able to efficiently analyze the capabilities, limits and sensitivities of complex modern processors in the face of constantly evolving application domains. MicroGrad uses Microprobe, a flexible code generation framework as its back-end and a Gradient Descent based tuning mechanism to efficiently enable the evolution of the test cases to suit tasks such as Workload Cloning and Stress Testing. MicroGrad can interface with a variety of execution infrastructure such as performance/power simulators as well as native hardware. Further, the modular \u201cabstract workload model\u201d approach to building MicroGrad allows it to be easily extended for further use - enabling more novel usecases, diverse platforms or robust tuning algorithms. In this paper, we evaluate MicroGrad over different use cases and architectures and showcase that MicroGrad can achieve greater than 95% accuracy across different tasks within few tuning epochs and low resource requirements. We also observe that MicroGrad's accuracy is 25-30% higher than competing techniques. At the same time, MicroGrad is able to reach tuning goals 4-5x faster compared to alternate mechanisms. Overall, MicroGrad's fast, resource efficient and accurate test case generation capability allows it to perform rapid evaluation of complex processors.", "venue": "2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Gokul Subramanian Ravi", "Ramon  Bertran", "Pradip  Bose", "Mikko  Lipasti"], "year": 2021, "n_citations": 0}
{"id": 418906, "s2_id": "c6ae8da189cdf95aa5a98c68b598f388bdeb317e", "title": "Supporting Massive DLRM Inference Through Software Defined Memory", "abstract": "Deep Learning Recommendation Models (DLRM) are widespread, account for a considerable data center footprint, and grow by more than 1.5x per year. With model size soon to be in terabytes range, leveraging Storage Class Memory (SCM) for inference enables lower power consumption and cost. This paper evaluates the major challenges in extending the memory hierarchy to SCM for DLRM, and presents different techniques to improve performance through a Software Defined Memory. We show how underlying technologies such as Nand Flash and 3DXP differentiate, and relate to real world scenarios, enabling from 5% to 29% power savings.", "venue": "ArXiv", "authors": ["Ehsan K. Ardestani", "Changkyu  Kim", "Seung Jae Lee", "Luoshang  Pan", "Valmiki  Rampersad", "Jens  Axboe", "Banit  Agrawal", "Fuxun  Yu", "Ansha  Yu", "Trung  Le", "Hector  Yuen", "Shishir  Juluri", "Akshat  Nanda", "Manoj  Wodekar", "Dheevatsa  Mudigere", "Krishnakumar  Nair", "Maxim  Naumov", "Chris  Peterson", "Mikhail  Smelyanskiy", "Vijay  Rao"], "year": 2021, "n_citations": 0}
{"id": 419207, "s2_id": "a2c885b1fe54e0849002d7728567dcaa9cb84396", "title": "Information Analysis Infrastructure for Diagnosis", "abstract": "A high-speed multiprocessor architecture for brain-like analyzing information represented in analytic, graph- and table forms of associative relations to search, recognize and make a decision in n-dimensional vector discrete space is offered. Vector-logical process models of actual applications, where the quality of solution is estimated by the proposed integral non-arithmetical metric of the interaction between binary vectors, are described. The theoretical proof of the metric for a vector logical space and the quality criteria for estimating solutions is created.", "venue": "ArXiv", "authors": ["Vladimir  Hahanov", "Wajeb  Gharibi", "Eugenia  Litvinova", "Svetlana  Chumachenko"], "year": 2012, "n_citations": 19}
{"id": 421313, "s2_id": "a054997d71345cd408d286a133453d055a68740d", "title": "Power optimized programmable embedded controller", "abstract": "Now a days, power has become a primary consideration in hardware design, and is critical in computer systems especially for portable devices with high performance and more functionality. Clock-gating is the most common technique used for reducing processor\u2019s power. In this work clock gating technique is applied to optimize the power of fully programmable Embedded Controller (PEC) employing RISC architecture. The CPU designed supports i) smart instruction set, ii) I/O port, UART iii) on-chip clocking to provide a range of frequencies , iv) RISC as well as controller concepts. The whole design is captured using VHDL and is implemented on FPGA chip using Xilinx .The architecture and clock gating technique together is found to reduce the power consumption by 33.33% of total power consumed by this chip.", "venue": "ArXiv", "authors": ["M.  Kamaraju", "K. Lal Kishore", "A. V. N. Tilak"], "year": 2010, "n_citations": 12}
{"id": 421886, "s2_id": "3753b38f9bb9f627096f6906d50b4beb1e1ec2a7", "title": "Tardis 2.0: Optimized time traveling coherence for relaxed consistency models", "abstract": "Cache coherence scalability is a big challenge in shared memory systems. Traditional protocols do not scale due to the storage and traffic overhead of cache invalidation. Tardis, a recently proposed coherence protocol, removes cache invalidation using logical timestamps and achieves excellent scalability. The original Tardis protocol, however, only supports the Sequential Consistency (SC) memory model, limiting its applicability. Tardis also incurs extra network traffic on some benchmarks due to renew messages, and has suboptimal performance when the program uses spinning to communicate between threads. In this paper, we address these downsides of Tardis protocol and make it significantly more practical. Specifically, we discuss the architectural, memory system and protocol changes required in order to implement the TSO consistency model on Tardis, and prove that the modified protocol satisfies TSO. We also describe modifications for Partial Store Order (PSO) and Release Consistency (RC). Finally, we propose optimizations for better leasing policies and to handle program spinning. On a set of benchmarks, optimized Tardis improves on a full-map directory protocol in the metrics of performance, storage and network traffic, while being simpler to implement.", "venue": "2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)", "authors": ["Xiangyao  Yu", "Hongzhe  Liu", "Ethan  Zou", "Srinivas  Devadas"], "year": 2016, "n_citations": 7}
{"id": 425995, "s2_id": "ed86adc77a70bc65465e2d370a4df70d85f0afc2", "title": "Benchmarking a New Paradigm: An Experimental Analysis of a Real Processing-in-Memory Architecture", "abstract": "Many modern workloads, such as neural networks, databases, and graph processing, are fundamentally memory-bound. For such workloads, the data movement between main memory and CPU cores imposes a significant overhead in terms of both latency and energy. Amajor reason is that this communication happens through a narrow bus with high latency and limited bandwidth, and the low data reuse in memory-bound workloads is insufficient to amortize the cost of main memory access. Fundamentally addressing this data movement bottleneck requires a paradigm where the memory system assumes an active role in computing by integrating processing capabilities. This paradigm is known as processing-in-memory (PIM). Recent research explores different forms of PIM architectures, motivated by the emergence of new 3D-stacked memory technologies that integrate memory with a logic layer where processing elements can be easily placed. Past works evaluate these architectures in simulation or, at best, with simplified hardware prototypes. In contrast, the UPMEM company has designed and manufactured the first publicly-available real-world PIM architecture. The UPMEM PIM architecture combines traditional DRAMmemory arrays with general-purpose in-order cores, called DRAM Processing Units (DPUs), integrated in the same chip. This paper provides the first comprehensive analysis of the first publicly-available real-world PIM architecture. We make two key contributions. First, we conduct an experimental characterization of the UPMEM-based PIM system using microbenchmarks to assess various architecture limits such as compute throughput and memory bandwidth, yielding new insights. Second, we present PrIM (Processing-In-Memory benchmarks), a benchmark suite of 16 workloads from different application domains (e.g., dense/sparse linear algebra, databases, data analytics, graph processing, neural networks, bioinformatics, image processing), which we identify as memory-bound. We evaluate the performance and scaling characteristics of PrIM benchmarks on the UPMEM PIM architecture, and compare their performance and energy consumption to their state-of-the-art CPU and GPU counterparts. Our extensive evaluation conducted on two real UPMEM-based PIM systems with 640 and 2,556 DPUs provides new insights about suitability of different workloads to the PIM system, programming recommendations for software designers, and suggestions and hints for hardware and architecture designers of future PIM systems.", "venue": "ArXiv", "authors": ["Juan  G'omez-Luna", "Izzat El Hajj", "Ivan  Fernandez", "Christina  Giannoula", "Geraldo F. Oliveira", "Onur  Mutlu"], "year": 2021, "n_citations": 8}
{"id": 432382, "s2_id": "ea3d94337982c94a93fcc3dd3235a6c02a4221e1", "title": "Spin-Orbit-Torque-based Devices, Circuits and Architectures", "abstract": "Spintronics, the use of spin of an electron instead of its charge, has received huge attention from research communities for different applications including memory, interconnects, logic implementation, neuromorphic computing, and many other applications. Here, in this paper, we review the works within spintronics, more specifically on spin-orbit torque (SOT) within different research groups. We also provide researchers an insight into the future potentials of the SOT-based designs. This comprehensive review paper covers different aspects of SOT-based design from device and circuit to architecture level as well as more ambitious and futuristic applications of such technology.", "venue": "ArXiv", "authors": ["Farshad  Moradi", "Hooman  Farkhani", "Behzad  Zeinali", "Hamdam  Ghanatian", "Johan Michel Alain Pelloux-Prayer", "Tim  Boehnert", "Mohammad  Zahedinejad", "Hadi  Heidari", "Vahid  Nabaei", "Ricardo  Ferreira", "Johan  Akerman", "Jens Kargaard Madsen"], "year": 2019, "n_citations": 2}
{"id": 435443, "s2_id": "8aa8ecccf3aa0f1509ca76ccca11befe3e673fc0", "title": "A Modern Primer on Processing in Memory", "abstract": "Modern computing systems are overwhelmingly designed to move data to computation. This design choice goes directly against at least three key trends in computing that cause performance, scalability and energy bottlenecks: (1) data access is a key bottleneck as many important applications are increasingly data-intensive, and memory bandwidth and energy do not scale well, (2) energy consumption is a key limiter in almost all computing platforms, especially server and mobile systems, (3) data movement, especially off-chip to on-chip, is very expensive in terms of bandwidth, energy and latency, much more so than computation. These trends are especially severely-felt in the data-intensive server and energy-constrained mobile systems of today. At the same time, conventional memory technology is facing many technology scaling challenges in terms of reliability, energy, and performance. As a result, memory system architects are open to organizing memory in different ways and making it more intelligent, at the expense of higher cost. The emergence of 3D-stacked memory plus logic, the adoption of error correcting codes inside the latest DRAM chips, proliferation of different main memory standards and chips, specialized for different purposes (e.g., graphics, low-power, high bandwidth, low latency), and the necessity of designing new solutions to serious reliability and security issues, such as the RowHammer phenomenon, are an evidence of this trend. This chapter discusses recent research that aims to practically enable computation close to data, an approach we call processing-in-memory (PIM). PIM places computation mechanisms in or near where the data is stored (i.e., inside the memory chips, in the logic layer of 3D-stacked memory, or in the memory controllers), so that data movement between the computation units and memory is reduced or eliminated.", "venue": "ArXiv", "authors": ["Onur  Mutlu", "Saugata  Ghose", "Juan  G'omez-Luna", "Rachata  Ausavarungnirun"], "year": 2020, "n_citations": 21}
{"id": 435761, "s2_id": "d1f67266e90d8bb37d0f07764457cc4d3dd163fb", "title": "Characterization and Mitigation of Electromigration Effects in TSV-Based Power Delivery Network Enabled 3D-Stacked DRAMs", "abstract": "With 3D-stacked DRAM architectures becoming more prevalent, it has become important to find ways to characterize and mitigate the adverse effects that can hinder their inherent access parallelism and throughput. One example of such adversities is the electromigration (EM) effects in the through-silicon vias (TSVs) of the power delivery network (PDN) of 3D-stacked DRAM architectures. Several prior works have addressed the effects of EM in TSVs of 3D integrated circuits. However, no prior work has addressed the effects of EM in the PDN TSVs on the performance and lifetime of 3D-stacked DRAMs. In this paper, we characterize the effects of EM in PDN TSVs on a Hybrid Memory Cube (HMC) architecture employing the conventional PDN design with clustered layout of power and ground TSVs. We then present a new PDN design with a distributed layout of power and ground TSVs and show that it can mitigate the adverse effects of EM on the HMC architecture performance without requiring additional power and ground pins. Our benchmark-driven simulation-based analysis shows that compared to the clustered PDN layout, our proposed distributed PDN layout improves the EM-affected lifetime of the HMC architecture by up to 10 years. During this useful lifetime, the HMC architecture also yields up to 1.10\u00d7 less average latency, 1.43\u00d7 more throughput, and 1.51\u00d7 less energy-delay product (EDP).", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Bobby  Bose", "Ishan G. Thakkar"], "year": 2021, "n_citations": 0}
{"id": 436423, "s2_id": "b00b016683929eb03f5a22b93b069b7048cd452e", "title": "VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing", "abstract": "The hardware implementation of deep neural networks (DNNs) has recently received tremendous attention: many applications in fact require high-speed operations that suit a hardware implementation. However, numerous elements and complex interconnections are usually required, leading to a large area occupation and copious power consumption. Stochastic computing (SC) has shown promising results for low-power area-efficient hardware implementations, even though existing stochastic algorithms require long streams that cause long latencies. In this paper, we propose an integer form of stochastic computation and introduce some elementary circuits. We then propose an efficient implementation of a DNN based on integral SC. The proposed architecture has been implemented on a Virtex7 field-programmable gate array, resulting in 45% and 62% average reductions in area and latency compared with the best reported architecture in the literature. We also synthesize the circuits in a 65-nm CMOS technology, and we show that the proposed integral stochastic architecture results in up to 21% reduction in energy consumption compared with the binary radix implementation at the same misclassification rate. Due to fault-tolerant nature of stochastic architectures, we also consider a quasi-synchronous implementation that yields 33% reduction in energy consumption with respect to the binary radix implementation without any compromise on performance.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Arash  Ardakani", "Fran\u00e7ois  Leduc-Primeau", "Naoya  Onizawa", "Takahiro  Hanyu", "Warren J. Gross"], "year": 2017, "n_citations": 101}
{"id": 440467, "s2_id": "7083ded2ff628f9c4b6f1adfea5ea2b7b790e098", "title": "GrateTile: Efficient Sparse Tensor Tiling for CNN Processing", "abstract": "We propose GrateTile, an efficient, hardware-friendly data storage scheme for sparse CNN feature maps (activations). It divides data into uneven-sized subtensors and, with small indexing overhead, stores them in a compressed yet randomly accessible format. This design enables modern CNN accelerators to fetch and decompressed sub-tensors on-the-fly in a tiled processing manner. GrateTile is suitable for architectures that favor aligned, coalesced data access, and only requires minimal changes to the overall architectural design. We simulate GrateTile with state-of-the-art CNNs and show an average of 55% DRAM bandwidth reduction while using only 0.6% of feature map size for indexing storage.", "venue": "2020 IEEE Workshop on Signal Processing Systems (SiPS)", "authors": ["Yu-Sheng  Lin", "Hung Chang Lu", "Yang-Bin  Tsao", "Yi-Min  Chih", "Wei-Chao  Chen", "Shao-Yi  Chien"], "year": 2020, "n_citations": 2}
{"id": 444094, "s2_id": "ea6724ac521f4773e07a5974340028482d149805", "title": "Evolutionary optimization in code-based test compression", "abstract": "Test data compression has become increasingly popular for distributing test complexity between automatic test equipment and on-chip structures. We provide a general formulation for the code-based test compression problem with fixed-length input blocks and propose a solution approach based on evolutionary algorithms. In contrast to existing code-based methods, we allow unspecified values in matching vectors, which allows encoding of arbitrary test sets using a relatively small number of codewords. Experimental results for both stuck-at and path delay fault test sets for ISCAS circuits demonstrate an improvement compared to existing techniques.", "venue": "Design, Automation and Test in Europe", "authors": ["Ilia  Polian", "Alexander  Czutro", "Bernd  Becker"], "year": 2005, "n_citations": 5}
{"id": 449980, "s2_id": "acf0ab723b2a582b48985bd1316dd19b3e3f8b2a", "title": "Data-Driven Offline Optimization For Architecting Hardware Accelerators", "abstract": "Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a \u201csimulation-driven\u201d approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a \u201cdata-driven\u201d, offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators\u2014tailored towards both single and multiple applications\u2014improving performance upon state-of-theart simulation-driven methods by about 1.54\u00d7 and 1.20\u00d7, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26\u00d7.", "venue": "ArXiv", "authors": ["Aviral  Kumar", "Amir  Yazdanbakhsh", "Milad  Hashemi", "Kevin  Swersky", "Sergey  Levine"], "year": 2021, "n_citations": 0}
{"id": 450889, "s2_id": "9913d9a0eb57cc1282e7f5f688dc7b2b590aa831", "title": "Design and implementation of real time AES-128 on real time operating system for multiple FPGA communication", "abstract": "Security is the most important part in data communication system, where more randomization in secret keys increases the security as well as complexity of the cryptography algorithms. As a result in recent dates these algorithms are compensating with enormous memory spaces and large execution time on hardware platform. Field programmable gate arrays (FPGAs), provide one of the major alternative in hardware platform scenario due to its reconfiguration nature, low price and marketing speed. In FPGA based embedded system we can use embedded processor to execute particular algorithm with the inclusion of a real time operating System (RTOS), where threads may reduce resource utilization and time consumption. A process in the runtime is separated in different smaller tasks which are executed by the scheduler to meet the real time dead line using RTOS. In this paper we demonstrate the design and implementation of a 128-bit Advanced Encryption Standard (AES) both symmetric key encryption and decryption algorithm by developing suitable hardware and software design on Xilinx Spartan- 3E (XC3S500E-FG320) device using an Xilkernel RTOS, the implementation has been tested successfully The system is optimized in terms of execution speed and hardware utilization.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Sangeet  Saha", "Suman  Sau", "Amlan  Chakrabarti"], "year": 2012, "n_citations": 5}
{"id": 453656, "s2_id": "306becf9f7e6e55213fe6ccd3a67bf5fe23d27d3", "title": "NV-Fogstore : Device-aware hybrid caching in fog computing environments", "abstract": "Edge caching via the placement of distributed storages throughout the network is a promising solution to reduce latency and network costs of content delivery. With the advent of the upcoming 5G future, billions of F-RAN (Fog-Radio Access Network) nodes will created and used for for the purpose of Edge Caching. Hence, the total amount of memory deployed at the edge is expected to increase 100 times. \nCurrently, used DRAM-based caches in CDN (Content Delivery Networks) are extremely power-hungry and costly. Our purpose is to reduce the cost of ownership and recurring costs (of power consumption) in an F-RAN node while maintaining Quality of Service. \nFor our purpose, we propose NV-FogStore, a scalable hybrid key-value storage architecture for the utilization of Non-Volatile Memories (such as RRAM, MRAM, Intel Optane) in Edge Cache. \nWe further describe in detail a novel, hierarchical, write-damage, size and frequency aware content caching policy H-GREEDY for our architecture. \nWe show that our policy can be tuned as per performance objectives, to lower the power, energy consumption and total cost over an only DRAM-based system for only a relatively smaller trade-off in the average access latency.", "venue": "ArXiv", "authors": ["Khushal  Sethi", "Manan  Suri"], "year": 2020, "n_citations": 0}
{"id": 456496, "s2_id": "5f87509eafe7fdfdcf4afe94a8d791265f13ac99", "title": "Conceptual Modeling for Computer Organization and Architecture", "abstract": "Email: sabah.alfedaghi@ku.edu.kw Abstract: Understanding computer system hardware, including how computers operate, is essential for undergraduate students in computer engineering and science. Literature shows students learning computer organization and assembly language often find fundamental concepts difficult to comprehend within the topic materials. Tools have been introduced to improve students\u2019 comprehension of the interaction between computer architecture, assembly language, and the operating system. One such tool is the Little Man Computer (LMC) model that operates in a way similar to a computer but that is easier to understand. Even though LMC does not have modern CPUs with multiple cores nor executes multiple instructions, it nevertheless shows the basic principles of the von Neumann architecture. LMC aims to introduce students to such concepts as code and instruction sets. In this paper, LMC is used for an additional purpose: a tool with which to experiment using a new modeling language (i.e., a thinging machine; TM) in the area of computer organization and architecture without involving complexity in the subject. That is, the simplicity of LMC facilitates the application of TM without going deep into computer organization/architecture materials. Accordingly, the paper (a) provides a new way for using the LMC model for whatever purpose (e.g., education) and (b) demonstrates that TM can be used to build an abstract level of description in the organization/architect field. The resultant schematics from the TM model of LMC offer an initial case study that supports our thesis that TM is a viable method for hardware/software-independent descriptions in the computer organization and architect field of study.", "venue": "ArXiv", "authors": ["Sabah  Al-Fedaghi"], "year": 2021, "n_citations": 0}
{"id": 457188, "s2_id": "5700bc5875066a01a1dd6e872e13a9f04e93cd9d", "title": "A 9.96 dB NCG FEC scheme and 164 bits/cycle low-complexity product decoder architecture", "abstract": "Powerful Forward Error Correction (FEC) schemes are used in optical communications to achieve bit-error rates below 10\u221215. These FECs follow one of two approaches: concatenation of simpler hard-decision codes or usage of inherently powerful soft-decision codes. The first approach yields lower Net Coding Gains (NCGs), but can usually work at higher code rates and have lower complexity decoders. In this work, we propose a novel FEC scheme based on a product code and a post-processing technique. It can achieve an NCG of 9.52 dB at a BER of 10\u221215 and 9.96 dB at a BER of 10\u221218, an error-correction performance that sits between that of current hard-decision and soft-decision FECs. A decoder architecture is designed, tested on FPGA and synthesized in 65 nm CMOS technology: its 164 bits/cycle worstcase information throughput can reach 100 Gb/s at the achieved frequency of 609 MHz. Its complexity is shown to be lower than that of hard-decision decoders in literature, and an order of magnitude lower than the estimated complexity of soft-decision decoders.", "venue": "ArXiv", "authors": ["Carlo  Condo", "Pascal  Giard", "Fran\u00e7ois  Leduc-Primeau", "Gabi  Sarkis", "Warren J. Gross"], "year": 2016, "n_citations": 2}
{"id": 459678, "s2_id": "b715fa5f7c71f62e62399ecba56fc2ab7688ea9f", "title": "MgX: Near-Zero Overhead Memory Protection with an Application to Secure DNN Acceleration", "abstract": "In this paper, we propose MgX, a near-zero overhead memory protection scheme for hardware accelerators. MgX minimizes the performance overhead of off-chip memory encryption and integrity verification by exploiting the application-specific aspect of accelerators. Accelerators tend to explicitly manage data movement between on-chip and off-chip memory, typically at an object granularity that is much larger than cache lines. Exploiting these accelerator-specific characteristics, MgX generates version numbers used in memory encryption and integrity verification only using on-chip state without storing them in memory, and also customizes the granularity of the memory protection to match the granularity used by the accelerator. To demonstrate the applicability of MgX, we present an in-depth study of MgX for deep neural network (DNN) and also describe implementations for H.264 video decoding and genome alignment. Experimental results show that applying MgX has less than 1% performance overhead for both DNN inference and training on state-of-the-art DNN architectures.", "venue": "ArXiv", "authors": ["Weizhe  Hua", "Muhammad  Umar", "Zhiru  Zhang", "G. Edward Suh"], "year": 2020, "n_citations": 2}
{"id": 465193, "s2_id": "78076c6b38ec0edf2bf3679e8a549bc66df44590", "title": "Throughput Optimizations for FPGA-based Deep Neural Network Inference", "abstract": "Abstract Deep neural networks are an extremely successful and widely used technique for various pattern recognition and machine learning tasks. Due to power and resource constraints, these computationally intensive networks are difficult to implement in embedded systems. Yet, the number of applications that can benefit from the mentioned possibilities is rapidly rising. In this paper, we propose novel architectures for the inference of previously learned and arbitrary deep neural networks on FPGA-based SoCs that are able to overcome these limitations. Our key contributions include the reuse of previously transferred weight matrices across multiple input samples, which we refer to as batch processing, and the usage of compressed weight matrices, also known as pruning. An extensive evaluation of these optimizations is presented. Both techniques allow a significant mitigation of data transfers and speed-up the network inference by one order of magnitude. At the same time, we surpass the data throughput of fully-featured x86-based systems while only using a fraction of their energy consumption.", "venue": "Microprocess. Microsystems", "authors": ["Thorbj\u00f6rn  Posewsky", "Daniel  Ziener"], "year": 2018, "n_citations": 17}
{"id": 468624, "s2_id": "63223b1f9ba5f6a90c7f4aeec969dc0ebc2fb456", "title": "MT-lib: A Topology-aware Message Transfer Library for Graph500 on Supercomputers", "abstract": "We present MT-lib, an efficient message transfer library for messages gather and scatter in benchmarks like Graph500 for Supercomputers. Our library includes MST version as well as new-MST version. The MT-lib is deliberately kept light-weight, efficient and friendly interfaces for massive graph traverse. MST provides (1) a novel non-blocking communication scheme with sending and receiving messages asynchronously to overlap calculation and communication;(2) merging messages according to the target process for reducing communication overhead;(3) a new communication mode of gathering intra-group messages before forwarding between groups for reducing communication traffic. In MT-lib, there are (1) one-sided message; (2) two-sided messages; and (3) two-sided messages with buffer, in which dynamic buffer expansion is built for messages delivery. We experimented with MST and then testing Graph500 with MST on Tianhe supercomputers. Experimental results show high communication efficiency and high throughputs for both BFS and SSSP communication operations.", "venue": "ArXiv", "authors": ["Xinbiao  Gan", "Wen  Tan"], "year": 2021, "n_citations": 0}
{"id": 468758, "s2_id": "82f96328a1fb7e0feaf52688d7c15821838c2db6", "title": "Effective and Fast: A Novel Sequential Single Path Search for Mixed-Precision Quantization", "abstract": "Since model quantization helps to reduce the model size and computation latency, it has been successfully applied in many applications of mobile phones, embedded devices and smart chips. The mixed-precision quantization model can match different quantization bit-precisions according to the sensitivity of different layers to achieve great performance. However, it is a difficult problem to quickly determine the quantization bit-precision of each layer in deep neural networks according to some constraints (e.g., hardware resources, energy consumption, model size and computation latency). To address this issue, we propose a novel sequential single path search (SSPS) method for mixed-precision quantization, in which the given constraints are introduced into its loss function to guide searching process. A single path search cell is used to combine a fully differentiable supernet, which can be optimized by gradient-based algorithms. Moreover, we sequentially determine the candidate precisions according to the selection certainties to exponentially reduce the search space and speed up the convergence of searching process. Experiments show that our method can efficiently search the mixed-precision models for different architectures (e.g., ResNet-20, 18, 34, 50 and MobileNet-V2) and datasets (e.g., CIFAR-10, ImageNet and COCO) under given constraints, and our experimental results verify that SSPS significantly outperforms their uniform counterparts.", "venue": "ArXiv", "authors": ["Qigong  Sun", "Licheng  Jiao", "Yan  Ren", "Xiufang  Li", "Fanhua  Shang", "Fang  Liu"], "year": 2021, "n_citations": 4}
{"id": 469574, "s2_id": "70b6aa9fff5d2e2a99207dd2aba90c5059dd14ac", "title": "Design of Synchronous Section-Carry Based Carry Lookahead Adders with Improved Figure of Merit", "abstract": "The section-carry based carry lookahead adder (SCBCLA) architecture was proposed as an efficient alternative to the conventional carry lookahead adder (CCLA) architecture for the physical implementation of computer arithmetic. In previous related works, self-timed SCBCLA architectures and synchronous SCBCLA architectures were realized using standard cells and FPGAs. In this work, we deal with improved realizations of synchronous SCBCLA architectures designed in a semi-custom fashion using standard cells. The improvement is quantified in terms of a figure of merit (FOM), where the FOM is defined as the inverse product of power, delay and area. Since power, delay and area of digital designs are desirable to be minimized, the FOM is desirable to be maximized. Starting from an efficient conventional carry lookahead generator, we show how an optimized section-carry based carry lookahead generator is realized. In comparison with our recent work dealing with standard cells based implementation of SCBCLAs to perform 32-bit addition of two binary operands, we show in this work that with improved section-carry based carry lookahead generators, the resulting SCBCLAs exhibit significant improvements in FOM. Compared to the earlier optimized hybrid SCBCLA, the proposed optimized hybrid SCBCLA improves the FOM by 88.3%. Even the optimized hybrid CCLA features improvement in FOM by 77.3% over the earlier optimized hybrid CCLA. However, the proposed optimized hybrid SCBCLA is still the winner and has a better FOM than the currently optimized hybrid CCLA by 15.3%. All the CCLAs and SCBCLAs are implemented to realize 32-bit dual-operand binary addition using a 32/28nm CMOS process.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 5}
{"id": 472238, "s2_id": "0eb5134093c2745de9060e2f2539c740cf9240b5", "title": "Analytical Modeling the Multi-Core Shared Cache Behavior With Considerations of Data-Sharing and Coherence", "abstract": "To mitigate the ever worsening \u201cPower wall\u201d and \u201cMemory wall\u201d problems, multi-core architectures with multi-level cache hierarchies have been widely accepted in modern processors. However, the complexity of the architectures makes modeling of shared caches extremely complex. In this article, we propose a data-sharing aware analytical model for estimating the miss rates of the downstream shared cache under multi-core scenarios. To avoid time-consuming full simulations of the cache architecture required by conventional approaches, the proposed model can also be integrated with our refined upstream cache analytical model, which also evaluates coherence misses with similar accuracies of state-of-the-art approach with only one tenth time overhead. We validate our analytical model against gem5 simulation results under 13 applications from PARSEC 2.1 benchmark suites. Compared to the results from gem5 simulations under 8 hardware configurations including dual-core and quad-core architectures, the average absolute error of the predicted shared L2 cache miss rates is less than 2% for all configurations. After integrated with the refined upstream model with coherence misses, the overall average absolute error in 4 hardware configurations is degraded to 4.82% due to the error accumulations. As an application case of the integrated model, we also evaluate the miss rates of 57 different multi-core and multi-level cache configurations.", "venue": "IEEE Access", "authors": ["Ming  Ling", "Xiaoqian  Lu", "Guangmin  Wang", "Jiancong  Ge"], "year": 2021, "n_citations": 0}
{"id": 472343, "s2_id": "e3f4a2bd3040a1e47319180fce0ea36dceec0dee", "title": "High-Performance Parallel Implementation of Genetic Algorithm on FPGA", "abstract": "Genetic algorithms (GAs) are used to solve search and optimization problems in which an optimal solution can be found using an iterative process with probabilistic and non-deterministic transitions. However, depending on the problem\u2019s nature, the time required to find a solution can be high in sequential machines due to the computational complexity of genetic algorithms. This work proposes a full-parallel implementation of a genetic algorithm on field-programmable gate array (FPGA). Optimization of the system\u2019s processing time is the main goal of this project. Results associated with the processing time and area occupancy (on FPGA) for various population sizes are analyzed. Studies concerning the accuracy of the GA response for the optimization of two variables functions were also evaluated for the hardware implementation. However, the high-performance implementation proposed in this paper is able to work with more variable from some adjustments on hardware architecture. The results showed that the GA full-parallel implementation achieved throughput about 16 millions of generations per second and speedups between 17 and 170,000 associated with several works proposed in the literature.", "venue": "Circuits Syst. Signal Process.", "authors": ["Matheus F. Torquato", "Marcelo A. C. Fernandes"], "year": 2019, "n_citations": 20}
{"id": 473424, "s2_id": "9b1e37fac354f799b67f9a932cedeecdb99b09a9", "title": "Design and ASIC implementation of DUC/DDC for communication systems", "abstract": "Communication systems use the concept of transmitting information using the electrical distribution network as a communication channel. To enable the transmission data signal modulated on a carrier signal is superimposed on the electrical wires. Typical power lines are designed to handle 50/60 Hz of AC power signal; however they can carry the signals up to 500 KHz frequency. This work aims to aid transmission/reception of an audio signal in the spectrum from 300 Hz to 4000 Hz using PLCC on a tunable carrier frequency in the spectrum from 200 KHz to 500 KHz. For digital amplitude modulation the sampling rate of the carrier and the audio signal has to be matched. Tunable carrier generation can be achieved with Direct Digital Synthesizers at a desired sampling rate. DSP Sample rate conversion techniques are very useful to make the sampling circuits to work on their own sampling rates which are fine for the data/modulated-carrier signal's bandwidth. This also simplifies the complexity of the sampling circuits. Digital Up Conversion (DUC) and Digital Down Conversion (DDC) are DSP sample rate conversion techniques which refer to increasing and decreasing the sampling rate of a signal respectively.", "venue": "VLSIC 2011", "authors": ["Naagesh S. Bhat"], "year": 2011, "n_citations": 2}
{"id": 479614, "s2_id": "4ff9d40a76adb7b889600f8045e3233195023933", "title": "Efficient Edge Detection on Low-Cost FPGAs", "abstract": "Improving the efficiency of edge detection in embedded applications, such as UAV control, is critical for reducing system cost and power dissipation. Field programmable gate arrays (FPGA) are a good platform for making improvements because of their specialised internal structure. However, current FPGA edge detectors do not exploit this structure well. A new edge detection architecture is proposed that is better optimised for FPGAs. The basis of the architecture is the Sobel edge kernels that are shown to be the most suitable because of their separability and absence of multiplications. Edge intensities are calculated with a new 4:2 compressor that consists of two custom-designed 3:2 compressors. Addition speed is increased by breaking carry propagation chains with look-ahead logic. Testing of the design showed it gives a 28% increase in speed and 4.4% reduction in area over previous equivalent designs, which demonstrated that it will lower the cost of edge detection systems, dissipate less power and still maintain high-speed control.", "venue": "ArXiv", "authors": ["Jamie  Schiel", "Andrew  Bainbridge-Smith"], "year": 2015, "n_citations": 3}
{"id": 484717, "s2_id": "49200e6265c17cf5025334fea8eea547e95430f2", "title": "NoCs in Heterogeneous 3D SoCs: Co-Design of Routing Strategies and Microarchitectures", "abstract": "Heterogeneous 3D System-on-Chips (3D SoCs) are the most promising design paradigm to combine sensing and computing within a single chip. A special characteristic of communication networks in heterogeneous 3D SoCs is the varying latency and throughput in each layer. As shown in this work, this variance drastically degrades the network performance. We contribute a co-design of routing algorithms and router microarchitecture that allows to overcome these performance limitations. We analyze the challenges of heterogeneity: Technology-aware models are proposed for communication and thereby identify layers in which packets are transmitted slower. The communication models are precise for latency and throughput under zero load. The technology model has an area error and a timing error of less than 7.4% for various commercial technologies from 90 to 28nm. Second, we demonstrate how to overcome limitations of heterogeneity by proposing two novel routing algorithms called Z<sup>+</sup>(XY)Z<sup>\u2212</sup> and ZXYZ that enhance latency by up to <inline-formula> <tex-math notation=\"LaTeX\">$6.5\\times $ </tex-math></inline-formula> compared to conventional dimension order routing. Furthermore, we propose a high vertical-throughput router microarchitecture that is adjusted to the routing algorithms and that fully overcomes the limitations of slower layers. We achieve an increased throughput of 2 to <inline-formula> <tex-math notation=\"LaTeX\">$4\\times $ </tex-math></inline-formula> compared to a conventional router. Thereby, the dynamic power of routers is reduced by up to 41.1% and we achieve improved flit latency of up to <inline-formula> <tex-math notation=\"LaTeX\">$2.26\\times $ </tex-math></inline-formula> at small total router area costs between 2.1% and 10.4% for realistic technologies and application scenarios.", "venue": "IEEE Access", "authors": ["Jan Moritz Joseph", "Lennart  Bamberg", "Dominik  Ermel", "Behnam Razi Perjikolaei", "Anna  Drewes", "Alberto  Garc\u00eda-Ortiz", "Thilo  Pionteck"], "year": 2019, "n_citations": 5}
{"id": 489281, "s2_id": "151c3c3c9759cbd08d53955272b05730c8926d9f", "title": "A programmable clock generator for automatic Quality Assurance of LOCx2", "abstract": "The upgrade of ATLAS Liquid Argon Calorimeter (LAr) Phase-1 trigger requires high-speed, low-latency data transmission to read out the Lar Trigger Digitizer Board (LTDB). A dual-channel transmitter ASIC LOCx2 have been designed and produced. In order to ensure all the LOCx2 chips behave properly, a Quality Assurance needs to be conducted before assembly. The problem I was trying to solve in this project is to yield a clock signal with continuously adjustable frequency and phase offset to generate and control an eye diagram for the QA. By configuring the registers of an any-frequency generator IC, Si5338, the clock signal whose frequency range from 5MHz to 200 MHz have been properly produced. For the purpose of further development, a C-language based DLL which packs up the function of adjusting frequency and setting phase offset was designed and built, and several evaluation was performed to ensure the robustness of DLL.", "venue": "ArXiv", "authors": ["Zhi-yue  Wang", "Tian-kuan  Liu", "Qi-jie  Tang", "Yi  Feng", "Jian  Wang"], "year": 2018, "n_citations": 0}
{"id": 489313, "s2_id": "0d863a9eedef627f6318da228ebc18ee3f1b5cd0", "title": "Optimizing Write Fidelity of MRAMs via Iterative Water-filling Algorithm", "abstract": "Magnetic random-access memory (MRAM) is a promising memory technology due to its high density, non-volatility, and high endurance. However, achieving high memory fidelity incurs significant write-energy costs, which should be reduced for large-scale deployment of MRAMs. In this paper, we formulate a biconvex optimization problem to optimize write fidelity given energy and latency constraints. The basic idea is to allocate non-uniform write pulses depending on the importance of each bit position. The fidelity measure we consider is mean squared error (MSE), for which we optimize write pulses via alternating convex search (ACS). By using Karush-Kuhn-Tucker (KKT) conditions, we derive analytic solutions and propose an iterative water-filling-type algorithm by leveraging the analytic solutions. Hence, the proposed iterative water-filling algorithm is computationally more efficient than the original ACS while their solutions are identical. Although the original ACS and the proposed iterative water-filling algorithm do not guarantee global optimality, the MSEs obtained by the proposed algorithm are comparable to the MSEs by complicated global nonlinear programming solvers. Furthermore, we prove that the proposed algorithm can reduce the MSE exponentially with the number of bits per word. For an 8-bit accessed word, the proposed algorithm reduces the MSE by a factor of 21. We also evaluate the proposed algorithm for MNIST dataset classification supposing that the model parameters of deep neural networks are stored in MRAMs. The numerical results show that the optimized write pulses can achieve 40% write energy reduction for a given classification accuracy. This work was presented in part at the IEEE International Symposium on Information Theory, Los Angeles, CA, USA, June 2020 [1]. Y. Kim and H. Choi are with the Department of Information and Communication Engineering, Daegu Gyeongbuk Institute of Science and Technology (DGIST), Daegu 42988, South Korea (e-mail: yjk@dgist.ac.kr). Y. Jeon and C. Guyot are with Western Digital Research, Milpitas, CA 95035 USA. Y. Cassuto is with the Viterbi Department of Electrical and Computer Engineering, Technion \u2013 Israel Institute of Technology, Haifa, Israel.", "venue": "ArXiv", "authors": ["Yongjune  Kim", "Yoocharn  Jeon", "Hyeokjin  Choi", "Cyril  Guyot", "Yuval  Cassuto"], "year": 2021, "n_citations": 0}
{"id": 492692, "s2_id": "08d7b35154e3733fd33e7eb721dc2c21a875fd5f", "title": "Microgrid - The microthreaded many-core architecture", "abstract": "Traditional processors use the von Neumann execution model, some other processors in the past have used the dataflow execution model. A combination of von Neuman model and dataflow model is also tried in the past and the resultant model is referred as hybrid dataflow execution model. We describe a hybrid dataflow model known as the microthreading. It provides constructs for creation, synchronization and communication between threads in an intermediate language. The microthreading model is an abstract programming and machine model for many-core architecture. A particular instance of this model is named as the microthreaded architecture or the Microgrid. This architecture implements all the concurrency constructs of the microthreading model in the hardware with the management of these constructs in the hardware.", "venue": "ArXiv", "authors": ["M. Irfan Uddin"], "year": 2013, "n_citations": 4}
{"id": 492711, "s2_id": "0b9078c176ded3a7d46dd56f242a74ab3e92a241", "title": "Graph Processing on FPGAs: Taxonomy, Survey, Challenges", "abstract": "Graph processing has become an important part of various areas, such as machine learning, computational sciences, medical applications, social network analysis, and many others. Various graphs, for example web or social networks, may contain up to trillions of edges. The sheer size of such datasets, combined with the irregular nature of graph processing, poses unique challenges for the runtime and the consumed power. Field Programmable Gate Arrays (FPGAs) can be an energy-efficient solution to deliver specialized hardware for graph processing. This is reflected by the recent interest in developing various graph algorithms and graph processing frameworks on FPGAs. To facilitate understanding of this emerging domain, we present the first survey and taxonomy on graph computations on FPGAs. Our survey describes and categorizes existing schemes and explains key ideas. Finally, we discuss research and engineering challenges to outline the future of graph computations on FPGAs.", "venue": "ArXiv", "authors": ["Maciej  Besta", "Dimitri  Stanojevic", "Johannes de Fine Licht", "Tal  Ben-Nun", "Torsten  Hoefler"], "year": 2019, "n_citations": 28}
{"id": 493861, "s2_id": "d2177743e33252be186e1fe131389d28fd1dddf9", "title": "Voltron: Understanding and Exploiting the Voltage-Latency-Reliability Trade-Offs in Modern DRAM Chips to Improve Energy Efficiency", "abstract": "This paper summarizes our work on experimental characterization and analysis of reduced-voltage operation in modern DRAM chips, which was published in SIGMETRICS 2017, and examines the work's significance and future potential. \nWe take a comprehensive approach to understanding and exploiting the latency and reliability characteristics of modern DRAM when the DRAM supply voltage is lowered below the nominal voltage level specified by DRAM standards. We perform an experimental study of 124 real DDR3L (low-voltage) DRAM chips manufactured recently by three major DRAM vendors. We find that reducing the supply voltage below a certain point introduces bit errors in the data, and we comprehensively characterize the behavior of these errors. We discover that these errors can be avoided by increasing the latency of three major DRAM operations (activation, restoration, and precharge). We perform detailed DRAM circuit simulations to validate and explain our experimental findings. We also characterize the various relationships between reduced supply voltage and error locations, stored data patterns, DRAM temperature, and data retention. \nBased on our observations, we propose a new DRAM energy reduction mechanism, called Voltron. The key idea of Voltron is to use a performance model to determine by how much we can reduce the supply voltage without introducing errors and without exceeding a user-specified threshold for performance loss. Our evaluations show that Voltron reduces the average DRAM and system energy consumption by 10.5% and 7.3%, respectively, while limiting the average system performance loss to only 1.8%, for a variety of memory-intensive quad-core workloads. We also show that Voltron significantly outperforms prior dynamic voltage and frequency scaling mechanisms for DRAM.", "venue": "ArXiv", "authors": ["Kevin K. Chang", "Abdullah Giray Yaglik\u00e7i", "Saugata  Ghose", "Aditya  Agrawal", "Niladrish  Chatterjee", "Abhijith  Kashyap", "Donghyuk  Lee", "Mike  O'Connor", "Hasan  Hassan", "Onur  Mutlu"], "year": 2018, "n_citations": 5}
{"id": 496049, "s2_id": "35f7b2ca4e15e89d9e9c183aec2e3c172b0dd8f5", "title": "Set-Based Obfuscation for Strong PUFs Against Machine Learning Attacks", "abstract": "Strong physical unclonable function (PUF) is a promising solution for device authentication in resource-constrained applications but vulnerable to machine learning (ML) attacks. In order to resist attack, many defenses have been proposed in recent years. However, these defenses incur high hardware overhead, degenerate reliability and are inefficient against advanced ML attacks such as approximation attacks. To address these issues, we propose a <underline>R</underline>andom <underline>S</underline>et-based <underline>O</underline>bfuscation (RSO) for Strong PUFs to resist ML attacks. The basic idea is that several stable responses are derived from the PUF itself and pre-stored as the <inline-formula> <tex-math notation=\"LaTeX\">$set$ </tex-math></inline-formula> for obfuscation in the testing phase, and then a true random number generator is used to select any two keys to obfuscate challenges and responses with XOR operations. When the number of challenge-response pairs (CRPs) collected by the attacker exceeds the given threshold, the <inline-formula> <tex-math notation=\"LaTeX\">$set$ </tex-math></inline-formula> will be updated immediately. In this way, ML attacks can be prevented with extremely low hardware overhead. Experimental results show that for a <inline-formula> <tex-math notation=\"LaTeX\">$64\\times 64$ </tex-math></inline-formula> Arbiter PUF, when the size of <inline-formula> <tex-math notation=\"LaTeX\">$set$ </tex-math></inline-formula> is 32 and even if 1 million CRPs are collected by attackers, the prediction accuracies of the several ML attacks we use are about 50% which is equivalent to the random guessing.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Jiliang  Zhang", "Chaoqun  Shen"], "year": 2021, "n_citations": 22}
{"id": 497468, "s2_id": "5f9674339765daf98cfb8768ab5a842135c2cde3", "title": "Exploiting Long-Distance Interactions and Tolerating Atom Loss in Neutral Atom Quantum Architectures", "abstract": "Quantum technologies currently struggle to scale beyond moderate scale prototypes and are unable to execute even reasonably sized programs due to prohibitive gate error rates or coherence times. Many software approaches rely on heavy compiler optimization to squeeze extra value from noisy machines but are fundamentally limited by hardware. Alone, these software approaches help to maximize the use of available hardware but cannot overcome the inherent limitations posed by the underlying technology.An alternative approach is to explore the use of new, though potentially less developed, technology as a path towards scalability. In this work we evaluate the advantages and disadvantages of a Neutral Atom (NA) architecture. NA systems offer several promising advantages such as long range interactions and native multiqubit gates which reduce communication overhead, overall gate count, and depth for compiled programs. Long range interactions, however, impede parallelism with restriction zones surrounding interacting qubit pairs. We extend current compiler methods to maximize the benefit of these advantages and minimize the cost.Furthermore, atoms in an NA device have the possibility to randomly be lost over the course of program execution which is extremely detrimental to total program execution time as atom arrays are slow to load. When the compiled program is no longer compatible with the underlying topology, we need a fast and efficient coping mechanism. We propose hardware and compiler methods to increase system resilience to atom loss dramatically reducing total computation time by circumventing complete reloads or full recompilation every cycle.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Jonathan M. Baker", "Andrew  Litteken", "Casey  Duckering", "Henry  Hoffmann", "Hannes  Bernien", "Frederic T. Chong"], "year": 2021, "n_citations": 1}
{"id": 497731, "s2_id": "360d083a618c363133e3a2961fb9971bcd508490", "title": "The Z1: Architecture and Algorithms of Konrad Zuse's First Computer", "abstract": "This paper provides the first comprehensive description of the Z1, the mechanical computer built by the German inventor Konrad Zuse in Berlin from 1936 to 1938. The paper describes the main structural elements of the machine, the high-level architecture, and the dataflow between components. The computer could perform the four basic arithmetic operations using floating-point numbers. Instructions were read from punched tape. A program consisted of a sequence of arithmetical operations, intermixed with memory store and load instructions, interrupted possibly by input and output operations. Numbers were stored in a mechanical memory. The machine did not include conditional branching in the instruction set. While the architecture of the Z1 is similar to the relay computer Zuse finished in 1941 (the Z3) there are some significant differences. The Z1 implements operations as sequences of microinstructions, as in the Z3, but does not use rotary switches as microsteppers. The Z1 uses a digital incrementer and a set of conditions which are translated into microinstructions for the exponent and mantissa units, as well as for the memory blocks. Microinstructions select one out of 12 layers in a machine with a 3D mechanical structure of binary mechanical elements. The exception circuits for mantissa zero, necessary for normalized floating-point, were lacking; they were first implemented in the Z3. The information for this article was extracted from careful study of the blueprints drawn by Zuse for the reconstruction of the Z1 for the German Technology Museum in Berlin, from some letters, and from sketches in notebooks. Although the machine has been in exhibition since 1989 (non-operational), no detailed high-level description of the machine\u2019s architecture had been available. This paper fills that gap.", "venue": "ArXiv", "authors": ["Ra\u00fal  Rojas"], "year": 2014, "n_citations": 4}
{"id": 498400, "s2_id": "d251f623aeb38c04600d68fec2827f83b40ed0db", "title": "Efficient implementation of GALS systems over commercial synchronous FPGAs: a new approach", "abstract": "The new vision presented is aimed to overcome the logic overhead issues that previous works exhibit when applying GALS techniques to programmable logic devices. The proposed new view relies in a 2-phase, bundled data parity based protocol for data transfer and clock generation tasks. The ability of the introduced methodology for smart real-time delay selection allows the implementation of a variety of new methodologies for electromagnetic interference mitigation and device environment changes adaptation.", "venue": "ArXiv", "authors": ["Javier D. Garcia-Lasheras"], "year": 2008, "n_citations": 2}
{"id": 502757, "s2_id": "8c877069094ce0e293ed464dc3af49a49681575e", "title": "Communication-avoiding micro-architecture to compute Xcorr scores for peptide identification", "abstract": "Database algorithms play a crucial part in systems biology studies by identifying proteins from mass spectrometry data. Many of these database search algorithms incur huge computational costs by computing similarity scores for each pair of sparse experimental spectrum and candidate theoretical spectrum vectors. Modern MS instrumentation techniques which are capable of generating high-resolution spectrometry data require comparison against an enormous search space, further emphasizing the need of efficient accelerators. Recent research has shown that the overall cost of scoring, and deducing peptides is dominated by the communication costs between different hierarchies of memory and processing units. However, these communication costs are seldom considered in accelerator-based architectures leading to inefficient DRAM accesses, and poor data-utilization due to irregular memory access patterns. In this paper, we propose a novel communication-avoiding microarchitecture to compute cross-correlation based similarity score by utilizing efficient local cache, and peptide pre-fetching to minimize DRAM accesses, and a custom-designed peptide broadcast bus to allow input reuse. An efficient bus arbitration scheme was designed, and implemented to minimize synchronization cost and exploit parallelism of processing elements. Our simulation results show that the proposed micro-architecture performs on average 24x better than a CPU implementation running on a 3.6 GHz Intel i7-4970 processor with 16GB memory.", "venue": "2021 31st International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Sumesh  Kumar", "Fahad  Saeed"], "year": 2021, "n_citations": 0}
{"id": 502949, "s2_id": "16fde6fe32ec7475adbe8b40ea4426399dd9ed6f", "title": "Design and simulation of a sigma delta ADC", "abstract": "In this report we describe the design and simulation of a Sigma Delta ADC in Matlan/Simulink", "venue": "ArXiv", "authors": ["Moslem  Rashidi"], "year": 2010, "n_citations": 3}
{"id": 503044, "s2_id": "8bd82492781d701dbaca9ace1a296e53f13cc4d0", "title": "Multi-placement structures for fast and optimized placement in analog circuit synthesis", "abstract": "The paper presents the novel idea of multi-placement structures, for a fast and optimized placement instantiation in analog circuit synthesis. These structures need to be generated only once for a specific circuit topology. When used in synthesis, these pre-generated structures instantiate various layout floorplans for the various sizes and parameters of a circuit. Unlike procedural layout generators, they enable fast placement of circuits while keeping the quality of the placements at a high level during the synthesis process. The fast placement is a result of high speed instantiation resulting from the efficiency of the multi-placement structure. The good quality of placements derives from the extensive and intelligent search process that is used to build the multi-placement structure. The target benchmarks of these structures are analog circuits in the vicinity of 25 modules. An algorithm for the generation of such multi-placement structures is presented. Experimental results show placement execution times with an average of a few milliseconds making them usable during layout-aware synthesis for optimized placements.", "venue": "Design, Automation and Test in Europe", "authors": ["Raoul F. Badaoui", "Ranga  Vemuri"], "year": 2005, "n_citations": 2}
{"id": 504383, "s2_id": "8b183e17329c911eb07c82ccd986988563ae138a", "title": "Massively Parallel Processor Architectures for Resource-aware Computing", "abstract": "We present a class of massively parallel processor architectures called invasive tightly coupled processor arrays (TCPAs). The presented processor class is a highly parame- terizable template, which can be tailored before runtime to fulfill costumers' requirements such as performance, area cost, and energy efficiency. These programmable accelerators are well suited for domain-specific computing from the areas of signal, image, and video processing as well as other streaming processing applications. To overcome future scaling issues (e.g., power con- sumption, reliability, resource management, as well as application parallelization and mapping), TCPAs are inherently designed in a way to support self-adaptivity and resource awareness at hardware level. Here, we follow a recently introduced resource- aware parallel computing paradigm called invasive computing where an application can dynamically claim, execute, and release resources. Furthermore, we show how invasive computing can be used as an enabler for power management. Finally, we will introduce ideas on how to realize fault-tolerant loop execution on such massively parallel architectures through employing on- demand spatial redundancies at the processor array level. I. INTRODUCTION The steady miniaturization of feature sizes allows to create increasingly complex Multi-Processor System-on-Chip (MP- SoC) architectures but raises also numerous questions. These challenges include imperfections and unreliability of the de- vices as well as scalability problems of the architectures, as for instance, how an optimal communication topology or memory architecture should look like. The situation is even more severe with respect to power consumption because chips can handle only a limited power budget\u2014but technology shrinking leads also to higher energy densities continuously. As a consequence, the potentially available chip area might not be fully utilized or at least not simultaneously. These phenomena are also known as power wall and utilization wall (1). Other scalability issues, caused by the sheer complexity of exponential growth, are related to resource management as well as parallelization and mapping approaches. This leads to the following conclusion: Future systems will only scale if the mapping and runtime methods will considerably improve\u2014this reasoning holds for both embedded and portable devices such as smartphones and tablets as well as large scale systems as used for high- performance computing. Customization and heterogeneity in the form of domain-specific components such as accelerators are the key to success for future performance gains (2).", "venue": "ArXiv", "authors": ["Vahid  Lari", "Alexandru  Tanase", "Frank  Hannig", "J\u00fcrgen  Teich"], "year": 2014, "n_citations": 5}
{"id": 504403, "s2_id": "7a96b2067f92a4cce895344f448696a87b8901e6", "title": "Sparse Matrix Multiplication on CAM Based Accelerator", "abstract": "Sparse matrix multiplication is an important component of linear algebra computations. In this paper, an architecture based on Content Addressable Memory (CAM) and Resistive Content Addressable Memory (ReCAM) is proposed for accelerating sparse matrix by sparse vector and matrix multiplication in CSR format. Using functional simulation, we show that the proposed ReCAM-based accelerator exhibits two orders of magnitude higher power efficiency as compared to existing sparse matrix-vector multiplication implementations.", "venue": "ArXiv", "authors": ["Leonid  Yavits", "Ran  Ginosar"], "year": 2017, "n_citations": 9}
{"id": 507842, "s2_id": "ad85b1db9c046ea8e48c803ff479f7b05b0e2ef2", "title": "A Full-stack Accelerator Search Technique for Vision Applications", "abstract": "The rapidly-changing ML model landscape presents a unique opportunity for building hardware accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. Although FAST can be used on any number and type of deep learning workload, in this paper we focus on optimizing for a single or small set of vision models, resulting in significantly faster and more powerefficient designs relative to a general purpose ML accelerator. When evaluated on EfficientNet [45], ResNet50v2 [18] and OCR [36] inference performance relative to a TPUv3 [22], designs generated by FAST optimized for single workloads can improve Perf/TDP (peak power) by over 6x in the best case and 4x on average. On a limited workload subset, FAST improves Perf/TDP 2.85x on average, with a reduction to 2.35x for a single design optimized over the set of workloads. In addition, we demonstrate a potential 1.8x speedup opportunity for TPU-v3 with improved scheduling.", "venue": "ArXiv", "authors": ["Dan  Zhang", "Safeen  Huda", "Ebrahim  Songhori", "Quoc  Le", "Anna  Goldie", "Azalia  Mirhoseini"], "year": 2021, "n_citations": 1}
{"id": 510978, "s2_id": "f5cd8e2bbb0fb7db4654b7fd4204a615230c4163", "title": "Multi-core: Adding a New Dimension to Computing", "abstract": "Invention of Transistors in 1948 started a new era in technology, called Solid State Electronics. Since then, sustaining development and advancement in electronics and fabrication techniques has caused the devices to shrink in size and become smaller, paving the quest for increasing density and clock speed. That quest has suddenly come to a halt due to fundamental bounds applied by physical laws. But, demand for more and more computational power is still prevalent in the computing world. As a result, the microprocessor industry has started exploring the technology along a different dimension. Speed of a single work unit (CPU) is no longer the concern, rather increasing the number of independent processor cores packed in a single package has become the new concern. Such processors are commonly known as multi-core processors. Scaling the performance by using multiple cores has gained so much attention from the academia and the industry, that not only desktops, but also laptops, PDAs, cell phones and even embedded devices today contain these processors. In this paper, we explore state of the art technologies for multi-core processors and existing software tools to support parallelism. We also discuss present and future trend of research in this field. From our survey, we conclude that next few decades are going to be marked by the success of this \"Ubiquitous parallel processing\".", "venue": "ArXiv", "authors": ["Md. Tanvir Al Amin"], "year": 2010, "n_citations": 4}
{"id": 511229, "s2_id": "97456e857b569b2f48118cebaa80e130fe68995a", "title": "Principles towards Real-Time Simulation of Material Point Method on Modern GPUs", "abstract": "Physics-based simulation has been actively employed in generating offline visual effects in the film and animation industry. However, the computations required for high-quality scenarios are generally immense, deterring its adoption in real-time applications, e.g., virtual production, avatar live-streaming, and cloud gaming. We summarize the principles that can accelerate the computation pipeline on single-GPU and multi-GPU platforms through extensive investigation and comprehension of modern GPU architecture. We further demonstrate the effectiveness of these principles by applying them to the material point method to build up our framework, which achieves 1.7\u00d7\u2013 8.6\u00d7 speedup on a single GPU and 2.5\u00d7\u201314.8\u00d7 on four GPUs compared to the state-of-the-art. Our pipeline is specifically designed for real-time applications (i.e., scenarios with small to medium particles) and achieves significant multi-GPU efficiency. We demonstrate our pipeline by simulating a snow scenario with 1.33M particles and a fountain scenario with 143K particles in real-time (on average, 68.5 and 55.9 frame-per-second, respectively) on four NVIDIA Tesla V100 GPUs interconnected with NVLinks.", "venue": "ArXiv", "authors": ["Yun  Fei", "Yuhan  Huang", "Ming  Gao"], "year": 2021, "n_citations": 0}
{"id": 512701, "s2_id": "15c82868fd5ee16c79ee142ca9556f318cc82acb", "title": "Coordinated Management of Processor Configuration and Cache Partitioning to Optimize Energy under QoS Constraints", "abstract": "An effective way to improve energy efficiency is to throttle hardware resources to meet a certain QoS target, specified as a performance constraint, associated with all applications running on a multicore system. Prior art has proposed resource management (RM) frameworks in which the share of the last-level cache (LLC) assigned to each processor core and the voltage-frequency (VF) setting for each core is managed in a coordinated fashion to reduce energy. A drawback of such a scheme is that, while one core gives up LLC resources for another core, the performance drop must be compensated by a higher VF setting which leads to a quadratic increase in energy consumption. By allowing each core to be adapted to exploit instruction and memory-level parallelism (ILP/MLP), substantially higher energy savings are enabled.This paper proposes a coordinated RM for LLC partitioning, processor adaptation, and per-core VF scaling. A first contribution is a systematic study of the resource trade-offs enabled when trading between the three classes of resources in a coordinated fashion. A second contribution is a new RM framework that utilizes these trade-offs to save more energy. Finally, a challenge to accurately model the impact of resource throttling on performance is to predict the amount of MLP with high accuracy. To this end, the paper contributes with a mechanism that estimates the effect of MLP over different processor configurations and LLC allocations. Overall, we show that up to 18% of energy, and on average 10%, can be saved using the proposed scheme.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Mehrzad  Nejat", "Madhavan  Manivannan", "Miquel  Peric\u00e0s", "Per  Stenstr\u00f6m"], "year": 2020, "n_citations": 3}
{"id": 515619, "s2_id": "16b441f7495e26ad56c1904ef2f4b0162bfd538f", "title": "Design of Novel Algorithm and Architecture for Gaussian Based Color Image Enhancement System for Real Time Applications", "abstract": "This paper presents the development of a new algorithm for Gaussian based color image enhancement system. The algorithm has been designed into architecture suitable for FPGA/ASIC implementation. The color image enhancement is achieved by first convolving an original image with a Gaussian kernel since Gaussian distribution is a point spread function which smoothen the image. Further, logarithm-domain processing and gain/offset corrections are employed in order to enhance and translate pixels into the display range of 0 to 255. The proposed algorithm not only provides better dynamic range compression and color rendition effect but also achieves color constancy in an image. The design exploits high degrees of pipelining and parallel processing to achieve real time performance. The design has been realized by RTL compliant Verilog coding and fits into a single FPGA with a gate count utilization of 321,804. The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148 FPGA device and is capable of processing high resolution color motion pictures of sizes of up to 1600x1200 pixels at the real time video rate of 116 frames per second. This shows that the proposed design would work for not only still images but also for high resolution video sequences.", "venue": "ArXiv", "authors": ["M. C. Hanumantharaju", "M.  Ravishankar", "D. R. Ramesh Babu"], "year": 2014, "n_citations": 11}
{"id": 515904, "s2_id": "bea6e89c0a4cf16fd4555af8c90dfbc597e087b2", "title": "QFlow: Quantitative Information Flow for Security-Aware Hardware Design in Verilog", "abstract": "The enormous amount of code required to design modern hardware implementations often leads to critical vulnerabilities being overlooked. Especially vulnerabilities that compromise the confidentiality of sensitive data, such as cryptographic keys, have a major impact on the trustworthiness of an entire system. Information flow analysis can elaborate whether information from sensitive signals flows towards outputs or untrusted components of the system. But most of these analytical strategies rely on the non-interference property, stating that the untrusted targets must not be influenced by the source\u2019s data, which is shown to be too inflexible for many applications. To address this issue, there are approaches to quantify the information flow between components such that insignificant leakage can be neglected. Due to the high computational complexity of this quantification, approximations are needed, which introduce mispredictions. To tackle those limitations, we reformulate the approximations. Further, we propose a tool QFlow with a higher detection rate than previous tools. It can be used by non-experienced users to identify data leakages in hardware designs, thus facilitating a security-aware design process.", "venue": "2021 IEEE 39th International Conference on Computer Design (ICCD)", "authors": ["Lennart M. Reimann", "Luca  Hanel", "Dominik  Sisejkovic", "Farhad  Merchant", "Rainer  Leupers"], "year": 2021, "n_citations": 0}
{"id": 525075, "s2_id": "08c63e79e147238da7fc22dd48a1555dd968575a", "title": "SQUASH: Simple QoS-Aware High-Performance Memory Scheduler for Heterogeneous Systems with Hardware Accelerators", "abstract": "Modern SoCs integrate multiple CPU cores and Hardware Accelerators (HWAs) that share the same main memory system, causing interference among memory requests from different agents. The result of this interference, if not controlled well, is missed deadlines for HWAs and low CPU performance. State-of-the-art mechanisms designed for CPU-GPU systems strive to meet a target frame rate for GPUs by prioritizing the GPU close to the time when it has to complete a frame. We observe two major problems when such an approach is adapted to a heterogeneous CPU-HWA system. First, HWAs miss deadlines because they are prioritized only when they are too close to their deadlines. Second, such an approach does not consider the diverse memory access characteristics of different applications running on CPUs and HWAs, leading to low performance for latency-sensitive CPU applications and deadline misses for some HWAs, including GPUs. In this paper, we propose a Simple QUality of service Aware memory Scheduler for Heterogeneous systems (SQUASH), that overcomes these problems using three key ideas, with the goal of meeting HWAs\u2019 deadlines while providing high CPU performance. First, SQUASH prioritizes a HWA when it is not on track to meet its deadline any time during a deadline period, instead of prioritizing it only when close to a deadline. Second, SQUASH prioritizes HWAs over memory-intensive CPU applications based on the observation that memory-intensive applications\u2019 performance is not sensitive to memory latency. Third, SQUASH treats short-deadline HWAs differently as they are more likely to miss their deadlines and schedules their requests based on worst-case memory access time estimates. Extensive evaluations across a wide variety of different workloads and systems show that SQUASH achieves significantly better CPU performance than the best previous scheduler while always meeting the deadlines for all HWAs, including GPUs, thereby largely improving frame rates.", "venue": "ArXiv", "authors": ["Hiroyuki  Usui", "Lavanya  Subramanian", "Kevin Kai-Wei Chang", "Onur  Mutlu"], "year": 2015, "n_citations": 14}
{"id": 528425, "s2_id": "6d4ea457177b37dada1cde840e11fc1c41592cc8", "title": "Smartphone Impostor Detection with Behavioral Data Privacy and Minimalist Hardware Support", "abstract": "Impostors are attackers who take over a smartphone and gain access to the legitimate user\u2019s confidential and private information. This paper proposes a defense-in-depth mechanism that can detect impostors quickly with simple Deep Learning algorithms, which can achieve better detection accuracy than the best prior work which used Machine Learning algorithms requiring computation of multiple features. Different from previous work, we then consider protecting the privacy of a user\u2019s behavioral (sensor) data by not exposing it outside the smartphone. For this scenario, we propose a Recurrent Neural Network (RNN) based Deep Learning algorithm that uses only the legitimate user\u2019s sensor data to learn his/her normal behavior. We propose to use Prediction Error Distribution (PED) to enhance the detection accuracy. To make the on-device, real-time detection possible, we show how a minimalist hardware module, dubbed SID for Smartphone Imposter Detector, can be designed and integrated into smartphones for self-contained impostor detection. Experimental results show that SID can support real-time impostor detection, at a very low hardware cost and energy consumption, compared to other RNN accelerators.", "venue": "ArXiv", "authors": ["Guangyuan  Hu", "Zecheng  He", "Ruby B. Lee"], "year": 2021, "n_citations": 1}
{"id": 530483, "s2_id": "730353ede6bf5a1a478fe2e44d34b9436ca48a1c", "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights", "abstract": "Machine learning (ML) models are widely used in many important domains. For efficiently processing these computational- and memory-intensive applications, tensors of these overparameterized models are compressed by leveraging sparsity, size reduction, and quantization of tensors. Unstructured sparsity and tensors with varying dimensions yield irregular computation, communication, and memory access patterns; processing them on hardware accelerators in a conventional manner does not inherently leverage acceleration opportunities. This article provides a comprehensive survey on the efficient execution of sparse and irregular tensor computations of ML models on hardware accelerators. In particular, it discusses enhancement modules in the architecture design and the software support, categorizes different hardware designs and acceleration techniques, analyzes them in terms of hardware and execution costs, analyzes achievable accelerations for recent DNNs, and highlights further opportunities in terms of hardware/software/model codesign optimizations (inter/intramodule). The takeaways from this article include the following: understanding the key challenges in accelerating sparse, irregular shaped, and quantized tensors; understanding enhancements in accelerator systems for supporting their efficient computations; analyzing tradeoffs in opting for a specific design choice for encoding, storing, extracting, communicating, computing, and load-balancing the nonzeros; understanding how structured sparsity can improve storage efficiency and balance computations; understanding how to compile and map models with sparse tensors on the accelerators; and understanding recent design trends for efficient accelerations and further opportunities.", "venue": "Proceedings of the IEEE", "authors": ["Shail  Dave", "Riyadh  Baghdadi", "Tony  Nowatzki", "Sasikanth  Avancha", "Aviral  Shrivastava", "Baoxin  Li"], "year": 2021, "n_citations": 7}
{"id": 531311, "s2_id": "ad5e42a227489a881eed4a1e53705f8267b74f3b", "title": "Tejas Simulator : Validation against Hardware", "abstract": "In this report we show results that validate the Tejas architectural simulator against native hardware. We report mean error rates of 11.45% and 18.77% for the SPEC2006 and Splash2 benchmark suites respectively. These error rates are competitive and in most cases better than the numbers reported by other contemporary simulators.", "venue": "ArXiv", "authors": ["Smruti R. Sarangi", "Rajshekar  Kalayappan", "Prathmesh  Kallurkar", "Seep  Goel"], "year": 2015, "n_citations": 5}
{"id": 534253, "s2_id": "b03f6c2ce3efd209427859d2c277309b9f5e51af", "title": "R3-DLA (Reduce, Reuse, Recycle): A More Efficient Approach to Decoupled Look-Ahead Architectures", "abstract": "Modern societies have developed insatiable demands for more computation capabilities. Exploiting implicit parallelism to provide automatic performance improvement remains a central goal in engineering future general-purpose computing systems. One approach is to use a separate thread context to perform continuous look-ahead to improve the data and instruction supply to the main pipeline. Such a decoupled look-ahead (DLA) architecture can be quite effective in accelerating a broad range of applications in a relatively straightforward implementation. It also has broad design flexibility as the look-ahead agent need not be concerned with correctness constraints. In this paper, we explore a number of optimizations that make the look-ahead agent more efficient and yet extract more utility from it. With these optimizations, a DLA architecture can achieve an average speedup of 1.4 over a state-of-the-art microarchitecture for a broad set of benchmark suites, making it a powerful tool to enhance single-thread performance.", "venue": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Sushant  Kondguli", "Michael  Huang"], "year": 2019, "n_citations": 6}
{"id": 536247, "s2_id": "12aac691cfdcd8fae14bda71ca93076009101bab", "title": "SIMDRAM: An End-to-End Framework for Bit-Serial SIMD Computing in DRAM", "abstract": "Processing-using-DRAM has been proposed for a limited set of basic operations (i.e., logic operations, addition). However, in order to enable full adoption of processing-using-DRAM, it is necessary to provide support for more complex operations. In this paper, we propose SIMDRAM, a flexible general-purpose processing-usingDRAM framework that (1) enables the efficient implementation of complex operations, and (2) provides a flexible mechanism to support the implementation of arbitrary user-defined operations. The SIMDRAM framework comprises three key steps. The first step builds an efficient MAJ/NOT representation of a given desired operation. The second step allocates DRAM rows that are reserved for computation to the operation\u2019s input and output operands, and generates the required sequence of DRAM commands to perform the MAJ/NOT implementation of the desired operation in DRAM. The third step uses the SIMDRAM control unit located inside the memory controller to manage the computation of the operation from start to end, by executing the DRAM commands generated in the second step of the framework. We design the hardware and ISA support for SIMDRAM framework to (1) address key system integration challenges, and (2) allow programmers to employ new SIMDRAM operations without hardware changes. We evaluate SIMDRAM for reliability, area overhead, throughput, and energy efficiency using a wide range of operations and seven real-world applications to demonstrate SIMDRAM\u2019s generality. Our evaluations using a single DRAM bank show that (1) over 16 operations, SIMDRAM provides 2.0\u00d7 the throughput and 2.6\u00d7 the energy efficiency of Ambit, a state-of-the-art processing-usingDRAM mechanism; (2) over seven real-world applications, SIMDRAM provides 2.5\u00d7 the performance of Ambit. Using 16 DRAM banks, SIMDRAM provides (1) 88\u00d7 and 5.8\u00d7 the throughput, and 257\u00d7 and 31\u00d7 the energy efficiency, of a CPU and a high-end GPU, respectively, over 16 operations; (2) 21\u00d7 and 2.1\u00d7 the performance of the CPU and GPU, over seven real-world applications. SIMDRAM incurs an area overhead of only 0.2% in a high-end CPU. *Nastaran Hajinazar and Geraldo F. Oliveira are co-primary authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASPLOS \u201921, April 19\u201323, 2021, Virtual, USA \u00a9 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8317-2/21/04. . . $15.00 https://doi.org/10.1145/3445814.3446749 CCS Concepts \u2022 Computer systems organization\u2192 Other architectures.", "venue": "ArXiv", "authors": ["Nastaran  Hajinazar", "Geraldo F. Oliveira", "Sven  Gregorio", "Joao  Ferreira", "Nika Mansouri Ghiasi", "Minesh  Patel", "Mohammed  Alser", "Saugata  Ghose", "Juan G'omez Luna", "Onur  Mutlu"], "year": 2021, "n_citations": 3}
{"id": 539520, "s2_id": "58219e7fdd934a9ed35addf032dc6539cb9c9f23", "title": "Energy-Proportional Data Center Network Architecture Through OS, Switch and Laser Co-design", "abstract": "Optical interconnects are already the dominant technology in large-scale data center networks. However, the high optical loss of many optical components coupled with the low efficiency of laser sources result in high aggregate power requirements for the thousands of optical transceivers used by these networks. As optical interconnects stay always on even as traffic demands ebb and flow, most of this power is wasted. We present LC DC , a data center network system architecture in which the operating system, the switch, and the optical components are co-designed to achieve energy proportionality. LC DC capitalizes on the path divergence of data center networks to turn on and off redundant paths according to traffic demand, while maintaining full connectivity. Turning off redundant paths allows the optical transceivers and their electronic drivers to power down and save energy. Maintaining full connectivity hides the laser turn-on delay. At the node layer, intercepting send requests within the OS allows for the NIC\u2019s laser turn-on delay to be fully overlapped with TCP/IP packet processing, and thus egress links can remain powered off until needed with zero performance penalty. We demonstrate the feasibility of LC DC by i) implementing the necessary modifications in the Linux kernel and device drivers, ii) implementing a 10 Gbit/s FPGA switch, and iii) performing physical experiments with optical devices and circuit simulations. Our results on university data center traces and models of Facebook and Microsoft data center traffic show that LC DC saves on average 60% of the optical transceivers power (68% max) at the cost of 6% higher packet delay.", "venue": "ArXiv", "authors": ["Haiyang  Han", "Nikos  Terzenidis", "Dimitris  Syrivelis", "Arash  Beldachi", "George T. Kanellos", "Yigit  Demir", "Jie  Gu", "Srikanth  Kandula", "Nikos  Pleros", "Fabi\u00e1n E. Bustamante", "Nikolaos  Hardavellas"], "year": 2021, "n_citations": 0}
{"id": 540604, "s2_id": "1989a8d1e9956aa1c493d70ed9a2062372ae9e74", "title": "Automating System Configuration", "abstract": "The increasing complexity of modern configurable systems makes it critical to improve the level of automation in the process of system configuration. Such automation can also improve the agility of the development cycle, allowing for rapid and automated integration of decoupled workflows. In this paper, we present a new framework for automated configuration of systems representable as state machines. The framework leverages model checking and satisfiability modulo theories (SMT) and can be applied to any application domain representable using SMT formulas. Our approach can also be applied modularly, improving its scalability. Furthermore, we show how optimization can be used to produce configurations that are best according to some metric and also more likely to be understandable to humans. We showcase this framework and its flexibility by using it to conFigure a CGRA memory tile for various image processing applications.", "venue": "2021 Formal Methods in Computer Aided Design (FMCAD)", "authors": ["Nestan  Tsiskaridze", "Maxwell  Strange", "Makai  Mann", "Kavya  Sreedhar", "Qiaoyi  Liu", "Mark  Horowitz", "Clark  Barrett"], "year": 2021, "n_citations": 0}
{"id": 542852, "s2_id": "6d3fd6c1f6b59c3f041a3bb416dfeb05f80a697e", "title": "Experimental Characterization, Optimization, and Recovery of Data Retention Errors in MLC NAND Flash Memory", "abstract": "This paper summarizes our work on experimentally characterizing, mitigating, and recovering data retention errors in multi-level cell (MLC) NAND flash memory, which was published in HPCA 2015, and examines the work's significance and future potential. Retention errors, caused by charge leakage over time, are the dominant source of flash memory errors. Understanding, characterizing, and reducing retention errors can significantly improve NAND flash memory reliability and endurance. In this work, we first characterize, with real 2Y-nm MLC NAND flash chips, how the threshold voltage distribution of flash memory changes with different retention ages -- the length of time since a flash cell was programmed. We observe from our characterization results that 1) the optimal read reference voltage of a flash cell, using which the data can be read with the lowest raw bit error rate (RBER), systematically changes with its retention age, and 2) different regions of flash memory can have different retention ages, and hence different optimal read reference voltages. \nBased on our findings, we propose two new techniques. First, Retention Optimized Reading (ROR) adaptively learns and applies the optimal read reference voltage for each flash memory block online. The key idea of ROR is to periodically learn a tight upper bound of the optimal read reference voltage, and from there approach the optimal read reference voltage. Our evaluations show that ROR can extend flash memory lifetime by 64% and reduce average error correction latency by 10.1%. Second, Retention Failure Recovery (RFR) recovers data with uncorrectable errors offline by identifying and probabilistically correcting flash cells with retention errors. Our evaluation shows that RFR essentially doubles the error correction capability.", "venue": "ArXiv", "authors": ["Yu  Cai", "Yixin  Luo", "Erich F. Haratsch", "Ken  Mai", "Saugata  Ghose", "Onur  Mutlu"], "year": 2018, "n_citations": 7}
{"id": 543312, "s2_id": "b8f1b104c09d8abff35cca9702edeb3415c841e4", "title": "Graph Neural Networks for Charged Particle Tracking on FPGAs", "abstract": "The determination of charged particle trajectories in collisions at the CERN Large Hadron Collider (LHC) is an important but challenging problem, especially in the high interaction density conditions expected during the future high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a type of geometric deep learning algorithm that has successfully been applied to this task by embedding tracker data as a graph\u2014nodes represent hits, while edges represent possible track segments\u2014and classifying the edges as true or fake track segments. However, their study in hardwareor software-based trigger applications has been limited due to their large computational cost. In this paper, we introduce an automated translation workflow, integrated into a broader tool called hls4ml, for converting GNNs into firmware for field-programmable gate arrays (FPGAs). We use this translation tool to implement GNNs for charged particle tracking, trained using the TrackML challenge dataset, on FPGAs with designs targeting different graph sizes, task complexites, and latency/throughput requirements. This work could enable the inclusion of charged particle tracking GNNs at the trigger level for HL-LHC experiments.", "venue": "ArXiv", "authors": ["Abdelrahman  Elabd", "Vesal  Razavimaleki", "Shi-Yu  Huang", "Javier  Duarte", "Markus  Atkinson", "Gage  DeZoort", "Peter  Elmer", "Jin-Xuan  Hu", "Shih-Chieh  Hsu", "Bo-Cheng  Lai", "Mark  Neubauer", "Isobel  Ojalvo", "Savannah  Thais"], "year": 2021, "n_citations": 0}
{"id": 543431, "s2_id": "172d00e6bffdb249b816286ed49906672bfb5ffa", "title": "Daisen: A Framework for Visualizing Detailed GPU Execution", "abstract": "Graphics Processing Units (GPUs) have been widely used to accelerate artificial intelligence, physics simulation, medical imaging, and information visualization applications. To improve GPU performance, GPU hardware designers need to identify performance issues by inspecting a huge amount of simulator\u2010generated traces. Visualizing the execution traces can reduce the cognitive burden of users and facilitate making sense of behaviors of GPU hardware components. In this paper, we first formalize the process of GPU performance analysis and characterize the design requirements of visualizing execution traces based on a survey study and interviews with GPU hardware designers. We contribute data and task abstraction for GPU performance analysis. Based on our task analysis, we propose Daisen, a framework that supports data collection from GPU simulators and provides visualization of the simulator\u2010generated GPU execution traces. Daisen features a data abstraction and trace format that can record simulator\u2010generated GPU execution traces. Daisen also includes a web\u2010based visualization tool that helps GPU hardware designers examine GPU execution traces, identify performance bottlenecks, and verify performance improvement. Our qualitative evaluation with GPU hardware designers demonstrates that the design of Daisen reflects the typical workflow of GPU hardware designers. Using Daisen, participants were able to effectively identify potential performance bottlenecks and opportunities for performance improvement. The open\u2010sourced implementation of Daisen can be found at gitlab.com/akita/vis. Supplemental materials including a demo video, survey questions, evaluation study guide, and post\u2010study evaluation survey are available at osf.io/j5ghq.", "venue": "Comput. Graph. Forum", "authors": ["Yifan  Sun", "Yixuan  Zhang", "Ali  Mosallaei", "Michael D. Shah", "Cody  Dunne", "David  Kaeli"], "year": 2021, "n_citations": 0}
{"id": 544771, "s2_id": "8b90ad80ed111e554daa004d72946bb903df7ae9", "title": "VLSI implementation of RSA encryption system using ancient Indian Vedic mathematics", "abstract": "This paper proposes the hardware implementation of RSA encryption/decryption algorithm using the algorithms of Ancient Indian Vedic Mathematics that have been modified to improve performance. The recently proposed hierarchical overlay multiplier architecture is used in the RSA circuitry for multiplication operation. The most significant aspect of the paper is the development of a division architecture based on Straight Division algorithm of Ancient Indian Vedic Mathematics and embedding it in RSA encryption/decryption circuitry for improved efficiency. The coding is done in Verilog HDL and the FPGA synthesis is done using Xilinx Spartan library. The results show that RSA circuitry implemented using Vedic division and multiplication is efficient in terms of area/speed compared to its implementation using conventional multiplication and division architectures.", "venue": "SPIE Microtechnologies", "authors": ["Himanshu  Thapliyal", "M. B. Srinivas"], "year": 2005, "n_citations": 63}
{"id": 546505, "s2_id": "ae994561b50530d66c0e738e1b9300735b42c3b8", "title": "Multi-Mode Inference Engine for Convolutional Neural Networks", "abstract": "During the past few years, interest in convolutional neural networks (CNNs) has risen constantly, thanks to their excellent performance on a wide range of recognition and classification tasks. However, they suffer from the high level of complexity imposed by the high-dimensional convolutions in convolutional layers. Within scenarios with limited hardware resources and tight power and latency constraints, the high computational complexity of CNNs makes them difficult to be exploited. Hardware solutions have striven to reduce the power consumption using low-power techniques, and to limit the processing time by increasing the number of processing elements (PEs). While most of ASIC designs claim a peak performance of a few hundred giga operations per seconds, their average performance is substantially lower when applied to state-of-the-art CNNs such as AlexNet, VGGNet and ResNet, leading to low resource utilization. Their performance efficiency is limited to less than 55% on average, which leads to unnecessarily high processing latency and silicon area. In this paper, we propose a dataflow which enables to perform both the fully-connected and convolutional computations for any filter/layer size using the same PEs. We then introduce a multi-mode inference engine (MMIE) based on the proposed dataflow. Finally, we show that the proposed MMIE achieves a performance efficiency of more than 84% when performing the computations of the three renown CNNs (i.e., AlexNet, VGGNet and ResNet), outperforming the best architecture in the state-of-the-art in terms of energy consumption, processing latency and silicon area.", "venue": "ArXiv", "authors": ["Arash  Ardakani", "Carlo  Condo", "Warren J. Gross"], "year": 2017, "n_citations": 3}
{"id": 546543, "s2_id": "da61d8a32a81d08bdb36f1676d6452bb275bea05", "title": "Compiling Halide Programs to Push-Memory Accelerators", "abstract": "Image processing and machine learning applications benefit tremendously from hardware acceleration, but existing compilers target either FPGAs, which sacrifice power and performance for flexible hardware, or ASICs, which rapidly become obsolete as applications change. Programmable domain-specific accelerators have emerged as a promising middle-ground between these two extremes, but such architectures have traditionally been difficult compiler targets. The main obstacle is that these accelerators often use a different memory abstraction than CPUs and GPUs: push memories that send a data stream from one computation kernel to other kernels, possibly reordered. To address the compilation challenges caused by push memories, we propose that the representation of memory in the middle and backend of the compiler be altered to combine storage with address generation and control logic in a single structure\u2014a unified buffer. We show that this compiler abstraction can be implemented efficiently on a programmable accelerator, and design a memory mapping algorithm that combines polyhedral analysis and software vectorization techniques to target our accelerator. Our evaluation shows that the compiler supports programmability while maintaining high performance. It can compile a wide range of image processing and machine learning applications to our accelerator with 4.7\u00d7 better runtime and 4.3\u00d7 better energyefficiency as compared to an FPGA.", "venue": "ArXiv", "authors": ["Qiaoyi  Liu", "Dillon  Huff", "Jeff  Setter", "Maxwell  Strange", "Kathleen  Feng", "Kavya  Sreedhar", "Ziheng  Wang", "Keyi  Zhang", "Mark  Horowitz", "Priyanka  Raina", "Fredrik  Kjolstad"], "year": 2021, "n_citations": 0}
{"id": 551199, "s2_id": "c151f9a9b0c2f09f9d49357c62b6ea5867797653", "title": "iThing: Designing Next-Generation Things with Battery Health Self-Monitoring Capabilities for Sustainable IoT in Smart Cities", "abstract": "An accurate and reliable technique for predicting Remaining Useful Life (RUL) for battery cells proves helpful in battery-operated IoT devices, especially in remotely operated sensor nodes. Datadriven methods have proved to be the most effective methods until now. These IoT devices have low computational capabilities to save costs, but Data-Driven battery health techniques often require a comparatively large amount of computational power to predict SOH and RUL due to most methods being feature-heavy. This issue calls for ways to predict RUL with the least amount of calculations and memory. This paper proposes an effective and novel peak extraction method to reduce computation and memory needs and provide accurate prediction methods using the least number of features while performing all calculations on-board. The model can self-sustain, requires minimal external interference, and hence operate remotely much longer. Experimental results prove the accuracy and reliability of this method. The Absolute Error (AE), Relative error (RE), and Root Mean Square Error (RMSE) are calculated to compare effectiveness. The training of the GPR model takes less than 2 seconds, and the correlation between SOH from peak extraction and RUL is 0.97.", "venue": "ArXiv", "authors": ["Aparna  Sinha", "Debanjan  Das", "Venkanna  Udutalapally", "Mukil Kumar Selvarajan", "Saraju P. Mohanty"], "year": 2021, "n_citations": 0}
{"id": 553110, "s2_id": "d0758b9368845a4618dcb4463705b5b5dd085e34", "title": "Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks", "abstract": "This paper presents the Neural Cache architecture, which re-purposes cache structures to transform them into massively parallel compute units capable of running inferences for Deep Neural Networks. Techniques to do in-situ arithmetic in SRAM arrays, create efficient data mapping and reducing data movement are proposed. The Neural Cache architecture is capable of fully executing convolutional, fully connected, and pooling layers in-cache. The proposed architecture also supports quantization in-cache. Our experimental results show that the proposed architecture can improve inference latency by 8.3\u00d7 over state-of-art multi-core CPU (Xeon E5), 7.7\u00d7 over server class GPU (Titan Xp), for Inception v3 model. Neural Cache improves inference throughput by 12.4\u00d7 over CPU (2.2\u00d7 over GPU), while reducing power consumption by 50% over CPU (53% over GPU).", "venue": "2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Charles  Eckert", "Xiaowei  Wang", "Jingcheng  Wang", "Arun  Subramaniyan", "Ravi R. Iyer", "Dennis  Sylvester", "David  Blaauw", "Reetuparna  Das"], "year": 2018, "n_citations": 146}
{"id": 556471, "s2_id": "ee11b358fdf31fea16e92934edcac5faabe4e538", "title": "Eva-CiM: A System-Level Performance and Energy Evaluation Framework for Computing-in-Memory Architectures", "abstract": "Computing-in-memory (CiM) architectures aim to reduce costly data transfers by performing arithmetic and logic operations in memory and hence relieve the pressure due to the memory wall. However, determining whether a given workload can really benefit from CiM, which memory hierarchy and what device technology should be adopted by a CiM architecture requires in-depth study that is not only time consuming but also demands significant expertise in architectures and compilers. This article presents an energy and performance evaluation framework, Eva-CiM, for systems based on CiM architectures. Eva-CiM encompasses a multilevel (from device to architecture) comprehensive tool chain that leverages existing modeling and simulation tools, such as GEM5, McPAT, and DESTINY. To support high-confidence prediction, rapid design space exploration and ease of use, Eva-CiM introduces several novel modeling/analysis approaches including models for capturing memory access and dependency-aware ISA traces, and for quantifying interactions between the host CPU and the CiM module. Eva-CiM can readily produce energy and performance estimates of the entire system for a given program, a processor architecture, and the CiM array and technology specifications. Eva-CiM is validated by comparing with DESTINY. Eva-CiM enables analyses including the system-level impact of CiM-supported accesses, whether a program is CiM-favorable as well as the pros and cons of increased memory size for CiM. Eva-CiM also facilitates exploration of different design configurations and technologies.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Di  Gao", "Dayane  Reis", "Xiaobo Sharon Hu", "Cheng  Zhuo"], "year": 2020, "n_citations": 7}
{"id": 560438, "s2_id": "6f129eb76bc3aa115f68e4d0ce0b6d7547904fdb", "title": "A Design Space Exploration Methodology for Parameter Optimization in Multicore Processors", "abstract": "Due to the increasing proliferation of computing systems in diverse application domains, the need for application-specific design of multicore/manycore processing platforms is paramount. In order to tailor processors for application-specific requirements, a multitude of processor design parameters need to be tuned accordingly. Tuning of processor design parameters involves rigorous and extensive design space exploration over large search spaces. In this paper, we propose an efficient design space exploration methodology for multicore parameter optimization. Our proposed methodology includes an intelligent initial parameter setting algorithm, the results of which are leveraged by two search algorithms -- exhaustive search and greedy search. We evaluate the methodology in a cycle-accurate simulator (ESESC) using standard set of PARSEC and SPLASH2 benchmarks for applications with low-power and high-performance requirements. The results reveal that the quality of solutions (design configurations) obtained from our methodology are within 1.35%-3.69% of the solutions obtained from fully exhaustive search while only exploring 2.74%-3% of the design space. Our methodology achieves on average a 35.32x speedup in design space exploration time as compared to fully exhaustive search in finding the best processor design configuration.", "venue": "ISVLSI", "authors": ["Prasanna  Kansakar", "Arslan  Munir"], "year": 2016, "n_citations": 0}
{"id": 560516, "s2_id": "83b651c02e9daec598ebec8f84e70c2c66bf6e72", "title": "Power Saving Evaluation with Automatic Offloading", "abstract": "Heterogeneous hardware other than small-core CPU such as GPU, FPGA, or many-core CPU is increasingly being used. However, heterogeneous hardware usage presents high technical skill barriers such as familiarity with CUDA. To overcome this challenge, I previously proposed environment-adaptive software that enables automatic conversion, automatic configuration, and high-performance and low-power operation of once-written code, in accordance with the hardware to be placed. I also previously verified performance improvement of automatic GPU and FPGA offloading. In this paper, I verify low-power operation with environment adaptation by evaluating power utilization after automatic offloading. I compare Watt*seconds of existing applications after automatic offloading with the case of CPU-only processing.", "venue": "The Proceedings of The 8th International Conference on Intelligent Systems and Image Processing 2021", "authors": ["Yoji  Yamato"], "year": 2021, "n_citations": 0}
{"id": 560842, "s2_id": "7c545e27defacfb1806d35ece2f9755e30542250", "title": "RepTFD: Replay Based Transient Fault Detection", "abstract": "The advances in IC process make future chip multiprocessors (CMPs) more and more vulnerable to transient faults. To detect transient faults, previous core-level schemes provide redundancy for each core separately. As a result, they may leave transient faults in the uncore parts, which consume over 50% area of a modern CMP, escaped from detection. This paper proposes RepTFD, the first core-level transient fault detection scheme with 100% coverage. Instead of providing redundancy for each core separately, RepTFD provides redundancy for a group of cores as a whole. To be specific, it replays the execution of the checked group of cores on a redundant group of cores. Through comparing the execution results between the two groups of cores, all malignant transient faults can be caught. Moreover, RepTFD adopts a novel pending period based record-replay approach, which can greatly reduce the number of execution orders that need to be enforced in the replay-run. Hence, RepTFD brings only 4.76% performance overhead in comparison to the normal execution without fault-tolerance according to our experiments on the RTL design of an industrial CMP named Godson-3. In addition, RepTFD only consumes about 0.83% area of Godson-3, while needing only trivial modifications to existing components of Godson-3.", "venue": "ArXiv", "authors": ["Lei  Li", "Tianshi  Chen", "Yunji  Chen", "Ling  Li", "Ruiyang  Wu"], "year": 2012, "n_citations": 0}
{"id": 564640, "s2_id": "8d657a8b52bf8c8900e6754a2369403baeda4b77", "title": "RHNAS: Realizable Hardware and Neural Architecture Search", "abstract": "The rapidly evolving field of Artificial Intelligence necessitates automated approaches to co-design neural network architecture and neural accelerators to maximize system efficiency and address productivity challenges. To enable joint optimization of this vast space, there has been growing interest in differentiable NN-HW co-design. Fully differentiable co-design has reduced the resource requirements for discovering optimized NN-HW configurations, but fail to adapt to general hardware accelerator search spaces. This is due to the existence of nonsynthesizable (invalid) designs in the search space of many hardware accelerators. To enable efficient and realizable co-design of configurable hardware accelerators with arbitrary neural network search spaces, we introduce RHNAS. RHNAS is a method that combines reinforcement learning for hardware optimization with differentiable neural architecture search. RHNAS discovers realizable NN-HW designs with 1.84\u00d7 lower latency and 1.86\u00d7 lower energydelay product (EDP) on ImageNet and 2.81\u00d7 lower latency and 3.30\u00d7 lower EDP on CIFAR-10 over the default hardware accelerator design.", "venue": "ArXiv", "authors": ["Yash  Akhauri", "Adithya  Niranjan", "J. Pablo Munoz", "Suvadeep  Banerjee", "Abhijit  Davare", "Pasquale  Cocchini", "Anton A. Sorokin", "Ravi  Iyer", "Nilesh  Jain"], "year": 2021, "n_citations": 1}
{"id": 566375, "s2_id": "789c415e7f6d85588bc1079401a8aba76f5661a4", "title": "A Microarchitecture Implementation Framework for Online Learning with Temporal Neural Networks", "abstract": "Temporal Neural Networks (TNNs) are spiking neural networks that use time as a resource to represent and process information, similar to the mammalian neocortex. In contrast to compute-intensive deep neural networks that employ separate training and inference phases, TNNs are capable of extremely efficient online incremental/continual learning and are excellent candidates for building edge-native sensory processing units. This work proposes a microarchitecture framework for implementing TNNs using standard CMOS. Gate-level implementations of three key building blocks are presented: 1) multi-synapse neurons, 2) multi-neuron columns, and 3) unsupervised and supervised online learning algorithms based on Spike Timing Dependent Plasticity (STDP). The proposed microarchitecture is embodied in a set of characteristic scaling equations for assessing the gate count, area, delay and power for any TNN design. Post-synthesis results (in 45nm CMOS) for the proposed designs are presented, and their online incremental learning capability is demonstrated.", "venue": "2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Harideep  Nair", "John Paul Shen", "James E. Smith"], "year": 2021, "n_citations": 0}
{"id": 570707, "s2_id": "dbc4b7508068f77b1ad9c24d963b446def8eed2c", "title": "1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design", "abstract": "The past decade has seen a remarkable series of advances in machine learning, and in particular deeplearning approaches based on artificial neural networks, to improve our abilities to build more accurate systems across a broad range of areas, including computer vision, speech recognition, language translation, and natural language understanding tasks. This paper is a companion paper to a keynote talk at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in machine learning, and their implications on the kinds of computational devices we need to build, especially in the post-Moore's Lawera. It also discusses some of the ways that machine learning may be able to help with some aspects of the circuit design process. Finally, it provides a sketch of at least one interesting direction towards much larger-scale multi-task models that are sparsely activated and employ much more dynamic, exampleand task-based routing than the machine learning models of today.", "venue": "2020 IEEE International Solid- State Circuits Conference - (ISSCC)", "authors": ["Jeffrey  Dean"], "year": 2020, "n_citations": 33}
{"id": 573222, "s2_id": "bad4a9d905e0500af45714b5d4449d8f44dedc70", "title": "Reversible Programmable Logic Array (RPLA) using Feynman & MUX Gates for Low Power Industrial Applications", "abstract": "This paper present the research work directed towards the design of reversible programmable logic array using very high speed integrated circuit hardware description language (VHDL). Reversible logic circuits have significant importance in bioinformatics, optical information processing, CMOS design etc. In this paper the authors propose the design of new RPLA using Feynman & MUX gate.VHDL based codes of reversible gates with simulating results are shown .This proposed RPLA may be further used to design any reversible logic function or Boolean function (Adder, subtractor etc.) which dissipate very low or ideally no heat.", "venue": "ArXiv", "authors": ["Pradeep  Singla", "Naveen Kr. Malik"], "year": 2012, "n_citations": 3}
{"id": 575558, "s2_id": "8993427c682496f43bb17a3701ef4c15a331ea89", "title": "On the role of system software in energy management of neuromorphic computing", "abstract": "Neuromorphic computing systems such as DYNAPs and Loihi have recently been introduced to the computing community to improve performance and energy efficiency of machine learning programs, especially those that are implemented using Spiking Neural Network (SNN). The role of a system software for neuromorphic systems is to cluster a large machine learning model (e.g., with many neurons and synapses) and map these clusters to the computing resources of the hardware. In this work, we formulate the energy consumption of a neuromorphic hardware, considering the power consumed by neurons and synapses, and the energy consumed in communicating spikes on the interconnect. Based on such formulation, we first evaluate the role of a system software in managing the energy consumption of neuromorphic systems. Next, we formulate a simple heuristic-based mapping approach to place the neurons and synapses onto the computing resources to reduce energy consumption. We evaluate our approach with 10 machine learning applications and demonstrate that the proposed mapping approach leads to a significant reduction of energy consumption of neuromorphic computing systems.", "venue": "CF", "authors": ["Twisha  Titirsha", "Shihao  Song", "Adarsha  Balaji", "Anup  Das"], "year": 2021, "n_citations": 9}
{"id": 578103, "s2_id": "aaf3cff620b578a19cf961444fe0a7316f23e704", "title": "Virtualization Architecture for NoC-based Reconfigurable Systems", "abstract": "We propose a virtualization architecture for NoC-based reconfigurable systems. The motivation of this work is to develop a service-oriented architecture that includes Partial Reconfigurable Region as a Service (PRRaaS) and Processing Element as a Service (PEaaS) for software applications. According to the requirements of software applications, new PEs can be created on-demand by (re)configuring the logic resource of the PRRs in the FPGA, while the configured PEs can also be virtualized to support multiple application tasks at the same time. As a result, such a two-level virtualization mechanism, including the gate-level virtualization and the PE-level virtualization, enables an SoC to be dynamically adapted to changing application requirements. Therefore, more software applications can be performed, and system performance can be further enhanced.", "venue": "ArXiv", "authors": ["Chun-Hsian  Huang", "Kwuan-Wei  Tseng", "Chih-Cheng  Lin", "Fang-Yu  Lin", "Pao-Ann  Hsiung"], "year": 2015, "n_citations": 1}
{"id": 578182, "s2_id": "84e4a33c7ce58731cef2240887806ca67305389e", "title": "Near-Memory Address Translation", "abstract": "Memory and logic integration on the same chip is becoming increasingly cost effective, creating the opportunity to offload data-intensive functionality to processing units placed inside memory chips. The introduction of memory-side processing units (MPUs) into conventional systems faces virtual memory as the first big showstopper: without efficient hardware support for address translation MPUs have highly limited applicability. Unfortunately, conventional translation mechanisms fall short of providing fast translations as contemporary memories exceed the reach of TLBs, making expensive page walks common.In this paper, we are the first to show that the historically important flexibility to map any virtual page to any page frame is unnecessary in today's servers. We find that while limiting the associativity of the virtual-to-physical mapping incurs no penalty, it can break the translate-then-fetch serialization if combined with careful data placement in the MPU's memory, allowing for translation and data fetch to proceed independently and in parallel. We propose the Distributed Inverted Page Table (DIPTA), a near-memory structure in which the smallest memory partition keeps the translation information for its data share, ensuring that the translation completes together with the data fetch. DIPTA completely eliminates the performance overhead of translation, achieving speedups of up to 3.81x and 2.13x over conventional translation using 4KB and 1GB pages respectively.", "venue": "2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)", "authors": ["Javier  Picorel", "Djordje  Jevdjic", "Babak  Falsafi"], "year": 2017, "n_citations": 14}
{"id": 578470, "s2_id": "6d434957fa022768fbf0f5dcc57e9810b941e31d", "title": "DNNExplorer: A Framework for Modeling and Exploring a Novel Paradigm of FPGA-based DNN Accelerator", "abstract": "Existing FPGA-based DNN accelerators typically fall into two design paradigms. Either they adopt a generic reusable architecture to support different DNN networks but leave some performance and efficiency on the table because of the sacrifice of design specificity. Or they apply a layer-wise tailor-made architecture to optimize layer-specific demands for computation and resources but loose the scalability of adaptation to a wide range of DNN networks. To overcome these drawbacks, this paper proposes a novel FPGA-based DNN accelerator design paradigm and its automation tool, called DNNExplorer, to enable fast exploration of various accelerator designs under the proposed paradigm and deliver optimized accelerator architectures for existing and emerging DNN networks. Three key techniques are essential for DNNExplorer's improved performance, better specificity, and scalability, including (1) a unique accelerator design paradigm with both high-dimensional design space support and fine-grained adjustability, (2) a dynamic design space to accommodate different combinations of DNN workloads and targeted FPGAs, and (3) a design space exploration (DSE) engine to generate optimized accelerator architectures following the proposed paradigm by simultaneously considering both FPGAs' computation and memory resources and DNN networks' layer-wise characteristics and overall complexity. Experimental results show that, for the same FPGAs, accelerators generated by DNNExplorer can deliver up to 4.2x higher performances (GOP/s) than the state-of-the-art layer-wise pipelined solutions generated by DNNBuilder [1] for VGG-like DNN with 38 CONV layers. Compared to accelerators with generic reusable computation units, DNNExplorer achieves up to 2.0x and 4.4x DSP efficiency improvement than a recently published accelerator design from academia (HybridDNN [2]) and a commercial DNN accelerator IP (Xilinx DPU [3]), respectively.", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Xiaofan  Zhang", "Hanchen  Ye", "Junsong  Wang", "Yonghua  Lin", "Jinjun  Xiong", "Wen-mei  Hwu", "Deming  Chen"], "year": 2020, "n_citations": 8}
{"id": 578581, "s2_id": "dcbe303e3e28702f15b31ff9ca2997d31164cb26", "title": "MemPool: A Shared-L1 Memory Many-Core Cluster with a Low-Latency Interconnect", "abstract": "A key challenge in scaling shared-L1 multi-core clusters towards many-core (more than 16 cores) configurations is to ensure low-latency and efficient access to the L1 memory. In this work we demonstrate that it is possible to scale up the shared-L1 architecture: We present MemPool, a 32 bit many-core system with 256 fast RV32IMA \u201cSnitch\u201d cores featuring application-tunable execution units, running at 700 MHz in typical conditions (TT/0.80 V/25 \u00b0C). MemPool is easy to program, with all the cores sharing a global view of a large L1 scratchpad memory pool, accessible within at most 5 cycles. In MemPool's physical-aware design, we emphasized the exploration, design, and optimization of the low-latency processor-to-L1-memory interconnect. We compare three candidate topologies, analyzing them in terms of latency, throughput, and back-end feasibility. The chosen topology keeps the average latency at fewer than 6 cycles, even for a heavy injected load of 0.33 request/core/cycle. We also propose a lightweight addressing scheme that maps each core private data to a memory bank accessible within one cycle, which leads to performance gains of up to 20 % in real-world signal processing benchmarks. The addressing scheme is also highly efficient in terms of energy consumption since requests to local banks consume only half of the energy required to access remote banks. Our design achieves competitive performance with respect to an ideal, non-implementable full-crossbar baseline.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Matheus  Cavalcante", "Samuel  Riedel", "Antonio  Pullini", "Luca  Benini"], "year": 2021, "n_citations": 2}
{"id": 579419, "s2_id": "7d2b8d394d7149b78ac677d47255a6978b15ddee", "title": "Energy- and performance-driven NoC communication architecture synthesis using a decomposition approach", "abstract": "In this paper, we present a methodology for customized communication architecture synthesis that matches the communication requirements of the target application. This is an important problem, particularly for network-based implementations of complex applications. Our approach is based on using frequently encountered generic communication primitives as an alphabet capable of characterizing any given communication pattern. The proposed algorithm searches through the entire design space for a solution that minimizes the system total energy consumption, while satisfying the other design constraints. Compared to the standard mesh architecture, the customized architecture generated by the newly proposed approach shows about 36% throughput increase and 51% reduction in the energy required to encrypt 128 bits of data with a standard encryption algorithm.", "venue": "Design, Automation and Test in Europe", "authors": ["\u00dcmit Y. Ogras", "Radu  Marculescu"], "year": 2005, "n_citations": 110}
{"id": 579714, "s2_id": "ea5ca99e26bcaa38087ada12fd64ed4a0140a098", "title": "Enabling Binary Neural Network Training on the Edge", "abstract": "The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloudbased infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higherprecision alternatives. In this paper, we demonstrate that they are also strongly robust to gradient quantization, thereby making the training of modern models on the edge a practical reality. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions and energy savings vs Courbariaux & Bengio\u2019s standard approach. Against the latter, we see coincident memory requirement and energy consumption drops of 2\u20136\u00d7, while reaching similar test accuracy in comparable time, across a range of small-scale models trained to classify popular datasets. We also showcase ImageNet training of ResNetE18, achieving a 3.12\u00d7 memory reduction over the aforementioned standard. Such savings will allow for unnecessary cloud offloading to be avoided, reducing latency, increasing energy efficiency and safeguarding privacy.", "venue": "EMDL@MobiSys", "authors": ["Erwei  Wang", "James J. Davis", "Daniele  Moro", "Piotr  Zielinski", "Claudionor  Coelho", "Satrajit  Chatterjee", "Peter Y. K. Cheung", "George A. Constantinides"], "year": 2021, "n_citations": 1}
{"id": 579743, "s2_id": "cd85eb07512e20e67aebaf184383a116a4ad7abe", "title": "Branch Predicting with Sparse Distributed Memories", "abstract": "Modern processors rely heavily on speculation to keep the pipeline filled and consequently execute and commit instructions as close to maximum capacity as possible. To improve instruction level parallelism, the processor core needs to fetch and decode multiple instructions per cycle and has come to rely on incredibly accurate branch prediction. However, this comes at cost of the increased area and complexity which is needed for modern high accuracy branch predictors. The key idea described in this work is to use hyperdimensional computing and sparse distributed memory principles to create a novel branch predictor that can deliver complex predictions for a fraction of the current area. Sparse distributed memories can store vast amounts of data in a compressed manner, theoretically enabling branch histories larger and more precise than the branch predictors used today to be stored with equal or smaller area footprint. Furthermore, as all the data is in a hashed format and due to the nature of the hashing scheme used, it is inherently harder to manipulate with known side channel attacks. We describe our proof-of-concept and evaluate it against a state-of-the-art academic TAGE predictor. Our experiments are conducted on realistic synthetic branch predictor patterns and the Championship Branch Prediction traces and show competitive accuracy. Finally, we describe techniques that can be used to solve some of the challenges of processing with hyperdimensional vectors in order to deliver timely predictions.", "venue": "ArXiv", "authors": ["Ilias  Vougioukas", "Andreas  Sandberg", "Nikos  Nikoleris"], "year": 2021, "n_citations": 0}
{"id": 581586, "s2_id": "6e0a256e89b567fb9ae56b5445807ece02578c5f", "title": "Taming Weak Memory Models", "abstract": "Speculative techniques in microarchitectures relax various dependencies in programs, which contributes to the complexity of (weak) memory models. We show using WMM, a new weak memory model, that the model becomes simpler if it includes load-value speculation and thus, does not enforce any dependency! However, in the absence of good value-prediction techniques, a programmer may end up paying a price for the extra fences. Thus, we also present WMM-D, which enforces the dependencies captured by the current microarchitectures. WMM-D is still much simpler than other existing models. We also show that non-atomic multi-copy stores arise as a result of sharing write-through caches. We think restricting microarchitectures to write-back caches (and thus simpler weak memory models) will not incur any performance penalty. Nevertheless, we present WMM-S, another extension to WMM, which could model the effects of non-atomic multi-copy stores. WMM, WMM-D, and WMM-S are all defined using Instantaneous Instruction Execution (I^2E), a new way of describing memory models without explicit reordering or speculative execution.", "venue": "ArXiv", "authors": ["Sizhuo  Zhang", "Arvind", "Muralidaran  Vijayaraghavan"], "year": 2016, "n_citations": 4}
{"id": 583047, "s2_id": "ea73c675abb8617c732cbfe49788f747581faad8", "title": "Row-Based Dual Vdd Assignment, for a Level Converter Free CSA Design and Its Near-Threshold Operation", "abstract": "Subthreshold circuit designs are very much popular for some of the ultra-low power applications, where the minimum energy consumption is the primary concern. But, due to the weak driving current, these circuits generally suffer from huge performance degradation. Therefore, in this paper, we primarily targeted analyzing the performance of a near-threshold circuit (NTC), which retains the excellent energy efficiency of the subthreshold design, while improving the performance to a certain extent. A modified row-based dual 4-operand carry save adder (CSA) design has been reported in the present work using 45\u2009nm technology. Moreover, to find out the effectiveness of the near-threshold operation of the 4-operand CSA design, it has been compared with the other design styles. From the simulation results, obtained for the frequency of 20\u2009MHz, we found that the proposed scheme of CSA design consumes Watt of average power (), which is almost 90.9% lesser than that of the conventional CSA design, whereas, looking at the perspective of maximum delay at output, the proposed scheme of CSA design provides a fair 44.37% improvement, compared to that of the subthreshold CSA design.", "venue": "ArXiv", "authors": ["Dipankar  Saha", "Aanan  Chatterjee", "Sayan  Chatterjee", "C. K. Sarkar"], "year": 2013, "n_citations": 2}
{"id": 584838, "s2_id": "438ad69970562920ee790cc50e7d1bf9d18e0e54", "title": "Marrying Many-core Accelerators and InfiniBand for a New Commodity Processor", "abstract": "During the last 15 years, the supercomputing industry has been using mass-produced, off-the-shelf components to build cluster computers. Such components are not perfect for HPC purposes, but are cheap due to effect of scale in their production. The coming exa-scale era changes the landscape: exa-scale computers will contain components in quantities large enough to justify their custom development and production. \nWe propose a new heterogeneous processor, equipped with a network controller and designed specifically for HPC. We then show how it can be used for enterprise computing market, guaranteeing its widespread adoption and therefore low production costs.", "venue": "ArXiv", "authors": ["Konstantin S. Solnushkin", "Yuichi  Tsujita"], "year": 2013, "n_citations": 0}
{"id": 592260, "s2_id": "f468f5353ccc155a1c0ff5d5acca975f64c6f929", "title": "A Flexible HLS Hoeffding Tree Implementation for Runtime Learning on FPGA", "abstract": "Decision trees are often preferred when implementing Machine Learning in embedded systems for their simplicity and scalability. Hoeffding Trees are a type of Decision Trees that take advantage of the Hoeffding Bound to allow them to learn patterns in data without having to continuously store the data samples for future reprocessing. This makes them especially suitable for deployment on embedded devices. In this work we highlight the features of an HLS implementation of the Hoeffding Tree. The implementation parameters include the feature size of the samples (D), the number of output classes (K), and the maximum number of nodes to which the tree is allowed to grow (Nd). We target a Xilinx MPSoC ZCU102, and evaluate: the design\u2019s resource requirements and clock frequency for different numbers of classes and feature size, the execution time on several synthetic datasets of varying sample sizes (N), number of output classes and the execution time and accuracy for two datasets from UCI. For a problem size of D3, K5, and N40000, a single decision tree operating at 103MHz is capable of 8.3\u00d7 faster inference than the 1.2 GHz ARM Cortex-A53 core. Compared to a reference implementation of the Hoeffding tree, we achieve comparable classification accuracy for the UCI datasets.", "venue": "ArXiv", "authors": ["Lu'is Miguel Sousa", "Nuno  Paulino", "Joao Canas Ferreira", "Joao  Bispo"], "year": 2021, "n_citations": 0}
{"id": 593882, "s2_id": "b30f43d01266e1dff5bbea8e72ca28742aaee6d9", "title": "Prevention of Microarchitectural Covert Channels on an Open-Source 64-bit RISC-V Core", "abstract": "Covert channels enable information leakage across security boundaries of the operating system. Microarchitectural covert channels exploit changes in execution timing resulting from competing access to limited hardware resources. We use the recent experimental support for time protection, aimed at preventing covert channels, in the seL4 microkernel and evaluate the efficacy of the mechanisms against five known channels on Ariane, an open-source 64-bit application-class RISC-V core. We confirm that without hardware support, these defences are expensive and incomplete. We show that the addition of a single-instruction extension to the RISC-V ISA, that flushes microarchitectural state, can enable the OS to close all five evaluated covert channels with low increase in context switch costs and negligible hardware overhead. We conclude that such a mechanism is essential for security.", "venue": "ArXiv", "authors": ["Nils  Wistoff", "Moritz  Schneider", "Frank K. G\u00fcrkaynak", "Luca  Benini", "Gernot  Heiser"], "year": 2020, "n_citations": 9}
{"id": 594435, "s2_id": "eb41a709da3f45464b5ad74f9509d0e6c4afe013", "title": "Design of a Transport Triggered Architecture Processor for Flexible Iterative Turbo Decoder", "abstract": "In order to meet the requirement of high data rates for the next generation wireless systems, the efficient implementation of receiver algorithms is essential. On the other hand, the rapid development of technology motivates the investigation of programmable implementations. This paper summarizes the design of a programmable turbo decoder as an applicationspecific instruction-set processor (ASIP) using Transport Triggered Architecture (TTA). The processor architecture is designed in such manner that it can be programmed to support other receiver algorithms, for example, decoding based on the Viterbi algorithm. Different suboptimal maximum a posteriori (MAP) algorithms are used and compared to one another for the softinput soft-output (SISO) component decoders in a single TTA processor. The max-log-MAP algorithm outperforms the other suboptimal algorithms in terms of latency. The design enables the designer to change the suboptimal algorithms according to the bit error rate (BER) performance requirement. Unlike many other programmable turbo decoder implementations, quadratic polynomial permutation (QPP) interleaver is used in this work for contention-free memory access and to make the processor 3GPP LTE compliant. Several optimization techniques to enable real time processing on programmable platforms are introduced. Using our method, with a single iteration 31.32 Mbps throughput is achieved for the max-log-MAP algorithm for a clock frequency of 200 MHz.", "venue": "ArXiv", "authors": ["Shahriar  Shahabuddin", "Janne  Janhunen", "Markku J. Juntti"], "year": 2015, "n_citations": 7}
{"id": 598077, "s2_id": "57f32185e2db8c373af40dcf643c81267a1a08c9", "title": "Architectural Techniques to Enable Reliable and Scalable Memory Systems", "abstract": "High capacity and scalable memory systems play a vital role in enabling our desktops, smartphones, and pervasive technologies like Internet of Things (IoT). Unfortunately, memory systems are becoming increasingly prone to faults. This is because we rely on technology scaling to improve memory density, and at small feature sizes, memory cells tend to break easily. Today, memory reliability is seen as the key impediment towards using high-density devices, adopting new technologies, and even building the next Exascale supercomputer. To ensure even a bare-minimum level of reliability, present-day solutions tend to have high performance, power and area overheads. Ideally, we would like memory systems to remain robust, scalable, and implementable while keeping the overheads to a minimum. This dissertation describes how simple cross-layer architectural techniques can provide orders of magnitude higher reliability and enable seamless scalability for memory systems while incurring negligible overheads.", "venue": "ArXiv", "authors": ["Prashant J. Nair"], "year": 2017, "n_citations": 2}
{"id": 598771, "s2_id": "87bbfc0b348628e8be843f91424c8a8f186625e4", "title": "GNNerator: A Hardware/Software Framework for Accelerating Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) apply deep learning to inputs represented as graphs. They use fully-connected layers to extract features from the nodes/edges of a graph and aggregate these features using message passing between nodes, thereby combining two distinct computational patterns: dense, regular computations and sparse, irregular computations. To address the computational challenges posed by GNNs, we propose GNNERATOR, an accelerator with heterogeneous compute engines optimized for these two patterns. Further, we propose feature-blocking, a novel GNN dataflow that beneficially trades off irregular memory accesses during aggregation for regular memory accesses during feature extraction. We show that GNNERATOR achieves speedups of 5.7-37x over an NVIDIA RTX 2080-Ti, and 2.3x-3.8x over HyGCN, a state-of-the-art GNN accelerator.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Jacob R. Stevens", "Dipankar  Das", "Sasikanth  Avancha", "Bharat  Kaul", "Anand  Raghunathan"], "year": 2021, "n_citations": 2}
{"id": 601840, "s2_id": "3649163f9fa31f7b537bbd6b1bcb5f72cb25044a", "title": "Rubik: A Hierarchical Architecture for Efficient Graph Learning", "abstract": "Graph convolutional network (GCN) emerges as a promising direction to learn the inductive representation in graph data commonly used in widespread applications, such as E-commerce, social networks, and knowledge graphs. However, learning from graphs is non-trivial because of its mixed computation model involving both graph analytics and neural network computing. To this end, we decompose the GCN learning into two hierarchical paradigms: graph-level and node-level computing. Such a hierarchical paradigm facilitates the software and hardware accelerations for GCN learning. \nWe propose a lightweight graph reordering methodology, incorporated with a GCN accelerator architecture that equips a customized cache design to fully utilize the graph-level data reuse. We also propose a mapping methodology aware of data reuse and task-level parallelism to handle various graphs inputs effectively. Results show that Rubik accelerator design improves energy efficiency by 26.3x to 1375.2x than GPU platforms across different datasets and GCN models.", "venue": "ArXiv", "authors": ["Xiaobing  Chen", "Yuke  Wang", "Xinfeng  Xie", "Xing  Hu", "Abanti  Basak", "Ling  Liang", "Mingyu  Yan", "Lei  Deng", "Yufei  Ding", "Zidong  Du", "Yunji  Chen", "Yuan  Xie"], "year": 2020, "n_citations": 6}
{"id": 608604, "s2_id": "57fb0783732e9fa816d24f8985ddcb93b484ec14", "title": "Optimizing Placement of Heap Memory Objects in Energy-Constrained Hybrid Memory Systems", "abstract": "Main memory (DRAM) significantly impacts the power and energy utilization of the overall server system. Non-Volatile Memory (NVM) devices, such as Phase Change Memory and Spin-Transfer Torque RAM, are suitable candidates for main memory to reduce energy consumption. But unlike DRAM, NVMs access latencies are higher than DRAM and NVM writes are more energy sensitive than DRAM write operations. Thus, Hybrid Main Memory Systems (HMMS) employing DRAM and NVM have been proposed to reduce the overall energy depletion of main memory while optimizing the performance of NVM. This paper proposes eMap, an optimal heap memory object placement planner in HMMS. eMap considers the object-level access patterns and energy consumption at the application level and provides an ideal placement strategy for each object to augment performance and energy utilization. eMap is equipped with two modules, eMPlan and eMDyn. Specifically, eMPlan is a static placement planner which provides one time placement policies for memory object to meet the energy budget while eMDyn is a runtime placement planner to consider the change in energy limiting constraint during the runtime and shuffles the memory objects by taking into account the access patterns as well as the migration cost in terms of energy and performance. The evaluation shows that our proposed solution satisfies both the energy limiting constraint and the performance. We compare our methodology with the state-of-the-art memory object classification and allocation (MOCA) framework. Our extensive evaluation shows that our proposed solution, eMPlan meets the energy constraint with 4.17 times less costly and reducing the energy consumption up to 14% with the same performance. eMDyn also satisfies the performance and energy requirement while considering the migration cost in terms of time and energy.", "venue": "ArXiv", "authors": ["Taeuk  Kim", "Safdar  Jamil", "Joongeon  Park", "Youngjae  Kim"], "year": 2020, "n_citations": 0}
{"id": 609962, "s2_id": "c16fbc4385dbb119e0d6b17a3a6a2200fd2b478f", "title": "Extending the RISC-V ISA for exploring advanced reconfigurable SIMD instructions", "abstract": "This paper presents a novel, non-standard set of vector instruction types for exploring custom SIMD instructions in a softcore. The new types allow simultaneous access to a relatively high number of operands, reducing the instruction count where applicable. Additionally, a high-performance open-source RISC-V (RV32 IM) softcore is introduced, optimised for exploring custom SIMD instructions and streaming performance. By providing instruction templates for instruction development in HDL/Verilog, efficient FPGA-based instructions can be developed with few low-level lines of code. In order to improve custom SIMD instruction performance, the softcore\u2019s cache hierarchy is optimised for bandwidth, such as with very wide blocks for the last-level cache. The approach is demonstrated on example memory-intensive applications on an FPGA. Although the exploration is based on the softcore, the goal is to provide a means to experiment with advanced SIMD instructions which could be loaded in future CPUs that feature reconfigurable regions as custom instructions. Finally, we provide some insights on the challenges and effectiveness of such future micro-architectures.", "venue": "ArXiv", "authors": ["Philippos  Papaphilippou", "Paul H. J. Kelly", "Wayne  Luk"], "year": 2021, "n_citations": 0}
{"id": 612414, "s2_id": "c7778f692b3f46cc0cb25a6737465f3e93a7e5bc", "title": "Selectively Delaying Instructions to Prevent Microarchitectural Replay Attacks", "abstract": "MicroScope, and microarchitectural replay attacks in general, take advantage of the characteristics of speculative execution to trap the execution of the victim application in an infinite loop, enabling the attacker to amplify a side-channel attack by executing it indefinitely. Due to the nature of the replay, it can be used to effectively attack security critical trusted execution environments (secure enclaves), even under conditions where a side-channel attack would not be possible. At the same time, unlike speculative side-channel attacks, MicroScope can be used to amplify the correct path of execution, rendering many existing speculative side-channel defences ineffective. In this work, we generalize microarchitectural replay attacks beyond MicroScope and present an efficient defence against them. We make the observation that such attacks rely on repeated squashes of so-called \u201creplay handles\u201d and that the instructions causing the side-channel must reside in the same reorder buffer window as the handles. We propose Delay-on-Squash, a technique for tracking squashed instructions and preventing them from being replayed by speculative replay handles. Our evaluation shows that it is possible to achieve full security against microarchitectural replay attacks with very modest hardware requirements, while still maintaining 97% of the insecure baseline performance.", "venue": "ArXiv", "authors": ["Christos  Sakalis", "Stefanos  Kaxiras", "Magnus  Sj\u00e4lander"], "year": 2021, "n_citations": 0}
{"id": 617259, "s2_id": "3823570da85168ebc1d07dc34af77099f1b146e6", "title": "Quantum Accelerator Stack: A Research Roadmap", "abstract": "This paper presents the definition and implementation of a quantum computer architecture to enable creating a new computational device a quantum computer as an accelerator A key question addressed is what such a quantum computer is and how it relates to the classical processor that controls the entire execution process. In this paper, we present explicitly the idea of a quantum accelerator which contains the full stack of the layers of an accelerator. Such a stack starts at the highest level describing the target application of the accelerator. Important to realise is that qubits are defined as perfect qubits, implying they do not decohere and perform good quantum gate operations. The next layer abstracts the quantum logic outlining the algorithm that is to be executed on the quantum accelerator. In our case, the logic is expressed in the universal quantum-classical hybrid computation language developed in the group, called OpenQL, which visualises the quantum processor as a computational accelerator. We also have to start thinking about how to verify, validate and test the quantum software such that the compiler generates a correct version of the quantum circuit. The OpenQL compiler translates the program to a common assembly language, called cQASM, which can be executed on a quantum simulator. The cQASM represents the instruction set that can be executed by the micro-architecture implemented in the quantum accelerator. In this context, we think about the necessity to develop a quantum operating system that manages all the hardware of the micro-architecture and also makes sure the qubits are in the right place. The layer below the micro-architecture is responsible of the mapping and routing of the qubits on the topology that allows the qubits to be placed such that two-qubit gates can be easily applied on the qubits, even when the nearest-neighbour constraint -constraint needs to be respected. At any moment in the future when we are capable of generating multiple good qubits, the compiler can convert the cQASM to generate the eQASM, which is executable on a particular experimental device incorporating the platform-specific parameters. This way, we are able to distinguish clearly the experimental research towards better qubits, and the industrial and societal applications that need to be developed and executed on a quantum device. We introduce two quantum applications. The first is already quite advanced and deals with the DNA-research that modern medicine is using for all its analyses. The second topic has started a couple of years ago and looks how quantum concepts can be used on financial challenges. For each, we emphasise the use of perfect qubits and supercomputers to compute the result. Keywords\u2014 Quantum computing, parallel architectures, parallel programming, quantum entanglement", "venue": "ArXiv", "authors": ["K.  Bertels", "A.  Sarkar", "A.  Krol", "R.  Budhrani", "J.  Samadi", "E.  Geoffroy", "J.  Matos", "R.  Abreu", "G.  Gielen", "I.  Ashraf"], "year": 2021, "n_citations": 5}
{"id": 620288, "s2_id": "052050849dae81bb652837c356f8e93c4b8f7132", "title": "Rethinking arithmetic for deep neural networks", "abstract": "We consider efficiency in the implementation of deep neural networks. Hardware accelerators are gaining interest as machine learning becomes one of the drivers of high-performance computing. In these accelerators, the directed graph describing a neural network can be implemented as a directed graph describing a Boolean circuit. We make this observation precise, leading naturally to an understanding of practical neural networks as discrete functions, and show that the so-called binarized neural networks are functionally complete. In general, our results suggest that it is valuable to consider Boolean circuits as neural networks, leading to the question of which circuit topologies are promising. We argue that continuity is central to generalization in learning, explore the interaction between data coding, network topology, and node functionality for continuity and pose some open questions for future research. As a first step to bridging the gap between continuous and Boolean views of neural network accelerators, we present some recent results from our work on LUTNet, a novel Field-Programmable Gate Array inference approach. Finally, we conclude with additional possible fruitful avenues for research bridging the continuous and discrete views of neural networks. This article is part of a discussion meeting issue \u2018Numerical algorithms for high-performance computational science\u2019.", "venue": "Philosophical Transactions of the Royal Society A", "authors": ["G. A. Constantinides"], "year": 2020, "n_citations": 2}
{"id": 624613, "s2_id": "0b84b42d01ade6c77a7f158b10015193bc429fba", "title": "2 P2P or Not 2 P2P?", "abstract": "In the hope of stimulating discussion, we present a heuristic decision tree that designers can use to judge how suitable a P2P solution might be for a particular problem. It is based on characteristics of a wide range of P2P systems from the literature, both proposed and deployed. These include budget, resource relevance, trust, rate of system change, and criticality.", "venue": "IPTPS", "authors": ["Mema  Roussopoulos", "Mary  Baker", "David S. H. Rosenthal", "Thomas J. Giuli", "Petros  Maniatis", "Jeffrey C. Mogul"], "year": 2004, "n_citations": 73}
{"id": 624901, "s2_id": "b1cdbfb3dcc8ef21492ce65b19d047122d34b947", "title": "Fault-Tolerant Nanosatellite Computing on a Budget", "abstract": "We present an on-board computer architecture designed for small satellites (< 50kg), which exploits software-fault-tolerance to achieve strong fault coverage with commodity hardware. Micro- and nanosatellites have become popular platforms for a variety of commercial and scientific applications, but today are considered suitable mainly for short and low-priority space missions due to their low reliability. In part, this can be attributed to their reliance upon cheap, low-feature size, COTS components originally designed for embedded and mobile-market applications, for which traditional hardware-voting concepts are ineffective. Software-fault-tolerance has been shown to be effective for such systems, but have largely been ignored by the space industry due to low maturity, as most have only been researched in theory. In practice, designers of payload instruments and miniaturized satellites are usually forced to sacrifice reliability in favor of delivering the level of performance necessary for cutting-edge science and innovative commercial applications. Thus, we developed a set of software measures facilitating fault tolerance based upon thread-level coarse-grain lockstep, which we validated through fault-injection. To offer strong long-term fault coverage, our architecture is implemented as tiled MPSoC on an FPGA, utilizing partial reconfiguration, as well as mixed criticality. This architecture can satisfy the high performance requirements of current and future scientific and commercial space missions at very low cost, while offering the strong fault-coverage guarantees necessary for platform control even for missions with a long duration. This architecture was developed for a 4-year ESA project. Together with two industrial partners, we are developing a prototype to then undergo radiation testing.", "venue": "2018 18th European Conference on Radiation and Its Effects on Components and Systems (RADECS)", "authors": ["Christian M. Fuchs", "Nadia  Murillo", "Aske  Plaat", "Erik van der Kouwe", "Daniel  Harsono", "Todor  Stefanov"], "year": 2018, "n_citations": 2}
{"id": 624980, "s2_id": "c438f3272d23381010b8f52a99d5435a8aa9c7f1", "title": "Majority and Minority Voted Redundancy for Safety-Critical Applications", "abstract": "A new majority and minority voted redundancy (MMR) scheme is proposed that can provide the same degree of fault tolerance as N-modular redundancy (NMR) but with fewer function units and a less sophisticated voting logic. Example NMR and MMR circuits were implemented using a 32/28nm CMOS process and compared. The results show that MMR circuits dissipate less power, occupy less area, and encounter less critical path delay than the corresponding NMR circuits while providing the same degree of fault tolerance. Hence the MMR is a promising alternative to the NMR to efficiently implement high levels of redundancy in safety-critical applications.", "venue": "2018 IEEE 61st International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["P.  Balasubramanian", "Douglas L. Maskell", "Nikos E. Mastorakis"], "year": 2018, "n_citations": 2}
{"id": 626724, "s2_id": "66591a6e543a02b27fcae921f48d45582be04bf7", "title": "On the Correlation of Geographic and Network Proximity at Internet Edges and Its Implications for Mobile Unicast and Multicast Routing", "abstract": "Significant effort has been invested recently to accelerate handover operations in a next generation mobile Internet. Corresponding works for developing efficient mobile multicast management are emergent. Both problems simultaneously expose routing complexity between subsequent points of attachment as a characteristic parameter for handover performance in access networks. As continuous mobility handovers necessarily occur between access routers located in geographic vicinity, this paper investigates on the hypothesis that geographically adjacent edge networks attain a reduced network distances as compared to arbitrary Internet nodes. We therefore evaluate and analyze edge distance distributions in various regions for clustered IP ranges on their geographic location such as a city. We use traceroute to collect packet forwarding path and round-trip-time of each intermediate node to scan-wise derive an upper bound of the node distances. Results of different scanning origins are compared to obtain the best estimation of network distance of each pair. Our results are compared with corresponding analysis of CAIDA Skitter data, overall leading to fairly stable, reproducible edge distance distributions. As a first conclusion on expected impact on handover performance measures, our results indicate a general optimum for handover anticipation time in 802.11 networks of 25 ms.", "venue": "Sixth International Conference on Networking (ICN'07)", "authors": ["Thomas C. Schmidt", "Matthias  W\u00e4hlisch", "Ying  Zhang"], "year": 2007, "n_citations": 4}
{"id": 627816, "s2_id": "066abe4af08190b3a6fa2be870b3e19072187e45", "title": "Characterizing the Communication Requirements of GNN Accelerators: A Model-Based Approach", "abstract": "Relational data present in real world graph representations demands for tools capable to study it accurately. In this regard Graph Neural Network (GNN) is a powerful tool, wherein various models for it have also been developed over the past decade. Recently, there has been a significant push towards creating accelerators that speed up the inference and training process of GNNs. These accelerators, however, do not delve into the impact of their dataflows on the overall data movement and, hence, on the communication requirements. In this paper, we formulate analytical models that capture the amount of data movement in the most recent GNN accelerator frameworks. Specifically, the proposed models capture the dataflows and hardware setup of these accelerator designs and expose their scalability characteristics for a set of hardware, GNN model and input graph parameters. Additionally, the proposed approach provides means for the comparative analysis of the vastly different GNN accelerators.", "venue": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Robert  Guirado", "Akshay  Jain", "Sergi  Abadal", "Eduard  Alarc'on"], "year": 2021, "n_citations": 3}
{"id": 630486, "s2_id": "8abf24446c8a3e0325a2130c8fc09bf44ac7a54d", "title": "Optimizing GPU Cache Policies for MI Workloads*", "abstract": "In recent years, machine intelligence (MI) applications have emerged as a major driver for the computing industry. Optimizing these workloads is important, but complicated. As memory demands grow and data movement overheads increasingly limit performance, determining the best GPU caching policy to use for a diverse range of MI workloads represents one important challenge. To study this, we evaluate 17 MI applications and characterize their behavior using a range of GPU caching strategies. In our evaluations, we find that the choice of caching policy in GPU caches involves multiple performance trade-offs and interactions, and there is no one-size-fits-all GPU caching policy for MI workloads. Based on detailed simulation results, we motivate and evaluate a set of cache optimizations that consistently match the performance of the best static GPU caching policies.", "venue": "2019 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Johnathan  Alsop", "Matthew D. Sinclair", "Srikant  Bharadwaj", "Alexandru  Dutu", "Anthony  Gutierrez", "Onur  Kayiran", "Michael  LeBeane", "Sooraj  Puthoor", "Xianwei  Zhang", "Tsung Tai Yeh", "Bradford M. Beckmann"], "year": 2019, "n_citations": 4}
{"id": 631158, "s2_id": "23503ca8b07531640ab57afaf21e038e8c10413f", "title": "Design and implementation of a digital clock showing digits in Bangla font using microcontroller AT89C4051", "abstract": "In this paper, a digital clock is designed where the microcontroller is used for timing controller and the font of the Bangla digits are designed, and programmed within the microcontroller. The design is cost effective, simple and easy for maintenance.", "venue": "ArXiv", "authors": ["Nasif  Muslim", "Md. Tanvir Adnan", "Mohammad Zahidul Kabir", "Md. Humayun Kabir", "Sheikh Mominul Islam"], "year": 2012, "n_citations": 1}
{"id": 634386, "s2_id": "025ac794b7dc13efa748fe374f877b7ba7b92ac3", "title": "Accelerating Algorithms using a Dataflow Graph in a Reconfigurable System", "abstract": "Abstract In this paper, the acceleration of algorithms using a design of a \ufb01eld pro-grammable gate array (FPGA) as a prototype of a static data\ufb02ow architec-ture is discussed. The static data\ufb02ow architecture using operators intercon-nected by parallel buses was implemented. Accelerating algorithms usinga data\ufb02ow graph in a recon\ufb01gurable system shows the potential for highcomputation rates. The results of benchmarks implemented using the staticdata\ufb02ow architecture are reported at the end of this paper. Keywords: Accelerating algorithms, Recon\ufb01gurable Computing, StaticData\ufb02ow Graph, Modules C to VHDL. 1. Introduction With the advent of recon\ufb01gurable computing, basically using a Field Pro-grammable Gate Array(FPGA), researchers are trying to explore the maxi-mum capacities of these devices, which are: \ufb02exibility, parallelism, optimiza-tion for power, security and real time applications [7, 14].Because of the complexity of the applications and the large possibilitiesto develop systems using FPGAs, many applications to convert algorithmsinto these devices associated with a General Purpose Processor (GPP) usinghigh level language like C and Java is one of the challenges for researchersnowadays, especially for accelerating algorithms [5, 8].The main aim of this project was to accelerate the algorithms whichconvert parts of programs written in C language into a static data\ufb02ow modelimplemented in a FPGA.", "venue": "ArXiv", "authors": ["Jorge Luiz e Silva", "Joelmir Jos\u00e9 Lopes", "Bruno de Abreu Silva", "Antonio Carlos Fernandes da Silva"], "year": 2011, "n_citations": 1}
{"id": 636091, "s2_id": "bc94e6d87809bb9ab3f65d3a6b336bf752611dee", "title": "Mitigating Wordline Crosstalk Using Adaptive Trees of Counters", "abstract": "DRAM technology scaling has the undesirable side effect of degrading cell reliability. One such concern of deeply scaled DRAMs is the increased coupling between adjacent cells, commonly referred to as crosstalk. High access frequency of certain rows in the DRAM may cause data loss in cells of physically adjacent rows due to crosstalk. The malicious exploit of this crosstalk by repeatedly accessing a row to induce this effect is known as row hammering. Additionally, inadvertent row hammering may also occur due to the natural weighted nature of applications' access patterns. In this paper, we analyze the efficiency of existing approaches for mitigating wordline crosstalk and demonstrate that they have been conservatively designed. Given the unbalanced nature of DRAM accesses, a small group of dynamically allocated counters in banks can deterministically detect \"hot\" rows and mitigate crosstalk. Based on our findings, we propose a Counter-based Adaptive Tree (CAT) approach to mitigate wordline crosstalk using adaptive trees of counters to guide appropriate refreshing of vulnerable rows. The key idea is to tune the distribution of the counters to the rows in a bank based on the memory reference patterns. In contrast to deterministic solutions, CAT utilizes fewer counters, making it practically feasible to be implemented on-chip. Compared to existing probabilistic approaches, CAT more precisely refreshes rows vulnerable to crosstalk based on their access frequency. Experimental results on workloads from four benchmark suites show that CAT reduces the Crosstalk Mitigation Refresh Power Overhead in quad-core systems to 7%, which is an improvement over the 21% and 18% incurred in the leading deterministic and probabilistic approaches, respectively. Moreover, CAT incurs very low performance overhead (~0.5%). Hardware synthesis evaluation shows that CAT can be implemented on-chip with only a nominal area overhead.", "venue": "2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Seyed Mohammad Seyedzadeh", "Alex K. Jones", "Rami G. Melhem"], "year": 2018, "n_citations": 20}
{"id": 636914, "s2_id": "78f9df2fe97b9ff22458791fef0ffa3833c68075", "title": "Hardware translation coherence for virtualized systems", "abstract": "To improve system performance, operating systems (OSes) often undertake activities that require modification of virtual-to-physical address translations. For example, the OS may migrate data between physical pages to manage heterogeneous memory devices. We refer to such activities as page remappings. Unfortunately, page remappings are expensive. We show that a big part of this cost arises from address translation coherence, particularly on systems employing virtualization. In response, we propose hardware translation invalidation and coherence or HATRIC, a readily implementable hardware mechanism to piggyback translation coherence atop existing cache coherence protocols. We perform detailed studies using KVM-based virtualization, showing that HATRIC achieves up to 30% performance and 10% energy benefits, for per-CPU area overheads of 0.2%. We also quantify HATRIC's benefits on systems running Xen and find up to 33% performance improvements.", "venue": "2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Zi  Yan", "J\u00e1n  Vesel\u00fd", "Guilherme  Cox", "Abhishek  Bhattacharjee"], "year": 2017, "n_citations": 23}
{"id": 637859, "s2_id": "c9849466d0181cf36b0815592670af5a4b0ccc9a", "title": "APEnet+: high bandwidth 3D torus direct network for petaflops scale commodity clusters", "abstract": "We describe herein the APElink+ board, a PCIe interconnect adapter featuring the latest advances in wire speed and interface technology plus hardware support for a RDMA programming model and experimental acceleration of GPU networking; this design allows us to build a low latency, high bandwidth PC cluster, the APEnet+ network, the new generation of our cost-effective, tens-of-thousands-scalable cluster network architecture. Some test results and characterization of data transmission of a complete testbench, based on a commercial development card mounting an Altera\u00ae FPGA, are provided.", "venue": "ArXiv", "authors": ["Roberto  Ammendola", "Andrea  Biagioni", "Ottorino  Frezza", "Francesca Lo Cicero", "Alessandro  Lonardo", "Pier Stanislao Paolucci", "Davide  Rossetti", "Andrea  Salamon", "Gaetano  Salina", "Francesco  Simula", "Laura  Tosoratto", "Piero  Vicini"], "year": 2011, "n_citations": 24}
{"id": 639180, "s2_id": "18ddeedc444720f55cd8bbc8d56084365830ace7", "title": "FPGA Implementation of a Novel Image Steganography for Hiding Images", "abstract": "As the complexity of current data flow systems and according infrastructure networks increases, the security of data transition through such platforms becomes more important. Thus, different areas of steganography turn to one of the most challengeable topics of current researches. In this paper a novel method is presented to hide an image into the host image and Hardware/Software design is proposed to implement our stagenography system on FPGA- DE2 70 Altera board. The size of the secret image is quadrant of the host image. Host image works as a cipher key to completely distort and encrypt the secret image using XOR operand. Each pixel of the secret image is composed of 8 bits (4 bit-pair) in which each bit-pair is distorted by XORing it with two LSB bits of the host image and putting the results in the location of two LSB bits of host image. The experimental results show the effectiveness of the proposed method compared to the most recently proposed algorithms by considering that the obtained information entropy for encrypt image is approximately equal to 8.", "venue": "ArXiv", "authors": ["Masoom  Nazari", "Mina Zolfy Lighvan", "Ziaeddin Daie Koozekonani", "Ali  Sadeghi"], "year": 2016, "n_citations": 1}
{"id": 639382, "s2_id": "7cc73f4c952f38e678237a57dd7b80d657be915c", "title": "Integration, verification and layout of a complex multimedia SOC", "abstract": "We present our experience of designing a single-chip controller for an advanced digital still camera from specification all the way to mass production. The process involves collaboration with camera system designer, IP vendors, EDA vendors, silicon wafer foundry, package and testing houses, and camera maker. We also co-work with academic research groups to develop a JPEG codec IP and memory BIST and SOC testing methodology. We cover the problems encountered, our solutions, and lessons learned.", "venue": "Design, Automation and Test in Europe", "authors": ["Chien-Liang  Chen", "Jiing-Yuan  Lin", "Youn-Long  Lin"], "year": 2005, "n_citations": 3}
{"id": 639993, "s2_id": "8e629d470618509bcf5fa41d4634e984360e7871", "title": "Queue management in network processors", "abstract": "One of the main bottlenecks when designing a network processing system is very often its memory subsystem. This is mainly due to the state-of-the-art network links operating at very high speeds and to the fact that in order to support advanced quality of service (QoS), a large number of independent queues is desirable. In this paper we analyze the performance bottlenecks of various data memory managers integrated in typical network processing units (NPU). We expose the performance limitations of software implementations utilizing the RISC processing cores typically found in most NPU architectures and we identify the requirements for hardware assisted memory management in order to achieve wire-speed operation at gigabit per second rates. Furthermore, we describe the architecture and performance of a hardware memory manager that fulfills those requirements. This memory manager, although it is implemented in a reconfigurable technology, can provide up to 6.2 Gbit/s of aggregate throughput, while handling 32 K independent queues.", "venue": "Design, Automation and Test in Europe", "authors": ["Ioannis  Papaefstathiou", "Theofanis  Orphanoudakis", "George  Kornaros", "Christoforos  Kachris", "Ioannis  Mavroidis", "Aristides  Nikologiannis"], "year": 2005, "n_citations": 11}
{"id": 642625, "s2_id": "5fd31f7b684ea8e99f5c2cc9781eeee66a12c919", "title": "Security Analysis of the Silver Bullet Technique for RowHammer Prevention", "abstract": "The purpose of this document is to study the security properties of the Silver Bullet algorithm against worst-case RowHammer attacks. We mathematically demonstrate that Silver Bullet, when properly configured and implemented in a DRAM chip, can securely prevent RowHammer attacks. The demonstration focuses on the most representative implementation of Silver Bullet, the patent claiming many implementation possibilities not covered in this demonstration. Our study concludes that Silver Bullet is a promising RowHammer prevention mechanism that can be configured to operate securely against RowHammer attacks at various efficiency-area tradeoff points, supporting relatively small hammer count values (e.g., 1000) and Silver Bullet table sizes (e.g., 1.06KB).", "venue": "ArXiv", "authors": ["Abdullah Giray Yaglik\u00e7i", "Jeremie S. Kim", "Fabrice  Devaux", "Onur  Mutlu"], "year": 2021, "n_citations": 2}
{"id": 643863, "s2_id": "0d9e9500e9ec8343e2e2962322b0ef3088945e86", "title": "Characterizing and Improving the Resilience of Accelerators in Autonomous Robots", "abstract": "Motion planning is a computationally intensive and well-studied problem in autonomous robots. However, motion planning hardware accelerators (MPA) must be soft-error resilient for deployment in safety-critical applications, and blanket application of traditional mitigation techniques is ill-suited due to cost, power, and performance overheads. We propose Collision Exposure Factor (CEF), a novel metric to assess the failure vulnerability of circuits processing spatial relationships, including motion planning. CEF is based on the insight that the safety violation probability increases with the surface area of the physical space exposed by a bit-flip. We evaluate CEF on four MPAs. We demonstrate empirically that CEF is correlated with safety violation probability, and that CEF-aware selective error mitigation provides 12.3\u00d7, 9.6\u00d7, and 4.2\u00d7 lower Failures-In-Time (FIT) rate on average for the same amount of protected memory compared to uniform, bit-position, and access-frequency-aware selection of critical data. Furthermore, we show how to employ CEF to enable fault characterization using 23,000\u00d7 fewer fault injection (FI) experiments than exhaustive FI, and evaluate our FI approach on different robots and MPAs. We demonstrate that CEF-aware FI can provide insights on vulnerable bits in an MPA while taking the same amount of time as uniform statistical FI. Finally, we use the CEF to formulate guidelines for designing soft-error resilient MPAs.", "venue": "ArXiv", "authors": ["Deval  Shah", "Zi Yu Xue", "Karthik  Pattabiraman", "Tor M. Aamodt"], "year": 2021, "n_citations": 0}
{"id": 645130, "s2_id": "743f963797706e4e0c60b55e32dcc0efb3e09dd6", "title": "A Mixed-Precision RISC-V Processor for Extreme-Edge DNN Inference", "abstract": "Low bit-width Quantized Neural Networks (QNNs) enable deployment of complex machine learning models on constrained devices such as microcontrollers (MCUs) by reducing their memory footprint. Fine-grained asymmetric quantization (i.e., different bit-widths assigned to weights and activations on a tensor-by-tensor basis) is a particularly interesting scheme to maximize accuracy under a tight memory constraint. However, the lack of sub-byte instruction set architecture (ISA) support in SoA microprocessors makes it hard to fully exploit this extreme quantization paradigm in embedded MCUs. Support for sub-byte and asymmetric QNNs would require many precision formats and an exorbitant amount of opcode space. In this work, we attack this problem with status-based SIMD instructions: rather than encoding precision explicitly, each operand's precision is set dynamically in a core status register. We propose a novel RISC-V ISA core MPIC (Mixed Precision Inference Core) based on the open-source RI5CY core. Our approach enables full support for mixed-precision QNN inference with 292 different combinations of operands at 16-, 8-, 4- and 2-bit precision, without adding any extra opcode or increasing the complexity of the decode stage. Our results show that MPIC improves both performance and energy efficiency by a factor of 1.1-4.9x when compared to software-based mixed-precision on RI5CY; with respect to commercially available Cortex-M4 and M7 microcontrollers, it delivers 3.6-11.7x better performance and 41-155x higher efficiency.", "venue": "2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Gianmarco  Ottavi", "Angelo  Garofalo", "Giuseppe  Tagliavini", "Francesco  Conti", "Luca  Benini", "Davide  Rossi"], "year": 2020, "n_citations": 6}
{"id": 646447, "s2_id": "312606937a7445dc593e2155b46ffd42dfa5afd0", "title": "GateKeeper-GPU: Fast and Accurate Pre-Alignment Filtering in Short Read Mapping", "abstract": "We introduce GateKeeper-GPU, a fast and accurate pre-alignment filter that efficiently reduces the need for expensive sequence alignment. GateKeeper-GPU improves the filtering accuracy of GateKeeper, and by exploiting the massive parallelism provided by GPU threads it concurrently examines numerous sequence pairs rapidly. GateKeeper-GPU is available at https://github.com/BilkentCompGen/GateKeeper-GPU. Please refer to the preprint at arXiv:2103.14978 for more information.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Z\u00fclal  Bing\u00f6l", "Mohammed  Alser", "Onur  Mutlu", "Ozcan  Ozturk", "Can  Alkan"], "year": 2021, "n_citations": 1}
{"id": 646689, "s2_id": "74eab4c2aa59d1a25836487ed5b4bd7fffd780bd", "title": "Microshift: An Efficient Image Compression Algorithm for Hardware", "abstract": "In this paper, we propose a lossy image compression algorithm called microshift. We employ an algorithm-hardware co-design methodology, yielding a hardware-friendly compression approach with low power consumption. In our method, the image is first micro-shifted, and then the sub-quantized values are further compressed. Two methods, FAST and MRF models, are proposed to recover the bitdepth by exploiting the spatial correlation of natural images. Both methods can decompress images progressively. On an average, our compression algorithm can compress images to 1.25-bits per pixel with a resulting quality that outperforms the state-of-the-art on-chip compression algorithms in both peak signal-to-noise ratio and structual similarity. Then, we propose a hardware architecture and implement the algorithm on an FPGA. The results on the ASIC design further validate the low-hardware complexity and high-power efficiency, showing that our method is promising, particularly for low-power wireless vision sensor networks.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "authors": ["Bo  Zhang", "Pedro V. Sander", "Chi-Ying  Tsui", "Amine  Bermak"], "year": 2019, "n_citations": 0}
{"id": 647697, "s2_id": "b52f76d74b98467f8b78a3a8281fe8fcd4769ace", "title": "A Configurable Memristor-based Finite Impulse Response Filter", "abstract": "There are two main methods to implement FIR filters: software and hardware. In the software method, an FIR filter can be implemented within the processor by programming; it uses too much memory and it is extremely time-consuming while it gives the design more configurability. In most hardware-based implementations of FIR filters, Analog-to-Digital (A/D) and Digital-to-Analog (D/A) converters are mandatory and increase the cost. The most important advantage of hardware implementation of a FIR filter is its higher speed compared to its software counterpart. In this work, considering the advantages of software and hardware approaches, a method to implement direct form FIR filters using analog components and memristors is proposed. Not only the A/D and D/A converters are omitted, but also using memristors avails configurability. A new circuit is presented to handle negative coefficients of the filter and memristance values are calculated using a heuristic method in order to achieve a better accuracy in setting coefficients. Moreover, an appropriate sample and delay topology is employed which overcomes the limitations of the previous research in implementation of high-order filters. Proper operation and usefulness of the proposed structures are all validated via simulation in Cadence.", "venue": "ArXiv", "authors": ["Mohammad  Hemmati", "Vahid  Rashtchi", "Ahmad  Maleki", "Siroos  Toofan"], "year": 2019, "n_citations": 1}
{"id": 647847, "s2_id": "273bc3a555c6afc0279d3b39070b310fcd3ffffa", "title": "architect: Arbitrary-Precision Hardware With Digit Elision for Efficient Iterative Compute", "abstract": "Many algorithms feature an iterative loop that converges to the result of interest. The numerical operations in such algorithms are generally implemented using finite-precision arithmetic, either fixed- or floating-point, most of which operate least-significant digit first. This results in a fundamental problem: if, after some time, the result has not converged, is this because we have not run the algorithm for enough iterations or because the arithmetic in some iterations was insufficiently precise? There is no easy way to answer this question, so users will often over-budget precision in the hope that the answer will always be to run for a few more iterations. We propose a fundamentally new approach: with the appropriate arithmetic able to generate results from most-significant digit first, we show that fixed compute-area hardware can be used to calculate an arbitrary number of algorithmic iterations to arbitrary precision, with both precision and approximant index increasing in lockstep. Consequently, datapaths constructed following our principles demonstrate efficiency over their traditional arithmetic equivalents where the latter\u2019s precisions are either under- or over-budgeted for the computation of a result to a particular accuracy. Use of most-significant digit-first arithmetic additionally allows us to declare certain digits to be stable at runtime, avoiding their recalculation in subsequent iterations and thereby increasing performance and decreasing memory footprints. Versus arbitrary-precision iterative solvers without the optimizations we detail herein, we achieve up-to $16\\times $ performance speedups and $1.9\\times $ memory savings for the evaluated benchmarks.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["He  Li", "James J. Davis", "John  Wickerson", "George A. Constantinides"], "year": 2020, "n_citations": 4}
{"id": 652553, "s2_id": "4f17bd15a6f86730ac2207167ccf36ec9e6c2391", "title": "Processing Data Where It Makes Sense: Enabling In-Memory Computation", "abstract": "Abstract Today\u2019s systems are overwhelmingly designed to move data to computation. This design choice goes directly against at least three key trends in systems that cause performance, scalability and energy bottlenecks: (1)\u00a0data access from memory is already a key bottleneck as applications become more data-intensive and memory bandwidth and energy do not scale well, (2)\u00a0energy consumption is a key constraint in especially mobile and server systems, (3)\u00a0data movement is very expensive in terms of bandwidth, energy and latency, much more so than computation. These trends are especially severely-felt in the data-intensive server and energy-constrained mobile systems of today. At the same time, conventional memory technology is facing many scaling challenges in terms of reliability, energy, and performance. As a result, memory system architects are open to organizing memory in different ways and making it more intelligent, at the expense of higher cost. The emergence of 3D-stacked memory plus logic as well as the adoption of error correcting codes inside DRAM chips, and the necessity for designing new solutions to serious reliability and security issues, such as the RowHammer phenomenon, are an evidence of this trend. In this work, we discuss some recent research that aims to practically enable computation close to data. After motivating trends in applications as well as technology, we discuss at least two promising directions for processing-in-memory (PIM): (1)\u00a0performing massively-parallel bulk operations in memory by exploiting the analog operational properties of DRAM, with low-cost changes, (2)\u00a0exploiting the logic layer in 3D-stacked memory technology to accelerate important data-intensive applications. In both approaches, we describe and tackle relevant cross-layer research, design, and adoption challenges in devices, architecture, systems, and programming models. Our focus is on the development of in-memory processing designs that can be adopted in real computing platforms at low cost.", "venue": "Microprocess. Microsystems", "authors": ["Onur  Mutlu", "Saugata  Ghose", "Juan  G\u00f3mez-Luna", "Rachata  Ausavarungnirun"], "year": 2019, "n_citations": 63}
{"id": 657802, "s2_id": "dc64c3b7791847a62d0b55e86385f9411a6566a6", "title": "System measurement of Intel AEP Optane DIMM", "abstract": "In recent years, memory wall has been a great performance bottleneck of computer system. To overcome it, Non-Volatile Main Memory (NVMM) technology has been discussed widely to provide a much larger main memory capacity. Last year, Intel released AEP Optane DIMM, which provides hundreds of GB capacity as a promising replacement of traditional DRAM memory. But as most key parameters of AEP is not open to users, there is a need to get to know them because they will guide a direction of further NVMM research. In this paper, we focus on measuring performance and architecture features of AEP DIMM. Together, we explore the design of DRAM cache which is an important part of DRAM-AEP hybrid memory system. As a result, we estimate the write latency of AEP DIMM which has not been measured accurately. And, we discover the current design parameters of DRAM cache, such as tag organization, cache associativity and set index mapping. All of these features are first published on academic paper which are greatly helpful to future NVMM optimizations.", "venue": "ArXiv", "authors": ["Tianyue  Lu", "Haiyang  Pan", "Mingyu  Chen"], "year": 2020, "n_citations": 0}
{"id": 657993, "s2_id": "3583c4198d2f60dd38e655f50fd28a5db4935aff", "title": "RFC-HyPGCN: A Runtime Sparse Feature Compress Accelerator for Skeleton-Based GCNs Action Recognition Model with Hybrid Pruning", "abstract": "Skeleton-based Graph Convolutional Networks (GCNs) models for action recognition have achieved excellent prediction accuracy in the field. However, limited by large model and computation complexity, GCNs for action recognition like 2s-AGCN have insufficient power-efficiency and throughput on GPU. Thus, the demand of model reduction and hardware acceleration for low-power GCNs action recognition application becomes continuously higher.To address challenges above, this paper proposes a runtime sparse feature compress accelerator with hybrid pruning method: RFC-HyPGCN. First, this method skips both graph and spatial convolution workloads by reorganizing the multiplication order. Following spatial convolutions channel-pruning dataflow, a coarse-grained pruning method on temporal filters is designed, together with sampling-like fine-grained pruning on time dimension. Later, we come up with an architecture where all convolutional layers are mapped on chip to pursue high throughput. To further reduce storage resource utilization, online sparse feature compress format is put forward. Features are divided and encoded into several banks according to presented format, then bank storage is split into depth-variable mini-banks. Furthermore, this work applies quantization, input-skipping and intra-PE dynamic data scheduling to accelerate the model. In experiments, proposed pruning method is conducted on 2s-AGCN, acquiring 3.0x-8.4x model compression ratio and 73.20% graph-skipping efficiency with balancing weight pruning. Implemented on Xilinx XCKU-115 FPGA, the proposed architecture has the peak performance of 1142 GOP/s and achieves up to 9.19x and 3.91x speedup over high-end GPU NVIDIA 2080Ti and NVIDIA V100, respectively. Compared with latest accelerator for action recognition GCNs models, our design reaches 22.9x speedup and 28.93% improvement on DSP efficiency.", "venue": "2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Dong  Wen", "Jingfei  Jiang", "Jinwei  Xu", "Kang  Wang", "Tao  Xiao", "Yang  Zhao", "Yong  Dou"], "year": 2021, "n_citations": 0}
{"id": 659908, "s2_id": "597d3fc0ab4a9eb587b6d9dc36cf4934ad1966ac", "title": "Phase-Priority based Directory Coherence for Multicore Processor", "abstract": "As the number of cores in a single chip increases, a typical implementation of coherence protocol adds significant hardware and complexity overhead. Besides, the performance of CMP system depends on the data access latency, which is highly affected by coherence protocol and on-chip interconnect. In this paper, we propose PPB (Phase-Priority Based) cache coherence protocol, an optimization of modern directory coherence protocol. We take advantage of the observation that transient states occur in directory coherence protocol, resulting in some unnecessary transient states and stalling. PPB cache coherence protocol decouples a coherence transaction and introduces the idea of phase message. This phase is considered as the priority of the message. Additionally, we also add new priority-based arbitrators in on-chip network to support PPB cache coherence protocol. This mechanism in on-chip network can support effective cache access, which makes the on-chip network more efficient. Our analysis on an execution-driven full system simulator using SPLASH-2 benchmark shows that PPB cache coherence outperforms a MESI based directory, and the number of unnecessary transient states and stalling reduces up to 24%. Also it reported the speedup of 7.4%. Other advantages of this strategy are reduced delay of flits and significantly less energy consumption in on-chip network.", "venue": "ArXiv", "authors": ["Gongming  Li", "Hong  An"], "year": 2013, "n_citations": 1}
{"id": 660965, "s2_id": "e49306a9b6524b67a00b16d72164f217d29064b4", "title": "E-BLOW: E-beam lithography overlapping aware stencil planning for MCC system", "abstract": "Electron beam lithography (EBL) is a promising maskless solution for the technology beyond 14nm logic node. To overcome its throughput limitation, recently the traditional EBL system is extended into MCC system. In this paper, we present E-BLOW, a tool to solve the overlapping aware stencil planning (OSP) problems in MCC system. EBLOW is integrated with several novel speedup techniques, i.e., successive relaxation, dynamic programming and KDTree based clustering, to achieve a good performance in terms of runtime and solution quality. Experimental results show that, compared with previous works, E-BLOW demonstrates better performance for both conventional EBL system and MCC system.", "venue": "2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)", "authors": ["Bei  Yu", "Kun  Yuan", "Jhih-Rong  Gao", "David Z. Pan"], "year": 2013, "n_citations": 19}
{"id": 665380, "s2_id": "6fe35df0e87df1fd45813ea0de144eed5df61279", "title": "Applicability of Partial Ternary Full Adder in Ternary Arithmetic Units", "abstract": "This paper explores whether or not a complete ternary full adder, whose input variables can independently be '0', '1', or '2', is indispensable in the arithmetic blocks of adder, subtractor, and multiplier. Our investigations show that none of the mentioned arithmetic units require a complete ternary full adder. Instead, they can be designed by use of partial ternary full adder, whose input carry never becomes '2'. Furthermore, some new ternary compressors are proposed in this paper without the requirement of complete ternary full adder. The usage of partial ternary full adder can help circuit designers to simplify their designs, especially in transistor level.", "venue": "ArXiv", "authors": ["Aida Ghorbani Asibelagh", "Reza Faghih Mirzaee"], "year": 2019, "n_citations": 0}
{"id": 665392, "s2_id": "da955aa5b20cb3abaf3bd80fd0f3fb9ec9491936", "title": "Automated flow for compressing convolution neural networks for efficient edge-computation with FPGA", "abstract": "Deep convolutional neural networks (CNN) based solutions are the current state- of-the-art for computer vision tasks. Due to the large size of these models, they are typically run on clusters of CPUs or GPUs. However, power requirements and cost budgets can be a major hindrance in adoption of CNN for IoT applications. Recent research highlights that CNN contain significant redundancy in their structure and can be quantized to lower bit-width parameters and activations, while maintaining acceptable accuracy. Low bit-width and especially single bit-width (binary) CNN are particularly suitable for mobile applications based on FPGA implementation, due to the bitwise logic operations involved in binarized CNN. Moreover, the transition to lower bit-widths opens new avenues for performance optimizations and model improvement. In this paper, we present an automatic flow from trained TensorFlow models to FPGA system on chip implementation of binarized CNN. This flow involves quantization of model parameters and activations, generation of network and model in embedded-C, followed by automatic generation of the FPGA accelerator for binary convolutions. The automated flow is demonstrated through implementation of binarized \"YOLOV2\" on the low cost, low power Cyclone- V FPGA device. Experiments on object detection using binarized YOLOV2 demonstrate significant performance benefit in terms of model size and inference speed on FPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire automated flow from trained models to FPGA synthesis can be completed within one hour.", "venue": "ArXiv", "authors": ["Farhan  Shafiq", "Takato  Yamada", "Antonio T. Vilchez", "Sakyasingha  Dasgupta"], "year": 2017, "n_citations": 3}
{"id": 673539, "s2_id": "f7abc664323a3ad3096b32646f214205018b7bd2", "title": "An infrastructure to functionally test designs generated by compilers targeting FPGAs", "abstract": "The paper presents an infrastructure to test the functionality of the specific architectures output by a highlevel compiler targeting dynamically reconfigurable hardware. It results in a suitable scheme to verify the architectures generated by the compiler, each time new optimization techniques are included or changes in the compiler are performed. We believe this kind of infrastructure is important to verify, by functional simulation, further research techniques, as far as compilation to field-programmable gate array (FPGA) platforms is concerned.", "venue": "Design, Automation and Test in Europe", "authors": ["Rui  Rodrigues", "Jo\u00e3o M. P. Cardoso"], "year": 2005, "n_citations": 6}
{"id": 673992, "s2_id": "7a0eaa084067f9ca5d00d0b3832ca6e437e86255", "title": "Low-Latency Asynchronous Logic Design for Inference at the Edge", "abstract": "Modern internet of things (IoT) devices leverage machine learning inference using sensed data on-device rather than offloading them to the cloud. Commonly known as inference at-the-edge, this gives many benefits to the users, including personalization and security. However, such applications demand high energy efficiency and robustness. In this paper we propose a method for reduced area and power overhead of self-timed early-propagative asynchronous inference circuits, designed using the principles of learning automata. Due to natural resilience to timing as well as logic underpinning, the circuits are tolerant to variations in environment and supply voltage whilst enabling the lowest possible latency. Our method is exemplified through an inference datapath for a low power machine learning application. The circuit builds on the Tsetlin machine algorithm further enhancing its energy efficiency. Average latency of the proposed circuit is reduced by 10\u00d7 compared with the synchronous implementation whilst maintaining similar area. Robustness of the proposed circuit is proven through post-synthesis simulation with 0.25 V to 1.2 V supply. Functional correctness is maintained and latency scales with gate delay as voltage is decreased.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Adrian  Wheeldon", "Alex  Yakovlev", "Rishad  Shafik", "Jordan  Morris"], "year": 2021, "n_citations": 4}
{"id": 677633, "s2_id": "bbb8d94b2b3b6dbad6127ff020b1992be4553eb1", "title": "CBP: Coordinated management of cache partitioning, bandwidth partitioning and prefetch throttling", "abstract": "Reducing the average memory access time is crucial for improving the performance of applications running on multicore architectures. With workload consolidation this becomes increasingly challenging due to shared resource contention. Techniques for partitioning of shared resources - cache and bandwidth - and prefetch throttling have been proposed to mitigate contention and reduce the average memory access time. However, existing proposals only employ a single or a subset of these techniques and are therefore not able to exploit the full potential of coordinated management of cache, bandwidth and prefetching. Our characterization results show that application performance, in several cases, is sensitive to prefetching, cache and bandwidth allocation altogether. Furthermore, the results show that managing these together provides higher performance potential during workload consolidation as it enables more resource trade-offs. In this paper, we propose CBP a coordination mechanism for dynamically managing prefetching throttling, cache and bandwidth partitioning, in order to reduce average memory access time and improve performance. CBP works by employing individual resource managers to determine the appropriate setting for each resource and a coordinating mechanism in order to enable inter-resource trade-offs. Our evaluation on a 16-core CMP shows that CBP, on average, improves performance by 11% compared to the state-of-the-art technique that manages cache partitioning and prefetching and by 50% compared to the baseline without cache partitioning, bandwidth partitioning and prefetch throttling.", "venue": "2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)", "authors": ["Nadja Ramh\u00f6j Holtryd", "Madhavan  Manivannan", "Per  Stenstr\u00f6m", "Miquel  Peric\u00e0s"], "year": 2021, "n_citations": 0}
{"id": 684446, "s2_id": "328f0495cf2d2b8de9c2f50568ca62ffc6577796", "title": "A signal processor for Gaussian message passing", "abstract": "In this paper, we present a novel signal processing unit built upon the theory of factor graphs, which is able to address a wide range of signal processing algorithms. More specifically, the demonstrated factor graph processor (FGP) is tailored to Gaussian message passing algorithms. We show how to use a highly configurable systolic array to solve the message update equations of nodes in a factor graph efficiently. A proper instruction set and compilation procedure is presented. In a recursive least squares channel estimation example we show that the FGP can compute a message update faster than a state-of-the-art DSP. The results demonstrate the usabilty of the FGP architecture as a flexible HW accelerator for signal-processing and communication systems.", "venue": "2014 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Harald  Kr\u00f6ll", "Stefan  Zwicky", "Reto  Odermatt", "Lukas  Bruderer", "Andreas Peter Burg", "Qiuting  Huang"], "year": 2014, "n_citations": 0}
{"id": 688496, "s2_id": "47b87a5703ecba6cd443a5e7fbae400e821acb61", "title": "IMCRYPTO: An In-Memory Computing Fabric for AES Encryption and Decryption", "abstract": "This paper proposes IMCRYPTO, an in-memory computing (IMC) fabric for accelerating AES encryption and decryption. IMCRYPTO employs a unified structure to implement encryption and decryption in a single hardware architecture, with combined (Inv)SubBytes and (Inv)MixColumns steps. Because of this step-combination, as well as the high parallelism achieved by multiple units of random-access memory (RAM) and random-access/content-addressable memory (RA/CAM) arrays, IMCRYPTO achieves high throughput encryption and decryption without sacrificing area and power consumption. Additionally, due to the integration of a RISC-V core, IMCRYPTO offers programmability and flexibility. IMCRYPTO improves the throughput per area by a minimum (maximum) of 3.3\u00d7 (223.1\u00d7) when compared to previous ASICs/IMC architectures for AES128 encryption. Projections show added benefit from emerging technologies of up to 5.3\u00d7 to the area-delay-power product of IMCRYPTO.", "venue": "ArXiv", "authors": ["Dayane  Reis", "Haoran  Geng", "Michael  Niemier", "Xiaobo Sharon Hu"], "year": 2021, "n_citations": 0}
{"id": 690895, "s2_id": "8391bbf2b281b87bc38297f926b1c0fff7099093", "title": "UltraShare: FPGA-based Dynamic Accelerator Sharing and Allocation", "abstract": "Despite all the available commercial and open-source frameworks to ease deploying FPGAs in accelerating applications, the current schemes fail to support sharing multiple accelerators among various applications. There are three main features that an accelerator sharing scheme requires to support: exploiting dynamic parallelism of multiple accelerators, sharing accelerators among multiple applications, and providing a nonblocking congestion-free environment for multiple applications to call multiple accelerators. In this paper, we developed a scalable fully functional hardware controller, called UltraShare, with a supporting software stack that provides a dynamic accelerator sharing scheme through an accelerators grouping mechanism. UltraShare allows software applications to fully utilize FPGA accelerators in a non-blocking congestion-free environment. Our experimental results for a simple scenario of a combination of three streaming accelerators invocation show an improvement of up to 8x in throughput of the accelerators by removing accelerators idle times.", "venue": "2019 International Conference on ReConFigurable Computing and FPGAs (ReConFig)", "authors": ["Siavash  Rezaei", "Eli  Bozorgzadeh", "Kanghee  Kim"], "year": 2019, "n_citations": 4}
{"id": 691862, "s2_id": "9f1e5b45973e960d4595c7cc3d334c0517e789d3", "title": "An efficient reverse-lookup table based strategy for solving the synonym and cache coherence problem in virtually indexed, virtually tagged caches", "abstract": "Virtually indexed and virtually tagged (VIVT) caches are an attractive option for micro-processor level-1 caches, because of their fast response time and because they are cheaper to implement than more complex caches such as virtually-indexed physical-tagged (VIPT) caches. The level-1 VIVT cache becomes even simpler to construct if it is implemented as a direct-mapped cache (VIVT-DM cache). However, VIVT and VIVTDM caches have some drawbacks. When the number of sets in the cache is larger than the smallest page size, there is a possibility of synonyms (two or more virtual addresses mapped to the same physical address) existing in the cache. Further, maintenance of cache coherence across multiple processors requires a physical to virtual translation mechanism in the hardware. We describe a simple, efficient reverse lookup table based approach to address the synonym and the coherence problems in VIVT (both set associative and direct-mapped) caches. In particular, the proposed scheme does not disturb the critical memory access paths in a typical micro-processor, and requires a low overhead for its implementation. We have implemented and validated the scheme in the AJIT 32-bit microprocessor core (an implementation of the SPARC-V8 ISA) and the implementation uses approximately 2% of the gates and 5.3% of the memory bits in the processor core.", "venue": "ArXiv", "authors": ["Madhav P. Desai", "Aniket  Deshmukh"], "year": 2021, "n_citations": 0}
{"id": 693855, "s2_id": "45a66fe41318101e6fe72274aec73262a638a4be", "title": "Reliability-centric high-level synthesis", "abstract": "The importance of addressing soft errors in both safety critical applications and commercial consumer products is increasing, mainly due to ever shrinking geometries, higher-density circuits, and employment of power-saving techniques such as voltage scaling and component shut-down. As a result, it is becoming necessary to treat reliability as a first-class citizen in system design. In particular, reliability decisions taken early in system design can have significant benefits in terms of design quality. Motivated by this observation, this paper presents a reliability-centric high-level synthesis approach that addresses the soft error problem. The proposed approach tries to maximize reliability of the design while observing the bounds on area and performance, and makes use of our reliability characterization of hardware components such as adders and multipliers. We implemented the proposed approach, performed experiments with several designs, and compared the results with those obtained by a prior proposal.", "venue": "Design, Automation and Test in Europe", "authors": ["Suleyman  Tosun", "Nazanin  Mansouri", "Ercument  Arvas", "Mahmut T. Kandemir", "Yuan  Xie"], "year": 2005, "n_citations": 69}
{"id": 698185, "s2_id": "ea05dea243e120469f1aab82ae326282816a7b6a", "title": "Testing compilers for programmable switches through switch hardware simulation", "abstract": "Programmable switches have emerged as powerful and flexible alternatives to fixed-function forwarding devices. But because of the unique hardware constraints of network switches, the design and implementation of compilers targeting these devices is tedious and error-prone. Despite the important role that compilers play in software development, there is a dearth of tools for testing compilers for programmable network devices. We present Druzhba, a programmable switch simulator used for testing compilers targeting programmable packet-processing substrates. We show that we can model the low-level behavior of a switch's programmable hardware. We further show how compiler developers can target Druzhba as a compiler backend. Generated machine code programs are fed into Druzhba and tested using a fuzzing-based approach that allows compiler developers to test the correctness of their compilers. Using a program-synthesis-based compiler as a case study, we demonstrate how Druzhba has been successful in testing compiler-generated machine code for our simulated switch pipeline instruction set.", "venue": "CoNEXT", "authors": ["Michael D. Wong", "Aatish  Varma", "Anirudh  Sivaraman"], "year": 2020, "n_citations": 1}
{"id": 699284, "s2_id": "c7fe31c57af039474350b16965e66c7c86794d28", "title": "FAST: DNN Training Under Variable Precision Block Floating Point with Stochastic Rounding", "abstract": "Block Floating Point (BFP) can efficiently support quantization for Deep Neural Network (DNN) training by providing a wide dynamic range via a shared exponent across a group of values. In this paper, we propose a Fast First, Accurate Second Training (FAST) system for DNNs, where the weights, activations, and gradients are represented in BFP. FAST supports matrix multiplication with variable precision BFP input operands, enabling incremental increases in DNN precision throughout training. By increasing the BFP precision across both training iterations and DNN layers, FAST can greatly shorten the training time while reducing overall hardware resource usage. Our FAST MultiplerAccumulator (fMAC) supports dot product computations under multiple BFP precisions. We validate our FAST system on multiple DNNs with different datasets, demonstrating a 2-6\u00d7 speedup in training on a single-chip platform over prior work based on mixed-precision or block floating point number systems while achieving similar performance in validation accuracy.", "venue": "ArXiv", "authors": ["Sai Qian Zhang", "Bradley  McDanel", "H. T. Kung"], "year": 2021, "n_citations": 0}
{"id": 700012, "s2_id": "9d887bd934ed022691aca3360feebc35464c4b6a", "title": "A Tapered Floating Point Extension for the Redundant Signed Radix 2 System Using the Canonical Recoding", "abstract": "A tapered floating point encoding is proposed which uses the redundant signed radix 2 system and is based on the canonical recoding. By making use of ternary technology, the encoding has a dynamic range exceeding that of the IEEE 7541985 Standard for Floating Point Arithmetic (IEEE-754-1985), and precision equal to or better than that of the IEEE-7541985 system and the recently proposed Posit system when equal input sizes are compared. In addition, the encoding is capable of supporting several proposed extensions, including extensions to integers, boolean values, complex numbers, higher number systems, low-dimensional vectors, and system artifacts such as machine instructions. A detailed analytic comparison is provided between the proposed encoding, the IEEE-754-1985 system, and the recently proposed Posit number system.", "venue": "ArXiv", "authors": ["Lucius T. Schoenbaum"], "year": 2021, "n_citations": 0}
{"id": 705896, "s2_id": "6683063210e5a51fe111a33255df4371d33512b3", "title": "Endurance-Aware Mapping of Spiking Neural Networks to Neuromorphic Hardware", "abstract": "Neuromorphic computing systems are embracing memristors to implement high density and low power synaptic storage as crossbar arrays in hardware. These systems are energy efficient in executing Spiking Neural Networks (SNNs). We observe that long bitlines and wordlines in a memristive crossbar are a major source of parasitic voltage drops, which create current asymmetry. Through circuit simulations, we show the significant endurance variation that results from this asymmetry. Therefore, if the critical memristors (ones with lower endurance) are overutilized, they may lead to a reduction of the crossbar\u2019s lifetime. We propose eSpine, a novel technique to improve lifetime by incorporating the endurance variation within each crossbar in mapping machine learning workloads, ensuring that synapses with higher activation are always implemented on memristors with higher endurance, and vice versa. eSpine works in two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to partition a workload into clusters of neurons and synapses, where each cluster can fit in a crossbar. Second, it uses an instance of Particle Swarm Optimization (PSO) to map clusters to tiles, where the placement of synapses of a cluster to memristors of a crossbar is performed by analyzing their activation within the workload. We evaluate eSpine for a state-of-the-art neuromorphic hardware model with phase-change memory (PCM)-based memristors. Using 10 SNN workloads, we demonstrate a significant improvement in the effective lifetime.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Twisha  Titirsha", "Shihao  Song", "Anup  Das", "Jeffrey  Krichmar", "Nikil  Dutt", "Nagarajan  Kandasamy", "Francky  Catthoor"], "year": 2021, "n_citations": 11}
{"id": 706243, "s2_id": "ac52711e476a454c5f65df661f5462f53251d892", "title": "A Survey on Hardware Implementations of Elliptic Curve Cryptosystems", "abstract": "In the past two decades, Elliptic Curve Cryptography (ECC) have become increasingly advanced. ECC, with much smaller key sizes, offers equivalent security when compared to other asymmetric cryptosystems. In this survey, an comprehensive overview of hardware implementations of ECC is provided. We first discuss different elliptic curves, point multiplication algorithms and underling finite field operations over binary fields F2m and prime fields Fp which are used in the literature for hardware implementation. Then methods, steps and considerations of ECC implementation are presented. The implementations of the ECC are categorized in two main groups based on implementation technologies consist of field programmable gate array (FPGA) based implementations and application specific integrated circuit (ASIC) implementations. Therefore, in these categories to have a better presentation and comparison, the implementations are presented and distinguished based on type of finite fields. The best and newest structures in the literature are described in more details for overall presentation of architectures and approaches in each group of implementations. High-speed implementation is an important factor in the ECC applications such as network servers. Also in smart cards, Wireless Sensor Networks (WSN) and Radio Frequency Identification (RFID) tags require to low-cost and lightweight implementations. Therefore, implementation methods related to these applications are explored. In addition, a classification of the previous works in terms of scalability, flexibility, performance and cost effectiveness is provided. Finally, some words and techniques about future works that should be considered are provided.", "venue": "ArXiv", "authors": ["Bahram  Rashidi"], "year": 2017, "n_citations": 8}
{"id": 707744, "s2_id": "5e0b354db1c0b47d50b01ada6eafbb899fdecaca", "title": "SIMF: Single-Instruction Multiple-Flush Mechanism for Processor Temporal Isolation", "abstract": "Microarchitectural timing attacks are a type of information leakage attack, which exploit the time-shared microarchitectural components, such as caches, translation look-aside buffers (TLBs), branch prediction unit (BPU), and speculative execution, in modern processors to leak critical information from a victim process or thread. To mitigate such attacks, the mechanism for flushing the on-core state is extensively used by operating-system-level solutions, since on-core state is too expensive to partition. In these systems, the flushing operations are implemented in software (using cache maintenance instructions), which severely limit the efficiency of timing attack protection. \nTo bridge this gap, we propose specialized hardware support, a single-instruction multiple-flush (SIMF) mechanism to flush the core-level state, which consists of L1 caches, BPU, TLBs, and register file. We demonstrate SIMF by implementing it as an ISA extension, i.e., flushx instruction, in scalar in-order RISC-V processor. The resultant processor is prototyped on Xilinx ZCU102 FPGA and validated with state-of-art seL4 microkernel, Linux kernel in multi-core scenarios, and a cache timing attack. Our evaluation shows that SIMF significantly alleviates the overhead of flushing by more than a factor of two in execution time and reduces dynamic instruction count by orders-of-magnitude.", "venue": "ArXiv", "authors": ["Tuo  Li", "Bradley  Hopkins", "Sri  Parameswaran"], "year": 2020, "n_citations": 1}
{"id": 708055, "s2_id": "948437ff78dcb67312393399f359b9dbfeae026e", "title": "Logic BIST: State-of-the-Art and Open Problems", "abstract": "Many believe that in-field hardware faults are too rare in practice to justify the need for Logic Built-In Self-Test (LBIST) in a design. Until now, LBIST was primarily used in safety-critical applications. However, this may change soon. First, even if costly methods like burn-in are applied, it is no longer possible to get rid of all latent defects in devices at leading- edge technology. Second, demands for high reliability spread to consumer electronics as smartphones replace our wallets and IDs. However, today many ASIC vendors are reluctant to use LBIST. In this paper, we describe the needs for successful deployment of LBIST in the industrial practice and discuss how these needs can be addressed. Our work is hoped to attract a wider attention to this important research topic. Index Terms\u2014LBIST, pseudo-random pattern, LFSR, MISR, in-field test. I. INTRODUCTION A wide-spread opinion is that in-field hardware faults are too rare in practice to justify the need for adding Logic Built-In Self-Test (LBIST) into a design. Until now, LBIST found its use mainly in safety-critical (automotive, medical, military), mission-critical (deep-space, aviation) and high-availability (telecom) applications. However, this may change soon. We expect that, in process technologies below 22nm, LBIST will become compulsory for Application-Specific Integrated Circuits (ASICs), Application-Specific Standard Products (AS- SPs) and complex commercial ICs. The reasons for this are twofold.", "venue": "ArXiv", "authors": ["Nan  Li", "Gunnar  Carlsson", "Elena  Dubrova", "Kim  Petersen"], "year": 2015, "n_citations": 4}
{"id": 715823, "s2_id": "7cecc6b017f312653f90a856a266c5b5fe2816a2", "title": "RISC-NN: Use RISC, NOT CISC as Neural Network Hardware Infrastructure", "abstract": "Neural Networks (NN) have been proven to be powerful tools to analyze Big Data. However, traditional CPUs cannot achieve the desired performance and/or energy efficiency for NN applications. Therefore, numerous NN accelerators have been used or designed to meet these goals. These accelerators all fall into three categories: GPGPUs, ASIC NN Accelerators and CISC NN Accelerators. GPGPUs achieve general purpose and high computing throughput, but cannot provide desired energy efficiency because their stream architecture cannot achieve efficient data reuse required by NN applications. ASIC NN Accelerators achieve best performance or energy efficiency through advanced data reuse optimization, however, they only support limited NN use cases. CISC NN Accelerators aim to achieve both general purpose and high energy efficiency by decomposing NN applications into multiple relatively simple matrix or vector CISC instructions. Though CISC NN Accelerators can achieve considerable smaller memory footprint than GPGPU thus improve energy efficiency; they still fail to provide same level of data reuse optimization achieved by ASIC NN Accelerators because of the inherited poor pragrammability of their CISC architecture. We argue that, for NN Accelerators, RISC is a better design choice than CISC, as is the case with general purpose processors. We propose RISC-NN, a novel many-core RISC-based NN accelerator that achieves high expressiveness and high parallelism and features strong programma1Taoran Xiang and Lunkai Zhang contribute equally to the article. 2Taoran Xiang, Shuqian An, Xiaochun Ye, Mingzhe Zhang, Yanhuan Liu, Mingyu Yan, Da Wang, Hao Zhang, Wenming Li, Ninghui Sun, and Dongrui Fan are with the State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China E-mail: taoran.xtr@gmail.com {anshuqian, yexiaochun, zhangmingzhe, liuyanhuan, yanmingyu, wangda, zhanghao, liwenming, sunninghui, fandr}@ict.ac.cn 3Lunkai Zhang contributed to this project when he was a postdoctoral researcher at the University of Chicago. He is now working at Intel. E-mail: lunkai.zhang.1984@gmail.com 4Dongrui Fan is with School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing 100190, China. bility and low control-hardware costs. We show that, RISCNN can implement all the necessary instructions of stateof-the-art CISC NN Accelerators; in the meantime, RISCNN manages to achieve advanced optimization such as multiple-level data reuse and support for Sparse NN applications which previously only existed in ASIC NN Accelerators. Experiment results show that, RISC-NN achieves on average 11.88\u00d7 performance efficiency compared with state-of-the-art Nvidia TITAN Xp GPGPU for various NN applications. RISC-NN also achieves on average 1.29\u00d7, 8.37\u00d7 and 21.71\u00d7 performance efficiency over CISC-based TPU in CNN, MLP and LSTM applications, respectively. Finally, RISC-NN can achieve additional 26.05% performance improvement and 33.13% energy reduction after applying pruning for Sparse NN applications.", "venue": "ArXiv", "authors": ["Taoran  Xiang", "Lunkai  Zhang", "Shuqian  An", "Xiaochun  Ye", "Mingzhe  Zhang", "Yanhuan  Liu", "Mingyu  Yan", "Da  Wang", "Hao  Zhang", "Wenming  Li", "Ninghui  Sun", "Dongrui  Fan"], "year": 2021, "n_citations": 0}
{"id": 717717, "s2_id": "f54f08a367f2f2a5c3bd75cc43bde6c468aa6a83", "title": "Hardware Trojan by Hot Carrier Injection", "abstract": "This paper discusses how hot carrier injection (HCI) can be exploited to create a trojan that will cause hardware failures. The trojan is produced not via additional logic circuitry but by controlled scenarios that maximize and accelerate the HCI effect in transistors. These scenarios range from manipulating the manufacturing process to varying the internal voltage distribution. This new type of trojan is difficult to test due to its gradual hardware degradation mechanism. This paper describes the HCI effect, detection techniques and discusses the possibility for maliciously induced HCI trojans.", "venue": "ArXiv", "authors": ["Yuriy  Shiyanovskii", "Francis G. Wolff", "Christos A. Papachristou", "Daniel J. Weyer", "W.  Clay"], "year": 2009, "n_citations": 5}
{"id": 717800, "s2_id": "71cf87600000b78082d639f992496afa18aeb9fe", "title": "DBOS: A Proposal for a Data-Centric Operating System", "abstract": "Current operating systems are complex systems that were designed before today's computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today's most important applications and propose a plan for the development of a DBOS proof of concept.", "venue": "ArXiv", "authors": ["Michael  Cafarella", "David  DeWitt", "Vijay  Gadepally", "Jeremy  Kepner", "Christos  Kozyrakis", "Tim  Kraska", "Michael  Stonebraker", "Matei  Zaharia"], "year": 2020, "n_citations": 5}
{"id": 719468, "s2_id": "7553c6f64b391887b6ad7481aded077b9aec9e39", "title": "Sphynx: A Shared Instruction Cache Exporatory Study", "abstract": "The Sphynx project was an exploratory study to discover what might be done to improve the heavy replication of in- structions in independent instruction caches for a massively parallel machine where a single program is executing across all of the cores. While a machine with only many cores (fewer than 50) might not have any issues replicating the instructions for each core, as we approach the era where thousands of cores can be placed on one chip, the overhead of instruction replication may become unacceptably large. We believe that a large amount of sharing should be possible when the ma- chine is configured for all of the threads to issue from the same set of instructions. We propose a technique that allows sharing an instruction cache among a number of independent processor cores to allow for inter-thread sharing and reuse of instruction memory. While we do not have test cases to demonstrate the potential magnitude of performance gains that could be achieved, the potential for sharing reduces the die area required for instruction storage on chip.", "venue": "ArXiv", "authors": ["Dong-hyeon  Park", "Akhil  Bagaria", "Fabiha  Hannan", "Eric  Storm", "Josef B. Spjut"], "year": 2014, "n_citations": 0}
{"id": 719976, "s2_id": "df06259f5c41e5d93a1a29a8ef586f9613b529bf", "title": "An Overview of In-memory Processing with Emerging Non-volatile Memory for Data-intensive Applications", "abstract": "The conventional von Neumann architecture has been revealed as a major performance and energy bottleneck for rising data-intensive applications. The decade-old idea of leveraging in-memory processing to eliminate substantial data movements has returned and led extensive research activities. The effectiveness of in-memory processing heavily relies on memory scalability, which cannot be satisfied by traditional memory technologies. Emerging non-volatile memories (eNVMs) that pose appealing qualities such as excellent scaling and low energy consumption, on the other hand, have been heavily investigated and explored for realizing in-memory processing architecture. In this paper, we summarize the recent research progress in eNVM-based in-memory processing from various aspects, including the adopted memory technologies, locations of the in-memory processing in the system, supported arithmetics, as well as applied applications.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Bing  Li", "Bonan  Yan", "Hai  Li"], "year": 2019, "n_citations": 9}
{"id": 722822, "s2_id": "61f3f1ea6332f473739aceb5db339a00ee3675e2", "title": "Accelerating Bulk Bit-Wise X(N)OR Operation in Processing-in-DRAM Platform", "abstract": "With Von-Neumann computing architectures struggling to address computationally- and memory-intensive big data analytic task today, Processing-in-Memory (PIM) platforms are gaining growing interests. In this way, processing-in-DRAM architecture has achieved remarkable success by dramatically reducing data transfer energy and latency. However, the performance of such system unavoidably diminishes when dealing with more complex applications seeking bulk bit-wise X(N)OR- or addition operations, despite utilizing maximum internal DRAM bandwidth and in-memory parallelism. In this paper, we develop DRIM platform that harnesses DRAM as computational memory and transforms it into a fundamental processing unit. DRIM uses the analog operation of DRAM sub-arrays and elevates it to implement bit-wise X(N)OR operation between operands stored in the same bit-line, based on a new dual-row activation mechanism with a modest change to peripheral circuits such sense amplifiers. The simulation results show that DRIM achieves on average 71x and 8.4x higher throughput for performing bulk bit-wise X(N)OR-based operations compared with CPU and GPU, respectively. Besides, DRIM outperforms recent processing-in-DRAM platforms with up to 3.7x better performance.", "venue": "ArXiv", "authors": ["Shaahin  Angizi", "Deliang  Fan"], "year": 2019, "n_citations": 4}
{"id": 723905, "s2_id": "90a9c1d5d19b8a170b902859c90c7256715a8163", "title": "A multi-Gbps unrolled hardware list decoder for a systematic polar code", "abstract": "Polar codes are a new class of block codes with an explicit construction that provably achieve the capacity of various communications channels, even with the low-complexity successive-cancellation (SC) decoding algorithm. Yet, the more complex successive-cancellation list (SCL) decoding algorithm is gathering more attention lately as it significantly improves the error-correction performance of short-to moderate-length polar codes, especially when they are concatenated with a cyclic redundancy check code. However, as SCL decoding explores several decoding paths, existing hardware implementations tend to be significantly slower than SC-based decoders. In this paper, we show how the unrolling technique, which has already been used in the context of SC decoding, can be adapted to SCL decoding yielding a multi-Gbps SCL-based polar decoder with an error-correction performance that is competitive when compared to an LDPC code of similar length and rate. Post-place-and-route ASIC results for 28 nm CMOS are provided showing that this decoder can sustain a throughput greater than 10 Gbps at 468 MHz with an energy efficiency of 7.25 pJ/bit.", "venue": "2016 50th Asilomar Conference on Signals, Systems and Computers", "authors": ["Pascal  Giard", "Alexios  Balatsoukas-Stimming", "Thomas Christoph M\u00fcller", "Andreas Peter Burg", "Claude  Thibeault", "Warren J. Gross"], "year": 2016, "n_citations": 6}
{"id": 728290, "s2_id": "013c6758cfac2a72fc70485567fb4e0e368f2899", "title": "What Your DRAM Power Models Are Not Telling You: Lessons from a Detailed Experimental Study", "abstract": "Main memory (DRAM) consumes as much as half of the total system power in a computer today, due to the increasing demand for memory capacity and bandwidth. There is a growing need to understand and analyze DRAM power consumption, which can be used to research new DRAM architectures and systems that consume less power. A major obstacle against such research is the lack of detailed and accurate information on the power consumption behavior of modern DRAM devices. Researchers have long relied on DRAM power models that are predominantly based off of a set of standardized current measurements provided by DRAM vendors, called IDD values. Unfortunately, we find that state-of-the-art DRAM power models are often highly inaccurate when compared with the real power consumed by DRAM. This is because existing DRAM power models (1) are based off of the worst-case power consumption of devices, as vendor specifications list the current consumed by the most power-hungry device sold; (2) do not capture variations in DRAM power consumption due to different data value patterns; and (3) do not account for any variation across different devices or within a device.", "venue": "SIGMETRICS", "authors": ["Saugata  Ghose", "Abdullah Giray Yaglik\u00e7i", "Raghav  Gupta", "Donghyuk  Lee", "Kais  Kudrolli", "William X. Liu", "Hasan  Hassan", "Kevin K. Chang", "Niladrish  Chatterjee", "Aditya  Agrawal", "Mike  O'Connor", "Onur  Mutlu"], "year": 2018, "n_citations": 0}
{"id": 737811, "s2_id": "9bc25860c60974331283216ef16425095477f84a", "title": "Procrustes: a Dataflow and Accelerator for Sparse Deep Neural Network Training", "abstract": "The success of DNN pruning has led to the development of energy-efficient inference accelerators that support pruned models with sparse weight and activation tensors. Because the memory layouts and dataflows in these architectures are optimized for the access patterns during inference, however, they do not efficiently support the emerging sparse training techniques.In this paper, we demonstrate (a) that accelerating sparse training requires a co-design approach where algorithms are adapted to suit the constraints of hardware, and (b) that hardware for sparse DNN training must tackle constraints that do not arise in inference accelerators. As proof of concept, we adapt a sparse training algorithm to be amenable to hardware acceleration; we then develop dataflow, data layout, and load-balancing techniques to accelerate it.The resulting system is a sparse DNN training accelerator that produces pruned models with the same accuracy as dense models without first training, then pruning, and finally retraining, a dense model. Compared to training the equivalent unpruned models using a state-of-the-art DNN accelerator without sparse training support, Procrustes consumes up to 3.26x less energy and offers up to 4x speedup across a range of models, while pruning weights by an order of magnitude and maintaining unpruned accuracy.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Dingqing  Yang", "Amin  Ghasemazar", "Xiaowei  Ren", "Maximilian  Golub", "Guy  Lemieux", "Mieszko  Lis"], "year": 2020, "n_citations": 9}
{"id": 738519, "s2_id": "2721af8273e9c5a15fc0d5a88272df53d357ce2f", "title": "CIPARSim: Cache intersection property assisted rapid single-pass FIFO cache simulation technique", "abstract": "An application's cache miss rate is used in timing analysis, system performance prediction and in deciding the best cache memory for an embedded system to meet tighter constraints. Single-pass simulation allows a designer to find the number of cache misses quickly and accurately on various cache memories. Such single-pass simulation systems have previously relied heavily on cache inclusion properties, which allowed rapid simulation of cache configurations for different applications. Thus far the only inclusion properties discovered were applicable to the Least Recently Used (LRU) replacement policy based caches. However, LRU based caches are rarely implemented in real life due to their circuit complexity at larger cache associativities. Embedded processors typically use a FIFO replacement policy in their caches instead, for which there are no full inclusion properties to exploit. In this paper, for the first time, we introduce a cache property called the \u201cIntersection Property\u201d that helps to reduce single-pass simulation time in a manner similar to inclusion property. An intersection property defines conditions that if met, prove a particular element exists in larger caches, thus avoiding further search time. We have discussed three such intersection properties for caches using the FIFO replacement policy in this paper. A rapid single-pass FIFO cache simulator \u201cCIPARSim\u201d has also been proposed. CIPARSim is the first single-pass simulator dependent on the FIFO cache properties to reduce simulation time significantly. CIPARSim's simulation time was up to 5 times faster (on average 3 times faster) compared to the state of the art single-pass FIFO cache simulator for the cache configurations tested. CIPARSim produces the cache hit and miss rates of an application accurately on various cache configurations. During simulation, CIPARSim's intersection properties alone predict up to 90% (on average 65%) of the total hits, reducing simulation time immensely.", "venue": "2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Mohammad Shihabul Haque", "Jorgen  Peddersen", "Sri  Parameswaran"], "year": 2011, "n_citations": 10}
{"id": 744143, "s2_id": "292839cb8e5c601be8cd467184939de7873fdd44", "title": "Chasing Carbon: The Elusive Environmental Footprint of Computing", "abstract": "Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Udit  Gupta", "Young Geun Kim", "Sylvia  Lee", "Jordan  Tse", "Hsien-Hsin S. Lee", "Gu-Yeon  Wei", "David  Brooks", "Carole-Jean  Wu"], "year": 2021, "n_citations": 11}
{"id": 746846, "s2_id": "6eac2d8a8043ed3e50b04df683f40d806f94006a", "title": "Mind mappings: enabling efficient algorithm-accelerator mapping space search", "abstract": "Modern day computing increasingly relies on specialization to satiate growing performance and efficiency requirements. A core challenge in designing such specialized hardware architectures is how to perform mapping space search, i.e., search for an optimal mapping from algorithm to hardware. Prior work shows that choosing an inefficient mapping can lead to multiplicative-factor efficiency overheads. Additionally, the search space is not only large but also non-convex and non-smooth, precluding advanced search techniques. As a result, previous works are forced to implement mapping space search using expert choices or sub-optimal search heuristics. This work proposes Mind Mappings, a novel gradient-based search method for algorithm-accelerator mapping space search. The key idea is to derive a smooth, differentiable approximation to the otherwise non-smooth, non-convex search space. With a smooth, differentiable approximation, we can leverage efficient gradient-based search algorithms to find high-quality mappings. We extensively compare Mind Mappings to black-box optimization schemes used in prior work. When tasked to find mappings for two important workloads (CNN and MTTKRP), Mind Mapping finds mappings that achieve an average 1.40\u00d7, 1.76\u00d7, and 1.29\u00d7 (when run for a fixed number of steps) and 3.16\u00d7, 4.19\u00d7, and 2.90\u00d7 (when run for a fixed amount of time) better energy-delay product (EDP) relative to Simulated Annealing, Genetic Algorithms and Reinforcement Learning, respectively. Meanwhile, Mind Mappings returns mappings with only 5.32\u00d7 higher EDP than a possibly unachievable theoretical lower-bound, indicating proximity to the global optima.", "venue": "ASPLOS", "authors": ["Kartik  Hegde", "Po-An  Tsai", "Sitao  Huang", "Vikas  Chandra", "Angshuman  Parashar", "Christopher W. Fletcher"], "year": 2021, "n_citations": 5}
{"id": 748370, "s2_id": "1176d77c22687a7bb2f436ece93695f033efc84d", "title": "Architecture for real time continuous sorting on large width data volume for fpga based applications", "abstract": "In engineering applications sorting is an important and widely studied problem where execution speed and resources used for computation are of extreme importance, especially if we think about real time data processing. Most of the traditional sorting techniques compute the process after receiving all of the data and hence the process needs large amount of resources for data storage. So, suitable design strategy needs to be adopted if we wish to sort a large amount of data in real time, which essential means higher speed of process execution and utilization of fewer resources in most of the cases. This paper proposes a single chip scalable architecture based on Field Programmable Gate Array(FPGA), for a modified counting sort algorithm where data acquisition and sorting is being done in real time scenario. Our design promises to work efficiently, where data can be accepted in the run time scenario without any need of prior storage of data and also the execution speed of our algorithm is invariant to the length of the data stream. The proposed design is implemented and verified on Spartan 3E(XC3S500E-FG320) FPGA system. The results prove that our design is better in terms of some of the design parameters compared to the existing research works.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Suman  Sau", "Amlan  Chakrabarti"], "year": 2012, "n_citations": 3}
{"id": 749016, "s2_id": "a0fc5d8a6942a6d8ebf8a5a8597ee4ee8d417f22", "title": "Global versus Local Weak-Indication Self-Timed Function Blocks - A Comparative Analysis", "abstract": "This paper analyzes the merits and demerits of global weak-indication self-timed function blocks versus local weak-indication self-timed function blocks, implemented using a delay-insensitive data code and adhering to 4-phase return-to-zero handshaking. A self-timed ripple carry adder is considered as an example function block for the analysis. The analysis shows that while global weak-indication could help in optimizing the power, latency and area parameters, local weak-indication facilitates the optimum performance in terms of realizing the data-dependent cycle time that is characteristic of a weak-indication self-timed design.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 9}
{"id": 751381, "s2_id": "68b7caaa68043762a52d0ba563469b590deeedba", "title": "Secure Internal Communication of a Trustzone-Enabled Heterogeneous Soc Lightweight Encryption", "abstract": "Security in TrustZone-enabled heterogeneous system-on-chip (SoC) is gaining increasing attention for several years. Mainly because this type of SoC can be found in more and more applications in servers or in the cloud. The inside-SoC communication layer is one of the main element of heterogeneous SoC; indeed all the data goes through it. Monitoring and controlling inside-SoC communications enables to fend off attacks before system corruption. In this article, we study the feasibility of encrypted data exchange between the secure software executed in a trusted execution environment (TEE) and the secure logic part of an heterogeneous SoC. Experiment are done with a Xilinx Zynq-7010 SoC and two lightweight stream ciphers. We show that using lightweight stream ciphers is an efficient solution without excessive overheads.", "venue": "2019 International Conference on Field-Programmable Technology (ICFPT)", "authors": ["El Mehdi Benhani", "Cuauhtemoc Mancillas L\u00f3pez", "Lilian  Bossuet"], "year": 2019, "n_citations": 1}
{"id": 756172, "s2_id": "d901691069689d87476e6f37f712e90b64735bf8", "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing", "abstract": "BERT is the most recent Transformer-based model that achieves state-of-the-art performance in various NLP tasks. In this paper, we investigate the hardware acceleration of BERT on FPGA for edge computing. To tackle the issue of huge computational complexity and memory footprint, we propose to fully quantize the BERT (FQ-BERT), including weights, activations, softmax, layer normalization, and all the intermediate results. Experiments demonstrate that the FQ-BERT can achieve 7.94\u00d7 compression for weights with negligible performance loss. We then propose an accelerator tailored for the FQ-BERT and evaluate on Xilinx ZCU102 and ZCU11 FPGA. It can achieve a performance-per-watt of 3.18 fps/W, which is 28.91\u00d7 and 12.72\u00d7 over Intel(R) Core(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Zejian  Liu", "Gang  Li", "Jian  Cheng"], "year": 2021, "n_citations": 2}
{"id": 760843, "s2_id": "8bc5f8ca6273053a26add57042d07e642c8f4c05", "title": "Comparative Analysis and Enhancement of CFG-based Hardware-Assisted CFI Schemes", "abstract": "Subverting the flow of instructions (e.g., by use of code-reuse attacks) still poses a serious threat to the security of today\u2019s systems. Various control flow integrity (CFI) schemes have been proposed as a powerful technique to detect and mitigate such attacks. In recent years, many hardware-assisted implementations of CFI enforcement based on control flow graphs (CFGs) have been presented by academia. Such approaches check whether control flow transfers follow the intended CFG by limiting the valid target addresses. However, these papers all target different platforms and were evaluated with different sets of benchmark applications, which makes quantitative comparisons hardly possible. For this paper, we have implemented multiple promising CFG-based CFI schemes on a common platform comprising a RISC-V within FPGA. By porting almost 40 benchmark applications to this system we can present a meaningful comparison of the various techniques in terms of run-time performance, hardware utilization, and binary size. In addition, we present an enhanced CFI approach that is inspired by what we consider the best concepts and ideas of previously proposed mechanisms. We have made this approach more practical and feature-complete by tackling some problems largely ignored previously. We show with this fine-grained scheme that CFI can be achieved with even less overheads than previously demonstrated.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Stefan  Tauner", "Mario  Telesklav"], "year": 2021, "n_citations": 0}
{"id": 761306, "s2_id": "522c718b7f5c424e4bfee91aea0567fbcccbc76e", "title": "Performance of Cache Memory Subsystems for Multicore Architectures", "abstract": "Advancements in multi-core have created interest among many research groups in finding out ways to harness the true power of processor cores. Recent research suggests that on-board component such as cache memory plays a crucial role in deciding the performance of multi-core systems. In this paper, performance of cache memory is evaluated through the parameters such as cache access time, miss rate and miss penalty. The influence of cache parameters over execution time is also discussed. Results obtained from simulated studies of multi-core environments with different instruction set architectures (ISA) like ALPHA and X86 are produced.", "venue": "ArXiv", "authors": ["N.  Ramasubramanian", "V.  SrinivasV.", "N. Ammasai Gounden"], "year": 2011, "n_citations": 9}
{"id": 762733, "s2_id": "7c0ae6af23a3e280711a6b894c1a388ce5d1d0f1", "title": "Fourier domain decoding algorithm of non-binary LDPC codes for parallel implementation", "abstract": "For decoding non-binary low-density parity-check (LDPC) codes, logarithm-domain sum-product (Log-SP) algorithms were proposed for reducing quantization effects of SP algorithm in conjunction with FFT. Since FFT is not applicable in the logarithm domain, the computations required at check nodes in the Log-SP algorithms are computationally intensive. What is worse, check nodes usually have higher degree than variable nodes. As a result, most of the time for decoding is used for check node computations, which leads to a bottleneck effect. In this paper, we propose a Log-SP algorithm in the Fourier domain. With this algorithm, the role of variable nodes and check nodes are switched. The intensive computations are spread over lower-degree variable nodes, which can be efficiently calculated in parallel. Furthermore, we develop a fast calculation method for the estimated bits and syndromes in the Fourier domain.", "venue": "ICASSP", "authors": ["Kenta  Kasai", "Kohichi  Sakaniwa"], "year": 2011, "n_citations": 1}
{"id": 763580, "s2_id": "83e8eddaa61ef11229d07133eaa326500b1de717", "title": "Memory and Parallelism Analysis Using a Platform-Independent Approach", "abstract": "Emerging computing architectures such as near-memory computing (NMC) promise improved performance for applications by reducing the data movement between CPU and memory. However, detecting such applications is not a trivial task. In this ongoing work, we extend the state-of-the-art platform-independent software analysis tool with NMC related metrics such as memory entropy, spatial locality, data-level, and basic-block-level parallelism. These metrics help to identify the applications more suitable for NMC architectures.", "venue": "SCOPES", "authors": ["Stefano  Corda", "Gagandeep  Singh", "Ahsan Javed Awan", "Roel  Jordans", "Henk  Corporaal"], "year": 2019, "n_citations": 5}
{"id": 764861, "s2_id": "08a3a22f1ac3c5ed637d146a57ebc1a6ccb8566c", "title": "Fast Processing of Large Graph Applications Using Asynchronous Architecture", "abstract": "Graph algorithms and techniques are increasingly being used in scientific and commercial applications to express relations and explore large data sets. Although conventional or commodity computer architectures, like CPU or GPU, can compute fairly well dense graph algorithms, they are often inadequate in processing large sparse graph applications. Memory access patterns, memory bandwidth requirements and on-chip network communications in these applications do not fit in the conventional program execution flow. In this work, we propose and design a new architecture for fast processing of large graph applications. To leverage the lack of the spatial and temporal localities in these applications and to support scalable computational models, we design the architecture around two key concepts. (1) The architecture is a multicore processor of independently clocked processing elements. These elements communicate in a self-timed manner and use handshaking to perform synchronization, communication, and sequencing of operations. By being asynchronous, the operating speed at each processing element is determined by actual local latencies rather than global worst-case latencies. We create a specialized ISA to support these operations. (2) The application compilation and mapping process uses a graph clustering algorithm to optimize parallel computing of graph operations and load balancing. Through the clustering process, we make scalability an inherent property of the architecture where task-to-element mapping can be done at the graph node level or at node cluster level. A prototyped version of the architecture outperforms a comparable CPU by 10~20x across all benchmarks and provides 2~5x better power efficiency when compared to a GPU.", "venue": "ArXiv", "authors": ["Michel A. Kinsy", "Rashmi S. Agrawal", "Hien D. Nguyen"], "year": 2017, "n_citations": 0}
{"id": 767420, "s2_id": "8f8a379b09975ca3461d829e67b42a4159d0cfa7", "title": "Efficient FPGA-based ECDSA Verification Engine for Permissioned Blockchains", "abstract": "As enterprises embrace blockchain technology, many real-world applications have been developed and deployed using permissioned blockchain platforms (access to network is controlled by allowing only nodes with known identities). Such blockchain platforms heavily depend on cryptography to provide a layer of trust within the network, thus verification of cryptographic signatures often becomes the bottleneck. The Elliptic Curve Digital Signature Algorithm (ECDSA) is the most commonly used cryptographic scheme in permissioned blockchains. In this paper, we propose an efficient implementation of ECDSA signature verification on an FPGA, in order to improve the performance of permissioned blockchains that aim to use FPGA-based hardware accelerators. In particular, we propose several optimizations for modular arithmetic (e.g., custom multipliers and fast modular reduction) and point arithmetic (e.g., significantly reduced number of point double and addition operations, and optimal width NAF representation). Based on these optimized modular and point arithmetic modules, we propose an ECDSA verification engine that can be used by any application for fast verification of ECDSA signatures. We further optimize our ECDSA verification engine for Hyperledger Fabric (one of the most widely used permissioned blockchain platforms) by moving carefully selected operations to a precomputation block, thus simplifying the critical path of ECDSA signature verification. From our implementation on Xilinx Alveo U250 accelerator board with target frequency of 250MHz, our ECDSA verification engine can perform a single verification in 760\u03bcs resulting in a throughput of 1, 315 verifications per second, which is ~2.5\u00d7 faster than stateof-the-art FPGA-based implementations [8, 18]. Our Hyperledger Fabric-specific ECDSA engine can perform a single verification in 368\u03bcs with a throughput of 2, 717 verifications per second.", "venue": "ArXiv", "authors": ["Rashmi  Agrawal", "Ji  Yang", "Haris  Javaid"], "year": 2021, "n_citations": 0}
{"id": 770006, "s2_id": "92f337fccf5ff2a04fc38619a4921625fcca736d", "title": "Q-VR: system-level design for future mobile collaborative virtual reality", "abstract": "High Quality Mobile Virtual Reality (VR) is what the incoming graphics technology era demands: users around the world, regardless of their hardware and network conditions, can all enjoy the immersive virtual experience. However, the state-of-the-art software-based mobile VR designs cannot fully satisfy the realtime performance requirements due to the highly interactive nature of user's actions and complex environmental constraints during VR execution. Inspired by the unique human visual system effects and the strong correlation between VR motion features and realtime hardware-level information, we propose Q-VR, a novel dynamic collaborative rendering solution via software-hardware co-design for enabling future low-latency high-quality mobile VR. At software-level, Q-VR provides flexible high-level tuning interface to reduce network latency while maintaining user perception. At hardware-level, Q-VR accommodates a wide spectrum of hardware and network conditions across users by effectively leveraging the computing capability of the increasingly powerful VR hardware. Extensive evaluation on real-world games demonstrates that Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to 6.7x) over the traditional local rendering design in commercial VR devices, and a 4.1x frame rate improvement over the state-of-the-art static collaborative rendering.", "venue": "ASPLOS", "authors": ["Chenhao  Xie", "Xie  Li", "Yang  Hu", "Huwan  Peng", "Michael  Taylor", "Shuaiwen Leon Song"], "year": 2021, "n_citations": 2}
{"id": 770739, "s2_id": "15e41afde3305823526e325532110dda01aeca1e", "title": "ChewBaccaNN: A Flexible 223 TOPS/W BNN Accelerator", "abstract": "Binary Neural Networks enable smart IoT devices, as they significantly reduce the required memory footprint and computational complexity while retaining a high network performance and flexibility. This paper presents ChewBaccaNN, a 0.7 mm2 sized binary convolutional neural network (CNN) accelerator designed in GlobalFoundries 22 nm technology. By exploiting efficient data re-use, data buffering, latch-based memories, and voltage scaling, a throughput of 241 GOPS is achieved while consuming just 1.1 mW at 0.4V/154MHz during inference of binary CNNs with up to 7\u00d77 kernels, leading to a peak core energy efficiency of 223 TOPS/W. ChewBaccaNN's flexibility allows to run a much wider range of binary CNNs than other accelerators, drastically improving the accuracy-energy tradeoff beyond what can be captured by the TOPS/W metric. In fact, it can perform CIFAR-10 inference at 86.8% accuracy with merely 1.3 J, thus exceeding the accuracy while at the same time lowering the energy cost by 2.8\u00d7 compared to even the most efficient and much larger analog processing-in-memory devices, while keeping the flexibility of running larger CNNs for higher accuracy when needed. It also runs a binary ResNet-18 trained on the 1000-class ILSVRC dataset and improves the energy efficiency by 4.4\u00d7 over accelerators of similar flexibility. Furthermore, it can perform inference on a binarized ResNet-18 trained with 8-bases Group-Net to achieve a 67.5% Top-1 accuracy with only 3.0mJ/frame\u2014at an accuracy drop of merely 1.8% from the fullprecision ResNet-18.", "venue": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Renzo  Andri", "Geethan  Karunaratne", "Lukas  Cavigelli", "Luca  Benini"], "year": 2021, "n_citations": 4}
{"id": 772293, "s2_id": "c173eab8439776a05a04d8504ff8776c6e09de5c", "title": "Designing a CPU model: from a pseudo-formal document to fast code", "abstract": "For validating low level embedded software, engineers use simulators that take the real binary as input. Like the real hardware, these full-system simulators are organized as a set of components. The main component is the CPU simulator (ISS), because it is the usual bottleneck for the simulation speed, and its development is a long and repetitive task. Previous work showed that an ISS can be generated from an Architecture Description Language (ADL). In the work reported in this paper, we generate a CPU simulator directly from the pseudo-formal descriptions of the reference manual. For each instruction, we extract the information describing its behavior, its binary encoding, and its assembly syntax. Next, after automatically applying many optimizations on the extracted information, we generate a SystemC/TLM ISS. We also generate tests for the decoder and a formal specification in Coq. Experiments show that the generated ISS is as fast and stable as our previous hand-written ISS.", "venue": "ArXiv", "authors": ["Fr\u00e9d\u00e9ric  Blanqui", "Claude  Helmstetter", "Vania  Joloboff", "Jean-Fran\u00e7ois  Monin", "Xiaomu  Shi"], "year": 2011, "n_citations": 11}
{"id": 772515, "s2_id": "2f3ed0c7448aa1f006b5e08f79315132283bf6f0", "title": "Threshold Voltage-Defined Switches for Programmable Gates", "abstract": "Semiconductor supply chain is increasingly getting exposed to variety of security attacks such as Trojan insertion, cloning, counterfeiting, reverse engineering (RE), piracy of Intellectual Property (IP) or Integrated Circuit (IC) and side-channel analysis due to involvement of untrusted parties. In this paper, we propose transistor threshold voltage-defined switches to camouflage the logic gate both logically and physically to resist against RE and IP piracy. The proposed gate can function as NAND, AND, NOR, OR, XOR, XNOR, INV and BUF robustly using threshold-defined switches. The camouflaged design operates at nominal voltage and obeys conventional reliability limits. The proposed gate can also be used to personalize the design during manufacturing.", "venue": "ArXiv", "authors": ["Anirudh  Iyengar", "Swaroop  Ghosh"], "year": 2015, "n_citations": 13}
{"id": 773789, "s2_id": "5e89c96d548f3bf2d5a0add1d74c4c3f025935f0", "title": "A New Design for Array Multiplier with Trade off in Power and Area", "abstract": "In this paper a low power and low area array multiplier with carry save adder is proposed. The proposed adder eliminates the final addition stage of the multiplier than the conventional parallel array multiplier. The conventional and proposed multiplier both are synthesized with 16-T full adder. Among Transmission Gate, Transmission Function Adder, 14-T, 16-T full adder shows energy efficiency. In the proposed 4x4 multiplier to add carry bits with out using Ripple Carry Adder (RCA) in the final stage, the carries given to the input of the next left column input. Due to this the proposed multiplier shows 56 less transistor count, then cause trade off in power and area. The proposed multiplier has shown 13.91% less power, 34.09% more speed and 59.91% less energy consumption for TSMC 0.18nm technology at a supply voltage 2.0V than the conventional multiplier.", "venue": "ArXiv", "authors": ["Nirlakalla  Ravi", "A.  Satish", "T. Jayachandra Prasad", "T. Subba Rao"], "year": 2011, "n_citations": 16}
{"id": 780182, "s2_id": "8e347c23da505cdff609c1c51d2b9898cba14b86", "title": "Memory Aware High-Level Synthesis for Embedded Systems", "abstract": "We introduce a new approach to take into account the memory architecture and the memory mapping in the High- Level Synthesis of Real-Time embedded systems. We formalize the memory mapping as a set of constraints used in the scheduling step. We use a memory mapping file to include those memory constraints in our HLS tool GAUT. Our scheduling algorithm exhibits a relatively low complexity that permits to tackle complex designs in a reasonable time. Finally, we show how to explore, with the help of GAUT, a wide range of solutions, and to reach a good tradeoff between time, power-consumption, and area.", "venue": "ArXiv", "authors": ["Gwenol\u00e9  Corre", "Eric  Senn", "Nathalie  Julien", "Eric  Martin"], "year": 2006, "n_citations": 2}
{"id": 781872, "s2_id": "50bfb8fde6b6a5950a29221deee13e4495f81bb4", "title": "PiDRAM: A Holistic End-to-end FPGA-based Framework for Processing-in-DRAM", "abstract": "Processing-using-memory (PuM) techniques leverage the analog operation of memory cells to perform computation. Several recent works have demonstrated PuM techniques (e.g., copy and initialization operations, bitwise operations, random number generation) in off-the-shelf DRAM devices. Since DRAM is the dominant memory technology as main memory in current computing systems, these PuM techniques represent an opportunity for alleviating the data movement bottleneck at very low cost. However, system integration of PuM techniques imposes non-trivial challenges (e.g., related to data allocation and alignment, memory coherence management) that are yet to be solved. Design space exploration of potential solutions to the PuM integration challenges requires appropriate tools to develop necessary hardware and software components. Unfortunately, current proprietary computing systems, specialized DRAM-testing platforms, or system simulators do not provide the flexibility and/or the holistic system view that is necessary to deal with PuM integration challenges. We design and develop PiDRAM, the first flexible endto-end framework that enables system integration studies and evaluation of real PuM techniques. PiDRAM provides software and hardware components to rapidly integrate PuM techniques across the whole system software and hardware stack (e.g., necessary modifications in the operating system, memory controller). We implement PiDRAM on an FPGAbased platform along with an open-source RISC-V system. To demonstrate the flexibility and ease of use of PiDRAM, we implement and evaluate two state-of-the-art PuM techniques. First, we implement in-memory copy and initialization. We propose solutions to integration challenges (e.g., memory coherence) and conduct a detailed end-to-end implementation study. Second, we implement a true random number generator in DRAM. Our results show that the in-memory copy and initialization techniques can improve the performance of bulk copy operations by 12.6\u00d7 and bulk initialization operations by 14.6\u00d7 on a real system. Implementing the true random number generator requires only 190 lines of Verilog and 74 lines of C code using PiDRAM\u2019s software and hardware components. We will release PiDRAM as an open-source tool researchers and industry can benefit from and build on.", "venue": "ArXiv", "authors": ["Ataberk  Olgun", "Juan  G\u00f3mez-Luna", "Konstantinos  Kanellopoulos", "Behzad  Salami", "Hasan  Hassan", "Oguz  Ergin", "Onur  Mutlu"], "year": 2021, "n_citations": 0}
{"id": 782440, "s2_id": "f588e219c98b81615fe22e97ebe3a573430d430d", "title": "FPGA Hardware Acceleration of Monte Carlo Simulations for the Ising Model", "abstract": "A two-dimensional Ising model with nearest-neighbors ferromagnetic interactions is implemented in a Field Programmable Gate Array (FPGA) board. Extensive Monte Carlo simulations were carried out using an efficient hardware representation of individual spins and a combined global-local LFSR random number generator. Consistent results regarding the descriptive properties of magnetic systems, like energy, magnetization and susceptibility are obtained while a speed-up factor of approximately six times is achieved in comparison to previous FPGA-based published works and almost <inline-formula><tex-math notation=\"LaTeX\">$10^4$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"ortegazamorano-ieq1-2505725.gif\"/></alternatives></inline-formula> times in comparison to a standard CPU simulation. A detailed description of the logic design used is given together with a careful analysis of the quality of the random number generator used. The obtained results confirm the potential of FPGAs for analyzing the statistical mechanics of magnetic systems.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Francisco  Ortega-Zamorano", "Marcelo A. Montemurro", "Sergio Alejandro Cannas", "Jos\u00e9 M. Jerez", "Leonardo  Franco"], "year": 2016, "n_citations": 10}
{"id": 782889, "s2_id": "96bdc40b85b9e3dc112fafa6a74fa55422428ea2", "title": "Workload-Aware DRAM Error Prediction using Machine Learning", "abstract": "The aggressive scaling of technology may have helped to meet the growing demand for higher memory capacity and density, but has also made DRAM cells more prone to errors. Such a reality triggered a lot of interest in modeling DRAM behavior for either predicting the errors in advance or for adjusting DRAM circuit parameters to achieve a better tradeoff between energy efficiency and reliability. Existing modeling efforts may have studied the impact of few operating parameters and temperature on DRAM reliability using custom FPGAs setups, however they neglected the combined effect of workload-specific features that can be systematically investigated only on a real system. In this paper, we present the results of our study on workload-dependent DRAM error behavior within a real server considering various operating parameters, such as the refresh rate, voltage and temperature. We show that the rate of single- and multi-bit errors may vary across workloads by 8x, indicating that program inherent features can affect DRAM reliability significantly. Based on this observation, we extract 249 features, such as the memory access rate, the rate of cache misses, the memory reuse time and data entropy, from various compute-intensive, caching and analytics benchmarks. We apply several supervised learning methods to construct the DRAM error behavior model for 72 server-grade DRAM chips using the memory operating parameters and extracted program inherent features. Our results show that, with an appropriate choice of program features and supervised learning method, the rate of single- and multi-bit errors can be predicted for a specific DRAM module with an average error of less than 10.5 %, as opposed to the 2.9x estimation error obtained for a conventional workload-unaware error model. Our model enables designers to predict DRAM errors in advance for less than a second and study the impact of any workload and applied software optimizations on DRAM reliability.", "venue": "2019 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Lev  Mukhanov", "Konstantinos  Tovletoglou", "Hans  Vandierendonck", "Dimitrios S. Nikolopoulos", "Georgios  Karakonstantis"], "year": 2019, "n_citations": 6}
{"id": 797588, "s2_id": "b283cebb135d16bcd2b08af275c44be86629150b", "title": "The Necessity for Hardware QoS Support for Server Consolidation and Cloud Computing", "abstract": "Chip multiprocessors (CMPs) are ubiquitous in most of today's computing fields. Although they provide noticeable benefits in terms of performance, cost and power efficiency, they also introduce some new issues. In this paper we analyze how the interference from Virtual Private Servers running in other cores is a significant component of performance unpredictability and can threaten the attainment of cloud computing. Even if virtualization is used, the sharing of the on-chip section of the memory hierarchy by different cores makes performance isolation strongly dependent on what is running elsewhere in the system. We will show in three actual computing systems, based on Sun UltraSparc T1, Sun UltraSparc T2 and Intel Xeon processors, how state-of-the-art virtualization techniques are unable to guarantee performance isolation in a representative workload such as SPECweb2005. In an especially conceived near worst-case scenario, it is possible to reduce the performance achieved by a Solaris Zones consolidated server for this suite of benchmarks in a Sun Fire T1000 and a Sun Enterprise T5120 by up to 80%. The performance drop observed by a Xen consolidated server running in a HP Proliant DL160 G5 is almost 45%. For all systems under study, off-chip bandwidth is shown to be the most critical resource.", "venue": "ArXiv", "authors": ["Javier  Merino", "Valentin  Puente", "Jos\u00e9-\u00c1ngel  Gregorio"], "year": 2012, "n_citations": 0}
{"id": 797883, "s2_id": "1d1576b2dfa0b22db1e61d6e919e8f2147e8b275", "title": "CrossLight: A Cross-Layer Optimized Silicon Photonic Neural Network Accelerator", "abstract": "Domain-specific neural network accelerators have seen growing interest in recent years due to their improved energy efficiency and performance compared to CPUs and GPUs. In this paper, we propose a novel cross-layer optimized neural network accelerator called CrossLight that leverages silicon photonics. CrossLight includes device-level engineering for resilience to process variations and thermal crosstalk, circuit-level tuning enhancements for inference latency reduction, and architecture-level optimizations to enable better resolution, energy-efficiency, and throughput. On average, CrossLight offers 9.5x lower energy-per-bit and 15.9x higher performance-per-watt than state-of-the-art photonic deep learning accelerators.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Febin  Sunny", "Asif  Mirza", "Mahdi  Nikdast", "Sudeep  Pasricha"], "year": 2021, "n_citations": 5}
{"id": 798401, "s2_id": "3ecf77ac7b3c11a3f841b6df938b1e32755b3c20", "title": "Mixed-Signal Charge-Domain Acceleration of Deep Neural Networks through Interleaved Bit-Partitioned Arithmetic", "abstract": "Albeit low-power, mixed-signal circuitry suffers from significant overhead of Analog to Digital (A/D) conversion, limited range for information encoding, and susceptibility to noise. This paper aims to address these challenges by offering and leveraging the following mathematical insight regarding vector dot-product---the basic operator in Deep Neural Networks (DNNs). This operator can be reformulated as a wide regrouping of spatially parallel low-bitwidth calculations that are interleaved across the bit partitions of multiple elements of the vectors. As such, the computational building block of our accelerator becomes a wide bit-interleaved analog vector unit comprising a collection of low-bitwidth multiply-accumulate modules that operate in the analog domain and share a single A/D converter(ADC). This bit-partitioning results in a lower-resolution ADC while the wide regrouping alleviates the need for A/D conversion per operation, amortizing its cost across multiple bit-partitions of the vector elements. Moreover, the low-bitwidth modules require smaller encoding range and also provide larger margins for noise mitigation. We also utilize the switched-capacitor design for our bit-level reformulation of DNN operations. The proposed switched-capacitor circuitry performs the regrouped multiplications in the charge domain and accumulates the results of the group in its capacitors over multiple cycles. The capacitive accumulation combined with wide bit-partitioned regrouping reduces the rate of A/D conversions, further improving the overall efficiency of the design. With such mathematical reformulation and its switched-capacitor implementation, we define one possible 3D-stacked microarchitecture, dubbed BiHiwe, that leverages clustering and hierarchical design to best utilize power-efficiency of the mixed-signal domain and 3D stacking. We also build models for noise, computational non-idealities, and variations. For ten DNN benchmarks, BiHiwe delivers 5.5x speedup over a leading purely-digital 3D-stacked accelerator Tetris, with a mere of less than 0.5% accuracy loss achieved by careful treatment of noise, computation error, and various forms of variation. Compared to RTX~2080~TI with tensor cores and Titan Xp GPUs, all with 8-bit execution, BiHiwe offers 35.4x and 70.1x higher Performance-per-Watt, respectively. Relative to the mixed-signal RedEye, ISAAC, and PipeLayer, BiHiwe offers 5.5x, 3.6x, and 9.6x improvement in Performance-per-Watt respectively. The results suggest that BiHiwe is an effective initial step in a road that combines mathematics, circuits, and architecture.", "venue": "PACT", "authors": ["Soroush  Ghodrati", "Hardik  Sharma", "Sean  Kinzer", "Amir  Yazdanbakhsh", "Kambiz  Samadi", "Nam Sung Kim", "Doug  Burger", "Hadi  Esmaeilzadeh"], "year": 2020, "n_citations": 10}
{"id": 808707, "s2_id": "c0f4ff1faf66ee5890815ea19ad2214c33b4f62e", "title": "Low-complexity Architecture for AR(1) Inference", "abstract": "In this Letter, we propose a low-complexity estimator for the correlation coefficient based on the signed $\\operatorname{AR}(1)$ process. The introduced approximation is suitable for implementation in low-power hardware architectures. Monte Carlo simulations reveal that the proposed estimator performs comparably to the competing methods in literature with maximum error in order of $10^{-2}$. However, the hardware implementation of the introduced method presents considerable advantages in several relevant metrics, offering more than 95% reduction in dynamic power and doubling the maximum operating frequency when compared to the reference method.", "venue": "ArXiv", "authors": ["Abel  Borges", "Renato J. Cintra", "Diego F. G. Coelho", "Vassil S. Dimitrov"], "year": 2020, "n_citations": 0}
{"id": 809384, "s2_id": "680daccbd4b662ea8ccba859bd42e4b9df97d27d", "title": "Understanding the Interactions of Workloads and DRAM Types: A Comprehensive Experimental Study", "abstract": "It has become increasingly difficult to understand the complex interaction between modern applications and main memory, composed of DRAM chips. Manufacturers are now selling and proposing many different types of DRAM, with each DRAM type catering to different needs (e.g., high throughput, low power, high memory density). At the same time, the memory access patterns of prevalent and emerging workloads are rapidly diverging, as these applications manipulate larger data sets in very different ways. As a result, the combined DRAM-workload behavior is often difficult to intuitively determine today, which can hinder memory optimizations in both hardware and software. In this work, we identify important families of workloads, as well as prevalent types of DRAM chips, and rigorously analyze the combined DRAM--workload behavior. To this end, we perform a comprehensive experimental study of the interaction between nine different DRAM types and 115 modern applications and multiprogrammed workloads. We draw 12 key observations from our characterization, enabled in part by our development of new metrics that take into account contention between memory requests due to hardware design. Notably, we find that (1) newer DRAM types such as DDR4 and HMC often do not outperform older types such as DDR3, due to higher access latencies and, in the case of HMC, poor exploitation of locality; (2) there is no single DRAM type that can cater to all components of a heterogeneous system (e.g., GDDR5 significantly outperforms other memories for multimedia acceleration, while HMC significantly outperforms other memories for network acceleration); and (3) there is still a strong need to lower DRAM latency, but unfortunately the current design trend of commodity DRAM is toward higher latencies to obtain other benefits. We hope that the trends we identify can drive optimizations in both hardware and software design.", "venue": "ArXiv", "authors": ["Saugata  Ghose", "Tianshi  Li", "Nastaran  Hajinazar", "Damla Senol Cali", "Onur  Mutlu"], "year": 2019, "n_citations": 7}
{"id": 811931, "s2_id": "1b8ed306f0c9cca532f493bb654d9eea5ba128d6", "title": "The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework", "abstract": "Computers continue to diversify with respect to system designs, emerging memory technologies, and application memory demands. Unfortunately, continually adapting the conventional virtual memory framework to each possible system configuration is challenging, and often results in performance loss or requires non-trivial workarounds. To address these challenges, we propose a new virtual memory framework, the Virtual Block Interface (VBI). We design VBI based on the key idea that delegating memory management duties to hardware can reduce the overheads and software complexity associated with virtual memory. VBI introduces a set of variable-sized virtual blocks (VBs) to applications. Each VB is a contiguous region of the globally-visible VBI address space, and an application can allocate each semantically meaningful unit of information (e.g., a data structure) in a separate VB. VBI decouples access protection from memory allocation and address translation. While the OS controls which programs have access to which VBs, dedicated hardware in the memory controller manages the physical memory allocation and address translation of the VBs. This approach enables several architectural optimizations to (1) efficiently and flexibly cater to different and increasingly diverse system configurations, and (2) eliminate key inefficiencies of conventional virtual memory. We demonstrate the benefits of VBI with two important use cases: (1) reducing the overheads of address translation (for both native execution and virtual machine environments), as VBI reduces the number of translation requests and associated memory accesses; and (2) two heterogeneous main memory architectures, where VBI increases the effectiveness of managing fast memory regions. For both cases, VBI significantly improves performance over conventional virtual memory.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Nastaran  Hajinazar", "Pratyush  Patel", "Minesh  Patel", "Konstantinos  Kanellopoulos", "Saugata  Ghose", "Rachata  Ausavarungnirun", "Geraldo Francisco de Oliveira", "Jonathan  Appavoo", "Vivek  Seshadri", "Onur  Mutlu"], "year": 2020, "n_citations": 8}
{"id": 812399, "s2_id": "42a7698624866f9ebbc50cdc6399d4c25a43650b", "title": "Cache Bypassing for Machine Learning Algorithms", "abstract": "Graphics Processing Units (GPUs) were once used solely for graphical computation tasks but with the increase in the use of machine learning applications, the use of GPUs to perform general purpose computing has increased in the last few years. GPUs employ a massive amount of threads, that in turn achieve a high amount parallelism, to perform tasks. Though GPUs have a high amount of computation power, they face the problem of cache contention due to the SIMT model that they use. A solution to this problem is called \"cache bypassing\". This paper presents a predictive model that analyzes the access patterns of various machine learning algorithms and determines whether certain data should be stored in the cache or not. It presents insights on how well each model performs on different datasets and also shows how minimizing the size of each model will affect its performance The performance of most of the models were found to be around 90% with KNN performing the best but not with the smallest size. We further increase the features by splitting the addresses into chunks of 4 bytes. We observe that this increased the performance of neural network substantially and increased the accuracy to 99.9% with three neurons.", "venue": "ArXiv", "authors": ["Asim  Ikram", "Muhammad Awais Ali", "Mirza Omer Beg"], "year": 2021, "n_citations": 0}
{"id": 812442, "s2_id": "3f1b7157b77ba8258f6548bf68f978203e465516", "title": "Implications of Integrated CPU-GPU Processors on Thermal and Power Management Techniques", "abstract": "Heterogeneous processors with architecturally different cores (CPU and GPU) integrated on the same die lead to new challenges and opportunities for thermal and power management techniques because of shared thermal/power budgets between these cores. In this paper, we show that new parallel programming paradigms (e.g., OpenCL) for CPU-GPU processors create a tighter coupling between the workload, the thermal/power management unit and the operating system. Using detailed thermal and power maps of the die from infrared imaging, we demonstrate that in contrast to traditional multi-core CPUs, heterogeneous processors exhibit higher coupled behavior for dynamic voltage and frequency scaling and workload scheduling, in terms of their effect on performance, power, and temperature. Further, we show that by taking the differences in core architectures and relative proximity of different computing cores on the die into consideration, better scheduling schemes could be implemented to reduce both the power density and peak temperature of the die. The findings presented in the paper can be used to improve thermal and power efficiency of heterogeneous CPU-GPU processors.", "venue": "ArXiv", "authors": ["Kapil  Dev", "Indrani  Paul", "Wei  Huang", "Yasuko  Eckert", "Wayne P. Burleson", "Sherief  Reda"], "year": 2018, "n_citations": 2}
{"id": 812539, "s2_id": "e421ba3167718763973b46466144f5523520397f", "title": "The Smart Parking Management System", "abstract": "With growing, Car parking increases with the number of car users. With the increased use of smartphones and their applications, users prefer mobile phone-based solutions. This paper proposes the Smart Parking Management System (SPMS) that depends on Arduino parts, Android applications, and based on IoT. This gave the client the ability to check available parking spaces and reserve a parking spot. IR sensors are utilized to know if a car park space is allowed. Its area data are transmitted using the WI-FI module to the server and are recovered by the mobile application which offers many options attractively and with no cost to users and lets the user check reservation details. With IoT technology, the smart parking system can be connected wirelessly to easily track available locations.", "venue": "ArXiv", "authors": ["A.  Elsonbaty", "Mahmoud  Shams"], "year": 2020, "n_citations": 1}
{"id": 821855, "s2_id": "5a1870b42e676e5c6af014a21e02462b4139b55f", "title": "Test time reduction reusing multiple processors in a network-on-chip based architecture", "abstract": "The increasing complexity and the short life cycles of embedded systems are pushing the current system-on-chip designs towards a rapid increase in the number of programmable processing units, while decreasing the gate count for custom logic. Considering this trend, this work proposes a test planning method capable of reusing available processors as test sources and sinks, and the on-chip network as the test access mechanism. Experimental results are based on ITC'02 benchmarks and on two open core processors, compliant with MIPS and SPARC instruction sets. The results show that the cooperative use of both the on-chip network and the embedded processors can increase the test parallelism and reduce the test time without additional cost in area and pins.", "venue": "Design, Automation and Test in Europe", "authors": ["Alexandre M. Amory", "Marcelo  Lubaszewski", "Fernando Gehm Moraes", "Edson I. Moreno"], "year": 2005, "n_citations": 4}
{"id": 822399, "s2_id": "32b159fd56dc1a5c3f62e064fd53d5c1e113f33a", "title": "Formalizing Memory Accesses and Interrupts", "abstract": "The hardware/software boundary in modern heterogeneous multicore computers is increasingly complex, and diverse across different platforms. A single memory access by a core or DMA engine traverses multiple hardware translation and caching steps, and the destination memory cell or register often appears at different physical addresses for different cores. Interrupts pass through a complex topology of interrupt controllers and remappers before delivery to one or more cores, each with specific constraints on their configurations. System software must not only correctly understand the specific hardware at hand, but also configure it appropriately at runtime. We propose a formal model of address spaces and resources in a system that allows us to express and verify invariants of the system's runtime configuration, and illustrate (and motivate) it with several real platforms we have encountered in the process of OS implementation.", "venue": "MARS@ETAPS", "authors": ["Reto  Achermann", "Lukas  Humbel", "David  Cock", "Timothy  Roscoe"], "year": 2017, "n_citations": 8}
{"id": 826358, "s2_id": "81bbfdef78aaa88f9aa0c7f5b016ab037c44f269", "title": "Rethinking Co-design of Neural Architectures and Hardware Accelerators", "abstract": "Neural architectures and hardware accelerators have been two driving forces for the progress in deep learning. Previous works typically attempt to optimize hardware given a fixed model architecture or model architecture given fixed hardware. And the dominant hardware architecture explored in this prior work is FPGAs. In our work, we target the optimization of hardware and software configurations on an industry-standard edge accelerator. We systematically study the importance and strategies of co-designing neural architectures and hardware accelerators. We make three observations: 1) the software search space has to be customized to fully leverage the targeted hardware architecture, 2) the search for the model architecture and hardware architecture should be done jointly to achieve the best of both worlds, and 3) different use cases lead to very different search outcomes. Our experiments show that the joint search method consistently outperforms previous platform-aware neural architecture search, manually crafted models, and the state-of-the-art EfficientNet on all latency targets by around 1% on ImageNet top-1 accuracy. Our method can reduce energy consumption of an edge accelerator by up to 2x under the same accuracy constraint, when co-adapting the model architecture and hardware accelerator configurations.", "venue": "ArXiv", "authors": ["Yanqi  Zhou", "Xuanyi  Dong", "Berkin  Akin", "Mingxing  Tan", "Daiyi  Peng", "Tianjian  Meng", "Amir  Yazdanbakhsh", "Da  Huang", "Ravi  Narayanaswami", "James  Laudon"], "year": 2021, "n_citations": 4}
{"id": 829831, "s2_id": "a97937f2c811210b9447a8e6ba6c2e438ca81807", "title": "An Asymmetric Adaptive SCL Decoder Hardware for Ultra-Low-Error-Rate Polar Codes", "abstract": "In theory, Polar codes do not exhibit an error floor under successive-cancellation (SC) decoding. In practice, frame error rate (FER) down to 10\u221212 has not been reported with a real SC list (SCL) decoder hardware. This paper presents an asymmetric adaptive SCL (A2SCL) decoder, implemented in real hardware, for high-throughput and ultra-reliable communications. We propose to concatenate multiple SC decoders with an SCL decoder, in which the numbers of SC/SCL decoders are balanced with respect to their area and latency. In addition, a novel unequal-quantization technique is adopted. The two optimizations are crucial for improving SCL throughput within limited chip area. As an application, we build a link-level FPGA emulation platform to measure ultra-low FERs of 3GPP NR Polar codes (with parity-check and CRC bits). It is flexible to support all list sizes up to 8, code lengths up to 1024 and arbitrary code rates. With the proposed hardware, decoding speed is 7000 times faster than a CPU core. For the first time, FER as low as 10\u221212 is measured and quantization effect is analyzed.", "venue": "2019 16th International Symposium on Wireless Communication Systems (ISWCS)", "authors": ["Jiajie  Tong", "Huazi  Zhang", "Lingchen  Huang", "Xiaocheng  Liu", "Jun  Wang"], "year": 2019, "n_citations": 1}
{"id": 830313, "s2_id": "2f0b45416b752642f8826888236404b7283c1e94", "title": "Memory-Aware Denial-of-Service Attacks on Shared Cache in Multicore Real-Time Systems", "abstract": "In this paper, we identify that memory performance plays a crucial role in the feasibility and effectiveness for performing denial-of-service attacks on shared cache. Based on this insight, we introduce new cache DoS attacks, which can be mounted from the user-space and can cause extreme WCET impacts to cross-core victims---even if the shared cache is partitioned---by taking advantage of the platform's memory address mapping information and HugePage support. We deploy these enhanced attacks on two popular embedded out-of-order multicore platforms using both synthetic and real-world benchmarks. The proposed DoS attacks achieve up to 75X WCET increases on the tested platforms.", "venue": "ArXiv", "authors": ["Michael  Bechtel", "Heechul  Yun"], "year": 2020, "n_citations": 1}
{"id": 830739, "s2_id": "b6451cfb71be72f8f9e0f5d2f529fea231adb382", "title": "Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer", "abstract": "Designing hardware accelerators for deep neural networks (DNNs) has been much desired. Nonetheless, most of these existing accelerators are built for either convolutional neural networks (CNNs) or recurrent neural networks (RNNs). Recently, the Transformer model is replacing the RNN in the natural language processing (NLP) area. However, because of intensive matrix computations and complicated data flow being involved, the hardware design for the Transformer model has never been reported. In this paper, we propose the first hardware accelerator for two key components, i.e., the multi-head attention (MHA) ResBlock and the position-wise feed-forward network (FFN) ResBlock, which are the two most complex layers in the Transformer. Firstly, an efficient method is introduced to partition the huge matrices in the Transformer, allowing the two ResBlocks to share most of the hardware resources. Secondly, the computation flow is well designed to ensure the high hardware utilization of the systolic array, which is the biggest module in our design. Thirdly, complicated nonlinear functions are highly optimized to further reduce the hardware complexity and also the latency of the entire system. Our design is coded using hardware description language (HDL) and evaluated on a Xilinx FPGA. Compared with the implementation on GPU with the same setting, the proposed design demonstrates a speed-up of 14.6 x in the MHA ResBlock, and 3.4 x in the FFN ResBlock, respectively. Therefore, this work lays a good foundation for building efficient hardware accelerators for multiple Transformer networks.", "venue": "2020 IEEE 33rd International System-on-Chip Conference (SOCC)", "authors": ["Siyuan  Lu", "Meiqi  Wang", "Shuang  Liang", "Jun  Lin", "Zhongfeng  Wang"], "year": 2020, "n_citations": 5}
{"id": 834686, "s2_id": "ad34e2d6cece61b5d862c906defa43244484bc64", "title": "CIDPro: Custom Instructions for Dynamic Program Diversification", "abstract": "Timing side-channel attacks pose a major threat to embedded systems due to their ease of accessibility. We propose CIDPro, a framework that relies on dynamic program diversification to mitigate timing side-channel leakage. The proposed framework integrates the widely used LLVM compiler infrastructure and the increasingly popular RISC-V FPGA soft-processor. The compiler automatically generates custom instructions in the security critical segments of the program, and the instructions execute on the RISC-V custom co-processor to produce diversified timing characteristics on each execution instance. CIDPro has been implemented on the Zynq7000 XC7Z020 FPGA device to study the performance overhead and security tradeoffs. Experimental results show that our solution can achieve 80% and 86% timing side-channel capacity reduction for two benchmarks with an acceptable performance overhead compared to existing solutions. In addition, the proposed method incurs only a negligible hardware area overhead of 1% slices of the entire RISC-V system.", "venue": "2018 28th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Thinh Hung Pham", "Alexander  Fell", "Arnab Kumar Biswas", "Siew-Kei  Lam", "Nandeesha  Veeranna"], "year": 2018, "n_citations": 3}
{"id": 835816, "s2_id": "65582abd2ea66caee431bcfe29b99cb811b40a9c", "title": "GCNear: A Hybrid Architecture for Efficient GCN Training with Near-Memory Processing", "abstract": "Recently, Graph Convolutional Networks (GCNs) have become state-of-the-art algorithms for analyzing noneuclidean graph data. However, it is challenging to realize efficient GCN training, especially on large graphs. The reasons are many-folded: 1) GCN training incurs a substantial memory footprint. Full-batch training on large graphs even requires hundreds to thousands of gigabytes of memory to buffer the intermediate data for back-propagation. 2) GCN training involves both memory-intensive data reduction and computation-intensive features/gradients update operations. Such a heterogeneous nature challenges current CPU/GPU platforms. 3) The irregularity of graphs and the complex training dataflow jointly increase the difficulty of improving a GCN training system\u2019s efficiency. This paper presents GCNear, a hybrid architecture to tackle these challenges. Specifically, GCNear adopts a DIMM-based memory system to provide easy-to-scale memory capacity. To match the heterogeneous nature, we categorize GCN training operations as memory-intensive Reduce and computation-intensive Update operations. We then offload Reduce operations to onDIMM NMEs, making full use of the high aggregated local bandwidth. We adopt a CAE with sufficient computation capacity to process Update operations. We further propose several optimization strategies to deal with the irregularity of GCN tasks and improve GCNear\u2019s performance. Comprehensive evaluations on twelve GCN training tasks demonstrate that GCNear achieves 24.8\u00d7 / 2.2\u00d7 geomean speedup and 61.9\u00d7 / 6.4\u00d7 (geomean) higher energy efficiency compared to Xeon E5-2698-v4 CPU and NVIDIA V100 GPU. To deal with deep GCN models and the ever-increasing graph scale, we also propose a Multi-GCNear system. Compared to state-of-the-art Roc and DistGNN systems, Multi-GCNear achieves up to 2.1\u00d7 and 3.1\u00d7 higher training speed, respectively.", "venue": "ArXiv", "authors": ["Zhe  Zhou", "Cong  Li", "Xuechao  Wei", "Guangyu  Sun"], "year": 2021, "n_citations": 0}
{"id": 840158, "s2_id": "ab4ad7b5768d79c6db39b86b730248f0c28df795", "title": "BinArray: A Scalable Hardware Accelerator for Binary Approximated CNNs", "abstract": "Deep Convolutional Neural Networks (CNNs) have become state-of-the art for classification tasks due to their superior accuracy. BinArray is a custom hardware accelerator for CNNs with binary approximated weights. The binary approximation used is a network compression technique that drastically reduces the number of multiplications required per inference with no or very little accuracy degradation. BinArray scales and allows to compromise between hardware resource usage and throughput by means of three design parameters transparent to the user. Furthermore, it is possible to select between high accuracy or throughput dynamically during runtime. BinArray has been optimized at the register transfer level and operates at 400 MHz as instruction-set processor within a heterogenous XC7Z045-2 FPGA-SoC platform. Experimental results show that BinArray scales to match the performance of other accelerators for different network sizes. Even for the largest MobileNet only 50% of the target device and only 96 DSP blocks are utilized.", "venue": "2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)", "authors": ["Mario  Fischer", "Juergen  Wassner"], "year": 2021, "n_citations": 0}
{"id": 840291, "s2_id": "40142f0905650eec92ba7b225b409af26158d967", "title": "Performance Analysis of Linear Algebraic Functions Using Reconfigurable Computing", "abstract": "This paper introduces a new mapping of geometrical transformation on the MorphoSys (M1) reconfigurable computing (RC) system. New mapping techniques for some linear algebraic functions are recalled. A new mapping for geometrical transformation operations is introduced and their performance on the M1 system is evaluated. The translation and scaling transformation addressed in this mapping employ some vector\u2013vector and vector\u2013scalar operations [6 and 7]. A performance analysis study of the M1 RC system is also presented to evaluate the efficiency of the algorithm execution. Numerical examples were simulated to validate our results, using the MorphoSys mULATE program, which emulates M1 operations.", "venue": "The Journal of Supercomputing", "authors": ["Issam W. Damaj", "Hassan B. Diab"], "year": 2004, "n_citations": 8}
{"id": 840406, "s2_id": "217aa2aad2578758656769641155f4611865f3ba", "title": "Cantilever-based biosensors in CMOS technology", "abstract": "Single-chip CMOS-based biosensors that feature microcantilevers as transducer elements are presented. The cantilevers are functionalized to capture specific analytes, e.g., proteins or DNA. The binding of the analyte changes the mechanical properties of the cantilevers such as surface stress and resonant frequency, which can be detected by an integrated Wheatstone bridge. The monolithic integrated readout allows for a high signal-to-noise ratio, lowers the sensitivity to external interference and enables autonomous device operation.", "venue": "Design, Automation and Test in Europe", "authors": ["Kay-Uwe  Kirstein", "Yue  Li", "Martin  Zimmermann", "Cyril  Vancura", "Tormod  Volden", "Wan Ho Song", "Jan  Lichtenberg", "Andreas  Hierlemann"], "year": 2005, "n_citations": 27}
{"id": 842103, "s2_id": "fceaa4770d988e1bef10a910b6a6f972ff5315f3", "title": "Accelerating Markov Random Field Inference with Uncertainty Quantification", "abstract": "Statistical machine learning methods have widespread applications in various domains. These methods include probabilistic algorithms, such as Markov Chain Monte-Carlo (MCMC), which rely on generating random numbers from probability distributions. These algorithms are computationally expensive on conventional processors, yet their statistical properties, namely interpretability and uncertainty quantification compared to deep learning, make them an attractive alternative approach. Therefore, hardware specialization can be adopted to address the shortcomings of conventional processors in running these applications. In this paper, we propose a high-throughput accelerator for first-order Markov Random Field (MRF) inference, a powerful model for representing a wide range of applications, using MCMC with Gibbs sampling. We propose a tiled architecture which takes advantage of near-memory computing, and memory banking and communication schemes tailored to the semantics of first-order MRF. Additionally, we propose a novel hybrid onchip/off-chip memory system and logging scheme to efficiently support uncertainty quantification. This memory system design is not specific to MRF models and is applicable to applications using probabilistic algorithms. In addition, it dramatically reduces offchip memory bandwidth requirements. We implemented an FPGA prototype of our proposed architecture using high-level synthesis tools and achieved 146MHz frequency for an accelerator with 32 function units on an Intel Arria 10 FPGA. Compared to prior work on FPGA, our accelerator achieves 26\u00d7 speedup. Furthermore, our proposed memory system and logging scheme to support uncertainty quantification reduces off-chip bandwidth consumption by 71% for two benchmark applications. ASIC analysis in 15nm technology node shows our design with 2048 function units running at 3GHz outperforms GPU implementations of motion estimation and stereo vision run on Nvidia RTX 2080 Ti by 120\u00d7-210\u00d7 while occupying only 7.7% of the area.", "venue": "ArXiv", "authors": ["Ramin  Bashizade", "Xiangyu  Zhang", "Sayan  Mukherjee", "Alvin R. Lebeck"], "year": 2021, "n_citations": 1}
{"id": 842920, "s2_id": "1d34cc17abb9d56bc1a3bbe0ac14b157d2c8cfe8", "title": "Deriving AOC C-Models from D&V Languages for Single- or Multi-Threaded Execution Using C or C++", "abstract": "The C language is getting more and more popular as a design and verification language (DVL). SystemC, ParC [1] and Cx [2] are based on C. C-models of the design and verification environment can also be generated from new DVLs (e.g. Chisel [3]) or classical DVLs such as VHDL or Verilog. The execution of these models is usually license free and presumably faster than their alternative counterparts (simulators). This paper proposes activity-dependent, ordered, cycle-accurate (AOC) C-models to speed up simulation time. It compares the results with alternative concepts. The paper also examines the execution of the AOC C-model on a multithreaded processor environment.", "venue": "MBMV", "authors": ["Tobias  Strauch"], "year": 2015, "n_citations": 2}
{"id": 844056, "s2_id": "2df595c48e616ce05edfb6093babbd7762667d5d", "title": "Reducing DRAM Access Latency by Exploiting DRAM Leakage Characteristics and Common Access Patterns", "abstract": "DRAM-based memory is a critical factor that creates a bottleneck on the system performance since the processor speed largely outperforms the DRAM latency. In this thesis, we develop a low-cost mechanism, called ChargeCache, which enables faster access to recently-accessed rows in DRAM, with no modifications to DRAM chips. Our mechanism is based on the key observation that a recently-accessed row has more charge and thus the following access to the same row can be performed faster. To exploit this observation, we propose to track the addresses of recently-accessed rows in a table in the memory controller. If a later DRAM request hits in that table, the memory controller uses lower timing parameters, leading to reduced DRAM latency. Row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency. We evaluate ChargeCache on a wide variety of workloads and show that it provides significant performance and energy benefits for both single-core and multi-core systems.", "venue": "ArXiv", "authors": ["Hasan  Hassan"], "year": 2016, "n_citations": 0}
{"id": 844072, "s2_id": "5519ab66d8f2984cf5f6a2bf26112e7f6b861444", "title": "High Performance and Optimal Configuration of Accurate Heterogeneous Block-Based Approximate Adder", "abstract": "Approximate computing is an emerging paradigm to improve power and performance efficiency for error-resilient application. Recent approximate adders have significantly extended the design space of accuracy-power configurable approximate adders, and find optimal designs by exploring the design space. In this paper, a new energy-efficient heterogeneous block-based approximate adder (HBBA) is proposed; which is a generic/configurable model that can be transformed to a particular adder by defining some configurations. An HBBA, in general, is composed of heterogeneous sub-adders, where each sub-adder can have a different configuration. A set of configurations of all the sub-adders in an HBBA defines its configuration. The block-based adders are approximated through inexact logic configuration and truncated carry chains. HBBA increases design space providing additional design points that fall on the Pareto-front and offer better power-accuracy tradeoff compared to other configurations. Furthermore, to avoid Mont-Carlo simulations, we propose an analytical modelling technique to evaluate the probability of error and Probability Mass Function (PMF) of error value. Moreover, the estimation method estimates delay, area and power of heterogeneous blockbased approximate adders. Thus, based on the analytical model and estimation method, the optimal configuration under a given error constraint can be selected from the whole design space of the proposed adder model by exhaustive search. The simulation results show that our HBBA provides improved accuracy in terms of error metrics compared to some state-of-the-art approximate adders. HBBA with 32 bits length serves about 15% reduction in area and up to 17% reduction in energy compared to stateof-the-art approximate adders.", "venue": "ArXiv", "authors": ["Ebrahim  Farahmand", "Ali  Mahani", "Muhammad Abdullah Hanif", "Muhammad  Shafique"], "year": 2021, "n_citations": 0}
{"id": 849462, "s2_id": "588f3da94cae1900fd978b80d96ac88a2e035683", "title": "Multipliers: comparison of Fourier transformation based method and Synopsys design technique for up to 32 bits inputs in regular and saturation arithmetics", "abstract": "The technique for hardware multiplication based upon Fourier transformation has been introduced. The technique has the highest efficiency on multiplication units with up to 8 bit range. Each multiplication unit is realized on base of the minimized Boolean functions. Experimental data showed that this technique the multiplication process speed up to 20% higher for 2-8 bit range of input operands and up to 3% higher for 8-32 bit range of input operands than analogues designed by Synopsys technique.", "venue": "ArXiv", "authors": ["Danila A. Gorodecky"], "year": 2016, "n_citations": 0}
{"id": 849772, "s2_id": "bbac786b104781aa6fa7a67e77f45c786a417934", "title": "Recent development in analog computation: a brief overview", "abstract": "The recent development in analog computation is reviewed in this paper. Analog computation was used in many applications where power and energy efficiency is of paramount importance. It is shown that by using innovative architecture and circuit design, analog computation systems can achieve much higher energy efficiency than their digital counterparts, as they are able to exploit the computational power inherent to the devices and physics. However, these systems do suffer from some disadvantages, such as lower accuracy and speed, and designers have come up with novel approaches to overcome them. The paper provides an overview of analog computation systems, from basic components such as memory and arithmetic elements, to architecture and system design.", "venue": "ArXiv", "authors": ["Yang  Xue"], "year": 2015, "n_citations": 9}
{"id": 855410, "s2_id": "ecb911fce98323a06dd2ec63f65b0dab3b38119a", "title": "Analysing digital in-memory computing for advanced finFET node", "abstract": "Digital In-memory computing improves energy efficiency and throughput of a data intensive process, which incur memory thrashing and, resulting multiple same memory accesses in a von Neumann architecture. Digital in-memory computing involves accessing multiple SRAM cells simultaneously, which may result in a bit flip when not timed critically. Therefore we discuss the transient voltage characteristics of the bitlines during an SRAM compute. To improve the packaging density and also avoid MOSFET downscaling issues, we use a 7-nm predictive PDK which uses a finFET node. The finFET process has discrete fins and a lower Voltage supply, which makes the design of in-memory compute SRAM difficult. In this paper, we design a 6T SRAM cell in 7-nm finFET node and compare its SNMs with a UMC 28nm node implementation. Further, we design and simulate the rest of the SRAM peripherals, and in-memory computation for an advanced finFET node.", "venue": "ArXiv", "authors": ["Veerendra S. Devaraddi", "Joycee M. Mekie"], "year": 2021, "n_citations": 0}
{"id": 855813, "s2_id": "3238dee3ca691dfd7a1685120467e809c4f34cb4", "title": "TSM: Temporal Shift Module for Efficient and Scalable Video Understanding on Edge Device", "abstract": "The explosive growth in video streaming requires video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. The key idea of TSM is to shift part of the channels along the temporal dimension, thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. TSM offers several unique advantages. Firstly, TSM has high performance; it ranks the first on the Something-Something leaderboard upon submission. Secondly, TSM has high efficiency; it achieves a high frame rate of 74fps and 29fps for online video recognition on Jetson Nano and Galaxy Note8. Thirdly, TSM has higher scalability compared to 3D networks, enabling large-scale Kinetics training on 1,536 GPUs in 15 minutes. Lastly, TSM enables action concepts learning, which 2D networks cannot model; we visualize the category attention map and find that spatial-temporal action detector emerges during the training of classification tasks. The code is publicly available.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": ["Ji  Lin", "Chuang  Gan", "Kuan  Wang", "Song  Han"], "year": 2020, "n_citations": 2}
{"id": 858194, "s2_id": "ba1a6b099a8cb52b723c272f0ef9d74c977d7fa3", "title": "CMOS-based biosensor arrays", "abstract": "CMOS-based sensor array chips provide new and attractive features as compared to today's standard tools for medical, diagnostic, and biotechnical applications. Examples for molecule- and cell-based approaches and related circuit design issues are discussed.", "venue": "Design, Automation and Test in Europe", "authors": ["Roland  Thewes", "Christian  Paulus", "Meinrad  Schienle", "Franz  Hofmann", "Alexander  Frey", "Ralf  Brederlow", "M.  Augustyniak", "Martin  Jenkner", "Bj\u00f6rn  Eversmann", "Petra  Schindler-Bauer", "Melanie  Atzesberger", "Birgit  Holzapfl", "Gottfried  Beer", "Thomas  Haneder", "Hans-Christian  Hanke"], "year": 2005, "n_citations": 4}
{"id": 859470, "s2_id": "2c1e6b6c5e4d61dfeab59493b03af7ccf96be0e2", "title": "SMASH: Sparse Matrix Atomic Scratchpad Hashing", "abstract": "of the Thesis SMASH: Sparse Matrix Atomic Scratchpad Hashing by Kaustubh Shivdikar Master of Science in Electrical and Computer Engineering Northeastern University, April 2021 Dr. David Kaeli, Adviser Abstract: Sparse matrices, more specifically Sparse Matrix-Matrix Multiply (SpGEMM) kernels, are commonly found in a wide range of applications, spanning graph-based path-finding to machine learning algorithms (e.g., neural networks). A particular challenge in implementing SpGEMM kernels has been the pressure placed on DRAM memory. One approach to tackle this problem is to use an inner product method for the SpGEMM kernel implementation. While the inner product produces fewer intermediate results, it can end up saturating the memory bandwidth, given the high number of redundant fetches of the input matrix elements. Using an outer product-based SpGEMM kernel can reduce redundant fetches, but at the cost of increased overhead due to extra computation and memory accesses for producing/managing partial products. In this thesis, we introduce a novel SpGEMM kernel implementation based on the rowwise product approach. We leverage atomic instructions to merge intermediate partial products as they are generated. The use of atomic instructions eliminates the need to create partial product matrices, thus eliminating redundant DRAM fetches. To evaluate our row-wise product approach, we map an optimized SpGEMM kernel to a custom accelerator designed to accelerate graph-based applications. The targeted accelerator is an experimental system named PIUMA, being developed by Intel. PIUMA provides several attractive features, including fast context switching, user-configurable caches, globally addressable memory, non-coherent caches, and asynchronous pipelines. We tailor our SpGEMM kernel to exploit many of the features of the PIUMA fabric. This thesis compares our SpGEMM implementation against prior solutions, all mapped to the PIUMA framework. We briefly describe some of the PIUMA architecture features and then delve into the details of our optimized SpGEMM kernel. Our SpGEMM kernel can achieve 9.4\u00d7 speedup as compared to competing approaches. Sparse matrices, more specifically Sparse Matrix-Matrix Multiply (SpGEMM) kernels, are commonly found in a wide range of applications, spanning graph-based path-finding to machine learning algorithms (e.g., neural networks). A particular challenge in implementing SpGEMM kernels has been the pressure placed on DRAM memory. One approach to tackle this problem is to use an inner product method for the SpGEMM kernel implementation. While the inner product produces fewer intermediate results, it can end up saturating the memory bandwidth, given the high number of redundant fetches of the input matrix elements. Using an outer product-based SpGEMM kernel can reduce redundant fetches, but at the cost of increased overhead due to extra computation and memory accesses for producing/managing partial products. In this thesis, we introduce a novel SpGEMM kernel implementation based on the rowwise product approach. We leverage atomic instructions to merge intermediate partial products as they are generated. The use of atomic instructions eliminates the need to create partial product matrices, thus eliminating redundant DRAM fetches. To evaluate our row-wise product approach, we map an optimized SpGEMM kernel to a custom accelerator designed to accelerate graph-based applications. The targeted accelerator is an experimental system named PIUMA, being developed by Intel. PIUMA provides several attractive features, including fast context switching, user-configurable caches, globally addressable memory, non-coherent caches, and asynchronous pipelines. We tailor our SpGEMM kernel to exploit many of the features of the PIUMA fabric. This thesis compares our SpGEMM implementation against prior solutions, all mapped to the PIUMA framework. We briefly describe some of the PIUMA architecture features and then delve into the details of our optimized SpGEMM kernel. Our SpGEMM kernel can achieve 9.4\u00d7 speedup as compared to competing approaches.", "venue": "ArXiv", "authors": ["Kaustubh  Shivdikar"], "year": 2021, "n_citations": 0}
{"id": 863218, "s2_id": "bc7eb8fbf8ac53eaafc8c8127ed2675f1d901da2", "title": "True-data Testbed for 5G/B5G Intelligent Network", "abstract": "Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile communications will shift from facilitating interpersonal communications to supporting Internet of Everything (IoE), where intelligent communications with full integration of big data and artificial intelligence (AI) will play an important role in improving network efficiency and providing high-quality service. As a rapid evolving paradigm, the AI-empowered mobile communications demand large amounts of data acquired from real network environment for systematic test and verification. Hence, we build the world's first true-data testbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site experimental networks, data acquisition & data warehouse, and AI engine & network optimization. In the TTIN, true network data acquisition, storage, standardization, and analysis are available, which enable system-level online verification of B5G/6G-orientated key technologies and support data-driven network optimization through the closed-loop control mechanism. This paper elaborates on the system architecture and module design of TTIN. Detailed technical specifications and some of the established use cases are also showcased.", "venue": "ArXiv", "authors": ["Yongming  Huang", "Shengheng  Liu", "Cheng  Zhang", "Xiaohu  You", "Hequan  Wu"], "year": 2020, "n_citations": 2}
{"id": 863635, "s2_id": "86d4e435eac2fdef98ba9fd42ecd73fe03b0cd1e", "title": "Sustainable AI: Environmental Implications, Challenges and Opportunities", "abstract": "This paper explores the environmental impact of the super-linear growth trends for AI from a holistic perspective, spanning Data, Algorithms, and System Hardware. We characterize the carbon footprint of AI computing by examining the model development cycle across industry-scale machine learning use cases and, at the same time, considering the life cycle of system hardware. Taking a step further, we capture the operational and manufacturing carbon footprint of AI computing and present an end-to-end analysis for what and how hardware-software design and at-scale optimization can help reduce the overall carbon footprint of AI. Based on the industry experience and lessons learned, we share the key challenges and chart out important development directions across the many dimensions of AI. We hope the key messages and insights presented in this paper can inspire the community to advance the field of AI in an environmentally-responsible manner.", "venue": "ArXiv", "authors": ["Carole-Jean  Wu", "Ramya  Raghavendra", "Udit  Gupta", "Bilge  Acun", "Newsha  Ardalani", "Kiwan  Maeng", "Gloria  Chang", "Fiona Aga Behram", "James  Huang", "Charles  Bai", "Michael  Gschwind", "Anurag  Gupta", "Myle  Ott", "Anastasia  Melnikov", "Salvatore  Candido", "David  Brooks", "Geeta  Chauhan", "Benjamin  Lee", "Hsien-Hsin S. Lee", "Bugra  Akyildiz", "Maximilian  Balandat", "Joe  Spisak", "Ravi  Jain", "Mike  Rabbat", "Kim  Hazelwood"], "year": 2021, "n_citations": 2}
{"id": 864934, "s2_id": "40980292172ec07e8e38ccd0546c5b6cc0baed55", "title": "Dynamic Computing Random Access Memory", "abstract": "The present von Neumann computing paradigm involves a significant amount of information transfer between a central processing unit and memory, with concomitant limitations in the actual execution speed. However, it has been recently argued that a different form of computation, dubbed memcomputing (Di Ventra and Pershin 2013 Nat. Phys. 9 200-2) and inspired by the operation of our brain, can resolve the intrinsic limitations of present day architectures by allowing for computing and storing of information on the same physical platform. Here we show a simple and practical realization of memcomputing that utilizes easy-to-build memcapacitive systems. We name this architecture dynamic computing random access memory (DCRAM). We show that DCRAM provides massively-parallel and polymorphic digital logic, namely it allows for different logic operations with the same architecture, by varying only the control signals. In addition, by taking into account realistic parameters, its energy expenditures can be as low as a few fJ per operation. DCRAM is fully compatible with CMOS technology, can be realized with current fabrication facilities, and therefore can really serve as an alternative to the present computing technology.", "venue": "Nanotechnology", "authors": ["Fabio L. Traversa", "Fabrizio  Bonani", "Yuriy V. Pershin", "Massimiliano Di Ventra"], "year": 2014, "n_citations": 36}
{"id": 870053, "s2_id": "9a93949ae23527184258ea282754e33ea556715d", "title": "Associative Memory For Reversible Programming and Charge Recovery", "abstract": "Presented below is an interesting type of associative memory called toggle memory based on the concept of T flip flops, as opposed to D flip flops. Toggle memory supports both reversible programming and charge recovery. Circuits designed using the principles delineated below permit matchlines to charge and discharge with near zero energy dissipation. The resulting lethargy is compensated by the massive parallelism of associative memory. Simulation indicates over 33x reduction in energy dissipation using a sinusoidal power supply at 2 MHz, assuming realistic 50 nm MOSFET models.", "venue": "ArXiv", "authors": ["John Robert Burger"], "year": 2006, "n_citations": 0}
{"id": 873818, "s2_id": "62f525b90fafd7c4639e7b147eb9b6a815470b06", "title": "Hardware Architecture of Embedded Inference Accelerator and Analysis of Algorithms for Depthwise and Large-Kernel Convolutions", "abstract": "In order to handle modern convolutional neural networks (CNNs) efficiently, a hardware architecture of CNN inference accelerator is proposed to handle depthwise convolutions and regular convolutions, which are both essential building blocks for embedded-computer-vision algorithms. Different from related works, the proposed architecture can support filter kernels with different sizes with high flexibility since it does not require extra costs for intra-kernel parallelism, and it can generate convolution results faster than the architecture of the related works. The experimental results show the importance of supporting depthwise convolutions and dilated convolutions with the proposed hardware architecture. In addition to depthwise convolutions with large-kernels, a new structure called DDC layer, which includes the combination of depthwise convolutions and dilated convolutions, is also analyzed in this paper. For face detection, the computational costs decrease by 30%, and the model size decreases by 20% when the DDC layers are applied to the network. For image classification, the accuracy is increased by 1% by simply replacing 3\u00d7 3 filters with 5\u00d7 5 filters in depthwise convolutions.", "venue": "ECCV Workshops", "authors": ["Tse-Wei  Chen", "Wei  Tao", "Deyu  Wang", "Dongchao  Wen", "Kinya  Osa", "Masami  Kato"], "year": 2020, "n_citations": 0}
{"id": 873845, "s2_id": "0929710faea865405c46706287b5b7861f6ff1e8", "title": "Data Criticality in Multithreaded Applications: An Insight for Many-Core Systems", "abstract": "Multithreaded applications are capable of exploiting the full potential of many-core systems. However, network-on-chip (NoC)-based intercore communication in many-core systems is responsible for 60%\u201375% of the miss latency experienced by multithreaded applications. Delay in the arrival of critical data at the requesting core severely hampers performance. This brief presents some interesting insights about how critical data are requested from the memory by multithreaded applications. Then it investigates the cause of delay in NoC and how it affects the performance. Finally, this brief shows how NoC-aware memory access optimizations can significantly improve performance. Our experimental evaluation considers Early Restart memory access optimization and demonstrates that by exploiting available NoC resources, critical data can be prioritized to reduce miss penalty by 11% and improve overall system performance by 9%.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Abhijit  Das", "John  Jose", "Prabhat  Mishra"], "year": 2021, "n_citations": 0}
{"id": 876965, "s2_id": "03e4c79715de9b1a15affe869c566e6e5c5a39fe", "title": "TMA: Tera-MACs/W Neural Hardware Inference Accelerator with a Multiplier-less Massive Parallel Processor", "abstract": "Computationally intensive Inference tasks of Deep neural networks have enforced revolution of new accelerator architecture to reduce power consumption as well as latency. The key figure of merit in hardware inference accelerators is the number of multiply-and-accumulation operations per watt (MACs/W), where, the state-of-the-arts MACs/W remains several hundreds Giga-MACs/W. We propose a Tera-MACS/W neural hardware inference Accelerator (TMA) with 8-bit activations and scalable integer weights less than 1-byte. The architectures main feature is configurable neural processing element for matrix-vector operations. The proposed neural processing element has Multiplier-less Massive Parallel Processor to work without any multiplications, which makes it attractive for energy efficient high-performance neural network applications. We benchmark our systems latency, power, and performance using Alexnet trained on ImageNet. Finally, we compared our accelerators throughput and power consumption to the prior works. The proposed accelerator outperforms the state of the art in terms of energy and area achieving 2.3 TMACS/W@1.0 V, 65 nm CMOS technology.", "venue": "Int. J. Circuit Theory Appl.", "authors": ["Hyunbin  Park", "Dohyun  Kim", "Shiho  Kim"], "year": 2021, "n_citations": 1}
{"id": 878878, "s2_id": "34e4ce08b55389d560e0157c7a817072f69e5648", "title": "Resource-Aware Just-in-Time OpenCL Compiler for Coarse-Grained FPGA Overlays", "abstract": "FPGA vendors have recently started focusing on OpenCL for FPGAs because of its ability to leverage the parallelism inherent to heterogeneous computing platforms. OpenCL allows programs running on a host computer to launch accelerator kernels which can be compiled at run-time for a specific architecture, thus enabling portability. However, the prohibitive compilation times (specifically the FPGA place and route times) are a major stumbling block when using OpenCL tools from FPGA vendors. The long compilation times mean that the tools cannot effectively use just-in-time (JIT) compilation or runtime performance scaling. Coarse-grained overlays represent a possible solution by virtue of their coarse granularity and fast compilation. In this paper, we present a methodology for run-time compilation of OpenCL kernels to a DSP block based coarse-grained overlay, rather than directly to the fine-grained FPGA fabric. The proposed methodology allows JIT compilation and on-demand resource-aware kernel replication to better utilize available overlay resources, raising the abstraction level while reducing compile times significantly. We further demonstrate that this approach can even be used for run-time compilation of OpenCL kernels on the ARM processor of the embedded heterogeneous Zynq device.", "venue": "ArXiv", "authors": ["Abhishek Kumar Jain", "Douglas L. Maskell", "Suhaib A. Fahmy"], "year": 2017, "n_citations": 2}
{"id": 881802, "s2_id": "8f407875eb303bbb6037e57efa57c0fffd4b6e00", "title": "LogicNets: Co-Designed Neural Networks and Circuits for Extreme-Throughput Applications", "abstract": "Deployment of deep neural networks for applications that require very high throughput or extremely low latency is a severe computational challenge, further exacerbated by inefficiencies in mapping the computation to hardware. We present a novel method for designing neural network topologies that directly map to a highly efficient FPGA implementation. By exploiting the equivalence of artificial neurons with quantized inputs/outputs and truth tables, we can train quantized neural networks that can be directly converted to a netlist of truth tables, and subsequently deployed as a highly pipelinable, massively parallel FPGA circuit. However, the neural network topology requires careful consideration since the hardware cost of truth tables grows exponentially with neuron fan-in. To obtain smaller networks where the whole netlist can be placed-and-routed onto a single FPGA, we derive a fan-in driven hardware cost model to guide topology design, and combine high sparsity with few-bit activation quantization to limit the neuron fan-in. We evaluate our approach on two tasks with very high intrinsic throughput requirements in high-energy physics and network intrusion detection. We show that the combination of sparsity and few-bit activation quantization results in high-speed circuits with small logic depth and low LUT cost, demonstrating competitive accuracy with less than 15 ns of inference latency and throughput in the hundreds of millions of inferences per second.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Yaman  Umuroglu", "Yash  Akhauri", "Nicholas J. Fraser", "Michaela  Blott"], "year": 2020, "n_citations": 13}
{"id": 882449, "s2_id": "b243e05464dea777b00d6c72848c94493b451dfc", "title": "Design space exploration in the microthreaded many-core architecture", "abstract": "Design space exploration is commonly performed in embedded system, where the architecture is a complicated piece of engineering. With the current trend of many-core systems, design space exploration in general-purpose computers can no longer be avoided. Microgrid is a complicated architecture, and therefor we need to perform design space exploration. Generally, simulators are used for the design space exploration of an architecture. Different simulators with different levels of complexity, simulation time and accuracy are used. Simulators with little complexity, low simulation time and reasonable accuracy are desirable for the design space exploration of an architecture. These simulators are referred as high-level simulators and are commonly used in the design of embedded systems. However, the use of high-level simulation for design space exploration in general-purpose computers is a relatively new area of research.", "venue": "ArXiv", "authors": ["M. Irfan Uddin"], "year": 2013, "n_citations": 5}
{"id": 885105, "s2_id": "aa97d9f856e51f27ea59751a37bf5aca5fa59a5b", "title": "Energy-Efficient Deflection-based On-chip Networks: Topology, Routing, Flow Control", "abstract": "As the number of cores scales to tens and hundreds, the energy consumption of routers across various types of on-chip networks in chip muiltiprocessors (CMPs) increases significantly. A major source of this energy consumption comes from the input buffers inside Network-on-Chip (NoC) routers, which are traditionally designed to maximize performance. To mitigate this high energy cost, many works propose bufferless router designs that utilize deflection routing to resolve port contention. While this approach is able to maintain high performance relative to its buffered counterparts at low network traffic, the bufferless router design suffers performance degradation under high network load. In order to maintain high performance and energy efficiency under both low and high network loads, this chapter discusses critical drawbacks of traditional bufferless designs and describes recent research works focusing on two major modifications to improve the overall performance of the traditional bufferless network-on-chip design. The first modification is a minimally-buffered design that introduces limited buffering inside critical parts of the on-chip network in order to reduce the number of deflections. The second modification is a hierarchical bufferless interconnect design that aims to further improve performance by limiting the number of hops each packet needs to travel while in the network. In both approaches, we discuss design tradeoffs and provide evaluation results based on common CMP configurations with various network topologies to show the effectiveness of each proposal.", "venue": "ArXiv", "authors": ["Rachata  Ausavarungnirun", "Onur  Mutlu"], "year": 2021, "n_citations": 0}
{"id": 885447, "s2_id": "8210f5f010721e17416f9758f06b01a78abf3bc1", "title": "DMR-based Technique for Fault Tolerant AES S-box Architecture", "abstract": "This paper presents a high-throughput fault-resilient hardware implementation of AES S-box, called HFS-box. If a transient natural or even malicious fault in each pipeline stage is detected, the corresponding error signal becomes high and as a result, the control unit holds the output of our proposed DMR voter till the fault effect disappears. The proposed low-cost HFS-box provides a high capability of fault-tolerant against transient faults with any duration by putting low area overhead, i.e. 137%, and low throughput degradation, i.e. 11.3%, on the original implementation.", "venue": "ArXiv", "authors": ["Mahdi  Taheri", "Saeideh  Sheikhpour", "Mohammad Saeed Ansari", "Ali  Mahani"], "year": 2020, "n_citations": 0}
{"id": 890383, "s2_id": "8daf73b0bf1520d70059770249310600335200de", "title": "A New Paradigm for Fault-Tolerant Computing with Interconnect Crosstalks", "abstract": "The CMOS integrated chips at advanced technology nodes are becoming more vulnerable to various sources of faults like manufacturing imprecisions, variations, aging, etc. Additionally, the intentional fault attacks (e.g., high power microwave, cybersecurity threats, etc.) and environmental effects (i.e., radiation) also pose reliability threats to integrated circuits. Though the traditional hardware redundancy-based techniques like Triple Modular Redundancy (TMR), Quadded Logic (QL) etc. mitigate the risk to some extent, they add huge hardware overhead and are not very effective. Truly polymorphic circuits that are inherently capable of achieving multiple functionalities in a limited footprint could enhance the fault-resilience/recovery of the circuits with limited overhead. We demonstrate a novel crosstalk logic based polymorphic circuit approach to achieve compact and efficient fault resilient circuits. We show a range of polymorphic primitive gates and their usage in an example functional unit. The functional unit is a single arithmetic circuit that is capable of delivering Multiplication/Sorting/Addition output depending on the control inputs. Using such polymorphic computing units in an ALU would imply that a correct path for functional output is possible even when 2/3rd of the ALU is damaged. Moreover, our benchmarking results show that the crosstalk polymorphic logic style achieves 28% and 62% reduction in transistor count compared to existing polymorphic techniques and CMOS based implementation, respectively. In conjunction with fault detection algorithms, the proposed polymorphic circuit concept can be transformative for fault tolerant circuit design directions with minimum overhead.", "venue": "2018 IEEE International Conference on Rebooting Computing (ICRC)", "authors": ["Naveen Kumar Macha", "Bhavana Tejaswini Repalle", "Sandeep  Geedipally", "Rafael  Rios", "Mostafizur  Rahman"], "year": 2018, "n_citations": 6}
{"id": 890629, "s2_id": "2d98a0b0e5db7809a10349f412e32a9fe06e4b53", "title": "Optimal Memoryless Encoding for Low Power Off-Chip Data Buses", "abstract": "Off-chip buses account for a significant portion of the total system power consumed in embedded systems. Bus encoding schemes have been proposed to minimize power dissipation, but none has been demonstrated to be optimal with respect to any measure. In this paper, we give the first provably optimal and explicit (polynomial-time constructible) families of memoryless codes for minimizing bit transitions in off-chip buses. Our results imply that having access to a clock does not make a memoryless encoding scheme that minimizes bit transitions more powerful", "venue": "2006 IEEE/ACM International Conference on Computer Aided Design", "authors": ["Yeow Meng Chee", "Charles J. Colbourn", "Alan C. H. Ling"], "year": 2006, "n_citations": 6}
{"id": 891183, "s2_id": "c7d8e4a2c0b3e85b1dfa3227f5d823487928d4ba", "title": "Hardware Trojan Detection Using Controlled Circuit Aging", "abstract": "This paper reports a novel approach that uses transistor aging in an integrated circuit (IC) to detect hardware Trojans. When a transistor is aged, it results in delays along several paths of the IC. This increase in delay results in timing violations that reveal as timing errors at the output of the IC during its operation. We present experiments using aging-aware standard cell libraries to illustrate the usefulness of the technique in detecting hardware Trojans. Combining IC aging with over-clocking produces a pattern of bit errors at the IC output by the induced timing violations. We use machine learning to learn the bit error distribution at the output of a clean IC. We differentiate the divergence in the pattern of bit errors because of a Trojan in the IC from this baseline distribution. We simulate the golden IC and show robustness to IC-to-IC manufacturing variations. The approach is effective and can detect a Trojan even if we place it far off the critical paths. Results on benchmarks from the Trust-hub show a detection accuracy of \u2265 99%.", "venue": "IEEE Access", "authors": ["Virinchi Roy Surabhi", "Prashanth  Krishnamurthy", "Hussam  Amrouch", "Kanad  Basu", "J\u00f6rg  Henkel", "Ramesh  Karri", "Farshad  Khorrami"], "year": 2020, "n_citations": 2}
{"id": 892446, "s2_id": "d414f69fc25cb051ec61e53974ed27097f588789", "title": "Performance Evaluation of Sparse Matrix Multiplication Kernels on Intel Xeon Phi", "abstract": "Intel Xeon Phi is a recently released high-performance coprocessor which features 61 cores each supporting 4 hardware threads with 512-bit wide SIMD registers achieving a peak theoretical performance of 1Tflop/s in double precision. Its design differs from classical modern processors; it comes with a large number of cores, the 4-way hyperthreading capability allows many applications to saturate the massive memory bandwidth, and its large SIMD capabilities allow to reach high computation throughput. The core of many scientific applications involves the multiplication of a large, sparse matrix with a single or multiple dense vectors which are not compute-bound but memory-bound. In this paper, we investigate the performance of the Xeon Phi coprocessor for these sparse linear algebra kernels. We highlight the important hardware details and show that Xeon Phi\u2019s sparse kernel performance is very promising and even better than that of cutting-edge CPUs and GPUs.", "venue": "PPAM", "authors": ["Erik  Saule", "Kamer  Kaya", "\u00dcmit V. \u00c7ataly\u00fcrek"], "year": 2013, "n_citations": 157}
{"id": 893145, "s2_id": "5eca0a60110715f2ca28cd9e4b632323120f465c", "title": "Characterizing Self-developing Biological Neural Networks: A First Step Towards Their Application to Computing Systems", "abstract": "Carbon nanotubes are often seen as the only alternative technology to silicon transistors. While they are the most likely short-term alternative, other longer-term alternatives should be studied as well, even if their properties are less familiar to chip designers. While contemplating biological neurons as an alternative component may seem preposterous at first sight, significant recent progress in CMOS-neuron interface suggests this direction may not be unrealistic; moreover, biological neurons are known to self-assemble into very large networks capable of complex information processing tasks, something that has yet to be achieved with other emerging technologies. \n \nThe first step to designing computing systems on top of biological neurons is to build an abstract model of self-assembled biological neural networks, much like computer architects manipulate abstract models of transistors and circuits. In this article, we propose a first model of the structure of biological neural networks. We provide empirical evidence that this model matches the biological neural networks found in living organisms, and exhibits the small-world graph structure properties commonly found in many large and self-organized systems, including biological neural networks. More importantly, we extract the simple local rules and characteristics governing the growth of such networks, enabling the development of potentially large but realistic biological neural networks, as would be needed for complex information processing/computing tasks. Based on this model, future work will be targeted to understanding the evolution and learning properties of such networks, and how they can be used to build computing systems.", "venue": "IWANN", "authors": ["Hugues  Berry", "Olivier  Temam"], "year": 2005, "n_citations": 1}
{"id": 893763, "s2_id": "338a34d1085f418046271543bec52e6cc8d6f5cc", "title": "Towards a theory of cache-efficient algorithms", "abstract": "We present a model that enables us to analyze the running time of an algorithm on a computer with a memory hierarchy with limited associativity, in terms of various cache parameters. Our cache model, an extension of Aggarwal and Vitter's I/O model, enables us to establish useful relationships between the cache complexity and the I/O complexity of computations. As a corollary, we obtain cache-efficient algorithms in the single-level cache model for fundamental problems like sorting, FFT, and an important subclass of permutations. We also analyze the average-case cache behavior of mergesort, show that ignoring associativity concerns could lead to inferior performance, and present supporting experimental evidence.We further extend our model to multiple levels of cache with limited associativity and present optimal algorithms for matrix transpose and sorting. Our techniques may be used for systematic exploitation of the memory hierarchy starting from the algorithm design stage, and for dealing with the hitherto unresolved problem of limited associativity.", "venue": "SODA '00", "authors": ["Sandeep  Sen", "Siddhartha  Chatterjee"], "year": 2000, "n_citations": 107}
{"id": 895280, "s2_id": "af58e7ffd68e1e96f478d2c74f2cd66439dcf186", "title": "FPGA implementation of short critical path CORDIC-based approximation of the eight-point DCT", "abstract": "This paper presents an efficient approach for multiplierless implementation for eight-point DCT approximation, which based on coordinate rotation digital computer (CORDIC) algorithm. The main design objective is to make critical path of corresponding circuits shorter and reduce the combinational delay of proposed scheme.", "venue": "ArXiv", "authors": ["Maxim  Vashkevich", "Marek  Parfieniuk", "Alexander A. Petrovsky"], "year": 2011, "n_citations": 2}
{"id": 895349, "s2_id": "ffafac9cbbde1939ffc060fceae6f40df37b5d57", "title": "EURETILE 2010-2012 summary: first three years of activity of the European Reference Tiled Experiment", "abstract": "Abstract This is the summary of first three years of activity of the EURETILE FP7 project 247846. EURETILE investigates and implements brain-inspired and fault-tolerant foundational innovations to the system architecture of massively parallel tiled computer architectures and the corresponding programming paradigm. The execution targets are a many-tile HW platform, and a many-tile simulator. A set of SW process - HW tile mapping candidates is generated by the holistic SW tool-chain using a combination of analytic and bio-inspired methods. The Hardware dependent Software is then generated, providing OS services with maximum efficiency/minimal overhead. The many-tile simulator collects profiling data, closing the loop of the SW tool chain. Fine-grain parallelism inside processes is exploited by optimized intra-tile compilation techniques, but the project focus is above the level of the elementary tile. The elementary HW tile is a multi-processor, which includes a fault tolerant Distributed Network Processor (for inter-tile communication) and ASIP accelerators. Furthermore, EURETILE investigates and implements the innovations for equipping the elementary HW tile with high-bandwidth, low-latency brain-like inter-tile communication emulating 3 levels of connection hierarchy, namely neural columns, cortical areas and cortex, and develops a dedicated cortical simulation benchmark: DPSNN-STDP (Distributed Polychronous Spiking Neural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverages on the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPES Integrated Project (2006-2009). The APE Parallel Computing Lab of INFN Roma is in charge of the EURETILE HW Design (QUonG system/APENet+ board/DNP (Distributed Network Processor) and Scientific Application Benchmarks. The Computer Engineering and Networks Laboratory (TIK) of ETH Zurich (Swiss Federal Institute of Technology) designs the high-level explicit parallel programming and automatic mapping tool (DOL/DAL) and a set of \u201cEmbedded Systems\u201d benchmarks. The Software for Systems on Silicon (SSS) of the ISS institute of RWTH Aachen, investigates and provides the parallel simulation technology and scalable simulation-based profiling/debugging support. The TIMA Laboratory of the University Joseph Fourier in Grenoble explores and deploys the HdS (Hardware dependent Software) including the distributed OS architecture. TARGET Compiler Technologies, the Belgian leading provider of retargetable software tools and compilers for the design, programming, and verification of application-specific processors (ASIPs), is in charge of the HW/SW Co-design tools for custom components of the EURETILE architecture. Grant Agreement no. 247846 Call: FP7-ICT-2009-4 Objective FET-ICT-2009.8.1 Concurrent Tera-device Computing Scientific Coordinator: Pier Stanislao Paolucci, Istituto Nazionale di Fisica Nucleare, Roma, Italy Administrative Coordinator: Michela Giovagnoli, Istituto Nazionale di Fisica Nucleare, Roma, Italy", "venue": "ArXiv", "authors": ["Pier Stanislao Paolucci", "Iuliana  Bacivarov", "Gert  Goossens", "Rainer  Leupers", "Fr\u00e9d\u00e9ric  Rousseau", "Christoph  Schumacher", "Lothar  Thiele", "Piero  Vicini"], "year": 2013, "n_citations": 15}
{"id": 895539, "s2_id": "239e9b4ed0e2aed3e6379ab48a2ba3db1b266815", "title": "Timely: Pushing Data Movements And Interfaces In Pim Accelerators Towards Local And In Time Domain", "abstract": "Resistive-random-access-memory (ReRAM) based processing-in-memory $(\\mathrm{R}^{2}\\mathrm{P}\\mathrm{I}\\mathrm{M})$ accelerators show promise in bridging the gap between Internet of Thing devices\u2019 constrained resources and Convolutional/Deep Neural Networks\u2019 (CNNs/DNNs\u2019) prohibitive energy cost. Specifically, $\\mathrm{R}^{2}\\mathrm{P}\\mathrm{I}\\mathrm{M}$ accelerators enhance energy efficiency by eliminating the cost of weight movements and improving the computational density through ReRAM\u2019s high density. However, the energy efficiency is still limited by the dominant energy cost of input and partial sum (Psum) movements and the cost of digital-to-analog (D/A) and analog-to-digital (AD) interfaces. In this work, we identify three energy-saving opportunities in $\\mathrm{R}^{2}\\mathrm{P}\\mathrm{I}\\mathrm{M}$ accelerators: analog data locality, time-domain interfacing, and input access reduction, and propose an innovative $\\mathrm{R}^{2}\\mathrm{P}\\mathrm{I}\\mathrm{M}$ accelerator called TIMELY, with three key contributions: (1) TIMELY adopts analog local buffers (ALBs) within ReRAM crossbars to greatly enhance the data locality, minimizing the energy overheads of both input and Psum movements; (2) TIMELY largely reduces the energy of each single D/A (and AD) conversion and the total number of conversions by using time-domain interfaces (TDIs) and the employed ALBs, respectively; (3) we develop an only-once input read $(\\mathrm{O}^{2}\\mathrm{I}\\mathrm{R})$ mapping method to further decrease the energy of input accesses and the number of D/A conversions. The evaluation with more than 10 CNN/DNN models and various chip configurations shows that, TIMELY outperforms the baseline $\\mathrm{R}^{2}\\mathrm{P}\\mathrm{I}\\mathrm{M}$ accelerator, PRIME, by one order of magnitude in energy efficiency while maintaining better computational density (up to $31.2\\times$) and throughput (up to $736.6\\times$). Furthermore, comprehensive studies are performed to evaluate the effectiveness of the proposed ALB, TDI, and $\\mathrm{O}^{2}\\mathrm{I}\\mathrm{R}$ in terms of energy savings and area reduction.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Weitao  Li", "Pengfei  Xu", "Yang  Zhao", "Haitong  Li", "Yuan  Xie", "Yingyan  Lin"], "year": 2020, "n_citations": 14}
{"id": 900318, "s2_id": "caa27d0418b7e09cc8bdf657500a5efb9a40e6af", "title": "A 2.0 Gb/s Throughput Decoder for QC-LDPC Convolutional Codes", "abstract": "This paper proposes a decoder architecture for low-density parity-check convolutional code (LDPCCC). Specifically, the LDPCCC is derived from a quasi-cyclic (QC) LDPC block code. By making use of the quasi-cyclic structure, the proposed LDPCCC decoder adopts a dynamic message storage in the memory and uses a simple address controller. The decoder efficiently combines the memories in the pipelining processors into a large memory block so as to take advantage of the data-width of the embedded memory in a modern field-programmable gate array (FPGA). A rate-5/6 QC-LDPCCC has been implemented on an Altera Stratix FPGA. It achieves up to 2.0 Gb/s throughput with a clock frequency of 100 MHz. Moreover, the decoder displays an excellent error performance of lower than 10-13 at a bit-energy-to-noise power-spectral-density ratio (Eb/N0) of 3.55 dB.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Chiu-Wing  Sham", "Xu  Chen", "Francis Chung-Ming Lau", "Yue  Zhao", "Wai Man Tam"], "year": 2013, "n_citations": 37}
{"id": 900638, "s2_id": "12d9a9a6346f42266ce6f36477ffcdf4f58505cc", "title": "On the Implementation of Fixed-point Exponential Function for Machine Learning and Signal Processing Accelerators", "abstract": "The natural exponential function is widely used in modeling many engineering and scientific systems. It is also an integral part of many neural network activation function such as sigmoid, tanh, ELU, RBF etc. Dedicated hardware accelerator and processors are designed for faster execution of such applications. Such accelerators can immensely benefit from an optimal implementation of exponential function. This can be achieved for most applications with the knowledge that the exponential function for a negative domain (R ) is more widely used than the positive domain (R). This paper presents an optimized implementation of exponential function for variable precision fixed point negative input. The implementation presented here significantly reduces the number of multipliers and adders. This is further optimized using mixed world-length implementation for the series expansion. The reduction in area and power consumption is more than 30% and 50% respectively over previous equivalent method.", "venue": "IEEE Design & Test", "authors": ["Mahesh  Chandra"], "year": 2021, "n_citations": 0}
{"id": 900681, "s2_id": "cdc8eee519e0d44a3588ed4de30a901f617eb440", "title": "Fast dynamic memory integration in co-simulation frameworks for multiprocessor system on-chip", "abstract": "The paper proposes a technique to integrate and simulate a dynamic memory in a multiprocessor framework based on C/C++/SystemC. Using the host machine's memory management capabilities, dynamic data processing is supported without compromising speed and accuracy of the simulation. A first prototype in a shared memory context is presented.", "venue": "Design, Automation and Test in Europe", "authors": ["Oreste  Villa", "Patrick  Schaumont", "Ingrid  Verbauwhede", "Matteo  Monchiero", "Gianluca  Palermo"], "year": 2005, "n_citations": 3}
{"id": 903503, "s2_id": "4db10f91621be538884bd4c752f2979ffc83c78e", "title": "Kraken: An Efficient Engine with a Uniform Dataflow for Deep Neural Networks", "abstract": "Deep neural networks (DNNs) have been successfully employed in a multitude of applications with remarkable performance. As such performance is achieved at a significant computational cost, several embedded applications demand fast and efficient hardware accelerators for DNNs. Previously proposed application specific integrated circuit (ASIC) architectures strive to utilize arrays of hundreds of processing elements (PEs) and reduce power-hungry DRAM accesses using multiple dataflows requiring complex PE architectures. These consume significant area and reduce the maximum clock frequency. This paper introduces the Kraken architecture, which optimally processes the convolutional layers, fully-connected layers, and matrix products of any DNN through a hardware-friendly uniform dataflow. This enables maximal data reuse of weights, inputs, and outputs, with a bare-bones PE design and on-the-fly dynamic reconfiguration. Kraken, implemented in 65-nm CMOS technology at 400 MHz, packs 672 PEs in 7.3 mm, with a peak performance of 537.6 Gops. Kraken processes the convolutional layers of AlexNet, VGG-16, and ResNet-50 at 336.6, 17.5, and 64.2 frames/s, respectively, hence outperforming the state-of-theart ASIC architectures in terms of overall performance efficiency, DRAM accesses, arithmetic intensity, and throughput, with 5.8\u00d7 more Gops/mm and 1.6\u00d7 more Gops/W.", "venue": "ArXiv", "authors": ["G  Abarajithan", "Chamira U. S. Edussooriya"], "year": 2021, "n_citations": 0}
{"id": 903637, "s2_id": "fdea7b4a436d57068aa49d87a68dbd623e658bf1", "title": "Weighing Up the New Kid on the Block: Impressions of using Vitis for HPC Software Development", "abstract": "The use of reconfigurable computing, and FPGAs in particular, has strong potential in the field of High Performance Computing (HPC). However the traditionally high barrier to entry when it comes to programming this technology has, until now, precluded widespread adoption. To popularise reconfigurable computing with communities such as HPC, Xilinx have recently released the first version of Vitis, a platform aimed at making the programming of FPGAs much more a question of software development rather than hardware design. However a key question is how well this technology fulfils the aim, and whether the tooling is mature enough such that software developers using FPGAs to accelerate their codes is now a more realistic proposition, or whether it simply increases the convenience for existing experts. To examine this question we use the Himeno benchmark as a vehicle for exploring the Vitis platform for building, executing and optimising HPC codes, describing the different steps and potential pitfalls of the technology. The outcome of this exploration is a demonstration that, whilst Vitis is an excellent step forwards and significantly lowers the barrier to entry in developing codes for FPGAs, it is not a silver bullet and an underlying understanding of dataflow style algorithmic design and appreciation of the architecture is still key to obtaining good performance on reconfigurable architectures.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Nick  Brown"], "year": 2020, "n_citations": 6}
{"id": 907095, "s2_id": "0c580c1831ed41846f702a963ce7ec8ee385b314", "title": "Automatic Hardware Synthesis for a Hybrid Reconfigurable CPU Featuring Philips CPLDs", "abstract": "A high-level architecture of a Hybrid Reconfigurable CPU, based on a Philips-supported core processor, is introduced. It features the Philips XPLA2 CPLD as a reconfigurable functional unit. A compilation chain is presented, in which automatic implementation of time-critical program segments in custom hardware is performed. The entire process is transparent from the programmer's point of view. The hardware synthesis module of the chain, which translates segments of assembly code into a hardware netlist, is discussed in details. Application examples are also presented.", "venue": "ArXiv", "authors": ["Bernardo  Kastrup"], "year": 1998, "n_citations": 5}
{"id": 907537, "s2_id": "38e118cbd40a21b314a3a79ab23ae6ed9a62fecc", "title": "Processor verification using efficient reductions of the logic of uninterpreted functions to propositional logic", "abstract": "The logic of Equality with Uninterpreted Functions (EUF) provides a means of abstracting the manipulation of data by a processor when verifying the correctness of its control logic. By reducing formulas in this logic to propositional formulas, we can apply Boolean methods such as ordered Binary Decision Diagrams (BDDs) and Boolean satisfiability checkers to perform the verification. We can exploit characteristics of the formulas describing the verification conditions to greatly simplfy the propostional formulas generated. We identify a class of terms we call \u201cp-terms\u201d for which equality comparisons can only be used in monotonically positive formulas. By applying suitable abstractions to the hardware model, we can express the functionality of data values and instruction addresses flowing through an instruction pipeline with p-terms. A decision procedure can exploit the restricted uses of p-terms by considering only \u201cmaximally diverse\u201d interpretations of the associated function symbols, where every function application yields a different value execept when constrainted by functional consistency. We present two methods to translate formulas in EUF into propositional logic. The first interprets the formula over a domain of fixed-length bit vectors and uses vectors of propositional variables to encode domain variables. The second generates formulas encoding the conditions under which pairs of terms have equal valuations, introducing propostional variables to encode the equality relations between pairs of terms. Both of these approaches can exploit maximal diversity to greatly reduce the number of propositional variables that need to be introduced and to reduce the overall formula sizes. We present experimental results demonstrating the efficiency of this approach when verifying pipelined processors using the method proposed by Burch and Dill. Exploiting positive equality allows us to overcome the experimental blow-up experienced previously when verifying microprocessors with load, store, and branch instructions.", "venue": "TOCL", "authors": ["Randal E. Bryant", "Steven M. German", "Miroslav N. Velev"], "year": 2001, "n_citations": 158}
{"id": 910201, "s2_id": "d8a274f624c591f16a603edac889920c9ef71b73", "title": "Verifying High-Level Latency-Insensitive Designs with Formal Model Checking", "abstract": "Latency-insensitive design mitigates increasing interconnect delay and enables productive component reuse in complex digital systems. This design style has been adopted in high-level design flows because untimed functional blocks connected through latency-insensitive interfaces provide a natural communication abstraction. However, latency-insensitive design with high-level languages also introduces a unique set of verification challenges that jeopardize functional correctness. In particular, bugs due to invalid consumption of inputs and deadlocks can be difficult to detect and debug with dynamic simulation methods. To tackle these two classes of bugs, we propose formal model checking methods to guarantee that a high-level latency-insensitive design is unaffected by invalid input data and is free of deadlock. We develop a well-structured verification wrapper for each property to automatically construct the corresponding formal model for checking. Our experiments demonstrate that the formal checks are effective in realistic bug scenarios from high-level designs.", "venue": "ArXiv", "authors": ["Steve  Dai", "Alicia  Klinefelter", "Haoxing  Ren", "Rangharajan  Venkatesan", "Ben  Keller", "Nathaniel  Pinckney", "Brucek  Khailany"], "year": 2021, "n_citations": 1}
{"id": 912740, "s2_id": "3d731a5e55bcc793c6676111e36edf6ef81bb146", "title": "Crosstalk based Fine-Grained Reconfiguration Techniques for Polymorphic Circuits", "abstract": "Truly polymorphic circuits, whose functionality/circuit behavior can be altered using a control variable, can provide tremendous benefits in multi-functional system design and resource sharing. For secure and fault tolerant hardware designs these can be crucial as well. Polymorphic circuits work in literature so far either rely on environmental parameters such as temperature, variation etc. or on special devices such as ambipolar FET, configurable magnetic devices, etc., that often result in inefficiencies in performance and/or realization. In this paper, we introduce a novel polymorphic circuit design approach where deterministic interference between nano-metal lines is leveraged for logic computing and configuration. For computing, the proposed approach relies on nano-metal lines, their interference and commonly used FETs. For polymorphism, it requires only an extra metal line that carries the control signal. In this paper, we show a wide range of crosstalk polymorphic logic gates and their evaluation results. We also show an example of a large circuit that performs both the functionalities of multiplier and sorter depending on the configuration signal. A comparison is made with respect to other existing approaches in literature, and transistor count is benchmarked. For crosstalk-polymorphic circuits, the transistor count reduction range from 25% to 83% with respect to various other approaches. For example, polymorphic AOI21-OA21 cell show 83%, 85% and 50% transistor count reduction, and Multiplier-Sorter circuit show 40%, 36% and 28% transistor count reduction with respect to CMOS, genetically evolved, and ambipolar transistor based polymorphic circuits, respectively.", "venue": "2018 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH)", "authors": ["Naveen Kumar Macha", "Sandeep  Geedipally", "Bhavana Tejaswini Repalle", "Md Arif Iqbal", "Wafi  Danesh", "Mostafizur  Rahman"], "year": 2018, "n_citations": 6}
{"id": 913083, "s2_id": "32ed9f3ce0a300aaa1422aa54b220f1df50e1ff2", "title": "Constructing a Weak Memory Model", "abstract": "Weak memory models are a consequence of the desire on part of architects to preserve all the uniprocessor optimizations while building a shared memory multiprocessor. The efforts to formalize weak memory models of ARM and POWER over the last decades are mostly empirical \u2013 they try to capture empirically observed behaviors \u2013 and end up providing no insight into the inherent nature of weak memory models. This paper takes a constructive approach to find a common base for weak memory models: we explore what a weak memory would look like if we constructed it with the explicit goal of preserving all the uniprocessor optimizations. We will disallow some optimizations which break a programmer's intuition in highly unexpected ways. The constructed model, which we call General Atomic Memory Model (GAM), allows all four load/store reorderings. We give the construction procedure of GAM, and provide insights which are used to define its operational and axiomatic semantics. Though no attempt is made to match GAM to any existing weak memory model, we show by simulation that GAM has comparable performance with other models. No deep knowledge of memory models is needed to read this paper.", "venue": "2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Sizhuo  Zhang", "Muralidaran  Vijayaraghavan", "Andrew  Wright", "Mehdi  Alipour", "Arvind"], "year": 2018, "n_citations": 7}
{"id": 916813, "s2_id": "411e5ed8ec40ee47c4b04c0a27fb3ef85c1e3156", "title": "ClosNets: a Priori Sparse Topologies for Faster DNN Training", "abstract": "Fully-connected layers in deep neural networks (DNN) are often the throughput and power bottleneck during training. This is due to their large size and low data reuse. Pruning dense layers can significantly reduce the size of these networks, but this approach can only be applied after training. In this work we propose a novel fully-connected layer that reduces the memory requirements of DNNs without sacrificing accuracy. We replace a dense matrix with products of sparse matrices whose topologies we pick in advance. This allows us to: (1) train significantly smaller networks without a loss in accuracy, and (2) store the network weights without having to store connection indices. We therefore achieve significant training speedups due to the smaller network size, and a reduced amount of computation per epoch. We tested several sparse layer topologies and found that Clos networks perform well due to their high path diversity, shallowness, and high model accuracy. With the ClosNets, we are able to reduce dense layer sizes by as much as an order of magnitude without hurting model accuracy.", "venue": "ArXiv", "authors": ["Mihailo  Isakov", "Michel A. Kinsy"], "year": 2018, "n_citations": 0}
{"id": 918217, "s2_id": "d3de5e29ea3b95315ce31dbbc8ea9158fee24e50", "title": "DRAB-LOCUS: An Area-Efficient AES Architecture for Hardware Accelerator Co-Location on FPGAs", "abstract": "Advanced Encryption Standard (AES) implementations on Field Programmable Gate Arrays (FPGA) commonly focus on maximizing throughput at the cost of utilizing high volumes of FPGA slice logic. High resource usage limits systems' abilities to implement other functions (such as video processing or machine learning) that may want to share the same FPGA resources. In this paper, we address the shared resource challenge by proposing and evaluating a low-area, but high-throughput, AES architecture. In contrast to existing work, our DSP/RAM-Based Low-CLB Usage (DRAB-LOCUS) architecture leverages block RAM tiles and Digital Signal Processing (DSP) slices to implement the AES Sub Bytes, Mix Columns, and Add Round Key sub-round transformations, reducing resource usage by a factor of 3 over traditional approaches. To achieve area-efficiency, we built an inner-pipelined architecture using the internal registers of block RAM tiles and DSP slices. Our DRAB-LOCUS architecture features a 12-stage pipeline capable of producing 7.055 Gbps of interleaved encrypted or decrypted data, and only uses 909 Look Up tables, 593 Flip Flops, 16 block RAMs, and 18 DSP slices in the target device.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Jacob T. Grycel", "Robert J. Walls"], "year": 2020, "n_citations": 0}
{"id": 918231, "s2_id": "ab8ab0e39fa68a1fa7f33af84c6821bb7da0dcd4", "title": "Amber*: Enabling Precise Full-System Simulation with Detailed Modeling of All SSD Resources", "abstract": "SSDs become a major storage component in modern memory hierarchies, and SSD research demands exploring future simulation-based studies by integrating SSD subsystems into a full-system environment. However, several challenges exist to model SSDs under a full-system simulations; SSDs are composed upon their own complete system and architecture, which employ all necessary hardware, such as CPUs, DRAM and interconnect network. Employing the hardware components, SSDs also require to have multiple device controllers, internal caches and software modules that respect a wide spectrum of storage interfaces and protocols. These SSD hardware and software are all necessary to incarnate storage subsystems under full-system environment, which can operate in parallel with the host system. In this work, we introduce a new SSD simulation framework, SimpleSSD 2.0, namely Amber, that models embedded CPU cores, DRAMs, and various flash technologies (within an SSD), and operate under the full system simulation environment by enabling a data transfer emulation. Amber also includes full firmware stack, including DRAM cache logic, flash firmware, such as FTL and HIL, and obey diverse standard protocols by revising the host DMA engines and system buses of a popular full system simulator's all functional and timing CPU models (gem5). The proposed simulator can capture the details of dynamic performance and power of embedded cores, DRAMs, firmware and flash under the executions of various OS systems and hardware platforms. Using Amber, we characterize several system-level challenges by simulating different types of full-systems, such as mobile devices and general-purpose computers, and offer comprehensive analyses by comparing passive storage and active storage architectures.", "venue": "2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Donghyun  Gouk", "Miryeong  Kwon", "Jie  Zhang", "Sungjoon  Koh", "Wonil  Choi", "Nam Sung Kim", "Mahmut T. Kandemir", "Myoungsoo  Jung"], "year": 2018, "n_citations": 20}
{"id": 918248, "s2_id": "9253e1a82cb8f85375c661338349780e1a742ba9", "title": "TicToc: Enabling Bandwidth-Efficient DRAM Caching for Both Hits and Misses in Hybrid Memory Systems", "abstract": "This paper investigates bandwidth-efficient DRAM caching for hybrid DRAM + 3D-XPoint memories. 3D-XPoint is becoming a viable alternative to DRAM as it enables high-capacity and non-volatile main memory systems. However, 3D-XPoint has several characteristics that limit it from outright replacing DRAM: 4-8x slower read, and even worse writes. As such, effective DRAM caching in front of 3D-XPoint is important to enable a high-capacity, low-latency, and high-write-bandwidth memory. There are currently two major approaches for DRAM cache design: (1) a Tag-Inside-Cacheline (TIC) organization that optimizes for hits, by storing tag next to each line such that one access gets both tag and data, and (2) a Tag-Outside-Cacheline (TOC) organization that optimizes for misses, by storing tags from multiple data lines together in a tag-line such that one access to a tag-line gets information on several data-lines. Ideally, we would like to have the low hit-latency of TIC designs, and the low miss-bandwidth of TOC designs. To this end, we propose a TicToc organization that provisions both TIC and TOC to get the hit and miss benefits of both. We find that naively combining both techniques actually performs worse than TIC individually, because one has to pay the bandwidth cost of maintaining both metadata. The main contribution of this work is developing architectural techniques to reduce bandwidth cost of accessing and maintaining both TIC and TOC metadata. We find that most of the update bandwidth is due to maintaining the TOC dirty information. We propose a DRAM Cache Dirtiness Bit technique that carries DRAM cache dirty information to last-level caches, to help prune repeated dirty-bit updates for known dirty lines. We also propose a Preemptive Dirty Marking (PDM) technique that predicts which lines will be written and proactively marks the dirty bit at install time, to help avoid the initial dirty-bit update for dirty lines. To support PDM, we develop a novel PC-based Write-Predictor to aid in marking only write-likely lines. Our evaluations on a 4GB DRAM cache in front of 3D-XPoint show that our TicToc organization enables 10% speedup over the baseline TIC, nearing the 14% speedup possible with an idealized DRAM cache design with 64MB of SRAM tags, while needing only 34KB SRAM.", "venue": "2019 IEEE 37th International Conference on Computer Design (ICCD)", "authors": ["Vinson  Young", "Zeshan  Chishti", "Moinuddin K. Qureshi"], "year": 2019, "n_citations": 2}
{"id": 920719, "s2_id": "5a7ab67d121ece54c2a39df3cf001a0fc1058fec", "title": "qBSA: Logic Design of a 32-bit Block-Skewed RSFQ Arithmetic Logic Unit", "abstract": "Single flux quantum (SFQ) circuits are an attractive beyond-CMOS technology because they promise two orders of magnitude lower power at clock frequencies exceeding 25 GHz. However, every SFQ gate is clocked creating very deep gate-level pipelines that are difficult to keep full, particularly for sequences that include data-dependent operations. This paper proposes to increase the throughput of SFQ pipelines by redesigning the datapath to accept and operate on least-significant bits (LSBs) clock cycles earlier than more significant bits. This skewed datapath approach reduces the latency of the LSB side which can be feedback earlier for use in subsequent data-dependent operations increasing their throughput. In particular, we propose to group the bits into 4-bit blocks that are operated on concurrently and create block-skewed datapath units for 32-bit operation. This skewed approach allows a subsequent data-dependent operation to start evaluating as soon as the first 4-bit block completes. Using this general approach, we develop a block-skewed MIPS-compatible 32-bit ALU. Our gate-level Verilog design improves the throughput of 32-bit data dependent operations by 2x and 1.5x compared to previously proposed 4-bit bit-slice and 32-bit Ladner-Fischer ALUs respectively. We have quantified the benefit of this design on instructions per cycle (IPC) for various RISC-V benchmarks assuming a range of non-ALU operation latencies from one to ten cycles. Averaging across benchmarks, our experimental results show that compared to the 32-bit Ladner-Fischer our proposed architecture provides a range of IPC improvements between 1.37x assuming one-cycle non-ALU latency to 1.2x assuming ten-cycle non-ALU latency. Moreover, our average IPC improvements compared to a 32-bit ALU based on the 4-bit bit-slice range from 2.93x to 4x.", "venue": "2019 IEEE International Superconductive Electronics Conference (ISEC)", "authors": ["Souvik  Kundu", "Gourav  datta", "Peter A. Beerel", "Massoud  Pedram"], "year": 2019, "n_citations": 3}
{"id": 924268, "s2_id": "3df0bb5da15f3b79ea85462dcbbe4752acd2b1d4", "title": "Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable Deep Neural Network Acceleration", "abstract": "Precision scaling has emerged as a popular technique to optimize the compute and storage requirements of Deep Neural Networks (DNNs). Efforts toward creating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum precision required to achieve a given network-level accuracy varies considerably across networks, and even across layers within a network, requiring support for variable precision in DNN hardware. Previous proposals such as bit-serial hardware incur high overheads, significantly diminishing the benefits of lower precision. To efficiently support precision re-configurability in DNN accelerators, we introduce an approximate computing method wherein DNN computations are performed block-wise (a block is a group of bits) and re-configurability is supported at the granularity of blocks. Results of block-wise computations are composed in an approximate manner to enable efficient re-configurability. We design a DNN accelerator that embodies approximate blocked computation and propose a method to determine a suitable approximation configuration for a given DNN. By varying the approximation configurations across DNNs, we achieve 1.11x-1.34x and 1.29x-1.6x improvement in system energy and performance respectively, over an 8-bit fixed-point (FxP8) baseline, with negligible loss in classification accuracy. Further, by varying the approximation configurations across layers and data-structures within DNNs, we achieve 1.14x-1.67x and 1.31x-1.93x improvement in system energy and performance respectively, with negligible accuracy loss.", "venue": "ArXiv", "authors": ["Reena  Elangovan", "Shubham  Jain", "Anand  Raghunathan"], "year": 2020, "n_citations": 0}
{"id": 927580, "s2_id": "d62466d89040f22cce76be920225bde732c8a4cb", "title": "A portable and Linux capable RISC-V computer system in Verilog HDL", "abstract": "RISC-V is an open and royalty free instruction set architecture which has been developed at the University of California, Berkeley. The processors using RISC-V can be designed and released freely. Because of this, various processor cores and system on chips (SoCs) have been released so far. However, there are a few public RISC-V computer systems that are portable and can boot Linux operating systems. In this paper, we describe a portable and Linux capable RISC-V computer system targeting FPGAs in Verilog HDL. This system can be implemented on an FPGA with fewer hardware resources, and can be implemented on low cost FPGAs or customized by introducing an accelerator. This paper also describes the knowledge obtained through the development of this RISC-V computer system.", "venue": "ArXiv", "authors": ["Junya  Miura", "Hiromu  Miyazaki", "Kenji  Kise"], "year": 2020, "n_citations": 2}
{"id": 929814, "s2_id": "68afea9c892ff499af788ccfa25fa17b507bc993", "title": "Accelerating Recommender Systems via Hardware \"scale-in\"", "abstract": "In today's era of \"scale-out\", this paper makes the case that a specialized hardware architecture based on \"scale-in\"--placing as many specialized processors as possible along with their memory systems and interconnect links within one or two boards in a rack--would offer the potential to boost large recommender system throughput by 12-62x for inference and 12-45x for training compared to the DGX-2 state-of-the-art AI platform, while minimizing the performance impact of distributing large models across multiple processors. By analyzing Facebook's representative model--Deep Learning Recommendation Model (DLRM)--from a hardware architecture perspective, we quantify the impact on throughput of hardware parameters such as memory system design, collective communications latency and bandwidth, and interconnect topology. By focusing on conditions that stress hardware, our analysis reveals limitations of existing AI accelerators and hardware platforms.", "venue": "ArXiv", "authors": ["Suresh  Krishna", "Ravi  Krishna"], "year": 2020, "n_citations": 4}
{"id": 929891, "s2_id": "8a27b424a834150e11591ad1ff2985584e4cfff8", "title": "A Fully Integrated 5-mW, 0.8-Gbps Energy-Efficient Chip-to-Chip Data Link for Ultralow-Power IoT End-Nodes in 65-nm CMOS", "abstract": "The increasing complexity of Internet-of-Things (IoT) applications and near-sensor processing algorithms is pushing the computational power of low-power, battery-operated end-node systems. This trend also reveals growing demands for high-speed and energy-efficient inter-chip communications to manage the increasing amount of data coming from off-chip sensors and memories. While traditional microcontroller interfaces such as SPIs cannot cope with tight energy and large bandwidth requirements, low-voltage swing transceivers can tackle this challenge, thanks to their capability to achieve several Gbps of the communication speed at milliwatt power levels. However, recent research on high-speed serial links focused on high-performance systems, with a power consumption significantly larger than the one of low-power IoT end-nodes, or on stand-alone designs not integrated at a system level. This article presents a low-swing transceiver for the energy-efficient and low-power chip-to-chip communication fully integrated within an IoT end-node system-on-chip, fabricated in CMOS 65-nm technology. The transceiver can be easily controlled via a software interface; thus, we can consider realistic scenarios for the data communication, which cannot be assessed in stand-alone prototypes. Chip measurements show that the transceiver achieves $8.46\\times $ higher energy efficiency at $15.9\\times $ higher performance than a traditional microcontroller interface such as a single-SPI.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Hayate  Okuhara", "Ahmed  Elnaqib", "Martino  Dazzi", "Pierpaolo  Palestri", "Simone  Benatti", "Luca  Benini", "Davide  Rossi"], "year": 2021, "n_citations": 0}
{"id": 930973, "s2_id": "6269f74efbb57e91dc55d40938fc08bfd30529a7", "title": "Optimum Reconfiguration of Routing Interconnection Network in APSoC Fabrics", "abstract": "This paper presents an automated algorithm for optimum configuration of routing interconnection network in Xilinx Zynq-7000 All programmable system-on-chip (APSoC) fabrics. A method to configure circuits with optimum routing resources is presented along with their performance parameters with and without the proposed algorithm. The proposed algorithm enables full control over routing resources for using different interconnection types in order to create routing-based circuit-under-test. The algorithm proposes the routing techniques through the 2-D array of switch matrices inside the interconnection network and automatically identifies the involved programmable interconnection points associated with a node. An experimental setup is proposed to measure the performance parameters such as slack time and power with and without the applied algorithm on the APSoC routing resources. The proposed setup requires no external equipment such as manufactured equipments or external instruments for performance measurement.", "venue": "ArXiv", "authors": ["Mostafa  Darvishi"], "year": 2020, "n_citations": 0}
{"id": 932533, "s2_id": "7ba387b4faa07e5b04a518cbf668f54a799ee0f1", "title": "An O(bn^2) Time Algorithm for Optimal Buffer Insertion with b Buffer Types", "abstract": "Buffer insertion is a popular technique to reduce the interconnect delay. The classic buffer insertion algorithm of van Ginneken has time complexity O(n^2), where n is the number of buffer positions. Lillis, Cheng and Lin extended van Ginneken's algorithm to allow b buffer types in time O (b^2 n^2). For modern design libraries that contain hundreds of buffers, it is a serious challenge to balance the speed and performance of the buffer insertion algorithm. In this paper, we present a new algorithm that computes the optimal buffer insertion in O (bn^2) time. The reduction is achieved by the observation that the (Q, C) pairs of the candidates that generate the new candidates must form a convex hull. On industrial test cases, the new algorithm is faster than the previous best buffer insertion algorithms by orders of magnitude.", "venue": "DATE '05", "authors": ["Zhuo  Li", "Weiping  Shi"], "year": 2005, "n_citations": 17}
{"id": 933316, "s2_id": "84ae7b4ef14fffe3bf7b52266767cb374645794f", "title": "The ARM Scalable Vector Extension", "abstract": "This article describes the ARM Scalable Vector Extension (SVE). Several goals guided the design of the architecture. First was the need to extend the vector processing capability associated with the ARM AArch64 execution state to better address the computational requirements in domains such as high-performance computing, data analytics, computer vision, and machine learning. Second was the desire to introduce an extension that can scale across multiple implementations, both now and into the future, allowing CPU designers to choose the vector length most suitable for their power, performance, and area targets. Finally, the architecture should avoid imposing a software development cost as the vector length changes and where possible reduce it by improving the reach of compiler auto-vectorization technologies. SVE achieves these goals. It allows implementations to choose a vector register length between 128 and 2,048 bits. It supports a vector-length agnostic programming model that lets code run and scale automatically across all vector lengths without recompilation. Finally, it introduces several innovative features that begin to overcome some of the traditional barriers to autovectorization.", "venue": "IEEE Micro", "authors": ["Nigel  Stephens", "Stuart  Biles", "Matthias  Boettcher", "Jacob  Eapen", "Mbou  Eyole", "Giacomo  Gabrielli", "Matt  Horsnell", "Grigorios  Magklis", "Alejandro  Martinez", "Nathana\u00ebl  Pr\u00e9millieu", "Alastair David Reid", "Alejandro  Rico", "Paul  Walker"], "year": 2017, "n_citations": 117}
{"id": 938091, "s2_id": "08f00a38b7bbb6d06b65ce74de87f914e981b582", "title": "LOCKE Detailed Specification Tables", "abstract": "This document shows the detailed specification of LOCKE coherence protocol for each cache controller, using a table-based technique. This representation provides clear, concise visual information yet includes sufficient detail (e.g., transient states) arguably lacking in the traditional, graphical form of state diagrams.", "venue": "ArXiv", "authors": ["Lucia G. Menezo", "Valentin  Puente", "Jos\u00e9-\u00c1ngel  Gregorio"], "year": 2012, "n_citations": 0}
{"id": 940049, "s2_id": "61c60fcb05042279d1a4d89a72f10484348d6012", "title": "Managing Hybrid Main Memories with a Page-Utility Driven Performance Model", "abstract": "Hybrid memory systems comprised of dynamic random access memory (DRAM) and non-volatile memory (NVM) have been proposed to exploit both the capacity advantage of NVM and the latency and dynamic energy advantages of DRAM. An important problem for such systems is how to place data between DRAM and NVM to improve system performance. In this paper, we devise the first mechanism, called UBM (page Utility Based hybrid Memory management), that systematically estimates the system performance benefit of placing a page in DRAM versus NVM and uses this estimate to guide data placement. UBM\u2019s estimation method consists of two major components. First, it estimates how much an application\u2019s stall time can be reduced if the accessed page is placed in DRAM. To do this, UBM comprehensively considers access frequency, row buffer locality, and memory level parallelism (MLP) to estimate the application\u2019s stall time reduction. Second, UBM estimates how much each application\u2019s stall time reduction contributes to overall system performance. Based on this estimation method, UBM can determine and place the most critical data in DRAM to directly optimize system performance. Experimental results show that UBM improves system performance by 14% on average (and up to 39%) compared to the best of three state-of-the-art mechanisms for a large number of data-intensive workloads from the SPEC CPU2006 and Yahoo Cloud Serving Benchmark (YCSB) suites.", "venue": "ArXiv", "authors": ["Yang  Li", "Jongmoo  Choi", "Jin  Sun", "Saugata  Ghose", "Hui  Wang", "Justin  Meza", "Jinglei  Ren", "Onur  Mutlu"], "year": 2015, "n_citations": 9}
{"id": 942681, "s2_id": "fdceea62710af269e0d3293374a4ca503fa4650f", "title": "Behaviour-based Knowledge Systems: An Epigenetic Path from Behaviour to Knowledge", "abstract": "In this paper we expose the theoretical background underlying our current research.\n This consists in the development of behaviour-based knowledge systems, for closing\n the gaps between behaviour-based and knowledge-based systems, and also\n between the understandings of the phenomena they model. We expose the\n requirements and stages for developing behaviour-based knowledge systems and\n discuss their limits. We believe that these are necessary conditions for the\n development of higher order cognitive capacities, in artificial and natural cognitive\n systems.", "venue": "ArXiv", "authors": ["Carlos  Gershenson"], "year": 2002, "n_citations": 9}
{"id": 945599, "s2_id": "750d5150309519dc9b0e04ada9c7e534e6fc8d95", "title": "Addressing multiple bit/symbol errors in DRAM subsystem", "abstract": "As DRAM technology continues to evolve towards smaller feature sizes and increased densities, faults in DRAM subsystem are becoming more severe. Current servers mostly use CHIPKILL based schemes to tolerate up-to one/two symbol errors per DRAM beat. Such schemes may not detect multiple symbol errors arising due to faults in multiple devices and/or data-bus, address bus. In this article, we introduce Single Symbol Correction Multiple Symbol Detection (SSCMSD)\u2014a novel error handling scheme to correct single-symbol errors and detect multi-symbol errors. Our scheme makes use of a hash in combination with Error Correcting Code (ECC) to avoid silent data corruptions (SDCs). We develop a novel scheme that deploys 32-bit CRC along with Reed-Solomon code to implement SSCMSD for a \u00d74 based DDR4 system. Simulation based experiments show that our scheme effectively guards against device, data-bus and address-bus errors only limited by the aliasing probability of the hash. Our novel design enabled us to achieve this without introducing additional READ latency. We need 19 chips per rank, 76 data bus-lines and additional hash-logic at the memory controller.", "venue": "PeerJ Comput. Sci.", "authors": ["Ravikiran  Yeleswarapu", "Arun K. Somani"], "year": 2021, "n_citations": 0}
{"id": 946949, "s2_id": "67b0d347a1eb079f7ce594b60509a6e1dcc6a008", "title": "FPGA-based Controller for a Mobile Robot", "abstract": "With application in the robotics and automation, more and more it becomes necessary the development of applications based on methodologies that facilitate future modifications, updates and enhancements in the original projected system. This project presents a conception of mobile robots using rapid prototyping, distributing the several control actions in growing levels of complexity and computing proposal oriented to embed systems implementation. This kind of controller can be tested on different platform representing the mobile robots using reprogrammable logic components (FPGA). This mobile robot will detect obstacle and also be able to control the speed. Different modules will be Actuators, Sensors, wireless transmission. All this modules will be interfaced using FPGA controller. I would like to construct a mechanically simple robot model, which can measure the distance from obstacle with the aid of sensor and accordingly should able to control the speed of motor. I would like to construct a mechanically simple robot model, which can measure the distance from obstacle with the aid of sensor and accordingly should able to control the speed of motor.", "venue": "ArXiv", "authors": ["Shilpa  Kale", "S. S. Shriramwar"], "year": 2009, "n_citations": 12}
{"id": 949577, "s2_id": "e5df0e3038ee17a506173fec4daab583581d5329", "title": "Adaptive Multi-bit SRAM Topology Based Analog PUF", "abstract": "Physically Unclonable Functions (PUFs) are lightweight cryptographic primitives for generating unique signatures from minuscule manufacturing variations. In this work, we present lightweight, area efficient and low power adaptive multi-bit SRAM topology based Current Mirror Array (CMA) analog PUF design for securing the sensor nodes, authentication and key generation. The proposed Strong PUF increases the complexity of the machine learning attacks thus making it difficult for the adversary. The design is based on scl180 library.", "venue": "ArXiv", "authors": ["Sudarshan  Sharma", "Dhruv  Thapar", "Nikhil  Bhelave", "Mrigank  Sharad"], "year": 2019, "n_citations": 0}
{"id": 952846, "s2_id": "69a12de1fc8446fdf28da8838bd00b436925efeb", "title": "Understanding the Limits of Conventional Hardware Architectures for Deep-Learning", "abstract": "Deep learning and hardware for it has garnered immense academic and industry interest in the past 5 years \u2013 including almost 100 startups, more than $5B of VC investment \u2013 and a re-relevance of the role of architecture. However, the state-of-art remains NVIDIA\u2019s TensorCore-based systems that provide i) top-of-line performance, ii) turnkey software stack, and iii) coverage across a wide-spectrum of DL network styles (DL-architecture in AI parlance). Other academic and industry efforts have included novel approaches like spatial dataflow, CGRAs, systolic arrays, blended FPGA LUTs with fixed function units and more. These have all necessitated their own innovations in architecture, compiler, and software stack integration. However, none of these have yet satisfied all the 3 metrics that NVIDIA\u2019s TensorCore and software stack provides, and generally seem to perform worse. In this paper, we systematically investigate the behavior of DL workloads and imputed needs on hardware/compiler/software. We show that SIMD/shortvector, caching, and synchronization in a fairly well-understood multicore chip organization we call UPCYCLE can achieve dayzero software maturity, and provide big integer factor speedups over the state-of-art NVIDIA solutions. Compared to an A100, UPCYCLE at small-batch size is geo-mean 7.7X faster for inference, geo-mean 11.7X faster at training, while consuming only half the power. Second, the UPCYCLE architecture requires no new compiler or software stack innovation. Third, it provides full DL-architecture coverage, and can be instantiated to provide training-optimized, inference-optimized, or balanced training and inference systems. Overall, this paper motivates the treatment of software maturity as a first class design constraint in developing new architectures for DL.", "venue": "ArXiv", "authors": ["Michael  Davies", "Adam  Labiosa", "Karthikeyan  Sankaralingam"], "year": 2021, "n_citations": 0}
{"id": 953166, "s2_id": "babbf7514524a3b0f28129623a46ed5712caba59", "title": "Reducing Load Latency with Cache Level Prediction", "abstract": "High load latency that results from deep cache hierarchies and relatively slow main memory is an important limiter of single-thread performance. Data prefetch helps reduce this latency by fetching data up the hierarchy before it is requested by load instructions. However, data prefetching has shown to be imperfect in many situations. We propose cachelevel prediction to complement prefetchers. Our method predicts which memory hierarchy level a load will access allowing the memory loads to start earlier, and thereby saves many cycles. The predictor provides high prediction accuracy at the cost of just one cycle added latency to L1 misses. Experimental results show speedup of 7.8% on generic, graph, and HPC applications over a baseline with aggressive prefetchers.", "venue": "ArXiv", "authors": ["Majid  Jalili", "Mattan  Erez"], "year": 2021, "n_citations": 0}
{"id": 961585, "s2_id": "5b98a62751e452f568a7920b2a654c42af550660", "title": "Space-efficient Routing Tables for Almost All Networks and the Incompressibility Method", "abstract": "We use the incompressibility method based on Kolmogorov complexity to determine the total number of bits of routing information for almost all network topologies. In most models for routing, for almost all labeled graphs, $\\Theta (n^2)$ bits are necessary and sufficient for shortest path routing. By \"almost all graphs\" we mean the Kolmogorov random graphs which constitute a fraction of 1 - 1/nc of all graphs on n nodes, where c > 0 is an arbitrary fixed constant. There is a model for which the average case lower bound rises to $\\Omega(n^2 \\log n )$ and another model where the average case upper bound drops to $O(n \\log^2 n)$. This clearly exposes the sensitivity of such bounds to the model under consideration. If paths have to be short, but need not be shortest (if the stretch factor may be larger than 1), then much less space is needed on average, even in the more demanding models. Full-information routing requires $\\Theta (n^3)$ bits on average. For worst-case static networks we prove an $\\Omega(n^2 \\log n )$ lower bound for shortest path routing and all stretch factors < 2 in some networks where free relabeling is not allowed.", "venue": "SIAM J. Comput.", "authors": ["Harry  Buhrman", "Jaap-Henk  Hoepman", "Paul M. B. Vit\u00e1nyi"], "year": 1999, "n_citations": 16}
{"id": 965136, "s2_id": "8ceecb2e8456032f27e2956047976b2f1a3384c2", "title": "Abstract Stobjs and Their Application to ISA Modeling", "abstract": "We introduce a new ACL2 feature, the abstract stobj, and show how to apply it to modeling the instruction set architecture of a microprocessor. Benefits of abstract stobjs over traditional (\"concrete\") stobjs can include faster execution, support for symbolic simulation, more efficient reasoning, and resilience of proof developments under modeling optimization.", "venue": "ACL2", "authors": ["Shilpi  Goel", "Warren A. Hunt", "Matt  Kaufmann"], "year": 2013, "n_citations": 20}
{"id": 965663, "s2_id": "ebf242d69efb26b16090ae7a27a475b090333405", "title": "AZP: Automatic Specialization for Zero Values in Gaming Applications", "abstract": "Recent research has shown that dynamic zeros in shader programs of gaming applications can be effectively leveraged with a profile-guided, code-versioning transform. This transform duplicates code, specializes one path assuming certain key program operands, called versioning variables, are zero, and leaves the other path unspecialized. Dynamically, depending on the versioning variable's value, either the specialized fast path or the default slow path will execute. Prior work applied this transform manually and showed promising gains on gaming applications. In this paper, we present AZP, an automatic compiler approach to perform the above code-versioning transform. Our framework automatically determines which versioning variables or combinations of them are profitable, and determines the code region to duplicate and specialize (called the versioning scope). AZP takes operand zero value probabilities as input and it then uses classical techniques such as constant folding and dead-code elimination to determine the most profitable versioning variables and their versioning scopes. This information is then used to affect the final transform in a straightforward manner. We demonstrate that AZP is able to achieve an average speedup of 16.4% for targeted shader programs, amounting to an average frame-rate speedup of 3.5% across a collection of modern gaming applications on an NVIDIA GeForce RTX 2080 GPU GPU.", "venue": "ArXiv", "authors": ["Mark W. Stephenson", "Ram  Rangan"], "year": 2020, "n_citations": 0}
{"id": 966212, "s2_id": "53f00b2a6c3759fcdb610c5fcb4b413fd965aea5", "title": "An Asynchronous Early Output Full Adder and a Relative-Timed Ripple Carry Adder", "abstract": "This article presents the design of a new asynchronous early output full adder which when cascaded leads to a relative-timed ripple carry adder (RCA). The relative-timed RCA requires imposing a very small relative-timing assumption to overcome the problem of gate orphans associated with internal carry propagation. The relative-timing assumption is however independent of the RCA size. The primary benefits of the relative-timed RCA are processing of valid data incurs data-dependent forward latency, while the processing of spacer involves a very fast constant time reverse latency of just 1 full adder delay which represents the ultimate in the design of an asynchronous RCA with the fastest reset. The secondary benefits of the relative-timed RCA are it achieves good optimization of power and area metrics simultaneously. A 32-bit relative-timed RCA constructed using the proposed early output full adder achieves respective reductions in forward latency by 67%, 10% and 3.5% compared to the optimized strong-indication, weak-indication, and early output 32-bit asynchronous RCAs existing in the literature. Based on a similar comparison, the proposed 32-bit relative-timed RCA achieves corresponding reductions in cycle time by 83%, 12.7% and 6.4%. In terms of area, the proposed 32-bit relative-timed RCA occupies 27% less Silicon than its optimized strong-indication counterpart and 17% less Silicon than its optimized weak-indication counterpart, and features increased area occupancy by a meager 1% compared to the optimized early output 32-bit asynchronous RCA. The average power dissipation of all the asynchronous 32-bit RCAs are found to be comparable since they all satisfy the monotonic cover constraint. The simulation results obtained correspond to a 32/28nm CMOS process.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 8}
{"id": 969592, "s2_id": "19f482d64f59b2b1c8b763c648c7c9d22fe2150f", "title": "Systematic figure of merit computation for the design of pipeline ADC", "abstract": "The emerging concept of SoC-AMS leads to research into new top-down methodologies to aid systems designers in sizing analog and mixed devices. The paper applies this idea to the high-level optimization of a pipeline ADC. Considering a given technology, it consists in comparing different configurations according to their imperfections and their architectures without FFT computation or time-consuming simulations. The final selection is based on a figure of merit.", "venue": "Design, Automation and Test in Europe", "authors": ["Ludovic  Barrandon", "S.  Crand", "Dominique  Houzet"], "year": 2005, "n_citations": 1}
{"id": 970060, "s2_id": "e60afa43509e6a007c6f1e6e1e8e48d377e9cb3f", "title": "STOMP: A Tool for Evaluation of Scheduling Policies in Heterogeneous Multi-Processors", "abstract": "The proliferation of heterogeneous chip multiprocessors in recent years has reached unprecedented levels. Traditional homogeneous platforms have shown fundamental limitations when it comes to enabling high-performance yet-ultra-low-power computing, in particular in application domains with real-time execution deadlines or criticality constraints. By combining the right set of general purpose cores and hardware accelerators together, along with proper chip interconnects and memory technology, heterogeneous chip multiprocessors have become an effective high-performance and low-power computing alternative. \nOne of the challenges of heterogeneous architectures relates to efficient scheduling of application tasks (processes, threads) across the variety of options in the chip. As a result, it is key to provide tools to enable early-stage prototyping and evaluation of new scheduling policies for heterogeneous platforms. In this paper, we present STOMP (Scheduling Techniques Optimization in heterogeneous Multi-Processors), a simulator for fast implementation and evaluation of task scheduling policies in multi-core/multi-processor systems with a convenient interface for \"plugging\" in new scheduling policies in a simple manner. Thorough validation of STOMP exhibits small relative errors when compared against closed-formed equivalent models during steady-state analysis.", "venue": "ArXiv", "authors": ["Augusto  Vega", "Aporva  Amarnath", "John-David  Wellman", "Hiwot  Kassa", "Subhankar  Pal", "Hubertus  Franke", "Alper  Buyuktosunoglu", "Ronald  Dreslinski", "Pradip  Bose"], "year": 2020, "n_citations": 3}
{"id": 970445, "s2_id": "4a69cf2338296fd06ec6ac6ba85d5638056d1f7c", "title": "Doubt and Redundancy Kill Soft Errors\u2014Towards Detection and Correction of Silent Data Corruption in Task-based Numerical Software", "abstract": "Resilient algorithms in high-performance computing are subject to rigorous non-functional constraints. Resiliency must not increase the runtime, memory footprint or I/O demands too significantly. We propose a task-based soft error detection scheme that relies on error criteria per task outcome. They formalise how \u201cdubious\u201d an outcome is, i.e. how likely it contains an error. Our whole simulation is replicated once, forming two teams of MPI ranks that share their task results. Thus, ideally each team handles only around half of the workload. If a task yields large error criteria values, i.e. is dubious, we compute the task redundantly and compare the outcomes. Whenever they disagree, the task result with a lower error likeliness is accepted. We obtain a self-healing, resilient algorithm which can compensate silent floating-point errors without a significant performance, I/O or memory footprint penalty. Case studies however suggest that a careful, domain-specific tailoring of the error criteria remains essential.", "venue": "2021 IEEE/ACM 11th Workshop on Fault Tolerance for HPC at eXtreme Scale (FTXS)", "authors": ["Philipp  Samfass", "Tobias  Weinzierl", "Anne  Reinarz", "Michael  Bader"], "year": 2021, "n_citations": 0}
{"id": 971537, "s2_id": "3e773299fadfa9f28bdc60b05513291d29a3fba2", "title": "A Variable Vector Length SIMD Architecture for HW/SW Co-designed Processors", "abstract": "Hardware/Software (HW/SW) co-designed processors provide a promising solution to the power and complexity problems of the modern microprocessors by keeping their hardware simple. Moreover, they employ several runtime optimizations to improve the performance. One of the most potent optimizations, vectorization, has been utilized by modern microprocessors, to exploit the data level parallelism through SIMD accelerators. Due to their hardware simplicity, these accelerators have evolved in terms of width from 64-bit vectors in Intel MMX to 512-bit wide vector units in Intel Xeon Phi and AVX-512. Although SIMD accelerators are simple in terms of hardware design, code generation for them has always been a challenge. Moreover, increasing vector lengths with each new generation add to this complexity. \nThis paper explores the scalability of SIMD accelerators from the code generation point of view. We discover that the SIMD accelerators remain underutilized at higher vector lengths mainly due to: a) reduced dynamic instruction stream coverage for vectorization and b) increase in permutations. Both of these factors can be attributed to the rigidness of the SIMD architecture. We propose a novel SIMD architecture that possesses the flexibility needed to support higher vector lengths. Furthermore, we propose Variable Length Vectorization and Selective Writing in a HW/SW co-designed environment to transparently target the flexibility of the proposed architecture. We evaluate our proposals using a set of SPECFP2006 and Physicsbench applications. Our experimental results show an average dynamic instruction reduction of 31% and 40% and an average speed up of 13% and 10% for SPECFP2006 and Physicsbench respectively, for 512-bit vector length, over the scalar baseline code.", "venue": "ArXiv", "authors": ["Rakesh  Kumar", "Alejandro  Martinez", "Antonio  Gonzalez"], "year": 2021, "n_citations": 0}
{"id": 974732, "s2_id": "071c8eb0cc84182f900265a192b047949e002d89", "title": "SLAP: a Split Latency Adaptive VLIW Pipeline Architecture Which Enables on-The-Fly Variable SIMD Vector-Length", "abstract": "Over the last decade the relative latency of access to shared memory by multicore increased as wire resistance dominated latency and low wire density layout pushed multi-port memories farther away from their ports. Various techniques were deployed to improve average memory access latencies, such as speculative pre-fetching and branch-prediction, often leading to high variance in execution time which is unacceptable in real-time systems. Smart DMAs can be used to directly copy data into a layer-1 SRAM, but with overhead. The VLIW architecture, the de-facto signal-processing engine, suffers badly from a breakdown in lock-step execution of scalar and vector instructions. We describe the Split Latency Adaptive Pipeline (SLAP) VLIW architecture, a cache performance improvement technology that requires zero change to object code, while removing smart DMAs and their overhead. SLAP builds on the Decoupled Access and Execute concept by 1) breaking lock-step execution of functional units, 2) enabling variable vector length for variable data-level parallelism, and 3) adding a novel triangular-load mechanism. We discuss the SLAP architecture and demonstrate the performance benefits on real traces from a wireless baseband-system (where even the most compute intensive functions suffer from an Amdahl\u2019s law limitation due to a mixture of scalar and vector processing).", "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Ashish  Shrivastava", "Alan  Gatherer", "Tong  Sun", "Sushma  Wokhlu", "Alex  Chandra"], "year": 2021, "n_citations": 0}
{"id": 975151, "s2_id": "f5b50dd03a148f3f757e28853d463c9b3b429de7", "title": "The gem5 Simulator: Version 20.0+", "abstract": "The open-source and community-supported gem5 simulator is one of the most popular tools for computer architecture research. This simulation infrastructure allows researchers to model modern computer hardware at the cycle level, and it has enough fidelity to boot unmodified Linux-based operating systems and run full applications for multiple architectures including x86, Arm, and RISC-V. The gem5 simulator has been under active development over the last nine years since the original gem5 release. In this time, there have been over 7500 commits to the codebase from over 250 unique contributors which have improved the simulator by adding new features, fixing bugs, and increasing the code quality. In this paper, we give and overview of gem5's usage and features, describe the current state of the gem5 simulator, and enumerate the major changes since the initial release of gem5. We also discuss how the gem5 simulator has transitioned to a formal governance model to enable continued improvement and community support for the next 20 years of computer architecture research.", "venue": "ArXiv", "authors": ["Jason  Lowe-Power", "Abdul Mutaal Ahmad", "Ayaz  Akram", "Mohammad  Alian", "Rico  Amslinger", "Matteo  Andreozzi", "Adria  Armejach", "Nils  Asmussen", "Srikant  Bharadwaj", "Gabe  Black", "Gedare  Bloom", "Bobby R. Bruce", "Daniel Rodrigues Carvalho", "Jeronimo  Castrillon", "Lizhong  Chen", "Nicolas  Derumigny", "Stephan  Diestelhorst", "Wendy  Elsasser", "Marjan  Fariborz", "Amin  Farmahini-Farahani", "Pouya  Fotouhi", "Ryan  Gambord", "Jayneel  Gandhi", "Dibakar  Gope", "Thomas  Grass", "Bagus  Hanindhito", "Andreas  Hansson", "Swapnil  Haria", "Austin  Harris", "Timothy  Hayes", "Adrian  Herrera", "Matthew  Horsnell", "Syed Ali Raza Jafri", "Radhika  Jagtap", "Hanhwi  Jang", "Reiley  Jeyapaul", "Timothy M. Jones", "Matthias  Jung", "Subash  Kannoth", "Hamidreza  Khaleghzadeh", "Yuetsu  Kodama", "Tushar  Krishna", "Tommaso  Marinelli", "Christian  Menard", "Andrea  Mondelli", "Tiago  Muck", "Omar  Naji", "Krishnendra  Nathella", "Hoa  Nguyen", "Nikos  Nikoleris", "Lena E. Olson", "Marc  Orr", "Binh  Pham", "Pablo  Prieto", "Trivikram  Reddy", "Alec  Roelke", "Mahyar  Samani", "Andreas  Sandberg", "Javier  Setoain", "Boris  Shingarov", "Matthew D. Sinclair", "Tuan  Ta", "Rahul  Thakur", "Giacomo  Travaglini", "Michael  Upton", "Nilay  Vaish", "Ilias  Vougioukas", "Zhengrong  Wang", "Norbert  Wehn", "Christian  Weis", "David A. Wood", "Hongil  Yoon", "'Eder F. Zulian"], "year": 2020, "n_citations": 28}
{"id": 978442, "s2_id": "ef97c2fc870bc7cfeb2d87c3ce40b60d7319959c", "title": "A Study on Performance and Power Efficiency of Dense Non-Volatile Caches in Multi-Core Systems", "abstract": "This paper presents a novel cache design based on Multi-Level Cell Spin-Transfer Torque RAM (MLC STT-RAM).Our design exploits the asymmetric nature of the MLC STT-RAM to build cache lines featuring heterogeneous performances, that is, half of the cache lines are read-friendly,while the other half are write-friendly--this asymmetry in read/write latencies are then used by a migration policy in order to overcome the high latency of the baseline MLC cache. Furthermore, in order to enhance the device lifetime, we propose to dynamically deactivate ways of a set in underutilized sets to convert MLC to Single-Level Cell (SLC)mode.Our experiments show that our design gives an average improvement of 12% in system performance and 26% in last-level cache(L3) access energy for various workloads.", "venue": "SIGMETRICS 2017", "authors": ["Amin  Jadidi", "Mohammad  Arjomand", "Mahmut T. Kandemir", "Chita R. Das"], "year": 2017, "n_citations": 6}
{"id": 979454, "s2_id": "0d573d5f27504e51727b8c1f2be2f206e6a9cc18", "title": "In-network Neural Networks", "abstract": "We present N2Net, a system that implements binary neural networks using commodity switching chips deployed in network switches and routers. Our system shows that these devices can run simple neural network models, whose input is encoded in the network packets' header, at packet processing speeds (billions of packets per second). Furthermore, our experience highlights that switching chips could support even more complex models, provided that some minor and cheap modifications to the chip's design are applied. We believe N2Net provides an interesting building block for future end-to-end networked systems.", "venue": "ArXiv", "authors": ["Giuseppe  Siracusano", "Roberto  Bifulco"], "year": 2018, "n_citations": 13}
{"id": 982637, "s2_id": "07cb02f4c5abc4a3fa779b75186493a046b81bd1", "title": "Reducing Competitive Cache Misses in Modern Processor Architectures", "abstract": "The trends in the development of the multicore processors very often have the first level of cache memory implemented in. The increasing number of threads inside the cores and access competitively to the shared cache memory becomes the reason for the increased number of the competitive cache misses and decline the performances. Develop of the modern processor architectures leads to an increased number of cache misses. This paper has been made an attempt to implement the technique of decreasing the number of the competitive cache misses in the first level of the cache memory. This technique enables a competitive access to the complete cache memory when there is a hit inside of it. But if there are cache misses, then the memory data through the techniques of replacement is put in a virtual part given to the threads so that the competitive cache misses can be avoided. The results gained by using a simulator of the processor show decrease of the number of the cache misses and increase the performances for 15%. The conclusion out of this research can be that the cache misses are a real challenge for the future designers of processors and they need to be paid more attention to.", "venue": "ArXiv", "authors": ["Milcho  Prisagjanec", "Pece  Mitrevski"], "year": 2017, "n_citations": 1}
{"id": 985703, "s2_id": "50e25c21583632a9f19e8687b38df869192f45c8", "title": "Hardware Architecture of Wireless Power Transfer, RFID, and WIPT Systems", "abstract": "The early effort on WPT can be traced back to late 1950s when a theoretical analysis from Goubau and Schwering showed that power could be transmitted over any distance with near 100% efficiency through a concentrated beam [1]. Three years later, this theory was confirmed by an experimental demonstration indicating the born of an effective WPT system [2]. Around 1963, the rectenna was invented, which is a memorable event in the history of the WPT development. The rectenna can efficiently convert electromagnetic (EM) energy arrived at a WPT receiver into the direct current (DC). The invention of the rectenna paved the way of long-distance WPT. In the early stage of WPT development, a representative experiment was to power an unmanned helicopter with the microwave energy reflected from an ellipsoidal reflector placed at the ground [3] . The small helicopter was able to hover several meters above the reflector without extra energy supply [4]. With an increased energy conversion efficiency in 2.4 \u2013 2.5 GHz frequency bands, the flight altitude of the microwavepowered aircraft was further improved. In 1988, Canadian Stationary High Altitude Relay Platform Program (SHARP) demonstrated an airplane with 4.57m wingspan. The airplane could fly at 50m altitude for 3.5 minutes through receiving energy radiated from a parabolic dish with 10 kW transmission power [5]. In 1968, Peter Glaser introduced a concept of solar power satellite (SPS) that captures solar power through the satellite running in the geostationary orbit, and then sends the harvested energy back to the earth via microwaves [6]. To examine the feasibility of a long range WPT from a satellite to the ground, a successful experiment was conducted by Raytheon company in 1975. Between 1978 and 1986, DOE (Department of Energy) and NASA (National Aeronautics And Space Administration) jointly investigated the satellite power system concept development and evaluation program [7]. Nowadays, SPS is still considered as one of potential candidates to replace the fossil fuel and nuclear energy for a green, safe, and sustainable power supply [8].", "venue": "ArXiv", "authors": ["Yu  Luo", "Lina  Pu"], "year": 2021, "n_citations": 0}
{"id": 988819, "s2_id": "27d8e86a04ffaa4bd9bbfb2b48c1daf8b306819a", "title": "High Bandwidth Memory on FPGAs: A Data Analytics Perspective", "abstract": "FPGA-based data processing in datacenters is increasing in popularity due to the demands of modern workloads and the resulting need for specialization in hardware. Driven by this trend, vendors are rapidly adapting reconfigurable devices to suit data and compute intensive workloads. Inclusion of High Bandwidth Memory (HBM) in FPGA devices is a recent example. HBM promises overcoming the bandwidth bottleneck, often faced by FPGA-based accelerators due to their throughput oriented design. In this paper, we study the usage and benefits of HBM on FPGAs from a data analytics perspective. We consider three workloads that are often performed in analytics oriented databases and implement them on FPGA showing in which cases they benefit from HBM: range selection, hash join, and stochastic gradient descent for linear model training. We integrate our designs into a columnar database (MonetDB) and show the trade-offs arising from the integration related to data movement and partitioning. In certain cases, FPGA+HBM based solutions are able to surpass the highest performance provided by either a 2-socket POWER9 system or a 14-core XeonE5 by up to 1.8x (selection), 12.9x (join), and 3.2x (SGD).", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Kaan  Kara", "Christoph  Hagleitner", "Dionysios  Diamantopoulos", "Dimitris  Syrivelis", "Gustavo  Alonso"], "year": 2020, "n_citations": 6}
{"id": 991969, "s2_id": "b9890d3879217accb8eb03ca1687798b40e2b425", "title": "Locally Served Network Computers", "abstract": "NCs are the natural evolution of PCs, ubiquitous computers everywhere. The current vision of NCs requires two improbable developments: (1) inexpensive high-bandwidth WAN links to the Internet, and (2) inexpensive centralized servers. The large NC bandwidth requirements will force each home or office to have a local server LAN attached to the NCs. These servers will be much less expensive to purchase and manage than a centralized solution. Centralized staff are expensive and unresponsive.", "venue": "ArXiv", "authors": ["Jim  Gray"], "year": 1998, "n_citations": 4}
{"id": 992251, "s2_id": "64ea655196aca2461ee37a4c5f8ff48a2588c928", "title": "Improving Network-on-Chip-based Turbo Decoder Architectures", "abstract": "In this work novel results concerning Network-on-Chip-based turbo decoder architectures are presented. Stemming from previous publications, this work concentrates first on improving the throughput by exploiting adaptive-bandwidth-reduction techniques. This technique shows in the best case an improvement of more than 60 Mb/s. Moreover, it is known that double-binary turbo decoders require higher area than binary ones. This characteristic has the negative effect of increasing the data width of the network nodes. Thus, the second contribution of this work is to reduce the network complexity to support double-binary codes, by exploiting bit-level and pseudo-floating-point representation of the extrinsic information. These two techniques allow for an area reduction of up to more than the 40 % with a performance degradation of about 0.2 dB.", "venue": "J. Signal Process. Syst.", "authors": ["Maurizio  Martina", "Guido  Masera"], "year": 2013, "n_citations": 6}
{"id": 996910, "s2_id": "02478f634411d157fed8976d5d3e7f9ed2087a0d", "title": "Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks", "abstract": "Deep neural network (DNN) classifiers are powerful tools that drive a broad spectrum of important applications, from image recognition to autonomous vehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks that affect virtually all state-of-the-art models. These attacks make small imperceptible modifications to inputs that are sufficient to induce the DNNs to produce the wrong classification. In this paper we propose a novel, lightweight adversarial correction and/or detection mechanism for image classifiers that relies on undervolting (running a chip at a voltage that is slightly below its safe margin). We propose using controlled undervolting of the chip running the inference process in order to introduce a limited number of compute errors. We show that these errors disrupt the adversarial input in a way that can be used either to correct the classification or detect the input as adversarial. We evaluate the proposed solution in an FPGA design and through software simulation. We evaluate 10 attacks on two popular DNNs and show an average detection rate of 80% to 95%.", "venue": "ArXiv", "authors": ["Saikat  Majumdar", "Mohammad Hossein Samavatian", "Kristin  Barber", "Radu  Teodorescu"], "year": 2021, "n_citations": 0}
{"id": 999222, "s2_id": "d1e4500d3eaa6329b290b448cc90e42043d33557", "title": "fault: A Python Embedded Domain-Specific Language for Metaprogramming Portable Hardware Verification Components", "abstract": "While hardware generators have drastically improved design productivity, they have introduced new challenges for the task of verification. To effectively cover the functionality of a sophisticated generator, verification engineers require tools that provide the flexibility of metaprogramming. However, flexibility alone is not enough; components must also be portable in order to encourage the proliferation of verification libraries as well as enable new methodologies. This paper introduces fault, a Python embedded hardware verification language that aims to empower design teams to realize the full potential of generators.", "venue": "CAV", "authors": ["Lenny  Truong", "Steven  Herbst", "Rajsekhar  Setaluri", "Makai  Mann", "Ross  Daly", "Keyi  Zhang", "Caleb  Donovick", "Daniel  Stanley", "Mark  Horowitz", "Clark  Barrett", "Pat  Hanrahan"], "year": 2020, "n_citations": 1}
{"id": 1004191, "s2_id": "ad8da339d241a5232c55c2872670404f7ccae9b7", "title": "Application Checkpoint and Power Study on Large Scale Systems", "abstract": "Power efficiency is critical in high performance computing (HPC) systems. To achieve high power efficiency on application level, it is vital importance to efficiently distribute power used by application checkpoints. In this study, we analyze the relation of application checkpoints and their power consumption. The observations could guide the design of power management.", "venue": "ArXiv", "authors": ["Yuping  Fan"], "year": 2021, "n_citations": 2}
{"id": 1004211, "s2_id": "af5e07e0a55bc1572f5dbcb33662dab1c36ae565", "title": "HeapSafe: Securing Unprotected Heaps in RISC-V", "abstract": "RISC-V is a promising open-source architecture primarily targeted for embedded systems. Programs compiled using the RISC-V toolchain can run bare-metal on the system, and, as such, can be vulnerable to several memory corruption vulnerabilities. In this work, we present HeapSafe, a lightweight hardware assisted heap-buffer protection scheme to mitigate heap overflow and use-after-free vulnerabilities in a RISC-V SoC. The proposed scheme tags pointers associated with heap buffers with metadata indices and enforces tag propagation for commonly used pointer operations. The HeapSafe hardware is decoupled from the core and is designed as a configurable coprocessor and is responsible for validating the heap buffer accesses. Benchmark results show a 1.5X performance overhead and 1.59% area overhead, while being 22% faster than a software protection. We further implemented a HeapSafe-nb, an asynchronous validation design, which improves performance by 27% over the synchronous HeapSafe.", "venue": "ArXiv", "authors": ["Asmit  De", "Swaroop  Ghosh"], "year": 2021, "n_citations": 0}
{"id": 1004622, "s2_id": "fd41e687feae2f4cc2e95fb7bc142402350f7e23", "title": "Analysis of Interference between RDMA and Local Access on Hybrid Memory System", "abstract": "We can use a hybrid memory system consisting of DRAM and Intel Optane DC Persistent Memory (We call it DCPM in this paper) as DCPM is now commercially available since April 2019. Even if the latency for DCPM is several times higher than that for DRAM, the capacity for DCPM is several times higher than that for DRAM and the cost of DCPM is also several times lower than that for DRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory system could improve the performance for in-memory database systems and virtual machine (VM) systems because these systems often consume a large amount of memory. Moreover, a high-speed shared storage system can be implemented by accessing DCPM via remote direct memory access (RDMA). I assume that some of the DCPM is often assigned as a shared area among other remote servers because applications executed on a server with a hybrid memory system often cannot use the entire capacity of DCPM. This paper evaluates the interference between local memory access and RDMA from a remote server. As a result, I indicate that the interference on this hybrid memory system is significantly different from that on a conventional DRAM-only memory system. I also believe that some kind of throttling implementation is needed when this interference occures.", "venue": "ArXiv", "authors": ["Kazuichi  Oe"], "year": 2020, "n_citations": 0}
{"id": 1004673, "s2_id": "1e50dff60e0ea7c33271a5ab1fd76f974355c8d1", "title": "Versatile Data Acquisition and Controls for Epics Using Vme-Based Fpgas", "abstract": "Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson National Accelerator Facility (Jefferson Lab) with versatile VME-based data acquisition and control interfaces with minimal development times. FPGAs have been used to interface with VME controllers using standard A16 and A24 address modes. VME vectored-interrupt capability has also been implemented in some applications. FPGA designs have additionally been used to provide control logic for numerous systems by interfacing with Analog to Digital Converters (ADC), Digital to Analog Converters (DAC), various interlocks, and other drive signals. The building blocks of these logic designs can be tailored to the individual needs of each system and provide system operators with read-backs and controls via a VME interface to an EPICS based computer. This versatility allows the system developer to choose components and define operating parameters and options that are not readily available commercially. Jefferson Lab has begun developing standard FPGA libraries that result in quick turn around times and inexpensive designs. There have been approximately twelve VME-based FPGA designs implemented in the RF and Electronic Support (RFES) Group at Jefferson Lab. FPGA logic density and device speed continue to increase which enables many system designs to be incorporated onto one FPGA. FPGA designs can manipulate data quickly due to the small processing overhead associated with a custom design. This coupled with physical performance advances and optimized logic from compiler tools makes FPGAs solutions faster than many microprocessors. The ability to modify and simulate this firmware enables a designer to easily add new enhancements to a system or modify existing parameters, permitting the design to be both flexible and expandable for future applications.", "venue": "ArXiv", "authors": ["T.  Allison", "R.  Flood"], "year": 2001, "n_citations": 2}
{"id": 1005975, "s2_id": "eb9ca5880981750ec52080f7dd96e75f7b6905d7", "title": "Towards Fully Intelligent Transportation through Infrastructure-Vehicle Cooperative Autonomous Driving: Challenges and Opportunities", "abstract": "The infrastructure-vehicle cooperative autonomous driving approach depends on the cooperation between intelligent roads and intelligent vehicles. This approach is not only safer but also more economical compared to the traditional on-vehicle-only autonomous driving approach. In this paper, we introduce our real-world deployment experiences of cooperative autonomous driving, and delve into the details of new challenges and opportunities. Specifically, based on our progress towards commercial deployment, we follow a three-stage development roadmap of the cooperative autonomous driving approach:infrastructureaugmented autonomous driving (IAAD), infrastructure-guided autonomous driving (IGAD), and infrastructure-planned autonomous driving (IPAD).", "venue": "ArXiv", "authors": ["Shaoshan  Liu", "Bo  Yu", "Jie  Tang", "Qi  Zhu"], "year": 2021, "n_citations": 0}
{"id": 1007517, "s2_id": "b455870ae376073245601d07d76d186021a7f278", "title": "Lord of the Ring(s): Side Channel Attacks on the CPU On-Chip Ring Interconnect Are Practical", "abstract": "We introduce the first microarchitectural side channel attacks that leverage contention on the CPU ring interconnect. There are two challenges that make it uniquely difficult to exploit this channel. First, little is known about the ring interconnect's functioning and architecture. Second, information that can be learned by an attacker through ring contention is noisy by nature and has coarse spatial granularity. To address the first challenge, we perform a thorough reverse engineering of the sophisticated protocols that handle communication on the ring interconnect. With this knowledge, we build a cross-core covert channel over the ring interconnect with a capacity of over 4 Mbps from a single thread, the largest to date for a cross-core channel not relying on shared memory. To address the second challenge, we leverage the fine-grained temporal patterns of ring contention to infer a victim program's secrets. We demonstrate our attack by extracting key bits from vulnerable EdDSA and RSA implementations, as well as inferring the precise timing of keystrokes typed by a victim user.", "venue": "USENIX Security Symposium", "authors": ["Riccardo  Paccagnella", "Licheng  Luo", "Christopher W. Fletcher"], "year": 2021, "n_citations": 10}
{"id": 1011720, "s2_id": "6e94102461c62642cc759ba59e033341c63a9402", "title": "A Lyra2 FPGA Core for Lyra2REv2-Based Cryptocurrencies", "abstract": "Lyra2REv2 is a hashing algorithm that consists of a chain of individual hashing algorithms and it is used as a proof-of-work function in several cryptocurrencies that aim to be ASIC-resistant. The most crucial hashing algorithm in the Lyra2REv2 chain is a specific instance of the general Lyra2 algorithm. In this work we present the first FPGA implementation of the aforementioned instance of Lyra2 and we explain how several properties of the algorithm can be exploited in order to optimize the design.", "venue": "2019 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Michiel Van Beirendonck", "Pascal  Giard", "Alexios  Balatsoukas-Stimming"], "year": 2019, "n_citations": 1}
{"id": 1013036, "s2_id": "5f548f10b5921882f7b4a4b6b7f4ee1a3e229c9c", "title": "A software framework for pipelined arithmetic algorithms in field programmable gate arrays", "abstract": "Abstract Pipelined algorithms implemented in field programmable gate arrays are extensively used for hardware triggers in the modern experimental high energy physics field and the complexity of such algorithms increases rapidly. For development of such hardware triggers, algorithms are developed in C++ , ported to hardware description language for synthesizing firmware, and then ported back to C++ for simulating the firmware response down to the single bit level. We present a C++ software framework which automatically simulates and generates hardware description language code for pipelined arithmetic algorithms.", "venue": "ArXiv", "authors": ["J. B. Kim", "E.  Won"], "year": 2017, "n_citations": 2}
{"id": 1014222, "s2_id": "303e6fe20afa7a2d584a25a92b2eb01871cc728a", "title": "Reduction in Packet Delay Through the use of Common Buffer over Distributed Buffer in the Routing Node of NOC Architecture", "abstract": "The continuous innovation of semiconductor technology enables more complex System on-Chip (SoC) designs. Tens, even hundreds of intellectual properties (IPs) are integrated into an SoC to provide various functions, including communications, networking, multimedia, storage, etc. The bus scheme connects multiple IP cores with a cost efficient shared medium. The bus-based scheme still fails to satisfy the requirements of future SoC mainly due to two major drawbacks. Non-scalable and the bandwidth is shared by all IPs and thus the bus becomes the performance bottleneck when more and more IPs are connected. In order to interconnect such a high number of elements on a die, researchers have turned to Network On Chip as a replacement to conventional shared buses and ad-hoc wiring solutions. They are attractive due to their regularity and modular design, which can lead to better routability, electrical characteristics and fault tolerance. Performance evaluation of the routing node in terms of latency is the characteristics of an efficient design of Buffer in input module. It is intended to study and quantify the behavior of the single packet array design in relation to the multiple packet array design. The utilization efficiency of the packet buffer array improves when a common buffer is used instead of individual buffers in each input port. First Poisson\u2019s Queuing model was prepared to manifest the differences in packet delays. The queuing model can be classified as (M/M/1); (32;FIFO). Arrival rate has been assumed to be Poisson distributed with a mean arrival rate ( \u03bb ) of 10 x 106. The service rate is assumed to be exponentially distributed with a mean service rate of 10.05 x 106. It has been observed that latency in Common Buffer improved by 46% over its distributed buffer A Simulink model later simulated on MATLAB to calculate the improvement in packet delay. It has been observed that the delay improved by approximately 40% through the use of a common buffer. A verilog RTL for both common and shared buffer has been prepared and later synthesized using Design Compiler of SYNOPSYS. In distributed buffer, arrival of data packet could be delayed by 2 or 4 clock cycles which lead to latency improvement either by 17 % or 34 % in a common buffer", "venue": "ArXiv", "authors": ["Nilesh A. Mohota", "Sanjay L. Badjate"], "year": 2013, "n_citations": 0}
{"id": 1017333, "s2_id": "4b1157e263d932099e0f830a95abc4fc9d9ff066", "title": "Hardware Implementation of Iterative Projection-Aggregation Decoding of Reed-Muller Codes", "abstract": "In this work, we present a simplification and a corresponding hardware architecture for hard-decision recursive projection-aggregation (RPA) decoding of Reed-Muller (RM) codes. In particular, we transform the recursive structure of RPA decoding into a simpler and iterative structure with minimal error-correction degradation. Our simulation results for RM(7,3) show that the proposed simplification has a small error-correcting performance degradation (0.005 in terms of channel crossover probability) while reducing the average number of computations by up to 40%. In addition, we describe the first fully parallel hardware architecture for simplified RPA decoding. We present FPGA implementation results for an RM(6,3) code on a Xilinx Virtex-7 FPGA showing that our proposed architecture achieves a throughput of 171 Mbps at a frequency of 80 MHz.", "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Marzieh  Hashemipour-Nazari", "Kees  Goossens", "Alexios  Balatsoukas-Stimming"], "year": 2021, "n_citations": 1}
{"id": 1019372, "s2_id": "f3628c556c9dbfc475f5d5eb3102bd0776e1bb59", "title": "E3NE: An End-to-End Framework for Accelerating Spiking Neural Networks with Emerging Neural Encoding on FPGAs", "abstract": "Compiler frameworks are crucial for the widespread use of FPGA-based deep learning accelerators. They allow researchers and developers, who are not familiar with hardware engineering, to harness the performance attained by domain-specific logic. There exists a variety of frameworks for conventional artificial neural networks. However, not much research effort has been put into the creation of frameworks optimized for spiking neural networks (SNNs). This new generation of neural networks becomes increasingly interesting for the deployment of AI on edge devices, which have tight power and resource constraints. Our end-to-end framework E3NE automates the generation of efficient SNN inference logic for FPGAs. Based on a PyTorch model and user parameters, it applies various optimizations and assesses trade-offs inherent to spike-based accelerators. Multiple levels of parallelism and the use of an emerging neural encoding scheme result in an efficiency superior to previous SNN hardware implementations. For a similar model, E3NE uses less than 50% of hardware resources and 20% less power, while reducing the latency by an order of magnitude. Furthermore, scalability and generality allowed the deployment of the large-scale SNN models AlexNet and VGG.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Daniel  Gerlinghoff", "Zhehui  Wang", "Xiaozhe  Gu", "Rick Siow Mong Goh", "Tao  Luo"], "year": 2021, "n_citations": 0}
{"id": 1022126, "s2_id": "ea0fc444907f452335f2d8ab4ebed461ff93328f", "title": "TLB and Pagewalk Performance in Multicore Architectures with Large Die-Stacked DRAM Cache", "abstract": "In this work we study the overheads of virtual-to-physical address translation in processor architectures, like x86-64, that implement paged virtual memory using a radix tree which are walked in hardware. Translation Lookaside Buffers are critical to system performance, particularly as applications demand larger memory footprints and with the adoption of virtualization; however the cost of a TLB miss potentially results in multiple memory accesses to retrieve the translation. Architectural support for superpages has been introduced to increase TLB hits but are limited by the operating systems ability to find contiguous memory. Numerous prior studies have proposed TLB designs to lower miss rates and reduce page walk overhead; however, these studies have modeled the behavior analytically. Further, to eschew the paging overhead for big-memory workloads and virtualization, Direct Segment maps part of a process linear virtual address space with segment registers albeit requiring a few application and operating system modifications. The recently evolved die-stacked DRAM technology promises a high bandwidth and large last-level cache, in the order of Gigabytes, closer to the processors. With such large caches the amount of data that can be accessed without causing a TLB fault - the reach of a TLB, is inadequate. TLBs are on the critical path for data accesses and incurring an expensive page walk can hinder system performance, especially when the data being accessed is a cache hit in the LLC. Hence, we are interested in exploring novel address translation mechanisms, commensurate to the size and latency of stacked DRAM. By accurately simulating the multitude of multi-level address translation structures using the QEMU based MARSSx86 full system simulator, we perform detailed study of TLBs in conjunction with the large LLCs using multi-programmed and multi-threaded workloads.", "venue": "ArXiv", "authors": ["Adarsh  Patil"], "year": 2020, "n_citations": 0}
{"id": 1022472, "s2_id": "a0a923f4b7f87746e48773854059339719e3343d", "title": "Out-of-Order Dataflow Scheduling for FPGA Overlays", "abstract": "We exploit floating-point DSPs in the Arria10 FPGA and multi-pumping feature of the M20K RAMs to build a dataflow-driven soft processor fabric for large graph workloads. In this paper, we introduce the idea of out-of-order node scheduling across a large number of local nodes (thousands) per processor by combining an efficient node tagging scheme along with leading-one detector circuits. We use a static one-time node labeling algorithm to sort nodes based on criticality to organize local memory inside each soft processor. This translates to a small ~6% memory overhead. When compared to a memory-expensive FIFO-based first-come-first-serve approach used in previous studies, we deliver up to 50% performance improvement while eliminating the cost of the FIFOs. On the Arria10 10AX115S board, we can create an overlay design of up to 300 processors connected by high bandwidth Hoplite NoC at frequencies up to 250MHz.", "venue": "ArXiv", "authors": ["Siddhartha", "Nachiket  Kapre"], "year": 2017, "n_citations": 0}
{"id": 1025930, "s2_id": "a9d1b69d50b771f99a132d01679279cc0be261b9", "title": "MultiAmdahl: Optimal Resource Allocation in Heterogeneous Architectures", "abstract": "Future multiprocessor chips will integrate many different units, each tailored to a specific computation. When designing such a system, the chip architect must decide how to distribute limited system resources such as area, power, and energy among the computational units. We extend MultiAmdahl, an analytical optimization technique for resource allocation in heterogeneous architectures, for energy optimality under a variety of constant system power scenarios. We conclude that reduction in constant system power should be met by reallocating resources from general-purpose computing to heterogeneous accelerator-dominated computing, to keep the overall energy consumption at a minimum. We extend this conclusion to offer an intuition regarding energy-optimal resource allocation in data center computing.", "venue": "ArXiv", "authors": ["Leonid  Yavits", "Amir  Morad", "Uri C. Weiser", "Ran  Ginosar"], "year": 2017, "n_citations": 0}
{"id": 1025953, "s2_id": "ed884ab2aecbb7648c439e6f291e0c9ab72e6252", "title": "Modeling interconnect variability using efficient parametric model order reduction", "abstract": "Assessing IC manufacturing process fluctuations and their impacts on IC interconnect performance has become unavoidable for modern DSM designs. However, the construction of parametric interconnect models is often hampered by the rapid increase in computational cost and model complexity. In this paper we present an efficient yet accurate parametric model order reduction algorithm for addressing the variability of IC interconnect performance. The efficiency of the approach lies in a novel combination of low-rank matrix approximation and multi-parameter moment matching. The complexity of the proposed parametric model order reduction is as low as that of a standard Krylov subspace method when applied to a nominal system. Under the projection-based framework, our algorithm also preserves the passivity of the resulting parametric models.", "venue": "Design, Automation and Test in Europe", "authors": ["Peng  Li", "Frank  Liu", "Xin  Li", "Lawrence T. Pileggi", "Sani R. Nassif"], "year": 2005, "n_citations": 76}
{"id": 1027052, "s2_id": "c9572cc6b2931dbe276f387a1d472f7037044b89", "title": "The Impact of On-chip Communication on Memory Technologies for Neuromorphic Systems", "abstract": "Emergent nanoscale non-volatile memory technologies with high integration density offer a promising solution to overcome the scalability limitations of CMOS-based neural networks architectures, by efficiently exhibiting the key principle of neural computation. Despite the potential improvements in computational costs, designing high-performance on-chip communication networks that support flexible, large-fanout connectivity remains as daunting task. In this paper, we elaborate on the communication requirements of large-scale neuromorphic designs, and point out the differences with the conventional network-on-chip architectures. We present existing approaches for on-chip neuromorphic routing networks, and discuss how new memory and integration technologies may help to alleviate the communication issues in constructing next-generation intelligent computing machines.", "venue": "Journal of Physics D: Applied Physics", "authors": ["Saber  Moradi", "Rajit  Manohar"], "year": 2018, "n_citations": 22}
{"id": 1029134, "s2_id": "7524ee9d8303f4947cd3ca9e3e4c74ee77bead0f", "title": "A cache management strategy to replace wear leveling techniques for embedded flash memory", "abstract": "Prices of NAND flash memories are falling drastically due to market growth and fabrication process mastering while research efforts from a technological point of view in terms of endurance and density are very active. NAND flash memories are becoming the most important storage media in mobile computing and tend to be less confined to this area. The major constraint of such a technology is the limited number of possible erase operations per block which tend to quickly provoke memory wear out. To cope with this issue, state-of-the-art solutions implement wear leveling policies to level the wear out of the memory and so increase its lifetime. These policies are integrated into the Flash Translation Layer (FTL) and greatly contribute in decreasing the write performance. In this paper, we propose to reduce the flash memory wear out problem and improve its performance by absorbing the erase operations throughout a dual cache system replacing FTL wear leveling and garbage collection services. We justify this idea by proposing a first performance evaluation of an exclusively cache based system for embedded flash memories. Unlike wear leveling schemes, the proposed cache solution reduces the total number of erase operations reported on the media by absorbing them in the cache for workloads expressing a minimal global sequential rate.", "venue": "2011 International Symposium on Performance Evaluation of Computer & Telecommunication Systems", "authors": ["Jalil  Boukhobza", "Pierre  Olivier", "St\u00e9phane  Rubini"], "year": 2011, "n_citations": 12}
{"id": 1029266, "s2_id": "85916c7577abc6dd775434e982717c27105826c5", "title": "Thermal Analysis of a 3D Stacked High-Performance Commercial Microprocessor using Face-to-Face Wafer Bonding Technology", "abstract": "3D integration technologies are seeing widespread adoption in the semiconductor industry to offset the limitations and slowdown of two-dimensional scaling. High-density 3D integration techniques such as face-to-face wafer bonding with sub-10 \u03bcm pitch can enable new ways of designing SoCs using all 3 dimensions, like folding a microprocessor design across multiple 3D tiers. However, overlapping thermal hotspots can be a challenge in such 3D stacked designs due to a general increase in power density. In this work, we perform a thorough thermal simulation study on sign-off quality physical design implementation of a state-of-the-art, high-performance, out-of-order microprocessor on a 7nm process technology. The physical design of the microprocessor is partitioned and implemented in a 2-tier, 3D stacked configuration with logic blocks and memory instances in separate tiers (logic-over-memory 3D). The thermal simulation model was calibrated to temperature measurement data from a high-performance, CPU-based 2D SoC chip fabricated on the same 7nm process technology. Thermal profiles of different 3D configurations under various workload conditions are simulated and compared. We find that stacking microprocessor designs in 3D without considering thermal implications can result in maximum die temperature up to 12\u00b0C higher than their 2D counterparts under the worst-case power-indicative workload. This increase in temperature would reduce the amount of time for which a power-intensive workload can be run before throttling is required. However, logic-over-memory partitioned 3D CPU implementation can mitigate this temperature increase by half, which makes the temperature of the 3D design only 6\u00b0C higher than the 2D baseline. We conclude that using thermal-aware design partitioning and improved cooling techniques can overcome the thermal challenges associated with 3D stacking.", "venue": "2020 IEEE 70th Electronic Components and Technology Conference (ECTC)", "authors": ["Rahul  Mathur", "Chien-Ju  Chao", "Rossana  Liu", "Nikhil  Tadepalli", "Pranavi  Chandupatla", "Shawn  Hung", "Xiaoqing  Xu", "Saurabh  Sinha", "Jaydeep  Kulkarni"], "year": 2020, "n_citations": 3}
{"id": 1029828, "s2_id": "8720ccfa48b369d6b9a148d194496c7809b75dec", "title": "MERIT: Tensor Transform for Memory-Efficient Vision Processing on Parallel Architectures", "abstract": "Computationally intensive deep neural networks (DNNs) are well- suited to run on GPUs, but newly developed algorithms usually require the heavily optimized DNN routines to work efficiently, and this problem could be even more difficult for specialized DNN architectures. In this article, we propose a mathematical formulation that can be useful for transferring the algorithm optimization knowledge across computing platforms. We discover that data movement and storage inside parallel processor architectures can be viewed as tensor transforms across memory hierarchies, making it possible to describe many memory optimization techniques mathematically. Such transform, which we call memory-efficient ranged inner-product tensor (MERIT) transform, can be applied to not only DNN tasks but also many traditional machine learning and computer vision computations. Moreover, the tensor transforms can be readily mapped to existing vector processor architectures. In this article, we demonstrate that many popular applications can be converted to a succinct MERIT notation on GPUs, speeding up GPU kernels up to 20 times while using only half as many code tokens. We also use the principle of the proposed transform to design a specialized hardware unit called MERIT-z processor. This processor can be applied to a variety of DNN tasks as well as other computer vision tasks while providing comparable area and power efficiency to dedicated DNN application-specific integrated circuits (ASICs).", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Yu-Sheng  Lin", "Wei-Chao  Chen", "Shao-Yi  Chien"], "year": 2020, "n_citations": 3}
{"id": 1031325, "s2_id": "3b0ba0d096ed43a61c921f196aa3784943d1d5b8", "title": "A Time Leap Challenge for SAT Solving", "abstract": "We compare the impact of hardware advancement and algorithm advancement for SAT-solving over the last two decades. In particular, we compare 20-year-old SAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old hardware. Our findings show that the progress on the algorithmic side has at least as much impact as the progress on the hardware side.", "venue": "CP", "authors": ["Johannes K. Fichte", "Markus  Hecher", "Stefan  Szeider"], "year": 2020, "n_citations": 7}
{"id": 1034082, "s2_id": "3b2491ddeeaa7beae4d311b217c292a9e16112cf", "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference", "abstract": "Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present FINN, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 \u03bcs latency on the MNIST dataset with 95.8% accuracy, and 21906 image classifications per second with 283 \u03bcs latency on the CIFAR-10 and SVHN datasets with respectively 80.1% and 94.9% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.", "venue": "FPGA", "authors": ["Yaman  Umuroglu", "Nicholas J. Fraser", "Giulio  Gambardella", "Michaela  Blott", "Philip Heng Wai Leong", "Magnus  Jahre", "Kees A. Vissers"], "year": 2017, "n_citations": 523}
{"id": 1038414, "s2_id": "c22e9475d4f6a214f27a78d89167da7d300f9e37", "title": "Correlation manipulating circuits for stochastic computing", "abstract": "Stochastic computing (SC) is an emerging computing technique that promises high density, low power, and error tolerant solutions. In SC, values are encoded as unary bitstreams and SC arithmetic circuits operate on one or more bitstreams. In many cases, the input bitstreams must be correlated or uncorrelated for SC arithmetic to produce accurate results. As a result, a key challenge for designing SC accelerators is manipulating the impact of correlation across SC operations. This paper presents and evaluates a set of novel correlation manipulating circuits to manage correlation in SC computation: a synchronizer, desynchronizer, and decorrelator. We then use these circuits to propose improved SC maximum, minimum, and saturating adder designs. Compared to existing correlation manipulation techniques, our circuits are more accurate and up to 3\u00d7 more energy efficient. In the context of an image processing pipeline, these circuits can reduce the total energy consumption by up to 24%.", "venue": "2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Vincent T. Lee", "Armin  Alaghi", "Luis  Ceze"], "year": 2018, "n_citations": 21}
{"id": 1038675, "s2_id": "fc2246157fffbcd02310307d875717b0e63717aa", "title": "System Performance with varying L1 Instruction and Data Cache Sizes: An Empirical Analysis", "abstract": "In this project, we investigate the fluctuations in performance caused by changing the Instruction (I-cache) size and the Data (D-cache) size in the L1 cache. We employ the Gem5 framework to simulate a system with varying specifications on a single host machine. We utilize the FreqMine benchmark available under the PARSEC suite as the workload program to benchmark our simulated system. The Out-order CPU (O3) with Ruby memory model was simulated in a Full-System X86 environment with Linux OS. The chosen metrics deal with Hit Rate, Misses, Memory Latency, Instruction Rate, and Bus Traffic within the system. Performance observed by varying L1 size within a certain range of values was used to compute Confidence Interval based statistics for relevant metrics. Our expectations, corresponding experimental observations, and discrepancies are also discussed in this report.", "venue": "ArXiv", "authors": ["Ramya  Akula", "Kartik  Jain", "Deep Jigar Kotecha"], "year": 2019, "n_citations": 2}
{"id": 1039757, "s2_id": "443701309d27b41bc23e28c950a052e1199fc0df", "title": "Fast and Generalized Polynomial Time Memory Consistency Verification", "abstract": "The problem of verifying multi-threaded execution against the memory consistency model of a processor is known to be an NP hard problem. However polynomial time algorithms exist that detect almost all failures in such execution. These are often used in practice for microprocessor verification. We present a low complexity and fully parallelized algorithm to check program execution against the processor consistency model. In addition our algorithm is general enough to support a number of consistency models without any degradation in performance. An implementation of this algorithm is currently used in practice to verify processors in the post silicon stage for multiple architectures.", "venue": "CAV", "authors": ["Amitabha  Roy", "Stephan  Zeisset", "Charles J. Fleckenstein", "John C. Huang"], "year": 2006, "n_citations": 47}
{"id": 1041546, "s2_id": "419bc6fb3cdfa2cd2318ad8f200f17061bbb033f", "title": "Locality-aware process scheduling for embedded MPSoCs", "abstract": "Utilizing on-chip caches in embedded multiprocessor-system-on-a-chip (MPSoC) based systems is critical from both performance and power perspectives. While most of the prior work that targets at optimizing cache behavior are performed at hardware and compilation levels, operating system (OS) can also play major role as it sees the global access pattern information across applications. This paper proposes a cache-conscious OS process scheduling strategy based on data reuse. The proposed scheduler implements two complementary approaches. First, the processes that do not share any data between them are scheduled at different cores if it is possible to do so. Second, the processes that could not be executed at the same time (due to dependences) but share data among each other are mapped to the same processor core so that they share the cache contents. Our experimental results using this new data locality aware OS scheduling strategy are promising, and show significant improvements in task completion times.", "venue": "Design, Automation and Test in Europe", "authors": ["Mahmut T. Kandemir", "Guilin  Chen"], "year": 2005, "n_citations": 20}
{"id": 1044114, "s2_id": "4bb6744b39b4c2b2c0aa52885842a917b1eaaabb", "title": "To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement Policies for DRAM Caches", "abstract": "This paper investigates intelligent replacement policies for improving the hit-rate of gigascale DRAM caches. Cache replacement policies are commonly used to improve the hit-rate of on-chip caches. The most effective replacement policies often require the cache to track and update per-line reuse state to inform their decision. A fundamental challenge on DRAM caches, however, is that stateful policies would require significant bandwidth to maintain per-line DRAM cache state. As such, DRAM cache replacement policies have primarily been stateless policies, such as always-install or probabilistic bypass. Unfortunately, we find that stateless policies are often too coarse-grain and become ineffective at the size and associativity of DRAM caches. Ideally, we want a replacement policy that can obtain the hit-rate benefits of stateful replacement policies, but keep the bandwidth-efficiency of stateless policies. We perform our study on a DRAM cache design similar to the one in Knights Landing, and find that tracking per-line reuse state can enable an effective replacement policy that can mitigate the common thrashing patterns seen in gigascale caches. We propose a stateful replacement/bypass policy called RRIP Age-On-Bypass (RRIP-AOB), that tracks reuse state for high-reuse lines, protects such lines by bypassing other lines, and Ages the state On cache Bypass. Unfortunately, such a stateful technique requires significant bandwidth to update state. To this end, we propose Efficient Tracking of Reuse (ETR). ETR makes state tracking efficient by accurately tracking the state of only one line from a region, and using the state of that line to guide the replacement decisions for other lines in that region. ETR reduces the bandwidth for tracking replacement state by 70%, and makes stateful policies practical for DRAM caches. Our evaluations with a 2GB DRAM cache, show that our RRIP-AOB and ETR techniques provide 18% speedup while needing <1KB SRAM.", "venue": "2019 IEEE 37th International Conference on Computer Design (ICCD)", "authors": ["Vinson  Young", "Moinuddin K. Qureshi"], "year": 2019, "n_citations": 1}
{"id": 1045048, "s2_id": "f074e93c4eb26db12a27302a1fa21f7868b47491", "title": "F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar Decoding", "abstract": "Creating virtual avatars with realistic rendering is one of the most essential and challenging tasks to provide highly immersive virtual reality (VR) experiences. It requires not only sophisticated deep neural network (DNN) based codec avatar decoders to ensure high visual quality and precise motion expression, but also efficient hardware accelerators to guarantee smooth real-time rendering using lightweight edge devices, like untethered VR headsets. Existing hardware accelerators, however, fail to deliver sufficient performance and efficiency targeting such decoders which consist of multi-branch DNNs and require demanding compute and memory resources. To address these problems, we propose an automation framework, called F-CAD (Facebook Codec avatar Accelerator Design), to explore and deliver optimized hardware accelerators for codec avatar decoding. Novel technologies include 1) a new accelerator architecture to efficiently handle multi-branch DNNs; 2) a multi-branch dynamic design space to enable fine-grained architecture configurations; and 3) an efficient architecture search for picking the optimized hardware design based on both application-specific demands and hardware resource constraints. To the best of our knowledge, F-CAD is the first automation tool that supports the whole design flow of hardware acceleration of codec avatar decoders, allowing joint optimization on decoder designs in popular machine learning frameworks and corresponding customized accelerator design with cycle-accurate evaluation. Results show that the accelerators generated by F-CAD can deliver up to 122.1 frames per second (FPS) and 91.6% hardware efficiency when running the latest codec avatar decoder. Compared to the state-of-the-art designs, F-CAD achieves 4.0\u00d7 and 2.8\u00d7 higher throughput, 62.5% and 21.2% higher efficiency than DNNBuilder [1] and HybridDNN [2] by targeting the same hardware device.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Xiaofan  Zhang", "Dawei  Wang", "Pierce  Chuang", "Shugao  Ma", "Deming  Chen", "Yuecheng  Li"], "year": 2021, "n_citations": 1}
{"id": 1045468, "s2_id": "1802a2c67b02ebeb95077cd2a6a29d411c66dd26", "title": "Time-Shared Execution of Realtime Computer Vision Pipelines by Dynamic Partial Reconfiguration", "abstract": "This paper presents an FPGA runtime framework that demonstrates the feasibility of using dynamic partial reconfiguration (DPR) for time-sharing an FPGA by multiple realtime computer vision pipelines. The presented time-sharing runtime framework manages an FPGA fabric that can be round-robin time-shared by different pipelines at the time scale of individual frames. In this new use-case, the challenge is to achieve useful performance despite high reconfiguration time. The paper describes the basic runtime support as well as four optimizations necessary to achieve realtime performance given the limitations of DPR on today's FPGAs. The paper provides a characterization of a working runtime framework prototype on a Xilinx ZC706 development board. The paper also reports the performance of streaming vision pipelines when time-shared.", "venue": "2018 28th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Marie  Nguyen", "James C. Hoe"], "year": 2018, "n_citations": 9}
{"id": 1048322, "s2_id": "9e6aca4c7febcb2661b0a921458637807301738e", "title": "Exploring the Feasibility of Using 3-D XPoint as an In-Memory Computing Accelerator", "abstract": "This article describes how 3-D XPoint memory arrays can be used as in-memory computing accelerators. We first show that thresholded matrix-vector multiplication (TMVM), the fundamental computational kernel in many applications including machine learning (ML), can be implemented within a 3-D XPoint array without requiring data to leave the array for processing. Using the implementation of TMVM, we then discuss the implementation of a binary neural inference engine. We discuss the application of the core concept to address issues such as system scalability, where we connect multiple 3-D XPoint arrays, and power integrity, where we analyze the parasitic effects of metal lines on noise margins. To assure power integrity within the 3-D XPoint array during this implementation, we carefully analyze the parasitic effects of metal lines on the accuracy of the implementations. We quantify the impact of parasitics on limiting the size and configuration of a 3-D XPoint array, and estimate the maximum acceptable size of a 3-D XPoint subarray.", "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits", "authors": ["Masoud  Zabihi", "Salonik  Resch", "H\u00fcsrev  Cilasun", "Zamshed I. Chowdhury", "Zhengyang  Zhao", "Ulya R. Karpuzcu", "Jian-Ping  Wang", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 0}
{"id": 1049068, "s2_id": "5e1f02ec3d9830c369facba2c141a344c566980d", "title": "PermDNN: Efficient Compressed DNN Architecture with Permuted Diagonal Matrices", "abstract": "Deep neural network (DNN) has emerged as the most important and popular artificial intelligent (AI) technique. The growth of model size poses a key energy efficiency challenge for the underlying computing platform. Thus, model compression becomes a crucial problem. However, the current approaches are limited by various drawbacks. Specifically, network sparsification approach suffers from irregularity, heuristic nature and large indexing overhead. On the other hand, the recent structured matrix-based approach (i.e., CirCNN) is limited by the relatively complex arithmetic computation (i.e., FFT), less flexible compression ratio, and its inability to fully utilize input sparsity. To address these drawbacks, this paper proposes PermDNN, a novel approach to generate and execute hardware-friendly structured sparse DNN models using permuted diagonal matrices. Compared with unstructured sparsification approach, PermDNN eliminates the drawbacks of indexing overhead, non-heuristic compression effects and time-consuming retraining. Compared with circulant structure-imposing approach, PermDNN enjoys the benefits of higher reduction in computational complexity, flexible compression ratio, simple arithmetic computation and full utilization of input sparsity. We propose PermDNN architecture, a multi-processing element (PE) fully-connected (FC) layer-targeted computing engine. The entire architecture is highly scalable and flexible, and hence it can support the needs of different applications with different model configurations. We implement a 32-PE design using CMOS 28nm technology. Compared with EIE, PermDNN achieves 3.3x~4.8x higher throughout, 5.9x~8.5x better area efficiency and 2.8x~4.0x better energy efficiency on different workloads. Compared with CirCNN, PermDNN achieves 11.51x higher throughput and 3.89x better energy efficiency.", "venue": "2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Chunhua  Deng", "Siyu  Liao", "Yi  Xie", "Keshab K. Parhi", "Xuehai  Qian", "Bo  Yuan"], "year": 2018, "n_citations": 63}
{"id": 1049840, "s2_id": "83da98b0358717a195985fc40dad7ed9d4bb4682", "title": "The Study of Transient Faults Propagation in Multithread Applications", "abstract": "Whereas contemporary Error Correcting Codes (ECC) designs occupy a significant fraction of total die area in chip-multiprocessors (CMPs), approaches to deal with the vulnerability increase of CMP architecture against Single Event Upsets (SEUs) and Multi-Bit Upsets (MBUs) are sought. In this paper, we focus on reliability assessment of multithreaded applications running on CMPs to propose an adaptive application-relevant architecture design to accommodate the impact of both SEUs and MBUs in the entire CMP architecture. This work concentrates on leveraging the intrinsic soft-error-immunity feature of Spin-Transfer Torque RAM (STT-RAM) as an alternative for SRAM-based storage and operation components. We target a specific portion of working set for reallocation to improve the reliability level of the CMP architecture design. A selected portion of instructions in multithreaded program which experience high rate of referencing with the lowest memory modification are ideal candidate to be stored and executed in STT-RAM based components. We argue about why we cannot use STT-RAM for the global storage and operation counterparts and describe the obtained resiliency compared to the baseline setup. In addition, a detail study of the impact of SEUs and MBUs on multithreaded programs will be presented in the Appendix.", "venue": "ArXiv", "authors": ["Navid  Khoshavi", "Armin  Samiei"], "year": 2016, "n_citations": 0}
{"id": 1050158, "s2_id": "0f5bede8ed9412fecffbeafdc4581a7b51753e95", "title": "Exploring Memory Access Patterns for Graph Processing Accelerators", "abstract": "Recent trends in business and technology (e.g., machine learning, social network analysis) benefit from storing and processing growing amounts of graph-structured data in databases and data science platforms. FPGAs as accelerators for graph processing with a customizable memory hierarchy promise solving performance problems caused by inherent irregular memory access patterns on traditional hardware (e.g., CPU). However, developing such hardware accelerators is yet time-consuming and difficult and benchmarking is non-standardized, hindering comprehension of the impact of memory access pattern changes and systematic engineering of graph processing accelerators. \nIn this work, we propose a simulation environment for the analysis of graph processing accelerators based on simulating their memory access patterns. Further, we evaluate our approach on two state-of-the-art FPGA graph processing accelerators and show reproducibility, comparablity, as well as the shortened development process by an example. Not implementing the cycle-accurate internal data flow on accelerator hardware like FPGAs significantly reduces the implementation time, increases the benchmark parameter transparency, and allows comparison of graph processing approaches.", "venue": "BTW", "authors": ["Jonas  Dann", "Daniel  Ritter", "Holger  Fr\u00f6ning"], "year": 2021, "n_citations": 1}
{"id": 1054616, "s2_id": "e9bb40e649bea68bd593fbd9d45d17cc45e53935", "title": "1.5 bit-per-stage 8-bit Pipelined CMOS A/D Converter for Neuromophic Vision Processor", "abstract": "Neuromorphic vision processor is an electronic implementation of vision algorithm processor on semiconductor. To image the world, a low-power CMOS image sensor array is required in the vision processor. The image sensor array is typically formed through photo diodes and analog to digital converter (ADC). To achieve low power acquisition, a low-power mid-resolution ADC is necessary. In this paper, a 1.8V, 8-bit, 166MS/s pipelined ADC was proposed in a 0.18 um CMOS technology. The ADC used operational amplifier sharing architecture to reduce power consumption and achieved maximum DNL of 0.24 LSB, maximum INL of 0.35 LSB, at a power consumption of 38.9mW. When input frequency is 10.4MHz, it achieved an SNDR 45.9dB, SFDR 50dB, and an ENOB of 7.33 bit.", "venue": "ArXiv", "authors": ["Yilei  Li", "Li  Du"], "year": 2017, "n_citations": 3}
{"id": 1058635, "s2_id": "7bc54b4fe635c2d9cf45b5c93e25b3fc0468a7ca", "title": "FusionAccel: A General Re-configurable Deep Learning Inference Accelerator on FPGA for Convolutional Neural Networks", "abstract": "The deep learning accelerator is one of the methods to accelerate deep learning network computations, which is mainly based on convolutional neural network acceleration. To address the fact that concurrent convolutional neural network accelerators are not solely open-source and the exclusiveness of platforms, FusionAccel, a scalable convolutional neural network accelerator hardware architecture with supporting software is proposed. It can adapt to different network structures and can be reconstructed before compilation and reconfigured at runtime. This paper realizes this RTL convolutional neural network accelerator design and functional verifications on a Xilinx Spartan-6 FPGA. The result is identical to that of Caffe-CPU. Since the entire project is based on RTL, it can be migrated to ASIC after replacing some FPGA-specific IPs.", "venue": "ArXiv", "authors": ["Shi  Shi"], "year": 2019, "n_citations": 0}
{"id": 1061678, "s2_id": "6b244f115952696007f81571ad6749027884642d", "title": "Neuron inspired data encoding memristive multi-level memory cell", "abstract": "Mapping neuro-inspired algorithms to sensor backplanes of on-chip hardware require shifting the signal processing from digital to the analog domain, demanding memory technologies beyond conventional CMOS binary storage units. Using memristors for building analog data storage is one of the promising approaches amongst emerging non-volatile memory technologies. Recently, a memristive multi-level memory cell for storing discrete analog values has been developed in which memory system is implemented combining memristors in voltage divider configuration. In given example, the memory cell of 3 sub-cells with a memristor in each was programmed to store ternary bits which overall achieved 10 and 27 discrete voltage levels. However, for further use of proposed memory cell in analog signal processing circuits data encoder is required to generate control voltages for programming memristors to store discrete analog values. In this paper, we present the design and performance analysis of data encoder that generates write pattern signals for 10 level memristive memory.", "venue": "ArXiv", "authors": ["Aidana  Irmanova", "Alex Pappachen James"], "year": 2018, "n_citations": 28}
{"id": 1061730, "s2_id": "c124f9c04bbbe4d9d3ea9e0cad9165bf530a546a", "title": "Synthesizing Compact Hardware for Accelerating Inference from Physical Signals in Sensors", "abstract": "We present dimensional circuit synthesis, a new method for generating digital logic circuits that improve the efficiency of training and inference of machine learning models from sensor data. The hardware accelerators that the method generates are compact enough (a few thousand gates) to allow integration within low-cost miniaturized sensor integrated circuits, right next to the sensor transducer. The method takes as input a description of physical properties of relevant signals in the sensor transduction process and generates as output a Verilog register transfer level (RTL) description for a circuit that computes low-level features that exploit the units of measure of the signals in the system. \nWe implement dimensional circuit synthesis as a backend to the compiler for Newton, a language for describing physical systems. We evaluate the backend implementation and the hardware it generates, on descriptions of 7 physical systems. The results show that our implementation of dimensional circuit synthesis generates circuits of as little as 1662 logic cells / 1239 gates for the systems we evaluate. \nWe synthesize the designs generated by the dimensional circuit synthesis compilation backend for a low-power miniature FPGA targeted by its manufacturer at sensor interface applications. The circuits which the method generated use as little as 27% of the resources of the 2.15x2.5 mm FPGA. We measure the power dissipation of the FPGA's isolated core supply rail and show that, driven with a pseudorandom signal input stream, the synthesized designs use as little as 1.0 mW and no more than 5.8 mW. These results show the feasibility of integrating physics-inspired machine learning methods within low-cost miniaturized sensor integrated circuits, right next to the sensor transducer.", "venue": "ArXiv", "authors": ["Vasileios  Tsoutsouras", "Max  Vigdorchik", "Phillip  Stanley-Marbell"], "year": 2020, "n_citations": 0}
{"id": 1064981, "s2_id": "aacf0490f7978fee0ed4249b25edce847db35e88", "title": "An Open-Source Platform for High-Performance Non-Coherent On-Chip Communication", "abstract": "On-chip communication infrastructure is a central component of modern systems-on-chip (SoCs), and it continues to gain importance as the number of cores, the heterogeneity of components, and the on-chip and off-chip bandwidth continue to grow. Decades of research on on-chip networks enabled cache-coherent shared-memory multiprocessors. However, communication fabrics that meet the needs of heterogeneous many-cores and accelerator-rich SoCs, which are not, or only partially, coherent, are a much less mature research area. \nIn this work, we present a modular, topology-agnostic, high-performance on-chip communication platform. The platform includes components to build and link subnetworks with customizable bandwidth and concurrency properties and adheres to a state-of-the-art, industry-standard protocol. We discuss microarchitectural trade-offs and timing/area characteristics of our modules and show that they can be composed to build high-bandwidth (e.g., 2.5 GHz and 1024 bit data width) end-to-end on-chip communication fabrics (not only network switches but also DMA engines and memory controllers) with high degrees of concurrency. We design and implement a state-of-the-art ML training accelerator, where our communication fabric scales to 1024 cores on a die, providing 32 TB/s cross-sectional bandwidth at only 24 ns round-trip latency between any two cores.", "venue": "IEEE Transactions on Computers", "authors": ["Andreas  Kurth", "Wolfgang  R\u00f6nninger", "Thomas  Benz", "Matheus A. Cavalcante", "Fabian  Schuiki", "Florian  Zaruba", "Luca  Benini"], "year": 2021, "n_citations": 6}
{"id": 1070496, "s2_id": "645b12d38788558b9f92d668521a155b99319c2d", "title": "Voltage Scaling for Partitioned Systolic Array in A Reconfigurable Platform", "abstract": "The exponential emergence of Field Programmable Gate Array (FPGA) has accelerated the research of hardware implementation of Deep Neural Network (DNN). Among all DNN processors, domain specific architectures, such as, Google\u2019s Tensor Processor Unit (TPU) have outperformed conventional GPUs. However, implementation of TPUs in reconfigurable hardware should emphasize energy savings to serve the green computing requirement. Voltage scaling, a popular approach towards energy savings, can be a bit critical in FPGA as it may cause timing failure if not done in an appropriate way. In this work, we present an ultra low power FPGA implementation of a TPU for edge applications. We divide the systolic-array of a TPU into different FPGA partitions, where each partition uses different near threshold (NTC) biasing voltages to run its FPGA cores. The biasing voltage for each partition is roughly calculated by the proposed offline schemes. However, further calibration of biasing voltage is done by the proposed online scheme. Four clustering algorithms based on the slack value of different design paths study the partitioning of FPGA. To overcome the timing failure caused by NTC, the higher slack paths are placed in lower voltage partitions and lower slack paths are placed in higher voltage partitions. The proposed architecture is simulated in Artix\u22127 FPGA using the V ivado design suite and Python tool. The simulation results substantiate the implementation of voltage scaled TPU in FPGAs and also justifies its power efficiency.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Sreetama  Sarkar", "Suman  Sau", "Koushik  Chakraborty", "Sanghamitra  Roy", "Amlan  Chakrabarti"], "year": 2021, "n_citations": 0}
{"id": 1072549, "s2_id": "71dc507208b046b9939d5dd3ea6c357074b7cc89", "title": "Efficient Training Convolutional Neural Networks on Edge Devices with Gradient-pruned Sign-symmetric Feedback Alignment", "abstract": "With the prosperity of mobile devices, the distributed learning approach enabling model training with decentralized data has attracted wide research. However, the lack of training capability for edge devices significantly limits the energy efficiency of distributed learning in real life. This paper describes a novel approach of training DNNs exploiting the redundancy and the weight asymmetry potential of conventional back propagation. We demonstrate that with negligible classification accuracy loss, the proposed approach outperforms the prior arts by 5x in terms of energy efficiency.", "venue": "Lecture Notes in Electrical Engineering", "authors": ["Ziyang  Hong", "C. Patrick Yue"], "year": 2021, "n_citations": 0}
{"id": 1077410, "s2_id": "e3b1f12a240ff79e7e6e53aadb36fc683b157e9b", "title": "How Data Volume Affects Spark Based Data Analytics on a Scale-up Server", "abstract": "Sheer increase in volume of data over the last decade has triggered research in cluster computing frameworks that enable web enterprises to extract big insights from big data. While Apache Spark is gaining popularity for exhibiting superior scale-out performance on the commodity machines, the impact of data volume on the performance of Spark based data analytics in scale-up configuration is not well understood. We present a deep-dive analysis of Spark based applications on a large scale-up server machine. Our analysis reveals that Spark based data analytics are DRAM bound and do not benefit by using more than 12 cores for an executor. By enlarging input data size, application performance degrades significantly due to substantial increase in wait time during I/O operations and garbage collection, despite 10\\% better instruction retirement rate (due to lower L1 cache misses and higher core utilization). We match memory behaviour with the garbage collector to improve performance of applications between 1.6x to 3x.", "venue": "BPOE", "authors": ["Ahsan Javed Awan", "Mats  Brorsson", "Vladimir  Vlassov", "Eduard  Ayguad\u00e9"], "year": 2015, "n_citations": 18}
{"id": 1077711, "s2_id": "2410f1c3a52e449ac968063e93277d536382a64e", "title": "Exploration of Low Numeric Precision Deep Learning Inference Using Intel\u00ae FPGAs", "abstract": "Convolutional neural networks (CNN) have been shown to maintain reasonable classification accuracy when quantized to lower precisions, however, quantizing to sub 8-bit activations and weights can result in classification accuracy falling below an acceptable threshold. Techniques exist for closing the accuracy gap of limited numeric precision networks typically by means of increasing computation. This results in a trade-off between throughput and accuracy and can be tailored for different networks through various combinations of activation and weight data widths. Customizable hardware architectures like FPGAs provide the opportunity for data width specific computation through unique logic configurations leading to highly optimized processing that is unattainable by full precision networks. Specifically, ternary and binary weighted networks offer an efficient method of inference for 2-bit and 1-bit data respectively. Most hardware architectures can take advantage of the memory storage and bandwidth savings that come along with a smaller datapath, but very few architectures can take full advantage of limited numeric precision at the computation level. In this paper, we present a hardware design for FPGAs that takes advantage of the bandwidth, memory, power, and computation savings of limited numerical precision data. We provide insights into the trade-offs between throughput and accuracy for various networks and how they map to our framework. Further, we show how limited numeric precision computation can be efficiently mapped onto FPGAs for both ternary and binary cases. Starting with Arria 10, we show a 2-bit activation and ternary weighted AlexNet running in hardware that achieves 3,700 images per second on the ImageNet dataset with a top-1 accuracy of 0.49. Using a hardware modeler designed for our low numeric precision framework we project performance most notably for a 55.5 TOPS Stratix 10 device running a modified ResNet-34 with only 3.7% accuracy degradation compared with single precision.", "venue": "2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Philip  Colangelo", "Nasibeh  Nasiri", "Eriko  Nurvitadhi", "Asit K. Mishra", "Martin  Margala", "Kevin  Nealis"], "year": 2018, "n_citations": 30}
{"id": 1089516, "s2_id": "04517328d54257ec57ea98432cc62b1a5f1494b5", "title": "Lightning: Striking the Secure Isolation on GPU Clouds with Transient Hardware Faults", "abstract": "GPU clouds have become a popular computing platform because of the cost of owning andmaintaining high-performance computing clusters. Many cloud architectures have also been proposed to ensure a secure execution environment for guest applications by enforcing strong security policies to isolate the untrusted hypervisor from the guest virtual machines (VMs). In this paper, we study the impact of GPU chip\u2019s hardware faults on the security of cloud \"trusted\" execution environment using Deep Neural Network (DNN) as the underlying application. We show that transient hardware faults of GPUs can be generated by exploiting the Dynamic Voltage and Frequency Scaling (DVFS) technology, and these faults may cause computation errors, but they have limited impact on the inference accuracy of DNN due to the robustness and fault-tolerant nature of well-developed DNN models. To take full advantage of these transient hardware faults, we propose the Lightning attack to locate the fault injection targets of DNNs and to control the fault injection precision in terms of timing and position. We conduct experiments on three commodity GPUs to attack four widely-used DNNs. Experimental results show that the proposed attack can reduce the inference accuracy of the models by as high as 78.3% and 64.5% on average. More importantly, 67.9% of the targeted attacks have successfully misled the models to give our desired incorrect inference result. This demonstrates that the secure isolation on GPU clouds is vulnerable against transient hardware faults and the computation results may not be trusted. CCS Concepts: \u2022 Security and privacy\u2192 Hardware attacks and countermeasures.", "venue": "ArXiv", "authors": ["Rihui  Sun", "Pefei  Qiu", "Yongqiang  Lyu", "Donsheng  Wang", "Jiang  Dong", "Gang  Qu"], "year": 2021, "n_citations": 0}
{"id": 1092900, "s2_id": "52856918470f96591955c5af6f6dc9f202c48216", "title": "A GPU Register File using Static Data Compression", "abstract": "GPUs rely on large register files to unlock thread-level parallelism for high throughput. Unfortunately, large register files are power hungry, making it important to seek for new approaches to improve their utilization. This paper introduces a new register file organization for efficient register-packing of narrow integer and floating-point operands designed to leverage on advances in static analysis. We show that the hardware/software co-designed register file organization yields a performance improvement of up to 79%, and 18.6%, on average, at a modest output-quality degradation.", "venue": "ICPP", "authors": ["Alexandra  Angerd", "Erik  Sintorn", "Per  Stenstr\u00f6m"], "year": 2020, "n_citations": 0}
{"id": 1097365, "s2_id": "cb1d2e1c649721cd952962ef2eb4dbc046abd1cf", "title": "Theoretical Engineering and Satellite Comlink of a PTVD-SHAM System", "abstract": "This paper focuses on super helical memory system's design, 'Engineering, Architectural and Satellite Communications' as a theoretical approach of an invention-model to 'store time-data'. The current release entails three concepts: 1- an in-depth theoretical physics engineering of the chip including its, 2- architectural concept based on VLSI methods, and 3- the time-data versus data-time algorithm. The 'Parallel Time Varying & Data Super-helical Access Memory' (PTVD-SHAM), possesses a waterfall effect in its architecture dealing with the process of voltage output-switch into diverse logic and quantum states described as 'Boolean logic & image-logic', respectively. Quantum dot computational methods are explained by utilizing coiled carbon nanotubes (CCNTs) and CNT field effect transistors (CNFETs) in the chip's architecture. Quantum confinement, categorized quantum well substrate, and B-field flux involvements are discussed in theory. Multi-access of coherent sequences of 'qubit addressing' in any magnitude, gained as pre-defined, here e.g., the 'big O notation' asymptotically confined into singularity while possessing a magnitude of 'infinity' for the orientation of array displacement. Gaussian curvature of k (k<0) is debated in aim of specifying the 2D electron gas characteristics, data storage system for defining short and long time cycles for different CCNT diameters where space-time continuum is folded by chance for the particle. Precise pre/post data timing for, e.g., seismic waves before earthquake mantle-reach event occurrence, including time varying self-clocking devices in diverse geographic locations for radar systems is illustrated in the Subsections of the paper. The theoretical fabrication process, electromigration between chip's components is discussed as well.", "venue": "ArXiv", "authors": ["Philip B. Alipour"], "year": 2007, "n_citations": 3}
{"id": 1101971, "s2_id": "66b00348ee1a8e86b2e609d19ff4e033fde965dc", "title": "Logic, Design & Organization of PTVD-SHAM; A Parallel Time Varying & Data Super-helical Access Memory", "abstract": "This paper encompasses a super helical memory system's design, 'Boolean logic & image-logic' as a theoretical concept of an invention-model to 'store time-data' in terms of anticipating the best memory location ever for data/time. A waterfall effect is deemed to assist the process of potential-difference output-switch into diverse logic states in quantum dot computational methods via utilizing coiled carbon nanotubes (CCNTs) and carbon nanotube field effect transistors (CNFETs). A 'quantum confinement' is thus derived for a flow of particles in a categorized quantum well substrate with a normalized capacitance rectifying high B-field flux into electromagnetic induction. Multi-access of coherent sequences of 'qubit addressing' is gained in any magnitude as pre-defined for the orientation of array displacement. Briefly, Gaussian curvature of k (k<0) for greater CCNT diameters, space-time continuum is folded by chance for the particle. This benefits from Maxwell-Lorentz theory in Minkowski's space-time viewpoint alike to crystal oscillators for precise data timing purposes and radar systems e.g., time varying self-clocking devices in diverse geographic locations. This application could also be optional for data depository versus extraction, in the best supercomputer system's locations, autonomously. For best performance in minimizing current limiting mechanisms including electromigration, a multilevel metallization and implant process forming elevated sources/drains for the circuit's staircase pyramidal construction, is discussed accordingly.", "venue": "ArXiv", "authors": ["P. B. Alipour"], "year": 2007, "n_citations": 2}
{"id": 1106876, "s2_id": "c2b9053d3b928a6752844f487feb2d38931bed92", "title": "Ultra-low Energy, High-Performance Dynamic Resistive Threshold Logic", "abstract": "We propose dynamic resistive threshold-logic (DRTL) design based on non-volatile resistive memory. A threshold logic gate (TLG) performs summation of multiple inputs multiplied by a fixed set of weights and compares the sum with a threshold. DRTL employs resistive memory elements to implement the weights and the thresholds, while a compact dynamic CMOS latch is used for the comparison operation. The resulting DRTL gate acts as a low-power, configurable dynamic logic unit and can be used to build fully pipelined, high-performance programmable computing blocks. Multiple stages in such a DRTL design can be connected using energy-efficient low swing programmable interconnect networks based on resistive switches. Owing to memory-based compact logic and interconnect design and highspeed dynamic-pipelined operation, DRTL can achieve more than two orders of magnitude improvement in energy-delay product as compared to look-up table based CMOS FPGA.", "venue": "ArXiv", "authors": ["Mrigank  Sharad", "Deliang  Fan", "Kaushik  Roy"], "year": 2013, "n_citations": 9}
{"id": 1107134, "s2_id": "10ca6fc3a9adf282073defda372355bfd668b31e", "title": "Design and implementation of MPICH2 over InfiniBand with RDMA support", "abstract": "Summary form only given. For several years, MPI has been the de facto standard for writing parallel applications. One of the most popular MPI implementations is MPICH. Its successor, MPICH2, features a completely new design that provides more performance and flexibility. To ensure portability, it has a hierarchical structure based on which porting can be done at different levels. In this paper, we present our experiences in designing and implementing MPICH2 over InfiniBand. Because of its high performance and open standard, InfiniBand is gaining popularity in the area of high-performance computing. Our study focuses on optimizing the performance of MPl-1 functions in MPICH2. One of our objectives is to exploit remote direct memory access (RDMA) in InfiniBand to achieve high performance. We have based our design on the RDMA channel interface provided by MP1CH2, which encapsulates architecture-dependent communication functionalities into a very small set of functions. Starting with a basic design, we apply different optimizations and also propose a zero-copy-based design. We characterize the impact of our optimizations and designs using microbenchmarks. We have also performed an application-level evaluation using the NAS parallel benchmarks. Our optimized MPICH2 implementation achieves 7.6/spl mu/s latency and 857 MB/s bandwidth, which are close to the raw performance of the underlying InfiniBand layer. Our study shows that the RDMA channel interface in MPICH2 provides a simple, yet powerful, abstraction that enables implementations with high performance by exploiting RDMA operations in InfiniBand. To the best of our knowledge, this is the first high-performance design and implementation ofMPICH2 on InfiniBand using RDMA support.", "venue": "18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.", "authors": ["Jiuxing  Liu", "Weihang  Jiang", "Pete  Wyckoff", "Dhabaleswar K. Panda", "David  Ashton", "Darius  Buntinas", "William  Gropp", "Brian R. Toonen"], "year": 2004, "n_citations": 118}
{"id": 1109170, "s2_id": "b6d10594e7758b685cba8821242d8c8bb66d855f", "title": "Symbolic Loop Compilation for Tightly Coupled Processor Arrays", "abstract": "Tightly Coupled Processor Arrays (TCPAs), a class of massively parallel loop accelerators, allow applications to offload computationally expensive loops for improved performance and energy efficiency. To achieve these two goals, executing a loop on a TCPA requires an efficient generation of specific programs as well as other configuration data for each distinct combination of loop bounds and number of available processing elements (PEs). Since both these parameters are generally unknown at compile time\u2014the number of available PEs due to dynamic resource management, and the loop bounds, because they depend on the problem size\u2014both the programs and configuration data must be generated at runtime. However, pure just-in-time compilation is impractical, because mapping a loop program onto a TCPA entails solving multiple NP-complete problems. As a solution, this article proposes a unique mixed static/dynamic approach called symbolic loop compilation. It is shown that at compile time, the NP-complete problems (modulo scheduling, register allocation, and routing) can still be solved to optimality in a symbolic way resulting in a so-called symbolic configuration, a space-efficient intermediate representation parameterized in the loop bounds and number of PEs. This phase is called symbolic mapping. At runtime, for each requested accelerated execution of a loop program with given loop bounds and known number of available PEs, a concrete configuration, including PE programs and configuration data for all other components, is generated from the symbolic configuration according to these parameter values. This phase is called instantiation. We describe both phases in detail and show that instantiation runs in polynomial time with its most complex step, program instantiation, not directly depending on the number of PEs and thus scaling to arbitrary sizes of TCPAs. To validate the efficiency of this mixed static/dynamic compilation approach, we apply symbolic loop compilation to a set of real-world loop programs from several domains, measuring both compilation time and space requirements. Our experiments confirm that a symbolic configuration is a space-efficient representation suited for systems with little memory\u2014in many cases, a symbolic configuration is smaller than even a single concrete configuration instantiated from it\u2014and that the times for the runtime phase of program instantiation and configuration loading are negligible and moreover independent of the size of the available processor array. To give an example, instantiating a configuration for a matrix-matrix multiplication benchmark takes equally long for 4\u00d7 4 and 32\u00d7 32 PEs.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Michael  Witterauf", "Dominik  Walter", "Frank  Hannig", "Jurgen  Teich"], "year": 2021, "n_citations": 2}
{"id": 1113650, "s2_id": "94c7c216c27e4d047bf9868429f7a5b52d4b0ddd", "title": "Multiplierless Approximate 4-point DCT VLSI Architectures for Transform Block Coding", "abstract": "Two multiplierless algorithms are proposed for 4 \u00d7 4 approximate-discrete cosine transform (DCT) for transform coding in digital video. Computational architectures for one-dimensional (1D)/2D realisations are implemented using Xilinx field programmable gate array devices. CMOS synthesis at the 45 nm node indicates real-time operation at 1 GHz yielding 4 \u00d7 4 block rates of 125 MHz at < 120 mW of dynamic power consumption.", "venue": "ArXiv", "authors": ["F\u00e1bio M. Bayer", "Renato J. Cintra", "Arjuna  Madanayake", "Uma Sadhvi Potluri"], "year": 2014, "n_citations": 21}
{"id": 1114099, "s2_id": "e7608aaa2d06d05ff6dd1ce54aaac887c4e869b2", "title": "FPMax: a 106GFLOPS/W at 217GFLOPS/mm2 Single-Precision FPU, and a 43.7GFLOPS/W at 74.6GFLOPS/mm2 Double-Precision FPU, in 28nm UTBB FDSOI", "abstract": "FPMax implements four FPUs optimized for latency or throughput workloads in two precisions, fabricated in 28nm UTBB FDSOI. Each unit's parameters, e.g pipeline stages, booth encoding etc., were optimized to yield 1.42ns latency at 110GLOPS/W (SP) and 1.39ns latency at 36GFLOPS/W (DP). At 100% activity, body-bias control improves the energy efficiency by about 20%; at 10% activity this saving is almost 2x. \nKeywords: FPU, energy efficiency, hardware generator, SOI", "venue": "ArXiv", "authors": ["Jing  Pu", "Sameh  Galal", "Xuan  Yang", "Ofer  Shacham", "Mark  Horowitz"], "year": 2016, "n_citations": 6}
{"id": 1115203, "s2_id": "e5adfeeb911746b09dbfd6a4723e42e56b9aeb4a", "title": "Performance Impact of Memory Channels on Sparse and Irregular Algorithms", "abstract": "Graph processing is typically considered to be a memory-bound rather than compute-bound problem. One common line of thought is that more available memory bandwidth corresponds to better graph processing performance. However, in this work we show that this is not necessarily the case. We demonstrate that the key factor in the utilization of the memory system for graph algorithms is not the raw bandwidth, or even latency of memory requests, but instead is the number of memory channels available to handle small data transfers with low locality. Using several widely used graph frameworks, including Gunrock (on the GPU) and GAPBS & Ligra (for CPUs), we characterize two very distinct memory hierarchies with respect to key graph analytics kernels. Our results show that the differences in peak bandwidths of several of the latest Pascal-generation GPU memory subsystems aren't reflected in the performance of various analytics. Furthermore, our experiments on CPU and Xeon Phi systems show that the number of memory channels utilized can be a decisive factor in performance across several different applications. For CPU systems with smaller thread counts, the memory channels can be underutilized while systems with high thread counts can oversaturate the memory subsystem, which leads to limited performance. Lastly, we model the performance of including more channels with narrower access widths than those found in existing memory subsystems, and we analyze the trade-offs in terms of the two most prominent types of memory accesses found in graph algorithms, streaming and random accesses.", "venue": "2019 IEEE/ACM 9th Workshop on Irregular Applications: Architectures and Algorithms (IA3)", "authors": ["Oded  Green", "James  Fox", "Jeffrey  Young", "Jun  Shirako", "David  Bader"], "year": 2019, "n_citations": 1}
{"id": 1131707, "s2_id": "20c558e5d997c7a2ed3cdd936e71b8698fbb1c33", "title": "Syntroids: Synthesizing a Game for FPGAs using Temporal Logic Specifications", "abstract": "We present Syntroids, a case study for the automatic synthesis of hardware from a temporal logic specification. Syntroids is a space shooter arcade game realized on an FPGA, where the control flow architecture has been completely specified in Temporal Stream Logic (TSL) and implemented using reactive synthesis. TSL is a recently introduced temporal logic that separates control and data. This leads to scalable synthesis, because the cost of the synthesis process is independent of the complexity of the handled data.In this case study, we report on our experience with the TSL-based development of the Syntroids game and on the implementation quality obtained with synthesis in comparison to manual programming. We also discuss solved and open challenges with respect to currently available synthesis tools.", "venue": "2019 Formal Methods in Computer Aided Design (FMCAD)", "authors": ["Gideon  Geier", "Philippe  Heim", "Felix  Klein", "Bernd  Finkbeiner"], "year": 2019, "n_citations": 5}
{"id": 1134550, "s2_id": "e3280773c7b248f6899268b7368bb2e88d85c403", "title": "Dynamic Warp Resizing in High-Performance SIMT", "abstract": "Modern GPUs synchronize threads grouped in a warp at every instruction. These results in improving SIMD efficiency and makes sharing fetch and decode resources possible. The number of threads included in each warp (or warp size) affects divergence, synchronization overhead and the efficiency of memory access coalescing. Small warps reduce the performance penalty associated with branch and memory divergence at the expense of a reduction in memory coalescing. Large warps enhance memory coalescing significantly but also increase branch and memory divergence. Dynamic workload behavior, including branch/memory divergence and coalescing, is an important factor in determining the warp size returning best performance. Optimal warp size can vary from one workload to another or from one program phase to the next. Based on this observation, we propose Dynamic Warp Resizing (DWR). DWR takes innovative microarchitectural steps to adjust warp size during runtime and according to program characteristics. DWR outperforms static warp size decisions, up to 1.7X to 2.28X, while imposing less than 1% area overhead. We investigate various alternative configurations and show that DWR performs better for narrower SIMD and larger caches.", "venue": "ArXiv", "authors": ["Ahmad  Lashgar", "Amirali  Baniasadi", "Ahmad  Khonsari"], "year": 2012, "n_citations": 1}
{"id": 1136420, "s2_id": "ea189ef71832e24f2b3e6986fdec6ba76534bb76", "title": "Open-Source Synthesizable Analog Blocks for High-Speed Link Designs: 20-GS/s 5b ENOB Analog-to-Digital Converter and 5-GHz Phase Interpolator", "abstract": "Using digital standard cells and digital place-and-route (PnR) tools, we created a 20 GS/s, 8-bit analog-to-digital converter (ADC) for use in high-speed serial link applications with an ENOB of 5.6, a DNL of 0.96 LSB, and an INL of 2.39 LSB, which dissipated 175 mW in 0.102 mm2 in a 16nm technology. The design is entirely described by HDL so that it can be ported to other processes with minimal effort and shared as open source.", "venue": "2020 IEEE Symposium on VLSI Circuits", "authors": ["Sung-Jin  Kim", "Zachary  Myers", "Steven  Herbst", "ByongChan  Lim", "Mark  Horowitz"], "year": 2020, "n_citations": 1}
{"id": 1138289, "s2_id": "bdfbdc1bd345b011038109dc763598154ed5c55e", "title": "An Evaluation of WebAssembly and eBPF as Offloading Mechanisms in the Context of Computational Storage", "abstract": "As the volume of data that needs to be processed continues to increase, we also see renewed interests in near-data processing in the form of computational storage, with eBPF (extended Berkeley Packet Filter) being proposed as a vehicle for computation offloading. However, discussions in this regard have so far ignored viable alternatives, and no convincing analysis has been provided. As such, we qualitatively and quantitatively evaluated eBPF against WebAssembly, a seemingly similar technology, in the context of computation offloading. This report presents our findings.", "venue": "ArXiv", "authors": ["Wenjun  Huang", "Marcus  Paradies"], "year": 2021, "n_citations": 0}
{"id": 1138356, "s2_id": "f1966d2bb06a2f5a8b2d84bb5211e9ac1fe66b77", "title": "Modeling Algorithms in SystemC and ACL2", "abstract": "We describe the formal language MASC, based on a subset of SystemC and intended for modeling algorithms to be implemented in hardware. By means of a special-purpose parser, an algorithm coded in SystemC is converted to a MASC model for the purpose of documentation, which in turn is translated to ACL2 for formal verification. The parser also generates a SystemC variant that is suitable as input to a high-level synthesis tool. As an illustration of this methodology, we describe a proof of correctness of a simple 32-bit radix-4 multiplier.", "venue": "ACL2", "authors": ["John W. O'Leary", "David M. Russinoff"], "year": 2014, "n_citations": 5}
{"id": 1151610, "s2_id": "f610c195759696ca6e08e637896a0e6e19d8b805", "title": "An Embedded RISC-V Core with Fast Modular Multiplication", "abstract": "One of the biggest concerns in IoT is privacy and security. Encryption and authentication need big power budgets, which battery-operated IoT end-nodes do not have. Hardware accelerators designed for specific cryptographic operations provide little to no flexibility for future updates. Custom instruction solutions are smaller in area and provide more flexibility for new methods to be implemented. One drawback of custom instructions is that the processor has to wait for the operation to finish. Eventually, the response time of the device to real-time events gets longer. In this work, we propose a processor with an extended custom instruction for modular multiplication, which blocks the processor, typically, two cycles for any size of modular multiplication when used in Partial Execution mode. We adopted embedded and compressed extensions of RISC-V for our proof-of-concept CPU. Our design is benchmarked on recent cryptographic algorithms in the field of elliptic-curve cryptography. Our CPU with 128-bit modular multiplication operates at 136MHz on ASIC and 81MHz on FPGA. It achieves up to 13x speed up on software implementations while reducing overall power consumption by up to 95\\% with 41\\% average area overhead over our base architecture.", "venue": "ArXiv", "authors": ["\u00d6mer Faruk Irmak", "Arda  Yurdakul"], "year": 2020, "n_citations": 1}
{"id": 1153883, "s2_id": "959b029003dfd89a1cad4bf2c89ed456a41baa0a", "title": "A transprecision floating-point cluster for efficient near-sensor data analytics", "abstract": "Recent applications in the domain of near-sensor computing require the adoption of floating-point arithmetic to reconcile high precision results with a wide dynamic range. In this paper, we propose a multi-core computing cluster that leverages the fined-grained tunable principles of transprecision computing to provide support to near-sensor applications at a minimum power budget. Our design - based on the open-source RISC-V architecture - combines parallelization and sub-word vectorization with near-threshold operation, leading to a highly scalable and versatile system. We perform an exhaustive exploration of the design space of the transprecision cluster on a cycle-accurate FPGA emulator, with the aim to identify the most efficient configurations in terms of performance, energy efficiency, and area efficiency. We also provide a full-fledged software stack support, including a parallel runtime and a compilation toolchain, to enable the development of end-to-end applications. We perform an experimental assessment of our design on a set of benchmarks representative of the near-sensor processing domain, complementing the timing results with a post place-&-route analysis of the power consumption. Finally, a comparison with the state-of-the-art shows that our solution outperforms the competitors in energy efficiency, reaching a peak of 97 Gflop/s/W on single-precision scalars and 162 Gflop/s/W on half-precision vectors.", "venue": "ArXiv", "authors": ["Fabio  Montagna", "Stefan  Mach", "Simone  Benatti", "Angelo  Garofalo", "Gianmarco  Ottavi", "Luca  Benini", "Davide  Rossi", "Giuseppe  Tagliavini"], "year": 2020, "n_citations": 3}
{"id": 1157278, "s2_id": "fb7667143989fd1cc3d08affd83fc6248e026f4a", "title": "Optically Connected Memory for Disaggregated Data Centers", "abstract": "Recent advances in integrated photonics enable the implementation of reconfigurable, high-bandwidth, and low energy-per-bit interconnects in next-generation data centers. We propose and evaluate an Optically Connected Memory (OCM) architecture that disaggregates the main memory from the computation nodes in data centers. OCM is based on micro-ring resonators (MRRs), and it does not require any modification to the DRAM memory modules. We calculate energy consumption from real photonic devices and integrate them into a system simulator to evaluate performance. Our results show that (1) OCM is capable of interconnecting four DDR4 memory channels to a computing node using two fibers with 1.07 pJ energy-per-bit consumption and (2) OCM performs up to 5.5x faster than a disaggregated memory with 40G PCIe NIC connectors to computing nodes.", "venue": "2020 IEEE 32nd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)", "authors": ["Jorge  Gonzalez", "Alexander  Gazman", "Maarten  Hattink", "Mauricio G. Palma", "Meisam  Bahadori", "Ruth  Rubio-Noriega", "Lois  Orosa", "Madeleine  Glick", "Onur  Mutlu", "Keren  Bergman", "Rodolfo  Azevedo"], "year": 2020, "n_citations": 1}
{"id": 1158319, "s2_id": "d8bda6644866dc7c7c02f662494a1a88588f13d3", "title": "Unified Characterization Platform for Emerging NVM Technology: Neural Network Application Benchmarking using off-the-Shelf NVM Chips", "abstract": "In this paper, we present a unified FPGA based electrical test-bench for characterizing different emerging NonVolatile Memory (NVM) chips. In particular, we present detailed electrical characterization and benchmarking of multiple commercially available, off-the-shelf, NVM chips viz.: MRAM, FeRAM, CBRAM, and ReRAM. We investigate important NVM parameters such as: (i) current consumption patterns, (ii) endurance, and (iii) error characterization. The proposed FPGA based testbench is then utilized for a Proof-of-Concept (PoC) Neural Network (NN) image classification application. Four emerging NVM chips are benchmarked against standard SRAM and Flash technology for the AI application as active weight memory during inference mode.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Supriya  Chakraborty", "Abhishek  Gupta", "Manan  Suri"], "year": 2020, "n_citations": 0}
{"id": 1159986, "s2_id": "2bfc4fe57d873a7958918b9316a4f5557bfa6e93", "title": "IMPULSE: A 65-nm Digital Compute-in-Memory Macro With Fused Weights and Membrane Potential for Spike-Based Sequential Learning Tasks", "abstract": "The inherent dynamics of the neuron membrane potential in spiking neural networks (SNNs) allows the processing of sequential learning tasks, avoiding the complexity of recurrent neural networks. The highly sparse spike-based computations in such spatiotemporal data can be leveraged for energy efficiency. However, the membrane potential incurs additional memory access bottlenecks in current SNN hardware. To that effect, we propose a 10T-SRAM compute-in-memory (CIM) macro, specifically designed for state-of-the-art SNN inference. It consists of a fused weight (<inline-formula> <tex-math notation=\"LaTeX\">$W_{\\mathrm{ MEM}}$ </tex-math></inline-formula>) and membrane potential (<inline-formula> <tex-math notation=\"LaTeX\">$V_{\\mathrm{ MEM}}$ </tex-math></inline-formula>) memory and inherently exploits sparsity in input spikes leading to ~97.4% reduction in energy-delay product (EDP) at 85% sparsity (typical of SNNs considered in this work) compared to the case of no sparsity. We propose staggered data mapping and reconfigurable peripherals for handling different bit precision requirements of <inline-formula> <tex-math notation=\"LaTeX\">$W_{\\mathrm{ MEM}}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$V_{\\mathrm{ MEM}}$ </tex-math></inline-formula>, while supporting multiple neuron functionalities. The proposed macro was fabricated in 65-nm CMOS technology, achieving energy efficiency of 0.99 TOPS/W at 0.85-V supply and 200-MHz frequency for signed 11-bit operations. We evaluate the SNN for sentiment classification from the IMDB dataset of movie reviews and achieve within ~1% accuracy difference and <inline-formula> <tex-math notation=\"LaTeX\">$\\sim 5\\times $ </tex-math></inline-formula> higher energy efficiency compared to a corresponding long short-term memory network.", "venue": "IEEE Solid-State Circuits Letters", "authors": ["Amogh  Agrawal", "Mustafa  Ali", "Minsuk  Koo", "Nitin  Rathi", "Akhilesh  Jaiswal", "Kaushik  Roy"], "year": 2021, "n_citations": 0}
{"id": 1163806, "s2_id": "2ab663ec364745c6916c54831431470481dfd169", "title": "Enabling Highly Efficient Capsule Networks Processing Through A PIM-Based Architecture Design", "abstract": "In recent years, the CNNs have achieved great successes in the image processing tasks, e.g., image recognition and object detection. Unfortunately, traditional CNN's classification is found to be easily misled by increasingly complex image features due to the usage of pooling operations, hence unable to preserve accurate position and pose information of the objects. To address this challenge, a novel neural network structure called Capsule Network has been proposed, which introduces equivariance through capsules to significantly enhance the learning ability for image segmentation and object detection. Due to its requirement of performing a high volume of matrix operations, CapsNets have been generally accelerated on modern GPU platforms that provide highly optimized software library for common deep learning tasks. However, based on our performance characterization on modern GPUs, CapsNets exhibit low efficiency due to the special program and execution features of their routing procedure, including massive unshareable intermediate variables and intensive synchronizations, which are very difficult to optimize at software level. To address these challenges, we propose a hybrid computing architecture design named PIM-CapsNet. It preserves GPU's on-chip computing capability for accelerating CNN types of layers in CapsNet, while pipelining with an off-chip in-memory acceleration solution that effectively tackles routing procedure's inefficiency by leveraging the processing-in-memory capability of today's 3D stacked memory. Using routing procedure's inherent parallellization feature, our design enables hierarchical improvements on CapsNet inference efficiency through minimizing data movement and maximizing parallel processing in memory. Evaluation results demonstrate that our proposed design can achieve substantial improvement on both performance and energy savings for CapsNet inference, with almost zero accuracy loss. The results also suggest good performance scalability in optimizing the routing procedure with increasing network size.", "venue": "2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Xingyao  Zhang", "Shuaiwen Leon Song", "Chenhao  Xie", "Jing  Wang", "Weigong  Zhang", "Xin  Fu"], "year": 2020, "n_citations": 14}
{"id": 1172345, "s2_id": "de0fe80fc64ab3813446422169070445abb7ba96", "title": "A Novel Method for Scalable VLSI Implementation of Hyperbolic Tangent Function", "abstract": "Hyperbolic tangent and Sigmoid functions are used as non-linear activation units in the artificial and deep neural networks. Since, these networks are computationally expensive, customized accelerators are designed for achieving the required performance at lower cost and power. The activation function and MAC units are the key building blocks of these neural networks. A low complexity and accurate hardware implementation of the activation function is required to meet the performance and area targets of such neural network accelerators. Moreover, a scalable implementation is required as the recent studies show that the DNNs may use different precision in different layers. This paper presents a novel method based on trigonometric expansion properties of the hyperbolic function for hardware implementation which can be easily tuned for different accuracy and precision requirements.", "venue": "ArXiv", "authors": ["Mahesh  Chandra"], "year": 2020, "n_citations": 2}
{"id": 1173312, "s2_id": "be71ec7cc49ac52a7248bda33e099497cd5bdef3", "title": "CUTIE: Beyond PetaOp/s/W Ternary DNN Inference Acceleration with Better-than-Binary Energy Efficiency", "abstract": "We present a 3.1 POp/s/W fully digital hardware accelerator for ternary neural networks. CUTIE, the Completely Unrolled Ternary Inference Engine, focuses on minimizing non-computational energy and switching activity so that dynamic power spent on storing (locally or globally) intermediate results is minimized. This is achieved by 1) a data path architecture completely unrolled in the feature map and filter dimensions to reduce switching activity by favoring silencing over iterative computation and maximizing data re-use, 2) targeting ternary neural networks which, in contrast to binary NNs, allow for sparse weights which reduce switching activity, and 3) introducing an optimized training method for higher sparsity of the filter weights, resulting in a further reduction of the switching activity. Compared with state-of-the-art accelerators, CUTIE achieves greater or equal accuracy while decreasing the overall core inference energy cost by a factor of 4.8x-21x.", "venue": "ArXiv", "authors": ["Moritz  Scherer", "Georg  Rutishauser", "Lukas  Cavigelli", "Luca  Benini"], "year": 2020, "n_citations": 1}
{"id": 1174542, "s2_id": "7df9c79149a99e4e8e3b5a5aefe389e6e1dc8463", "title": "Asynchronous Early Output Block Carry Lookahead Adder with Improved Quality of Results", "abstract": "A new asynchronous early output, relative-timed block carry lookahead adder (BCLA) incorporating redundant carries is proposed. Compared to the best of existing semi-custom asynchronous carry lookahead adders (CLAs) employing delay-insensitive data encoding and following a 4-phase handshaking, the proposed BCLA with redundant carries achieves 14.9% reduction in cycle time and 12.3% reduction in area with no power penalty. A hybrid variant involving a ripple carry adder (RCA) in the least significant stages i.e. BCLA-RCA is also considered that achieves 15.2% reduction in cycle time and 11.2% reduction in area over the best of existing hybrid CLARCA variants without power penalty.", "venue": "2018 IEEE 61st International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["P.  Balasubramanian", "Douglas L. Maskell", "Nikos E. Mastorakis"], "year": 2018, "n_citations": 0}
{"id": 1175944, "s2_id": "febec20576b8fbd0247b1fd89696ad132f50a93b", "title": "A Low-Power 9-bit Pipelined CMOS ADC with Amplifier and Comparator Sharing Technique", "abstract": "This paper describes a pipelined analog-to-digital converter (ADC) employing a power and area efficient architecture. The adjacent stages of a pipeline share operational amplifiers. In order to keep accuracy of the amplifiers in the first stages, they use a partially sharing technique. The feature of the proposed scheme is that it also shares the comparators. The capacitors of the first stages of a pipeline are scaled down along a pipeline for a further reducing the chip area and its power consumption. A 9-bit 20-MSamples/s ADC, intended for use in multi-channel mixed-signal chips, has been fabricated via Europractice in a 180-nm CMOS process from UMC. The prototype ADC shows a spurious-free dynamic range of 58.5 dB at a sample rate of 20 MSamples/s, when a 400 kHz input signal with a swing of 1 dB below full scale is applied. The effective number of bits is 8.0 at the same conditions. ADC occupies an active area of 0.4 mm2 and dissipates 8.6 mW at a 1.8 V supply.", "venue": "ArXiv", "authors": ["Yuri  Bocharov", "Vladimir  Butuzov", "Dmitry  Osipov"], "year": 2012, "n_citations": 2}
{"id": 1179172, "s2_id": "f80f88718ac967fcb3469cfba237a5edec113411", "title": "The Architectural Implications of Distributed Reinforcement Learning on CPU-GPU Systems", "abstract": "With deep reinforcement learning (RL) methods achieving results that exceed human capabilities in games, robotics, and simulated environments, continued scaling of RL training is crucial to its deployment in solving complex real-world problems. However, improving the performance scalability and power efficiency of RL training through understanding the architectural implications of CPU-GPU systems remains an open problem. In this work we investigate and improve the performance and power efficiency of distributed RL training on CPU-GPU systems by approaching the problem not solely from the GPU microarchitecture perspective but following a holistic system-level analysis approach. We quantify the overall hardware utilization on a state-of-the-art distributed RL training framework and empirically identify the bottlenecks caused by GPU microarchitectural, algorithmic, and system-level design choices. We show that the GPU microarchitecture itself is well-balanced for state-of-the-art RL frameworks, but further investigation reveals that the number of actors running the environment interactions and the amount of hardware resources available to them are the primary performance and power efficiency limiters. To this end, we introduce a new system design metric, CPU/GPU ratio, and show how to find the optimal balance between CPU and GPU resources when designing scalable and efficient CPU-GPU systems for RL training.", "venue": "ArXiv", "authors": ["Ahmet  Inci", "Evgeny  Bolotin", "Yaosheng  Fu", "Gal  Dalal", "Shie  Mannor", "David  Nellans", "Diana  Marculescu"], "year": 2020, "n_citations": 1}
{"id": 1182106, "s2_id": "1e37db5ce68189137d0c1388f4c2ffb1ed794cc6", "title": "A Single-Cycle MLP Classifier Using Analog MRAM-based Neurons and Synapses", "abstract": "In this paper, spin-orbit torque (SOT) magnetoresistive random-access memory (MRAM) devices are leveraged to realize sigmoidal neurons and binarized synapses for a single-cycle analog in-memory computing (IMC) architecture. First, an analog SOT-MRAM-based neuron bitcell is proposed which achieves a 12x reduction in power-area-product compared to the previous most power- and area-efficient analog sigmoidal neuron design. Next, proposed neuron and synapse bit cells are used within memory subarrays to form an analog IMC-based multilayer perceptron (MLP) architecture for the MNIST pattern recognition application. The architecture-level results exhibit that our analog IMC architecture achieves at least two and four orders of magnitude performance improvement compared to a mixed-signal analog/digital IMC architecture and a digital GPU implementation, respectively while realizing a comparable classification accuracy.", "venue": "ArXiv", "authors": ["Ramtin  Zand"], "year": 2020, "n_citations": 0}
{"id": 1183357, "s2_id": "93f4fd6d59d6eec910d4b7563186b3cd01271028", "title": "A handy systematic method for data hazards detection in an instruction set of a pipelined microprocessor", "abstract": "It is intended in this document to introduce a handy systematic method for enumerating all possible data dependency cases that could occur between any two instructions that might happen to be processed at the same time at different stages of the pipeline. Given instructions of the instruction set, specific information about operands of each instruction and when an instruction reads or writes data, the method could be used to enumerate all possible data hazard cases and to determine whether forwarding or stalling is suitable for resolving each case.", "venue": "ArXiv", "authors": ["Ahmed M. Mahran"], "year": 2012, "n_citations": 0}
{"id": 1188598, "s2_id": "257788ee8175b5c7c7b1057cad60b453a0fcd3fa", "title": "You Only Spike Once: Improving Energy-Efficient Neuromorphic Inference to ANN-Level Accuracy", "abstract": "In the past decade, advances in Artificial Neural Networks (ANNs) have allowed them to perform extremely well for a wide range of tasks. In fact, they have reached human parity when performing image recognition, for example. Unfortunately, the accuracy of these ANNs comes at the expense of a large number of cache and/or memory accesses and compute operations. Spiking Neural Networks (SNNs), a type of neuromorphic, or brain-inspired network, have recently gained significant interest as power-efficient alternatives to ANNs, because they are sparse, accessing very few weights, and typically only use addition operations instead of the more power-intensive multiply-and-accumulate (MAC) operations. The vast majority of neuromorphic hardware designs support rate-encoded SNNs, where the information is encoded in spike rates. Rate-encoded SNNs could be seen as inefficient as an encoding scheme because it involves the transmission of a large number of spikes. A more efficient encoding scheme, Time-To-First-Spike (TTFS) encoding, encodes information in the relative time of arrival of spikes. While TTFS-encoded SNNs are more efficient than rate-encoded SNNs, they have, up to now, performed poorly in terms of accuracy compared to previous methods. \nHence, in this work, we aim to overcome the limitations of TTFS-encoded neuromorphic systems. To accomplish this, we propose: (1) a novel optimization algorithm for TTFS-encoded SNNs converted from ANNs and (2) a novel hardware accelerator for TTFS-encoded SNNs, with a scalable and low-power design. \nOverall, our work in TTFS encoding and training improves the accuracy of SNNs to achieve state-of-the-art results on MNIST MLPs, while reducing power consumption by 1.29$\\times$ over the state-of-the-art neuromorphic hardware.", "venue": "ArXiv", "authors": ["P  Srivatsa", "Kyle Timothy Ng Chu", "Yaswanth  Tavva", "Jibin  Wu", "Malu  Zhang", "Haizhou  Li", "Trevor E. Carlson"], "year": 2020, "n_citations": 3}
{"id": 1190683, "s2_id": "1901da4a02c0c4bf7f62424c3eecd1f977c3f35a", "title": "Network Function Virtualization based on FPGAs: A Framework for all-Programmable network devices", "abstract": "Network Function Virtualization (NFV) refers to the use of commodity hardware resources as the basic platform to perform specialized network functions as opposed to specialized hardware devices. Currently, NFV is mainly implemented based on general purpose processors, or general purpose network processors. In this paper we propose the use of FPGAs as an ideal platform for NFV that can be used to provide both the flexibility of virtualizations and the high performance of the specialized hardware. We present the early attempts of using FPGAs dynamic reconfiguration in network processing applications to provide flexible network functions and we present the opportunities for an FPGA-based NFV platform.", "venue": "ArXiv", "authors": ["Christoforos  Kachris", "Georgios Ch. Sirakoulis", "Dimitrios  Soudris"], "year": 2014, "n_citations": 24}
{"id": 1194504, "s2_id": "5618974acb02f7b8084123970fc0c0e375a2091a", "title": "CADS: Core-Aware Dynamic Scheduler for Multicore Memory Controllers", "abstract": "Memory controller scheduling is crucial in multicore processors, where DRAM bandwidth is shared. Since increased number of requests from multiple cores of processors becomes a source of bottleneck, scheduling the requests efficiently is necessary to utilize all the computing power these processors offer. However, current multicore processors are using traditional memory controllers, which are designed for single-core processors. They are unable to adapt to changing characteristics of memory workloads that run simultaneously on multiple cores. Existing schedulers may disrupt locality and bank parallelism among data requests coming from different cores. Hence, novel memory controllers that consider and adapt to the memory access characteristics, and share memory resources efficiently and fairly are necessary. We introduce Core-Aware Dynamic Scheduler (CADS) for multicore memory controller. CADS uses Reinforcement Learning (RL) to alter its scheduling strategy dynamically at runtime. Our scheduler utilizes locality among data requests from multiple cores and exploits parallelism in accessing multiple banks of DRAM. CADS is also able to share the DRAM while guaranteeing fairness to all cores accessing memory. Using CADS policy, we achieve 20% better cycles per instruction (CPI) in running memory intensive and compute intensive PARSEC parallel benchmarks simultaneously, and 16% better CPI with SPEC 2006 benchmarks.", "venue": "8th International Conference on Soft Computing, Artificial Intelligence and Applications", "authors": ["Eduardo Olmedo Sanchez", "Xian-He  Sun"], "year": 2019, "n_citations": 1}
{"id": 1197508, "s2_id": "e36c9f2c5c4791cf504760988de33b39a013373f", "title": "Neural Network Model Extraction Attacks in Edge Devices by Hearing Architectural Hints", "abstract": "As neural networks continue their reach into nearly every aspect of software operations, the details of those networks become an increasingly sensitive subject. Even those that deploy neural networks embedded in physical devices may wish to keep the inner working of their designs hidden -- either to protect their intellectual property or as a form of protection from adversarial inputs. The specific problem we address is how, through heavy system stack, given noisy and imperfect memory traces, one might reconstruct the neural network architecture including the set of layers employed, their connectivity, and their respective dimension sizes. Considering both the intra-layer architecture features and the inter-layer temporal association information introduced by the DNN design empirical experience, we draw upon ideas from speech recognition to solve this problem. We show that off-chip memory address traces and PCIe events provide ample information to reconstruct such neural network architectures accurately. We are the first to propose such accurate model extraction techniques and demonstrate an end-to-end attack experimentally in the context of an off-the-shelf Nvidia GPU platform with full system stack. Results show that the proposed techniques achieve a high reverse engineering accuracy and improve the one's ability to conduct targeted adversarial attack with success rate from 14.6\\%$\\sim$25.5\\% (without network architecture knowledge) to 75.9\\% (with extracted network architecture).", "venue": "ASPLOS 2020", "authors": ["Xing  Hu", "Ling  Liang", "Lei  Deng", "Shuangchen  Li", "Xinfeng  Xie", "Yu  Ji", "Yufei  Ding", "Chang  Liu", "Timothy  Sherwood", "Yuan  Xie"], "year": 2020, "n_citations": 23}
{"id": 1197895, "s2_id": "187f51d55e4a33b48fa54330b4317bb3353ef2ef", "title": "ROBIN: A Robust Optical Binary Neural Network Accelerator", "abstract": "Domain specific neural network accelerators have garnered attention because of their improved energy efficiency and inference performance compared to CPUs and GPUs. Such accelerators are thus well suited for resource-constrained embedded systems. However, mapping sophisticated neural network models on these accelerators still entails significant energy and memory consumption, along with high inference time overhead. Binarized neural networks (BNNs), which utilize single-bit weights, represent an efficient way to implement and deploy neural network models on accelerators. In this paper, we present a novel optical-domain BNN accelerator, named ROBIN, which intelligently integrates heterogeneous microring resonator optical devices with complementary capabilities to efficiently implement the key functionalities in BNNs. We perform detailed fabrication-process variation analyses at the optical device level, explore efficient corrective tuning for these devices, and integrate circuit-level optimization to counter thermal variations. As a result, our proposed ROBIN architecture possesses the desirable traits of being robust, energy-efficient, low latency, and high throughput, when executing BNN models. Our analysis shows that ROBIN can outperform the best-known optical BNN accelerators and many electronic accelerators. Specifically, our energy-efficient ROBIN design exhibits energy-per-bit values that are \u223c4 \u00d7 lower than electronic BNN accelerators and \u223c933 \u00d7 lower than a recently proposed photonic BNN accelerator, while a performance-efficient ROBIN design shows \u223c3 \u00d7 and \u223c25 \u00d7 better performance than electronic and photonic BNN accelerators, respectively.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Febin P. Sunny", "Asif  Mirza", "Mahdi  Nikdast", "Sudeep  Pasricha"], "year": 2021, "n_citations": 0}
{"id": 1201229, "s2_id": "1aa171d081ffbac3f782b8b22e21c8cf18f17d5d", "title": "Tree parity machine rekeying architectures", "abstract": "The necessity of securing the communication between hardware components in embedded systems becomes increasingly important with regard to the secrecy of data and particularly its commercial use. We suggest a low-cost (i.e., small logic-area) solution for flexible security levels and short key lifetimes. The basis is an approach for symmetric key exchange using the synchronization of tree parity machines. Fast successive key generation enables a key exchange within a few milliseconds, given realistic communication channels with a limited bandwidth. For demonstration, we evaluate characteristics of a standard-cell ASIC design realization as IP-core in 0.18/spl mu/-technology.", "venue": "IEEE Transactions on Computers", "authors": ["Markus  Volkmer", "Sebastian  Wallner"], "year": 2005, "n_citations": 40}
{"id": 1201765, "s2_id": "9a4cacbc5fa41e6978d610a748963889c680ae4e", "title": "FPGA-Based Near-Memory Acceleration of Modern Data-Intensive Applications", "abstract": "Modern data-intensive applications demand high computational capabilities with strict power constraints. Unfortunately, such applications suffer from a significant waste of both execution cycles and energy in current computing systems due to the costly data movement between the computation units and the memory units. Genome analysis and weather prediction are two examples of such applications. Recent field-programmable gate arrays (FPGAs) couple a reconfigurable fabric with high-bandwidth memory (HBM) to enable more efficient data movement and improve overall performance and energy efficiency. This trend is an example of a paradigm shift to near-memory computing. We leverage such an FPGA with HBM for improving the prealignment filtering step of genome analysis and representative kernels from a weather prediction model. Our evaluation demonstrates large speedups and energy savings over a high-end IBM POWER9 system and a conventional FPGA board with DDR4 memory. We conclude that FPGA-based near-memory computing has the potential to alleviate the data movement bottleneck for modern data-intensive applications.", "venue": "IEEE Micro", "authors": ["Gagandeep  Singh", "Mohammed  Alser", "Damla Senol Cali", "Dionysios  Diamantopoulos", "Juan  G\u00f3mez-Luna", "Henk  Corporaal", "Onur  Mutlu"], "year": 2021, "n_citations": 4}
{"id": 1202169, "s2_id": "7fc7bd4b681f2027abca4cb748294bff9802eb56", "title": "BioSEAL: In-Memory Biological Sequence Alignment Accelerator for Large-Scale Genomic Data", "abstract": "Genome sequences contain hundreds of millions of DNA base pairs. Finding the degree of similarity between two genomes requires executing a compute-intensive dynamic programming algorithm, such as Smith-Waterman. Traditional von Neumann architectures have limited parallelism and cannot provide an efficient solution for large-scale genomic data. Approximate heuristic methods (e.g. BLAST) are commonly used. However, they are suboptimal and still compute-intensive. In this work, we present BioSEAL, a biological sequence alignment accelerator. BioSEAL is a massively parallel non-von Neumann processing-in-memory architecture for large-scale DNA and protein sequence alignment. BioSEAL is based on resistive content addressable memory, capable of energy-efficient and highperformance associative processing. We present an associative processing algorithm for entire database sequence alignment on BioSEAL and compare its performance and power consumption with state-of-art solutions. We show that BioSEAL can achieve up to 57\u00d7 speedup and 156\u00d7 better energy efficiency, compared with existing solutions for genome sequence alignment and protein sequence database search.", "venue": "SYSTOR", "authors": ["Roman  Kaplan", "Leonid  Yavits", "Ran  Ginosar"], "year": 2020, "n_citations": 5}
{"id": 1202749, "s2_id": "b555873cfc3d581d3b235a5beabe2fa62cbef2d8", "title": "Extending Memory Capacity in Consumer Devices with Emerging Non-Volatile Memory: An Experimental Study", "abstract": "The number and diversity of consumer devices are growing rapidly, alongside their target applications' memory consumption. Unfortunately, DRAM scalability is becoming a limiting factor to the available memory capacity in consumer devices. As a potential solution, manufacturers have introduced emerging non-volatile memories (NVMs) into the market, which can be used to increase the memory capacity of consumer devices by augmenting or replacing DRAM. Since entirely replacing DRAM with NVM in consumer devices imposes large system integration and design challenges, recent works propose extending the total main memory space available to applications by using NVM as swap space for DRAM. However, no prior work analyzes the implications of enabling a real NVM-based swap space in real consumer devices. In this work, we provide the first analysis of the impact of extending the main memory space of consumer devices using off-the-shelf NVMs. We extensively examine system performance and energy consumption when the NVM device is used as swap space for DRAM main memory to effectively extend the main memory capacity. For our analyses, we equip real web-based Chromebook computers with the Intel Optane SSD, which is a state-of-the-art low-latency NVM-based SSD device. We compare the performance and energy consumption of interactive workloads running on our Chromebook with NVM-based swap space, where the Intel Optane SSD capacity is used as swap space to extend main memory capacity, against two state-of-the-art systems: (i) a baseline system with double the amount of DRAM than the system with the NVM-based swap space; and (ii) a system where the Intel Optane SSD is naively replaced with a state-of-the-art (yet slower) off-the-shelf NAND-flash-based SSD, which we use as a swap space of equivalent size as the NVM-based swap space.", "venue": "ArXiv", "authors": ["Geraldo F. Oliveira", "Saugata  Ghose", "Juan  G\u00f3mez-Luna", "Amirali  Boroumand", "Alexis  Savery", "Sonny  Rao", "Salman  Qazi", "Gwendal  Grignou", "Rahul  Thakur", "Eric  Shiu", "Onur  Mutlu"], "year": 2021, "n_citations": 1}
{"id": 1204304, "s2_id": "559d7157f89ca87d829ad1f152168c77ec55cfd2", "title": "Power and Execution Time Measurement Methodology for SDF Applications on FPGA-based MPSoCs", "abstract": "Timing and power consumption play an important role in the design of embedded systems. Furthermore, both properties are directly related to the safety requirements of many embedded systems. With regard to availability requirements, power considerations are of uttermost importance for battery operated systems. Validation of timing and power requires observability of these properties. In many cases this is difficult, because the observability is either not possible or requires big extra effort in the system validation process. In this paper, we present a measurement-based approach for the joint timing and power analysis of Synchronous Dataflow (SDF) applications running on a shared memory multiprocessor systems-on-chip (MPSoC) architecture. As a proof-of-concept, we implement an MPSoC system with configurable power and timing measurement interfaces inside a Field Programmable Gate Array (FPGA). Our experiments demonstrate the viability of our approach being able of accurately analyzing different mappings of image processing applications (Sobel filter and JPEG encoder) on an FPGA-based MPSoC implementation.", "venue": "ArXiv", "authors": ["Christof  Schlaak", "Maher  Fakih", "Ralf  Stemmer"], "year": 2017, "n_citations": 9}
{"id": 1206852, "s2_id": "71a05dd352dc7796ecee8fb517d9d3c841a35c74", "title": "SIMDive: Approximate SIMD Soft Multiplier-Divider for FPGAs with Tunable Accuracy", "abstract": "The ever-increasing quest for data-level parallelism and variable precision in ubiquitous multimedia and Deep Neural Network (DNN) applications has motivated the use of Single Instruction, Multiple Data (SIMD) architectures. To alleviate energy as their main resource constraint, approximate computing has re-emerged, albeit mainly specialized for their Application-Specific Integrated Circuit (ASIC) implementations. This paper, presents for the first time, an SIMD architecture based on novel multiplier and divider with tunable accuracy, targeted for Field-Programmable Gate Arrays (FPGAs). The proposed hybrid architecture implements Mitchell's algorithms and supports precision variability from 8 to 32 bits. Experimental results obtained from Vivado, multimedia and DNN applications indicate superiority of proposed architecture (both in SISD and SIMD) over accurate and state-of-the-art approximate counterparts. In particular, the proposed SISD divider outperforms the accurate Intellectual Property (IP) divider provided by Xilinx with 4x higher speed and 4.6x less energy and tolerating only 0.8% error. Moreover, the proposed SIMD multiplier-divider supersede accurate SIMD multiplier by achieving up to 26%, 45%, 36%, and 56% improvement in area, throughput, power, and energy, respectively.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Zahra  Ebrahimi", "Salim  Ullah", "Akash  Kumar"], "year": 2020, "n_citations": 3}
{"id": 1206897, "s2_id": "f35f90aa43aee6e2613dab44c27fe8177d696d0a", "title": "A Flexible Design for Optimization of Hardware Architecture in Distributed Arithmetic based FIR Filters", "abstract": "Abstract \u2014 FIR filters are used in many performance/power critical applications such as mobile communication devices, analogue to digital converters and digital signal processing applications. Design of appropriate FIR filters usually causes the order of filter to be increased. Synthesis and tape-out of high-order FIR filters with reasonable delay, area and power has become an important challenge for hardware designers. In many cases the complexity of high-order filters causes the constraints of the total design could not be satisfied. In this paper efficient hardware architecture is proposed for distributed arithmetic (DA) based FIR filters. The architecture is based on optimized combination of Look-up Tables (LUTs) and compressors. The optimized system level solution is obtained from a set of dynamic-programming optimization algorithms. The experiments show the proposed design reduced the delay cost between 16%-62.5% in comparison of previous optimized structures for DA-based architectures.", "venue": "ArXiv", "authors": ["Fazel  Sharifi", "Saba  Amanollahi", "Mohammad Amin Taherkhani", "Omid  Hashemipour"], "year": 2014, "n_citations": 2}
{"id": 1208414, "s2_id": "9c1df4a48ac5a89e64a69c3030cd3a2f2275e86e", "title": "SpinAPS: A High-Performance Spintronic Accelerator for Probabilistic Spiking Neural Networks", "abstract": "We discuss a high-performance and high-throughput hardware accelerator for probabilistic Spiking Neural Networks (SNNs) based on Generalized Linear Model (GLM) neurons, that uses binary STT-RAM devices as synapses and digital CMOS logic for neurons. The inference accelerator, termed \"SpinAPS\" for Spintronic Accelerator for Probabilistic SNNs, implements a principled direct learning rule for first-to-spike decoding without the need for conversion from pre-trained ANNs. The proposed solution is shown to achieve comparable performance with an equivalent ANN on handwritten digit and human activity recognition benchmarks. The inference engine, SpinAPS, is shown through software emulation tools to achieve 4x performance improvement in terms of GSOPS/W/mm2 when compared to an equivalent SRAM-based design. The architecture leverages probabilistic spiking neural networks that employ first-to-spike decoding rule to make inference decisions at low latencies, achieving 75% of the test performance in as few as 4 algorithmic time steps on the handwritten digit benchmark. The accelerator also exhibits competitive performance with other memristor-based DNN/SNN accelerators and state-of-the-art GPUs.", "venue": "ArXiv", "authors": ["Anakha V Babu", "Osvaldo  Simeone", "Bipin  Rajendran"], "year": 2020, "n_citations": 1}
{"id": 1209976, "s2_id": "f8eabcaf52de8d38c2dc215aa5725c3327ed8333", "title": "Anadaptive embedded architecture for real-time Particle Image Velocimetry algorithms", "abstract": "Particle Image Velocimetry (PIV) is a method of imaging and analysing fields of flows. The PIV techniques compute and display all the motion vectors of the field in a resulting image. Speeds more than thousand vectors per second can be required, each speed being environment-dependent. Essence of this work is to propose an adaptive FPGA-based system for real-time PIV algorithms. The proposed structure is generic so that this unique structure can be re-used for any PIV applications that uses the cross-correlation technique. The major structure remains unchanged, adaptations only concern the number of processing operations. The required speed (corresponding to the number of vector per second) is obtained thanks to a parallel processing strategy. The image processing designer duplicates the processing modules to distribute the operations. The result is a FPGA-based architecture, which is easily adapted to algorithm specifications without any hardware requirement. The design flow is fast and reliable.", "venue": "2006 14th European Signal Processing Conference", "authors": ["Alain  Aubert", "Nathalie  Bochard", "Virginie  Fresse"], "year": 2006, "n_citations": 4}
{"id": 1210928, "s2_id": "1b39ec73ba68c53b0b26ba540970f667765a7518", "title": "Effective Cache Apportioning for Performance Isolation Under Compiler Guidance", "abstract": "With a growing number of cores per socket in modern datacenters where multi-tenancy of a diverse set of applications must be efficiently supported, effective sharing of the last level cache is a very important problem. This is challenging because modern workloads exhibit dynamic phase behaviour their cache requirements & sensitivity vary across different execution points. To tackle this problem, we propose ComCAS, a compiler guided cache apportioning system that provides smart cache allocation to co-executing applications in a system. The front-end of Com-CAS is primarily a compilerframework equipped with learning mechanisms to predict cache requirements, while the backend consists of allocation framework with pro-active scheduler that apportions cache dynamically to co-executing applications. Our system improved average throughput by 21%, with a maximum of 54% while maintaining the worst individual application execution time degradation within 15% to meet SLA requirements.", "venue": "ArXiv", "authors": ["Bodhisatwa  Chatterjee", "Sharjeel  Khan", "Santosh  Pande"], "year": 2021, "n_citations": 0}
{"id": 1214207, "s2_id": "a97d9e4a5e516faaca44598679d9ea14b456fb3b", "title": "Recursive descriptions of polar codes", "abstract": "Polar codes are recursive general concatenated codes. This property motivates a recursive formalization of the known decoding algorithms: Successive Cancellation, Successive Cancellation with Lists and Belief Propagation. Using such description allows an easy development of these algorithms for arbitrary polarizing kernels. Hardware architectures for these decoding algorithms are also described in a recursive way, both for Arikan's standard polar codes and for arbitrary polarizing kernels.", "venue": "Adv. Math. Commun.", "authors": ["Noam  Presman", "Simon  Litsyn"], "year": 2017, "n_citations": 4}
{"id": 1214441, "s2_id": "3ee38e39d77a6d885b30946b3715462053bf4594", "title": "Improving Dependability of Neuromorphic Computing With Non-Volatile Memory", "abstract": "As process technology continues to scale aggressively, circuit aging in a neuromorphic hardware due to negative bias temperature instability (NBTI) and time-dependent dielectric breakdown (TDDB) is becoming a critical reliability issue and is expected to proliferate when using non-volatile memory (NVM) for synaptic storage. This is because NVM devices require high voltages and currents to access their synaptic weights, which further accelerate the circuit aging in neuromorphic hardware. Current methods for qualifying reliability are overly conservative, since they estimate circuit aging considering worst-case operating conditions and unnecessarily constrain performance. This paper proposes RENEU, a reliability-oriented approach to map machine learning applications to neuromorphic hardware, with the aim of improving system-wide reliability, without compromising key performance metrics such as execution time of these applications on the hardware. Fundamental to RENEU is a novel formulation of the aging of CMOS-based circuits in a neuromorphic hardware considering different failure mechanisms. Using this formulation, RENEU develops a system- wide reliability model which can be used inside a design-space exploration framework involving the mapping of neurons and synapses to the hardware. To this end, RENEU uses an instance of Particle Swarm Optimization (PSO) to generate mappings that are Pareto-optimal in terms of performance and reliability. We evaluate RENEU using different machine learning applications on a state-of-the-art neuromorphic hardware with NVM synapses. Our results demonstrate an average 38% reduction in circuit aging, leading to an average 18% improvement in the lifetime of the hardware compared to current practices. RENEU only introduces a marginal performance overhead of 5% compared to a performance-oriented state-of-the-art.", "venue": "2020 16th European Dependable Computing Conference (EDCC)", "authors": ["Shihao  Song", "Anup  Das", "Nagarajan  Kandasamy"], "year": 2020, "n_citations": 18}
{"id": 1214535, "s2_id": "0bb47eedf92b2fd3fedb34c5526aac7ffe92367f", "title": "Hardware-aware Design of Multiplierless Second-Order IIR Filters with Minimum Adders", "abstract": "In this work we optimally solve the problem of multiplierless design of second-order Infinite Impulse Response filters with minimum number of adders. Given a frequency specification, we design a stable direct form filter with hardwareaware fixed-point coefficients that yielding minimal number of adders when replacing all the multiplications by bit shifts and additions. The coefficient design, quantization and implementation, typically conducted independently, are now gathered into one global optimization problem, modeled through integer linear programming and efficiently solved using generic solvers. We guarantee the frequency-domain specifications and stability, which together with optimal number of adders will significantly simplify design-space exploration for filter designers. The optimal filters are implemented within the FloPoCo IP core generator and synthesized for Field Programmable Gate Arrays. With respect to state-of-the-art three-step filter design methods, our one-step design approach achieves, on average, 42% reduction in number of lookup tables and 21% improvement in delay.", "venue": "ArXiv", "authors": ["R\u00e9mi  Garcia", "Anastasia  Volkova", "Martin  Kumm", "Alexandre  Goldsztejn", "Jonas  K\u00fchle"], "year": 2021, "n_citations": 0}
{"id": 1230699, "s2_id": "a4c1fedaefad0fb6fb971d4434b6a186f839b8f9", "title": "Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications", "abstract": "Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.", "venue": "EMNLP", "authors": ["Matthew  Khoury", "Rumen  Dangovski", "Longwu  Ou", "Preslav  Nakov", "Yichen  Shen", "Li  Jing"], "year": 2020, "n_citations": 0}
{"id": 1232504, "s2_id": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f", "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch", "abstract": "Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.", "venue": "ICLR", "authors": ["Aojun  Zhou", "Yukun  Ma", "Junnan  Zhu", "Jianbo  Liu", "Zhijie  Zhang", "Kun  Yuan", "Wenxiu  Sun", "Hongsheng  Li"], "year": 2021, "n_citations": 20}
{"id": 1233816, "s2_id": "95c352d421332ba8e28199cd3c853dde621b2ab4", "title": "Novel BCD adders and their reversible logic implementation for IEEE 754r format", "abstract": "IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and a major enhancement to the standard is the addition of decimal format. This paper proposes two novel BCD adders called carry skip and carry look-ahead BCD adders respectively. Furthermore, in the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. It is not possible to realize quantum computing without reversible logic. Thus, this paper also paper provides the reversible logic implementation of the conventional BCD adder as the well as the proposed carry skip BCD adder using a recently proposed TSG gate. Furthermore, a new reversible gate called TS-3 is also being proposed and it has been shown that the proposed reversible logic implementation of the BCD adders is much better compared to recently proposed one, in terms of number of reversible gates used and garbage outputs produced. The reversible BCD circuits designed and proposed here form the basis of the decimal ALU of a primitive quantum CPU.", "venue": "19th International Conference on VLSI Design held jointly with 5th International Conference on Embedded Systems Design (VLSID'06)", "authors": ["Himanshu  Thapliyal", "Saurabh  Kotiyal", "M. B. Srinivas"], "year": 2006, "n_citations": 82}
{"id": 1234605, "s2_id": "1e4d081c6fa2103ccd0b9d977d98dffaff3a6f3c", "title": "Computer Architecture with Associative Processor Replacing Last-Level Cache and SIMD Accelerator", "abstract": "This study presents a computer architecture, where a last-level cache and a SIMD accelerator are replaced by an associative processor. Associative processor combines data storage and data processing, and functions as a massively parallel SIMD processor and a memory at the same time. An analytic performance model of this computer architecture is introduced. Comparative analysis supported by cycle-accurate simulation and emulation shows that this architecture may outperform a conventional computer architecture comprising a SIMD coprocessor and a shared last-level cache while consuming less power.", "venue": "IEEE Transactions on Computers", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar"], "year": 2015, "n_citations": 35}
{"id": 1235653, "s2_id": "df6ff98e2879440c9dff6943808817c342f5e610", "title": "CLAASIC: a Cortex-Inspired Hardware Accelerator", "abstract": "This work explores the feasibility of specialized hardware implementing the Cortical Learning Algorithm (CLA) in order to fully exploit its inherent advantages. This algorithm, which is inspired in the current understanding of the mammalian neo-cortex, is the basis of the Hierarchical Temporal Memory (HTM). In contrast to other machine learning (ML) approaches, the structure is not application dependent and relies on fully unsupervised continuous learning. We hypothesize that a hardware implementation will be able not only to extend the already practical uses of these ideas to broader scenarios but also to exploit the hardware-friendly CLA characteristics. The architecture proposed will enable an unfeasible scalability for software solutions and will fully capitalize on one of the many CLA advantages: low computational requirements and reduced storage utilization. Compared to a state-of-the-art CLA software implementation it could be possible to improve by 4 orders of magnitude in performance and up to 8 orders of magnitude in energy efficiency. We propose to use a packet-switched network to tackle this. The paper addresses the fundamental issues of such an approach, proposing solutions to achieve scalable solutions. We will analyze cost and performance when using well-known architecture techniques and tools. The results obtained suggest that even with CMOS technology, under constrained cost, it might be possible to implement a large-scale system. We found that the proposed solutions enable a saving of 90% of the original communication costs running either synthetic or realistic workloads.", "venue": "J. Parallel Distributed Comput.", "authors": ["Valentin  Puente", "Jos\u00e9-\u00c1ngel  Gregorio"], "year": 2019, "n_citations": 2}
{"id": 1240048, "s2_id": "21809bad9cbacfef2f30822753d483a829f541ba", "title": "Using Silent Writes in Low-Power Traffic-Aware ECC", "abstract": "Using Error Detection Code (EDC) and Error Correction Code (ECC) is a noteworthy way to increase cache memories robustness against soft errors. EDC enables detecting errors in cache memory while ECC is used to correct erroneous cache blocks. \n \nECCs are often costly as they impose considerable area and energy overhead on cache memory. Reducing this overhead has been the subject of many studies. In particular, a previous study has suggested mapping ECC to the main memory at the expense of high cache traffic and energy. A major source of this excessive traffic and energy is the high frequency of cache writes. In this work, we show that a significant portion of cache writes are silent, i.e., they write the same data already existing. We build on this observation and introduce Trafficaware ECC (or simply TCC). TCC detects silent writes by an efficient mechanism. Once such writes are detected updating their ECC is avoided effectively reducing L2 cache traffic and access frequency. \n \nUsing our solution, we reduce L2 cache access frequency by 8% while maintaining performance. We reduce L2 cache dynamic and overall cache energy by up to 32% and 8%, respectively. Furthermore, TCC reduces L2 cache miss rate by 3%.", "venue": "PATMOS", "authors": ["Mostafa  Kishani", "Amirali  Baniasadi", "Hossein  Pedram"], "year": 2011, "n_citations": 1}
{"id": 1240868, "s2_id": "2a7bacbbc2a98a4bcd4a98508debe49e1d7bfe71", "title": "Optimizing Design Verification using Machine Learning: Doing better than Random", "abstract": "As integrated circuits have become progressively more complex, constrained random stimulus has become ubiquitous as a means of stimulating a designs functionality and ensuring it fully meets expectations. In theory, random stimulus allows all possible combinations to be exercised given enough time, but in practice with highly complex designs a purely random approach will have difficulty in exercising all possible combinations in a timely fashion. As a result it is often necessary to steer the Design Verification (DV) environment to generate hard to hit combinations. The resulting constrained-random approach is powerful but often relies on extensive human expertise to guide the DV environment in order to fully exercise the design. As designs become more complex, the guidance aspect becomes progressively more challenging and time consuming often resulting in design schedules in which the verification time to hit all possible design coverage points is the dominant schedule limitation. This paper describes an approach which leverages existing constrained-random DV environment tools but which further enhances them using supervised learning and reinforcement learning techniques. This approach provides better than random results in a highly automated fashion thereby ensuring DV objectives of full design coverage can be achieved on an accelerated timescale and with fewer resources. \nTwo hardware verification examples are presented, one of a Cache Controller design and one using the open-source RISCV-Ariane design and Google's RISCV Random Instruction Generator. We demonstrate that a machine-learning based approach can perform significantly better on functional coverage and reaching complex hard-to-hit states than a random or constrained-random approach.", "venue": "ArXiv", "authors": ["William  Hughes", "Sandeep  Srinivasan", "Rohit  Suvarna", "Maithilee  Kulkarni"], "year": 2019, "n_citations": 1}
{"id": 1250352, "s2_id": "2dc40065e94bfabfe66317c9a46640c294ed93ac", "title": "Copycat: A High Precision Real Time NAND Simulator", "abstract": "In this paper, we describe the design and implementation of a high precision real time NAND simulator called Copycat that runs on a commodity multi-core desktop environment. This NAND simulator facilitates the development of embedded flash memory management software such as the flash translation layer (FTL). The simulator also allows a comprehensive fault injection for testing the reliability of the FTL. Compared against a real FPGA implementation, the simulator's response time deviation is under 0.28% on average, with a maximum of 10.12%.", "venue": "ArXiv", "authors": ["Juyong  Shin", "Jongbo  Bae", "Ansu  Na", "Sang Lyul Min"], "year": 2016, "n_citations": 0}
{"id": 1250695, "s2_id": "7f6815d717b9c26cd934e22d303c15194558deed", "title": "Resistive Threshold Logic", "abstract": "We report a resistance-based threshold logic family useful for mimicking brain-like large variable logic functions in VLSI. A universal boolean logic cell based on an analog resistive divider and threshold logic circuit is presented. The resistive divider is implemented using memristors, and provides output voltage as a summation of weighted product of input voltages. The output of the resistive divider is converted into a binary value by a threshold operation implemented by CMOS inverter and/or Opamp. A universal cell structure is presented to decrease the overall implementation complexity and number of components. When the number of input variables becomes very high, the proposed cell offers advantages of smaller area and design simplicity in comparison with CMOS-based logic circuits.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Alex Pappachen James", "Linu Rose V. J. Francis", "Dinesh Sasi Kumar"], "year": 2014, "n_citations": 32}
{"id": 1254163, "s2_id": "b656e5330190aa1b5b228c00861b5df7a7459e39", "title": "Open-Source Verification with Chisel and Scala", "abstract": "Performance increase with general-purpose processors has come to a halt. We can no longer depend on Moore\u2019s Law to increase computing performance. The only way to achieve higher performance or lower energy consumption is by building domain-specific hardware accelerators. To efficiently design and verify those domain-specific accelerators, we need agile hardware development. One of the main obstacles when proposing such a modern method is the lack of modern tools to attack it. To be able to verify a design in such a time-constrained development method, one needs to have efficient tools both for design and verification. This paper thus proposes ChiselVerify, an open-source tool for verifying circuits described in any Hardware Description Language. It builds on top of the Chisel hardware construction language and uses Scala to drive the verification using a testing strategy inspired by the Universal Verification Methodology (UVM) and adapted for designs described in Chisel. ChiselVerify is created based on three key ideas. First, our solution highly increases the productivity of the verification engineer, by allowing hardware testing to be done in a modern high-level programming environment. Second, the framework functions with any hardware description language thanks to the flexibility of Chisel blackboxes. Finally, the solution is well integrated into the existing Chisel universe, making it an extension of currently existing testing libraries. We implement ChiselVerify in a way inspired by the functionalities found in SystemVerilog. This allows one to use functional coverage, constrained-random verification, bus functional models, transaction-level modeling and much more during the verification process of a design in a contemporary high-level programming ecosystem.", "venue": "ArXiv", "authors": ["Andrew  Dobis", "Tjark  Petersen", "Kasper Juul Hesse Rasmussen", "Enrico  Tolotto", "Hans Jakob Damsgaard", "Simon Thye Andersen", "Richard  Lin", "Martin  Schoeberl"], "year": 2021, "n_citations": 1}
{"id": 1254219, "s2_id": "c7e8394118d83bc1bdd83b9b41b78e30a08485d4", "title": "Efficient Uncertainty Modeling for System Design via Mixed Integer Programming", "abstract": "The post-Moore era casts a shadow of uncertainty on many aspects of computer system design. Managing that uncertainty requires new algorithmic tools to make quantitative assessments. While prior uncertainty quantification methods, such as generalized polynomial chaos (gPC), show how to work precisely under the uncertainty inherent to physical devices, these approaches focus solely on variables from a continuous domain. However, as one moves up the system stack to the architecture level many parameters are constrained to a discrete (integer) domain. This paper proposes an efficient and accurate uncertainty modeling technique, named mixed generalized polynomial chaos (M-gPC), for architectural uncertainty analysis. The M-gPC technique extends the generalized polynomial chaos (gPC) theory originally developed in the uncertainty quantification community, such that it can efficiently handle the mixed-type (i.e., both continuous and discrete) uncertainties in computer architecture design. Specifically, we employ some stochastic basis functions to capture the architecture-level impact caused by uncertain parameters in a simulator. We also develop a novel mixed-integer programming method to select a small number of uncertain parameter samples for detailed simulations. With a few highly informative simulation samples, an accurate surrogate model is constructed in place of cycle-level simulators for various architectural uncertainty analysis. In the chip-multiprocessor (CMP) model, we are able to estimate the propagated uncertainties with only 95 samples whereas Monte Carlo requires $5\\times 10^{4}$ samples to achieve the similar accuracy. We also demonstrate the efficiency and effectiveness of our method on a detailed DRAM subsystem.", "venue": "2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Zichang  He", "Weilong  Cui", "Chunfeng  Cui", "Timothy  Sherwood", "Zheng  Zhang"], "year": 2019, "n_citations": 6}
{"id": 1260037, "s2_id": "2c3e336e1c22635cd739e070cf50c8fc558e11ea", "title": "Real time ridge orientation estimation for fingerprint images", "abstract": "Fingerprint verification is an important bio-metric technique for personal identification. Most of the automatic verification systems are based on matching of fingerprint minutiae. Extraction of minutiae is an essential process which requires estimation of orientation of the lines in an image. Most of the existing methods involve intense mathematical computations and hence are performed through software means. In this paper a hardware scheme to perform real time orientation estimation is presented which is based on pipelined architecture. Synthesized circuits proved the functionality and accuracy of the suggested method.", "venue": "ArXiv", "authors": ["Eman  Alibeigi", "Shadrokh  Samavi", "Shahram  Shirani", "Zahra  Rahmani"], "year": 2017, "n_citations": 1}
{"id": 1261206, "s2_id": "a1f547705b776e90020ebdcf819710bb3328ae75", "title": "Energy-aware routing for e-textile applications", "abstract": "As the scale of electronic devices shrinks, \"electronic textiles\" (e-textiles) will make possible a wide variety of novel applications which are currently infeasible. Due to the wearability concerns, low-power techniques are critical for e-textile applications. In this paper, we address the issue of energy-aware routing for e-textile platforms and propose an efficient algorithm to solve it. The platform we consider consists of dedicated components for e-textiles, including computational modules, dedicated transmission lines and thin-film batteries on fiber substrates. Furthermore, we derive an analytical upper bound for the achievable number of jobs completed over all possible routing strategies. From a practical standpoint, for the advanced encryption standard (AES) cipher, the routing technique we propose achieves about fifty percent of this analytical upper bound. Moreover, compared to the non-energy-aware counterpart, our routing technique increases the number of encryption jobs completed by one order of magnitude.", "venue": "Design, Automation and Test in Europe", "authors": ["Jung-Chun  Kao", "Radu  Marculescu"], "year": 2005, "n_citations": 9}
{"id": 1264798, "s2_id": "09c2206da25b42a2ae8f7b1d0c1be4e4bcb44836", "title": "Physical Time-Varying Transfer Functions as Generic Low-Overhead Power-SCA Countermeasure", "abstract": "Mathematically-secure cryptographic algorithms leak significant side channel information through their power supplies when implemented on a physical platform. These side channel leakages can be exploited by an attacker to extract the secret key of an embedded device. The existing state-of-the-art countermeasures mainly focus on the power balancing, gate-level masking, or signal-to-noise (SNR) reduction using noise injection and signature attenuation, all of which suffer either from the limitations of high power/area overheads, performance degradation or are not synthesizable. In this article, we propose a generic low-overhead digital-friendly power SCA countermeasure utilizing physical Time-Varying Transfer Functions (TVTF) by randomly shuffling distributed switched capacitors to significantly obfuscate the traces in the time domain. System-level simulation results of the TVTF-AES implemented in TSMC 65nm CMOS technology show > 4000x MTD improvement over the unprotected implementation with nearly 1.25x power and 1.2x area overheads, and without any performance degradation.", "venue": "IACR Cryptol. ePrint Arch.", "authors": ["Archisman  Ghosh", "Debayan  Das", "Shreyas  Sen"], "year": 2020, "n_citations": 1}
{"id": 1265493, "s2_id": "056440d590e817dc1ca143d36b32740960ac53da", "title": "SOC testing methodology and practice", "abstract": "On a commercial digital still camera (DSC) controller chip, we practice a novel SOC test integration platform, solving real problems in test scheduling, test IO reduction, timing of functional test, scan IO sharing, embedded memory built-in self-test (BIST), etc. The chip has been fabricated and tested successfully by our approach. Test results prove that short test integration cost, short test time, and small area overhead can be achieved. To support SOC testing, a memory BIST compiler and an SOC testing integration system have been developed.", "venue": "Design, Automation and Test in Europe", "authors": ["Cheng-Wen  Wu"], "year": 2005, "n_citations": 8}
{"id": 1267174, "s2_id": "f65b1ab45ad48608309fea614cd4618fa3c734f3", "title": "Low-Cost Floating-Point Processing in ReRAM for Scientific Computing", "abstract": "We propose ReFloat, a principled approach for low-cost floating-point processing in ReRAM. The exponent offsets based on a base are stored by a flexible and fine-grained floating-point number representation. The key motivation is that, while the number of exponent bits must be reduced due to the exponential relation to the computation latency and hardware cost, the convergence still requires sufficient accuracy for exponents. Our design reconciles the conflicting goals by storing the exponent offsets from a common base among matrix values in a block, which is the granularity of computation in ReRAM. Due to the value locality, the differences among the exponents in a block are small, thus the offsets require much less number of bits to represent exponents. In essence, ReFloat enables the principled local fine-tuning of floating-point representation. Based on the idea, we define a flexible ReFloat format that specifies matrix block size, and the number of bits for exponent and fraction. To determine the base for each block, we propose an optimization method that minimizes the difference between the exponents of the original matrix block and the converted block. We develop the conversion scheme from default double-precision floating-point format to ReFloat format, the computation procedure, and the low-cost floating-point processing architecture in ReRAM.", "venue": "ArXiv", "authors": ["Linghao  Song", "Fan  Chen", "Xuehai  Qian", "Hai  Li", "Yiran  Chen"], "year": 2020, "n_citations": 0}
{"id": 1268808, "s2_id": "21567de3dcafb030ccdf988022022a6ca0636567", "title": "Residual-Based Detections and Unified Architecture for Massive MIMO Uplink", "abstract": "Massive multiple-input multiple-output (M-MIMO) technique brings better energy efficiency and coverage but higher computational complexity than small-scale MIMO. For linear detections such as minimum mean square error (MMSE), prohibitive complexity lies in solving large-scale linear equations. For a better trade-off between bit-error-rate (BER) performance and computational complexity, iterative linear algorithms like conjugate gradient (CG) have been applied and have shown their feasibility in recent years. In this paper, residual-based detection (RBD) algorithms are proposed for M-MIMO detection, including minimal residual (MINRES) algorithm, generalized minimal residual (GMRES) algorithm, and conjugate residual (CR) algorithm. RBD algorithms focus on the minimization of residual norm per iteration, whereas most existing algorithms focus on the approximation of exact signal. Numerical results have shown that, for 64-QAM 128 \u00d7 8 MIMO, RBD algorithms are only 0.13 dB away from the exact matrix inversion method when BER=\u200910\u2212\u20094. Stability of RBD algorithms has also been verified in various correlation conditions. Complexity comparison has shown that, CR algorithm require 87% less complexity than the traditional method for 128 \u00d7 60 MIMO. The unified hardware architecture is proposed with flexibility, which guarantees a low-complexity implementation for a family of RBD M-MIMO detectors.", "venue": "J. Signal Process. Syst.", "authors": ["Chuan  Zhang", "Yufeng  Yang", "Shunqing  Zhang", "Zaichen  Zhang", "Xiaohu  You"], "year": 2019, "n_citations": 4}
{"id": 1274574, "s2_id": "49855d84e8a9ae1dea13157cd34be430055dd589", "title": "Neuromorphic Algorithm-hardware Codesign for Temporal Pattern Learning", "abstract": "Neuromorphic computing and spiking neural networks (SNN) mimic the behavior of biological systems and have drawn interest for their potential to perform cognitive tasks with high energy efficiency. However, some factors such as temporal dynamics and spike timings prove critical for information processing but are often ignored by existing works, limiting the performance and applications of neuromorphic computing. On one hand, due to the lack of effective SNN training algorithms, it is difficult to utilize the temporal neural dynamics. Many existing algorithms still treat neuron activation statistically. On the other hand, utilizing temporal neural dynamics also poses challenges to hardware design. Synapses exhibit temporal dynamics, serving as memory units that hold historical information, but are often simplified as a connection with weight. Most current models integrate synaptic activations in some storage medium to represent membrane potential and institute a hard reset of membrane potential after the neuron emits a spike. This is done for its simplicity in hardware, requiring only a \u201cclear\u201d signal to wipe the storage medium, but destroys temporal information stored in the neuron.In this work, we derive an efficient training algorithm for Leaky Integrate and Fire neurons, which is capable of training a SNN to learn complex spatial temporal patterns. We achieved competitive accuracy on two complex datasets. We also demonstrate the advantage of our model by a novel temporal pattern association task. Codesigned with this algorithm, we have developed a CMOS circuit implementation for a memristor-based network of neuron and synapses which retains critical neural dynamics with reduced complexity. This circuit implementation of the neuron model is simulated to demonstrate its ability to react to temporal spiking patterns with an adaptive threshold.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Haowen  Fang", "Brady  Taylor", "Ziru  Li", "Zaidao  Mei", "Hai  Li", "Qinru  Qiu"], "year": 2021, "n_citations": 0}
{"id": 1275364, "s2_id": "09d5cfa2a1090e8aa4ba0c69d03fcbadf86b2dff", "title": "Stack up your chips: Betting on 3D integration to augment Moore's Law scaling", "abstract": "3D integration, i.e., stacking of integrated circuit layers using parallel or sequential processing is gaining rapid industry adoption with the slowdown of Moore's law scaling. 3D stacking promises potential gains in performance, power and cost but the actual magnitude of gains varies depending on end-application, technology choices and design. In this talk, we will discuss some key challenges associated with 3D design and how design-for-3D will require us to break traditional silos of micro-architecture, circuit/physical design and manufacturing technology to work across abstractions to enable the gains promised by 3D technologies.", "venue": "ArXiv", "authors": ["Saurabh  Sinha", "Xiaoqing  Xu", "Mudit  Bhargava", "Shidhartha  Das", "Brian  Cline", "Greg  Yeric"], "year": 2020, "n_citations": 2}
{"id": 1276787, "s2_id": "073e1280b0487134eb2d4e9cdd1c69c0a7088bd1", "title": "Electromagnetic fault injection against a System-on-Chip, toward new micro-architectural fault models", "abstract": "Electromagnetic fault injection (EMFI) is a well known technique used to disturb the behaviour of a chip for weakening its security. These attacks are mostly done on simple microcontrollers. On these targets, the fault effects are relatively simple and understood. Exploiting EMFI on modern system-on-chips (SoCs), the fast and complex chips ubiquitous today, requires to understand the impact of such faults. In this paper, we propose an experimental setup and a forensic process to create exploitable faults and assess their impact on the SoC micro-architecture. On our targeted SoC (a BCM2837), the observed behaviours are radically different to what were obtained with state-of-the-art fault injection attacks on microcontrollers. SoC subsystems (L1 caches, L2 cache, memory management unit (MMU)) can be individually targeted leading to new fault models. We also highlight the differences in the fault impact with and without an operating system (OS). This shows the importance of the software layers in the exploitation of a fault. With this work, we demonstrate that the complexity and the speed of SoCs do not protect them against hardware fault attacks. To conclude our work, we introduce countermeasures to protect the SoC caches and MMU against EMFI attacks based on the disclosed faults effects.", "venue": "ArXiv", "authors": ["Thomas  Trouchkine", "S'ebanjila Kevin Bukasa", "Mathieu  Escouteloup", "Ronan  Lashermes", "Guillaume  Bouffard"], "year": 2019, "n_citations": 1}
{"id": 1277631, "s2_id": "e601d7168b234055d1fb01a64bc51e0ed0bde663", "title": "SMART Paths for Latency Reduction in ReRAM Processing-In-Memory Architecture for CNN Inference", "abstract": "This research work proposes a design of an analog ReRAM-based PIM (processing-in-memory) architecture for fast and efficient CNN (convolutional neural network) inference. For the overall architecture, we use the basic hardware hierarchy such as node, tile, core, and subarray. On the top of that, we design intra-layer pipelining, inter-layer pipelining, and batch pipelining to exploit parallelism in the architecture and increase overall throughput for the inference of an input image stream. We also optimize the performance of the NoC (network-on-chip) routers by decreasing hop counts using SMART (single-cycle multi-hop asynchronous repeated traversal) flow control. Finally, we experiment with weight replications for different CNN layers in VGG (A-E) for large-scale data set ImageNet. In our simulation, we achieve 40.4027 TOPS (tera-operations per second) for the best-case performance, which corresponds to over 1029 FPS (frames per second). We also achieve 3.5914 TOPS/W (tera-operaions per second per watt) for the best-case energy efficiency. In addition, the architecture with aggressive pipelining and weight replications can achieve 14X speedup compared to the baseline architecture with basic pipelining, and SMART flow control achieves 1.08X speedup in this architecture compared to the baseline. Last but not least, we also evaluate the performance of SMART flow control using synthetic traffic.", "venue": "ArXiv", "authors": ["Sho  Ko", "Shimeng  Yu"], "year": 2020, "n_citations": 2}
{"id": 1277769, "s2_id": "a3d98c14aac4b8779dd2ac5057b062a0cfae6d9a", "title": "Cross-Layer Optimization for High Speed Adders: A Pareto Driven Machine Learning Approach", "abstract": "In spite of maturity to the modern electronic design automation (EDA) tools, optimized designs at architectural stage may become suboptimal after going through physical design flow. Adder design has been such a long studied fundamental problem in very large-scale integration industry yet designers cannot achieve optimal solutions by running EDA tools on the set of available prefix adder architectures. In this paper, we enhance a state-of-the-art prefix adder synthesis algorithm to obtain a much wider solution space in architectural domain. On top of that, a machine learning-based design space exploration methodology is applied to predict the Pareto frontier of the adders in physical domain, which is infeasible by exhaustively running EDA tools for innumerable architectural solutions. Considering the high cost of obtaining the true values for learning, an active learning algorithm is proposed to select the representative data during learning process, which uses less labeled data while achieving better quality of Pareto frontier. Experimental results demonstrate that our framework can achieve Pareto frontier of high quality over a wide design space, bridging the gap between architectural and physical designs. Source code and data are available at https://github.com/yuzhe630/adder-DSE.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Yuzhe  Ma", "Subhendu  Roy", "Jin  Miao", "Jiamin  Chen", "Bei  Yu"], "year": 2019, "n_citations": 11}
{"id": 1279628, "s2_id": "ee3d7cfbf7884d1c71483eba8ebca96aa9a1b8a1", "title": "Neural Network-Inspired Analog-to-Digital Conversion to Achieve Super-Resolution with Low-Precision RRAM Devices", "abstract": "Recent works propose neural network- (NN-) inspired analog-to-digital converters (NNADCs) and demonstrate their great potentials in many emerging applications. These NNADCs often rely on resistive random-access memory (RRAM) devices to realize the NN operations and require high-precision RRAM cells (6\u223c12-bit) to achieve a moderate quantization resolution (4\u223c8-bit). Such optimistic assumption of RRAM resolution, however, is not supported by fabrication data of RRAM arrays in large-scale production process. In this paper, we propose an NN-inspired super-resolution ADC based on low-precision RRAM devices by taking the advantage of a co-design methodology that combines a pipelined hardware architecture with a custom NN training framework. Results obtained from SPICE simulations demonstrate that our method leads to robust design of a 14-bit super-resolution ADC using 3-bit RRAM devices with improved power and speed performance and competitive figure-of-merits (FoMs). In addition to the linear uniform quantization, the proposed ADC can also support configurable high-resolution nonlinear quantization with high conversion speed and low conversion energy, enabling future intelligent analog-to-information interfaces for near-sensor analytics and processing.", "venue": "2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Weidong  Cao", "Liu  Ke", "Ayan  Chakrabarti", "Xuan  Zhang"], "year": 2019, "n_citations": 3}
{"id": 1280253, "s2_id": "661fac7c4ee7cbdf1a89daa2e66560547fa1e8af", "title": "eQASM: An Executable Quantum Instruction Set Architecture", "abstract": "A widely-used quantum programming paradigm comprises of both the data flow and control flow. Existing quantum hardware cannot well support the control flow, significantly limiting the range of quantum software executable on the hardware. By analyzing the constraints in the control microarchitecture, we found that existing quantum assembly languages are either too high-level or too restricted to support comprehensive flow control on the hardware. Also, as observed with the quantum microinstruction set QuMIS [1], the quantum instruction set architecture (QISA) design may suffer from limited scalability and flexibility because of microarchitectural constraints. It is an open challenge to design a scalable and flexible QISA which provides a comprehensive abstraction of the quantum hardware. In this paper, we propose an executable QISA, called eQASM, that can be translated from quantum assembly language (QASM), supports comprehensive quantum program flow control, and is executed on a quantum control microarchitecture. With efficient timing specification, single-operationmultiple-qubit execution, and a very-long-instruction-word architecture, eQASM presents better scalability than QuMIS. The definition of eQASM focuses on the assembly level to be expressive. Quantum operations are configured at compile time instead of being defined at QISA design time. We instantiate eQASM into a 32-bit instruction set targeting a seven-qubit superconducting quantum processor. We validate our design by performing several experiments on a two-qubit quantum processor. \u00a9 2019 IEEE.", "venue": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Xiang  Fu", "L.  Riesebos", "M. A. Rol", "Jeroen van Straten", "J. van Someren", "Nader  Khammassi", "Imran  Ashraf", "R. F. L. Vermeulen", "V.  Newsum", "K. K. L. Loh", "J. C. de Sterke", "W. J. Vlothuizen", "R. N. Schouten", "Carmen G. Almud\u00e9ver", "L.  DiCarlo", "Koen  Bertels"], "year": 2019, "n_citations": 37}
{"id": 1281778, "s2_id": "86d9aa28dae852386df5c6c285d4f72f164f60e9", "title": "LaKe: An Energy Efficient, Low Latency, Accelerated Key-Value Store", "abstract": "Key-value store is a popular type of cloud computing applications. The performance of key-value store applications have been shown to be very sensitive to load within the data center, and in particular to latency. As load within data center increases, it is becoming hard to maintain key-value store applications' performance, without exceeding both the processing capacity of hosts and the power budgets of racks. In this paper, we present LaKe: a low latency, power efficient key-value store design for cloud applications. LaKe is a modular design, combining multiple cores and cache layering, both in hardware and software. LaKe achieves full line rate throughput, while maintaining a latency of 1.1us and better power efficiency than existing hardware based memcached designs. Using the modularity of our design, we study trade-offs in the use of on-chip memory, SRAM and DRAM in accelerated designs and provide insights for future architectures.", "venue": "ArXiv", "authors": ["Yuta  Tokusashi", "Hiroki  Matsutani", "Noa  Zilberman"], "year": 2018, "n_citations": 4}
{"id": 1284940, "s2_id": "0ff00b7069e90ebdc7e14d1d287b5fd0d1edf198", "title": "Multiscale Co-Design Analysis of Energy, Latency, Area, and Accuracy of a ReRAM Analog Neural Training Accelerator", "abstract": "Neural networks are an increasingly attractive algorithm for natural language processing and pattern recognition. Deep networks with >50 M parameters are made possible by modern graphics processing unit clusters operating at <50 pJ per op and more recently, production accelerators are capable of <5 pJ per operation at the board level. However, with the slowing of CMOS scaling, new paradigms will be required to achieve the next several orders of magnitude in performance per watt gains. Using an analog resistive memory (ReRAM) crossbar to perform key matrix operations in an accelerator is an attractive option. This paper presents a detailed design using the state-of-the-art 14/16 nm process development kit for of an analog crossbar circuit block designed to process three key kernels required in training and inference of neural networks. A detailed circuit and device-level analysis of energy, latency, area, and accuracy are given and compared with relevant designs using standard digital ReRAM and static random access memory (SRAM) operations. It is shown that the analog accelerator has <inline-formula> <tex-math notation=\"LaTeX\">$270\\times $ </tex-math></inline-formula> energy and <inline-formula> <tex-math notation=\"LaTeX\">$540\\times $ </tex-math></inline-formula> latency advantage over a similar block utilizing only digital ReRAM and takes only 11 fJ per multiply and accumulate. Compared with an SRAM-based accelerator, the energy is <inline-formula> <tex-math notation=\"LaTeX\">$430\\times $ </tex-math></inline-formula> better and latency is <inline-formula> <tex-math notation=\"LaTeX\">$34\\times $ </tex-math></inline-formula> better. Although training accuracy is degraded in the analog accelerator, several options to improve this are presented. The possible gains over a similar digital-only version of this accelerator block suggest that continued optimization of analog resistive memories is valuable. This detailed circuit and device analysis of a training accelerator may serve as a foundation for further architecture-level studies.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Matthew J. Marinella", "Sapan  Agarwal", "Alexander  Hsia", "Isaac  Richter", "Robin  Jacobs-Gedrim", "John  Niroula", "Steven J. Plimpton", "Engin  Ipek", "Conrad D. James"], "year": 2018, "n_citations": 71}
{"id": 1286217, "s2_id": "c82d665b76a380b13c6fc6fb637cc020204036b0", "title": "Cost Modeling and Projection for Stacked Nanowire Fabric", "abstract": "To continue scaling beyond 2-D CMOS with 3-D integration, any new 3-D IC technology has to be comparable or better than 2-D CMOS in terms of scalability, enhanced functionality, density, power, performance, cost, and reliability. Transistor-level 3-D integration carries the most potential in this regard. Recently, we proposed a stacked horizontal nanowire based transistor-level 3-D integration approach, called SN3D [1][2] that solves scaling challenges and achieves tremendous benefits with respect to 2-D CMOS while keeping manageable thermal profile. In this paper, we present the cost analysis of SN3D and show comparison with 2-D CMOS (2D), conventional TSV based 3-D (T3D) and Monolithic 3-D integrations (M3D). In our cost model, we capture the implications of manufacturing, circuit density, interconnects, bonding and heat in determining die cost, and evaluate how cost scales as transistor count increases. Since SN3D is a new 3-D IC fabric, based on our proposed manufacturing pathway[1] we assumed complexity of fabrication steps as proportionality constants in our cost estimation model. Our analysis revealed 86%, 72% and 74% reduction in area; 55%, 43% and 43% reduction in interconnects distribution and total interconnect length for SN3D, which largely contributed to 70%, 67% and 68% reduction in cost in comparison to 2D, T3D and M3D respectively.", "venue": "ArXiv", "authors": ["Naveen Kumar Macha", "Mostafizur  Rahman"], "year": 2017, "n_citations": 1}
{"id": 1295231, "s2_id": "3bc02630df6171a8af4af487da21e1df92d86781", "title": "ENTRA: Whole-Systems Energy Transparency", "abstract": "Abstract Promoting energy efficiency to a first class system design goal is an important research challenge. Although more energy-efficient hardware can be designed, it is software that controls the hardware; for a given system the potential for energy savings is likely to be much greater at the higher levels of abstraction in the system stack. Thus the greatest savings are expected from energy-aware software development, which is the vision of the EU ENTRA project. This article presents the concept of energy transparency as a foundation for energy-aware software development. We show how energy modelling of hardware is combined with static analysis to allow the programmer to understand the energy consumption of a program without executing it, thus enabling exploration of the design space taking energy into consideration. The paper concludes by summarising the current and future challenges identified in the ENTRA project.", "venue": "Microprocess. Microsystems", "authors": ["Kerstin  Eder", "John P. Gallagher", "Pedro  L\u00f3pez-Garc\u00eda", "Henk L. Muller", "Zorana  Bankovic", "Kyriakos  Georgiou", "R\u00e9my  Haemmerl\u00e9", "Manuel V. Hermenegildo", "Bishoksan  Kafle", "Steve  Kerrison", "Maja H. Kirkeby", "Maximiliano  Klemen", "Xueliang  Li", "Umer  Liqat", "Jeremy  Morse", "Morten  Rhiger", "Mads  Rosendahl"], "year": 2016, "n_citations": 12}
{"id": 1296873, "s2_id": "fd8e22d6877aa1c94a06013dda0fee2e87245aa4", "title": "Improved Analytical Delay Models for RC-Coupled Interconnects", "abstract": "As the process technologies scale into deep submicron region, crosstalk delay is becoming increasingly severe, especially for global on-chip buses. To cope with this problem, accurate delay models of coupled interconnects are needed. In particular, delay models based on analytical approaches are desirable, because they not only are largely transparent to technology, but also explicitly establish the connections between delays of coupled interconnects and transition patterns, thereby enabling crosstalk alleviating techniques such as crosstalk avoidance codes (CACs). Unfortunately, existing analytical delay models, such as the widely cited model in [1], have limited accuracy and do not account for loading capacitance. In this paper, we propose analytical delay models for coupled interconnects that address these disadvantages. By accounting for more wires and eschewing the Elmore delay, our delay models achieve better accuracy than the model in [1].", "venue": "IEEE Trans. Very Large Scale Integr. Syst.", "authors": ["Feng  Shi", "Xuebin  Wu", "Zhiyuan  Yan"], "year": 2014, "n_citations": 6}
{"id": 1299358, "s2_id": "d0cabf2a41312d0b0c4317c38cc3d6856729cf1a", "title": "Design Space Exploration of Power Delivery For Advanced Packaging Technologies", "abstract": "In this paper, a design space exploration of power delivery networks is performed for multi-chip 2.5-D and 3-D IC technologies. The focus of the paper is the effective placement of the voltage regulator modules (VRMs) for power supply noise (PSN) suppression. Multiple on-package VRM configurations have been analyzed and compared. Additionally, 3D IC chip-on-VRM and backside-of-the-package VRM configurations are studied. From the PSN perspective, the 3D IC chip-on-VRM case suppresses the PSN the most even with high current density hotspots. The paper also studies the impact of different parameters such as VRM-chip distance on the package, on-chip decoupling capacitor density, etc. on the PSN.", "venue": "ArXiv", "authors": ["Md Obaidul Hossen", "Yang  Zhang", "Hesam Fathi Moghadam", "Yue  Zhang", "Michael  Dayringer", "Muhannad S Bakir"], "year": 2020, "n_citations": 0}
{"id": 1301066, "s2_id": "737b358192c2dde4465bd9616a1b5eae05278b5a", "title": "Design and Performance Analysis of hybrid adders for high speed arithmetic circuit", "abstract": "Adder cells using Gate Diffusion Technique (GDI) & PTL-GDI technique are described in this paper. GDI technique allows reducing power consumption, propagation delay and low PDP (power delay product) whereas Pass Transistor Logic (PTL) reduces the count of transistors used to make different logic gates, by eliminating redundant transistors. Performance comparison with various Hybrid Adder is been presented. In this paper, we propose two new designs based on GDI & PTL techniques, which is found to be much more power efficient in comparison with existing design technique. Only 10 transistors are used to implement the SUM & CARRY function for both the designs. The SUM and CARRY cell are implemented in a cascaded way i.e. firstly the XOR cell is implemented and then using XOR as input SUM as well as CARRY cell is implemented. For Proposed GDI adder the SUM as well as CARRY cell is designed using GDI technique. On the other hand in Proposed PTL-GDI adder the SUM cell is constructed using PTL technique and the CARRY cell is designed using GDI technique. The advantages of both the designs are discussed. The significance of these designs is substantiated by the simulation results obtained from Cadence Virtuoso 180nm environment.", "venue": "VLSIC 2012", "authors": ["Rajkumar  Sarma", "Veerati  Raju"], "year": 2012, "n_citations": 17}
{"id": 1301226, "s2_id": "2154d8fc2caa334d5fa405a8d81fe6894fe9759e", "title": "Loom: Exploiting Weight and Activation Precisions to Accelerate Convolutional Neural Networks", "abstract": "Loom (LM), a hardware inference accelerator for Convolutional Neural Networks (CNNs) is presented. In LM every bit of data precision that can be saved translates to proportional performance gains. For both weights and activations LM exploits profile-derived per layer precisions. However, at runtime LM further trims activation precisions at a much smaller than a layer granularity. On average, across several image classification CNNs and for a configuration that can perform the equivalent of 128 16b \u00d7 16b multiply-accumulate operations per cycle LM outperforms a state-of-the-art bit-parallel accelerator [3] by 3.19 \u00d7 without any loss in accuracy while being 2.59 \u00d7 more energy efficient. LM can trade-off accuracy for additional improvements in execution performance and energy efficiency and compares favorably to an accelerator that targeted only activation precisions.", "venue": "2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)", "authors": ["Sayeh  Sharify", "Alberto Delmas Lascorz", "Patrick  Judd", "Andreas  Moshovos"], "year": 2018, "n_citations": 50}
{"id": 1301376, "s2_id": "944e53e1e40fc8c3bd88f90927857e964095fd31", "title": "Exploiting Semiconductor Properties for Hardware Trojans", "abstract": "This paper discusses the possible introduction of hidden reliability defects during CMOS foundry fabrication processes that may lead to accelerated wearout of the devices. These hidden defects or hardware Trojans can be created by deviation from foundry design rules and processing parameters. The Trojans are produced by exploiting time-based wearing mechanisms (HCI, NBTI, TDDB and EM) and/or condition-based triggers (ESD, Latchup and Softerror). This class of latent damage is difficult to test due to its gradual degradation nature. The paper describes life-time expectancy results for various Trojan induced scenarios. Semiconductor properties, processing and design parameters critical for device reliability and Trojan creation are discussed.", "venue": "ArXiv", "authors": ["Yuriy  Shiyanovskii", "Francis G. Wolff", "Christos A. Papachristou", "Daniel J. Weyer", "W.  Clay"], "year": 2009, "n_citations": 12}
{"id": 1302809, "s2_id": "e219b20904fa39fabf72af2a4e347ab9b0fe0585", "title": "Bipedal Humanoid Hardware Design: A Technology Review", "abstract": "\n \n As new technological advancements are made, humanoid robots that utilise them are being designed and manufactured. For optimal design choices, a broad overview with insight on the advantages and disadvantages of available technologies is necessary. This article intends to provide an analysis on the established approaches and contrast them with emerging ones.\n \n \n A clear shift in the recent design features of humanoid robots is developing, which is supported by literature. As humanoid robots are meant to leave laboratories and traverse the world, compliance and more efficient locomotion are necessary. The limitations of highly rigid actuation are being tackled by different research groups in unique ways. Some focus on modifying the kinematic structure, while others change the actuation scheme. With new manufacturing capabilities, previously impossible designs are becoming feasible.\n \n \n A comprehensive review on the technologies crucial for bipedal humanoid robots was performed. Different mechanical concepts have been discussed, along with the advancements in actuation, sensing, and manufacturing. The paper is supplemented with a list of the recently developed platforms along with a selection of their specifications.\n", "venue": "Current Robotics Reports", "authors": ["Grzegorz  Ficht", "Sven  Behnke"], "year": 2021, "n_citations": 0}
{"id": 1303016, "s2_id": "71cc87465cc3d6dbcb4b1382a58f9b56711c3c95", "title": "MetaSys: A Practical Open-Source Metadata Management System to Implement and Evaluate Cross-Layer Optimizations", "abstract": "This paper introduces the first open-source FPGAbased infrastructure, MetaSys, with a prototype in a RISCV core, to enable the rapid implementation and evaluation of a wide range of cross-layer techniques in real hardware. Hardware-software cooperative techniques are powerful approaches to improve the performance, quality of service, and security of general-purpose processors. They are however typically challenging to rapidly implement and evaluate in real hardware as they require full-stack changes to the hardware, OS, system software, and instruction-set architecture (ISA). MetaSys implements a rich hardware-software interface and lightweight metadata support that can be used as a common basis to rapidly implement and evaluate new cross-layer techniques. We demonstrate MetaSys\u2019s versatility and ease-of-use by implementing and evaluating three cross-layer techniques for: (i) prefetching for graph analytics; (ii) bounds checking in memory unsafe languages, and (iii) return address protection in stack frames; each technique only requiring ~100 lines of Chisel code over MetaSys. Using MetaSys, we perform the first detailed experimental study to quantify the performance overheads of using a single metadata management system to enable multiple cross-layer optimizations in CPUs. We identify the key sources of bottlenecks and system inefficiency of a general metadata management system. We design MetaSys to minimize these inefficiencies and provide increased versatility compared to previously-proposed metadata systems. Using three use cases and a detailed characterization, we demonstrate that a common metadata management system can be used to efficiently support diverse crosslayer techniques in CPUs.", "venue": "ArXiv", "authors": ["Nandita  Vijaykumar", "Ataberk  Olgun", "Konstantinos  Kanellopoulos", "Nisa  Bostanci", "Hasan  Hassan", "Mehrshad  Lotfi", "Phillip B. Gibbons", "Onur Mutlu University of Toronto", "Carnegie Mellon University", "TOBB University of Economics", "Technology", "Max Plank Institute", "ETH  Zurich"], "year": 2021, "n_citations": 0}
{"id": 1315646, "s2_id": "ae72e98241ffd165f95f132c802dd2ae11fc0f1b", "title": "Leaking Secrets through Modern Branch Predictor in the Speculative World", "abstract": "Transient execution attacks that exploit speculation have raised significant concerns in computer systems. Typically, branch predictors are leveraged to trigger mis-speculation in transient execution attacks. In this work, we demonstrate a new class of speculation-based attacks that targets the branch prediction unit (BPU). We find that speculative resolution of conditional branches (i.e., in nested speculation) alter the states of pattern history table (PHT) in modern processors, which are not restored after the corresponding branches are later squashed. Such characteristic allows attackers to exploit the BPU as the secret transmitting medium in transient execution attacks. To evaluate the discovered vulnerability, we build a novel attack framework, BranchSpectre, that enables exfiltration of unintended secrets through observing speculative PHT updates (in the form of covert and side channels). We further investigate the PHT collision mechanism in the history-based predictor and the branch prediction mode transitions in Intel processors. Built upon such knowledge, we implement an ultra-high speed covert channel (BranchSpectre-cc) as well as two side channels (i.e., BranchSpectre-v1 and BranchSpectre-v2) that merely rely on BPU for mis-speculation trigger and secret inference in the speculative domain. Notably, BranchSpectre side channels can take advantage of much simpler code patterns than those used in Spectre attacks. We present an extensive BranchSpectre code gadget analysis on a set of popular real-world application code bases followed by a demonstration of side channel attack on OpenSSL. The evaluation results show substantially wider existence and higher exploitability of BranchSpectre code patterns in real-world software. Finally, we discuss several secure branch prediction mechanisms that can mitigate transient execution attacks exploiting modern branch predictors.", "venue": "IEEE Transactions on Computers", "authors": ["Md Hafizul Islam Chowdhuryy", "Fan  Yao"], "year": 2021, "n_citations": 2}
{"id": 1322615, "s2_id": "a559717fc77c041ab28e3f51ceba0ddae82547ab", "title": "A compiler infrastructure for accelerator generators", "abstract": "We present Calyx, a new intermediate language (IL) for compiling high-level programs into hardware designs. Calyx combines a hardware-like structural language with a software-like control flow representation with loops and conditionals. This split representation enables a new class of hardware-focused optimizations that require both structural and control flow information which are crucial for high-level programming models for hardware design. The Calyx compiler lowers control flow constructs using finite-state machines and generates synthesizable hardware descriptions. We have implemented Calyx in an optimizing compiler that translates high-level programs to hardware. We demonstrate Calyx using two DSL-to-RTL compilers, a systolic array generator and one for a recent imperative accelerator language, and compare them to equivalent designs generated using high-level synthesis (HLS). The systolic arrays are 4.6\u00d7 faster and 1.11\u00d7 larger on average than HLS implementations, and the HLS-like imperative language compiler is within a few factors of a highly optimized commercial HLS toolchain. We also describe three optimizations implemented in the Calyx compiler.", "venue": "ASPLOS", "authors": ["Rachit  Nigam", "Samuel  Thomas", "Zhijing  Li", "Adrian  Sampson"], "year": 2021, "n_citations": 4}
{"id": 1323854, "s2_id": "36a3023a534be0c5a42e7c10083cf7e670066bd4", "title": "FINN-L: Library Extensions and Design Trade-Off Analysis for Variable Precision LSTM Networks on FPGAs", "abstract": "It is well known that many types of artificial neural networks, including recurrent networks, can achieve a high classification accuracy even with low-precision weights and activations. The reduction in precision generally yields much more efficient hardware implementations in regards to hardware cost, memory requirements, energy, and achievable throughput. In this paper, we present the first systematic exploration of this design space as a function of precision for Bidirectional Long Short-Term Memory (BiLSTM) neural network. Specifically, we include an in-depth investigation of precision vs. accuracy using a fully hardware-aware training flow, where during training quantization of all aspects of the network including weights, input, output and in-memory cell activations are taken into consideration. In addition, hardware resource cost, power consumption and throughput scalability are explored as a function of precision for FPGA-based implementations of BiLSTM, and multiple approaches of parallelizing the hardware. We provide the first open source HLS library extension of FINN for parameterizable hardware architectures of LSTM layers on FPGAs which offers full precision flexibility and allows for parameterizable performance scaling offering different levels of parallelism within the architecture. Based on this library, we present an FPGA-based accelerator for BiLSTM neural network designed for optical character recognition, along with numerous other experimental proof points for a Zynq UltraScale+ XCZU7EV MPSoC within the given design space.", "venue": "2018 28th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Vladimir  Rybalkin", "Alessandro  Pappalardo", "Muhammad Mohsin Ghaffar", "Giulio  Gambardella", "Norbert  Wehn", "Michaela  Blott"], "year": 2018, "n_citations": 46}
{"id": 1324529, "s2_id": "17a29313e6bde605389d1dd7aa1483c8f4651668", "title": "Dynamic cache reconfiguration based techniques for improving cache energy efficiency", "abstract": "Modern multicore processors are employing large last-level caches, for example Intel's E7-8800 processor uses 24MB L3 cache. Further, with each CMOS technology generation, leakage energy has been dramatically increasing and hence, leakage energy is expected to become a major source of energy dissipation, especially in last-level caches (LLCs). The conventional schemes of cache energy saving either aim at saving dynamic energy or are based on properties specific to first-level caches, and thus these schemes have limited utility for last-level caches. Further, several other techniques require offline profiling or per-application tuning and hence are not suitable for product systems. In this research, we propose novel cache leakage energy saving schemes for single-core and multicore systems; desktop, QoS, real-time and server systems. We propose software-controlled, hardware-assisted techniques which use dynamic cache reconfiguration to configure the cache to the most energy efficient configuration while keeping the performance loss bounded. To profile and test a large number of potential configurations, we utilize low-overhead, micro-architecture components, which can be easily integrated into modern processor chips. We adopt a system-wide approach to save energy to ensure that cache reconfiguration does not increase energy consumption of other components of the processor. We have compared our techniques with the state-of-art techniques and have found that our techniques outperform them in their energy efficiency. This research has important applications in improving energy-efficiency of higher-end embedded, desktop, server processors and multitasking systems. We have also proposed performance estimation approach for efficient design space exploration and have implemented time-sampling based simulation acceleration approach for full-system architectural simulators.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 9}
{"id": 1325760, "s2_id": "1fe59da0136fa73107eb358a73583d71d9ff0080", "title": "ShEF: Shielded Enclaves for Cloud FPGAs", "abstract": "FPGAs are now used in public clouds to accelerate a wide range of applications, including many that operate on sensitive data such as financial and medical records. We present ShEF, a trusted execution environment (TEE) for cloud-based reconfigurable accelerators. ShEF is independent from CPUbased TEEs and allows secure execution under a threat model where the adversary can control all software running on the CPU connected to the FPGA, has physical access to the FPGA, and can compromise the FPGA interface logic of the cloud provider. ShEF provides a secure boot and remote attestation process that relies solely on existing FPGA mechanisms for root of trust. It also includes a Shield component that provides secure access to data while the accelerator is in use. The Shield is highly customizable and extensible, allowing users to craft a bespoke security solution that fits their accelerator\u2019s memory access patterns, bandwidth, and security requirements at minimum performance and area overheads. We describe a prototype implementation of ShEF for existing cloud FPGAs and measure the performance benefits of customizable security using five accelerator designs.", "venue": "ArXiv", "authors": ["Mark  Zhao", "Mingyu  Gao", "Christos  Kozyrakis"], "year": 2021, "n_citations": 1}
{"id": 1326357, "s2_id": "e69b7b09c5298a710c8f4f8cd3e2c373cb81a1f8", "title": "Design of Parity Preserving Logic Based Fault Tolerant Reversible Arithmetic Logic Unit", "abstract": "Reversible Logic is gaining significant consideration as the potential logic design style for implementation in modern nanotechnology and quantum computing with minimal impact on physical entropy .Fault Tolerant reversible logic is one class of reversible logic that maintain the parity of the input and the outputs. Significant contributions have been made in the literature towards the design of fault tolerant reversible logic gate structures and arithmetic units, however, there are not many efforts directed towards the design of fault tolerant reversible ALUs. Arithmetic Logic Unit (ALU) is the prime performing unit in any computing device and it has to be made fault tolerant. In this paper we aim to design one such fault tolerant reversible ALU that is constructed using parity preserving reversible logic gates. The designed ALU can generate up to seven Arithmetic operations and four logical operations.", "venue": "VLSIC 2013", "authors": ["Rakshith  Saligram", "Shrihari Shridhar Hegde", "Shashidhar A. Kulkarni", "H. R. Bhagyalakshmi", "M. K. Venkatesha"], "year": 2013, "n_citations": 18}
{"id": 1329103, "s2_id": "d92e7f5c2afc1be28fba42a0de168b80d05eb615", "title": "Lupulus: A Flexible Hardware Accelerator for Neural Networks", "abstract": "Neural networks have become indispensable for a wide range of applications, but they suffer from high computationaland memory-requirements, requiring optimizations from the algorithmic description of the network to the hardware implementation. Moreover, the high rate of innovation in machine learning makes it important that hardware implementations provide a high level of programmability to support current and future requirements of neural networks. In this work, we present a flexible hardware accelerator for neural networks, called Lupulus, supporting various methods for scheduling and mapping of operations onto the accelerator. Lupulus was implemented in a 28nm FD-SOI technology and demonstrates a peak performance of 380GOPS/GHz with latencies of 21.4ms and 183.6ms for the convolutional layers of AlexNet and VGG-16, respectively.", "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Andreas Toftegaard Kristensen", "Robert  Giterman", "Alexios  Balatsoukas-Stimming", "Andreas  Burg"], "year": 2020, "n_citations": 0}
{"id": 1331122, "s2_id": "d1fef5bc2da7c2705f247c9369221966f046857a", "title": "FPDeep: Scalable Acceleration of CNN Training on Deeply-Pipelined FPGA Clusters", "abstract": "Deep convolutional Neural Networks (CNNs) have revolutionized numerous applications, but the demand for ever more performance remains unabated. Scaling CNN computations to larger clusters is generally done by distributing tasks in batch mode using methods such as distributed synchronous SGD. Among the issues with this approach is that, to make the distributed cluster work with high utilization, the workload distributed to each node must be large; this implies nontrivial growth in the SGD mini-batch size. In this article we propose a framework, called FPDeep, which uses a hybrid of model and layer parallelism to configure distributed reconfigurable clusters to train CNNs. This approach has numerous benefits. First, the design does not suffer from performance loss due to batch size growth. Second, work and storage are balanced among nodes through novel workload and weight partitioning schemes. Part of the mechanism is the surprising finding that it is preferable to store excess weights in neighboring devices rather than in local off-chip memory. Third, the entire system is a fine-grained pipeline. This leads to high parallelism and utilization and also minimizes the time that features need to be cached while waiting for back-propagation. As a result, storage demand is reduced to the point where only on-chip memory is used for the convolution layers. And fourth, we find that the simplest topology, a 1D array, is preferred for interconnecting the FPGAs thus enabling widespread applicability. We evaluate FPDeep with the Alexnet, VGG-16, and VGG-19 benchmarks. Results show that FPDeep has good scalability to a large number of FPGAs, with the limiting factor being the FPGA-to-FPGA bandwidth. But with 250 Gb/s bidirectional bandwidth per FPGA, which is easily supported by current generation FPGAs, FPDeep performance shows linearity up to 100 FPGAs. Energy efficiency is evaluated with respect to GOPs/J. FPDeep provides, on average, 6.4\u00d7 higher energy efficiency than comparable GPU servers.", "venue": "IEEE Transactions on Computers", "authors": ["Tianqi  Wang", "Tong  Geng", "Ang  Li", "Xi  Jin", "Martin  Herbordt"], "year": 2020, "n_citations": 15}
{"id": 1331238, "s2_id": "bf64c4408a56fdb6448b651832a4dcdf7a6df55a", "title": "FPGA Implementation of ECG feature extraction using Time domain analysis", "abstract": "An electrocardiogram (ECG) feature extraction system has been developed and evaluated using Virtex-6 FPGA kit which belongs to Xilinx Ltd. In time domain, Pan-Tompkins algorithm is used for QRS detection and it is followed by a feature extractor block to extract ECG features. This whole system can be used to detect cardiac arrhythmia. The completed algorithm was implemented on Virtex-6(XC6VLX240-T) device and tested using hardware co-simulation in Modelsim and simulink environment. The software generated ECG signals are obtained from MIT-BIH arrhythmia Database [1]. The memory and time complexities of the implemented design were recorded and feature extraction has been done. We have achieved satisfactory results which is mainly due to parallel implementation. Therefore accurate arrhythmia detection using hardware implementation a viable approach.", "venue": "ArXiv", "authors": ["Naveen Sai Madiraju", "Naresh  Kurella", "Rama  Valapudasu"], "year": 2018, "n_citations": 1}
{"id": 1333295, "s2_id": "3a89a7df00e7d7103193acc988b1af7ce4ce7358", "title": "GnetDet: Object Detection Optimized on a 224mW CNN Accelerator Chip at the Speed of 106FPS", "abstract": "Object detection is widely used on embedded devices. With the wide availability of CNN (Convolutional Neural Networks) accelerator chips, the object detection applications are expected to run with low power consumption, and high inference speed. In addition, the CPU load is expected to be as low as possible for a CNN accelerator chip working as a co-processor with a host CPU. In this paper, we optimize the object detection model on the CNN accelerator chip by minimizing the CPU load. The resulting model is called GnetDet. The experimental result shows that the GnetDet model running on a 224mW chip achieves the speed of 106FPS with excellent accuracy.", "venue": "ArXiv", "authors": ["Baohua  Sun", "Tao  Zhang", "Jiapeng  Su", "Hao  Sha"], "year": 2021, "n_citations": 0}
{"id": 1339006, "s2_id": "790c4ea1e7732acd4bc3c6bc1e0a4eaf9e91be4a", "title": "Area Optimized Quasi Delay Insensitive Majority Voter for TMR Applications", "abstract": "Mission-critical and safety-critical applications generally tend to incorporate triple modular redundancy (TMR) to embed fault tolerance in their physical implementations. In a TMR realization, an original function block, which may be a circuit or a system, and two exact copies of the function block are used to successfully overcome any temporary fault or permanent failure of an arbitrary function block during the routine operation. The corresponding outputs of the function blocks are majority voted using 3-input majority voters whose outputs define the outputs of a TMR realization. Hence, a 3-input majority voter forms an important component of a TMR realization. Many synchronous majority voters and an asynchronous non-delay insensitive majority voter have been presented in the literature. Recently, quasi delay insensitive (QDI) asynchronous majority voters for TMR applications were also discussed in the literature. In this regard, this paper presents a new QDI asynchronous majority voter for TMR applications, which is better optimized in area compared to the existing QDI majority voters. The proposed QDI majority voter requires 30.2% less area compared to the best of the existing QDI majority voters, and this could be useful for resource-constrained fault tolerance applications. The example QDI TMR circuits were implemented using a 32/28nm complementary metal oxide semiconductor (CMOS) process. The delay insensitive dual rail code was used for data encoding, and 4-phase return-to-zero and return-to-one handshake protocols were used for data communication.", "venue": "2019 3rd European Conference on Electrical Engineering and Computer Science (EECS)", "authors": ["P  Balasubramanian", "D L Maskell", "N E Mastorakis"], "year": 2019, "n_citations": 3}
{"id": 1341949, "s2_id": "586c58d882729edebe7308b6996a3e9402fdbc63", "title": "Making Belady-Inspired Replacement Policies More Effective Using Expected Hit Count", "abstract": "Memory-intensive workloads operate on massive amounts of data that cannot be captured by last-level caches (LLCs) of modern processors. Consequently, processors encounter frequent off-chip misses, and hence, lose a significant performance potential. One way to reduce the number of off-chip misses is through using a well-behaved replacement policy in the LLC. Existing processors employ a variation of least recently used (LRU) policy to determine a victim for replacement. Unfortunately, there is a large gap between what LRU offers and that of Belady's MIN, which is the optimal replacement policy. Belady's MIN requires selecting a victim with the longest reuse distance, and hence, is unfeasible due to the need to know the future. Consequently, Belady-inspired replacement polices use Belady's MIN to derive an indicator to help them choose a victim for replacement. \nIn this work, we show that the indicator that is used in the state-of-the-art Belady-inspired replacement policy is not decisive in picking a victim in a considerable number of cases, and hence, the policy has to rely on a standard metric (e.g., recency or frequency) to pick a victim, which is inefficient. We observe that there exist strong correlations among the hit counts of cache blocks in the same region of memory when Belady's MIN is the replacement policy. Taking advantage of this observation, we propose an expected-hit-count indicator for the memory regions and use it to improve the victim selection mechanism of Belady-inspired replacement policies when the main indicator is not decisive. Our proposal offers a 5.2\\% performance improvement over the baseline LRU and outperforms Hawkeye, which is the state-of-the-art replacement policy.", "venue": "ArXiv", "authors": ["Seyed Armin Vakil-Ghahani", "Sara  Mahdizadeh-Shahri", "Mohammad  Bakhshalipour", "Pejman  Lotfi-Kamran", "Hamid  Sarbazi-Azad"], "year": 2018, "n_citations": 2}
{"id": 1346918, "s2_id": "ee483a2152cd51e3f241dc1a4a5c1e07a7b5b2f9", "title": "HoLiSwap: Reducing Wire Energy in L1 Caches", "abstract": "This paper describes HoLiSwap a method to reduce L1 cache wire energy, a significant fraction of total cache energy, by swapping hot lines to the cache way nearest to the processor. We observe that (i) a small fraction (<3%) of cache lines (hot lines) serve over 60% of the L1 cache accesses and (ii) the difference in wire energy between the nearest and farthest cache subarray can be over 6$\\times$. Our method exploits this difference in wire energy to dynamically identify hot lines and swap them to the nearest physical way in a set-associative L1 cache. This provides up to 44% improvement in the wire energy (1.82% saving in overall system energy) with no impact on the cache miss rate and 0.13% performance drop. We also show that HoLiSwap can simplify way-prediction.", "venue": "ArXiv", "authors": ["Yatish  Turakhia", "Subhasis  Das", "Tor M. Aamodt", "William J. Dally"], "year": 2017, "n_citations": 0}
{"id": 1350230, "s2_id": "aa0412b427e4d76b093416c763d7c48cb7848d3a", "title": "Floorplanning and topology generation for application-specific Network-on-Chip", "abstract": "Network-on-Chip(NoC) architectures have been proposed as a promising alternative to classical bus-based communication architectures. In this paper, we propose a two phases framework to solve application-specific NoCs topology generation problem. At floorplanning phase, we carry out partition driven floorplanning. At post-floorplanning phase, a heuristic method and a min-cost max-flow algorithm is used to insert switches and network interfaces. Finally, we allocate paths to minimize power consumption. The experimental results show our algorithm is effective for power saving.", "venue": "2010 15th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Bei  Yu", "Sheqin  Dong", "Song  Chen", "Satoshi  Goto"], "year": 2010, "n_citations": 44}
{"id": 1351471, "s2_id": "5529ad5e0f07947c1e720a2d134237eae9edd9a0", "title": "AdaptivFloat: A Floating-point based Data Type for Resilient Deep Learning Inference", "abstract": "Conventional hardware-friendly quantization methods, such as fixed-point or integer, tend to perform poorly at very low word sizes as their shrinking dynamic ranges cannot adequately capture the wide data distributions commonly seen in sequence transduction models. We present AdaptivFloat, a floating-point inspired number representation format for deep learning that dynamically maximizes and optimally clips its available dynamic range, at a layer granularity, in order to create faithful encoding of neural network parameters. AdaptivFloat consistently produces higher inference accuracies compared to block floating-point, uniform, IEEE-like float or posit encodings at very low precision ($\\leq$ 8-bit) across a diverse set of state-of-the-art neural network topologies. And notably, AdaptivFloat is seen surpassing baseline FP32 performance by up to +0.3 in BLEU score and -0.75 in word error rate at weight bit widths that are $\\leq$ 8-bit. Experimental results on a deep neural network (DNN) hardware accelerator, exploiting AdaptivFloat logic in its computational datapath, demonstrate per-operation energy and area that is 0.9$\\times$ and 1.14$\\times$, respectively, that of equivalent bit width integer-based accelerator variants.", "venue": "ArXiv", "authors": ["Thierry  Tambe", "En-Yu  Yang", "Zishen  Wan", "Yuntian  Deng", "Vijay Janapa Reddi", "Alexander  Rush", "David  Brooks", "Gu-Yeon  Wei"], "year": 2019, "n_citations": 15}
{"id": 1366297, "s2_id": "4efa1679620d87693ddf0467232596ab2c6da044", "title": "Reversible Squaring Circuit For Low Power Digital Signal Processing", "abstract": "With the high demand of low power digital systems, energy dissipation in the digital system is one of the limiting factors. Reversible logic is one of the alternate to reduce heat/energy dissipation in the digital circuits and have a very significant importance in bioinformatics, optical information processing, CMOS design etc. In this paper the authors propose the design of new 2- bit binary Squaring circuit used in most of the digital signal processing hardware using Feynman & MUX gate. The proposed squaring circuit having less garbage outputs, constant inputs, Quantum cost and Total logical calculation i.e. less delay as compared to the traditional method of squaring operation by reversible multiplier. The simulating results and quantized results are also shown in the paper which shows the greatest improvement in the design against the previous methodology.", "venue": "ArXiv", "authors": ["Pradeep  Singla", "Devraj  Gautam"], "year": 2014, "n_citations": 1}
{"id": 1366345, "s2_id": "c11a11d98838927428dfde57e00295769d61c55d", "title": "Allocating the chains of consecutive additions for optimal fixed-point data path synthesis", "abstract": "Minimization of computational errors in the fixed-point data path is often difficult task. Many signal processing algorithms use chains of consecutive additions. The analyzing technique that can be applied to fixed-point data path synthesis has been proposed. This technique takes advantage of allocating the chains of consecutive additions in order to predict growing width of the data path and minimize the design complexity and computational errors.", "venue": "2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["Ilya Y. Zhbannikov", "Gregory W. Donohoe"], "year": 2012, "n_citations": 1}
{"id": 1367130, "s2_id": "4c77fb4565922930fe97d699250949e693c409f6", "title": "Where to Encode: A Performance Analysis of x86 and Arm-based Amazon EC2 Instances", "abstract": "Video streaming became an undivided part of the Internet. To efficiently utilise the limited network bandwidth it is essential to encode the video content. However, encoding is a computationally intensive task, involving high-performance resources provided by private infrastructures or public clouds. Public clouds, such as Amazon EC2, provide a large portfolio of services and instances optimized for specific purposes and budgets. The majority of Amazon\u2019s instances use x86 processors, such as Intel Xeon or AMD EPYC. However, following the recent trends in computer architecture, Amazon introduced Arm-based instances that promise up to 40% better cost performance ratio than comparable x86 instances for specific workloads. We evaluate in this paper the video encoding performance of x86 and Arm instances of four instance families using the latest FFmpeg version and two video codecs. We examine the impact of the encoding parameters, such as different presets and bitrates, on the time and cost for encoding. Our experiments reveal that Arm instances show high time and cost saving potential of up to 33.63% for specific bitrates and presets, especially for the x264 codec. However, the x86 instances are more general and achieve low encoding times, regardless of the codec.", "venue": "2021 IEEE 17th International Conference on eScience (eScience)", "authors": ["Roland  Math'a", "Dragi  Kimovski", "Anatoliy  Zabrovskiy", "Christian  Timmerer", "Radu  Prodan"], "year": 2021, "n_citations": 0}
{"id": 1371006, "s2_id": "cb7771aae6fa2b4ed42c09f8ed6f749098eefcd5", "title": "Early Output Hybrid Input Encoded Asynchronous Full Adder and Relative-Timed Ripple Carry Adder", "abstract": "This paper presents a new early output hybrid input encoded asynchronous full adder designed using dual-rail and 1-of-4 delay-insensitive data codes. The proposed full adder when cascaded to form a ripple carry adder (RCA) necessitates the use of a small relative-timing assumption with respect to the internal carries, which is independent of the RCA size. The forward latency of the proposed hybrid input encoded full adder based RCA is data-dependent while its reverse latency is the least equaling the propagation delay of just one full adder. Compared to the best of the existing hybrid input encoded full adders based 32-bit RCAs, the proposed early output hybrid input encoded full adder based 32-bit RCA enables respective reductions in forward latency and area by 7.9% and 5.6% whilst dissipating the same average power; in terms of the theoretically computed cycle time, the latter reports a 10.9% reduction compared to the former.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "K.  Prasad"], "year": 2016, "n_citations": 7}
{"id": 1371104, "s2_id": "b14ce06cd5a4bb9e70c489a0d1cdf4233a547519", "title": "A High-Level Reconfigurable Computing Platform Software Frameworks", "abstract": "Reconfigurable computing refers to the use of processors, such as Field Programmable Gate Arrays (FPGAs), that can be modified at the hardware level to take on different processing tasks. A reconfigurable computing platform describes the hardware and software base on top of which modular extensions can be created, depending on the desired application. Such reconfigurable computing platforms can take on varied designs and implementations, according to the constraints imposed and features desired by the scope of applications. This paper introduces a PC-based reconfigurable computing platform software frameworks that is flexible and extensible enough to abstract the different hardware types and functionality that different PCs may have. The requirements of the software platform, architectural issues addressed, rationale behind the decisions made, and frameworks design implemented are discussed.", "venue": "ArXiv", "authors": ["Darran  Nathan", "Kelvin Lim Mun Kit", "Kelly Choo Hon Min", "Philip Wong Jit Chin", "Andreas  Weisensee"], "year": 2004, "n_citations": 2}
{"id": 1371132, "s2_id": "f75414646fcef34a18bee7129d34f9626b635336", "title": "Elasticlave: An Efficient Memory Model for Enclaves", "abstract": "Trusted-execution environments (TEE), like Intel SGX, isolate user-space applications into secure enclaves without trusting the OS. Thus, TEEs reduce the trusted computing base, but add one to two orders of magnitude slow-down. The performance cost stems from a strict memory model, which we call the spatial isolation model, where enclaves cannot share memory regions with each other. In this work, we present Elasticlave---a new TEE memory model that allows enclaves to selectively and temporarily share memory with other enclaves and the OS. Elasticlave eliminates the need for expensive data copy operations, while offering the same level of application-desired security as possible with the spatial model. We prototype Elasticlave design on an RTL-designed cycle-level RISC-V core and observe 1 to 2 orders of magnitude performance improvements over the spatial model implemented with the same processor configuration. Elasticlave has a small TCB. We find that its performance characteristics and hardware area footprint scale well with the number of shared memory regions it is configured to support.", "venue": "ArXiv", "authors": ["Zhijingcheng  Yu", "Shweta  Shinde", "Trevor E. Carlson", "Prateek  Saxena"], "year": 2020, "n_citations": 3}
{"id": 1374153, "s2_id": "06cf46de57bf82559ba67ad4f6e8e1096f7fdef3", "title": "GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent", "abstract": "In this paper, we present GradPIM, a processingin-memory architecture which accelerates parameter updates of deep neural networks training. As one of processing-in-memory techniques that could be realized in the near future, we propose an incremental, simple architectural design that does not invade the existing memory protocol. Extending DDR4 SDRAM to utilize bank-group parallelism makes our operation designs in processing-in-memory (PIM) module efficient in terms of hardware cost and performance. Our experimental results show that the proposed architecture can improve the performance of DNN training and greatly reduce memory bandwidth requirement while posing only a minimal amount of overhead to the protocol and DRAM area.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Heesu  Kim", "Hanmin  Park", "Taehyun  Kim", "Kwanheum  Cho", "Eojin  Lee", "Soojung  Ryu", "Hyuk-Jae  Lee", "Kiyoung  Choi", "Jinho  Lee"], "year": 2021, "n_citations": 2}
{"id": 1374248, "s2_id": "f55f6a67209fd95b3a0505641e76aa43bfcabb57", "title": "CAP-RAM: A Charge-Domain In-Memory Computing 6T-SRAM for Accurate and Precision-Programmable CNN Inference", "abstract": "A compact, accurate, and bitwidth-programmable in-memory computing (IMC) static random-access memory (SRAM) macro, named CAP-RAM, is presented for energy-efficient convolutional neural network (CNN) inference. It leverages a novel charge-domain multiply-and-accumulate (MAC) mechanism and circuitry to achieve superior linearity under process variations compared to conventional IMC designs. The adopted semi-parallel architecture efficiently stores filters from multiple CNN layers by sharing eight standard 6T SRAM cells with one charge-domain MAC circuit. Moreover, up to six levels of bit-width of weights with two encoding schemes and eight levels of input activations are supported. A 7-bit charge-injection SAR (ciSAR) analog-to-digital converter (ADC) getting rid of sample and hold (S&H) and input/reference buffers further improves the overall energy efficiency and throughput. A 65-nm prototype validates the excellent linearity and computing accuracy of CAP-RAM. A single $512\\times 128$ macro stores a complete pruned and quantized CNN model to achieve 98.8% inference accuracy on the MNIST data set and 89.0% on the CIFAR-10 data set, with a 573.4-giga operations per second (GOPS) peak throughput and a 49.4-tera operations per second (TOPS)/W energy efficiency.", "venue": "IEEE Journal of Solid-State Circuits", "authors": ["Zhiyu  Chen", "Zhanghao  Yu", "Qing  Jin", "Yan  He", "Jingyu  Wang", "Sheng  Lin", "Dai  Li", "Yanzhi  Wang", "Kaiyuan  Yang"], "year": 2021, "n_citations": 1}
{"id": 1376083, "s2_id": "108629f1358f3478ce308ca364be1e529f80b446", "title": "TDO-CIM: Transparent Detection and Offloading for Computation In-memory", "abstract": "Computation in-memory is a promising non-von Neumann approach aiming at completely diminishing the data transfer to and from the memory subsystem. Although a lot of architectures have been proposed, compiler support for such architectures is still lagging behind. In this paper, we close this gap by proposing an end-to-end compilation flow for in-memory computing based on the LLVM compiler infrastructure. Starting from sequential code, our approach automatically detects, opti-mizes, and offloads kernels suitable for in-memory acceleration. We demonstrate our compiler tool-flow on the PolyBench/C benchmark suite and evaluate the benefits of our proposed in-memory architecture simulated in Gem5 by comparing it with a state-of-the-art von Neumann architecture.", "venue": "2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Kanishkan  Vadivel", "Lorenzo  Chelini", "Ali  BanaGozar", "Gagandeep  Singh", "Stefano  Corda", "Roel  Jordans", "Henk  Corporaal"], "year": 2020, "n_citations": 3}
{"id": 1385152, "s2_id": "d0d3d93406c98311498807a894f4f9b8004f9e50", "title": "Aging-Aware Request Scheduling for Non-Volatile Main Memory", "abstract": "Modern computing systems are embracing non-volatile memory (NVM) to implement high-capacity and low-cost main memory. Elevated operating voltages of NVM accelerate the aging of CMOS transistors in the peripheral circuitry of each memory bank. Aggressive device scaling increases power density and temperature, which further accelerates aging, challenging the reliable operation of NVM-based main memory. We propose HEBE, an architectural technique to mitigate the circuit aging-related problems of NVM-based main memory. HEBE is built on three contributions. First, we propose a new analytical model that can dynamically track the aging in the peripheral circuitry of each memory bank based on the bank\u2019s utilization. Second, we develop an intelligent memory request scheduler that exploits this aging model at run time to de-stress the peripheral circuitry of a memory bank only when its aging exceeds a critical threshold. Third, we introduce an isolation transistor to decouple parts of a peripheral circuit operating at different voltages, allowing the decoupled logic blocks to undergo long-latency de-stress operations independently and off the critical path of memory read and write accesses, improving performance. We evaluate HEBE with workloads from the SPEC CPU2017 Benchmark suite. Our results show that HEBE significantly improves both performance and lifetime of NVM-based main memory.", "venue": "2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Shihao  Song", "Anup  Das", "Onur  Mutlu", "Nagarajan  Kandasamy"], "year": 2021, "n_citations": 15}
{"id": 1389416, "s2_id": "fe8e30310a6ed07cfd0cd85551c35a246f95aa8e", "title": "ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network Design in FPGA-Based Systems", "abstract": "The high computational complexity, memory footprints, and energy requirements of machine learning models, such as Artificial Neural Networks (ANNs), hinder their deployment on resource-constrained embedded systems. Most state-of-the-art works have considered this problem by proposing various low bit-width data representation schemes and optimized arithmetic operators\u2019 implementations. To further elevate the implementation gains offered by these individual techniques, there is a need to cross-examine and combine these techniques\u2019 unique features. This paper presents ExPAN(N)D, a framework to analyze and ingather the efficacy of the Posit number representation scheme and the efficiency of fixed-point arithmetic implementations for ANNs. The Posit scheme offers a better dynamic range and higher precision for various applications than IEEE 754 single-precision floating-point format. However, due to the dynamic nature of the various fields of the Posit scheme, the corresponding arithmetic circuits have higher critical path delay and resource requirements than the single-precision-based arithmetic units. Towards this end, we propose a novel Posit to fixed-point converter for enabling high-performance and energy-efficient hardware implementations for ANNs with minimal drop in the output accuracy. We also propose a modified Posit-based representation to store the trained parameters of a network. With the proposed Posit to fixed-point converter-based designs, we provide multiple design points with varying accuracy-performance trade-offs for an ANN. For instance, compared to the lowest power dissipating Posit-only accelerator design, one of our proposed designs results in 80% and 48% reduction in power dissipation and LUT utilization respectively, with marginal increase in classification error for Imagenet dataset classification using VGG-16.", "venue": "IEEE Access", "authors": ["Suresh  Nambi", "Salim  Ullah", "Aditya  Lohana", "Siva Satyendra Sahoo", "Farhad  Merchant", "Akash  Kumar"], "year": 2021, "n_citations": 4}
{"id": 1389595, "s2_id": "42823d3b9291146b5cc118f3f8f91bb4305ae6d2", "title": "Near-Chip Dynamic Vision Filtering for Low-Bandwidth Pedestrian Detection", "abstract": "This paper presents a novel end-to-end system for pedestrian detection using Dynamic Vision Sensors (DVSs). We target applications where multiple sensors transmit data to a local processing unit, which executes a detection algorithm. Our system is composed of (i) a near-chip event filter that compresses and denoises the event stream from the DVS, and (ii) a Binary Neural Network (BNN) detection module that runs on a low-computation edge computing device (in our case a STM32F4 microcontroller). We present the system architecture and provide an end-to-end implementation for pedestrian detection in an office environment. Our implementation reduces transmission size by up to 99.6% compared to transmitting the raw event stream. Our detector is able to perform a detection every 450 ms, with an overall testing F1 score of 83%. The low bandwidth and energy properties of our system make it ideal for IoT applications.", "venue": "2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Anthony  Bisulco", "Fernando Cladera Ojeda", "Volkan  Isler", "Daniel D. Lee"], "year": 2020, "n_citations": 0}
{"id": 1389981, "s2_id": "573b449a97b2e4d2d1608c9b973a0bcb36772872", "title": "Practical Byte-Granular Memory Blacklisting using Califorms", "abstract": "Recent rapid strides in memory safety tools and hardware have improved software quality and security. While coarse-grained memory safety has improved, achieving memory safety at the granularity of individual objects remains a challenge due to high performance overheads usually between ~1.7x--2.2x. In this paper, we present a novel idea called Califorms, and associated program observations, to obtain a low overhead security solution for practical, byte-granular memory safety. The idea we build on is called memory blacklisting, which prohibits a program from accessing certain memory regions based on program semantics. State of the art hardware-supported memory blacklisting, while much faster than software blacklisting, creates memory fragmentation (on the order of few bytes) for each use of the blacklisted location. We observe that metadata used for blacklisting can be stored in dead spaces in a program's data memory and that this metadata can be integrated into the microarchitecture by changing the cache line format. Using these observations, a Califorms based system proposed in this paper reduces the performance overheads of memory safety to ~1.02x--1.16x while providing byte-granular protection and maintaining very low hardware overheads. Moreover, the fundamental idea of storing metadata in empty spaces and changing cache line formats can be used for other security and performance applications.", "venue": "MICRO", "authors": ["Hiroshi  Sasaki", "Miguel A. Arroyo", "M. Tarek Ibn Ziad", "Koustubha  Bhat", "Kanad  Sinha", "Simha  Sethumadhavan"], "year": 2019, "n_citations": 12}
{"id": 1393695, "s2_id": "ed19e1b38c3effaea5538b48de214bed574279e0", "title": "Monitoring Large Crowds With WiFi: A Privacy-Preserving Approach", "abstract": "This paper presents a crowd monitoring system based on the passive detection of probe requests. The system meets strict privacy requirements and is suited to monitoring events or buildings with a least a few hundreds of attendees. We present our counting process and an associated mathematical model. From this model, we derive a concentration inequality that highlights the accuracy of our crowd count estimator. Then, we describe our system. We present and discuss our sensor hardware, our computing system architecture, and an efficient implementation of our counting algorithm---as well as its space and time complexity. We also show how our system ensures the privacy of people in the monitored area. Finally, we validate our system using nine weeks of data from a public library endowed with a camera-based counting system, which generates counts against which we compare those of our counting system. This comparison empirically quantifies the accuracy of our counting system, thereby showing it to be suitable for monitoring public areas. Similarly, the concentration inequality provides a theoretical validation of the system.", "venue": "ArXiv", "authors": ["Jean-Fran\u00e7ois  Determe", "Sophia  Azzagnuni", "Utkarsh  Singh", "Fran\u00e7ois  Horlin", "Philippe De Doncker"], "year": 2020, "n_citations": 0}
{"id": 1394501, "s2_id": "e97163e5c17a7ac972c1f502c5c99a05d59aa564", "title": "FPGA impementation of erasure-only reed Solomon decoders for hybrid-ARQ systems", "abstract": "This paper presents the usage of the Reed Solomon Codes as the Forward Error Correction (FEC) unit of the Hybrid Automatic Repeat Request (ARQ) methods. Parametric and flexible FPGA implementation details of such Erasure-Only RS decoders with high symbol lengths (e.g. GF(232)) have been presented. The design is based on the GF(2m) multiplier logic core operating at a single clock cycle, where the resource utilization and throughput are both directly proportional to the number of these cores. For a fixed implementation, the throughput inversely decreases with the number of erasures to be corrected. Implementation in Zynq7020 SoC device of an example GF(232)-RS Decoder capable of correcting 64-erasures with a single multiplier resulted in 1641-LUTs and 188-FFs achieving 15Mbps, whereas the design with 8 multipliers resulted in 6128-LUTs and 628-FFs achieving 100Mbps.", "venue": "2016 24th Signal Processing and Communication Application Conference (SIU)", "authors": ["Cansu  Sen", "Soner  Yesil", "Ertugrul  Kolagasioglu"], "year": 2016, "n_citations": 0}
{"id": 1394525, "s2_id": "a111b81cfd2f47412084cad7941ddd62eed2b4d2", "title": "Bright-field AAPSM conflict detection and correction", "abstract": "As feature sizes shrink, it will be necessary to use AAPSM (alternating-aperture phase shift masking) to image critical features, especially on the polysilicon layer. This imposes additional constraints on the layouts beyond traditional design rules. Of particular note is the requirement that all critical features be flanked by opposite-phase shifters, while the shifters obey minimum width and spacing requirements. A layout is called phase-assignable if it satisfies this requirement. If a layout is not phase-assignable, the phase conflicts have to be removed to enable the use of AAPSM for the layout. Previous work has sought to detect a suitable set of phase conflicts to be removed, as well as correct them. The contributions of this paper are the following: (1) a new approach to detect a minimal set of phase conflicts (also referred to as AAPSM conflicts), which when corrected will produce a phase-assignable layout; (2) a novel layout modification scheme for correcting these AAPSM conflicts. The proposed approach for conflict detection shows significant improvements in the quality of results and runtime for real industrial circuits, when compared to previous methods. To the best of our knowledge, this is the first time layout modification results are presented for bright-field AAPSM. Our experiments show that the percentage area increase for making a layout phase-assignable ranges from 0.7-11.8%.", "venue": "Design, Automation and Test in Europe", "authors": ["Charles C. Chiang", "Andrew B. Kahng", "Subarna  Sinha", "Xu  Xu", "Alex  Zelikovsky"], "year": 2005, "n_citations": 13}
{"id": 1394778, "s2_id": "ea6fa81df014ca1aa849241edd35250863fad7a3", "title": "Accelerating Genome Sequence Analysis via Efficient Hardware/Algorithm Co-Design", "abstract": "Genome sequence analysis plays a pivotal role in enabling many medical and scientific advancements in personalized medicine, outbreak tracing, the understanding of evolution, and forensics. Modern genome sequencing machines can rapidly generate massive amounts of genomics data at low cost. However, the analysis of genome sequencing data is currently bottlenecked by the computational power and memory bandwidth limitations of existing systems, as many of the steps in genome sequence analysis must process a large amount of data. Our goals in this dissertation are to (1) characterize the real-system behavior of the genome sequence analysis pipeline and its associated tools, (2) expose the bottlenecks and tradeoffs of the pipeline and tools, and (3) co-design fast and efficient algorithms along with scalable and energy-efficient customized hardware accelerators for the key pipeline bottlenecks to enable faster genome sequence analysis. First, we comprehensively analyze the tools in the genome assembly pipeline for long reads in multiple dimensions (i.e., accuracy, performance, memory usage, and scalability), uncovering bottlenecks and tradeoffs that different combinations of tools and different underlying systems lead to. We show that we need high-performance, memory-efficient, low-power, and scalable designs for genome sequence analysis in order to exploit the advantages that genome sequencing provides. Second, we propose GenASM, an acceleration framework that builds upon bitvector-based approximate string matching (ASM) to accelerate multiple steps of the genome sequence analysis pipeline. We co-design our highly-parallel, scalable and memory-efficient algorithms with low-power and area-efficient hardware accelerators. We evaluate GenASM for three different use cases of ASM in genome sequence analysis and show that GenASM is significantly faster and more powerand area-efficient than state-of-the-art software and hardware tools for each of these use cases. Third, we implement an FPGA-based prototype for GenASM, where state-of-the-art 3D-stacked memory (HBM2) offers high memory bandwidth and FPGA resources offer high parallelism by instantiating multiple copies of the GenASM accelerators. Fourth, we propose GenGraph, the first hardware acceleration framework for sequence-to-graph mapping. Instead of representing the reference genome as a single linear DNA sequence, genome graphs provide a better representation of the diversity among populations by encoding variations across individuals in a graph data structure, avoiding a bias towards any one reference. GenGraph enables the efficient mapping of a sequenced genome to a graph-based reference, providing more comprehensive and accurate genome sequence analysis. Overall, we demonstrate that genome sequence analysis can be accelerated by codesigning scalable and energy-efficient customized accelerators along with efficient algorithms for the key steps of genome sequence analysis.", "venue": "ArXiv", "authors": ["Damla Senol Cali"], "year": 2021, "n_citations": 0}
{"id": 1395311, "s2_id": "68e99fe4109c563fc07b560fc04830fcb54ac4bb", "title": "Quantum Cost Efficient Reversible BCD Adder for Nanotechnology Based Systems", "abstract": "Reversible logic allows low power dissipating circuit design and founds its application in cryptography, digital signal processing, quantum and optical information processing. This paper presents a novel quantum cost efficient reversible BCD adder for nanotechnology based systems using PFAG gate. It has been demonstrated that the proposed design offers less hardware complexity and requires minimum number of garbage outputs than the existing counterparts. The remarkable property of the proposed designs is that its quantum realization is given in NMR technology.", "venue": "ArXiv", "authors": ["Md. Saiful Islam", "Mohd. Zulfiquar Hafiz", "Zerina  Begum"], "year": 2011, "n_citations": 6}
{"id": 1400894, "s2_id": "0f87f3ac5030288835b7418c0330687dbeba583d", "title": "Hardware Trojan with Frequency Modulation", "abstract": "The use of third-party IP cores in implementing applications in FPGAs has given rise to the threat of malicious alterations through the insertion of hardware Trojans. To address this threat, it is important to predict the way hardware Trojans are built and to identify their weaknesses. This paper describes a logic family for implementing robust hardware Trojans, which can evade the two major detection methods, namely unused-circuit identification and side-channel analysis. This robustness is achieved by encoding information in frequency rather than amplitude so that the Trojan trigger circuitry's state will never stay constant during \u2018normal\u2019 operation. In addition, the power consumption of Trojan circuits built using the proposed logic family can be concealed with minimal design effort and supplementary hardware resources. Defense measures against hardware Trojans with frequency modulation are described.", "venue": "2021 IEEE 12th Latin America Symposium on Circuits and System (LASCAS)", "authors": ["Ash  Luft", "Mihai  Sima", "Michael  McGuire"], "year": 2021, "n_citations": 0}
{"id": 1401003, "s2_id": "9c28f42520bd429dfb1813929e5c07f77c533ae1", "title": "New Models for Understanding and Reasoning about Speculative Execution Attacks", "abstract": "Spectre and Meltdown attacks and their variants exploit hardware performance optimization features to cause security breaches. Secret information is accessed and leaked through covert or side channels. New attack variants keep appearing and we do not have a systematic way to capture the critical characteristics of these attacks and evaluate why they succeed or fail.In this paper, we provide a new attack-graph model for reasoning about speculative execution attacks. We model attacks as ordered dependency graphs, and prove that a race condition between two nodes can occur if there is a missing dependency edge between them. We define a new concept, \u201csecurity dependency\u201d, between a resource access and its prior authorization operation. We show that a missing security dependency is equivalent to a race condition between authorization and access, which is a root cause of speculative execution attacks. We show detailed examples of how our attack graph models the Spectre and Meltdown attacks, and is generalizable to all the attack variants published so far. This attack model is also very useful for identifying new attacks and for generalizing defense strategies. We identify several defense strategies with different performance-security tradeoffs. We show that the defenses proposed so far all fit under one of our defense strategies. We also explain how attack graphs can be constructed and point to this as promising future work for tool designers.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Zecheng  He", "Guangyuan  Hu", "Ruby  Lee"], "year": 2021, "n_citations": 6}
{"id": 1401897, "s2_id": "2182c21985f749d0557004c4ca0142310f46a1e8", "title": "Exploiting inter- and intra-memory asymmetries for data mapping in hybrid tiered-memories", "abstract": "Modern computing systems are embracing hybrid memory comprising of DRAM and non-volatile memory (NVM) to combine the best properties of both memory technologies, achieving low latency, high reliability, and high density. A prominent characteristic of DRAM-NVM hybrid memory is that it has NVM access latency much higher than DRAM access latency. We call this inter-memory asymmetry. We observe that parasitic components on a long bitline are a major source of high latency in both DRAM and NVM, and a significant factor contributing to high-voltage operations in NVM, which impact their reliability. We propose an architectural change, where each long bitline in DRAM and NVM is split into two segments by an isolation transistor. One segment can be accessed with lower latency and operating voltage than the other. By introducing tiers, we enable non-uniform accesses within each memory type (which we call intra-memory asymmetry), leading to performance and reliability trade-offs in DRAM-NVM hybrid memory. We show that our hybrid tiered-memory architecture has a tremendous potential to improve performance and reliability, if exploited by an efficient page management policy at the operating system (OS). Modern OSes are already aware of inter-memory asymmetry. They migrate pages between the two memory types during program execution, starting from an initial allocation of the page to a randomly-selected free physical address in the memory. We extend existing OS awareness in three ways. First, we exploit both inter- and intra-memory asymmetries to allocate and migrate memory pages between the tiers in DRAM and NVM. Second, we improve the OS\u2019s page allocation decisions by predicting the access intensity of a newly-referenced memory page in a program and placing it to a matching tier during its initial allocation. This minimizes page migrations during program execution, lowering the performance overhead. Third, we propose a solution to migrate pages between the tiers of the same memory without transferring data over the memory channel, minimizing channel occupancy and improving performance. Our overall approach, which we call MNEME, to enable and exploit asymmetries in DRAM-NVM hybrid tiered memory improves both performance and reliability for both single-core and multi-programmed workloads.", "venue": "ISMM", "authors": ["Shihao  Song", "Anup  Das", "Nagarajan  Kandasamy"], "year": 2020, "n_citations": 17}
{"id": 1403207, "s2_id": "986a19ec7063422dc0456923b56a1505bc660eca", "title": "Architectural Analysis of FPGA Technology Impact", "abstract": "The use of high-level languages for designing hardware is gaining popularity since they increase design productivity by providing higher abstractions. However, one drawback of such abstraction level has been the difficulty of relating the low-level implementation problems back to the original high-level design, which is paramount for architectural optimization. In this work (developed between April 2013 and April 2014), we propose a methodology to analyze the effects of technology over the architecture, and to generate architectural-level area, delay and power metrics. Such feedback allows the designer to quickly gauge the impact of architectural decisions on the quality of generated hardware and opens the door to automatic architectural analysis. We demonstrate the use of our technique on three FPGA platforms using two designs: a Reed-Solomon error correction decoder and a 32-bit pipelined processor implementation.", "venue": "ArXiv", "authors": ["Oriol  Arcas", "Abhinav  Agarwal"], "year": 2020, "n_citations": 0}
{"id": 1406552, "s2_id": "84b4e9c48404f1b7bb80f87fcccd3a2362a61a0a", "title": "A Novel Methodology for Thermal Analysis & 3-Dimensional Memory Integration", "abstract": "The semiconductor industry is reaching a fascinating confluence in several evolutionary trends that will likely lead to a number of revolutionary changes in the design, implementation, scaling, and the use of computer systems. However, recently Moore's law has come to a stand-still since device scaling beyond 65 nm is not practical. 2D integration has problems like memory latency, power dissipation, and large foot-print. 3D technology comes as a solution to the problems posed by 2D integration. The utilization of 3D is limited by the problem of temperature crisis. It is important to develop an accurate power profile extraction methodology to design 3D structure. In this paper, design of 3D integration of memory is considered and hence the static power dissipation of the memory cell is analysed in transistor level and is used to accurately model the inter-layer thermal effects for 3D memory stack. Subsequently, packaging of the chip is considered and modelled using an architecture level simulator. This modelling is intended to analyse the thermal effects of 3D memory, its reliability and lifetime of the chip, with greater accuracy.", "venue": "ArXiv", "authors": ["Annmol  Cherian", "Ajay  Augustine", "Jemy  Jose", "Vinod  Pangracious"], "year": 2011, "n_citations": 2}
{"id": 1407995, "s2_id": "83002f8953f97f3bbecde969b70071da0b517474", "title": "RASA: Efficient Register-Aware Systolic Array Matrix Engine for CPU", "abstract": "As AI-based applications become pervasive, CPU vendors are starting to incorporate matrix engines within the datapath to boost efficiency. Systolic arrays have been the premier architectural choice as matrix engines in offload accelerators. However, we demonstrate that incorporating them inside CPUs can introduce under-utilization and stalls due to limited register storage to amortize the fill and drain times of the array. To address this, we propose RASA, Register-Aware Systolic Array. We develop techniques to divide an execution stage into several sub-stages and overlap instructions to hide overheads and run them concurrently. RASA-based designs improve performance significantly with negligible area and power overhead.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Geonhwa  Jeong", "Eric  Qin", "Ananda  Samajdar", "Christopher J. Hughes", "Sreenivas  Subramoney", "Hyesoon  Kim", "Tushar  Krishna"], "year": 2021, "n_citations": 0}
{"id": 1409969, "s2_id": "9a41e3be3067831cdb0a62fc6d5cbcd8a781efef", "title": "Tetris: Re-architecting Convolutional Neural Network Computation for Machine Learning Accelerators", "abstract": "Inference efficiency is the predominant consideration in designing deep learning accelerators. Previous work mainly focuses on skipping zero values to deal with remarkable ineffectual computation, while zero bits in non-zero values, as another major source of ineffectual computation, is often ignored. The reason lies on the difficulty of extracting essential bits during operating multiply-and-accumulate (MAC) in the processing element. Based on the fact that zero bits occupy as high as 68.9% fraction in the overall weights of modern deep convolutional neural network models, this paper firstly proposes a weight kneading technique that could eliminate ineffectual computation caused by either zero value weights or zero bits in non-zero weights, simultaneously. Besides, a split-and-accumulate (SAC) computing pattern in replacement of conventional MAC, as well as the corresponding hardware accelerator design called Tetris are proposed to support weight kneading at the hardware level. Experimental results prove that Tetris could speed up inference up to 1.50x, and improve power efficiency up to 5.33x compared with the state-of-the-art baselines.", "venue": "2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Hang  Lu", "Xin  Wei", "Ning  Lin", "Guihai  Yan", "Xiaowei  Li"], "year": 2018, "n_citations": 10}
{"id": 1410921, "s2_id": "3c8c28f979db20da445d9baf9fbf49522b19dff2", "title": "SysScale: Exploiting Multi-domain Dynamic Voltage and Frequency Scaling for Energy Efficient Mobile Processors", "abstract": "There are three domains in a modern thermally-constrained mobile system-on-chip (SoC): compute, IO, and memory. We observe that a modern SoC typically allocates a fixed power budget, corresponding to worst-case performance demands, to the IO and memory domains even if they are underutilized. The resulting unfair allocation of the power budget across domains can cause two major issues: 1) the IO and memory domains can operate at a higher frequency and voltage than necessary, increasing power consumption and 2) the unused power budget of the IO and memory domains cannot be used to increase the throughput of the compute domain, hampering performance. To avoid these issues, it is crucial to dynamically orchestrate the distribution of the SoC power budget across the three domains based on their actual performance demands.We propose SysScale, a new multi-domain power management technique to improve the energy efficiency of mobile SoCs. SysScale is based on three key ideas. First, SysScale introduces an accurate algorithm to predict the performance (e.g., bandwidth and latency) demands of the three SoC domains. Second, SysScale uses a new DVFS (dynamic voltage and frequency scaling) mechanism to distribute the SoC power to each domain according to the predicted performance demands. This mechanism is designed to minimize the significant latency overheads associated with applying DVFS across multiple domains. Third, in addition to using a global DVFS mechanism, SysScale uses domain-specialized techniques to optimize the energy efficiency of each domain at different operating points.We implement SysScale on an Intel Skylake microprocessor for mobile devices and evaluate it using a wide variety of SPEC CPU2006, graphics (3DMark), and battery life workloads (e.g., video playback). On a 2-core Skylake, SysScale improves the performance of SPEC CPU2006 and 3DMark workloads by up to 16% and 8.9% (9.2% and 7.9% on average), respectively. For battery life workloads, which typically have fixed performance demands, SysScale reduces the average power consumption by up to 10.7% (8.5% on average), while meeting performance demands.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Jawad  Haj-Yahya", "Mohammed  Alser", "Jeremie  Kim", "Abdullah Giray Yaglik\u00e7i", "Nandita  Vijaykumar", "Efraim  Rotem", "Onur  Mutlu"], "year": 2020, "n_citations": 11}
{"id": 1411413, "s2_id": "e49c412fc5b18f62508f52e41eb5a3c45703c92d", "title": "A Novel Architecture of Area Efficient FFT Algorithm for FPGA Implementation", "abstract": "Fast Fourier transform (FFT) of large number of samples requires huge hardware resources of field programmable gate arrays (FPGA), which needs more area and power. In this paper, we have presented an area efficient architecture of a FFT processor that reuses the butterfly elements several times. The FFT processor is simulated using VHDL and the results are validated on Virtex-6 FPGA. The proposed architecture out performs the conventional architecture of a N-point FFT processor in terms of area which is reduced by a factor of logN 2with negligible increase in processing time.", "venue": "CARN", "authors": ["Atin  Mukherjee", "Amitabha  Sinha", "Debesh  Choudhury"], "year": 2016, "n_citations": 6}
{"id": 1418760, "s2_id": "cab3ccc56702e4c9ee9311ac164dd53cb74b7038", "title": "FIGARO: Improving System Performance via Fine-Grained In-DRAM Data Relocation and Caching", "abstract": "Main memory, composed of DRAM, is a performance bottleneck for many applications, due to the high DRAM access latency. In-DRAM caches work to mitigate this latency by augmenting regular-latency DRAM with small-but-fast regions of DRAM that serve as a cache for the data held in the regular-latency (i.e., slow) region of DRAM. While an effective in-DRAM cache can allow a large fraction of memory requests to be served from a fast DRAM region, the latency savings are often hindered by inefficient mechanisms for migrating (i.e., relocating) copies of data into and out of the fast regions. Existing in-DRAM caches have two sources of inefficiency: (1) their data relocation granularity is an entire multi-kilobyte row of DRAM, even though much of the row may never be accessed due to poor data locality; and (2) because the relocation latency increases with the physical distance between the slow and fast regions, multiple fast regions are physically interleaved among slow regions to reduce the relocation latency, resulting in increased hardware area and manufacturing complexityWe propose a new substrate, FIGARO, that uses existing shared global buffers among subarrays within a DRAM bank to provide support for in-DRAM data relocation across subar-rays at the granularity of a single cache block. FIGARO has a distance-independent latency within a DRAM bank, and avoids complex modifications to DRAM (such as the interleaving of fast and slow regions). Using FIGARO, we design a fine-grained in-DRAM cache called FIGCache. The key idea of FIGCache is to cache only small, frequently-accessed portions of different DRAM rows in a designated region of DRAM. By caching only the parts of each row that are expected to be accessed in the near future, we can pack more of the frequently-accessed data into FIGCache, and can benefit from additional row hits in DRAM (i.e., accesses to an already-open row, which have a lower latency than accesses to an unopened row). FIGCache provides benefits for systems with both heterogeneous DRAM banks (i.e., banks with fast regions and slow regions) and conventional homogeneous DRAM banks (i.e., banks with only slow regions)Our evaluations across a wide variety of applications show that FIGCache improves the average performance of a system using DDR4 DRAM by 16.3% and reduces average DRAM energy consumption by 7.8% for 8-core workloads, over a conventional system without in-DRAM caching. We show that FIGCache outperforms state-of-the-art in-DRAM caching techniques, and that its performance gains are robust across many system and mechanism parameters.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Yaohua  Wang", "Lois  Orosa", "Xiangjun  Peng", "Yang  Guo", "Saugata  Ghose", "Minesh  Patel", "Jeremie S. Kim", "Juan  G\u00f3mez-Luna", "Mohammad  Sadrosadati", "Nika  Mansouri-Ghiasi", "Onur  Mutlu"], "year": 2020, "n_citations": 19}
{"id": 1429311, "s2_id": "50cd377cebde03667700b590cd2ff05f449f35e4", "title": "Pseudo-Ring Testing Schemes and Algorithms of RAM Built-In and Embedded Self-Testing", "abstract": "Scan and ring schemes of the pseudo-ring memory selftesting are investigated. Both schemes are based on emulation of the linear or nonlinear feedback shift register by memory itself. Peculiarities of the pseudo-ring schemes implementation for multi-port and embedded memories, and for register file are described. It is shown that only small additional logic is required and allows microcontrollers at-speed testing. Also, in this article,are given the a posteriori values of some type of memories faults coverage when pseudo-ring testing schemes are applied.", "venue": "ArXiv", "authors": ["Diana  Bodean", "Ghenadie  Bodean", "Wajeb  Gharibi"], "year": 2011, "n_citations": 1}
{"id": 1430873, "s2_id": "6543d19cd9f36f8d4ce4d19c13115c00f28535af", "title": "End-User Effects of Microreboots in Three-Tiered Internet Systems", "abstract": "Microreboots restart fine-grained components of software systems \"with a clean slate,\" and only take a fraction of the time needed for full system reboot. Microreboots provide an application-generic recovery technique for Internet services, which can be supported entirely in middleware and requires no changes to the applications or any a priori knowledge of application semantics. \nThis paper investigates the effect of microreboots on end-users of an eBay-like online auction application; we find that microreboots are nearly as effective as full reboots, but are significantly less disruptive in terms of downtime and lost work. In our experiments, microreboots reduced the number of failed user requests by 65% and the perceived downtime by 78% compared to a server process restart. We also show how to replace user-visible transient failures with transparent call-retry, at the cost of a slight increase in end-user-visible latency during recovery. Due to their low cost, microreboots can be used aggressively, even when their necessity is less than certain, hence adding to the reduced recovery time a reduction in the fault detection time, which further improves availability.", "venue": "ArXiv", "authors": ["George  Candea", "Armando  Fox"], "year": 2004, "n_citations": 4}
{"id": 1431587, "s2_id": "fa2d84a8c6c58c6d56eb91989f2214fdcfde2b94", "title": "Automated Circuit Approximation Method Driven by Data Distribution", "abstract": "We propose an application-tailored data-driven fully automated method for functional approximation of combinational circuits. We demonstrate how an application-level error metric such as the classification accuracy can be translated to a component-level error metric needed for an efficient and fast search in the space of approximate low-level components that are used in the application. This is possible by employing a weighted mean error distance (WMED) metric for steering the circuit approximation process which is conducted by means of genetic programming. WMED introduces a set of weights (calculated from the data distribution measured on a selected signal in a given application) determining the importance of each input vector for the approximation process. The method is evaluated using synthetic benchmarks and application-specific approximate MAC (multiply-and-accumulate) units that are designed to provide the best trade-offs between the classification accuracy and power consumption of two image classifiers based on neural networks.", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Zdenek  Vas\u00edcek", "Vojtech  Mrazek", "Luk\u00e1s  Sekanina"], "year": 2019, "n_citations": 5}
{"id": 1432798, "s2_id": "b14e144153b4c3f7faaa82d53ca5da902ce1981a", "title": "Logic Shrinkage: Learned FPGA Netlist Sparsity for Efficient Neural Network Inference", "abstract": "FPGA-specific DNN architectures using the native LUTs as independently trainable inference operators have been shown to achieve favorable area-accuracy and energy-accuracy tradeoffs. The first work in this area, LUTNet, exhibited state-of-the-art performance for standard DNN benchmarks. In this paper, we propose the learned optimization of such LUT-based topologies, resulting in higher-efficiency designs than via the direct use of off-the-shelf, hand-designed networks. Existing implementations of this class of architecture require the manual specification of the number of inputs per LUT,K . Choosing appropriateK a priori is challenging, and doing so at even high granularity, e.g. per layer, is a time-consuming and error-prone process that leaves FPGAs\u2019 spatial flexibility underexploited. Furthermore, prior works see LUT inputs connected randomly, which does not guarantee a good choice of network topology. To address these issues, we propose logic shrinkage, a fine-grained netlist pruning methodology enabling K to be automatically learned for every LUT in a neural network targeted for FPGA inference. By removing LUT inputs determined to be of low importance, our method increases the efficiency of the resultant accelerators. Our GPU-friendly solution to LUT input removal is capable of processing large topologies during their training with negligible slowdown. With logic shrinkage, we better the area and energy efficiency of the best-performing LUTNet implementation of the CNV network classifying CIFAR-10 by 1.54\u00d7 and 1.31\u00d7, respectively, while matching its accuracy. This implementation also reaches 2.71\u00d7 the area efficiency of an equally accurate, heavily pruned BNN. On ImageNet with the Bi-Real Net architecture, employment of logic shrinkage results in a post-synthesis area reduction of 2.67\u00d7 vs LUTNet, allowing for implementation that was previously impossible on today\u2019s largest FPGAs. CCS CONCEPTS \u2022 Hardware \u2192 Reconfigurable logic and FPGAs; Logic synthesis; \u2022 Computing methodologies\u2192Machine learning. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. FPGA \u201922, February 27\u2013March 1, 2022, Virtual event. \u00a9 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9149-8/22/02. . . $15.00 https://doi.org/10.1145/3490422.3502360", "venue": "ArXiv", "authors": ["Erwei  Wang", "James J. Davis", "Georgios-Ilias  Stavrou", "Peter Y. K. Cheung", "George A. Constantinides", "Mohamed  Abdelfattah"], "year": 2021, "n_citations": 0}
{"id": 1441674, "s2_id": "8a21a2d52d3f26f8733aea127d0d1f386aa92a09", "title": "Triangle Counting Accelerations: From Algorithm to In-Memory Computing Architecture", "abstract": "Triangles are the basic substructure of networks and triangle counting (TC) has been a fundamental graph computing problem in numerous fields such as social network analysis. Nevertheless, like other graph computing problems, due to the high memory-computation ratio and random memory access pattern, TC involves a large amount of data transfers thus suffers from the bandwidth bottleneck in the traditional Von-Neumann architecture. To overcome this challenge, in this paper, we propose to accelerate TC with the emerging processing-in-memory (PIM) architecture through an algorithm-architecture co-optimization manner. To enable the efficient in-memory implementations, we come up to reformulate TC with bitwise logic operations (such as AND), and develop customized graph compression and mapping techniques for efficient data flow management. With the emerging computational Spin-Transfer Torque Magnetic RAM (STT-MRAM) array, which is one of the most promising PIM enabling techniques, the device-to-architecture co-simulation results demonstrate that the proposed TC in-memory accelerator outperforms the state-of-the-art GPU and FPGA accelerations by 12.2\u00d7 and 31.8\u00d7, respectively, and achieves a 34\u00d7 energy efficiency improvement over the FPGA accelerator.", "venue": "IEEE Transactions on Computers", "authors": ["Xueyan  Wang", "Jianlei  Yang", "Yinglin  Zhao", "Xiaotao  Jia", "Rong  Yin", "Xuhang  Chen", "Gang  Qu", "Weisheng  Zhao"], "year": 2021, "n_citations": 0}
{"id": 1443484, "s2_id": "3d482a21876dd1c7614e1d919e82a8d3aca6e3e8", "title": "SNRA: A Spintronic Neuromorphic Reconfigurable Array for In-Circuit Training and Evaluation of Deep Belief Networks", "abstract": "In this paper, a spintronic neuromorphic reconfigurable Array (SNRA)is developed to fuse together power-efficient probabilistic and in-field programmable deterministic computing during both training and evaluation phases of restricted Boltzmann machines (RBMs). First, probabilistic spin logic devices are used to develop an RBM realization which is adapted to construct deep belief networks (DBNs)having one to three hidden layers of size 10 to 800 neurons each. Second, we design a hardware implementation for the contrastive divergence (CD)algorithm using a four-state finite state machine capable of unsupervised training in N+3 clocks where N denotes the number of neurons in each RBM. The functionality of our proposed CD hardware implementation is validated using ModelSim simulations. We synthesize the developed Verilog HDL implementation of our proposed test/train control circuitry for various DBN topologies where the maximal RBM dimensions yield resource utilization ranging from 51 to 2,421 lookup tables (LUTs). Next, we leverage spin Hall effect (SHE)-magnetic tunnel junction (MTJ)based non-volatile LUTs circuits as an alternative for static random access memory (SRAM)-based LUTs storing the deterministic logic configuration to form a reconfigurable fabric. Finally, we compare the performance of our proposed SNRA with SRAM-based configurable fabrics focusing on the area and power consumption induced by the LUTs used to implement both CD and evaluation modes. The results obtained indicate more than 80% reduction in combined dynamic and static power dissipation, while achieving at least 50% reduction in device count.", "venue": "2018 IEEE International Conference on Rebooting Computing (ICRC)", "authors": ["Ramtin  Zand", "Ronald F. DeMara"], "year": 2018, "n_citations": 2}
{"id": 1445549, "s2_id": "15d72ff80525d6622b8fdce0c378066b29dab751", "title": "Synthesizing Brain-Network-Inspired Interconnections for Large-Scale Network-on-Chips", "abstract": "\n Brain network is a large-scale complex network with scale-free, small-world, and modularity properties, which largely supports this high-efficiency massive system. In this article, we propose to synthesize brain-network-inspired interconnections for large-scale network-on-chips. First, we propose a method to generate brain-network-inspired topologies with limited scale-free and power-law small-world properties, which have a low total link length and extremely low average hop count approximately proportional to the logarithm of the network size. In addition, given the large-scale applications, considering the modularity of the brain-network-inspired topologies, we present an application mapping method, including task mapping and deterministic deadlock-free routing, to minimize the power consumption and hop count. Finally, a cycle-accurate simulator\n \n BookSim2\n \n is used to validate the architecture performance with different synthetic traffic patterns and large-scale test cases, including real-world communication networks for the graph processing application. Experiments show that, compared with other topologies and methods, the brain-network-inspired network-on-chips (NoCs) generated by the proposed method present significantly lower average hop count and lower average latency. Especially in graph processing applications with a power-law and tightly coupled inter-core communication, the brain-network-inspired NoC has up to 70% lower average hop count and 75% lower average latency than mesh-based NoCs.\n", "venue": "ACM Transactions on Design Automation of Electronic Systems", "authors": ["Mengke  Ge", "Xiaobing  Ni", "Qi  Xu", "Song  Chen", "Jinglei  Huang", "Yi  Kang", "Feng  Wu"], "year": 2022, "n_citations": 0}
{"id": 1445909, "s2_id": "6dabe97a96ce27be7fe0b25b321ab808368be1c4", "title": "Behavioural transformation to improve circuit performance in high-level synthesis", "abstract": "Early scheduling algorithms usually adjusted the clock cycle duration to the execution time of the slowest operation. This resulted in large slack times wasted in those cycles executing faster operations. To reduce the wasted times multi-cycle and chaining techniques have been employed. While these techniques have produced successful designs, their effectiveness are often limited due to the area increment that may derive from chaining, and the extra latencies that may derive from multicycling. In this paper we present an optimization method that solves the time-constrained scheduling problem by transforming behavioural specifications into new ones whose subsequent synthesis substantially improves circuit performance. Our proposal breaks up some of the specification operations, allowing their execution during several possibly unconsecutive cycles, and also the calculation of several data-dependent operation fragments in the same cycle. To do so, it takes into account the circuit latency and the execution time of every specification operation. The experimental results carried out show that circuits obtained from the optimized specification are on average 60% faster than those synthesized from the original specification, with only slight increments in the circuit area.", "venue": "Design, Automation and Test in Europe", "authors": ["Rafael  Ruiz-Sautua", "Mar\u00eda C. Molina", "Jose Manuel Mendias", "Rom\u00e1n  Hermida"], "year": 2005, "n_citations": 7}
{"id": 1448129, "s2_id": "ea4c3cda2f27cc9428789321363ba46304635b80", "title": "Error Characterization, Mitigation, and Recovery in Flash-Memory-Based Solid-State Drives", "abstract": "NAND flash memory is ubiquitous in everyday life today because its capacity has continuously increased and cost has continuously decreased over decades. This positive growth is a result of two key trends: 1) effective process technology scaling; and 2) multi-level (e.g., MLC, TLC) cell data coding. Unfortunately, the reliability of raw data stored in flash memory has also continued to become more difficult to ensure, because these two trends lead to 1) fewer electrons in the flash memory cell floating gate to represent the data; and 2) larger cell-to-cell interference and disturbance effects. Without mitigation, worsening reliability can reduce the lifetime of NAND flash memory. As a result, flash memory controllers in solid-state drives (SSDs) have become much more sophisticated: they incorporate many effective techniques to ensure the correct interpretation of noisy data stored in flash memory cells. In this article, we review recent advances in SSD error characterization, mitigation, and data recovery techniques for reliability and lifetime improvement. We provide rigorous experimental data from state-of-the-art MLC and TLC NAND flash devices on various types of flash memory errors, to motivate the need for such techniques. Based on the understanding developed by the experimental characterization, we describe several mitigation and recovery techniques, including 1) cell-to-cell interference mitigation; 2) optimal multi-level cell sensing; 3) error correction using state-of-the-art algorithms and methods; and 4) data recovery when error correction fails. We quantify the reliability improvement provided by each of these techniques. Looking forward, we briefly discuss how flash memory and these techniques could evolve into the future.", "venue": "Proceedings of the IEEE", "authors": ["Yu  Cai", "Saugata  Ghose", "Erich F. Haratsch", "Yixin  Luo", "Onur  Mutlu"], "year": 2017, "n_citations": 150}
{"id": 1448433, "s2_id": "f65463c12f2fe4ed36f8a9f91f181adb2a8e1532", "title": "Diametrical Mesh Of Tree (D2D-MoT) Architecture: A Novel Routing Solution For NoC", "abstract": "Network-on-chip (NoC) is a new aspect for designing of future System-On-Chips (SoC) where a vast number of IP cores are connected through interconnection network. The communication between the nodes occurred by routing packets rather than wires. It supports high degree of scalability, reusability and parallelism in communication. In this paper, we present a Mesh routing architecture, which is called Diametrical 2D Mesh of Tree, based on Mesh-of-Tree (MoT) routing and Diametrical 2D Mesh. It has the advantage of having small diameter as well as large bisection width and small node degree clubbed with being the fastest network in terms of speed. The routing algorithm ensures that the packets will always reach from source to sink through shortest path and is deadlock free.", "venue": "ArXiv", "authors": ["Prasun  Ghosal", "Sankar  Karmakar"], "year": 2012, "n_citations": 1}
{"id": 1449762, "s2_id": "368cb72583badb4f400e84ff46ca3efeb3d738ea", "title": "CAFFEINE: template-free symbolic model generation of analog circuits via canonical form functions and genetic programming", "abstract": "The paper presents a method to generate automatically compact symbolic performance models of analog circuits with no prior specification of an equation template. The approach takes SPICE simulation data as input, which enables modeling of any nonlinear circuits and circuit characteristics. Genetic programming is applied as a means of traversing the space of possible symbolic expressions. A grammar is specially designed to constrain the search to a canonical form for functions. Novel evolutionary search operators are designed to exploit the structure of the grammar. The approach generates a set of symbolic models which collectively provide a tradeoff between error and model complexity. Experimental results show that the symbolic models generated are compact and easy to understand, making this an effective method for aiding understanding in analog design. The models also demonstrate better prediction quality than posynomials. We name the approach CAFFEINE (canonical functional form expressions in evolution).", "venue": "Design, Automation and Test in Europe", "authors": ["Trent  McConaghy", "Tom  Eeckelaert", "Georges G. E. Gielen"], "year": 2005, "n_citations": 36}
{"id": 1449986, "s2_id": "36aa18c94ceb40c47705c88e3175709d3b385fd5", "title": "Recycled Error Bits: Energy-Efficient Architectural Support for Higher Precision Floating Point", "abstract": "In this work, we provide energy-efficient architectural support for floating point accuracy. Our goal is to provide accuracy that is far greater than that provided by the processor's hardware floating point unit (FPU). Specifically, for each floating point addition performed, we \"recycle\" that operation's error: the difference between the finite-precision result produced by the hardware and the result that would have been produced by an infinite-precision FPU. We make this error architecturally visible such that it can be used, if desired, by software. Experimental results on physical hardware show that software that exploits architecturally recycled error bits can achieve accuracy comparable to a 2B-bit FPU with performance and energy that are comparable to a B-bit FPU.", "venue": "ArXiv", "authors": ["Ralph  Nathan", "Bryan  Anthonio", "Shih-Lien  Lu", "Helia  Naeimi", "Daniel J. Sorin", "Xiaobai  Sun"], "year": 2013, "n_citations": 0}
{"id": 1450663, "s2_id": "432d963aeaf0f0831d576cc6a1b5f8ce121a9507", "title": "Toward Taming the Overhead Monster for Data-Flow Integrity", "abstract": "Data-Flow Integrity (DFI) is a well-known approach to effectively detecting a wide range of software attacks. However, its real-world application has been quite limited so far because of the prohibitive performance overhead it incurs. Moreover, the overhead is enormously difficult to overcome without substantially lowering the DFI criterion. In this work, an analysis is performed to understand the main factors contributing to the overhead. Accordingly, a hardware-assisted parallel approach is proposed to tackle the overhead challenge. Simulations on SPEC CPU 2006 benchmark show that the proposed approach can completely enforce the DFI defined in the original seminal work while reducing performance overhead by 4\u00d7, on average.", "venue": "ACM Transactions on Design Automation of Electronic Systems", "authors": ["Lang  Feng", "Jiayi  Huang", "Jeff  Huang", "Jiang  Hu"], "year": 2022, "n_citations": 0}
{"id": 1451036, "s2_id": "d69a4636d061ffbb102e3cefc47bd3c32f213f31", "title": "Evaluation of Hybrid Run-Time Power Models for the ARM Big.LITTLE Architecture", "abstract": "Heterogeneous processors, formed by binary compatible CPU cores with different microarchitectures, enable energy reductions by better matching processing capabilities and software application requirements. This new hardware platform requires novel techniques to manage power and energy to fully utilize its capabilities, particularly regarding the mapping of workloads to appropriate cores. In this paper we validate relevant published work related to power modelling for heterogeneous systems and propose a new approach for developing run-time power models that uses a hybrid set of physical predictors, performance events and CPU state information. We demonstrate the accuracy of this approach compared with the state-of-the-art and its applicability to energy aware scheduling. Our results are obtained on a commercially available platform built around the Samsung Exynos 5 Octa SoC, which features the ARM big.LITTLE heterogeneous architecture.", "venue": "2015 IEEE 13th International Conference on Embedded and Ubiquitous Computing", "authors": ["Krastin  Nikov", "Jos\u00e9 L. N\u00fa\u00f1ez-Y\u00e1\u00f1ez", "Matthew  Horsnell"], "year": 2015, "n_citations": 11}
{"id": 1454640, "s2_id": "59fb8a6dea2cf38981ef0ba40bc9488c21661ed0", "title": "ALIGN: A System for Automating Analog Layout", "abstract": "<italic>Editor\u2019s note:</italic> This article describes a correct by construction approach to synthesize electrically and designs compliant design. By taking advantage of layout hierarchies they are able to apply this to an interesting class of circuits. \u2014 <italic>Sherief Reda, Brown University</italic> \u2014 <italic>Leon Stock, IBM</italic> \u2014 <italic>Pierre-Emmanuel Gaillardon, University of Utah</italic>", "venue": "IEEE Design & Test", "authors": ["Tonmoy  Dhar", "Kishor  Kunal", "Yaguang  Li", "Meghna  Madhusudan", "Jitesh  Poojary", "Arvind K. Sharma", "Wenbin  Xu", "Steven M. Burns", "Ramesh  Harjani", "Jiang  Hu", "Desmond A. Kirkpatrick", "Parijat  Mukherjee", "Soner  Yaldiz", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 6}
{"id": 1457938, "s2_id": "2e2f166ae3eab38f97a7c1ab6b35cc2ac3abd048", "title": "IceClave: A Trusted Execution Environment for In-Storage Computing", "abstract": "In-storage computing with modern solid-state drives (SSDs) enables developers to offload programs from the host to the SSD. It has been proven to be an effective approach to alleviate the I/O bottleneck. To facilitate in-storage computing, many frameworks have been proposed. However, few of them treat the in-storage security as the first citizen. Specifically, since modern SSD controllers do not have a trusted execution environment, an offloaded (malicious) program could steal, modify, and even destroy the data stored in the SSD. In this paper, we first investigate the attacks that could be conducted by offloaded in-storage programs. To defend against these attacks, we build a lightweight trusted execution environment, named IceClave for in-storage computing. IceClave enables security isolation between in-storage programs and flash management functions that include flash address translation, data access control, and garbage collection, with TrustZone extensions. IceClave also achieves security isolation between in-storage programs by enforcing memory integrity verification of in-storage DRAM with low overhead. To protect data loaded from flash chips, IceClave develops a lightweight data encryption/decryption mechanism in flash controllers. We develop IceClave with a full system simulator. We evaluate IceClave with a variety of data-intensive applications such as databases. Compared to state-of-the-art in-storage computing approaches, IceClave introduces only 7.6% performance overhead, while enforcing security isolation in the SSD controller with minimal hardware cost. IceClave still keeps the performance benefit of in-storage computing by delivering up to 2.31 \u00d7 better performance than the conventional host-based trusted computing approach.", "venue": "MICRO", "authors": ["Luyi  Kang", "Yuqi  Xue", "Weiwei  Jia", "Xiaohao  Wang", "Jongryool  Kim", "Changhwan  Youn", "Myeong Joon Kang", "Hyung Jin Lim", "Bruce  Jacob", "Jian  Huang"], "year": 2021, "n_citations": 0}
{"id": 1458702, "s2_id": "39e65f784cb612d28b23dd674aec027f5f397d51", "title": "Composite Enclaves: Towards Disaggregated Trusted Execution", "abstract": "The ever-rising computation demand is forcing the move from the CPU to heterogeneous specialized hardware, which is readily available across modern datacenters through disaggregated infrastructure. On the other hand, trusted execution environments (TEEs), one of the most promising recent developments in hardware security, can only protect code confined in the CPU, limiting TEEs\u2019 potential and applicability to a handful of applications. We observe that the TEEs\u2019 hardware trusted computing base (TCB) is fixed at design time, which in practice leads to using untrusted software to employ peripherals in TEEs. Based on this observation, we propose composite enclaves with a configurable hardware and software TCB, allowing enclaves access to multiple computing and IO resources. Finally, we present two case studies of composite enclaves: i) an FPGA platform based on RISC-V Keystone connected to emulated peripherals and sensors, and ii) a large-scale accelerator. These case studies showcase a flexible but small TCB (2.5 KLoC for IO peripherals and drivers), with a low-performance overhead (only around 220 additional cycles for a context switch), thus demonstrating the feasibility of our approach and showing that it can work with a wide range of specialized hardware.", "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.", "authors": ["Moritz  Schneider", "Aritra  Dhar", "Ivan  Puddu", "Kari  Kostiainen", "Srdjan  Capkun"], "year": 2022, "n_citations": 0}
{"id": 1460022, "s2_id": "8d550985c900bfea8f4bf7ca78386fa123580873", "title": "Timing model extraction for sequential circuits considering process variations", "abstract": "As semiconductor devices continue to scale down, process variations become more relevant for circuit design. Facing such variations, statistical static timing analysis is introduced to model variations more accurately so that the pessimism in traditional worst case timing analysis is reduced. Because all delays are modeled using correlated random variables, most statistical timing methods are much slower than corner based timing analysis. To speed up statistical timing analysis, we propose a method to extract timing models for flip-flop and latch based sequential circuits respectively. When such a circuit is used as a module in a hierarchical design, the timing model instead of the original circuit is used for timing analysis. The extracted timing models are much smaller than the original circuits. Experiments show that using extracted timing models accelerates timing verification by orders of magnitude compared to previous approaches using flat netlists directly. Accuracy is maintained, however, with the mean and standard deviation of the clock period both showing usually less than 1% error compared to Monte Carlo simulation on a number of benchmark circuits.", "venue": "2009 IEEE/ACM International Conference on Computer-Aided Design - Digest of Technical Papers", "authors": ["Bing  Li", "Ning  Chen", "Ulf  Schlichtmann"], "year": 2009, "n_citations": 3}
{"id": 1461119, "s2_id": "ef223eb2f9107c88808f05588566bcf83b80d390", "title": "Dynamically Improving Branch Prediction Accuracy Between Contexts", "abstract": "Branch prediction is a standard feature in most processors, significantly improving the run time of programs by allowing a processor to predict the direction of a branch before it has been evaluated. Current branch prediction methods can achieve excellent prediction accuracy through global tables, various hashing methods, and even machine learning techniques such as SVMs or neural networks. Such designs, however, may lose effectiveness when attempting to predict across context switches in the operating system. Such a scenario may lead to destructive interference between contexts, therefore reducing overall predictor accuracy. To solve this problem, we propose a novel scheme for deciding whether a context switch produces destructive or constructive interference. First, we present evidence that shows that destructive interference can have a significant negative impact on prediction accuracy. Second, we present an extensible framework that keeps track of context switches and prediction accuracy to improve overall accuracy. Experimental results show that this framework effectively reduces the effect of destructive interference on branch prediction.", "venue": "ArXiv", "authors": ["Adam  Auten", "Tanishq  Dubey", "Rohan  Mathur"], "year": 2018, "n_citations": 0}
{"id": 1462707, "s2_id": "f9e9c3b7f456521c110c0ceb8c061f94899664f7", "title": "Software Pipelining for Quantum Loop Programs", "abstract": "We propose a method for performing software pipelining on quantum for-loop programs, exploiting parallelism in and across iterations. We redefine concepts that are useful in program optimization, including array aliasing, instruction dependency and resource conflict, this time in optimization of quantum programs. Using the redefined concepts, we present a software pipelining algorithm exploiting instruction-level parallelism in quantum loop programs. The optimization method is then evaluated on some test cases, including popular applications like QAOA, and compared with several baseline results. The evaluation results show that our approach outperforms loop optimizers exploiting only in-loop optimization chances by reducing total depth of the loop program to close to the optimal program depth obtained by full loop unrolling, while generating much smaller code in size. This is the first step towards optimization of a quantum program with such loop control flow as far as we know.", "venue": "ArXiv", "authors": ["Jingzhe  Guo", "Mingsheng  Ying"], "year": 2020, "n_citations": 0}
{"id": 1463108, "s2_id": "325e963f9c8393d691d0599564a73b485515fcb0", "title": "TRIM: A Design Space Exploration Model for Deep Neural Networks Inference and Training Accelerators", "abstract": "There is increasing demand for specialized hardware for training deep neural networks, both in edge/IoT environments and in high performance computing systems. The design space of such hardware is very large due to the wide range of processing architectures, deep neural network configurations, and dataflow options. This makes developing deep neural network processors quite complex, especially for training. We present TRIM, an infrastructure to help hardware architects explore the design space of deep neural network accelerators for both inference and training in the early design stages. The model evaluates at the whole network level, considering both inter-layer and intra-layer activities. Given applications, essential hardware specifications, and a design goal, TRIM can quickly explore different hardware design options, select the optimal dataflow and guide new hardware architecture design. We validated TRIM with FPGA-based implementation of deep neural network accelerators and ASIC-based architectures. We also show how to use TRIM to explore the design space through several case studies. TRIM is a powerful tool to help architects evaluate different hardware choices to develop efficient inference and training architecture design.", "venue": "ArXiv", "authors": ["Yangjie  Qi", "Shuo  Zhang", "Tarek M. Taha"], "year": 2021, "n_citations": 0}
{"id": 1464733, "s2_id": "0e1629fefa210ad703f2b95a950bb0b12f015262", "title": "Quantum Circuit Designs of Integer Division Optimizing T-Count and T-Depth", "abstract": "Quantum circuits for basic mathematical functions such as division are required to implement scientific computing algorithms on quantum computers. In this work, we propose two designs for quantum integer division. The designs are based on quantum Clifford+T gates and are optimized for T-count and T-depth. Quantum circuits that are based on Clifford+T gates can be made fault tolerant in nature but the T gate is very costly to implement. As a result, reducing T-count and T-depth have become important optimization goals. Existing quantum hardware is limited in terms of number of available qubits. Thus, ancillary qubits are a circuit overhead that needs to be kept to a minimum. We propose two quantum integer division circuits. The first quantum integer division circuit is based on the non-restoring division algorithm. The proposed non-restoring division circuit is optimized for total quantum hardware (T-count and T-depth) cost but requires 2* n + 1 ancillary qubits. We also propose a quantum integer division circuit based on the restoring division algorithm. The proposed restoring division circuit is optimized for total qubits. The design requires only n ancillary qubits but will need more quantum hardware than the non-restoring division circuit. Both proposed quantum circuits are based on (i) a new quantum conditional addition circuit, (ii) a new quantum adder-subtractor and (iii) a new quantum subtraction circuit. Further, both designs are compared and shown to be superior to existing work in terms of T-count and T-depth. The proposed quantum non-restoring integer division circuit has a 96% improvement in terms of T-count and a 93% improvement in terms of T-depth compared to existing work. The proposed quantum restoring integer division circuit has a 91% improvement in terms of T-count and a 86% improvement in terms of T-count compared to the existing work.", "venue": "2017 IEEE International Symposium on Nanoelectronic and Information Systems (iNIS)", "authors": ["Himanshu  Thapliyal", "Edgard  Mu\u00f1oz-Coreas", "T. S. S. Varun", "Travis S. Humble"], "year": 2017, "n_citations": 15}
{"id": 1466694, "s2_id": "9797d79a99ce20adf6cb22019f2206ccd993b485", "title": "Multi-Mode Unrolled Architectures for Polar Decoders", "abstract": "In this work, we present a family of architectures for polar decoders using a reduced-complexity successive-cancellation decoding algorithm that employs unrolling to achieve extremely high throughput values while retaining moderate implementation complexity. The resulting fully-unrolled, deeply-pipelined architecture is capable of achieving a coded throughput in excess of 1 Tbps on a 65 nm ASIC at 500 MHz-three orders of magnitude greater than current state-of-the-art polar decoders. However, unrolled decoders are built for a specific, fixed code. Therefore we also present a new method to enable the use of multiple code lengths and rates in a fully-unrolled polar decoder architecture. This method leads to a length- and rate-flexible decoder while retaining the very high speed typical to unrolled decoders. The resulting decoders can decode a master polar code of a given rate and length, and several shorter codes of different rates and lengths. We present results for two versions of a multi-mode decoder supporting eight and ten different polar codes, respectively. Both are capable of a peak throughput of 25.6 Gbps. For each decoder, the energy efficiency for the longest supported polar code is shown to be of 14.8 pJ/bit at 250 MHz and of 8.8 pJ/bit at 500 MHz.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Pascal  Giard", "Gabi  Sarkis", "Claude  Thibeault", "Warren J. Gross"], "year": 2016, "n_citations": 24}
{"id": 1468263, "s2_id": "a023265f8ff9071f95ca709b1d35eb54b72339db", "title": "Dagger: Towards Efficient RPCs in Cloud Microservices With Near-Memory Reconfigurable NICs", "abstract": "Cloud applications are increasingly relying on hundreds of loosely-coupled microservices to complete user requests that meet an application's end-to-end QoS requirements. Communication time between services accounts for a large fraction of the end-to-end latency and can introduce performance unpredictability and QoS violations. This letter presents our early work on Dagger, a hardware acceleration platform for networking, designed specifically with the unique qualities of microservices in mind. The Dagger architecture relies on an FPGA-based NIC, closely coupled with the processor over a configurable memory interconnect, designed to offload and accelerate RPC stacks. Unlike the traditional cloud systems that use PCIe links as the NIC I/O interface, we leverage memory-interconnected FPGAs as networking devices to provide the efficiency, transparency, and programmability needed for fine-grained microservices. We show that this considerably improves CPU utilization and performance for cloud RPCs.", "venue": "IEEE Computer Architecture Letters", "authors": ["Nikita  Lazarev", "Neil  Adit", "Shaojie  Xiang", "Zhiru  Zhang", "Christina  Delimitrou"], "year": 2020, "n_citations": 2}
{"id": 1474137, "s2_id": "f3c4e8f65b7c988d08373f23b7b191534f5677e1", "title": "The Anatomy of the Grid: Enabling Scalable Virtual Organizations", "abstract": "\u201cGrid\u201d computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high performance orientation. In this article, the authors define this new field. First, they review the \u201cGrid problem,\u201d which is defined as flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources\u2014what is referred to as virtual organizations. In such settings, unique authentication, authorization, resource access, resource discovery, and other challenges are encountered. It is this class of problem that is addressed by Grid technologies. Next, the authors present an extensible and open Grid architecture, in which protocols, services, application programming interfaces, and software development kits are categorized according to their roles in enabling resource sharing. The authors describe requirements that they believe any such mechanisms must satisfy and discuss the importance of defining a compact set of intergrid protocols to enable interoperability among different Grid systems. Finally, the authors discuss how Grid technologies relate to other contemporary technologies, including enterprise integration, application service provider, storage service provider, and peer-to-peer computing. They maintain that Grid concepts and technologies complement and have much to contribute to these other approaches.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Ian T. Foster", "Carl  Kesselman", "Steven  Tuecke"], "year": 2001, "n_citations": 4752}
{"id": 1478377, "s2_id": "1438cf1ad9b1467f872b721d27df17cb43e0b528", "title": "Programmable Ethernet Switches and Their Applications", "abstract": "Simplicity, cost effectiveness, scalability, and economies of scale make Ethernet a popular choice for (a) local area networks (LAN), as well as for (b) storage area networks (SAN), and increasingly (c) metropolitan-area networks (MAN). Applications of Ethernet in the SAN and MAN arena elevate it from a LAN technology to a ubiquitous networking technology. With the expanded applicability of Ethernet there are certain adaptability issues which prompt rethinking of some of its architectural features. The Spanning-Tree based switching mechanism, considered to be very efficient at avoiding loops in LAN environments, is a performance bottleneck in the metro network context. Absence of an explicit switching path selection mechanism prohibits traffic engineering in metro networks. Prolonged spanning tree reconstruction periods after failures make Ethernet unsuitable to support critical applications. Lack of usage regulation mechanisms leads to insufficient isolation between different users, resulting in QoS problems. \nModern Ethernet switches support many advanced features which can be controlled through programmable interfaces. These features are VLAN tagging, rate limiting, and status monitoring. Conventionally, these features are mostly used to statically configure an Ethernet switch. This research proposes to use these features as dynamic control mechanisms in the context of metro and cluster networks to: (1) Maximize physical network link resources by enabling MPLS like traffic engineering, (2) Minimize failure recovery time, and (3) Enforce QoS requirements. \nWith these programmatic configurable control mechanisms, standard Ethernet switches can be used as effective building blocks for metropolitan-area Ethernet networks (MEN), storage-area networks (SAN), and computation cluster interconnects. We validated the usefulness of this new level of control over Ethernet switches with a MEN architecture that features multi-fold throughput gains and sub-second failure recovery time. \nWe discuss how a comprehensive resource management system can be devised using these mechanisms that can result in high performance and fault tolerant metro Ethernet, Storage, and Cluster networks. We also discuss how a network topology can be efficiently evolved by correlating the traffic profile characteristics of the end users and the traffic engineering required in the network. We describe a design methodology for Ethernet-based SAN fabrics utilizing this network evolution technique. \nTo this effect, we develop a network topology planning tool to minimize network infrastructure deployment cost. This topology planning work is specifically targeted towards providing automated tools to design Ethernet storage area network and cluster interconnect topologies with redundancy and fault-tolerance support.", "venue": "ArXiv", "authors": ["Srikant  Sharma", "Tzi-cker  Chiueh"], "year": 2004, "n_citations": 1}
{"id": 1482172, "s2_id": "850b61ea2454e9337d7bacd9219041ee336b18e8", "title": "Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement Learning Problems", "abstract": "Reinforcement learning, augmented by the representational power of deep neural networks, has shown promising results on high-dimensional problems, such as game playing and robotic control. However, the sequential nature of these problems poses a fundamental challenge for computational efficiency. Recently, alternative approaches such as evolutionary strategies and deep neuroevolution demonstrated competitive results with faster training time on distributed CPU cores. Here we report record training times (running at about 1 million frames per second) for Atari 2600 games using deep neuroevolution implemented on distributed FPGAs. Combined hardware implementation of the game console, image preprocessing and the neural network in an optimized pipeline, multiplied with the system level parallelism enabled the acceleration. These results are the first application demonstration on the IBM Neural Computer, which is a custom designed system that consists of 432 Xilinx FPGAs interconnected in a 3D mesh network topology. In addition to high performance, experiments also showed improvement in accuracy for all games compared to the CPU implementation of the same algorithm.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Alexis  Asseman", "Nicolas  Antoine", "Ahmet S. Ozcan"], "year": 2021, "n_citations": 2}
{"id": 1483173, "s2_id": "4c38c00e765a348542e334c71122e0fb6c35fcab", "title": "An Area-Efficient FPGA Overlay using DSP Block based Time-multiplexed Functional Units", "abstract": "Coarse grained overlay architectures improve FPGA design productivity by providing fast compilation and software-like programmability. Throughput oriented spatially configurable overlays typically suffer from area overheads due to the requirement of one functional unit for each compute kernel operation. Hence, these overlays have often been of limited size, supporting only relatively small compute kernels while consuming considerable FPGA resources. This paper examines the possibility of sharing the functional units among kernel operations for reducing area overheads. We propose a linear interconnected array of time-multiplexed FUs as an overlay architecture with reduced instruction storage and interconnect resource requirements, which uses a fully-pipelined, architecture-aware FU design supporting a fast context switching time. The results presented show a reduction of up to 85% in FPGA resource requirements compared to existing throughput oriented overlay architectures, with an operating frequency which approaches the theoretical limit for the FPGA device.", "venue": "ArXiv", "authors": ["Xiangwei  Li", "Abhishek Kumar Jain", "Douglas L. Maskell", "Suhaib A. Fahmy"], "year": 2016, "n_citations": 9}
{"id": 1485957, "s2_id": "9467cb15ef0cc67aa7e505c67f8a9c84dc6d3643", "title": "MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications", "abstract": "Recent breakthroughs in ML have produced new classes of models that allow ML inference to run directly on milliwatt-powered IoT devices. On one hand, existing ML-to-FPGA compilers are designed for deep neural-networks on large FPGAs. On the other hand, general-purpose HLS tools fail to exploit properties specific to ML inference, thereby resulting in suboptimal performance. We propose MAFIA, a tool to compile ML inference on small form-factor FPGAs for IoT applications. MAFIA provides native support for linear algebra operations and can express a variety of ML algorithms, including state-of-the-art models. We show that MAFIA-generated programs outperform best-performing variant of a commercial HLS compiler by 2.5 \u00d7 on average.", "venue": "2021 31st International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Nikhil Pratap Ghanathe", "Vivek  Seshadri", "Rahul  Sharma", "Steve  Wilton", "Aayan  Kumar"], "year": 2021, "n_citations": 0}
{"id": 1486764, "s2_id": "35f0a82d15962001be390b376aaf35b80104bdff", "title": "Statistical Timing Analysis and Criticality Computation for Circuits With Post-Silicon Clock Tuning Elements", "abstract": "Post-silicon clock tuning elements are widely used in high-performance designs to mitigate the effects of process variations and aging. Located on clock paths to flip-flops, these tuning elements can be configured through the scan chain so that clock skews to these flip-flops can be adjusted after manufacturing. Owing to the delay compensation across consecutive register stages enabled by the clock tuning elements, higher yield and enhanced robustness can be achieved. These benefits are, nonetheless, attained by increasing die area due to the inserted clock tuning elements. For balancing performance improvement and area cost, an efficient timing analysis algorithm is needed to evaluate the performance of such a circuit. So far this evaluation is only possible by Monte Carlo simulation which is very time-consuming. In this paper, we propose an alternative method using graph transformation, which computes a parametric minimum clock period and is more than 104 times faster than Monte Carlo simulation while maintaining a good accuracy. This method also identifies the gates that are critical to circuit performance, so that a fast analysis-optimization flow becomes possible.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Bing  Li", "Ulf  Schlichtmann"], "year": 2015, "n_citations": 18}
{"id": 1486809, "s2_id": "7f27e5f6f6ab1a65132a36027c91fe598c9145a4", "title": "Evaluating the Cost of Atomic Operations on Modern Architectures", "abstract": "Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add (FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs between these operations and various characteristics of such systems, such as the structure of caches, are unclear and have not been thoroughly analyzed. In this paper we establish an evaluation methodology, develop a performance model, and present a set of detailed benchmarks for latency and bandwidth of different atomics. We consider various state-of-the-art x86 architectures: Intel Haswell, Xeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising performance relationships between the considered atomics and architectural properties such as the coherence state of the accessed cache lines. One key finding is that all the tested atomics have comparable latency and bandwidth even if they are characterized by different consensus numbers. Another insight is that the design of atomics prevents any instruction-level parallelism even if there are no dependencies between the issued operations. Finally, we discuss solutions to the discovered performance issues in the analyzed architectures. Our analysis can be used for making better design and algorithmic decisions in parallel programming on various architectures deployed in both off-the-shelf machines and large compute systems.", "venue": "2015 International Conference on Parallel Architecture and Compilation (PACT)", "authors": ["Hermann  Schweizer", "Maciej  Besta", "Torsten  Hoefler"], "year": 2015, "n_citations": 72}
{"id": 1492804, "s2_id": "f2ff8e6b618b9fcb57e9b77ed43e61f9134c91df", "title": "Loom: exploiting weight and activation precisions to accelerate convolutional neural networks", "abstract": "ABSTRACT \u2022 Hardware inference accelerator for CNNs. \u2022 Targets area constrained System-on-a-Chip designs. \u2022 Processes both weights and activations bit-serially. \u2022 Weights are supplied via a High Bandwidth Memory v2 (HBM2) interface. \u2022 Lower precision performance gain \u2013 Convolutional layer: 256/(Pa x Pw) \u2013 Fully-connected layer: 16/Pw \u2022 Performance and energy efficiency improvements over a stateof-the-art bit-parallel accelerator \u2013 2.34x performance improvement \u2013 2.23x more energy efficiency \u2022 Enables performance, energy efficiency and accuracy trade-off", "venue": "DAC", "authors": ["Sayeh  Sharify", "Alberto Delmas Lascorz", "Kevin  Siu", "Patrick  Judd", "Andreas  Moshovos"], "year": 2018, "n_citations": 16}
{"id": 1492814, "s2_id": "011199d49b6e156df5521b4ed9d12fab9bbe50c6", "title": "Hardware Functional Obfuscation With Ferroelectric Active Interconnects", "abstract": "Tonggunag Yu, Yixin Xu, Shan Deng, Zijian Zhao, Nicolas Jao, You Sung Kim, Stefan Duenkel, Sven Beyer, Kai Ni2\u2217, Sumitha George4\u2217, Vijaykrishnan Narayanan Pennsylvania State University, State College, PA 16802, USA Rochester Institute of Technology, Rochester, NY 14623, USA GLOBALFOUNDRIES Fab1 LLC & Co. KG, Dresden, Germany North Dakota State University, Fargo, ND 58102, USA \u2217To whom correspondence should be addressed Email: sumitha.george@ndsu.edu, kai.ni@rit.edu", "venue": "ArXiv", "authors": ["Tongguang  Yu", "Yixin  Xu", "Shan  Deng", "Zijian  Zhao", "Nicholas  Jao", "You Sung Kim", "Stefan  D\u00fcnkel", "Sven  Beyer", "Kai  Ni", "Sumitha  George", "Narayanan  Vijaykrishnan"], "year": 2021, "n_citations": 0}
{"id": 1494467, "s2_id": "42d9ad76e3ce53cfcf5b2a2da8458a5f9e590d7b", "title": "Multi-core processors - An overview", "abstract": "Microprocessors have revolutionized the world we live in and continuous efforts are being made to manufacture not only faster chips but also smarter ones. A number of techniques such as data level parallelism, instruction level parallelism and hyper threading (Intel's HT) already exists which have dramatically improved the performance of microprocessor cores. This paper briefs on evolution of multi-core processors followed by introducing the technology and its advantages in today's world. The paper concludes by detailing on the challenges currently faced by multi-core processors and how the industry is trying to address these issues.", "venue": "ArXiv", "authors": ["Balaji  Venu"], "year": 2011, "n_citations": 16}
{"id": 1497886, "s2_id": "264dce7a5ef6bf6b98b51f012acc144e95c94bfb", "title": "Design and Evaluation Frameworks for Advanced RISC-based Ternary Processor", "abstract": "In this paper, we introduce the design and verification frameworks for developing a fully-functional emerging ternary processor. Based on the existing compiling environments for binary processors, for the given ternary instructions, the software-level framework provides an efficient way to convert the given programs to the ternary assembly codes. We also present a hardware-level framework to rapidly evaluate the performance of a ternary processor implemented in arbitrary design technology. As a case study, the fully-functional 9-trit advanced RISC-based ternary (ART-9) core is newly developed by using the proposed frameworks. Utilizing 24 custom ternary instructions, the 5-stage ART-9 prototype architecture is successfully verified by a number of test programs including dhrystone benchmark in a ternary domain, achieving the processing efficiency of 57.8 DMIPS/W and 3.06\u00d7 10 DMIPS/W in the FPGA-level ternary-logic emulations and the emerging CNTFET ternary gates, respectively.", "venue": "ArXiv", "authors": ["Dongyun  Kam", "Jung Gyu Min", "Jongho  Yoon", "Sunmean  Kim", "Seokhyeong  Kang", "Youngjoo  Lee"], "year": 2021, "n_citations": 0}
{"id": 1506165, "s2_id": "8772c241ff99f8ad5b51c4f127a15f99a8858385", "title": "A Simple Multi-Processor Computer Based on Subleq", "abstract": "Subleq (Subtract and Branch on result Less than or Equal to zero) is both an instruction set and a programming language for One Instruction Set Computer (OISC). We describe a hardware implementation of an array of 28 one-instruction Subleq processors on a low-cost FPGA board. Our test results demonstrate that computational power of our Subleq OISC multi-processor is comparable to that of CPU of a modern personal computer. Additionally, we provide implementation details of our complier from a C-style language to Subleq.", "venue": "ArXiv", "authors": ["Oleg  Mazonka", "Alex  Kolodin"], "year": 2011, "n_citations": 16}
{"id": 1507801, "s2_id": "a07b3dc4d43ce52a483215c8db17ba8b82259b02", "title": "FASHION: Fault-Aware Self-Healing Intelligent On-chip Network", "abstract": "To avoid packet loss and deadlock scenarios that arise due to faults or power gating in multicore and many-core systems, the network-on-chip needs to possess resilient communication and load-balancing properties. In this work, we introduce the Fashion router, a self-monitoring and self-reconfiguring design that allows for the on-chip network to dynamically adapt to component failures. First, we introduce a distributed intelligence unit, called Self-Awareness Module (SAM), which allows the router to detect permanent component failures and build a network connectivity map. Using local information, SAM adapts to faults, guarantees connectivity and deadlock-free routing inside the maximal connected subgraph and keeps routing tables up-to-date. Next, to reconfigure network links or virtual channels around faulty/power-gated components, we add bidirectional link and unified virtual channel structure features to the Fashion router. This version of the router, named Ex-Fashion, further mitigates the negative system performance impacts, leads to larger maximal connected subgraph and sustains a relatively high degree of fault-tolerance. To support the router, we develop a fault diagnosis and recovery algorithm executed by the Built-In Self-Test, self-monitoring, and self-reconfiguration units at runtime to provide fault-tolerant system functionalities. The Fashion router places no restriction on topology, position or number of faults. It drops 54.3-55.4% fewer nodes for same number of faults (between 30 and 60 faults) in an 8x8 2D-mesh over other state-of-the-art solutions. It is scalable and efficient. The area overheads are 2.311% and 2.659% when implemented in 8x8 and 16x16 2D-meshes using the TSMC 65nm library at 1.38GHz clock frequency.", "venue": "ArXiv", "authors": ["Pengju  Ren", "Michel A. Kinsy", "Mengjiao  Zhu", "Shreeya  Khadka", "Mihailo  Isakov", "Aniruddh  Ramrakhyani", "Tushar  Krishna", "Nanning  Zheng"], "year": 2017, "n_citations": 0}
{"id": 1513731, "s2_id": "ddf7190d582ad7a1b1b070c53fc4fa09039a8bdb", "title": "The Effect of Scheduling on Link Capacity in Multi-hopWireless Networks", "abstract": "Existing models of Multi-Hop Wireless Networks (MHWNs) assume that interference estimators of link quality such as observed busy time predict the capacity of the links. We show that these estimators do not capture the intricate interactions that occur at the scheduling level, which have a large impact on effective link capacity under contention based MAC protocols. We observe that scheduling problems arise only among those interfering sources whose concurrent transmissions cannot be prevented by the MAC protocol's collision management mechanisms; other interfering sources can arbitrate the medium and coexist successfully. Based on this observation, we propose a methodology for rating links and show that it achieves high correlation with observed behavior in simulation. We then use this rating as part of a branch-and-bound framework based on a linear programming formulation for traffic engineering in static MHWNs and show that it achieves considerable improvement in performance relative to interference based models.", "venue": "ArXiv", "authors": ["Vinay  Kolar", "Nael B. Abu-Ghazaleh"], "year": 2006, "n_citations": 10}
{"id": 1518185, "s2_id": "27ec65c0441743ae5641a043be98c044783c860a", "title": "On hierarchical statistical static timing analysis", "abstract": "Statistical static timing analysis deals with the increasing variations in manufacturing processes to reduce the pessimism in the worst case timing analysis. Because of the correlation between delays of circuit components, timing model generation and hierarchical timing analysis face more challenges than in static timing analysis. In this paper, a novel method to generate timing models for combinational circuits considering variations is proposed. The resulting timing models have accurate input-output delays and are about 80% smaller than the original circuits. Additionally, an accurate hierarchical timing analysis method at design level using pre-characterized timing models is proposed. This method incorporates the correlation between modules by replacing independent random variables to improve timing accuracy. Experimental results show that the correlation between modules strongly affects the delay distribution of the hierarchical design and the proposed method has good accuracy compared with Monte Carlo simulation, but is faster by three orders of magnitude.", "venue": "2009 Design, Automation & Test in Europe Conference & Exhibition", "authors": ["Bing  Li", "Ning  Chen", "Manuel  Schmidt", "Walter  Schneider", "Ulf  Schlichtmann"], "year": 2009, "n_citations": 20}
{"id": 1520532, "s2_id": "07b1f537f00ebed9a1f33d675639bebe7d012070", "title": "Helix: Algorithm/Architecture Co-design for Accelerating Nanopore Genome Base-calling", "abstract": "Nanopore genome sequencing is the key to enabling personalized medicine, global food security, and virus surveillance. The state-of-the-art base-callers adopt deep neural networks (DNNs) to translate electrical signals generated by nanopore sequencers to digital DNA symbols. A DNN-based base-caller consumes 44.5% of total execution time of a nanopore sequencing pipeline. However, it is difficult to quantize a base-caller and build a power-efficient processing-in-memory (PIM) to run the quantized base-caller. Although conventional network quantization techniques reduce the computing overhead of a base-caller by replacing floating-point multiply-accumulations by cheaper fixed-point operations, it significantly increases the number of systematic errors that cannot be corrected by read votes. The power density of prior nonvolatile memory (NVM)-based PIMs has already exceeded memory thermal tolerance even with active heat sinks, because their power efficiency is severely limited by analog-to-digital converters (ADC). Finally, Connectionist Temporal Classification (CTC) decoding and read voting cost 53.7% of total execution time in a quantized base-caller, and thus became its new bottleneck. In this paper, we propose a novel algorithm/architecture co-designed PIM, Helix, to power-efficiently and accurately accelerate nanopore base-calling. From algorithm perspective, we present systematic error aware training to minimize the number of systematic errors in a quantized base-caller. From architecture perspective, we propose a low-power SOT-MRAM-based ADC array to process analog-to-digital conversion operations and improve power efficiency of prior DNN PIMs. Moreover, we revised a traditional NVM-based dot-product engine to accelerate CTC decoding operations, and create a SOT-MRAM binary comparator array to process read voting. Compared to state-of-the-art PIMs, Helix improves base-calling throughput by 6x, throughput per Watt by 11.9x and per mm2 by 7.5x without degrading base-calling accuracy.", "venue": "PACT", "authors": ["Qian  Lou", "Sarath  Janga", "Lei  Jiang"], "year": 2020, "n_citations": 0}
{"id": 1521153, "s2_id": "5d312ddb9c02055334dd4a960daea44121c99ae4", "title": "A fault-tolerant structure for reliable multi-core systems based on hardware-software co-design", "abstract": "To cope with the soft errors and make full use of the multi-core system, this paper gives an efficient fault-tolerant hardware and software co-designed architecture for multi-core systems. And with a not large number of test patterns, it will use less than 33% hardware resources compared with the traditional hardware redundancy (TMR) and it will take less than 50% time compared with the traditional software redundancy (time redundant).Therefore, it will be a good choice for the fault-tolerant architecture for the future high-reliable multi-core systems.", "venue": "2010 11th International Symposium on Quality Electronic Design (ISQED)", "authors": ["Bingbing  Xia", "Fei  Qiao", "Huazhong  Yang", "Hui  Wang"], "year": 2010, "n_citations": 5}
{"id": 1527261, "s2_id": "a7f16519c2483e497551673aacd5e874880cc33b", "title": "Routing in Networks on Chip with Multiplicative Circulant Topology", "abstract": "The development of multi-core processor systems is a demanded branch of science and technology. The appearance of processors with dozens and hundreds of cores poses to the developers the question of choosing the optimal topology capable to provide efficient routing in a network with a large number of nodes. In this paper, we consider the possibility of using multiplicative circulants as a topology for networks-on-chip. A specialized routing algorithm for networks with multiplicative circulant topology, taking into account topology features and having a high scalability, has been developed.", "venue": "ArXiv", "authors": ["M. A. Shchegoleva", "Yu. A. Romanov", "E. V. Lezhnev", "A. A Amerikanov"], "year": 2019, "n_citations": 2}
{"id": 1531094, "s2_id": "3af8a493cf756f9fe72623204a11e378a9cd71a5", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference", "abstract": "Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements. We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization. Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 \u00d7, 2.5 \u00d7, and 53 \u00d7 lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.", "venue": "MICRO", "authors": ["Thierry  Tambe", "Coleman  Hooper", "Lillian  Pentecost", "Tianyu  Jia", "En-Yu  Yang", "Marco  Donato", "Victor  Sanh", "Paul N. Whatmough", "Alexander M. Rush", "David  Brooks", "Gu-Yeon  Wei"], "year": 2021, "n_citations": 8}
{"id": 1535114, "s2_id": "b8c5447bba1a4098e3198b11b806180033f14589", "title": "HeM3D: Heterogeneous Manycore Architecture Based on Monolithic 3D Vertical Integration", "abstract": "Heterogeneous manycore architectures are the key to efficiently execute compute- and data-intensive applications. Through silicon via (TSV)-based 3D manycore system is a promising solution in this direction as it enables integration of disparate computing cores on a single system. However, the achievable performance of conventional through-silicon-via (TSV)-based 3D systems is ultimately bottlenecked by the horizontal wires (wires in each planar die). Moreover, current TSV 3D architectures suffer from thermal limitations. Hence, TSV-based architectures do not realize the full potential of 3D integration. Monolithic 3D (M3D) integration, a breakthrough technology to achieve - More Moore and More Than Moore - and opens up the possibility of designing cores and associated network routers using multiple layers by utilizing monolithic inter-tier vias (MIVs) and hence, reducing the effective wire length. Compared to TSV-based 3D ICs, M3D offers the true benefits of vertical dimension for system integration: the size of a MIV used in M3D is over 100x smaller than a TSV. In this work, we demonstrate how M3D-enabled vertical core and uncore elements offer significant performance and thermal improvements in manycore heterogeneous architectures compared to its TSV-based counterpart. To overcome the difficult optimization challenges due to the large design space and complex interactions among the heterogeneous components (CPU, GPU, Last Level Cache, etc.) in an M3D-based manycore chip, we leverage novel design-space exploration algorithms to trade-off different objectives. The proposed M3D-enabled heterogeneous architecture, called HeM3D, outperforms its state-of-the-art TSV-equivalent counterpart by up to 18.3% in execution time while being up to 19 degrees Celcius cooler.", "venue": "ACM Trans. Design Autom. Electr. Syst.", "authors": ["Aqeeb Iqbal Arka", "Biresh Kumar Joardar", "Ryan Gary Kim", "Dae Hyun Kim", "Janardhan Rao Doppa", "Partha Pratim Pande"], "year": 2021, "n_citations": 1}
{"id": 1535654, "s2_id": "eb1848ab955896ee7d5a331e4f7a9f2401b6fccf", "title": "Single Storage Semi-Global Matching for Real Time Depth Processing", "abstract": "Depth-map is the key computation in computer vision and robotics. One of the most popular approach is via computation of disparity-map of images obtained from Stereo Camera. Semi Global Matching (SGM) method is a popular choice for good accuracy with reasonable computation time. To use such compute-intensive algorithms for real-time applications such as for autonomous aerial vehicles, blind Aid, etc. acceleration using GPU, FPGA is necessary. In this paper, we show the design and implementation of a stereo-vision system, which is based on FPGA-implementation of More Global Matching(MGM). MGM is a variant of SGM. We use 4 paths but store a single cumulative cost value for a corresponding pixel. Our stereo-vision prototype uses Zedboard containing an ARM-based Zynq-SoC, ZED-stereo-camera / ELP stereo-camera / Intel RealSense D435i, and VGA for visualization. The power consumption attributed to the custom FPGA-based acceleration of disparity map computation required for depth-map is just 0.72 watt. The update rate of the disparity map is realistic 10.5 fps.", "venue": "ArXiv", "authors": ["Prathmesh  Sawant", "Yashwant  Temburu", "Mandar  Datar", "Imran  Ahmed", "Vinayak  Shriniwas", "Sachin  Patkar"], "year": 2020, "n_citations": 0}
{"id": 1537043, "s2_id": "87f19670326d0a5f041c6a23b03cd4995a73745c", "title": "Mithril: Cooperative Row Hammer Protection on Commodity DRAM Leveraging Managed Refresh", "abstract": "Since its public introduction in the mid-2010s, the Row Hammer (RH) phenomenon has drawn significant attention from the research community due to its security implications. Although many RH-protection schemes have been proposed by processor vendors, DRAM manufacturers, and academia, they still have shortcomings. Solutions implemented in the memory controller (MC) incur increasingly higher costs due to their conservative design for the worst case in terms of the number of DRAM banks and RH threshold to support. Meanwhile, DRAM-side implementation either has a limited time margin for RH-protection measures or requires extensive modifications to the standard DRAM interface. Recently, a new command for RH-protection has been introduced in the DDR5/LPDDR5 standards, referred to as refresh management (RFM). RFM enables the separation of the tasks for RHprotection to both MC and DRAM by having the former generate an RFM command at a specific activation frequency and the latter take proper RH-protection measures within a given time window. Although promising, no existing study presents and analyzes RFM-based solutions for RH-protection. In this paper, we propose Mithril, the first RFM interfacecompatible, DRAM-MC cooperative RH-protection scheme providing deterministic protection guarantees. Mithril has minimal energy overheads for common use cases without adversarial memory access patterns. We also introduce Mithril+, an optional extension to provide minimal performance overheads at the expense of a tiny modification to the MC, while utilizing existing DRAM commands.", "venue": "ArXiv", "authors": ["Michael Jaemin Kim", "Jaehyun  Park", "Yeonhong  Park", "Wanju  Doh", "Namhoon  Kim", "Tae Jun Ham", "Jae W. Lee", "Jung Ho Ahn"], "year": 2021, "n_citations": 1}
{"id": 1539399, "s2_id": "67d237e3cf207e9ecfe1e484ebfb6d2dea7cc0ac", "title": "Improving DRAM Performance, Security, and Reliability by Understanding and Exploiting DRAM Timing Parameter Margins", "abstract": "Characterization of real DRAM devices has enabled findings in DRAM device properties, which has led to proposals that significantly improve overall system performance by reducing DRAM access latency and power consumption. In addition to improving system performance, a deeper understanding of DRAM technology via characterization can also improve device reliability and security. These can beseen with the recent discoveries of 1) DRAM-based true random number generators (TRNGs), a method for generating true random numbers using DRAM devices which can be used in many applications, 2) DRAM-based physical unclonable functions (PUFs), a method for generating unique device-dependent keys for identification and authentication, and 3) the RowHammer vulnerability, a phenomenon where repeatedly accessing a DRAM row can cause failures in unaccessed neighboring DRAM rows.To advance DRAM-based discoveries and mechanisms, this dissertation rigorously characterizes many modern commodity DRAM devices and shows that by exploitingDRAM access timing margins within manufacturer-recommended DRAM timing specifications, we can significantly improve system performance, reduce power consumption, and improve device reliability and security. First, we characterize DRAM timing parameter margins and find that certain regions of DRAM can be accessed faster than other regions due to DRAM cell process manufacturing variation. We exploit this by enabling variable access times depending on the DRAM cells being accessed, whichnot only improves overall system performance, but also decreases power consumption. Second, we find that we can uniquely identify DRAM devices by the locations offailures that result when we access DRAM with timing parameters reduced below specification values. Because we induce these failures with DRAM accesses, we cangenerate these unique identifiers significantly more quickly than prior work. Third, we propose a random number generator that is based on our observation that timingfailures in certain DRAM cells are randomly induced and can thus be repeatedly polled to very quickly generate true random values. Finally, we characterize the RowHammersecurity vulnerability on a wide range of modern DRAM chips while violating the DRAM refresh requirement in order to directly characterize the underlying DRAM technology without the interference of refresh commands. We demonstrate with our characterization of real chips, that existing RowHammer mitigation mechanisms eitherare not scalable or suffer from prohibitively large performance overheads in projected future devices and it is critical to research more effective solutions to RowHammer.Overall, our studies build a new understanding of modern DRAM devices to improve computing system performance, reliability and security all at the same time.", "venue": "ArXiv", "authors": ["Jeremie  Kim"], "year": 2021, "n_citations": 1}
{"id": 1545692, "s2_id": "04116b49551ab5ad67be5c9dd87e6d57684668ff", "title": "Structured Inverted-File k-Means Clustering for High-Dimensional Sparse Data", "abstract": "This paper presents an architecture-friendly k-means clustering algorithm called SIVF for a large-scale and high-dimensional sparse data set. Algorithm efficiency on time is often measured by the number of costly operations such as similarity calculations. In practice, however, it depends greatly on how the algorithm adapts to an architecture of the computer system which it is executed on. Our proposed SIVF employs invariant centroid-pair based filter (ICP) to decrease the number of similarity calculations between a data object and centroids of all the clusters. To maximize the ICP performance, SIVF exploits for a centroid set an inverted-file that is structured so as to reduce pipeline hazards. We demonstrate in our experiments on real large-scale document data sets that SIVF operates at higher speed and with lower memory consumption than existing algorithms. Our performance analysis reveals that SIVF achieves the higher speed by suppressing performance degradation factors of the number of cache misses and branch mispredictions rather than less similarity calculations.", "venue": "ArXiv", "authors": ["Kazuo  Aoyama", "Kazumi  Saito"], "year": 2021, "n_citations": 0}
{"id": 1549301, "s2_id": "8363bea7e355ea2576cb9a27232a7cea35bda9d3", "title": "Sense amplifier comparator with offset correction for decision feedback equalization based receivers", "abstract": "Abstract A decision feedback circuit with integrated offset compensation is presented in this paper. The circuit is built around the sense amplifier comparator. The loop latency is reduced by closing the feedback loop around the first stage. The feedback loop is implemented using a switched capacitor network that picks from one of pre-computed voltages to be fed back. The comparator's offset that is to be compensated for, is added in the same path. Hence, an extra offset correction input is not required. The circuit is used as a receiver for a 10\u00a0mm low swing interconnect implemented in UMC 130\u00a0nm CMOS technology. The circuit is tested at a frequency of 1\u00a0GHz and it consumes 145\u00a0 \u03bc A from a 1.2\u00a0V supply at this frequency.", "venue": "Microelectron. J.", "authors": ["Naveen  Kadayinti", "Dinesh Kumar Sharma"], "year": 2017, "n_citations": 0}
{"id": 1553292, "s2_id": "c61c9c69a8af910328b3e9729a38dfc8b71820b2", "title": "A Ring Router Microarchitecture for NoCs", "abstract": "Network-on-Chip (NoC) has become a popular choice for connecting a large number of processing cores in chip multiprocessor design. In a conventional NoC design, most of the area in the router is occupied by the buffers and the crossbar switch. These two components also consume the majority of the router's power. Much of the research in NoC has been based on the conventional router microarchitecture. We propose a novel router microarchitecture that treats the router itself as a small network of the ring topology. It eliminates the large crossbar switch in the conventional design. In addition, network latency is much reduced. Simulation and circuit synthesis show that the proposed microarchitecture can reduce the latency, area and power by 53%, 34% and 27%, respectively, compared to the conventional design.", "venue": "ArXiv", "authors": ["Wo-Tak  Wu"], "year": 2020, "n_citations": 0}
{"id": 1553876, "s2_id": "bdeef933197933bc3eb8adcbf3f02185fa037248", "title": "Stochastic power grid analysis considering process variations", "abstract": "In this paper, we investigate the impact of interconnect and device process variations on voltage fluctuations in power grids. We consider random variations in the power grid's electrical parameters as spatial stochastic processes and propose a new and efficient method to compute the stochastic voltage response of the power grid. Our approach provides an explicit analytical representation of the stochastic voltage response using orthogonal polynomials in a Hilbert space. The approach has been implemented in a prototype software called OPERA (Orthogonal Polynomial Expansions for Response Analysis). Use of OPERA on industrial power grids demonstrated speed-ups of up to two orders of magnitude. The results also show a significant variation of about /spl plusmn/35% in the nominal voltage drops at various nodes of the power grids and demonstrate the need for variation-aware power grid analysis.", "venue": "Design, Automation and Test in Europe", "authors": ["Praveen  Ghanta", "Sarma B. K. Vrudhula", "Rajendran  Panda", "Janet  Roveda"], "year": 2005, "n_citations": 65}
{"id": 1558919, "s2_id": "809dddb3e7a2daedf05df2ba9501aa18c789b8a4", "title": "PThammer: Cross-User-Kernel-Boundary Rowhammer through Implicit Accesses", "abstract": "Rowhammer is a hardware vulnerability in DRAM memory, where repeated access to memory can induce bit flips in neighboring memory locations. Being a hardware vulnerability, rowhammer bypasses all of the system memory protection, allowing adversaries to compromise the integrity and confidentiality of data. Rowhammer attacks have shown to enable privilege escalation, sandbox escape, and cryptographic key disclosures.Recently, several proposals suggest exploiting the spatial proximity between the accessed memory location and the location of the bit flip for a defense against rowhammer. These all aim to deny the attacker\u2019s permission to access memory locations near sensitive data.In this paper, we question the core assumption underlying these defenses. We present PThammer, a confused-deputy attack that causes accesses to memory locations that the attacker is not allowed to access. Specifically, PThammer exploits the address translation process of modern processors, inducing the processor to generate frequent accesses to protected memory locations. We implement PThammer, demonstrating that it is a viable attack, resulting in a system compromise (e.g., kernel privilege escalation). We further evaluate the effectiveness of proposed software-only defenses showing that PThammer can overcome those.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Zhi  Zhang", "Yueqiang  Cheng", "Dongxi  Liu", "Surya  Nepal", "Zhi  Wang", "Yuval  Yarom"], "year": 2020, "n_citations": 7}
{"id": 1562844, "s2_id": "89c8d854455c251537b5ccf7d95d45ddc7c1959f", "title": "FPGA with Improved Routability and Robustness in 130nm CMOS with Open-Source CAD Targetability", "abstract": "This paper outlines an FPGA VLSI design methodology that was used to realize a fully functioning FPGA chip in 130nm CMOS with improved routability and memory robustness. The architectural design space exploration and synthesis capability were enabled by the Verilog-to-Routing CAD tool. The capabilities of this tool were extended to enable bitstream generation and deployment. To validate the architecture and bitstream implementation, a Chisel (Constructing Hardware in the Embedded Scala Language) model of the FPGA was created to rapidly verify the microarchitectural details of the device prior to schematic design. A custom carrier board and configuration tool were used to verify correct operational characteristics of the FPGA over various resource utilizations and clock frequencies.", "venue": "ArXiv", "authors": ["Guanshun  Yu", "Tom Y. Cheng", "Blayne  Kettlewell", "Harrison  Liew", "Mingoo  Seok", "Peter R. Kinget"], "year": 2017, "n_citations": 1}
{"id": 1563480, "s2_id": "6e5c212113ac4b61d082642f25ef0c13ceefeb81", "title": "Function Interface Models for Hardware Compilation: Types, Signatures, Protocols", "abstract": "The problem of synthesis of gate-level descriptions of digital circuits from behavioural specifications written in higher-level programming languages (hardware compilation) has been studied for a long time yet a definitive solution has not been forthcoming. The argument of this essay is mainly methodological, bringing a perspective that is informed by recent developments in programming-language theory. We argue that one of the major obstacles in the way of hardware compilation becoming a useful and mature technology is the lack of a well defined function interface model, i.e. a canonical way in which functions communicate with arguments. We discuss the consequences of this problem and propose a solution based on new developments in programming language theory. We conclude by presenting a prototype implementation and some examples illustrating our principles.", "venue": "ArXiv", "authors": ["Dan R. Ghica"], "year": 2009, "n_citations": 4}
{"id": 1564964, "s2_id": "807b06c50ef3cd252ea5b7b38e63a7ac3407d851", "title": "Accurate measurement of power consumption overhead during FPGA dynamic partial reconfiguration", "abstract": "In the context of embedded systems design, two important challenges are still under investigation. First, improve real-time data processing, reconfigurability, scalability, and self-adjusting capabilities of hardware components. Second, reduce power consumption through low-power design techniques as clock gating, logic gating, and dynamic partial reconfiguration (DPR) capabilities. Today, several application, e.g., cryptography, Software-defined radio or aerospace missions exploit the benefits of DPR of programmable logic devices. The DPR allows well defined reconfigurable FPGA region to be modified during runtime. However, it introduces an overhead in term of power consumption and time during the reconfiguration phase. In this paper, we present an investigation of power consumption overhead of the DPR process using a high-speed digital oscilloscope and the shunt resistor method. Results in terms of reconfiguration time and power consumption overhead for Virtex 5 FPGAs are shown.", "venue": "2016 International Symposium on Wireless Communication Systems (ISWCS)", "authors": ["Amor  Nafkha", "Yves  Lou\u00ebt"], "year": 2016, "n_citations": 16}
{"id": 1565697, "s2_id": "0dcc3d397c7ecd42c225e54b0f8c511c8f7f1116", "title": "Proposal of ROS-compliant FPGA Component for Low-Power Robotic Systems", "abstract": "In recent years, robots are required to be autonomous and their robotic software are sophisticated. Robots have a problem of insufficient performance, since it cannot equip with a high-performance microprocessor due to battery-power operation. On the other hand, FPGA devices can accelerate specific functions in a robot system without increasing power consumption by implementing customized circuits. But it is difficult to introduce FPGA devices into a robot due to large development cost of an FPGA circuit compared to software. Therefore, in this study, we propose an FPGA component technology for an easy integration of an FPGA into robots, which is compliant with ROS (Robot Operating System). As a case study, we designed ROS-compliant FPGA component of image labeling using Xilinx Zynq platform. The developed ROS-component FPGA component performs 1.7 times faster compared to the ordinary ROS software component.", "venue": "ArXiv", "authors": ["Kazushi  Yamashina", "Takeshi  Ohkawa", "Kanemitsu  Ootsu", "Takashi  Yokota"], "year": 2015, "n_citations": 10}
{"id": 1567844, "s2_id": "e67544760ca3c673edb2fda9a52caaa3d7b8c448", "title": "TrojanZero: Switching Activity-Aware Design of Undetectable Hardware Trojans with Zero Power and Area Footprint", "abstract": "Conventional Hardware Trojan (HT) detection techniques are based on the validation of integrated circuits to determine changes in their functionality, and on non-invasive side-channel analysis to identify the variations in their physical parameters. In particular, almost all the proposed side-channel power-based detection techniques presume that HTs are detectable because they only add gates to the original circuit with a noticeable increase in power consumption. This paper demonstrates how undetectable HTs can be realized with zero impact on the power and area footprint of the original circuit. Towards this, we propose a novel concept of TrojanZero and a systematic methodology for designing undetectable HTs in the circuits, which conceals their existence by gate-level modifications. The crux is to salvage the cost of the HT from the original circuit without being detected using standard testing techniques. Our methodology leverages the knowledge of transition probabilities of the circuit nodes to identify and safely remove expendable gates, and embeds malicious circuitry at the appropriate locations with zero power and area overheads when compared to the original circuit. We synthesize these designs and then embed in multiple ISCAS85 benchmarks using a 65nm technology library, and perform a comprehensive power and area characterization. Our experimental results demonstrate that the proposed TrojanZero designs are undetectable by the state-of-the-art power-based detection methods.", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Imran Hafeez Abbassi", "Faiq  Khalid", "Semeen  Rehman", "Awais M. Kamboh", "Axel  Jantsch", "Siddharth  Garg", "Muhammad  Shafique"], "year": 2019, "n_citations": 11}
{"id": 1569355, "s2_id": "ceae40ef15d8230940716b2f9c4a1f705ea8b05c", "title": "The Granularity Gap Problem: A Hurdle for Applying Approximate Memory to Complex Data Layout", "abstract": "The main memory access latency has not much improved for more than two decades while the CPU performance had been exponentially increasing until recently.Approximate memory is a technique to reduce the DRAM access latency in return of losing data integrity. It is expected to be beneficial for applications that are robust to noisy input and intermediate data such as artificial intelligence, image/video processing, and big-data analytics. To obtain reasonable outputs from applications on approximate memory, it is crucial to protect critical data while accelerating accesses to non-critical data. We refer the minimum size of a continuous memory region that the same error rate is applied in approximate memory to as the approximation granularity. A fundamental limitation of approximate memory is that the approximation granularity is as large as a few kilo bytes. However, applications may have critical and non-critical data interleaved with smaller granularity. For example, a data structure for graph nodes can have pointers (critical) to neighboring nodes and its score (non-critical, depending on the use-case). This data structure cannot be directly mapped to approximate memory due to the gap between the approximation granularity and the granularity of data criticality. We refer to this issue as the granularity gap problem. In this paper, we first show that many applications potentially suffer from this problem. Then we propose a framework to quantitatively evaluate the performance overhead of a possible method to avoid this problem using known techniques.The evaluation results show that the performance overhead is non-negligible compared to expected benefit from approximate memory,suggesting that the granularity gap problem is a significant concern.", "venue": "ICPE", "authors": ["Soramichi  Akiyama", "Ryota  Shioya"], "year": 2021, "n_citations": 0}
{"id": 1572111, "s2_id": "17bdf7f98449687acfcee3b5517ac760d5424d4c", "title": "Repairability Enhancement of Scalable Systems with Locally Shared Spares", "abstract": "Future systems based on nano-scale devices will provide great potentials for scaling up in system complexity, yet they will be highly susceptible to operational faults. While spare units can be generally used to enhance reliability, they must be shared in a limited way among functional units to ensure low-cost overheads when systems scale up. Furthermore, the efficiency of achieving reliability using spare units heavily depends on the replacement mechanisms of such spares. While global and chained replacement approaches can take advantage of the entire replacement capabilities in the network, they usually impose some sort of disturbance to all the functional units in the system during the repair process, thus are dreadfully expensive in terms of performance overhead for systems with high fault rates. In this paper, we focus on a low-cost, fast, immediate replacement mechanism that can be implemented locally with minimum disturbance to the system. The proposed schemes aim for maintaining the system with high fault rates in such a low-cost, fast repairable status for many faults before invoking the more expensive, yet optimal, approaches. First, we propose an online repair algorithm: as faults occur during the run-time of the system, the proposed algorithm makes a choice of a spare unit (among several candidates), such that the overall impact on system repairability in the future is minimized. Second, we propose a network enhancement approach, which identifies and connects the vulnerable units to the exploitable spares, thus strengthening the entire system at a low cost.", "venue": "ArXiv", "authors": ["Soroush  Khaleghi", "Wenjing  Rao"], "year": 2018, "n_citations": 0}
{"id": 1577417, "s2_id": "c7e2f70bf833703678e01ed86a190ab0e6d350da", "title": "GPU4S: Embedded GPUs in space - Latest project updates", "abstract": "Abstract Following the trend of other safety-critical industries like automotive and avionics, the space domain is witnessing an increase in the on-board computing performance demands. This raise in performance needs comes from both control and payload parts of the spacecraft and calls for advanced electronics systems able to provide high computational power under the constraints of the harsh space environment. On the non-technical side, for strategic reasons it is mandatory to get European independence on the used computing technology. In this project, we study the applicability of embedded GPUs in space, which have shown a dramatic improvement of their performance per-watt ratio coming from their proliferation in consumer markets based on competitive European technology. To that end, we perform an analysis of the existing space application domains to identify which software domains can benefit from their use. Moreover, we survey the embedded GPU domain in order to assess whether embedded GPUs can provide the required computational power and identify the challenges which need to be addressed for their adoption in space. In this paper, we describe the steps followed in the project, as well as a summary of results obtained from our analyses so far in the project.", "venue": "Microprocess. Microsystems", "authors": ["Leonidas  Kosmidis", "Iv\u00e1n  Rodriguez", "\u00c1lvaro  Jover", "Sergi  Alcaide", "J\u00e9r\u00f4me  Lachaize", "Jaume  Abella", "Olivier  Notebaert", "Francisco J. Cazorla", "David  Steenari"], "year": 2020, "n_citations": 1}
{"id": 1577730, "s2_id": "1eea37d4f84872d5444caf968b1b238c29e28617", "title": "Charge-based computing with analogue reconfigurable gates", "abstract": "As the world enters the age of ubiquitous computing, the need for reconfigurable hardware operating close to the fundamental limits of energy consumption becomes increasingly pressing. Simultaneously, scaling-driven performance improvements within the framework of traditional analogue and digital design become progressively more restricted by fundamental physical constraints. Thus, a true paradigm shift in electronics design is required for fuelling the next big burst in technology. Here we lay the foundations of a new design paradigm that fuses analogue and digital thinking by combining digital electronics with memristive devices for achieving charge-based computation; information processing where every dissipated charge counts. This is realised by introducing memristive devices into standard logic gates, thus rendering them reconfigurable and able to perform analogue computation at a power cost close to digital. The power of this concept is then showcased by experimentally demonstrating a hardware data clusterer and a fuzzy NAND gate using this principle.", "venue": "ArXiv", "authors": ["Alexantrou  Serb", "Ali  Khiat", "Themistoklis  Prodromakis"], "year": 2017, "n_citations": 4}
{"id": 1578539, "s2_id": "f0c73ba8bfe3a3a98b3fa78c701284c3d465a082", "title": "ScotGrid: A Prototype Tier 2 Centre", "abstract": "ScotGrid is a prototype regional computing centre formed as a collaboration between the universities of Durham, Edinburgh and Glasgow as part of the UK's national particle physics grid, GridPP. We outline the resources available at the three core sites and our optimisation efforts for our user communities. We discuss the work which has been conducted in extending the centre to embrace new projects both from particle physics and new user communities and explain our methodology for doing this.", "venue": "ArXiv", "authors": ["A.  Earl", "P.  Clark", "S.  Thorn"], "year": 2004, "n_citations": 1}
{"id": 1581436, "s2_id": "b1063de3db4ba60219b74dd36017996e00e8f20e", "title": "Very low cost entropy source based on chaotic dynamics retrofittable on networked devices to prevent RNG attacks", "abstract": "Good quality entropy sources are indispensable in most modern cryptographic protocols. Unfortunately, many currently deployed networked devices do not include them and may be vulnerable to Random Number Generator (RNG) attacks. Since most of these systems allow firmware upgrades and have serial communication facilities, the potential for retrofitting them with secure hardware-based entropy sources exists. To this aim, very low-cost, robust, easy to deploy solutions are required. Here, a retrofittable, sub 10$ entropy source based on chaotic dynamics is illustrated, capable of a 32 kbit/s rate or more and offering multiple serial communication options including USB, I2C, SPI or USART. Operation is based on a loop built around the Analog to Digital Converter (ADC) hosted on a standard microcontroller.", "venue": "2014 21st IEEE International Conference on Electronics, Circuits and Systems (ICECS)", "authors": ["Mattia  Fabbri", "Sergio  Callegari"], "year": 2014, "n_citations": 4}
{"id": 1582518, "s2_id": "94a58fe2a2e3e34eb040b383d2f6f9bc336515a9", "title": "Reliable system specification for self-checking data-paths", "abstract": "The design of reliable circuits has received a lot of attention in the past, leading to the definition of several design techniques introducing fault detection and fault tolerance properties in systems for critical applications/environments. Such design methodologies tackled the problem at different abstraction levels, from switch-level to logic, RT level, and more recently to system level. The aim of this paper is to introduce a novel system-level technique based on the redefinition of the operator functionality in the system specification. This technique provides reliability properties to the system data path, transparently with respect to the designer. Feasibility, fault coverage, performance degradation and overheads are investigated on a FIR circuit.", "venue": "Design, Automation and Test in Europe", "authors": ["Cristiana  Bolchini", "Fabio  Salice", "Donatella  Sciuto", "Luigi  Pomante"], "year": 2005, "n_citations": 13}
{"id": 1582723, "s2_id": "6c29d99009e1c2dd43a0fc52ccaba70a9575af08", "title": "ASIC-based Implementation of Synchronous Section-Carry Based Carry Lookahead Adders", "abstract": "The section-carry based carry lookahead adder (SCBCLA) topology was proposed as an improved high-speed alternative to the conventional carry lookahead adder (CCLA) topology in previous works. Self-timed and FPGA-based implementations of SCBCLAs and CCLAs were considered earlier, and it was found that SCBCLAs could help in delay reduction i.e. pave the way for improved speed compared to CCLAs at the expense of some increase in area and/or power parameters. In this work, we consider semi-custom ASIC-based implementations of different variants of SCBCLAs and CCLAs to perform 32-bit dual-operand addition. Based on the simulation results for 32-bit dual-operand addition obtained by targeting a high-end 32/28nm CMOS process, it is found that an optimized SCBCLA architecture reports a 9.8% improvement in figure-of-merit (FOM) compared to an optimized CCLA architecture, where the FOM is defined as the inverse of the product of power, delay, and area. It is generally inferred from the simulations that the SCBCLA architecture could be more beneficial compared to the CCLA architecture in terms of the design metrics whilst benefitting a variety of computer arithmetic operations involving dual-operand and/or multi-operand additions. Also, it is observed that heterogeneous CLA architectures tend to fare well compared to homogeneous CLA architectures, as substantiated by the simulation results.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 4}
{"id": 1583444, "s2_id": "24c3c129b6f2a6884c4b146985b706686d4303ec", "title": "Assessing random dynamical network architectures for nanoelectronics", "abstract": "Independent of the technology, it is generally expected that future nanoscale devices will be built from vast numbers of densely arranged devices that exhibit high failure rates. Other than that, there is little consensus on what type of technology and computing architecture holds most promises to go far beyond todaypsilas top-down engineered silicon devices. Cellular automata (CA) have been proposed in the past as a possible class of architectures to the von Neumann computing architecture, which is not generally well suited for future massively parallel and fine-grained nanoscale electronics. While the top-down engineered semiconducting technology favors regular and locally interconnected structures, future bottom-up self-assembled devices tend to have irregular structures because of the current lack of precise control over these processes. In this paper, we will assess random dynamical networks, namely random Boolean networks (RBNs) and random threshold networks (RTNs), as alternative computing architectures and models for future information processing devices. We will illustrate that - from a theoretical perspective - they offer superior properties over classical CA-based architectures, such as inherent robustness as the system scales up, more efficient information processing capabilities, and manufacturing benefits for bottom-up designed devices, which motivates this investigation. We will present recent results on the dynamic behavior and robustness of such random dynamical networks while also including manufacturing issues in the assessment.", "venue": "2008 IEEE International Symposium on Nanoscale Architectures", "authors": ["Christof  Teuscher", "Natali  Gulbahce", "Thimo  Rohlf"], "year": 2008, "n_citations": 11}
{"id": 1586803, "s2_id": "e6dd204b1639af77eb53ab42bc9d95a3c3358022", "title": "FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer Learning", "abstract": "The computational demands of computer vision tasks based on state-of-the-art Convolutional Neural Network (CNN) image classification far exceed the energy budgets of mobile devices. This paper proposes FixyNN, which consists of a fixed-weight feature extractor that generates ubiquitous CNN features, and a conventional programmable CNN accelerator which processes a dataset-specific CNN. Image classification models for FixyNN are trained end-to-end via transfer learning, with the common feature extractor representing the transfered part, and the programmable part being learnt on the target dataset. Experimental results demonstrate FixyNN hardware can achieve very high energy efficiencies up to 26.6 TOPS/W ($4.81 \\times$ better than iso-area programmable accelerator). Over a suite of six datasets we trained models via transfer learning with an accuracy loss of $<1\\%$ resulting in up to 11.2 TOPS/W - nearly $2 \\times$ more efficient than a conventional programmable CNN accelerator of the same area.", "venue": "ArXiv", "authors": ["Paul N. Whatmough", "Chuteng  Zhou", "Patrick  Hansen", "Shreyas K. Venkataramanaiah", "Jae-sun  Seo", "Matthew  Mattina"], "year": 2019, "n_citations": 37}
{"id": 1587592, "s2_id": "3b98ba0b2cb308b87c78c154732e7dda22893d2e", "title": "Neat: Low-Complexity, Efficient On-Chip Cache Coherence", "abstract": "Cache coherence protocols such as MESI that use writer-initiated invalidation have high complexity\u2014and sometimes have poor performance and energy usage, especially under false sharing. Such protocols require numerous transient states, a shared directory, and support for core-to-core communication, while also suffering under false sharing. An alternative to MESI\u2019s writer-initiated invalidation is self-invalidation, which achieves lower complexity than MESI but adds high performance costs or relies on programmer annotations or specific data access patterns. This paper presents Neat, a low-complexity, efficient cache coherence protocol. Neat uses self-invalidation, thus avoiding MESI\u2019s transient states, directory, and core-to-core communication requirements. Neat uses novel mechanisms that effectively avoid many unnecessary self-invalidations. An evaluation shows that Neat is simple and has lower verification complexity than the MESI protocol. Neat not only outperforms state-of-theart self-invalidation protocols, but its performance and energy consumption are comparable to MESI\u2019s, and it outperforms MESI under false sharing.", "venue": "ArXiv", "authors": ["Rui  Zhang", "Swarnendu  Biswas", "Vignesh  Balaji", "Michael D. Bond", "Brandon  Lucia"], "year": 2021, "n_citations": 0}
{"id": 1595374, "s2_id": "0cc9f0cf9fb45e57cdfea47f84b20474df4b9c61", "title": "Tiered-Latency DRAM: Enabling Low-Latency Main Memory at Low Cost", "abstract": "This paper summarizes the idea of Tiered-Latency DRAM (TL-DRAM), which was published in HPCA 2013, and examines the work's significance and future potential. The capacity and cost-per-bit of DRAM have historically scaled to satisfy the needs of increasingly large and complex computer systems. However, DRAM latency has remained almost constant, making memory latency the performance bottleneck in today's systems. We observe that the high access latency is not intrinsic to DRAM, but a trade-off is made to decrease the cost per bit. To mitigate the high area overhead of DRAM sensing structures, commodity DRAMs connect many DRAM cells to each sense amplifier through a wire called a bitline. These bit-lines have a high parasitic capacitance due to their long length, and this bitline capacitance is the dominant source of DRAM latency. Specialized low-latency DRAMs use shorter bitlines with fewer cells, but have a higher cost-per-bit due to greater sense amplifier area overhead. To achieve both low latency and low cost per bit, we introduce Tiered-Latency DRAM (TL-DRAM). In TL-DRAM, each long bitline is split into two shorter segments by an isolation transistor, allowing one of the two segments to be accessed with the latency of a short-bitline DRAM without incurring a high cost per bit. We propose mechanisms that use the low-latency segment as a hardware-managed or software-managed cache. Our evaluations show that our proposed mechanisms improve both performance and energy efficiency for both single-core and multiprogrammed workloads. Tiered-Latency DRAM has inspired several other works on reducing DRAM latency with little to no architectural modification.", "venue": "ArXiv", "authors": ["Donghyuk  Lee", "Yoongu  Kim", "Vivek  Seshadri", "Jamie  Liu", "Lavanya  Subramanian", "Onur  Mutlu"], "year": 2018, "n_citations": 1}
{"id": 1600282, "s2_id": "726650bd2aede13935385d44470292da10a7ac56", "title": "IRONHIDE: A Secure Multicore that Efficiently Mitigates Microarchitecture State Attacks for Interactive Applications", "abstract": "Microprocessors enable aggressive hardware virtualization by means of which multiple processes temporally execute on the system. These security-critical and ordinary processes interact with each other to assure application progress. However, temporal sharing of hardware resources exposes the processor to various microarchitecture state attacks. State-of-the-art secure processors, such as MI6 adopt Intel's SGX enclave execution model. MI6 architects strong isolation by statically isolating shared memory state, and purging the microarchitecture state of private core, cache, and TLB resources on every enclave entry and exit. The purging overhead significantly impacts performance as the interactivity across the secure and insecure processes increases. This paper proposes IRONHIDE that implements strong isolation in the context of multicores to form spatially isolated secure and insecure clusters of cores. For an interactive application comprising of secure and insecure processes, IRONHIDE pins the secure process(es) to the secure cluster, where they execute and interact with the insecure process(es) without incurring the microarchitecture state purging overheads on every interaction event. IRONHIDE improves performance by 2.1x over the MI6 baseline for a set of user and OS interactive applications. Moreover, IRONHIDE improves performance by 20% over an SGX-like baseline, while also ensuring strong isolation guarantees against microarchitecture state attacks.", "venue": "2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Hamza  Omar", "Omer  Khan"], "year": 2020, "n_citations": 4}
{"id": 1600845, "s2_id": "59c44928c4b85e6460a2e7a46da0c3586e2f770e", "title": "Hard Disk Drive as a Magnetomechanical Logic Device", "abstract": "We consider the conditions how two binary numbers can be superimposed on the same track with the use of different recording magnetic fields. As a result the average magnetization of longitudinal medium along the track can have three states: -M, 0 and +M. Possibility to perform logic operations with these states is considered. We demonstrate OR, AND, XOR and NOT operations and discuss a modification of a recording device.", "venue": "ArXiv", "authors": ["Vladimir L. Safonov"], "year": 2006, "n_citations": 0}
{"id": 1606061, "s2_id": "41e70b64c645693fac4c640a25a77909a32e4b7f", "title": "Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead", "abstract": "Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. \u2014Partha Pratim Pande, Washington State University", "venue": "IEEE Design & Test", "authors": ["Muhammad  Shafique", "Mahum  Naseer", "Theocharis  Theocharides", "Christos  Kyrkou", "Onur  Mutlu", "Lois  Orosa", "Jungwook  Choi"], "year": 2020, "n_citations": 36}
{"id": 1607321, "s2_id": "56b5574b1c02e65b8d2250bcd198d6fbeb5789e1", "title": "HERMES: A Hierarchical Broadcast-Based Silicon Photonic Interconnect for Scalable Many-Core Systems", "abstract": "Optical interconnection networks, as enabled by recent advances in silicon photonic device and fabrication technology, have the potential to address on-chip and off-chip communication bottlenecks in many-core systems. Although several designs have shown superior power efficiency and performance compared to electrical alternatives, these networks will not scale to the thousands of cores required in the future. \nIn this paper, we introduce Hermes, a hybrid network composed of an optimized broadcast for power-efficient low-latency global-scale coordination and circuit-switch sub-networks for high-throughput data delivery. This network will scale for use in thousand core chip systems. At the physical level, SoI-based adiabatic coupler has been designed to provide low-loss and compact optical power splitting. Based on the adiabatic coupler, a topology based on 2-ary folded butterfly is designed to provide linear power division in a thousand core layout with minimal cross-overs. To address the network agility and provide for efficient use of optical bandwidth, a flow control and routing mechanism is introduced to dynamically allocate bandwidth and provide fairness usage of network resources. At the system level, bloom filter-based filtering for localization of communication are designed for reducing global traffic. In addition, a novel greedy-based data and workload migration are leveraged to increase the locality of communication in a NUCA (non-uniform cache access) architecture. First order analytic evaluation results have indicated that Hermes is scalable to at least 1024 cores and offers significant performance improvement and power savings over prior silicon photonic designs.", "venue": "ArXiv", "authors": ["Moustafa  Mohamed", "Zheng  Li", "Xi  Chen", "Alan Rolf Mickelson"], "year": 2014, "n_citations": 4}
{"id": 1609933, "s2_id": "b47afa0414c402a76b4c0da8810c1783ae27914f", "title": "Accelerating bandwidth-bound deep learning inference with main-memory accelerators", "abstract": "Matrix-matrix multiplication operations (GEMMs) are important in many HPC and machine-learning applications. They are often mapped to discrete accelerators (e.g., GPUs) to improve performance. However, we find that large tall/skinny and fat/short matrices benefit little from discrete acceleration and also do not perform well on a CPU. Such matrices are prevalent in important workloads, such as deep-learning inference within large-scale datacenters. We demonstrate the large potential of accelerating these GEMMs with processing in the main CPU memory, where processing in memory units (PIMs) take advantage of otherwise untapped bandwidth without requiring data copies. We develop a novel GEMM execution flow and corresponding memory-side address-generation logic that exploits GEMM locality and enables long-running PIM kernels despite the complex address-mapping functions employed by the CPU. Our evaluation of StepStone variants at the channel, device, and within-device PIM levels demonstrate 12X better minimum latency than a CPU and 2.8X greater throughput for strict query latency constraints. End-to-end performance analysis of recent recommendation and language models shows that StepStone outperforms a fast CPU by up to 16X and also the best prior main-memory acceleration approaches by up to 2.4X.", "venue": "SC", "authors": ["Benjamin Y. Cho", "Jeageun  Jung", "Mattan  Erez"], "year": 2021, "n_citations": 2}
{"id": 1612629, "s2_id": "f8f688fe7a98db4f93b63f7a6a52dd884620e75f", "title": "BILBO-friendly hybrid BIST architecture with asymmetric polynomial reseeding", "abstract": "By advances in technology, integrated circuits have come to include more functionality and more complexity in a single chip. Although methods of testing have improved, but the increase in complexity of circuits, keeps testing a challenging problem. Two important challenges in testing of digital circuits are test time and accessing the circuit under test (CUT) for testing. These challenges become even more important in complex system on chip (SoC) zone. This paper presents a multistage test strategy to be implemented on a BIST architecture for reducing test time of a simple core as solution for more global application of SoC testing strategy. This strategy implements its test pattern generation and output response analyzer in a BILBO architecture. The proposed method benefits from an irregular polynomial BILBO (IP-BILBO) structure to improve its test results. Experimental results on ISCAS-89 benchmark circuits show an average of 35% improvement in test time in proportion to pervious work.", "venue": "The 16th CSI International Symposium on Computer Architecture and Digital Systems (CADS 2012)", "authors": ["Elaheh  Sadredini", "Mohammadreza  Najafi", "Mahmood  Fathy", "Zainalabedin  Navabi"], "year": 2012, "n_citations": 7}
{"id": 1612954, "s2_id": "a647632e20e6dae839b1a4f10a581530da45e8e7", "title": "RVCoreP : An optimized RISC-V soft processor of five-stage pipelining", "abstract": "RISC-V is a RISC based open and loyalty free instruction set architecture which has been developed since 2010, and can be used for cost-effective soft processors on FPGAs. The basic 32-bit integer instruction set in RISC-V is defined as RV32I, which is sufficient to support the operating system environment and suits for embedded systems. In this paper, we propose an optimized RV32I soft processor named RVCoreP adopting five-stage pipelining. The processor applies three effective optimization methods to improve the operating frequency. These methods are instruction fetch unit optimization including pipelined branch prediction mechanism, ALU optimization, and data alignment and sign-extension optimization for data memory output. We implement RVCoreP in Verilog HDL and verify the behavior using Verilog simulation and an actual Xilinx Atrix-7 FPGA board. We evaluate IPC (instructions per cycle), operating frequency, hardware resource utilization, and processor performance. From the evaluation results, we show that RVCoreP achieves 30.0% performance improvement compared with VexRiscv, which is a high-performance and open source RV32I processor selected from some related works.", "venue": "IEICE Trans. Inf. Syst.", "authors": ["Hiromu  Miyazaki", "Takuto  Kanamori", "Md Ashraful Islam", "Kenji  Kise"], "year": 2020, "n_citations": 2}
{"id": 1613595, "s2_id": "8a29378973987bdb040f35349d1c5a86a538c0fc", "title": "Hierarchical Temporal Memory Using Memristor Networks: A Survey", "abstract": "This paper presents a survey of the currently available hardware designs for implementation of the human cortex inspired algorithm, Hierarchical Temporal Memory (HTM). In this review, we focus on the state-of-the-art advances of memristive HTM implementation and related HTM applications. With the advent of edge computing, HTM can be a potential algorithm to implement on-chip near sensor data processing. The comparison of analog memristive circuit implementations with the digital and mixed-signal solutions is provided. The advantages of memristive HTM over digital implementations against performance metrics such as processing speed, reduced on-chip area, and power dissipation are discussed. The limitations and open problems concerning the memristive HTM, such as the design scalability, sneak currents, leakage, parasitic effects, lack of the analog learning circuits implementations, and unreliability of the memristive devices integrated with CMOS circuits are also discussed.", "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence", "authors": ["Olga  Krestinskaya", "Irina  Dolzhikova", "Alex Pappachen James"], "year": 2018, "n_citations": 26}
{"id": 1617380, "s2_id": "e68bd42e8f46beb7bcccbdb1a42eeace4f0f2717", "title": "Exploiting real-time FPGA based adaptive systems technology for real-time sensor fusion in next generation automotive safety systems", "abstract": "We present a system for the boresighting of sensors using inertial measurement devices as the basis for developing a range of dynamic real-time sensor fusion applications. The proof of concept utilizes a COTS FPGA platform for sensor fusion and real-time correction of a misaligned video sensor. We exploit a custom-designed 32-bit soft processor core and C-based design-and-synthesis for rapid, platform-neutral development. Kalman filter and sensor fusion techniques established in advanced aviation systems are applied to automotive vehicles with results exceeding typical industry requirements for sensor alignment. Results of static and dynamic tests demonstrate that using inexpensive accelerometers mounted on (or during assembly of) a sensor and an inertial measurement unit (IMU) fixed to a vehicle can be used to compute the misalignment of the sensor to the IMU and thus the vehicle. In some cases, the model predications and test results exceeded the requirements by an order of magnitude with a 3-sigma or 99% confidence.", "venue": "Design, Automation and Test in Europe", "authors": ["Steve  Chappell", "Alistair  Macarthur", "Dan  Preston", "Dave  Olmstead", "Bob  Flint", "Chris  Sullivan"], "year": 2005, "n_citations": 7}
{"id": 1619628, "s2_id": "97b2d1bc0e63a141848d60222d7bded1e5d812f7", "title": "Performance Analysis of 6T and 9T SRAM", "abstract": "The SRAM cell is made up of latch, which ensures that the cell data is preserved as long as power is turned on and refresh operation is not required for the SRAM cell. SRAM is widely used for on-chip cache memory in microprocessors, game software, computers, workstations, portable handheld devices due to high data speed, low power consumption, low voltage supply, no-refresh needed. Therefore, to build a reliable cache/memory, the individual cell (SRAM) must be designed to have high Static Noise Margin (SNM). In sub-threshold region, conventional 6T-cell SRAM experiences poor read and write ability, and reduction in the SNM at various fluctuation of the threshold voltage, supply voltage down scaling, and technology scaling in nano-meter ranges (180nm, 90nm, 45nm, 22nm, 16nm and 10nm). Thus, noise margin becomes worse during read and write operations compared to hold operation which the internal feedback operates independent of the access transistors. Due to these limitations of the conventional 6T SRAM cell, we have proposed a 9T SRAM that will drastically minimize these limitations; the extra three transistors added to the 6T topology will improve the read, hold and write SNM. The design and simulation results were carried out using Cadence Virtuoso to evaluate the performance of 6T and 9T SRAM cells.", "venue": "ArXiv", "authors": ["Apollos  Ezeogu"], "year": 2019, "n_citations": 4}
{"id": 1623192, "s2_id": "65942c2d3677de2acca670b09b2aa70983e5d0d8", "title": "RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing", "abstract": "Personalized recommendation systems leverage deep learning models and account for the majority of data center AI cycles. Their performance is dominated by memory-bound sparse embedding operations with unique irregular memory access patterns that pose a fundamental challenge to accelerate. This paper proposes a lightweight, commodity DRAM compliant, near-memory processing solution to accelerate personalized recommendation inference. The in-depth characterization of production-grade recommendation models shows that embedding operations with high model-, operator- and data-level parallelism lead to memory bandwidth saturation, limiting recommendation inference performance. We propose RecNMP which provides a scalable solution to improve system throughput, supporting a broad range of sparse embedding models. RecNMP is specifically tailored to production environments with heavy co-location of operators on a single server. Several hardware/software co-optimization techniques such as memory-side caching, table-aware packet scheduling, and hot entry profiling are studied, providing up to $9.8 \\times$ memory latency speedup over a highly-optimized baseline. Overall, RecNMP offers $4.2 \\times$ throughput improvement and 45.8% memory energy savings.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Liu  Ke", "Udit  Gupta", "Carole-Jean  Wu", "Benjamin Youngjae Cho", "Mark  Hempstead", "Brandon  Reagen", "Xuan  Zhang", "David  Brooks", "Vikas  Chandra", "Utku  Diril", "Amin  Firoozshahian", "Kim  Hazelwood", "Bill  Jia", "Hsien-Hsin S. Lee", "Meng  Li", "Bert  Maher", "Dheevatsa  Mudigere", "Maxim  Naumov", "Martin  Schatz", "Mikhail  Smelyanskiy", "Xiaodong  Wang"], "year": 2020, "n_citations": 46}
{"id": 1624647, "s2_id": "1cf8190ff1087454c31eb0c5bb3de5f12a53aa5d", "title": "Architectural improvements and 28 nm FPGA implementation of the APEnet+ 3D Torus network for hybrid HPC systems", "abstract": "Modern Graphics Processing Units (GPUs) are now considered accelerators for general purpose computation. A tight interaction between the GPU and the interconnection network is the strategy to express the full potential on capability computing of a multi-GPU system on large HPC clusters; that is the reason why an efficient and scalable interconnect is a key technology to finally deliver GPUs for scientific HPC. In this paper we show the latest architectural and performance improvement of the APEnet+ network fabric, a FPGA-based PCIe board with 6 fully bidirectional off-board links with 34 Gbps of raw bandwidth per direction, and X8 Gen2 bandwidth towards the host PC. The board implements a Remote Direct Memory Access (RDMA) protocol that leverages upon peer-to-peer (P2P) capabilities of Fermi- and Kepler-class NVIDIA GPUs to obtain real zero-copy, low-latency GPU-to-GPU transfers. Finally, we report on the development activities for 2013 focusing on the adoption of the latest generation 28 nm FPGAs and the preliminary tests performed on this new platform.", "venue": "ArXiv", "authors": ["Roberto  Ammendola", "Andrea  Biagioni", "Ottorino  Frezza", "Francesca Lo Cicero", "Pier Stanislao Paolucci", "Alessandro  Lonardo", "Davide  Rossetti", "Francesco  Simula", "Laura  Tosoratto", "Piero  Vicini"], "year": 2013, "n_citations": 0}
{"id": 1625179, "s2_id": "194b64bd5085708dd23155a127df36fea408af1c", "title": "Rainbow: A composable coherence protocol for multi\u2010chip servers", "abstract": "The use of multi\u2010chip modules (MCM) and/or multi\u2010socket boards is the most suitable approach to increase the computation density of servers while keep chip yield attained. This article introduces a new coherence protocol suitable, in terms of complexity and scalability, for this class of systems. The proposal uses two complementary ideas: (1) A mechanism that dissociates complexity from performance by means of colored\u2010token counting, (2) A construct that optimizes performance and cost by means of two functionally symmetrical structures working in the last level cache of each chip and each memory controller. The coordinated work of both structures minimizes the coherence\u2010related effects on the average memory latency perceived by the processor. Our proposal is able to improve on the performance of a HyperTransport\u2010like coherence protocol by from 25% to 60%.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Lucia G. Menezo", "Valentin  Puente", "Jose A. Gregorio"], "year": 2020, "n_citations": 0}
{"id": 1627044, "s2_id": "c662015473873d9c8c4195bf812fcf9d0cd0caa4", "title": "An efficient floating point multiplier design for high speed applications using Karatsuba algorithm and Urdhva-Tiryagbhyam algorithm", "abstract": "Floating point multiplication is a crucial operation in high power computing applications such as image processing, signal processing etc. And also multiplication is the most time and power consuming operation. This paper proposes an efficient method for IEEE 754 floating point multiplication which gives a better implementation in terms of delay and power. A combination of Karatsuba algorithm and Urdhva-Tiryagbhyam algorithm (Vedic Mathematics) is used to implement unsigned binary multiplier for mantissa multiplication. The multiplier is implemented using Verilog HDL, targeted on Spartan-3E and Virtex-4 FPGA.", "venue": "2015 International Conference on Signal Processing and Communication (ICSC)", "authors": ["S.  Arish", "R. K. Sharma"], "year": 2015, "n_citations": 16}
{"id": 1627386, "s2_id": "0c778c4133cbd1c7b24c92d360f7ce0aadae5c25", "title": "Top-Down Transaction-Level Design with TL-Verilog", "abstract": "Transaction-Level Verilog (TL-Verilog) is an emerging extension to SystemVerilog that supports a new design methodology, called transaction-level design. A transaction, in this methodology, is an entity that moves through structures like pipelines, arbiters, and queues, A transaction might be a machine instruction, a flit of a packet, or a memory read/write. Transaction logic, like packet header decode or instruction execution, that operates on the transaction can be placed anywhere along the transaction's flow. Tools produce the logic to carry signals through their flows to stitch the transaction logic. \nWe implemented a small library of TL-Verilog flow components, and we illustrate the use of these components in a top-down design methodology. We construct a hypothetical microarchitecture simply by instantiating components. Within the flows created by these components, we add combinational transaction logic, enabling verification activities and performance evaluation to begin. We then refine the model by positioning the transaction logic within its flow to produce a high-quality register-transfer-level (RTL) implementation.", "venue": "ArXiv", "authors": ["Steven F. Hoover", "Ahmed  Salman"], "year": 2018, "n_citations": 1}
{"id": 1630288, "s2_id": "10e6e5f3648fb74fad68827739cf15915c7301df", "title": "RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks with Fine-Grain Utilization", "abstract": "Many emerging cyber-physical systems, such as autonomous vehicles and robots, rely heavily on artificial intelligence and machine learning algorithms to perform important system operations. Since these highly parallel applications are computationally intensive, they need to be accelerated by graphics processing units (GPUs) to meet stringent timing constraints. However, despite the wide adoption of GPUs, efficiently scheduling multiple GPU applications while providing rigorous real-time guarantees remains a challenge. In this paper, we propose RTGPU, which can schedule the execution of multiple GPU applications in real-time to meet hard deadlines. Each GPU application can have multiple CPU execution and memory copy segments, as well as GPU kernels. We start with a model to explicitly account for the CPU and memory copy segments of these applications. We then consider the GPU architecture in the development of a precise timing model for the GPU kernels and leverage a technique known as persistent threads to implement fine-grained kernel scheduling with improved performance through interleaved execution. Next, we propose a general method for scheduling parallel GPU applications in real time. Finally, to schedule multiple parallel GPU applications, we propose a practical real-time scheduling algorithm based on federated scheduling and grid search (for GPU kernel segments) with uniprocessor fixed priority scheduling (for multiple CPU and memory copy segments). Our approach provides superior schedulability compared with previous work, and gives real-time guarantees to meet hard deadlines for multiple GPU applications according to comprehensive validation and evaluation on a real NVIDIA GTX1080Ti GPU system.", "venue": "ArXiv", "authors": ["An  Zou", "Jing  Li", "Christopher D. Gill", "Xuan  Zhang"], "year": 2021, "n_citations": 0}
{"id": 1638718, "s2_id": "39fc8003d6829d1897ecd31c5abb3a271aeda669", "title": "Hardware Implementation of Successive-Cancellation Decoders for Polar Codes", "abstract": "The recently-discovered polar codes are seen as a major breakthrough in coding theory; they provably achieve the theoretical capacity of discrete memoryless channels using the low-complexity successive cancellation decoding algorithm. Motivated by recent developments in polar coding theory, we propose a family of efficient hardware implementations for successive cancellation (SC) polar decoders. We show that such decoders can be implemented with O(N) processing elements and O(N) memory elements. Furthermore, we show that SC decoding can be implemented in the logarithmic domain, thereby eliminating costly multiplication and division operations, and reducing the complexity of each processing element greatly. We also present a detailed architecture for an SC decoder and provide logic synthesis results confirming the linear complexity growth of the decoder as the code length increases.", "venue": "J. Signal Process. Syst.", "authors": ["Camille  Leroux", "Alexandre J. Raymond", "Gabi  Sarkis", "Ido  Tal", "Alexander  Vardy", "Warren J. Gross"], "year": 2012, "n_citations": 60}
{"id": 1640623, "s2_id": "5b579588fb6d31db175ed98fd541a73cac3fe274", "title": "Variable Block Carry Skip Logic using Reversible Gates", "abstract": "Reversible circuits have applications in digital signal processing, computer graphics, quantum computation and cryptography. In this paper, a generalized k*k reversible gate family is proposed and a 3*3 gate of the family is discussed. Inverter, AND, OR, NAND, NOR, and EXOR gates can be realized by this gate. Implementation of a full-adder circuit using two such 3*3 gates is given. This full-adder circuit contains only two reversible gates and produces no extra garbage outputs. The proposed full-adder circuit is efficient in terms of gate count, garbage outputs and quantum cost. A 4-bit carry skip adder is designed using this full-adder circuit and a variable block carry skip adder is discussed. Necessary equations required to evaluate these adder are presented.", "venue": "ArXiv", "authors": ["Md. Rafiqul Islam", "Md. Saiful Islam", "Muhammad Rezaul Karim", "Abdullah Al Mahmud", "Hafiz Md. Hasan Babu"], "year": 2010, "n_citations": 14}
{"id": 1649238, "s2_id": "394a3b11dbee1d5ce04e2f3e7f6cdd4ef81e38b1", "title": "Skew-Oblivious Data Routing for Data Intensive Applications on FPGAs with HLS", "abstract": "FPGAs have become emerging computing infrastructures for accelerating applications in datacenters. Meanwhile, high-level synthesis (HLS) tools have been proposed to ease the programming of FPGAs. Even with HLS, irregular data-intensive applications require explicit optimizations, among which multiple processing elements (PEs) with each owning a private BRAM-based buffer are usually adopted to process multiple data per cycle. Data routing, which dynamically dispatches multiple data to designated PEs, avoids data replication in buffers compared to statically assigning data to PEs, hence saving BRAM usage. However, the workload imbalance among PEs vastly diminishes performance when processing skew datasets. In this paper, we propose a skew-oblivious data routing architecture that allocates secondary PEs and schedules them to share the workload of the overloaded PEs at run-time. In addition, we integrate the proposed architecture into a framework called Ditto to minimize the development efforts for applications that require skew handling. We evaluate Ditto on five commonly used applications: histogram building, data partitioning, pagerank, heavy hitter detection and hyperloglog. The results demonstrate that the generated implementations are robust to skew datasets and outperform the state-of-the-art designs in both throughput and BRAM usage efficiency.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Xinyu  Chen", "Hongshi  Tan", "Yao  Chen", "Bingsheng  He", "Weng-Fai  Wong", "Deming  Chen"], "year": 2021, "n_citations": 0}
{"id": 1655890, "s2_id": "643138c18c03b4895fb82fa5a1c326cc0538c0cf", "title": "Improving DRAM performance by parallelizing refreshes with accesses", "abstract": "Modern DRAM cells are periodically refreshed to prevent data loss due to leakage. Commodity DDR (double data rate) DRAM refreshes cells at the rank level. This degrades performance significantly because it prevents an entire DRAM rank from serving memory requests while being refreshed. DRAM designed for mobile platforms, LPDDR (low power DDR) DRAM, supports an enhanced mode, called per-bank refresh, that refreshes cells at the bank level. This enables a bank to be accessed while another in the same rank is being refreshed, alleviating part of the negative performance impact of refreshes. Unfortunately, there are two shortcomings of per-bank refresh employed in today's systems. First, we observe that the perbank refresh scheduling scheme does not exploit the full potential of overlapping refreshes with accesses across banks because it restricts the banks to be refreshed in a sequential round-robin order. Second, accesses to a bank that is being refreshed have to wait. To mitigate the negative performance impact of DRAM refresh, we propose two complementary mechanisms, DARP (Dynamic Access Refresh Parallelization) and SARP (Subarray Access Refresh Parallelization). The goal is to address the drawbacks of per-bank refresh by building more efficient techniques to parallelize refreshes and accesses within DRAM. First, instead of issuing per-bank refreshes in a round-robin order, as it is done today, DARP issues per-bank refreshes to idle banks in an out-of-order manner. Furthermore, DARP proactively schedules refreshes during intervals when a batch of writes are draining to DRAM. Second, SARP exploits the existence of mostly-independent subarrays within a bank. With minor modifications to DRAM organization, it allows a bank to serve memory accesses to an idle subarray while another subarray is being refreshed. Extensive evaluations on a wide variety of workloads and systems show that our mechanisms improve system performance (and energy efficiency) compared to three state-of-the-art refresh policies and the performance benefit increases as DRAM density increases.", "venue": "2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Kevin K. Chang", "Donghyuk  Lee", "Zeshan  Chishti", "Alaa R. Alameldeen", "Chris  Wilkerson", "Yoongu  Kim", "Onur  Mutlu"], "year": 2014, "n_citations": 182}
{"id": 1658034, "s2_id": "fc29fd70332a3e4b70c65b5e2b1d59b94fb2e171", "title": "Control Variate Approximation for DNN Accelerators", "abstract": "In this work, we introduce a control variate approximation technique for low error approximate Deep Neural Network (DNN) accelerators. The control variate technique is used in Monte Carlo methods to achieve variance reduction. Our approach significantly decreases the induced error due to approximate multiplications in DNN inference, without requiring time-exhaustive retraining compared to state-of-the-art. Leveraging our control variate method, we use highly approximated multipliers to generate power-optimized DNN accelerators. Our experimental evaluation on six DNNs, for Cifar-10 and Cifar100 datasets, demonstrates that, compared to the accurate design, our control variate approximation achieves same performance and 24% power reduction for a merely 0.16% accuracy loss.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Georgios  Zervakis", "Ourania  Spantidi", "Iraklis  Anagnostopoulos", "Hussam  Amrouch", "Jorg  Henkel"], "year": 2021, "n_citations": 1}
{"id": 1667387, "s2_id": "4c010249d6e0d9cc2cce36040aec7da3744eec9e", "title": "A high-performance triple patterning layout decomposer with balanced density", "abstract": "Triple patterning lithography (TPL) has received more and more attentions from industry as one of the leading candidate for 14nm/11nm nodes. In this paper, we propose a high performance layout decomposer for TPL. Density balancing is seamlessly integrated into all key steps in our TPL layout decomposition, including density-balanced semi-definite programming (SDP), density-based mapping, and density-balanced graph simplification. Our new TPL decomposer can obtain high performance even compared to previous state-of-the-art layout decomposers which are not balanced-density aware, e.g., by Yu et al. (ICCAD'11), Fang et al. (DAC'12), and Kuang et al. (DAC'13). Furthermore, the balanced-density version of our decomposer can provide more balanced density which leads to less edge placement error (EPE), while the conflict and stitch numbers are still very comparable to our non-balanced-density baseline.", "venue": "2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Bei  Yu", "Yen-Hung  Lin", "Gerard  Luk-Pat", "Duo  Ding", "Kevin  Lucas", "David Z. Pan"], "year": 2013, "n_citations": 56}
{"id": 1667429, "s2_id": "c6f89f02330976d504e04d3b47721ca92b026a82", "title": "Performance Modeling of Streaming Kernels and Sparse Matrix-Vector Multiplication on A64FX", "abstract": "The A64FX CPU powers the current #1 supercomputer on the Top500 list. Although it is a traditional cache-based multicore processor, its peak performance and memory bandwidth rival accelerator devices. Generating efficient code for such a new architecture requires a good understanding of its performance features. Using these features, we construct the Execution-Cache-Memory (ECM) performance model for the A64FX processor in the FX700 supercomputer and validate it using streaming loops. We also identify architectural peculiarities and derive optimization hints. Applying the ECM model to sparse matrix-vector multiplication (SpMV), we motivate why the CRS matrix storage format is inappropriate and how the SELL-C-\u03c3 format with suitable code optimizations can achieve bandwidth saturation for SpMV.", "venue": "2020 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)", "authors": ["Christie L. Alappat", "Jan  Laukemann", "Thomas  Gruber", "Georg  Hager", "Gerhard  Wellein", "Nils  Meyer", "Tilo  Wettig"], "year": 2020, "n_citations": 5}
{"id": 1667610, "s2_id": "703beb96bf664cc78c937345c96abe049bbc17f4", "title": "G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency", "abstract": "Graph Neural Networks (GNNs) have emerged as the state-of-the-art (SOTA) method for graph-based learning tasks. However, it still remains prohibitively challenging to inference GNNs over large graph datasets, limiting their application to large-scale real-world tasks. While end-to-end jointly optimizing GNNs and their accelerators is promising in boosting GNNs' inference efficiency and expediting the design process, it is still underexplored due to the vast and distinct design spaces of GNNs and their accelerators. In this work, we propose G-CoS, a GNN and accelerator co-search framework that can automatically search for matched GNN structures and accelerators to maximize both task accuracy and acceleration efficiency. Specifically, G-CoS integrates two major enabling components: (1) a generic GNN accelerator search space which is applicable to various GNN structures and (2) a one-shot GNN and accelerator co-search algorithm that enables simultaneous and efficient search for optimal GNN structures and their matched accelerators. To the best of our knowledge, G-CoS is the first co-search framework for GNNs and their accelerators. Extensive experiments and ablation studies show that the GNNs and accelerators generated by G-CoS consistently outperform SOTA GNNs and GNN accelerators in terms of both task accuracy and hardware efficiency, while only requiring a few hours for the end-to-end generation of the best matched GNNs and their accelerators.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Yongan  Zhang", "Haoran  You", "Yonggan  Fu", "Tong  Geng", "Ang  Li", "Yingyan  Lin"], "year": 2021, "n_citations": 3}
{"id": 1671628, "s2_id": "19222f2fb07e7d30877d441be8bd39f9e12e3941", "title": "Hardware-Efficient Structure of the Accelerating Module for Implementation of Convolutional Neural Network Basic Operation", "abstract": "This paper presents a structural design of the hardware-efficient module for implementation of convolution neural network (CNN) basic operation with reduced implementation complexity. For this purpose we utilize some modification of the Winograd minimal filtering method as well as computation vectorization principles. This module calculate inner products of two consecutive segments of the original data sequence, formed by a sliding window of length 3, with the elements of a filter impulse response. The fully parallel structure of the module for calculating these two inner products, based on the implementation of a naive method of calculation, requires 6 binary multipliers and 4 binary adders. The use of the Winograd minimal filtering method allows to construct a module structure that requires only 4 binary multipliers and 8 binary adders. Since a high-performance convolutional neural network can contain tens or even hundreds of such modules, such a reduction can have a significant effect.", "venue": "ArXiv", "authors": ["Aleksandr  Cariow", "Galina  Cariowa"], "year": 2018, "n_citations": 3}
{"id": 1673076, "s2_id": "98b7fd2122da5828aeab4e609499b52b05a8036a", "title": "Non-Blocking Simultaneous Multithreading: Embracing the Resiliency of Deep Neural Networks", "abstract": "Deep neural networks (DNNs) are known for their inability to utilize underlying hardware resources due to hard-ware susceptibility to sparse activations and weights. Even in finer granularities, many of the non-zero values hold a portion of zero-valued bits that may cause inefficiencies when executed on hard-ware. Inspired by conventional CPU simultaneous multithreading (SMT) that increases computer resource utilization by sharing them across several threads, we propose non-blocking SMT (NB-SMT) designated for DNN accelerators. Like conventional SMT, NB-SMT shares hardware resources among several execution flows. Yet, unlike SMT, NB-SMT is non-blocking, as it handles structural hazards by exploiting the algorithmic resiliency of DNNs. Instead of opportunistically dispatching instructions while they wait in a reservation station for available hardware, NB-SMT temporarily reduces the computation precision to accommodate all threads at once, enabling a non-blocking operation. We demonstrate NB-SMT applicability using SySMT, an NB-SMT-enabled output-stationary systolic array (OS-SA). Compared with a conventional OS-SA, a 2-threaded SySMT consumes 1.4\u00d7 the area and delivers 2\u00d7 speedup with 33% energy savings and less than 1% accuracy degradation of state-of-the-art CNNs with ImageNet. A 4-threaded SySMT consumes 2.5\u00d7 the area and delivers, for example, 3.4\u00d7 speedup and 39%\u00d7energy savings with 1% accuracy degradation of 40%-pruned ResNet-18.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Gil  Shomron", "Uri  Weiser"], "year": 2020, "n_citations": 5}
{"id": 1673973, "s2_id": "f033e52e59a0e9aba9fa4d1cf5bb5678fa0e7fe4", "title": "Interval Semantics for Standard Floating-Point Arithmetic", "abstract": "Research Report DCS-323-IRDepartment of Computer Science, University of VictoriaAbstractIf the non-zero \ufb01nite \ufb02oating-point numbers are interpreted as point intervals, thenthe e\ufb00ect of rounding can be interpreted as computing one of the bounds of the resultaccording to interval arithmetic. We give an interval interpretation for the signed zerosand in\ufb01nities, so that the unde\ufb01ned operations \u00b10 \u2217 \u00b1\u221e, \u00b1\u221e \u2212 \u00b1\u221e, \u00b1\u221e/\u00b1 \u221e, and\u00b10/\u00b10 become de\ufb01ned.In this way no operation remains that gives rise to an error condition. Mathemat-ically questionable features of the \ufb02oating-point standard become well-de\ufb01ned sets ofreals. Interval semantics provides a basis for the veri\ufb01cation of numerical algorithms.We derive the results of the newly de\ufb01ned operations and consider the implications forhardware implementation.", "venue": "ArXiv", "authors": ["William W. Edmonson", "M. H. van Emden"], "year": 2008, "n_citations": 2}
{"id": 1674275, "s2_id": "9ec4871cfc9663376239d016ddeb408abf5edcca", "title": "Hardware Virtualization Support In INTEL, AMD And IBM Power Processors", "abstract": "At present, the mostly used and developed mechanism is hardware virtualization which provides a common platform to run multiple operating systems and applications in independent partitions. More precisely, it is all about resource virtualization as the term hardware virtualization is emphasized. In this paper, the aim is to find out the advantages and limitations of current virtualization techniques, analyze their cost and performance and also depict which forthcoming hardware virtualization techniques will able to provide efficient solutions for multiprocessor operating systems. This is done by making a methodical literature survey and statistical analysis of the benchmark reports provided by SPEC (Standard Performance Evaluation Corporation) and TPC (Transaction processing Performance Council). Finally, this paper presents the current aspects of hardware virtualization which will help the IT managers of the large organizations to take effective decision while choosing server with virtualization support. Again, the future works described in section 4 of this paper focuses on some real world challenges such as abstraction of multiple servers, language level virtualization, pre-virtualization etc. which may be point of great interest for the researchers.", "venue": "ArXiv", "authors": ["Kamanashis  Biswas", "Md. Ashraful Islam"], "year": 2009, "n_citations": 5}
{"id": 1674867, "s2_id": "34043aeb14dc563f5cd2f8962112cbc284b14f98", "title": "YodaNN: An Architecture for Ultralow Power Binary-Weight CNN Acceleration", "abstract": "Convolutional neural networks (CNNs) have revolutionized the world of computer vision over the last few years, pushing image classification beyond human accuracy. The computational effort of today\u2019s CNNs requires power-hungry parallel processors or GP-GPUs. Recent developments in CNN accelerators for system-on-chip integration have reduced energy consumption significantly. Unfortunately, even these highly optimized devices are above the power envelope imposed by mobile and deeply embedded applications and face hard limitations caused by CNN weight I/O and storage. This prevents the adoption of CNNs in future ultralow power Internet of Things end-nodes for near-sensor analytics. Recent algorithmic and theoretical advancements enable competitive classification accuracy even when limiting CNNs to binary (+1/\u22121) weights during training. These new findings bring major optimization opportunities in the arithmetic core by removing the need for expensive multiplications, as well as reducing I/O bandwidth and storage. In this paper, we present an accelerator optimized for binary-weight CNNs that achieves 1.5 TOp/s at 1.2 V on a core area of only 1.33 million gate equivalent (MGE) or 1.9 mm 2 and with a power dissipation of 895  $\\mu$ W in UMC 65-nm technology at 0.6 V. Our accelerator significantly outperforms the state-of-the-art in terms of energy and area efficiency achieving 61.2 TOp/s/W@0.6 V and 1.1 TOp/s/MGE@1.2 V, respectively.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Renzo  Andri", "Lukas  Cavigelli", "Davide  Rossi", "Luca  Benini"], "year": 2018, "n_citations": 156}
{"id": 1675594, "s2_id": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "abstract": "State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving, Exploiting sparsity saves 10x, Weight sharing gives 8x, Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88x104 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.", "venue": "2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Song  Han", "Xingyu  Liu", "Huizi  Mao", "Jing  Pu", "Ardavan  Pedram", "Mark  Horowitz", "William J. Dally"], "year": 2016, "n_citations": 1672}
{"id": 1678500, "s2_id": "e1080fe14d8b52c273460cb9521958e2c03efa93", "title": "Klessydra-T: Designing Vector Coprocessors for Multithreaded Edge-Computing Cores", "abstract": "Computation-intensive kernels, such as convolutions, matrix multiplication, and Fourier transform, are fundamental to edge-computing AI, signal processing, and cryptographic applications. Interleaved-multithreading (IMT) processor cores are interesting to pursue energy efficiency and low hardware cost for edge computing, yet they need hardware acceleration schemes to run heavy computational workloads. Following a vector approach to accelerate computations, this article explores possible alternatives to implement vector coprocessing units in RISC-V cores, showing the synergy between IMT and data-level parallelism in the target workloads.", "venue": "IEEE Micro", "authors": ["Abdallah  Cheikh", "Stefano  Sordillo", "Antonio  Mastrandrea", "Francesco  Menichelli", "Giuseppe  Scotti", "Mauro  Olivieri"], "year": 2021, "n_citations": 2}
{"id": 1681294, "s2_id": "2ad20f077797408476788b991f978bff88b9a083", "title": "DB4HLS: A Database of High-Level Synthesis Design Space Explorations", "abstract": "High-level synthesis (HLS) frameworks allow to easily specify a large number of variants of the same hardware design by only acting on optimization directives. Nonetheless, the hardware synthesis of implementations for all possible combinations of directive values is impractical even for simple designs. Addressing this shortcoming, many HLS design space exploration (DSE) strategies have been proposed to devise directive settings leading to high-quality implementations while limiting the number of synthesis runs. All these works require considerable efforts to validate the proposed strategies and/or to build the knowledge base employed to tune abstract models, as both tasks mandate the syntheses of large collections of implementations. Currently, such data gathering is performed ad hoc: 1) leading to a lack of standardization, hampering comparisons between DSE alternatives; and 2) posing a very high burden to researchers willing to develop novel DSE strategies. Against this backdrop, we here introduce DB4HLS, a database of exhaustive HLS explorations comprising more than 100 000 design points collected over four years equivalent of synthesis time. The open structure of DB4HLS allows the incremental integration of new DSEs, which can be easily defined with a dedicated domain-specific language. We think that of our database, available at https://www.db4hls.inf.usi.ch/, will be a valuable tool for the research community investigating automated strategies for the optimization of HLS-based hardware designs.", "venue": "IEEE Embedded Systems Letters", "authors": ["Lorenzo  Ferretti", "Jihye  Kwon", "Giovanni  Ansaloni", "Giuseppe Di Guglielmo", "Luca  Carloni", "Laura  Pozzi"], "year": 2021, "n_citations": 1}
{"id": 1681837, "s2_id": "8b82a2dfa007ad20064a632cfeaf6e835e90ac0e", "title": "Pyxis: An Open-Source Performance Dataset of Sparse Accelerators", "abstract": "Specialized accelerators provide gains of performance and efficiency in specific domains of applications. Sparse data structures or/and representations exist in a wide range of applications. However, it is challenging to design accelerators for sparse applications because no analytic architecture or performance-level models are able to fully capture the spectrum of the sparse data. Accelerator researchers rely on real execution to get precise feedback for their designs. In this work, we present PYXIS, a performance dataset for specialized accelerators on sparse data. PYXIS collects accelerator designs and real execution performance statistics. Currently, there are 73.8 K instances in PYXIS. PYXIS is open-source, and we are constantly growing PYXIS with new accelerator designs and performance statistics. PYXIS can benefit researchers in the fields of accelerator, architecture, performance, algorithm and many related topics.", "venue": "ArXiv", "authors": ["Linghao  Song", "Yuze  Chi", "Jason  Cong"], "year": 2021, "n_citations": 0}
{"id": 1681988, "s2_id": "f59598f6c7d42460f325f5750544014b033be202", "title": "Virtual memory partitioning for enhancing application performance in mobile platforms", "abstract": "Recently, the amount of running software on smart mobile devices is gradually increasing due to the introduction of application stores. The application store is a type of digital distribution platform for application software, which is provided as a component of an operating system on a smartphone or tablet. Mobile devices have limited memory capacity and, unlike server and desktop systems, due to their mobility they do not have a memory slot that can expand the memory capacity. Low memory killer (LMK) and out-of-memory killer (OOMK) are widely used memory management solutions in mobile systems. They forcibly terminate applications when the available physical memory becomes insufficient. In addition, before the forced termination, the memory shortage incurs thrashing and fragmentation, thus slowing down application performance. Although the existing page reclamation mechanism is designed to secure available memory, it could seriously degrade user responsiveness due to the thrashing. Memory management is therefore still important especially in mobile devices with small memory capacity. This paper presents a new memory partitioning technique that resolves the deterioration of the existing application life cycle induced by LMK and OOMK. It provides a completely isolated virtual memory node at the operating system level. Evaluation results demonstrate that the proposed method improves application execution time under memory shortage, compared with methods in previous studies.", "venue": "IEEE Transactions on Consumer Electronics", "authors": ["Geunsik  Lim", "Changwoo  Min", "Young Ik Eom"], "year": 2013, "n_citations": 12}
{"id": 1682801, "s2_id": "706e7b81ee7fa07c1f67d433febd754c344293ef", "title": "Efficient Similarity-aware Compression to Reduce Bit-writes in Non-Volatile Main Memory for Image-based Applications", "abstract": "Image bitmaps have been widely used in in-memory applications, which consume lots of storage space and energy. Compared with legacy DRAM, non-volatile memories (NVMs) are suitable for bitmap storage due to the salient features in capacity and power savings. However, NVMs suffer from higher latency and energy consumption in writes compared with reads. Although compressing data in write accesses to NVMs on-the-fly reduces the bit-writes in NVMs, existing precise or approximate compression schemes show limited performance improvements for data of bitmaps, due to the irregular data patterns and variance in data. We observe that the data containing bitmaps show the pixel-level similarity due to the analogous contents in adjacent pixels. By exploiting the pixel-level similarity, we propose SimCom, an efficient similarity-aware compression scheme in hardware layer, to compress data for each write access on-the-fly. The idea behind SimCom is to compress continuous similar words into the pairs of base words with runs. With the aid of domain knowledge of images, SimCom adaptively selects an appropriate compression mode to achieve an efficient trade-off between image quality and memory performance. We implement SimCom on GEM5 with NVMain and evaluate the performance with real-world workloads. Our results demonstrate that SimCom reduces 33.0%, 34.8% write latency and saves 28.3%, 29.0% energy than state-of-the-art FPC and BDI with minor quality loss of 3%.", "venue": "ArXiv", "authors": ["Zhangyu  Chen", "Yu  Hua", "Pengfei  Zuo", "Yuanyuan  Sun", "Yuncheng  Guo"], "year": 2019, "n_citations": 0}
{"id": 1683111, "s2_id": "0068aa2fb735142c06c559eec453136c9cdd334c", "title": "Multi-core architectures: Complexities of performance prediction and the impact of cache topology", "abstract": "The balance metric is a simple approach to estimate the performance of bandwidth-limited loop kernels. However, applying the method to in-cache situations and modern multi-core architectures yields unsatisfactory results. This paper analyzes the in uence of cache hierarchy design on performance predictions for bandwidth-limited loop kernels on current mainstream processors. We present a diagnostic model with improved predictive power, correcting the limitations of the simple balance metric. The importance of code execution overhead even in bandwidth-bound situations is emphasized. Finally we analyze the impact of synchronization overhead on multi-threaded performance with a special emphasis on the in uence of cache topology.", "venue": "ArXiv", "authors": ["Jan  Treibig", "Georg  Hager", "Gerhard  Wellein"], "year": 2009, "n_citations": 13}
{"id": 1685580, "s2_id": "a35e03454175eef05e240d634ca02678a953e80d", "title": "eWake: A Novel Architecture for Semi-Active Wake-Up Radios Attaining Ultra-High Sensitivity at Extremely-Low Consumption", "abstract": "In this work we propose a new scheme for semi-passive Wake-Up Receiver circuits that exhibits remarkable sensitivity beyond -70 dBm, while state-of-the-art receivers illustrate sensitivity of up to -55 dBm. The receiver employs the typical principle of an envelope detector that harvests RF energy from its antenna, while it employs a nano-power operation amplifier to intensify the obtained signal prior to the final decoding that is realized with the aid of a comparator circuit. It operates at the 868 MHz ISM band using OOK signals propagated through LoRa transceivers, while also supporting addressing capabilities in order to awake only the specified network\u2019s nodes. The power expenditure of the developed receiver is as low as 580 nA, remaining at the same power consumption levels as the state-of-the-art implementations.", "venue": "ArXiv", "authors": ["Giannis  Kazdaridis", "Nikos  Sidiropoulos", "Ioannis  Zografopoulos", "Thanasis  Korakis"], "year": 2021, "n_citations": 1}
{"id": 1688104, "s2_id": "b360dd90e9ef6604be0b205686b894514f2998a1", "title": "A Memory Controller with Row Buffer Locality Awareness for Hybrid Memory Systems", "abstract": "Non-volatile memory (NVM) is a class of promising scalable memory technologies that can potentially offer higher capacity than DRAM at the same cost point. Unfortunately, the access latency and energy of NVM is often higher than those of DRAM, while the endurance of NVM is lower. Many DRAM-NVM hybrid memory systems use DRAM as a cache to NVM, to achieve the low access latency, low energy, and high endurance of DRAM, while taking advantage of the large capacity of NVM. A key question for a hybrid memory system is what data to cache in DRAM to best exploit the advantages of each technology while avoiding the disadvantages of each technology as much as possible. \nWe propose a new memory controller design that improves hybrid memory performance and energy efficiency. We observe that both DRAM and NVM banks employ row buffers that act as a cache for the most recently accessed memory row. Accesses that are row buffer hits incur similar latencies (and energy consumption) in both DRAM and NVM, whereas accesses that are row buffer misses incur longer latencies (and higher energy consumption) in NVM than in DRAM. To exploit this, we devise a policy that caches heavily-reused data that frequently misses in the NVM row buffers into DRAM. Our policy tracks the row buffer miss counts of recently-used rows in NVM, and caches in DRAM the rows that are predicted to incur frequent row buffer misses. Our proposed policy also takes into account the high write latencies of NVM, in addition to row buffer locality and more likely places the write-intensive pages in DRAM instead of NVM.", "venue": "ArXiv", "authors": ["HanBin  Yoon", "Justin  Meza", "Rachata  Ausavarungnirun", "Rachael A. Harding", "Onur  Mutlu"], "year": 2018, "n_citations": 2}
{"id": 1701511, "s2_id": "afa68c3a74e8c0cae45bc2495b066fd978e924bd", "title": "Quantum Computers", "abstract": "This research paper gives an overview of quantum computers \u2013 description of their operation, differences between quantum and silicon computers, major construction problems of a quantum computer and many other basic aspects. No special scientific knowledge is necessary for the reader. Introduction to Quantum Computers Around 2030 computers might not have any transistors and chips. Think of a computer that is much faster than a common classical silicon computer. This might be a quantum computer. Theoretically it can run without energy consumption and billion times faster than today\u2019s PIII computers. Scientists already think about a quantum computer, as a next generation of classical computers. Gershenfeld says that if making transistors smaller and smaller is continued with the same rate as in the past years, then by the year of 2020, the width of a wire in a computer chip will be no more than a size of a single atom. These are sizes for which rules of classical physics no longer apply. Computers designed on today's chip technology will not continue to get cheaper and better. Because of its great power, quantum computer is an attractive next step in computer technology. (Manay, 1998, p. 5). A technology of quantum computers is also very different. For operation, quantum computer uses quantum bits (qubits). Qubit has a quaternary nature. Quantum mechanic\u2019s laws are completely different from the laws of a classical physics. A qubit can exist not only in the states corresponding to the logical values 0 or 1 as in the case of a classical bit, but also in a superposition state. A qubit is a bit of information that can be both zero and one simultaneously (Superposition state). Thus, a computer working on a qubit rather than a standard bit can make calculations using both values simultaneously. A qubyte, is made up of eight qubits and can have all values from zero to 255 simultaneously. \u201cMulti-qubyte systems have a power beyond anything possible with classical computers.\u201d (Quantum Computers & Moore's Law, p.1) Forty qubits could have the same power as modern supercomputers. According to Chuang a supercomputer needs about a month to find a phone number from the database consisting of world's phone books, where a quantum computer is able to solve this task in 27 minutes. Massachusetts Institute of Technology, Oxford University, IBM and Los Alamos National Laboratory are the most successful in development of quantum computer. (West, 2000, &7) History of Quantum Computers In 1982 R.Feynman presented an interesting idea how the quantum system can be used for computation reasons. He also gave an explanation how effects of quantum physics could be simulated by such quantum computer. This was very interesting idea which can be used for future research of quantum effects. Every experiment 2 investigating the effects and laws of quantum physics is complicated and expensive. Quantum computer would be a system performing such experiments permanently. Later in 1985, it was proved that a quantum computer would be much more powerful than a classical one. (West, 2000, p. 3) The Major Difference between Quantum and Classical Computers The memory of a classical computer is a string of 0s and 1s, and it can perform calculations on only one set of numbers simultaneously. The memory of a quantum computer is a quantum state that can be a superposition of different numbers. A quantum computer can do an arbitrary reversible classical computation on all the numbers simultaneously. Performing a computation on many different numbers at the same time and then interfering all the results to get a single answer, makes a quantum computer much powerful than a classical one. (West, 2000) The Potential and Power of Quantum Computing Quantum computer with 500 qubits gives 2 superposition states. Each state would be classically equivalent to a single list of 500 1's and 0's. Such computer could operate on 2 states simultaneously. Eventually, observing the system would cause it to collapse into a single quantum state corresponding to a single answer, a single list of 500 1's and 0's, as dictated by the measurement axiom of quantum mechanics. This kind of computer is equivalent to a classical computer with approximately 10 processors. (West, 2000, p. 3) Moore's Law for Quantum Computers According to Moore's Law, the number of transistors of a microprocessor continues to double in every 18 months. According to such evolution if there is a classical computer in year 2020, it will run at 40 GHz CPU speed with 160 Gb RAM. If we use an analogue of Moor\u2019s law for quantum computers, the number of quantum bits would be double in every 18 months. But adding just one qubit is already enough to double a speed. So, the speed of quantum computer will increase more than just doubling it. (Quantum Computers & Moore's Law, \u00a71) Some Problems in Production of Quantum Computers Any kind of measurement of quantum state parameters considers interaction process with environment (with other particles particle of light for example), which causes a change of some parameters of this quantum state. Measurement of superposition quantum state will collapse it into a classical state. This is called decoherence. This is the major obstacle in a process of producing of a quantum computer. If decoherence problem cannot be solved, a quantum computer will be no better than a silicon one. (Daniel, 1999) In order to make quantum computers powerful, many operations must be performed before quantum coherence is lost. It can be impossible to construct a quantum computer that will make calculations before decohering. But if one makes a quantum computer, where the number of errors is low enough, than it is possible to use an error-correcting code for preventing data looses even when qubits in the computer 3 decohere. There are a lot of error-correcting codes. One of the simplest classical errorcorrecting codes is called repetition code. 0 is encoded as 000 and 1 as 111. Then if only one bit is flipped, one gets a state for example 011 that can be corrected to its original state 111. The signs of states in a quantum superposition are also important, but sign errors can also be corrected. There exists even a theory about quantum errorcorrecting codes. (Daniel, 1999, p. 1) Another problem is hardware for quantum computers. Nuclear Magnetic Resonance (NMR) technology is the most popular today, because of some successful experiments. MIT and Los Alamos National Laboratory have constructed a simple quantum computer using NMR technology. Some other designs are based on ion trap and quantum electrodynamics (QED). All of these methods have significant limitations. Nobody knows what the architecture of future quantum computers hardware will be. (West, 2000, p. 6) Future Benefits of Quantum Computers 1. Cryptography and Peter Shor\u2019s Algorithm In 1994 Peter Shor (Bell Laboratories) found out the first quantum algorithm that, in principle, can perform an efficient factorization. This became a complex application that only a quantum computer could do. Factoring is one of the most important problems in cryptography. For instance, the security of RSA (electronic banking security system) public key cryptography depends on factoring and it is a big problem. Because of many useful features of quantum computer, scientists put more efforts to build it. However, breaking any kind of current encryption that takes almost centuries on existing computers, may just take a few years on quantum computer. (Maney, 1998) 2. Artificial Intelligence It has been mentioned that quantum computers will be much faster and consequently will perform a large amount of operations in a very short period of time. On the other side, increasing the speed of operation will help computers to learn faster even using the one of the simplest methods mistake bound model for learning. 3. Other Benefits High performance will allow us in development of complex compression algorithms, voice and image recognition, molecular simulations, true randomness and quantum communication. Randomness is important in simulations. Molecular simulations are important for developing simulation applications for chemistry and biology. With the help of quantum communication both receiver and sender are alerted when an eavesdropper tries to catch the signal. Quantum bits also allow more information to be communicated per bit. Quantum computers make communication more secure. Strange Thing about Quantum Computers \u201cOn the theory side, quantum mechanics delves deep into areas that are nearly unthinkable. For instance, it's possible that a quantum computer holds an infinite number of right answers for an infinite number of parallel universes. It just happens to give you the right answer for the universe you happen to be in at the time. \"It takes a 4 great deal of courage to accept these things,\" says Charles Bennett of IBM, one of the best known quantum computing scientists. \"If you do, you have to believe in a lot of other strange things.\" \" (Manay, 1998) Dancing Chloroform Atoms A few years ago, Gershenfeld and Chuang made the first quantum computer. It was based on nuclear magnetic resonance technology. The program was performing a simple search using Grover\u2019s algorithm. In comparison to classical computers it took one item out of four in just one step, instead of making two or three steps as classical computes. The price for making the first 2-qubit computer was approximately $1 million. Entanglement of Quantum Systems According to quantum mechanics an outside force acting on two particles of the quantum system can cause them to become entangled. The quantum state of this system can contain all positions of spins (internal magnetic moments) of each particle. The total spin of the system can only be equal to certain discrete values with different probabilities. Measurements of total spin of certain quantum systems showed that positions of spins of some particle are not inde", "venue": "ArXiv", "authors": ["Archil  Avaliani"], "year": 2004, "n_citations": 737}
{"id": 1702126, "s2_id": "f3d9e08c987200f7332c4246eca49b24be9ab58f", "title": "The Renewed Case for the Reduced Instruction Set Computer: Avoiding ISA Bloat with Macro-Op Fusion for RISC-V", "abstract": "This report makes the case that a well-designed Reduced Instruction Set Computer (RISC) can match, and even exceed, the performance and code density of existing commercial Complex Instruction Set Computers (CISC) while maintaining the simplicity and cost-effectiveness that underpins the original RISC goals. \nWe begin by comparing the dynamic instruction counts and dynamic instruction bytes fetched for the popular proprietary ARMv7, ARMv8, IA-32, and x86-64 Instruction Set Architectures (ISAs) against the free and open RISC-V RV64G and RV64GC ISAs when running the SPEC CINT2006 benchmark suite. RISC-V was designed as a very small ISA to support a wide range of implementations, and has a less mature compiler toolchain. However, we observe that on SPEC CINT2006 RV64G executes on average 16% more instructions than x86-64, 3% more instructions than IA-32, 9% more instructions than ARMv8, but 4% fewer instructions than ARMv7. \nCISC x86 implementations break up complex instructions into smaller internal RISC-like micro-ops, and the RV64G instruction count is within 2% of the x86-64 retired micro-op count. RV64GC, the compressed variant of RV64G, is the densest ISA studied, fetching 8% fewer dynamic instruction bytes than x86-64. We observed that much of the increased RISC-V instruction count is due to a small set of common multi-instruction idioms. \nExploiting this fact, the RV64G and RV64GC effective instruction count can be reduced by 5.4% on average by leveraging macro-op fusion. Combining the compressed RISC-V ISA extension with macro-op fusion provides both the densest ISA and the fewest dynamic operations retired per program, reducing the motivation to add more instructions to the ISA. This approach retains a single simple ISA suitable for both low-end and high-end implementations, where high-end implementations can boost performance through microarchitectural techniques.", "venue": "ArXiv", "authors": ["Christopher  Celio", "Daniel Palmer Dabbelt", "David A. Patterson", "Krste  Asanovic"], "year": 2016, "n_citations": 10}
{"id": 1703046, "s2_id": "be7b40ba74dec5a3cfa0b36c1066d27692da4271", "title": "Petascale computational systems", "abstract": "A balanced cyberinfrastructure is necessary to meet growing data-intensitive scientific needs. We believe that available resources should be allocated to benefit the broadest cross-section of the scientific community. Given the power-law distribution of problem sizes, this means that about half of funding agency resources should be spent on tier-1 centers at the petascale level and the other half dedicated to tier-2 and tier-3 centers on a cost-sharing basis. Funding agencies should support balanced systems, not just CPU farms, as well as petascale IO and networking. They should also allocate resources for a balanced tier-1 through tier-3 cyberinfrastructure.", "venue": "Computer", "authors": ["Gordon  Bell", "Jim  Gray", "Alexander S. Szalay"], "year": 2006, "n_citations": 158}
{"id": 1705331, "s2_id": "deaa7d7ed40e7fd8836ad6e7320fb33e9ea92993", "title": "Instruction-set selection for multi-application based ASIP design: An instruction-level study", "abstract": "Efficiency in embedded systems is paramount to achieve high performance while consuming less area and power. Processors in embedded systems have to be designed carefully to achieve such design constraints. Application Specific Instruction set Processors (ASIPs) exploit the nature of applications to design an optimal instruction set. Despite being not general to execute any application, ASIPs are highly preferred in the embedded systems industry where the devices are produced to satisfy a certain type of application domain/s (either intra-domain or inter-domain). Typically, ASIPs are designed from a base-processor and functionalities are added for applications. This paper studies the multi-application ASIPs and their instruction sets, extensively analysing the instructions for inter-domain and intra-domain designs. Metrics analysed are the reusable instructions and the extra cost to add a certain application. A wide range of applications from various application benchmarks (MiBench, MediaBench and SPEC2006) and domains are analysed for two different architectures (ARM-Thumb and PISA). Our study shows that the intra-domain applications contain larger number of common instructions, whereas the inter-domain applications have very less common instructions, regardless of the architecture (and therefore the ISA).", "venue": "2012 IEEE 6th International Conference on Information and Automation for Sustainability", "authors": ["Roshan G. Ragel", "Swarnalatha  Radhakrishnan", "Jude Angelo Ambrose"], "year": 2012, "n_citations": 1}
{"id": 1705441, "s2_id": "e67d5322ce568adb9f7757473105c7c51122d3b8", "title": "Laconic Deep Learning Computing", "abstract": "We motivate a method for transparently identifying ineffectual computations in unmodified Deep Learning models and without affecting accuracy. Specifically, we show that if we decompose multiplications down to the bit level the amount of work performed during inference for image classification models can be consistently reduced by two orders of magnitude. In the best case studied of a sparse variant of AlexNet, this approach can ideally reduce computation work by more than 500x. We present Laconic a hardware accelerator that implements this approach to improve execution time, and energy efficiency for inference with Deep Learning Networks. Laconic judiciously gives up some of the work reduction potential to yield a low-cost, simple, and energy efficient design that outperforms other state-of-the-art accelerators. For example, a Laconic configuration that uses a weight memory interface with just 128 wires outperforms a conventional accelerator with a 2K-wire weight memory interface by 2.3x on average while being 2.13x more energy efficient on average. A Laconic configuration that uses a 1K-wire weight memory interface, outperforms the 2K-wire conventional accelerator by 15.4x and is 1.95x more energy efficient. Laconic does not require but rewards advances in model design such as a reduction in precision, the use of alternate numeric representations that reduce the number of bits that are \"1\", or an increase in weight or activation sparsity.", "venue": "ArXiv", "authors": ["Sayeh  Sharify", "Mostafa  Mahmoud", "Alberto Delmas Lascorz", "Milos  Nikolic", "Andreas  Moshovos"], "year": 2018, "n_citations": 3}
{"id": 1706091, "s2_id": "a8f60529cf68b0139d3ef368546861a35096f916", "title": "Low power oriented CMOS circuit optimization protocol", "abstract": "Low power oriented circuit optimization consists in selecting the best alternative between gate sizing, buffer insertion and logic structure transformation, for satisfying a delay constraint at minimum area cost. In this paper, we used a closed form model of delay in CMOS structures to define metrics for a deterministic selection of the optimization alternative. The target is delay constraint satisfaction with minimum area cost. We validate the design space exploration method, defining maximum and minimum delay bounds on logical paths. Then we adapt this method to a \"constant sensitivity method\" allowing us to size a circuit at minimum area under a delay constraint. An optimisation protocol is finally defined to manage the performance constraint/circuit structure trade-off. These methods are implemented in an optimization tool (POPS) and validated by comparing, on a 0.25 /spl mu/m process, the optimization efficiency obtained on various benchmarks (ISCAS'85) to that resulting from an industrial tool.", "venue": "Design, Automation and Test in Europe", "authors": ["Alexandre  Verle", "Xavier  Michel", "Nadine  Az\u00e9mard", "Philippe  Maurine", "Daniel  Auvergne"], "year": 2005, "n_citations": 2}
{"id": 1706355, "s2_id": "7996deb99c9765fb9ff51d2cd91543bb840c52f2", "title": "An 826 MOPS, 210uW/MHz Unum ALU in 65 nm", "abstract": "To overcome the limitations of conventional floating-point number formats, an interval arithmetic and variable-width storage format called universal number (unum) has been recently introduced [1]. This paper presents the first (to the best of our knowledge) silicon implementation measurements of an application-specific integrated circuit (ASIC) for unum floating-point arithmetic. The designed chip includes a 128-bit wide unum arithmetic unit to execute additions and subtractions, while also supporting lossless (for intermediate results) and lossy (for external data movements) compression units to exploit the memory usage reduction potential of the unum format. Our chip, fabricated in a 65 nm CMOS process, achieves a maximum clock frequency of 413 MHz at 1.2 V with an average measured power of 210uW/MHz.", "venue": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Florian  Glaser", "Stefan  Mach", "Abbas  Rahimi", "Frank K. G\u00fcrkaynak", "Qiuting  Huang", "Luca  Benini"], "year": 2018, "n_citations": 15}
{"id": 1707467, "s2_id": "cc752a5d0bb935eacc7ba4c531430fc3215409e6", "title": "AMOEBA: a coarse grained reconfigurable architecture for dynamic GPU scaling", "abstract": "Different GPU applications exhibit varying scalability patterns with network-on-chip (NoC), coalescing, memory and control divergence, and L1 cache behavior. A GPU consists of several Streaming Multi-processors (SMs) that collectively determine how shared resources are partitioned and accessed. Recent years have seen divergent paths in SM scaling towards scale-up (fewer, larger SMs) vs. scale-out (more, smaller SMs). However, neither scaling up nor scaling out can meet the scalability requirement of all applications running on a given GPU system, which inevitably results in performance degradation and resource under-utilization for some applications. In this work, we investigate major design parameters that influence GPU scaling. We then propose AMOEBA, a solution to GPU scaling through reconfigurable SM cores. AMOEBA monitors and predicts application scalability at run-time and adjusts the SM configuration to meet program requirements. AMOEBA also enables dynamic creation of heterogeneous SMs through independent fusing or splitting. AMOEBA is a microarchitecture-based solution and requires no additional programming effort or custom compiler support. Our experimental evaluations with application programs from various benchmark suites indicate that AMOEBA is able to achieve a maximum performance gain of 4.3x, and generates an average performance improvement of 47% when considering all benchmarks tested.", "venue": "ICS", "authors": ["Xianwei  Cheng", "Hui  Zhao", "Mahmut  Kandemir", "Beilei  Jiang", "Gayatri  Mehta"], "year": 2020, "n_citations": 0}
{"id": 1708985, "s2_id": "81697a55c51157f5375c7eaaf357147397eaa2f7", "title": "Integrated Optimization of Partitioning, Scheduling, and Floorplanning for Partially Dynamically Reconfigurable Systems", "abstract": "Confronted with the challenge of high performance for applications and the restriction of hardware resources for field-programmable gate arrays (FPGAs), partial dynamic reconfiguration technology is anticipated to accelerate the reconfiguration process and alleviate the device shortage. In this paper, we propose an integrated optimization framework for task partitioning, scheduling, and floorplanning on partially dynamically reconfigurable FPGAs. The partition, schedule, and floorplan of the tasks are represented by the partitioned sequence triple (P-ST) (<italic>PS</italic>, <italic>QS</italic>, <italic>RS</italic>), where (<italic>PS</italic>, <italic>QS</italic>) is a hybrid nested sequence pair for representing the spatial and temporal partitions, as well as the floorplan, and <italic>RS</italic> is the partitioned dynamic configuration order of the tasks. The floorplanning and scheduling of task modules can be computed from the P-ST in <inline-formula> <tex-math notation=\"LaTeX\">${O}$ </tex-math></inline-formula>(<inline-formula> <tex-math notation=\"LaTeX\">${n}^{{2}}$ </tex-math></inline-formula>) time. To integrate the exploration of the scheduling and floorplanning design space, we use a simulated annealing-based search engine and elaborate a perturbation method, where a randomly chosen task module is removed from the partition sequence triple and then reinserted into a proper position selected from all the <inline-formula> <tex-math notation=\"LaTeX\">${O}$ </tex-math></inline-formula>(<inline-formula> <tex-math notation=\"LaTeX\">${n}^{ {3}}$ </tex-math></inline-formula>) possible combinations of partition, schedule and floorplan. We also prove a sufficient and necessary condition for the feasibility of the partitioning of tasks and scheduling of task configurations, and derive conditions for the feasibility of the insertion points in a P-ST. The experimental results demonstrate the efficiency and effectiveness of the proposed framework.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Song  Chen", "Jinglei  Huang", "Xiaodong  Xu", "Bo  Ding", "Qi  Xu"], "year": 2020, "n_citations": 5}
{"id": 1714476, "s2_id": "23198a2ba2d7ec1b06c2cb059ea96063ed755e93", "title": "Reconfigurable and approximate computing for video coding", "abstract": "The Chapter begins with a discussion of the constraints and needs of video coding systems. The lack in flexibility of traditional monolithic codec specifications, not suitable to model commonalities among codecs and foster reusability among successive codec generations/updates, was the main trigger for the development of a new standard initiative within the ISO/IEC MPEG committee, called reconfigurable video coding (RVC). The MPEG-RVC framework exploits the dataflow nature behind video coding to foster flexible and reconfigurable codec design, as well as to support dynamic reconfiguration. The Chapter goes on to consider that the inherent resiliency of various functional blocks (like motion estimation in the high-efficiency video coding, HEVC) and the varying levels of user perception make video coding suitable to apply approximate computing techniques. Approximate computing, if properly supported at design time, allows achieving run-time trade-offs, representing a new direction in hardware-software codesign research. The main assumption behind approximate computing, exploited within video coding, is that the degree of accuracy (in this case during codec execution) is not required to be the same all the time. The final part of the Chapter attempts to put together the concepts addressed and remarks on which are, in the authors' opinion, some interesting research directions.", "venue": "VLSI Architectures for Future Video Coding", "authors": ["Francesca  Palumbo", "Carlo  Sau"], "year": 2019, "n_citations": 0}
{"id": 1715917, "s2_id": "d2102d46b64b000b1ad8361c25fae21a39a62c72", "title": "A 0.5GHz 0.35mW LDO-Powered Constant-Slope Phase Interpolator With 0.22% INL", "abstract": "Clock generators are an essential and critical building block of any communication link, whether it be wired or wireless, and they are increasingly critical given the push for lower I/O power and higher bandwidth in Systems-on-Chip (SoCs) for the Internet-of-Things (IoT). One recurrent issue with clock generators is multiple-phase generation, especially for low-power applications; several methods of phase generation have been proposed, one of which is phase interpolation. We propose a phase interpolator (PI) that employs the concept of constant-slope operation. Consequently, a low-power highly-linear operation is coupled with the wide dynamic range (i.e., phase wrapping) capabilities of a PI. Furthermore, the PI is powered by a low-dropout regulator (LDO) supporting fast transient operation. Implemented in 65-nm CMOS technology, it consumes <inline-formula> <tex-math notation=\"LaTeX\">$350~\\mu \\text{W}$ </tex-math></inline-formula> at a 1.2-V supply and a 0.5-GHz clock; it achieves energy efficiency <inline-formula> <tex-math notation=\"LaTeX\">$4\\times -15\\times $ </tex-math></inline-formula> lower than state-of-the-art (SoA) digital-to-time converters (DTCs) and an integral non-linearity (INL) of <inline-formula> <tex-math notation=\"LaTeX\">$2.5\\times -3.1\\times $ </tex-math></inline-formula> better than SoA PIs, striking a good balance between linearity and energy efficiency.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Ahmed  Elnaqib", "Hayate  Okuhara", "Taekwang  Jang", "Davide  Rossi", "Luca  Benini"], "year": 2021, "n_citations": 1}
{"id": 1717520, "s2_id": "aee4bc0f72242426b75f7f31294062113506896d", "title": "FLYSIG: dataflow oriented delay-insensitive processor for rapid prototyping of signal processing", "abstract": "As the one-chip integration of HW modules designed by different companies becomes more and more popular, reliability of a HW design and evaluation of the timing behavior during the prototype stage are absolutely necessary. One way to guarantee reliability is the use of robust design styles, e.g., delay insensitivity. For early timing evaluation, two aspects must be considered: a) the timing needs to be proportional to technology variations, and b) the implemented architecture should be identical for prototype and target. The first can be met also by delay insensitive implementation. The latter one is the key point. A unified architecture is needed for prototyping as well as implementation. Our new approach to rapid prototyping of signal processing tasks is based on a configurable, delay insensitive implemented processor called FLYSIG (dataflow oriented delay-insensitive signal processing). In essence, the FLYSIG processor can be understood as a complex FPGA where the CLBs are substituted by bit serial operators. The general concept is detailed and first experimental results are given for demonstration of the main advantages: delay insensitive design style, direct correspondence between prototyping and target architecture, high performance and reasonable shortening of the design cycle.", "venue": "Proceedings. Ninth International Workshop on Rapid System Prototyping (Cat. No.98TB100237)", "authors": ["Wolfram  Hardt", "Bernd  Kleinjohann"], "year": 1998, "n_citations": 9}
{"id": 1731355, "s2_id": "92ffeed399d9bf544a4dd0b699b5abda2a7f1a6c", "title": "A 256-kb 9T Near-Threshold SRAM With 1k Cells per Bitline and Enhanced Write and Read Operations", "abstract": "In this paper, we present a new 9T SRAM cell that has good write ability and improves read stability at the same time. Simulation results show that the proposed design increases read static noise margin and ION/IOFF of read path by 219% and 113%, respectively, at supply voltage of 300-mV over conventional 6T SRAM cell in a 90-nm CMOS technology. The proposed design lets us reduce the minimum operating voltage of SRAM (VDDmin) to 350 mV, whereas conventional 6T SRAM cannot operate successfully with an acceptable failure rate at supply voltages below 725 mV. We also compared our design with three other SRAM cells from recent literature. To verify the proposed design, a 256-kb SRAM is designed using new 9T and conventional 6T SRAM cells. Operating at their minimum possible VDDs, the proposed design decreases write and read power per operation by 92% and 93%, respectively, over the conventional rival. The area of the proposed SRAM cell is increased by 83% over a conventional 6T one. However, due to large ION/IOFF of read path for 9T cell, we are able to put 1k cells in each column of 256-kb SRAM block, resulting in the possibility for sharing write and read circuitries of each column between more cells compared with conventional 6T. Thus, the area overhead of 256kb SRAM based on new 9T cell is reduced to 37% compared with 6T SRAM.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Ghasem  Pasandi", "Sied Mehdi Fakhraie"], "year": 2015, "n_citations": 53}
{"id": 1733334, "s2_id": "01fb05cb6bff252f829ead807251c6329a5f8d0b", "title": "RowClone: Accelerating Data Movement and Initialization Using DRAM", "abstract": "In existing systems, to perform any bulk data movement operation (copy or initialization), the data has to first be read into the on-chip processor, all the way into the L1 cache, and the result of the operation must be written back to main memory. This is despite the fact that these operations do not involve any actual computation. RowClone exploits the organization and operation of commodity DRAM to perform these operations completely inside DRAM using two mechanisms. The first mechanism, Fast Parallel Mode, copies data between two rows inside the same DRAM subarray by issuing back-to-back activate commands to the source and the destination row. The second mechanism, Pipelined Serial Mode, transfers cache lines between two banks using the shared internal bus. RowClone significantly reduces the raw latency and energy consumption of bulk data copy and initialization. This reduction directly translates to improvement in performance and energy efficiency of systems running copy or initialization-intensive workloads", "venue": "ArXiv", "authors": ["Vivek  Seshadri", "Yoongu  Kim", "Chris  Fallin", "Donghyuk  Lee", "Rachata  Ausavarungnirun", "Gennady  Pekhimenko", "Yixin  Luo", "Onur  Mutlu", "Phillip B. Gibbons", "Michael A. Kozuch", "Todd C. Mowry"], "year": 2018, "n_citations": 4}
{"id": 1736573, "s2_id": "f7bbf43a58a3bcee0ddaa60f29b3f1ff9f9a9628", "title": "Automated Formal Equivalence Verification of Pipelined Nested Loops in Datapath Designs", "abstract": "In this paper, we present an efficient formal approach to check the equivalence of synthesized RTL against the high-level specification in the presence of pipelining transformations. To increase the scalability of our proposed method, we dynamically divide the designs into several smaller parts called segments by introducing cut-points. Then we employ Modular Horner Expansion Diagram (M-HED) to check whether the specification and implementation are equivalent or not. In an iterative manner, the equivalence checking for each segment is performed. At each step, the equivalent nodes and those nodes which have an impact on them are removed until the whole design is covered. Our proposed method enables us to deal with the equivalence checking problem for behaviorally synthesized designs even in the presence of pipelines for nested loops. The empirical results demonstrate the efficiency and scalability of our proposed method in terms of run-time and memory usage for several large designs synthesized by a commercial behavioral synthesis tool. Average improvements in terms of the memory usage and run time in comparison with SMT- and SAT-based equivalence checking are 16.7x and 111.9x, respectively.", "venue": "ArXiv", "authors": ["Payman  Behnam", "Bijan  Alizadeh", "Sajjad  Taheri"], "year": 2017, "n_citations": 1}
{"id": 1748812, "s2_id": "c3e4ab00ecaef32c472e63c39ef9629a225a1ba2", "title": "Operation Merging for Hardware Implementations of Fast Polar Decoders", "abstract": "Polar codes are a class of linear block codes that provably achieves channel capacity. They have been selected as a coding scheme for the control channel of enhanced mobile broadband (eMBB) scenario for 5th generation wireless communication networks (5G) and are being considered for additional use scenarios. As a result, fast decoding techniques for polar codes are essential. Previous works targeting improved throughput for successive-cancellation (SC) decoding of polar codes are semi-parallel implementations that exploit special maximum-likelihood (ML) nodes. In this work, we present a new fast simplified SC (Fast-SSC) decoder architecture. Compared to a baseline Fast-SSC decoder, our solution is able to reduce the memory requirements. We achieve this through a more efficient memory utilization, which also enables to execute multiple operations in a single clock cycle. Finally, we propose new special node merging techniques that improve the throughput further, and detail a new Fast-SSC-based decoder architecture to support merged operations. The proposed decoder reduces the operation sequence requirement by up to 39%, which enables to reduce the number of time steps to decode a codeword by 35%. ASIC implementation results with 65 nm TSMC technology show that the proposed decoder has a throughput improvement of up to 31% compared to previous Fast-SSC decoder architectures.", "venue": "J. Signal Process. Syst.", "authors": ["Furkan  Ercan", "Thibaud  Tonnellier", "Carlo  Condo", "Warren J. Gross"], "year": 2019, "n_citations": 5}
{"id": 1748891, "s2_id": "eab8bb10c0fe28cdb4026544e08eea87a81970ee", "title": "FPGA-based real-time 105-channel data acquisition platform for imaging system", "abstract": "In this paper, a real-time 105-channel data acquisition platform based on FPGA for imaging will be implemented for mm-wave imaging systems. PC platform is also realized for imaging results monitoring purpose. Mm-wave imaging expands our vision by letting us see things under poor visibility conditions. With this extended vision ability, a wide range of military imaging missions would benefit, such as surveillance, precision targeting, navigation, and rescue. Based on the previously designed imager modules, this project would go on finishing the PCB design (both schematic and layout) of the following signal processing systems consisting of Programmable Gain Amplifier(PGA) (4 PGA for each ADC) and 16-channel Analog to Digital Converter (ADC) (7 ADC in total). Then the system verification would be performed on the Artix-7 35T Arty FPGA with the developing of proper controlling code to configure the ADC and realize the communication between the FPGA and the PC (through both UART and Ethernet). For the verification part, a simple test on a breadboard with a simple analog input (generated from a resistor divider) would first be performed. After the PCB design is finished, the whole system would be tested again with a precise reference and analog input.", "venue": "ArXiv", "authors": ["Chengkai  Guo", "Jason Y. Du"], "year": 2017, "n_citations": 0}
{"id": 1752147, "s2_id": "e4e4bb72212d8c3e3a1bc90fc6acda3211fc4c58", "title": "A Case for 3D Integrated System Design for Neuromorphic Computing & AI Applications", "abstract": "Over the last decade, artificial intelligence has found many applications areas in the society. As AI solutions have become more sophistication and the use cases grew, they highlighted the need to address performance and energy efficiency challenges faced during the implementation process. To address these challenges, there has been growing interest in neuromorphic chips. Neuromorphic computing relies on non von Neumann architectures as well as novel devices, circuits and manufacturing technologies to mimic the human brain. Among such technologies, 3D integration is an important enabler for AI hardware and the continuation of the scaling laws. In this paper, we overview the unique opportunities 3D integration provides in neuromorphic chip design, discuss the emerging opportunities in next generation neuromorphic architectures and review the obstacles. Neuromorphic architectures, which relied on the brain for inspiration and emulation purposes, face grand challenges due to the limited understanding of the functionality and the architecture of the human brain. Yet, high-levels of investments are dedicated to develop neuromorphic chips. We argue that 3D integration not only provides strategic advantages to the cost-effective and flexible design of neuromorphic chips, it may provide design flexibility in incorporating advanced capabilities to further benefits the designs in the future.", "venue": "Int. J. Semantic Comput.", "authors": ["Eren  Kurshan", "Hai  Li", "Mingoo  Seok", "Yuan  Xie"], "year": 2020, "n_citations": 0}
{"id": 1754440, "s2_id": "d22dedf43720f33cd08410feafc6d1be71ce4b0a", "title": "A case for superconducting accelerators", "abstract": "As scaling of CMOS slows down, there is growing interest in alternative technologies that can improve performance and energy-efficiency. Superconducting circuits based on Josephson Junctions (JJ) is an emerging technology that provides devices which can be switched with pico-second latencies and consumes two orders of magnitude lower switching energy compared to CMOS. While JJ-based circuits can operate at high frequencies and are energy-efficient, the technology faces three critical challenges: limited device density and lack of area-efficient technology for memory structures, low gate fanout, and new failure modes of Flux-Traps that occurs due to the operating environment. Limited memory density restricts the use of superconducting technology in the near term to application domains that have high compute intensity but require negligible amount of memory. In this paper, we study the use of superconducting technology to build an accelerator for SHA-256 engines commonly used in Bitcoin mining. We show that merely porting existing CMOS-based accelerator to superconducting technology provides 10.6X improvement in energy efficiency. Redesigning the accelerator to suit the unique constraints of superconducting technology (such as low fanout) improves the energy efficiency to 12.2X. We also investigate solutions to make the accelerator tolerant of new fault modes and show how this fault-tolerant design can be leveraged to reduce the operating current, thereby improving the overall energy-efficiency to 46X.", "venue": "CF", "authors": ["Swamit S. Tannu", "Poulami  Das", "Michael L. Lewis", "Robert F. Krick", "Douglas M. Carmean", "Moinuddin K. Qureshi"], "year": 2019, "n_citations": 4}
{"id": 1758813, "s2_id": "52ae5b70f1472d2f3e5ed96b4c440bdf6c7fbfe6", "title": "Breaking the Memory Wall for AI Chip with a New Dimension", "abstract": "Recent advancements in deep learning have led to the widespread adoption of artificial intelligence (AI) in applications such as computer vision and natural language processing. As neural networks become deeper and larger, AI modeling demands outstrip the capabilities of conventional chip architectures. Memory bandwidth falls behind processing power. Energy consumption comes to dominate the total cost of ownership. Currently, memory capacity is insufficient to support the most advanced NLP models. In this work, we present a 3D AI chip, called Sunrise, with near-memory computing architecture to address these three challenges. This distributed, near-memory computing architecture allows us to tear down the performance-limiting memory wall with an abundance of data bandwidth. We achieve the same level of energy efficiency on 40nm technology as competing chips on 7nm technology. By moving to similar technologies as other AI chips, we project to achieve more than ten times the energy efficiency, seven times the performance of the current state-of-the-art chips, and twenty times of memory capacity as compared with the best chip in each benchmark.", "venue": "2020 5th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM)", "authors": ["Eugene  Tam", "Shenfei  Jiang", "Paul  Duan", "Shawn  Meng", "Yue  Pang", "Cayden  Huang", "Yi  Han", "Jacke  Xie", "Yuanjun  Cui", "Jinsong  Yu", "Minggui  Lu"], "year": 2020, "n_citations": 0}
{"id": 1760488, "s2_id": "5b688d3a1d6cb71383323c2b8207fc861969551f", "title": "Exploiting Extended Krylov Subspace for the Reduction of Regular and Singular Circuit Models", "abstract": "During the past decade, Model Order Reduction (MOR) has become key enabler for the efficient simulation of large circuit models. MOR techniques based on moment-matching are well established due to their simplicity and computational performance in the reduction process. However, moment-matching methods based on the ordinary Krylov subspace are usually inadequate to accurately approximate the original circuit behaviour. In this paper, we present a moment-matching method which is based on the extended Krylov subspace and exploits the superposition property in order to deal with many terminals. The proposed method can handle large-scale regular and singular circuits, and generate accurate and efficient reduced-order models for circuit simulation. Experimental results on industrial IBM power grid benchmarks demonstrate that our method achieves an error reduction up to 83.69% over a standard Krylov subspace technique.", "venue": "2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Chrysostomos  Chatzigeorgiou", "Dimitrios  Garyfallou", "George  Floros", "Nestor  Evmorfopoulos", "George  Stamoulis"], "year": 2021, "n_citations": 0}
{"id": 1769923, "s2_id": "6929acb6bed560e7cf885843bc30d197ec5b6aea", "title": "Asynchronous Logic Circuits and Sheaf Obstructions", "abstract": "This article exhibits a particular encoding of logic circuits into a sheaf formalism. The central result of this article is that there exists strictly more information available to a circuit designer in this setting than exists in static truth tables, but less than exists in event-level simulation. This information is related to the timing behavior of the logic circuits, and thereby provides a ''bridge'' between static logic analysis and detailed simulation.", "venue": "Electron. Notes Theor. Comput. Sci.", "authors": ["Michael  Robinson"], "year": 2012, "n_citations": 16}
{"id": 1773820, "s2_id": "29c761b1100f18745a2ddaa8f79e81c1de068d7c", "title": "Fast Parallel Integer Adder in Binary Representation", "abstract": "An integer adder for integers in the binary representation is one of the basic operations of any digital processor. For adding two integers of N bits each, the serial adder takes as many clock ticks. For achieving higher speeds, parallel circuits are discussed in the literature, and these circuits usually operate in two levels. At the lower level, integers represented by blocks of smaller number of bits are added, and in a cascade of stages in the next level, the carries produced in previous addition operations are summed to the augends. In this paper, we describe a fast method and an improvement of it. The first attempt resembles the operation method of the merge sort algorithm, from which some important properties of carries produced in each stage are analysed and assimilated, resulting in a parallel adder that runs in time comparable to the existing methods. After that, the crucial insights are brought to fruition in an improved design, which takes 2 clock ticks to perform the addition operation requiring only O(square(N)) space. The number of bits N is chosen usually to be a positive integer power of 2. The speedup is achieved by special purpose circuits for increment operations by i-th power of 2 , for i = 0, 1, ..., N-1, each operation taking only a single clock tick to complete. The usefulness of this adder for multiplication operation is discussed. The standard multiplication method utilizes quantizer and 3-bit to 2-bit consolidation circuits to produce an integer that represents in binary the number of 1s in a column corresponding to a place (weighted coefficient) of nonnegative integer power of 2. The last two consolidated integers are added by an adder in the end.", "venue": "ArXiv", "authors": ["Duggirala Meher Krishna", "Duggirala  Ravi"], "year": 2019, "n_citations": 0}
{"id": 1774671, "s2_id": "2f6b9e7684e9f596bfc9c59f28dafec7f4fc92b7", "title": "Partial sums computation in polar codes decoding", "abstract": "Polar codes are the first error-correcting codes to provably achieve the channel capacity but with infinite code-lengths. For finite code lengths the existing decoder architectures are limited in working frequency by the partial sums computation unit. We explain in this paper how the partial sums computation can be seen as a matrix multiplication. Then, an efficient hardware implementation of this product is investigated. It has reduced logic resources and interconnections. Formalized architectures, to compute partial sums and to generate the bits of the generator matrix \u03ba\u2297n, are presented. The proposed architecture allows removing the multiplexing resources used to assigned to each processing elements the required partial sums.", "venue": "2015 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Guillaume  Berhault", "Camille  Leroux", "Christophe  J\u00e9go", "Dominique  Dallet"], "year": 2015, "n_citations": 8}
{"id": 1775325, "s2_id": "bf1ba9499b8398eb835d3bfaf70a1940b90bdabf", "title": "Agon: A Scalable Competitive Scheduler for Large Heterogeneous Systems", "abstract": "This work proposes a competitive scheduling approach, designed to scale to large heterogeneous multicore systems. This scheduler overcomes the challenges of (1) the high computation overhead of near-optimal schedulers, and (2) the error introduced by inaccurate performance predictions. This paper presents Agon, a neural network-based classifier that selects from a range of schedulers, from simple to very accurate, and learns which scheduler provides the right balance of accuracy and overhead for each scheduling interval. Agon also employs a de-noising frontend allowing the individual schedulers to be tolerant towards noise in performance predictions, producing better overall schedules. By avoiding expensive scheduling overheads, Agon improves average system performance by 6% on average, approaching the performance of an oracular scheduler (99.1% of oracle performance).", "venue": "ArXiv", "authors": ["Andreas  Prodromou", "Ashish  Venkat", "Dean M. Tullsen"], "year": 2021, "n_citations": 0}
{"id": 1775521, "s2_id": "474854b748f054ef5222cccd46bced92040096e2", "title": "A Flip-Syndrome-List Polar Decoder Architecture for Ultra-Low-Latency Communications", "abstract": "We consider practical hardware implementation of polar decoders. To reduce latency due to the serial nature of successive cancellation, existing optimizations improve parallelism with two approaches, i.e., multi-bit decision or reduced path splitting. In this paper, we combine the two procedures into one with an error-pattern-based architecture. It simultaneously generates a set of candidate paths for multiple bits with pre-stored patterns. For rate-1 (R1) or single parity-check nodes, we prove that a small number of deterministic patterns are required to guarantee performance preservation. For general nodes, low-weight error patterns are indexed by syndrome in a look-up table and retrieved in  $O(1)$  time. The proposed flip-syndrome-list decoder fully parallelizes all constituent code blocks without sacrificing performance, and thus is suitable for ultra-low-latency applications. Meanwhile, two code construction optimizations are presented to further reduce complexity and improve performance.", "venue": "IEEE Access", "authors": ["Huazi  Zhang", "Jiajie  Tong", "Rong  Li", "Pengcheng  Qiu", "Yourui  Huangfu", "Chen  Xu", "Xianbin  Wang", "Jun  Wang"], "year": 2019, "n_citations": 5}
{"id": 1778124, "s2_id": "f42babb568065a6aab47836cd1bc30f35975dd46", "title": "HAPPY: Hybrid Address-based Page Policy in DRAMs", "abstract": "Memory controllers have used static page closure policies to decide whether a row should be left open, open-page policy, or closed immediately, close-page policy, after the row has been accessed. The appropriate choice for a particular access can reduce the average memory latency. However, since application access patterns change at run time, static page policies cannot guarantee to deliver optimum execution time. Hybrid page policies have been investigated as a means of covering these dynamic scenarios and are now implemented in state-of-the-art processors. Hybrid page policies switch between open-page and close-page policies while the application is running, by monitoring the access pattern of row hits/conflicts and predicting future behavior. Unfortunately, as the size of DRAM memory increases, fine-grain tracking and analysis of memory access patterns does not remain practical. We propose a compact memory address-based encoding technique which can improve or maintain the performance of DRAMs page closure predictors while reducing the hardware overhead in comparison with state-of-the-art techniques. As a case study, we integrate our technique, HAPPY, with a state-of-the-art Intel-adaptive monitor (e.g. part of the Intel Xeon X5650) and a traditional Hybrid page policy. We evaluate them across 70 memory intensive workload mixes consisting of single-thread and multi-thread applications. The experimental results show that using the HAPPY encoding applied to the Intel-adaptive page closure policy can reduce the hardware overhead by 5x for the evaluated 64 GB memory (up to 40\u00d7 for a 512 GB memory) while maintaining the prediction accuracy.", "venue": "MEMSYS", "authors": ["Mohsen  Ghasempour", "Aamer  Jaleel", "Jim D. Garside", "Mikel  Luj\u00e1n"], "year": 2016, "n_citations": 11}
{"id": 1778658, "s2_id": "5626263c5fea4b05a45cbd6dd1f77e9a218fab21", "title": "Towards a Trusted Execution Environment via Reconfigurable FPGA", "abstract": "Trusted Execution Environments (TEEs) are used to protect sensitive data and run secure execution for securitycritical applications, by providing an environment isolated from the rest of the system. However, over the last few years, TEEs have been proven weak, as either TEEs built upon securityoriented hardware extensions (e.g., Arm TrustZone) or resorting to dedicated secure elements were exploited multiple times. In this project, we introduce Trusted Execution Environments On-Demand (TEEOD), a novel TEE design that leverages the programmable logic (PL) in the heterogeneous system on chips (SoC) as the secure execution environment. Unlike other TEE designs, TEEOD can provide high-bandwidth connections and physical on-chip isolation. We implemented a proof-of-concept (PoC) implementation targeting an Ultra96-V2 platform. The conducted evaluation demonstrated TEEOD can host up to 6 simultaneous enclaves with a resource usage per enclave of 7.0%, 3.8%, and 15.3% of the total LUTs, FFs, and BRAMS, respectively. To demonstrate the practicability of TEEOD in realworld applications, we successfully run a legacy open-source Bitcoin wallet.", "venue": "ArXiv", "authors": ["S'ergio  Pereira", "David  Cerdeira", "Cristiano  Rodrigues", "Sandro  Pinto"], "year": 2021, "n_citations": 0}
{"id": 1781930, "s2_id": "5f4b5d593e3ca3c07a9d1be4f639957aa8d464fa", "title": "Synthesis of Majority Expressions through Primitive Function Manipulation", "abstract": "Due to technology advancements and circuits miniaturization, the study of logic systems that can be applied to nanotechnology has been progressing steadily. Among the creation of nanoelectronic circuits the reversible and majority logic stand out. This paper proposes the MPC (Majority Primitives Combination) algorithm, used for majority logic synthesis. The algorithm receives a truth table as input and returns a majority function that covers the same set of minterms. The formulation of a valid output function is made with the combination of previously optimized functions. As cost criteria the algorithm searches for a function with the least number of levels, followed by the least number of gates, inverters, and gate inputs. In this paper it\u2019s also presented a comparison between the MPC and the exact_mig, currently considered the best algorithm for majority synthesis. The exact_mig encodes the exact synthesis of majority functions using the number of levels and gates as cost criteria. The MPC considers two additional cost criteria, the number of inverters and the number of gate inputs, with the goal to further improve exact_mig results. Therefore, the MPC aims to synthesize functions with the same amount of levels and gates, but with less inverters and gate inputs. Tests have shown that both algorithms return optimal solutions for all functions with 3 input variables. For functions with 4 inputs, the MPC is able to further improve 66% functions and achieves equal results for 11%. For functions with 5 input variables, out of a sample of 1000 randomly generated functions, the MPC further improved 48% functions and achieved equal results for 11%.@@@Due to technology advancements and circuits miniaturization, the study of logic systems that can be applied to nanotechnology has been progressing steadily. Among the creation of nanoelectronic circuits the reversible and majority logic stand out. This paper proposes the MPC (Majority Primitives Combination) algorithm, used for majority logic synthesis. The algorithm receives a truth table as input and returns a majority function that covers the same set of minterms. The formulation of a valid output function is made with the combination of previously optimized functions. As cost criteria the algorithm searches for a function with the least number of levels, followed by the least number of gates, inverters, and gate inputs. In this paper it\u2019s also presented a comparison between the MPC and the exact_mig, currently considered the best algorithm for majority synthesis. The exact_mig encodes the exact synthesis of majority functions using the number of levels and gates as cost criteria. The MPC considers two additional cost criteria, the number of inverters and the number of gate inputs, with the goal to further improve exact_mig results. Therefore, the MPC aims to synthesize functions with the same amount of levels and gates, but with less inverters and gate inputs. Tests have shown that both algorithms return optimal solutions for all functions with 3 input variables. For functions with 4 inputs, the MPC is able to further improve 66% functions and achieves equal results for 11%. For functions with 5 input variables, out of a sample of 1000 randomly generated functions, the MPC further improved 48% functions and achieved equal results for 11%.", "venue": "Advanced Boolean Techniques", "authors": ["Evandro C. Ferraz", "Jeferson de Lima Muniz", "Alexandre C. R. da Silva", "Gerhard W. Dueck"], "year": 2019, "n_citations": 0}
{"id": 1782864, "s2_id": "7f9883c8c819a02ee9344077baa0c1cc3efdcc21", "title": "Augmented Memory Computing: Dynamically Augmented SRAM Storage for Data Intensive Applications", "abstract": "The emergence of various data-intensive applications like artificial intelligence, machine learning etc., has highlighted the energy and throughput bottlenecks inherent in existing computing systems. Memory storage is a major component for such data intensive computational platforms. Consequently, memory-centric research investigations to improve the overall energy-efficiency and throughput of a given computing chip is being actively pursued. Exploration of novel memory technologies for high-density on-chip memories and in-memory computing are the key examples of such memory-centric approaches. In this paper, we propose a novel memory-centric scheme based on CMOS SRAM. Our proposal aims at dynamically increasing the on-chip memory storage capacity of SRAM arrays on-demand. The proposed scheme called Augmented Memory Computing allows an SRAM cell to operate in two different modes 1) the Normal mode and 2) the Augmented mode. In the Normal mode of operation, the SRAM cell functions like a standard 6 transistor (6T) SRAM cell, storing one bit of data in static format. While in the Augmented mode, each SRAM cell can store >1 bit of data (in a dynamic fashion). Specifically, we propose two novel SRAM cells an 8 transistor (8T) dual bit storage augmented cell and a 7 transistor (7T) ternary bit storage augmented cell. The proposed 8T dual bit SRAM cell when operated in the Augmented mode, can store a static bit of data while also, simultaneously, storing another bit in a dynamic form. Thus, when operated in Augmented mode, the 8T SRAM cell can store two bits of data one SRAM-like data and one DRAM-like data, thereby increasing or augmenting the memory storage capacity. On the other hand, the proposed 7T ternary bit storage augmented cell can either store a single SRAM data in Normal mode or can be configured to operate in Augmented mode, wherein it can store ternary data (3 levels (0,0), (0,1), (1,0)) in a dynamic manner. Thus, based on the mode of operation, the proposed augmented memory bitcells can either store one static bit of data or >1 bit of data in a dynamic format. We show the feasibility of our proposed bit-cells through extensive simulations at Globalfoundries 22nm FDX node. It is worth mentioning, the novel scheme of augmented memory bit-cells can be seamlessly combined with existing inmemory computing approaches for added energy and throughput benefits.", "venue": "ArXiv", "authors": ["Haripriya  Sheshadri", "Shwetha  Vijayakumar", "Ajey  Jacob", "Akhilesh  Jaiswal"], "year": 2021, "n_citations": 0}
{"id": 1784071, "s2_id": "8a598122d115657b59a61eca932837884122205d", "title": "Cluster Computing: A High-Performance Contender", "abstract": "When you first heard people speak of Piles of PCs, the first thing that came to mind may have been a cluttered computer room with processors, monitors, and snarls of cables all around. Collections of computers have undoubtedly become more sophisticated than in the early days of shared drives and modem connections. No matter what you call them, Clusters of Workstations (COW), Networks of Workstations (NOW), Workstation Clusters (WCs), Clusters of PCs (CoPs), clusters of computers are now filling the processing niche once occupied by more powerful stand-alone machines. This article discusses the need for cluster computing technology, Technologies, Components, and Applications, Supercluster Systems and Issues, The Need for a New Task Force, and Cluster Computing Educational Resources.", "venue": "Computer", "authors": ["Mark  Baker", "Rajkumar  Buyya", "Daniel C. Hyde"], "year": 1999, "n_citations": 31}
{"id": 1791959, "s2_id": "0ea81a2be6071bc25493eef12ddf4f9b6675576a", "title": "Cluster Computing White Paper", "abstract": "Cluster computing is not a new area of computing. It is, however, evident that there is a growing interest in its usage in all areas where applications have traditionally used parallel or distributed computing platforms. The growing interest has been fuelled in part by the availability of powerful microprocessors and high-speed networks as off-the-shelf commodity components as well as in part by the rapidly maturing software components available to support high performance and high availability applications. \nThis White Paper has been broken down into eleven sections, each of which has been put together by academics and industrial researchers who are both experts in their fields and where willing to volunteer their time and effort to put together this White Paper. The status of this paper is draft and we are at the stage of publicizing its presence and making a Request For Comments (RFC).", "venue": "ArXiv", "authors": ["Mark  Baker"], "year": 2000, "n_citations": 108}
{"id": 1794760, "s2_id": "6ff7fd341b0a4ab4d919f8ce3b35d447668e80ae", "title": "Improving the Performance and Endurance of Persistent Memory with Loose-Ordering Consistency", "abstract": "Persistent memory provides high-performance data persistence at main memory. Memory writes need to be performed in strict order to satisfy storage consistency requirements and enable correct recovery from system crashes. Unfortunately, adhering to such a strict order significantly degrades system performance and persistent memory endurance. This paper introduces a new mechanism, Loose-Ordering Consistency (LOC), that satisfies the ordering requirements at significantly lower performance and endurance loss. LOC consists of two key techniques. First, Eager Commit eliminates the need to perform a persistent commit record write within a transaction. We do so by ensuring that we can determine the status of all committed transactions during recovery by storing necessary metadata information statically with blocks of data written to memory. Second, Speculative Persistence relaxes the write ordering between transactions by allowing writes to be speculatively written to persistent memory. A speculative write is made visible to software only after its associated transaction commits. To enable this, our mechanism supports the tracking of committed transaction ID and multi-versioning in the CPU cache. Our evaluations show that LOC reduces the average performance overhead of memory persistence from 66.9% to 34.9% and the memory write traffic overhead from 17.1% to 3.4% on a variety of workloads.", "venue": "ArXiv", "authors": ["Youyou  Lu", "Jiwu  Shu", "Long  Sun", "Onur  Mutlu"], "year": 2017, "n_citations": 6}
{"id": 1795833, "s2_id": "fc345feee402d8a762cd72c11fecf800c92ccd23", "title": "Generation and Validation of Custom Multiplication IP Blocks from the Web", "abstract": "Every CPU carries one or more arithmetical and logical units. One popular operation that is performed by these units is multiplication. Automatic generation of custom VHDL models for performing this operation, allows the designer to achieve a time efficient design space exploration. Although these units are heavily utilized in modern digital circuits and DSP, there is no tool, accessible from the web, to generate the HDL description of such designs for arbitrary and different input bitwidths. In this paper, we present our web accessible tool to construct completely custom optimized multiplication units together with random generated test vectors for their verification. Our novel tool is one of the firsts web based EDA tools to automate the design of such units and simultaneously provide custom testbenches to verify their correctness. Our synthesized circuits on Xilinx Virtex 6 FPGA, operate up to 589 Mhz.", "venue": "ArXiv", "authors": ["Minas  Dasygenis"], "year": 2015, "n_citations": 0}
{"id": 1802493, "s2_id": "2737fb48ffb2b7b7bfbab993b4c5614b4b30d362", "title": "MTJ-Based Hardware Synapse Design for Quantized Deep Neural Networks", "abstract": "Quantized neural networks (QNNs) are being actively researched as a solution for the computational complexity and memory intensity of deep neural networks. This has sparked efforts to develop algorithms that support both inference and training with quantized weight and activation values without sacrificing accuracy. A recent example is the GXNOR framework for stochastic training of ternary and binary neural networks. In this paper, we introduce a novel hardware synapse circuit that uses magnetic tunnel junction (MTJ) devices to support the GXNOR training. Our solution enables processing near memory (PNM) of QNNs, therefore can further reduce the data movements from and into the memory. We simulated MTJ-based stochastic training of a TNN over the MNIST and SVHN datasets and achieved an accuracy of 98.61% and 93.99%, respectively.", "venue": "ArXiv", "authors": ["Tzofnat  Greenberg-Toledo", "Ben  Perach", "Daniel  Soudry", "Shahar  Kvatinsky"], "year": 2019, "n_citations": 1}
{"id": 1807546, "s2_id": "3e234d3c5aa3730c05986090b8c42121c90aa4e9", "title": "Uncovering In-DRAM RowHammer Protection Mechanisms:A New Methodology, Custom RowHammer Patterns, and Implications", "abstract": "The RowHammer vulnerability in DRAM is a critical threat to system security. To protect against RowHammer, vendors commit to security-through-obscurity: modern DRAM chips rely on undocumented, proprietary, on-die mitigations, commonly known as Target Row Refresh (TRR). At a high level, TRR detects and refreshes potential RowHammer-victim rows, but its exact implementations are not openly disclosed. Security guarantees of TRR mechanisms cannot be easily studied due to their proprietary nature. To assess the security guarantees of recent DRAM chips, we present Uncovering TRR (U-TRR), an experimental methodology to analyze in-DRAM TRR implementations. U-TRR is based on the new observation that data retention failures in DRAM enable a side channel that leaks information on how TRR refreshes potential victim rows. U-TRR allows us to (i) understand how logical DRAM rows are laid out physically in silicon; (ii) study undocumented on-die TRR mechanisms; and (iii) combine (i) and (ii) to evaluate the RowHammer security guarantees of modern DRAM chips. We show how U-TRR allows us to craft RowHammer access patterns that successfully circumvent the TRR mechanisms employed in 45 DRAM modules of the three major DRAM vendors. We find that the DRAM modules we analyze are vulnerable to RowHammer, having bit flips in up to 99.9% of all DRAM rows.", "venue": "MICRO", "authors": ["Hasan  Hassan", "Yahya Can Tugrul", "Jeremie S. Kim", "Victor van der Veen", "Kaveh  Razavi", "Onur  Mutlu"], "year": 2021, "n_citations": 5}
{"id": 1816854, "s2_id": "97db5832995ac8a4f4d2863d6a818cd31ff8347f", "title": "Low Complexity Multiply-Accumulate Units for Convolutional Neural Networks with Weight-Sharing", "abstract": "Convolutional neural networks (CNNs) are one of the most successful machine-learning techniques for image, voice, and video processing. CNNs require large amounts of processing capacity and memory bandwidth. Hardware accelerators have been proposed for CNNs that typically contain large numbers of multiply-accumulate (MAC) units, the multipliers of which are large in integrated circuit (IC) gate count and power consumption. \u201cWeight-sharing\u201d accelerators have been proposed where the full range of weight values in a trained CNN are compressed and put into bins, and the bin index is used to access the weight-shared value. We reduce power and area of the CNN by implementing parallel accumulate shared MAC (PASM) in a weight-shared CNN. PASM re-architects the MAC to instead count the frequency of each weight and place it in a bin. The accumulated value is computed in a subsequent multiply phase, significantly reducing gate count and power consumption of the CNN. In this article, we implement PASM in a weight-shared CNN convolution hardware accelerator and analyze its effectiveness. Experiments show that for a clock speed 1GHz implemented on a 45nm ASIC process our approach results in fewer gates, smaller logic, and reduced power with only a slight increase in latency. We also show that the same weight-shared-with-PASM CNN accelerator can be implemented in resource-constrained FPGAs, where the FPGA has limited numbers of digital signal processor (DSP) units to accelerate the MAC operations.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["James  Garland", "David  Gregg"], "year": 2018, "n_citations": 12}
{"id": 1817358, "s2_id": "6c15f9e2c91726d284ba5891fdb1966665657185", "title": "Design Space Exploration to Find the Optimum Cache and Register File Size for Embedded Applications", "abstract": "In the future, embedded processors must process more computation-intensive network applications and internet traffic and packet-processing tasks become heavier and sophisticated. Since the processor performance is severely related to the average memory access delay and also the number of processor registers affects the performance, cache and register file are two major parts in designing embedded processor architecture. Although increasing cache and register file size leads to performance improvement in embedded applications and packet-processing tasks in high traffic networks with too much packets, the increased area, power consumption and memory hierarchy delay are the overheads of these techniques. Therefore, implementing these components in the optimum size is of significant interest in the design of embedded processors. This paper explores the effect of cache and register file size on the processor performance to calculate the optimum size of these components for embedded applications. Experimental results show that although having bigger cache and register file is one of the performance improvement approaches in embedded processors, however, by increasing the size of these parameters over a threshold level, performance improvement is saturated and then, decreased.", "venue": "ArXiv", "authors": ["Mehdi  Alipour", "Mostafa E. Salehi", "Hesamodin Shojaei Baghini"], "year": 2012, "n_citations": 17}
{"id": 1823061, "s2_id": "cf997bc88d998aa48a1a4600c82a753397655edc", "title": "Optimising Design Verification Using Machine Learning: An Open Source Solution", "abstract": "With the complexity of Integrated Circuits increasing, design verification has become the most time consuming part of the ASIC design flow. Nearly 70% of the SoC design cycle is consumed by verification. The most commonly used approach to test all corner cases is through the use of Constrained Random Verification. Random stimulus is given in order to hit all possible combinations and test the design thoroughly. However, this approach often requires significant human expertise to reach all corner cases. This paper presents an alternative using Machine Learning to generate the input stimulus. This will allow for faster thorough verification of the design with less human intervention. Furthermore, it is proposed to use the open source verification environment 'Cocotb'. Based on Python, it is simple, intuitive and has a vast library of functions for machine learning applications. This makes it more convenient to use than the bulkier approach using traditional Hardware Verification Languages such as System Verilog or Specman E.", "venue": "ArXiv", "authors": ["B. Samhita Varambally", "Naman  Sehgal"], "year": 2020, "n_citations": 1}
{"id": 1827985, "s2_id": "229a8012a4538becc7828a523d92bece18c51512", "title": "Leaking Control Flow Information via the Hardware Prefetcher", "abstract": "Modern processor designs use a variety of microarchitectural methods to achieve high performance. Unfortunately, new side-channels have often been uncovered that exploit these enhanced designs. One area that has received little attention from a security perspective is the processor\u2019s hardware prefetcher, a critical component used to mitigate DRAM latency in today\u2019s systems. Prefetchers, like branch predictors, hold critical state related to the execution of the application, and have the potential to leak secret information. But up to now, there has not been a demonstration of a generic prefetcher side-channel that could be actively exploited in today\u2019s hardware. In this paper, we present AfterImage, a new side-channel that exploits the Intel Instruction Pointer-based stride prefetcher. We observe that, when the execution of the processor switches between different private domains, the prefetcher trained by one domain can be triggered in another. To the best of our knowledge, this work is the first to publicly demonstrate a methodology that is both algorithm-agnostic and also able to leak kernel data into userspace. AfterImage is different from previous works, as it leaks data on the non-speculative path of execution. Because of this, a large class of work that has focused on protecting transient, branch-outcome-based data will be unable to block this side-channel. By reverse-engineering the IP-stride prefetcher in modern Intel processors, we have successfully developed three variants of AfterImage to leak control flow information across code regions, processes and the user-kernel boundary. We find a high level of accuracy in leaking information with our methodology (from 91%, up to 99%), and propose two mitigation techniques to block this side-channel, one of which can be used on hardware systems today.", "venue": "ArXiv", "authors": ["Yun  Chen", "Lingfeng  Pei", "Trevor E. Carlson"], "year": 2021, "n_citations": 0}
{"id": 1828793, "s2_id": "f156b5f100a8b52a87073993de3a54f159ce71d7", "title": "Net2: A Graph Attention Network Method Customized for Pre-Placement Net Length Estimation", "abstract": "Net length is a key proxy metric for optimizing timing and power across various stages of a standard digital design flow. However, the bulk of net length information is not available until cell placement, and hence it is a significant challenge to explicitly consider net length optimization in design stages prior to placement, such as logic synthesis. This work addresses this challenge by proposing a graph attention network method with customization, called Net2, to estimate individual net length before cell placement. Its accuracy-oriented version Net2a achieves about 15% better accuracy than several previous works in identifying both long nets and long critical paths. Its fast version Net2f is more than 1000\u00d7 faster than placement while still outperforms previous works and other neural network techniques in terms of various accuracy metrics.", "venue": "2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Zhiyao  Xie", "Rongjian  Liang", "Xiaoqing  Xu", "Jiang  Hu", "Yixiao  Duan", "Yiran  Chen"], "year": 2021, "n_citations": 2}
{"id": 1832406, "s2_id": "14dc299ef6b9d15b285987d6fae5b941f6a017e8", "title": "A Scalable High-Performance Priority Encoder Using 1D-Array to 2D-Array Conversion", "abstract": "In our prior study of an <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {L}$ </tex-math></inline-formula>-bit priority encoder (PE), a so-called one-directional-array to two-directional-array conversion method is deployed to turn an <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {L}$ </tex-math></inline-formula>-bit input data into an <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {M\\times N}$ </tex-math></inline-formula>-bit matrix. Following this, an <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {N}$ </tex-math></inline-formula>-bit PE and an <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {M}$ </tex-math></inline-formula>-bit PE are employed to obtain a row index and column index. From those, the highest priority bit of <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {L}$ </tex-math></inline-formula>-bit input data is achieved. This brief extends our previous work to construct a scalable architecture of high-performance large-sized PEs. An optimum pair of (<inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {M}$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {N}$ </tex-math></inline-formula>) and look-ahead signal are proposed to improve the overall PE performance significantly. The evaluation is achieved by implementing a variety of PEs whose <inline-formula> <tex-math notation=\"LaTeX\">$\\boldsymbol {L}$ </tex-math></inline-formula> varies from 4-bit to 4096-bit in 180-nm CMOS technology. According to post-place-and-route simulation results, at PE size of 64 bits, 256 bits, and 2048 bits the operating frequencies reach 649 MHz, 520 MHz, and 370 MHz, which are 1.2 times, 1.5 times, and 1.4 times, as high as state-of-the-art ones.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Xuan-Thuan  Nguyen", "Hong-Thu  Nguyen", "Cong-Kha  Pham"], "year": 2017, "n_citations": 11}
{"id": 1842225, "s2_id": "fa97d7527bf4d19b9fceb4e8c8c1ffb1b5182d47", "title": "High-Level Combined Deterministic and Pseudo-exhuastive Test Generation for RISC Processors", "abstract": "Recent safety standards set stringent requirements for the target fault coverage in embedded microprocessors, with the objective to guarantee robustness and functional safety of the critical electronic systems. This motivates the need for improving the quality of test generation for microprocessors. A new high-level implementation-independent test generation method for RISC processors is proposed. The set of instructions of the processor is partitioned into groups. For each group, a dedicated test template is created, to be used for generating two test programs, for testing the control and the data paths respectively. For testing the control part, a novel high-level control fault model is proposed. Using this model, a set of deterministic test data operands are generated for each instruction of the given group. The advantage of the high-level fault model is that it covers larger than SAF fault class including multiple fault coverage in the control part. For generating the data path test, pseudo-exhaustive data operands are used. We investigated the feasibility of the approach and demonstrated high efficiency of the generated test programs for testing the execute module of the miniMIPS RISC processor.", "venue": "2019 IEEE European Test Symposium (ETS)", "authors": ["Adeboye Stephen Oyeniran", "Raimund  Ubar", "Maksim  Jenihhin", "Cemil Cem G\u00fcrsoy", "Jaan  Raik"], "year": 2019, "n_citations": 3}
{"id": 1846041, "s2_id": "729ba8795ee1a722fdbfc0bf4a4c33f7d7ece35b", "title": "Flexible-Latency DRAM: Understanding and Exploiting Latency Variation in Modern DRAM Chips", "abstract": "This article summarizes key results of our work on experimental characterization and analysis of latency variation and latency-reliability trade-offs in modern DRAM chips, which was published in SIGMETRICS 2016, and examines the work's significance and future potential. \nThe goal of this work is to (i) experimentally characterize and understand the latency variation across cells within a DRAM chip for these three fundamental DRAM operations, and (ii) develop new mechanisms that exploit our understanding of the latency variation to reliably improve performance. To this end, we comprehensively characterize 240 DRAM chips from three major vendors, and make six major new observations about latency variation within DRAM. Notably, we find that (i) there is large latency variation across the cells for each of the three operations; (ii) variation characteristics exhibit significant spatial locality: slower cells are clustered in certain regions of a DRAM chip; and (iii) the three fundamental operations exhibit different reliability characteristics when the latency of each operation is reduced. \nBased on our observations, we propose Flexible-LatencY DRAM (FLY-DRAM), a mechanism that exploits latency variation across DRAM cells within a DRAM chip to improve system performance. The key idea of FLY-DRAM is to exploit the spatial locality of slower cells within DRAM, and access the faster DRAM regions with reduced latencies for the fundamental operations. Our evaluations show that FLY-DRAM improves the performance of a wide range of applications by 13.3%, 17.6%, and 19.5%, on average, for each of the three different vendors' real DRAM chips, in a simulated 8-core system.", "venue": "ArXiv", "authors": ["Kevin K. Chang", "Abhijith  Kashyap", "Hasan  Hassan", "Saugata  Ghose", "Kevin  Hsieh", "Donghyuk  Lee", "Tianshi  Li", "Gennady  Pekhimenko", "Samira  Khan", "Onur  Mutlu"], "year": 2018, "n_citations": 1}
{"id": 1846712, "s2_id": "e962a61071b6a3a751a7faeb04dc94ddaf4a38a8", "title": "LayerPipe: Accelerating Deep Neural Network Training by Intra-Layer and Inter-Layer Gradient Pipelining and Multiprocessor Scheduling", "abstract": "The time required for training the neural networks increases with size, complexity, and depth. Training model parameters by backpropagation inherently creates feedback loops. These loops hinder efficient pipelining and scheduling of the tasks within the layer and between consecutive layers. Prior approaches, such as PipeDream, have exploited the use of delayed gradient to achieve inter-layer pipelining. However, these approaches treat the entire backpropagation as a single task; this leads to an increase in computation time and processor underutilization. This paper presents novel optimization approaches where the gradient computations with respect to the weights and the activation functions are considered independently; therefore, these can be computed in parallel. This is referred to as intra-layer optimization. Additionally, the gradient computation with respect to the activation function is further divided into two parts and distributed to two consecutive layers. This leads to balanced scheduling where the computation time of each layer is the same. This is referred to as inter-layer optimization. The proposed system, referred to as LayerPipe, reduces the number of clock cycles required for training while maximizing processor utilization with minimal inter-processor communication overhead. LayerPipe achieves an average speedup of 25 % and upwards of 80% with 7 to 9 processors with less communication overhead when compared to PipeDream.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Nanda K. Unnikrishnan", "Keshab K. Parhi"], "year": 2021, "n_citations": 0}
{"id": 1847858, "s2_id": "a6cee4e77b227f9e43af764855846e7e683b6bd5", "title": "Performance Comparison of some Synchronous Adders", "abstract": "This technical note compares the performance of some synchronous adders which correspond to the following architectures: i) ripple carry adder (RCA), ii) recursive carry lookahead adder (RCLA), iii) hybrid RCLA-RCA with the RCA used in the least significant adder bit positions, iv) block carry lookahead adder (BCLA), v) hybrid BCLA-RCA with the RCA used in the least significant adder bit positions, and vi) non-uniform input partitioned carry select adders (CSLAs) without and with the binary to excess-1 code (BEC) converter. The 32-bit addition was considered as an example operation. The adder architectures mentioned were implemented by targeting a typical case PVT specification (high threshold voltage, supply voltage of 1.05V and operating temperature of 25 degrees Celsius) of the Synopsys 32/28nm CMOS technology. The comparison leads to the following observations: i) the hybrid CCLA-RCA is preferable to the other adders in terms of the speed, the power-delay product, and the energy-delay product, ii) the non-uniform input partitioned CSLA without the BEC converter is preferable to the other adders in terms of the area-delay product, and iii) the RCA incorporating the full adder present in the standard digital cell library is preferable to the other adders in terms of the power-delay-area product.", "venue": "ArXiv", "authors": ["P.  Balasubramanian"], "year": 2018, "n_citations": 2}
{"id": 1851954, "s2_id": "c50e20fd223807a6a692103bcd5530f0388c15e7", "title": "GRIP: A Graph Neural Network Accelerator Architecture", "abstract": "We present GRIP, a graph neural network accelerator architecture designed for low-latency inference. AcceleratingGNNs is challenging because they combine two distinct types of computation: arithmetic-intensive vertex-centric operations and memory-intensive edge-centric operations. GRIP splits GNN inference into a fixed set of edge- and vertex-centric execution phases that can be implemented in hardware. We then specialize each unit for the unique computational structure found in each phase.For vertex-centric phases, GRIP uses a high performance matrix multiply engine coupled with a dedicated memory subsystem for weights to improve reuse. For edge-centric phases, GRIP use multiple parallel prefetch and reduction engines to alleviate the irregularity in memory accesses. Finally, GRIP supports severalGNN optimizations, including a novel optimization called vertex-tiling which increases the reuse of weight data.We evaluate GRIP by performing synthesis and place and route for a 28nm implementation capable of executing inference for several widely-used GNN models (GCN, GraphSAGE, G-GCN, and GIN). Across several benchmark graphs, it reduces 99th percentile latency by a geometric mean of 17x and 23x compared to a CPU and GPU baseline, respectively, while drawing only 5W.", "venue": "ArXiv", "authors": ["Kevin  Kiningham", "Christopher  Re", "Philip  Levis"], "year": 2020, "n_citations": 15}
{"id": 1852144, "s2_id": "3b6318097b49e5373fe1de9d408689fb416477de", "title": "Denial-of-Service Attacks on Shared Cache in Multicore: Analysis and Prevention", "abstract": "In this paper we investigate the feasibility of denial-of-service (DoS) attacks on shared caches in multicore platforms. With carefully engineered attacker tasks, we are able to cause more than 300X execution time increases on a victim task running on a dedicated core on a popular embedded multicore platform, regardless of whether we partition its shared cache or not. Based on careful experimentation on real and simulated multicore platforms, we identify an internal hardware structure of a non-blocking cache, namely the cache writeback buffer, as a potential target of shared cache DoS attacks. We propose an OS-level solution to prevent such DoS attacks by extending a state-of-the-art memory bandwidth regulation mechanism. We implement the proposed mechanism in Linux on a real multicore platform and show its effectiveness in protecting against cache DoS attacks.", "venue": "2019 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)", "authors": ["Michael Garrett Bechtel", "Heechul  Yun"], "year": 2019, "n_citations": 22}
{"id": 1857802, "s2_id": "f2df5f1f1c1d93b3e09c489b1c7111e92f46f200", "title": "FPGA based implementation of deep neural networks using on-chip memory only", "abstract": "Deep neural networks (DNNs) demand a very large amount of computation and weight storage, and thus efficient implementation using special purpose hardware is highly desired. In this work, we have developed an FPGA based fixed-point DNN system using only on-chip memory not to access external DRAM. The execution time and energy consumption of the developed system is compared with a GPU based implementation. Since the capacity of memory in FPGA is limited, only 3-bit weights are used for this implementation, and training based fixed-point weight optimization is employed. The implementation using Xilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmark and a phoneme recognition task on TIMIT corpus. The obtained speed is about one quarter of a GPU based implementation and much better than that of a PC based one. The power consumption is less than 5 Watt at the full speed operation resulting in much higher efficiency compared to GPU based systems.", "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Jinhwan  Park", "Wonyong  Sung"], "year": 2016, "n_citations": 51}
{"id": 1859105, "s2_id": "5e8d49570b86043b9021558687545d5262557eb1", "title": "Power comparison of CMOS and adiabatic full adder circuit", "abstract": "Full adders are important components in applications such as digital signal processors (DSP) architectures and microprocessors. Apart from the basic addition adders also used in performing useful operations such as subtraction, multiplication, division, address calculation, etc. In most of these systems the adder lies in the critical path that determines the overall performance of the system. In this paper conventional complementary metal oxide semiconductor (CMOS) and adiabatic adder circuits are analyzed in terms of power and transistor count using 0.18UM technology.", "venue": "VLSIC 2011", "authors": ["Sunil Gavaskar Reddy", "Rajendra  Prasad"], "year": 2011, "n_citations": 28}
{"id": 1860000, "s2_id": "91e028dd30c114f1a0af539cf6ac86c20eda9f73", "title": "In-DRAM Bulk Bitwise Execution Engine", "abstract": "Many applications heavily use bitwise operations on large bitvectors as part of their computation. In existing systems, performing such bulk bitwise operations requires the processor to transfer a large amount of data on the memory channel, thereby consuming high latency, memory bandwidth, and energy. In this paper, we describe Ambit, a recently-proposed mechanism to perform bulk bitwise operations completely inside main memory. Ambit exploits the internal organization and analog operation of DRAM-based memory to achieve low cost, high performance, and low energy. Ambit exposes a new bulk bitwise execution model to the host processor. Evaluations show that Ambit significantly improves the performance of several applications that use bulk bitwise operations, including databases.", "venue": "ArXiv", "authors": ["Vivek  Seshadri", "Onur  Mutlu"], "year": 2019, "n_citations": 38}
{"id": 1860599, "s2_id": "7574bf92b17321c93df3c5b398df12de64eb5e3f", "title": "Power and Accuracy of Multi-Layer Perceptrons (MLPs) under Reduced-voltage FPGA BRAMs Operation", "abstract": "In this paper, we exploit the aggressive supply voltage underscaling technique in Block RAMs (BRAMs) of Field Programmable Gate Arrays (FPGAs) to improve the energy efficiency of Multi-Layer Perceptrons (MLPs). Additionally, we evaluate and improve the resilience of this accelerator. Through experiments on several representative FPGA fabrics, we observe that until a minimum safe voltage level, i.e., Vmin the MLP accuracy is not affected. This safe region involves a large voltage guardband. Also, it involves a narrower voltage region where faults start to appear in memories due to the increased circuit delay, but these faults are masked by MLP, and thus, its accuracy is not affected. However, further undervolting causes significant accuracy loss as a result of the fast-increasing high fault rates. Based on the characterization of these undervolting faults, we propose fault mitigation techniques that can effectively improve the resilience behavior of such accelerator. Our evaluation is based on four FPGA platforms. On average, we achieve >90% energy saving with a negligible accuracy loss of up to 0.1%.", "venue": "ArXiv", "authors": ["Behzad  Salami", "Osman  Unsal", "Adrian  Cristal"], "year": 2020, "n_citations": 0}
{"id": 1860740, "s2_id": "b609e040aeed2da79950244ecfa0121ff51d7270", "title": "Efficient Sealable Protection Keys for RISC-V", "abstract": "With the continuous increase in the number of software-based attacks, there has been a growing effort towards isolating sensitive data and trusted software components from untrusted third-party components. A hardware-assisted intra-process isolation mechanism enables software developers to partition a process into isolated components and in turn secure sensitive data from untrusted components. However, most of the existing hardware-assisted intra-process isolation mechanisms in modern processors, such as ARM and IBM Power, rely on costly kernel operations for switching between trusted and untrusted domains. Recently, Intel introduced a new hardware feature for intra-process memory isolation, called Memory Protection Keys (MPK), which enables a user-space process to switch the domains in an efficient way. While the efficiency of Intel MPK enables developers to leverage it for common use cases such as Code-Pointer Integrity, the limited number of unique domains (16) prohibits its use in cases such as OpenSSL where a large number of domains are required. Moreover, Intel MPK suffers from the protection key use-after-free vulnerability. To address these shortcomings, in this paper, we propose an efficient intra-process isolation technique for the RISC-V open ISA, called SealPK, which supports up to 1024 unique domains. SealPK prevents the protection key use-after-free problem by leveraging a lazy de-allocation approach. To further strengthen SealPK, we devise three novel sealing features to protect the allocated domains, their associated pages, and their permissions from modifications or tampering by an attacker. To demonstrate the feasibility of our design, we implement SealPK on a RISC-V Rocket processor, provide the OS support for it, and prototype our design on an FPGA. We demonstrate the efficiency of SealPK by leveraging it to implement an isolated shadow stack on our FPGA prototype.", "venue": "ArXiv", "authors": ["Leila  Delshadtehrani", "Sadullah  Canakci", "Manuel  Egele", "Ajay  Joshi"], "year": 2020, "n_citations": 3}
{"id": 1861965, "s2_id": "d309dd36900e8f736a0e5c1a53f103a05530c4b2", "title": "Design and implementation of an out-of-order execution engine of floating-point arithmetic operations", "abstract": "In this thesis, work is undertaken towards the design in hardware description languages and implementation in FPGA of an out of order execution engine of floating point arithmetic operations. This thesis work, is part of a project called Lagarto.", "venue": "ArXiv", "authors": ["Crist\u00f3bal Ram\u00edrez Lazo"], "year": 2021, "n_citations": 1}
{"id": 1864524, "s2_id": "58a24e0669414d1f5fe988b0b3eb2e0c00c1e81c", "title": "Popcorns-Pro: A Cooperative Network-Server Approach for Data Center Energy Optimization", "abstract": "Data centers have become a popular computing platform for various applications, and they account for nearly 2% of total US energy consumption. Therefore, it has become important to optimize data center power, and reduce their energy footprint. With newer powerefficient designs in data center infrastructure and cooling equipment, active components such as servers and data center networks consume a majority of power. Most existing work optimize power in servers and networks independently, and do not address them together in a holistic fashion that has the potential to achieve greater power savings. In this article, we present Popcorns-Pro, a cooperative server-network framework for energy optimization. We present a comprehensive power model for heterogeneous data center switches along with low power mode designs in combination with the server power model. We design job scheduling algorithms that place tasks onto servers in a power-aware manner, such that servers and network switches can take effective advantage of low power states and available network link capacities. Our experimental results show that we are able to achieve significantly higher savings upto 80% compared to the previously well-known server and network power optimization policies.", "venue": "ArXiv", "authors": ["Sai Santosh Dayapule", "Kathy  Nguyen", "Gregory  Kahl", "Suresh  Subramaniam", "Guru  Venkataramani"], "year": 2021, "n_citations": 0}
{"id": 1864886, "s2_id": "2d7cb1dd9006c30ea9d2c7ed6c84dc95ec490356", "title": "Physarum machine: Implementation of Kolmogorov-Uspensky machine in biological substrat", "abstract": "This invention relates to a method and apparatus for the automatic assembly of a flexible interlayer sheet with one or more bent glass sheets using residual heat from bending the glass sheets to make the flexible interlayer sheet sufficiently tacky to avoid relative sliding between the sheets comprising the subassembly or sandwich that is assembled.", "venue": "ArXiv", "authors": ["Andrew  Adamatzky"], "year": 2007, "n_citations": 31}
{"id": 1870309, "s2_id": "7beecdc95b11a0ecb23217bf55a6535e7645ff4d", "title": "Tiered-Latency DRAM (TL-DRAM)", "abstract": "This paper summarizes the idea of Tiered-Latency DRAM, which was published in HPCA 2013. The key goal of TL-DRAM is to provide low DRAM latency at low cost, a critical problem in modern memory systems. To this end, TL-DRAM introduces heterogeneity into the design of a DRAM subarray by segmenting the bitlines, thereby creating a low-latency, low-energy, low-capacity portion in the subarray (called the near segment), which is close to the sense amplifiers, and a high-latency, high-energy, high-capacity portion, which is farther away from the sense amplifiers. Thus, DRAM becomes heterogeneous with a small portion having lower latency and a large portion having higher latency. Various techniques can be employed to take advantage of the low-latency near segment and this new heterogeneous DRAM substrate, including hardware-based caching and software based caching and memory allocation of frequently used data in the near segment. Evaluations with simple such techniques show significant performance and energy-efficiency benefits.", "venue": "ArXiv", "authors": ["Donghyuk  Lee", "Yoongu  Kim", "Vivek  Seshadri", "Jamie  Liu", "Lavanya  Subramanian", "Onur  Mutlu"], "year": 2016, "n_citations": 0}
{"id": 1870524, "s2_id": "bdd18f7bc867fe501f6a3e2ff728c64b8aed7545", "title": "Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions by Exploiting DRAM Data Retention Characteristics", "abstract": "Increasing single-cell DRAM error rates have pushed DRAM manufacturers to adopt on-die error-correction coding (ECC), which operates entirely within a DRAM chip to improve factory yield. The on-die ECC function and its effects on DRAM reliability are considered trade secrets, so only the manufacturer knows precisely how on-die ECC alters the externally-visible reliability characteristics. Consequently, on-die ECC obstructs third-party DRAM customers (e.g., test engineers, experimental researchers), who typically design, test, and validate systems based on these characteristicsTo give third parties insight into precisely how on-die ECC transforms DRAM error patterns during error correction, we introduce Bit-Exact ECC Recovery (BEER), a new methodology for determining the full DRAM on-die ECC function (i.e., its parity-check matrix) without hardware tools, prerequisite knowledge about the DRAM chip or on-die ECC mechanism, or access to ECC metadata (e.g., error syndromes, parity information). BEER exploits the key insight that non-intrusively inducing data-retention errors with carefully-crafted test pat-terns reveals behavior that is unique to a specific ECC functionWe use BEER to identify the ECC functions of 80 real LPDDR4 DRAM chips with on-die ECC from three major DRAM manufacturers. We evaluate BEER\u2019s correctness in simulation and performance on a real system to show that BEER is effective and practical across a wide range of on-die ECC functions. To demonstrate BEER\u2019s value, we propose and discuss several ways that third parties can use BEER to improve their design and testing practices. As a concrete example, we introduce and evaluate BEEP, the first error profiling method-ology that uses the known on-die ECC function to recover the number and bit-exact locations of unobservable raw bit errors responsible for observable post-correction errors.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Minesh  Patel", "Jeremie S. Kim", "Taha  Shahroodi", "Hasan  Hassan", "Onur  Mutlu"], "year": 2020, "n_citations": 15}
{"id": 1870550, "s2_id": "cd009ef1e0ddb7db0bb95b1ab0314b0034642b6d", "title": "Selfish vs. Unselfish Optimization of Network Creation", "abstract": "We investigate several variants of a network creation model: a group of agents builds up a network between them while trying to keep the costs of this network small. The cost function consists of two addends, namely (i) a constant amount for each edge an agent buys and (ii) the minimum number of hops it takes sending messages to other agents. Despite the simplicity of this model, various complex network structures emerge depending on the weight between the two addends of the cost function and on the selfish or unselfish behaviour of the agents.", "venue": "ArXiv", "authors": ["Johannes J. Schneider", "Scott  Kirkpatrick"], "year": 2005, "n_citations": 8}
{"id": 1876182, "s2_id": "5367bb4b26da10dfdc8b6cd11808a3d4874fbfdf", "title": "Fast Prefix Adders for Non-uniform Input Arrival Times", "abstract": "We consider the problem of constructing fast and small parallel prefix adders for non-uniform input arrival times. In modern computer chips, adders with up to hundreds of inputs occur frequently, and they are often embedded into more complex circuits, e.g. multipliers, leading to instance-specific non-uniform input arrival times. Most previous results are based on representing binary carry-propagate adders as parallel prefix graphs, in which pairs of generate and propagate signals are combined using complex gates called prefix gates. Examples of commonly-used adders are constructed based on the Kogge\u2013Stone or Ladner\u2013Fischer prefix graphs. Adders constructed in this model usually minimize the delay in terms of these prefix gates. However, the delay in terms of logic gates can be worse by a factor of two. In contrast, we aim to minimize the delay of the underlying logic circuit directly. We prove a lower bound on the delay of a carry bit computation achievable by any prefix carry bit circuit and develop an algorithm that computes a prefix carry bit circuit with optimum delay up to a small additive constant. Our algorithm improves the running time of a previous dynamic program for constructing a prefix carry bit from $$\\mathcal {O}(n^3)$$O(n3) to $$\\mathcal {O}(n \\log ^2 n)$$O(nlog2n) while simultaneously improving the delay and size guarantee, where n is the number of bits in the summands. Furthermore, we use this algorithm as a subroutine to compute a full adder in near-linear time, reducing the delay approximation factor of 2 from previous approaches to 1.441 for our algorithm.", "venue": "Algorithmica", "authors": ["Stephan  Held", "Sophie  Spirkl"], "year": 2015, "n_citations": 7}
{"id": 1877191, "s2_id": "cad4fa7045571613332f849aa07e841a7ab19fd4", "title": "Arsenal of Hardware Prefetchers", "abstract": "Hardware prefetching is one of the latency tolerance optimization techniques that tolerate costly DRAM accesses. Though hardware prefetching is one of the fundamental mechanisms prevalent on most of the commercial machines, there is no prefetching technique that works well across all the access patterns and different types of workloads. Through this paper, we propose Arsenal, a prefetching framework which allows the advantages provided by different data prefetchers to be combined, by dynamically selecting the best-suited prefetcher for the current workload. Thus effectively improving the versatility of the prefetching system. It bases on the classic Sandbox prefetcher that dynamically adapts and utilizes multiple offsets for sequential prefetchers. We take it to the next step by switching between prefetchers like Multi look Ahead Offset Prefetching and Timing SKID Prefetcher on the run. Arsenal utilizes a space-efficient pooling filter, Bloom filters, that keeps track of useful prefetches of each of these component prefetchers and thus helps to maintain a score for each of the component prefetchers. This approach is shown to provide better speedup than anyone prefetcher alone. Arsenal provides a performance improvement of 44.29% on the single-core mixes and 19.5% for some of the selected 25 representative multi-core mixes.", "venue": "ArXiv", "authors": ["Dishank  Yadav", "Chaitanya  Paikara"], "year": 2019, "n_citations": 0}
{"id": 1878014, "s2_id": "16491bcbf9457b1c252ca3e4c49d1b86e875387b", "title": "Energy-efficient Machine Learning in Silicon: A Communications-inspired Approach", "abstract": "This position paper advocates a communications-inspired approach to the design of machine learning systems on energy-constrained embedded `always-on' platforms. The communications-inspired approach has two versions - 1) a deterministic version where existing low-power communication IC design methods are repurposed, and 2) a stochastic version referred to as Shannon-inspired statistical information processing employing information-based metrics, statistical error compensation (SEC), and retraining-based methods to implement ML systems on stochastic circuit/device fabrics operating at the limits of energy-efficiency. The communications-inspired approach has the potential to fully leverage the opportunities afforded by ML algorithms and applications in order to address the challenges inherent in their deployment on energy-constrained platforms.", "venue": "ArXiv", "authors": ["Naresh R. Shanbhag"], "year": 2016, "n_citations": 10}
{"id": 1878577, "s2_id": "a5d258aaad44fb799ad49f633f78e66c54f693bf", "title": "A RISC-V SystemC-TLM simulator", "abstract": "This work presents a SystemC-TLM based simulator for a RISC-V microcontroller. This simulator is focused on simplicity and easy expandable of a RISC-V. It is built around a full RISC-V instruction set simulator that supports full RISC-V ISA and extensions M, A, C, Zicsr and Zifencei. The ISS is encapsulated in a TLM-2 wrapper that enables it to communicate with any other TLM-2 compatible module. The simulator also includes a very basic set of peripherals to enable a complete SoC simulator. The running code can be compiled with standard tools and using standard C libraries without modifications. The simulator is able to correctly execute the riscv-compliance suite. The entire simulator is published as a docker image to ease its installation and use by developers. A porting of FreeRTOSv10.2.1 for the simulated SoC is also published.", "venue": "ArXiv", "authors": ["Marius  Mont'on"], "year": 2020, "n_citations": 1}
{"id": 1881130, "s2_id": "06482dea3d8a0df60a35b29db2dba636125e36a9", "title": "Circuit-level modeling for concurrent testing of operational defects due to gate oxide breakdown", "abstract": "As device sizes shrink and current densities increase, the probability of device failures due to gate oxide break-down (OBD) also increases. To provide designs that are tolerant to such failures, we must investigate and understand the manifestations of this physical phenomenon at the circuit and system level. In this paper, we develop a model for operational OBD defects, and we explore how to test for faults due to OBD. For a NAND gate, we derive the necessary input conditions that excite and detect errors due to OBD defects at the gate level. We show that traditional pattern generators fail to exercise all of these defects. Finally, we show that these test patterns can be propagated and justified for a combinational circuit in a manner similar to traditional ATPG.", "venue": "Design, Automation and Test in Europe", "authors": ["Jonathan R. Carter", "Sule  Ozev", "Daniel J. Sorin"], "year": 2005, "n_citations": 25}
{"id": 1882004, "s2_id": "a4cd5bcc1afbabc306429ad1cd1da5c4132af44c", "title": "CapStore: Energy-Efficient Design and Management of the On-Chip Memory for CapsuleNet Inference Accelerators", "abstract": "Deep Neural Networks (DNNs) have been established as the state-of-the-art algorithm for advanced machine learning applications. Recently, CapsuleNets have improved the generalization ability, as compared to DNNs, due to their multi-dimensional capsules. However, they pose high computational and memory requirements, which makes energy-efficient inference a challenging task. In this paper, we perform an extensive analysis to demonstrate their key limitations due to intense memory accesses and large on-chip memory requirements. To enable efficient CaspuleNet inference accelerators, we propose a specialized on-chip memory hierarchy which minimizes the off-chip memory accesses, while efficiently feeding the data to the accelerator. We analyze the on-chip memory requirements for each memory component of the architecture. By leveraging this analysis, we propose a methodology to explore different on-chip memory designs and a power-gating technique to further reduce the energy consumption, depending upon the utilization across different operations of a CapsuleNet. Our memory designs can significantly reduce the energy consumption of the on-chip memory by up to 86%, when compared to a state-of-the-art memory design. Since the power consumption of the memory elements is the major contributor in the power breakdown of the CapsuleNet accelerator, as we will also show in our analyses, the proposed memory design can effectively reduce the overall energy consumption of the complete CapsuleNet accelerator architecture.", "venue": "ArXiv", "authors": ["Alberto  Marchisio", "Muhammad  Shafique"], "year": 2019, "n_citations": 5}
{"id": 1883753, "s2_id": "a3a10727a34480eab2d4e9c846fd034c78e90ae0", "title": "ODIN: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-Situ Neural Network Processing in Phase Change RAM", "abstract": "Due to the very rapidly growing use of Artificial Neural Networks (ANNs) in real-world applications related to machine learning and Artificial Intelligence (AI), several hardware accelerator designs for ANNs have been proposed recently. In this paper, we present a novel processing-in-memory (PIM) engine called ODIN that employs hybrid binary-stochastic bit-parallel arithmetic inside phase change RAM (PCRAM) to enable a low-overhead in-situ acceleration of all essential ANN functions such as multiply-accumulate (MAC), nonlinear activation, and pooling. We mapped four ANN benchmark applications on ODIN to compare its performance with a conventional processor-centric design and a crossbar-based in-situ ANN accelerator from prior work. The results of our analysis for the considered ANN topologies indicate that our ODIN accelerator can be at least 5.8\u00d7 faster and 23.2\u00d7 more energy-efficient, and up to 90.8\u00d7 faster and 1554\u00d7 more energy-efficient, compared to the crossbar-based in-situ ANN accelerator from prior work.", "venue": "ArXiv", "authors": ["Supreeth Mysore Shivanandamurthy", "Ishan. G. Thakkar", "Sayed Ahmad Salehi"], "year": 2021, "n_citations": 0}
{"id": 1885940, "s2_id": "150d19647466359e4ba03859b57cdf79cb89561d", "title": "LILLIPUT: A Lightweight Low-Latency Lookup-Table Based Decoder for Near-term Quantum Error Correction", "abstract": "The error rates of quantum devices are orders of magnitude higher than what is needed to run most quantum applications. To close this gap, Quantum Error Correction (QEC) encodes logical qubits and distributes information using several physical qubits. By periodically executing a syndrome extraction circuit on the logical qubits, information about errors (called syndrome) is extracted while running programs. A decoder uses these syndromes to identify and correct errors in real time, which is required to use feedback implemented in quantum algorithms [32]. Unfortunately, software decoders are slow and hardware decoders are fast but less accurate. Thus, almost all QEC studies so far have relied on offline decoding. To enable real-time decoding in near-term QEC, we propose LILLIPUT\u2013 a Lightweight Low Latency Look-Up Table decoder. LILLIPUT consists of two parts\u2013 First, it translates syndromes into error detection events that index into a LookUp Table (LUT) whose entry provides the error information in real-time. Second, it programs the LUTs with error assignments for all possible error events by running a software decoder offline. LILLIPUT tolerates an error on any operation in the quantum hardware, including gates and measurement, and the number of tolerated errors grows with the size of the code. It needs <7% logic on off-the-shelf FPGAs that allows it to be easily integrated alongside the control and readout circuits in existing systems [9]. LILLIPUT incurs a latency of few nanoseconds and enables real-time decoding. We also propose Compressed LUTs (CLUTs) to reduce the memory needed by LILLIPUT. By exploiting the fact that not all error events are equally likely and only storing data for the most probable error events, CLUTs reduce the memory needed by up-to 107x (from 148 MB to 1.38 MB) without degrading accuracy.", "venue": "ArXiv", "authors": ["Poulami  Das", "Aditya  Locharla", "Cody  Jones"], "year": 2021, "n_citations": 1}
{"id": 1891580, "s2_id": "5fd875e5129c3976ab68e26d97b9406b65b0063f", "title": "G-GPU: A Fully-Automated Generator of GPU-like ASIC Accelerators", "abstract": "Modern Systems on Chip (SoC), almost as a rule, require accelerators for achieving energy efficiency and high performance for specific tasks that are not necessarily well suited for execution in standard processing units. Considering the broad range of applications and necessity for specialization, the design of SoCs has thus become expressively more challenging. In this paper, we put forward the concept of G-GPU, a general-purpose GPU-like accelerator that is not application-specific but still gives benefits in energy efficiency and throughput. Furthermore, we have identified an existing gap for these accelerators in ASIC, for which no known automated generation platform/tool exists. Our solution, called GPUPlanner, is an open-source generator of accelerators, from RTL to GDSII, that addresses this gap. Our analysis results show that our automatically generated G-GPU designs are remarkably efficient when compared against the popular CPU architecture RISC-V, presenting speed-ups of up to 223 times in raw performance and up to 11 times when the metric is performance derated by area. These results are achieved by executing a design space exploration of the GPU-like accelerators, where the memory hierarchy is broken in a smart fashion and the logic is pipelined on demand. Finally, tapeout-ready layouts of the G-GPU in 65nm CMOS are presented.", "venue": "ArXiv", "authors": ["Tiago Diadami Perez", "Marcio M. Gon\u00e7alves", "Jos\u00e9 Rodrigo Azambuja", "Leonardo  Gobatto", "Marcelo  Brandalero", "Samuel Nascimento Pagliarini"], "year": 2021, "n_citations": 0}
{"id": 1891791, "s2_id": "aac95d44cf39e38d8cf70c9ccc5eec032bc24c54", "title": "Accelerating Geometric Multigrid Preconditioning with Half-Precision Arithmetic on GPUs", "abstract": "With the hardware support for half-precision arithmetic on NVIDIA V100 GPUs, high-performance computing applications can benefit from lower precision at appropriate spots to speed up the overall execution time. In this paper, we investigate a mixed-precision geometric multigrid method to solve large sparse systems of equations stemming from discretization of elliptic PDEs. While the final solution is always computed with high-precision accuracy, an iterative refinement approach with multigrid preconditioning in lower precision and residuum scaling is employed. We compare the FP64 baseline for Poisson's equation to purely FP16 multigrid preconditioning and to the employment of FP16-FP32-FP64 combinations within a mesh hierarchy. While the iteration count is almost not affected by using lower accuracy, the solver runtime is considerably decreased due to the reduced memory transfer and a speedup of up to 2.5x is gained for the overall solver. We investigate the performance of selected kernels with the hierarchical Roofline model.", "venue": "ArXiv", "authors": ["Kyaw L. Oo", "Andreas  Vogel"], "year": 2020, "n_citations": 2}
{"id": 1892293, "s2_id": "c08f8314dbeaea6bad74c3b68ad0eec62f6f9f0a", "title": "Flare: flexible in-network allreduce", "abstract": "The allreduce operation is one of the most commonly used communication routines in distributed applications. To improve its bandwidth and to reduce network traffic, this operation can be accelerated by offloading it to network switches, that aggregate the data received from the hosts, and send them back the aggregated result. However, existing solutions provide limited customization opportunities and might provide suboptimal performance when dealing with custom operators and data types, with sparse data, or when reproducibility of the aggregation is a concern. To deal with these problems, in this work we design a flexible programmable switch by using as a building block PsPIN, a RISC-V architecture implementing the sPIN programming model. We then design, model, and analyze different algorithms for executing the aggregation on this architecture, showing performance improvements compared to state-of-the-art approaches.", "venue": "SC", "authors": ["Daniele De Sensi", "Salvatore Di Girolamo", "Saleh  Ashkboos", "Shigang  Li", "Torsten  Hoefler"], "year": 2021, "n_citations": 2}
{"id": 1897058, "s2_id": "9ddfa92ac52f10727e83c2d64477304061e074bf", "title": "A Fast Improved Fat Tree Encoder for Wave Union TDC in an FPGA", "abstract": "Up to the present, the wave union method can achieve the best timing performance in FPGA based TDC designs. However, it should be guaranteed in such a structure that the non-thermometer code to binary code (NTH2B) encoding process should be finished within just one system clock cycle. So the implementation of the NTH2B encoder is quite challenging considering the high speed requirement. Besides, the high resolution wave union TDC also demands the encoder to convert an ultra-wide input code to a binary code. We present a fast improved fat tree encoder (IFTE) to fulfill such requirements, in which bubble error suppression is also integrated. With this encoder scheme, a wave union TDC with 7.7 ps RMS and 3.8 ps effective bin size was implemented in an FPGA from Xilinx Virtex 5 family. An encoding time of 8.33 ns was achieved for a 276-bit non-thermometer code to a 9-bit binary code conversion. We conducted a series of tests on the oscillating period of the wave union launcher, as well as the overall performance of the TDC; test results indicate that the IFTE works well. In fact, in the implementation of this encoder, no manual routing or special constrains were required; therefore, this IFTE structure could also be further applied in other delay chain based FPGA TDCs.", "venue": "ArXiv", "authors": ["Qi  Shen", "Lei  Zhao", "Shubin  Liu", "Shengkai  Liao", "Binxiang  Qi", "Xueye  Hu", "Chengzhi  Peng", "Qi  An"], "year": 2013, "n_citations": 4}
{"id": 1897557, "s2_id": "4a8e3f9774f4e506fccdfb7e312d1c6d512b3b08", "title": "New trends in parallel and distributed simulation: From many-cores to Cloud Computing", "abstract": "Abstract Recent advances in computing architectures and networking are bringing parallel computing systems to the masses so increasing the number of potential users of these kinds of systems. In particular, two important technological evolutions are happening at the ends of the computing spectrum: at the \u201csmall\u201d scale, processors now include an increasing number of independent execution units (cores), at the point that a mere CPU can be considered a parallel shared-memory computer; at the \u201clarge\u201d scale, the Cloud Computing paradigm allows applications to scale by offering resources from a large pool on a pay-as-you-go model. Multi-core processors and Clouds both require applications to be suitably modified to take advantage of the features they provide. Despite laying at the extreme of the computing architecture spectrum \u2013 multi-core processors being at the small scale, and Clouds being at the large scale \u2013 they share an important common trait: both are specific forms of parallel/distributed architectures. As such, they present to the developers well known problems of synchronization, communication, workload distribution, and so on. Is parallel and distributed simulation ready for these challenges? In this paper, we analyze the state of the art of parallel and distributed simulation techniques, and assess their applicability to multi-core architectures or Clouds. It turns out that most of the current approaches exhibit limitations in terms of usability and adaptivity which may hinder their application to these new computing architectures. We propose an adaptive simulation mechanism, based on the multi-agent system paradigm, to partially address some of those limitations. While it is unlikely that a single approach will work well on both settings above, we argue that the proposed adaptive mechanism has useful features which make it attractive both in a multi-core processor and in a Cloud system. These features include the ability to reduce communication costs by migrating simulation components, and the support for adding (or removing) nodes to the execution architecture at runtime. We will also show that, with the help of an additional support layer, parallel and distributed simulations can be executed on top of unreliable resources.", "venue": "Simul. Model. Pract. Theory", "authors": ["Gabriele  D'Angelo", "Moreno  Marzolla"], "year": 2014, "n_citations": 61}
{"id": 1900051, "s2_id": "9f2ecd50d9a92e469321decba7bab852635af2c4", "title": "Optimal Scheduling for Exposed Datapath Architectures with Buffered Processing Units by ASP", "abstract": "Abstract Conventional processor architectures are restricted in exploiting instruction level parallelism (ILP) due to the relatively low number of programmer-visible registers. Therefore, more recent processor architectures expose their datapaths so that the compiler (1) can schedule parallel instructions to different processing units and (2) can make effective use of local storage of the processing units. Among these architectures, the Synchronous Control Asynchronous Dataflow (SCAD) architecture is a new exposed datapath architecture whose processing units are equipped with first-in first-out (FIFO) buffers at their input and output ports. In contrast to register-based machines, the optimal code generation for SCAD is still a matter of research. In particular, SAT and SMT solvers were used to generate optimal resource constrained and optimal time constrained schedules for SCAD, respectively. As Answer Set Programming (ASP) offers better flexibility in handling such scheduling problems, we focus in this paper on using an answer set solver for both resource and time constrained optimal SCAD code generation. As a major benefit of using ASP, we are able to generate all optimal schedules for a given program which allows one to study their properties. Furthermore, the experimental results of this paper demonstrate that the answer set solver can compete with SAT solvers and outperforms SMT solvers. This paper is under consideration for acceptance in TPLP.", "venue": "Theory and Practice of Logic Programming", "authors": ["MARC  DAHLEM", "ANOOP  BHAGYANATH", "KLAUS  SCHNEIDER"], "year": 2018, "n_citations": 1}
{"id": 1900188, "s2_id": "b1522be73436ebe406156e151c81861636dc6c4e", "title": "Synchronization processor synthesis for latency insensitive systems", "abstract": "In this paper we present our contribution in terms of synchronization processor for a SoC design methodology based on the theory of the latency insensitive systems (LIS) of Carloni et al. (2001). Our contribution consists in IP encapsulation into a new wrapper model whose speed and area are optimized and synthetizability guaranteed. The main benefit of our approach is to preserve the local IP performance when encapsulating them and reduce SoC silicon area.", "venue": "Design, Automation and Test in Europe", "authors": ["Pierre  Bomel", "Eric  Martin", "Emmanuel  Boutillon"], "year": 2005, "n_citations": 21}
{"id": 1902099, "s2_id": "5fae280d9f4ff2605eaca97fb873761e989cddef", "title": "High-Throughput VLSI Architecture for GRAND", "abstract": "Guessing Random Additive Noise Decoding (GRAND) is a recently proposed universal decoding algorithm for linear error correcting codes. Since GRAND does not depend on the structure of the code, it can be used for any code encountered in contemporary communication standards or may even be used for random linear network coding. This property makes this new algorithm particularly appealing. Instead of trying to decode the received vector, GRAND attempts to identify the noise that corrupted the codeword. To that end, GRAND relies on the generation of test error patterns that are successively applied to the received vector. In this paper, we propose the first hardware architecture for the GRAND algorithm. Considering GRAND with ABandonment (GRANDAB) that limits the number of test patterns, the proposed architecture only needs $2+{\\sum}_{i=2}^n \\ \\left\\lfloor\\frac{i}{2} \\right\\rfloor$ time steps to perform the ${\\sum}_{i=1}^3 \\ \\binom{n}{i}$ queries required when AB = 3. For a code length of 128, our proposed hardware architecture demonstrates only a fraction (1.2%) of the total number of performed queries as time steps. Synthesis result using TSMC 65nm CMOS technology shows that average throughputs of 32 Gbps to 64 Gbps can be achieved at an SNR of 10 dB for a code length of 128 and code rates rate higher than 0.75, transmitted over an AWGN channel. Comparisons with a decoder tailored for a (79, 64) BCH code show that the proposed architecture can achieve a slightly higher average throughput at high SNRs, while obtaining the same decoding performance.", "venue": "2020 IEEE Workshop on Signal Processing Systems (SiPS)", "authors": ["Syed Mohsin Abbas", "Thibaud  Tonnellier", "Furkan  Ercan", "Warren J. Gross"], "year": 2020, "n_citations": 9}
{"id": 1904164, "s2_id": "f6a32eb3c31f1b85bdb12320a25e4a0ce8bef31c", "title": "The Preliminary Evaluation of a Hypervisor-based Virtualization Mechanism for Intel Optane DC Persistent Memory Module", "abstract": "Non-volatile memory (NVM) technologies, being accessible in the same manner as DRAM, are considered indispensable for expanding main memory capacities. Intel Optane DCPMM is a long-awaited product that drastically increases main memory capacities. However, a substantial performance gap exists between DRAM and DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and 407% higher than those of DRAM, respectively. The read/write bandwidths were 37% and 8% of those of DRAM. This performance gap in main memory presents a new challenge to researchers; we need a new system software technology supporting emerging hybrid memory architecture. In this paper, we present RAMinate, a hypervisor-based virtualization mechanism for hybrid memory systems, and a key technology to address the performance gap in main memory systems. It provides great flexibility in memory management and maximizes the performance of virtual machines (VMs) by dynamically optimizing memory mappings. Through experiments, we confirmed that even though a VM has only 1% of DRAM in its RAM, the performance degradation of the VM was drastically alleviated by memory mapping optimization. The elapsed time to finish the build of Linux Kernel in the VM was 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495 seconds). When the optimization mechanism was disabled, the elapsed time increased to 624 seconds (i.e. 26% increase from the 100% DRAM case).", "venue": "ArXiv", "authors": ["Takahiro  Hirofuchi", "Ryousei  Takano"], "year": 2019, "n_citations": 12}
{"id": 1917901, "s2_id": "3cc6c2db24a2a14560f151867f2ac0b681ec3238", "title": "Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on Embedded FPGAs", "abstract": "Using FPGAs to accelerate ConvNets has attracted significant attention in recent years. However, FPGA accelerator design has not leveraged the latest progress of ConvNets. As a result, the key application characteristics such as frames-per-second (FPS) are ignored in favor of simply counting GOPs, and results on accuracy, which is critical to application success, are often not even reported. In this work, we adopt an algorithm-hardware co-design approach to develop a ConvNet accelerator called Synetgy and a novel ConvNet model called DiracDeltaNet. Both the accelerator and ConvNet are tailored to FPGA requirements. DiracDeltaNet, as the name suggests, is a ConvNet with only $1\\times 1$ convolutions while spatial convolutions are replaced by more efficient shift operations. DiracDeltaNet achieves competitive accuracy on ImageNet (89.0% top-5), but with 48\u00d7 fewer parameters and 65\u00d7 fewer OPs than VGG16. We further quantize DiracDeltaNet's weights to 1-bit and activations to 4-bits, with less than 1% accuracy loss. These quantizations exploit well the nature of FPGA hardware. In short, DiracDeltaNet's small model size, low computational OP count, ultra-low precision and simplified operators allow us to co-design a highly customized computing unit for an FPGA. We implement the computing units for DiracDeltaNet on an Ultra96 SoC system through high-level synthesis. Our accelerator's final top-5 accuracy of 88.2% on ImageNet, is higher than all the previously reported embedded FPGA accelerators. In addition, the accelerator reaches an inference speed of 96.5 FPS on the ImageNet classification task, surpassing prior works with similar accuracy by at least 16.9\u00d7.", "venue": "FPGA", "authors": ["Yifan  Yang", "Qijing  Huang", "Bichen  Wu", "Tianjun  Zhang", "Liang  Ma", "Giulio  Gambardella", "Michaela  Blott", "Luciano  Lavagno", "Kees A. Vissers", "John  Wawrzynek", "Kurt  Keutzer"], "year": 2019, "n_citations": 57}
{"id": 1920007, "s2_id": "5a4c5c0dd49c4dd12a07141a77d005cf00b331cc", "title": "A VLSI design flow for secure side-channel attack resistant ICs", "abstract": "The paper presents a digital VLSI design flow to create secure, side-channel attack (SCA) resistant integrated circuits. The design flow starts from a normal design in a hardware description language, such as VHDL or Verilog, and provides a direct path to an SCA resistant layout. Instead of a full custom layout or an iterative design process with extensive simulations, a few key modifications are incorporated in a regular synchronous CMOS standard cell design flow. We discuss the basis for side-channel attack resistance and adjust the library databases and constraints files of the synthesis and place-and-route procedures accordingly. Experimental results show that a DPA (differential power analysis) attack on a regular single ended CMOS standard cell implementation of a module of the DES algorithm discloses the secret key after 200 measurements. The same attack on a secure version still does not disclose the secret key after more than 2000 measurements.", "venue": "Design, Automation and Test in Europe", "authors": ["Kris  Tiri", "Ingrid  Verbauwhede"], "year": 2005, "n_citations": 102}
{"id": 1920106, "s2_id": "5b252fb4359c558b82790b085fe3d4c7dfbefe4e", "title": "A Cache-Coloring Based Technique for Saving Leakage Energy In Multitasking Systems", "abstract": "There has been a significant increase in leakage energy dissipation of CMOS circuits with each technology generation. Further, due to their large size, last level caches (LLCs) spend a large fraction of their energy in the form of leakage energy and hence, addressing this has become extremely important to meet the challenges of chip power budget. For addressing this, several techniques have been proposed. However, most of these techniques require offline profiling and hence cannot be used for real-life systems which usually run multitasking programs, with possible pre-emptions. In this paper, we propose a dynamic profiling based technique for saving cache leakage energy in multitasking systems. Our technique uses a small coloring-based profiling cache, to estimate performance and energy consumption of multiple cache configurations and then selects the best (least-energy) configuration among them. Our technique uses non-intrusive profiling and saves energy despite intra-task and inter-task variations; thus, it is suitable for multitasking systems. Simulations performed using workloads from SPEC2006 suite show the superiority of our technique over an existing cache energy saving technique. With a 2MB baseline cache, the average saving in memory sub-system energy is 22.8%.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 2}
{"id": 1922668, "s2_id": "fa64b359d2c9dee2ed42e5e03531718ec504cfdf", "title": "An IoT Endpoint System-on-Chip for Secure and Energy-Efficient Near-Sensor Analytics", "abstract": "Near-sensor data analytics is a promising direction for internet-of-things endpoints, as it minimizes energy spent on communication and reduces network load - but it also poses security concerns, as valuable data are stored or sent over the network at various stages of the analytics pipeline. Using encryption to protect sensitive data at the boundary of the on-chip analytics engine is a way to address data security issues. To cope with the combined workload of analytics and encryption in a tight power envelope, we propose Fulmine, a system-on-chip (SoC) based on a tightly-coupled multi-core cluster augmented with specialized blocks for compute-intensive data processing and encryption functions, supporting software programmability for regular computing tasks. The Fulmine SoC, fabricated in 65-nm technology, consumes less than 20mW on average at 0.8V achieving an efficiency of up to 70pJ/B in encryption, 50pJ/px in convolution, or up to 25MIPS/mW in software. As a strong argument for real-life flexible application of our platform, we show experimental results for three secure analytics use cases: secure autonomous aerial surveillance with a state-of-the-art deep convolutional neural network (CNN) consuming 3.16pJ per equivalent reduced instruction set computer operation, local CNN-based face detection with secured remote recognition in 5.74pJ/op, and seizure detection with encrypted data collection from electroencephalogram within 12.7pJ/op.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Francesco  Conti", "Robert  Schilling", "Pasquale Davide Schiavone", "Antonio  Pullini", "Davide  Rossi", "Frank Ka\u011fan G\u00fcrkaynak", "Michael  Muehlberghuber", "Michael  Gautschi", "Igor  Loi", "Germain  Haugou", "Stefan  Mangard", "Luca  Benini"], "year": 2017, "n_citations": 87}
{"id": 1926566, "s2_id": "08c977c75acdeb7509c6aed1d55cfcc0b29729a5", "title": "HIR: An MLIR-based Intermediate Representation for Hardware Accelerator Description", "abstract": "The emergence of machine learning, image and audio processing on edge devices has motivated research towards power efficient custom hardware accelerators. Though FPGAs are an ideal target for energy efficient custom accelerators, the difficulty of hardware design and the lack of vendor agnostic, standardized hardware compilation infrastructure has hindered their adoption. This paper introduces HIR, an MLIR-based intermediate representation (IR) to describe hardware accelerator designs. HIR combines high level language features, such as loops and multidimensional tensors, with programmer defined explicit scheduling, to provide a high-level IR suitable for DSL compiler pipelines without compromising control over the micro-architecture of the accelerator. HIR\u2019s explicit schedules allow it to express fine-grained, synchronization-free parallelism and optimizations such as retiming and pipelining. Built as a dialect in MLIR, it draws from best IR practices learnt from communities like those of LLVM. While offering rich optimization opportunities and a high level abstraction, HIR enables sharing of optimizations, utilities and passes with software compiler infrastructure. Our implementation shows that the code generation time of the HIR code generator is on average 1112\u00d7 lower than that of Xilinx Vivado HLS on a range of kernels without a compromise on the quality of the generated hardware.We believe that these are significant steps forward in the design of IRs for hardware synthesis and in equipping domain-specific languages with a productive and performing compilation path to custom hardware acceleration.", "venue": "ArXiv", "authors": ["Kingshuk  Majumder", "Uday  Bondhugula"], "year": 2021, "n_citations": 1}
{"id": 1928488, "s2_id": "d0d96aed2fe01c64ae21966530d5693d822eef7f", "title": "NB-FEB: An Easy-to-Use and Scalable Universal Synchronization Primitive for Parallel Programming", "abstract": "This paper addresses the problem of universal synchronization \nprimitives that can support scalable thread synchronization \nfor large-scale many-core architectures. The universal \nsynchronization primitives that have been deployed widely \nin conventional architectures, are the compare-and-swap (CAS) \nand load-linked/store-conditional (LL/SC) primitives. However, \nsuch synchronization primitives are expected to reach \ntheir scalability limits in the evolution to many-core architectures \nwith thousands of cores. \nWe introduce a non-blocking full/empty bit primitive, or \nNB-FEB for short, as a promising synchronization primitive \nfor parallel programming on may-core architectures. We show \nthat the NB-FEB primitive is universal, scalable, feasible and \nconvenient to use. NB-FEB, together with registers, can solve \nthe consensus problem for an arbitrary number of processes \n(universality). NB-FEB is combinable, namely its memory requests \nto the same memory location can be combined into \nonly one memory request, which consequently mitigates performance \ndegradation due to synchronization \"hot spots\" (scalability). \nSince NB-FEB is a variant of the original full/empty \nbit that always returns a value instead of waiting for a conditional \nflag, it is as feasible as the original full/empty bit, which \nhas been implemented in many computer systems (feasibility). \nThe original full/empty bit is well-known as a special-purpose \nprimitive for fast producer-consumer synchronization and has \nbeen used extensively in the specific domain of applications. \nIn this paper, we show that NB-FEB can be deployed easily \nas a general-purpose primitive. Using NB-FEB, we construct \na non-blocking software transactional memory system \ncalled NBFEB-STM, which can be used to handle concurrent \nthreads conveniently. NBFEB-STM is space efficient: \nthe space complexity of each object updated by N concurrent \nthreads/transactions is \u0398(N), the optimal.", "venue": "ArXiv", "authors": ["Phuong Hoai Ha", "Philippas  Tsigas", "Otto J. Anshus"], "year": 2008, "n_citations": 1}
{"id": 1930298, "s2_id": "4005bedc25bf8cd5bc40b95c9cac59f2efca0e89", "title": "System Reliability, Fault Tolerance and Design Metrics Tradeoffs in the Distributed Minority and Majority Voting Based Redundancy Scheme", "abstract": "The distributed minority and majority voting based redundancy (DMMR) scheme was recently proposed as an efficient alternative to the conventional N-modular redundancy (NMR) scheme for the physical design of mission/safety-critical circuits and systems. The DMMR scheme enables significant improvements in fault tolerance and design metrics compared to the NMR scheme albeit at the expense of a slight decrease in the system reliability. In this context, this paper studies the system reliability, fault tolerance and design metrics tradeoffs in the DMMR scheme compared to the NMR scheme when the majority logic group of the DMMR scheme is increased in size relative to the minority logic group. Some example DMMR and NMR systems were realized using a 32/28nm CMOS process and compared. The results show that 5-of-M DMMR systems have a similar or better fault tolerance whilst requiring similar or fewer function modules than their counterpart NMR systems and simultaneously achieve optimizations in design metrics. Nevertheless, 3-of-M DMMR systems have the upper hand with respect to fault tolerance and design metrics optimizations than the comparable NMR and 5-of-M DMMR systems. With regard to system reliability, NMR systems are closely followed by 5-of-M DMMR systems which are closely followed by 3-of-M DMMR systems. The verdict is 3-of-M DMMR systems are preferable to implement higher levels of redundancy from a combined system reliability, fault tolerance and design metrics perspective to realize mission/safety-critical circuits and systems.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 3}
{"id": 1930513, "s2_id": "f2dd47b0e1a6544101ae4eff45191873de966c41", "title": "BrainScaleS Large Scale Spike Communication using Extoll", "abstract": "The BrainScaleS Neuromorphic Computing System is currently connected to a compute cluster via Gigabit-Ethernet network technology. This is convenient for the currently used experiment mode, where neuronal networks cover at most one wafer module. When modelling networks of larger size, as for example a full sized cortical microcircuit model, one has to think about connecting neurons across wafer modules to larger networks. This can be done, using the Extoll networking technology, which provides high bandwidth and low latencies, as well as a low overhead packet protocol format.", "venue": "ArXiv", "authors": ["Tobias  Thommes", "Niels  Buwen", "Andreas  Gr\u00fcbl", "Eric  M\u00fcller", "Ulrich  Br\u00fcning", "Johannes  Schemmel"], "year": 2021, "n_citations": 0}
{"id": 1931206, "s2_id": "a4885bb7919be492d5cc0c864093a500840d071d", "title": "Comparing ternary and binary adders and multipliers", "abstract": "While many papers have proposed implementations of ternary adders and ternary multipliers, no comparisons have generally been done with the corresponding binary ones. We compare the implementations of binary and ternary adders and multipliers with the same computing capability according to the basic blocks that are 1-bit and 1-trit adders and 1-bit and 1-trit multipliers. Then we compare the complexity of these basic blocks by using the same CNTFET technology to evaluate the overall complexity of N-bit adders and M-trit adders on one side, and NxN bit multipliers and MxM trits multipliers with M = N/IR (IR = log(3)/log(2) is the information ratio). While ternary adders and multipliers have less input and output connections and use less basic building blocks, the complexity of the ternary building blocks is too high and the ternary adders and multipliers cannot compete with the binary ones.", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2019, "n_citations": 3}
{"id": 1931601, "s2_id": "e8d09103e8ca001cbee0079de220410941845ae7", "title": "Maximum Power Point Tracking Circuit for an Energy Harvester in 130 nm CMOS Technology", "abstract": "This paper presents design of a Maximum Power Point Tracking (MPPT) circuit and its functionality for tuning the maximum power transfer from an energy harvester (EH) unit. Simple and practical \u201cPerturb and Observe\u201d (P&O) algorithm is investigated and implemented. We describe the circuit functionality and the improvements that have been introduced to the original algorithm. The proposed MPPT design is divided into three main blocks. The output signal is being generated by the PWM or PFM block. The tracking speed has been enhanced by implementing a variable step size in the \u201cTracking Block\u201d. Finally, the overall power consumption of the MPPT circuit itself is controlled by the \u201cPower Management Block\u201d, which manages delivering the clock signal to the rest of the circuit. The RTL code of the proposed MPPT has been created in Verilog, synthesized and placed-and-routed in a general purpose 130 nm CMOS technology.", "venue": "2020 International Conference on Applied Electronics (AE)", "authors": ["Adam  Hudec", "Luk\u00e1s  Nagy", "Martin  Kov\u00e1c", "Viera  Stopjakov\u00e1"], "year": 2020, "n_citations": 0}
{"id": 1931611, "s2_id": "8d2f143a72b096797c3e063943fc14d5d3c8d351", "title": "Performance monitoring for multicore embedded computing systems on FPGAs", "abstract": "When designing modern embedded computing systems, most software programmers choose to use multicore processors, possibly in combination with general-purpose graphics processing units (GPGPUs) and/or hardware accelerators. They also often use an embedded Linux O/S and run multi-application workloads that may even be multi-threaded. Modern FPGAs are large enough to combine multicore hard/soft processors with multiple hardware accelerators as custom compute units, enabling entire embedded compute systems to be implemented on a single FPGA. Furthermore, the large FPGA vendors also support embedded Linux kernels for both their soft and embedded processors. When combined with high-level synthesis to generate hardware accelerators using a C-to-gates flows, the necessary primitives for a framework that can enable software designers to use FPGAs as their custom compute platform now exist. However, in order to ensure that computing resources are integrated and shared effectively, software developers need to be able to monitor and debug the runtime performance of the applications in their workload. This paper describes ABACUS, a performance-monitoring framework that can be used to debug the execution behaviours and interactions of multi-application workloads on multicore systems. We also discuss how this framework is extensible for use with hardware accelerators in heterogeneous systems.", "venue": "ArXiv", "authors": ["Lesley  Shannon", "Eric  Matthews", "Nicholas C. Doyle", "Alexandra  Fedorova"], "year": 2015, "n_citations": 6}
{"id": 1932587, "s2_id": "b3a8dfd5b6fed6cd828a17a4cd0f1ca5b317f96b", "title": "DAG-based Scheduling with Resource Sharing for Multi-task Applications in a Polyglot GPU Runtime", "abstract": "GPUs are readily available in cloud computing and personal devices, but their use for data processing acceleration has been slowed down by their limited integration with common programming languages such as Python or Java. Moreover, using GPUs to their full capabilities requires expert knowledge of asynchronous programming. In this work, we present a novel GPU run time scheduler for multi-task GPU computations that transparently provides asynchronous execution, space-sharing, and transfer-computation overlap without requiring in advance any information about the program dependency structure. We leverage the GrCUDA polyglot API to integrate our scheduler with multiple high-level languages and provide a platform for fast prototyping and easy GPU acceleration. We validate our work on 6 benchmarks created to evaluate task-parallelism and show an average of 44% speedup against synchronous execution, with no execution time slowdown compared to hand-optimized host code written using the C++ CUDA Graphs API.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Alberto  Parravicini", "Arnaud  Delamare", "Marco  Arnaboldi", "Marco D. Santambrogio"], "year": 2021, "n_citations": 2}
{"id": 1935073, "s2_id": "7e5b4128bf26b432619b56d9a7b0b6eb28bb85a1", "title": "A Configurable BNN ASIC using a Network of Programmable Threshold Logic Standard Cells", "abstract": "This paper presents Tulip, a new architecture for a binary neural network (BNN) that uses an optimal schedule for executing the operations of an arbitrary BNN. It was constructed with the goal of maximizing energy efficiency per classification. At the top-level, Tulip consists of a collection of unique processing elements (TULIP-PEs) that are organized in a SIMD fashion. Each Tulip- Peconsists of a small network of binary neurons, and a small amount of local memory per neuron. The unique aspect of the binary neuron is that it is implemented as a mixed-signal circuit that natively performs the inner-product and thresholding operation of an artificial binary neuron. Moreover, the binary neuron, which is implemented as a single CMOS standard cell, is reconfigurable, and with a change in a single parameter, can implement all standard operations involved in a BNN. We present novel algorithms for mapping arbitrary nodes of a BNN onto the TULIP-PEs. Tulip was implemented as an ASIC in TSMC 40nm-LP technology. To provide a fair comparison, a recently reported BNN that employs a conventional MAC-based arithmetic processor was also implemented in the same technology. The results show that Tulip is consistently 3X more energy-efficient than the conventional design, without any penalty in performance, area, or accuracy.", "venue": "2020 IEEE 38th International Conference on Computer Design (ICCD)", "authors": ["Ankit  Wagle", "Sunil  Khatri", "Sarma B. K. Vrudhula"], "year": 2020, "n_citations": 1}
{"id": 1939043, "s2_id": "828947e3e9e06c506e6a30e3eb7e176c0b8b953d", "title": "Trustworthy AI Inference Systems: An Industry Research View", "abstract": "In this work, we provide an industry research view for approaching the design, deployment, and operation of trustworthy Artificial Intelligence (AI) inference systems. Such systems provide customers with timely, informed, and customized inferences to aid their decision, while at the same time utilizing appropriate security protection mechanisms for AI models. Additionally, such systems should also use Privacy-Enhancing Technologies (PETs) to protect customers' data at any time. \nTo approach the subject, we start by introducing trends in AI inference systems. We continue by elaborating on the relationship between Intellectual Property (IP) and private data protection in such systems. Regarding the protection mechanisms, we survey the security and privacy building blocks instrumental in designing, building, deploying, and operating private AI inference systems. For example, we highlight opportunities and challenges in AI systems using trusted execution environments combined with more recent advances in cryptographic techniques to protect data in use. Finally, we outline areas of further development that require the global collective attention of industry, academia, and government researchers to sustain the operation of trustworthy AI inference systems.", "venue": "ArXiv", "authors": ["Rosario  Cammarota", "Matthias  Schunter", "Anand  Rajan", "Fabian  Boemer", "'Agnes  Kiss", "Amos  Treiber", "Christian  Weinert", "Thomas  Schneider", "Emmanuel  Stapf", "Ahmad-Reza  Sadeghi", "Daniel  Demmler", "Huili  Chen", "Siam Umar Hussain", "Sadegh  Riazi", "Farinaz  Koushanfar", "Saransh  Gupta", "Tajan Simunic Rosing", "Kamalika  Chaudhuri", "Hamid  Nejatollahi", "Nikil  Dutt", "Mohsen  Imani", "Kim  Laine", "Anuj  Dubey", "Aydin  Aysu", "Fateme Sadat Hosseini", "Chengmo  Yang", "Eric  Wallace", "Pamela  Norton"], "year": 2020, "n_citations": 2}
{"id": 1943273, "s2_id": "788b9e288c8db9decbbb2668fdee3737e386e143", "title": "The Secure Machine: Efficient Secure Execution On Untrusted Platforms", "abstract": "In this work we present the Secure Machine, SeM for short, a CPU architecture extension for secure computing. SeM uses a small amount of in-chip additional hardware that monitors key communication channels inside the CPU chip, and only acts when required. SeM provides confidentiality and integrity for a secure program without trusting the platform software or any off-chip hardware. SeM supports existing binaries of single- and multi-threaded applications running on single- or multi-core, multi-CPU. The performance reduction caused by it is only few percent, most of which is due to the memory encryption layer that is commonly used in many secure architectures. \nWe also developed SeM-Prepare, a software tool that automatically instruments existing applications (binaries) with additional instructions so they can be securely executed on our architecture without requiring any programming efforts or the availability of the desired program`s source code. \nTo enable secure data sharing in shared memory environments, we developed Secure Distributed Shared Memory (SDSM), an efficient (time and memory) algorithm for allowing thousands of compute nodes to share data securely while running on an untrusted computing environment. SDSM shows a negligible reduction in performance, and it requires negligible and hardware resources. We developed Distributed Memory Integrity Trees, a method for enhancing single node integrity trees for preserving the integrity of a distributed application running on an untrusted computing environment. We show that our method is applicable to existing single node integrity trees such as Merkle Tree, Bonsai Merkle Tree, and Intel`s SGX memory integrity engine. All these building blocks may be used together to form a practical secure system, and some can be used in conjunction with other secure systems.", "venue": "ArXiv", "authors": ["Ofir  Shwartz", "Yitzhak  Birk"], "year": 2018, "n_citations": 0}
{"id": 1946274, "s2_id": "6361b22bea82bb6e5b853c4075fcd1fb43c744f9", "title": "DiaSys: Improving SoC insight through on-chip diagnosis", "abstract": "To find the cause of a functional or non-functional defect (bug) in software running on a multi-processor System-on-Chip (MPSoC), developers need insight into the chip. Tracing systems provide this insight non-intrusively, at the cost of high off-chip bandwidth requirements. This I/O bottleneck limits the observability, a problem becoming more severe as more functionality is integrated on-chip. In this paper, we present DiaSys, an MPSoC diagnosis system with the potential to replace today's tracing systems. Its main idea is to partially execute the analysis of observation data on the chip; in consequence, more information and less data is sent to the attached host PC. With DiaSys, the data analysis is performed by the diagnosis application. Its input are events, which are generated by observation hardware at interesting points in the program execution (like a function call). Its outputs are events with higher information density. The event transformation is modeled as dataflow application. For execution, it is mapped in part to dedicated and distributed on-chip components, and in part to the host PC; the off-chip boundary is transparent to the developer of the diagnosis application. We implement DiaSys as extension to an existing SoC with four tiles and a mesh network running on an FPGA platform. Two usage examples confirm that DiaSys is flexible enough to replace a tracing system, while significantly lowering the off-chip bandwidth requirements. In our examples, the debugging of a race-condition bug, and the creation of a lock contention profile, we see a reduction of trace bandwidth of more than three orders of magnitude, compared to a full trace created by a common tracing system.", "venue": "J. Syst. Archit.", "authors": ["Philipp  Wagner", "Thomas  Wild", "Andreas  Herkersdorf"], "year": 2017, "n_citations": 3}
{"id": 1948603, "s2_id": "d68b5beb488f13bae0af4dbf52acda0594128a03", "title": "ZnG: Architecting GPU Multi-Processors with New Flash for Scalable Data Analysis", "abstract": "We propose ZnG, a new GPU-SSD integrated architecture, which can maximize the memory capacity in a GPU and address performance penalties imposed by an SSD. Specifically, ZnG replaces all GPU internal DRAMs with an ultra-low-latency SSD to maximize the GPU memory capacity. ZnG further removes performance bottleneck of the SSD by replacing its flash channels with a high-throughput flash network and integrating SSD firmware in the GPU\u2019s MMU to reap the benefits of hardware accelerations. Although flash arrays within the SSD can deliver high accumulated bandwidth, only a small fraction of such bandwidth can be utilized by GPU\u2019s memory requests due to mismatches of their access granularity. To address this, ZnG employs a large L2 cache and flash registers to buffer the memory requests. Our evaluation results indicate that ZnG can achieve $7.5\\times$ higher performance than prior work.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Jie  Zhang", "Myoungsoo  Jung"], "year": 2020, "n_citations": 4}
{"id": 1948779, "s2_id": "7d74b8ee36620fab91c7a0e687467c6e4d33a01a", "title": "Redundant logic insertion and fault tolerance improvement in combinational circuits", "abstract": "This paper presents a novel method to identify and insert redundant logic into a combinational circuit to improve its fault tolerance without having to replicate the entire circuit as is the case with conventional redundancy techniques. In this context, it is discussed how to estimate the fault masking capability of a combinational circuit using the truth-cum-fault enumeration table, and then it is shown how to identify the logic that can introduced to add redundancy into the original circuit without affecting its native functionality and with the aim of improving its fault tolerance though this would involve some trade-off in the design metrics. However, care should be taken while introducing redundant logic since redundant logic insertion may give rise to new internal nodes and faults on those may impact the fault tolerance of the resulting circuit. The combinational circuit that is considered and its redundant counterparts are all implemented in semi-custom design style using a 32/28nm CMOS digital cell library and their respective design metrics and fault tolerances are compared.", "venue": "2017 International Conference on Circuits, System and Simulation (ICCSS)", "authors": ["P.  Balasubramanian", "R. T. Naayagi"], "year": 2017, "n_citations": 6}
{"id": 1949841, "s2_id": "7086772257d2e6c2c0455dbb7029d908057623ee", "title": "Optimal Final Carry Propagate Adder Design for Parallel Multipliers", "abstract": "Based on the ASIC layout level simulation of 7 types of adder structures each of four different sizes, i.e. a total of 28 adders, we propose expressions for the width of each of the three regions of the final Carry Propagate Adder (CPA) to be used in parallel multipliers. We also propose the types of adders to be used in each region that would lead to the optimal performance of the hybrid final adders in parallel multipliers. This work evaluates the complete performance of the analyzed designs in terms of delay, area, power through custom design and layout in 0.18 um CMOS process technology.", "venue": "ArXiv", "authors": ["B.  Ramkumar", "Harish M. Kittur"], "year": 2011, "n_citations": 2}
{"id": 1951630, "s2_id": "db47cafd579d2936ba03fcb6a43c68d3f6cdd16f", "title": "FPGA implementation of a reconfigurable Viterbi decoder for WiMAX receiver", "abstract": "Field Programmable Gate Array technology (FPGA) is a highly configurable option for implementing many sophisticated signal processing tasks in Software Defined Radios (SDRs). Those types of radios are realized using highly configurable hardware platforms. Convolutional codes are used in every robust digital communication system and Viterbi algorithm is employed in wireless communications to decode the convolutional codes. Such decoders are complex and dissipate large amount of power. In this paper, a low power-reconfigurable Viterbi decoder for WiMAX receiver is described using a VHDL code for FPGA implementation. The proposed design is implemented on Xilinx Virtex-II Pro, XC2vpx30 FPGA using the FPGA Advantage Pro package provided by Mentor Graphics and ISE 10.1 by Xilinx.", "venue": "2009 International Conference on Microelectronics - ICM", "authors": ["Sherif Welsen Shaker", "Salwa H. El-Ramly", "Khaled Ali Shehata"], "year": 2009, "n_citations": 27}
{"id": 1954345, "s2_id": "3c4a541269a404250b0ce5b710ab98025f8fe82d", "title": "A Model Study of an All-Digital, Discrete-Time and Embedded Linear Regulator", "abstract": "With an increasing number of power-states, finer- grained power management and larger dynamic ranges of digital circuits, the integration of compact, scalable linear-regulators embedded deep within logic blocks has become important. While analog linear-regulators have traditionally been used in digital ICs, the need for digitally implementable designs that can be synthesized and embedded in digital functional units for ultra fine- grained power management has emerged. This paper presents the circuit design and control models of an all-digital, discrete-time linear regulator and explores the parametric design space for transient response time and loop stability.", "venue": "ArXiv", "authors": ["Saad Bin Nasir", "Arijit  Raychowdhury"], "year": 2015, "n_citations": 6}
{"id": 1956497, "s2_id": "567c1a99a5c59736f57dec499e39bab3c7fb4acc", "title": "1-bit Massive MU-MIMO Precoding in VLSI", "abstract": "Massive multi-user (MU) multiple-input multiple-output (MIMO) will be a core technology in fifth-generation (5G) wireless systems as it offers significant improvements in spectral efficiency compared to existing multi-antenna technologies. The presence of hundreds of antenna elements at the base station (BS), however, results in excessively high hardware costs and power consumption, and requires high interconnect throughput between the baseband-processing unit and the radio unit. Massive MU-MIMO that uses low-resolution analog-to-digital and digital-to-analog converters (DACs) has the potential to address all these issues. In this paper, we focus on downlink precoding for massive MU-MIMO systems with 1-bit DACs at the BS. The objective is to design precoders that simultaneously mitigate MU interference and quantization artifacts. We propose two nonlinear 1-bit precoding algorithms and corresponding very large-scale integration (VLSI) designs. Our algorithms rely on biconvex relaxation, which enables the design of efficient 1-bit precoding algorithms that achieve superior error-rate performance compared with that of linear precoding algorithms followed by quantization. To showcase the efficacy of our algorithms, we design VLSI architectures that enable efficient 1-bit precoding for massive MU-MIMO systems, in which hundreds of antennas serve tens of user equipments. We present corresponding field-programmable gate array (FPGA) reference implementations to demonstrate that 1-bit precoding enables reliable and high-rate downlink data transmission in practical systems.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Oscar  Casta\u00f1eda", "Sven  Jacobsson", "Giuseppe  Durisi", "Mikael  Coldrey", "Tom  Goldstein", "Christoph  Studer"], "year": 2017, "n_citations": 68}
{"id": 1957037, "s2_id": "f45575ff017bc188729c35e5ef25cfb7ab5c160a", "title": "An Electro-Photonic System for Accelerating Deep Neural Networks", "abstract": "The number of parameters in deep neural networks (DNNs) is scaling at about 5\u00d7 the rate of Moore\u2019s Law. To sustain the pace of growth of the DNNs, new technologies and computing architectures are needed. Photonic computing systems are promising avenues, since they can perform the dominant general matrix-matrix multiplication (GEMM) operations in DNNs at a higher throughput than their electrical counterpart. However, purely photonic systems face several challenges including a lack of photonic memory, the need for conversion circuits, and the accumulation of noise. In this paper, we propose a hybrid electrophotonic system realizing the best of both worlds to accelerate DNNs. In contrast to prior work in photonic and electronic accelerators, we adopt a system-level perspective. Our electro-photonic system includes an electronic host processor and DRAM, and a custom electro-photonic hardware accelerator called ADEPT. The fused hardware accelerator leverages a photonic computing unit for performing highly-efficient GEMM operations and a digital electronic ASIC for storage and for performing nonGEMM operations. We also identify architectural optimization opportunities for improving the overall ADEPT\u2019s efficiency. We evaluate ADEPT using three state-of-the-art neural networks\u2014 ResNet-50, BERT-large, and RNN-T\u2014to show its general applicability in accelerating today\u2019s DNNs. A head-to-head comparison of ADEPT with systolic array architectures shows that ADEPT can provide, on average, 7.19\u00d7 higher inference throughput per watt.", "venue": "ArXiv", "authors": ["Cansu  Demirkiran", "Furkan  Eris", "Gongyu  Wang", "Jonathan  Elmhurst", "Nick  Moore", "Nicholas C. Harris", "Ayon  Basumallik", "Vijay Janapa Reddi", "Ajay  Joshi", "Darius  Bunandar"], "year": 2021, "n_citations": 1}
{"id": 1958823, "s2_id": "5acc7de71d485a066a69ee132a4384ddde747e01", "title": "Layer-Specific Optimization for Mixed Data Flow With Mixed Precision in FPGA Design for CNN-Based Object Detectors", "abstract": "Convolutional neural networks (CNNs) require both intensive computation and frequent memory access, which lead to a low processing speed and large power dissipation. Although the characteristics of the different layers in a CNN are frequently quite different, previous hardware designs have employed common optimization schemes for them. This paper proposes a layer-specific design that employs different organizations that are optimized for the different layers. The proposed design employs two layer-specific optimizations: layer-specific mixed data flow and layer-specific mixed precision. The mixed data flow aims to minimize the off-chip access while demanding a minimal on-chip memory (BRAM) resource of an FPGA device. The mixed precision quantization is to achieve both a lossless accuracy and an aggressive model compression, thereby further reducing the off-chip access. A Bayesian optimization approach is used to select the best sparsity for each layer, achieving the best trade-off between the accuracy and compression. This mixing scheme allows the entire network model to be stored in BRAMs of the FPGA to aggressively reduce the off-chip access, and thereby achieves a significant performance enhancement. The model size is reduced by 22.66-28.93 times compared to that in a full-precision network with a negligible degradation of accuracy on VOC, COCO, and ImageNet datasets. Furthermore, the combination of mixed dataflow and mixed precision significantly outperforms the previous works in terms of both throughput, off-chip access, and on-chip memory requirement.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "authors": ["Duy Thanh Nguyen", "Hyun  Kim", "Hyuk-Jae  Lee"], "year": 2021, "n_citations": 4}
{"id": 1965175, "s2_id": "09582c92e4c156c544a5cd5730f38d810ad7e2eb", "title": "Demonstrating Analog Inference on the BrainScaleS-2 Mobile System", "abstract": "We present the BrainScaleS-2 mobile system as a compact analog inference engine based on the BrainScaleS-2 ASIC and demonstrate its capabilities at classifying a medical electrocardiogram dataset. The analog network core of the ASIC is utilized to perform the multiply-accumulate operations of a convolutional deep neural network. We measure a total energy consumption of 192 \u03bcJ for the ASIC and achieve a classification time of 276 \u03bcs per electrocardiographic patient sample. Patients with atrial fibrillation are correctly identified with a detection rate of (93.7 \u00b1 0.7) % at (14.0 \u00b1 1.0) % false positives. The system is directly applicable to edge inference applications due to its small size, power envelope and flexible I/O capabilities. Possible future applications can furthermore combine conventional machine learning layers with online-learning in spiking neural networks on a single BrainScaleS-2 ASIC. The system has successfully participated and proven to operate reliably in the independently judged competition Pilotinnovationswettbewerb \u201eEnergieeffizientes KI-System\u201c of the German Federal Ministry of Education and Research (BMBF).", "venue": "ArXiv", "authors": ["Yannik  Stradmann", "Sebastian  Billaudelle", "Oliver  Breitwieser", "Falk Leonard Ebert", "Arne  Emmel", "Dan Husmann de Oliveira", "Joscha  Ilmberger", "Eric  M\u00fcller", "Philipp  Spilger", "Johannes  Weis", "Johannes  Schemmel"], "year": 2021, "n_citations": 0}
{"id": 1971057, "s2_id": "4b68285de4d748073db18f8a3e4a96dc275446b6", "title": "A Roadmap for Enabling a Future-Proof In-Network Computing Data Plane Ecosystem", "abstract": "As the vision of in-network computing becomes more mature, we see two parallel evolutionary trends. First, we see the evolution of richer, more demanding applications that require capabilities beyond programmable switching ASICs. Second, we see the evolution of diverse data plane technologies with many other future capabilities on the horizon. While some point solutions exist to tackle the intersection of these trends, we see several ecosystem-level disconnects today; e.g., the need to refactor applications for new data planes, lack of systematic guidelines to inform the development of future data plane capabilities, and lack of holistic runtime frameworks for network operators. In this paper, we use simple-yet-instructive emerging application-data plane combination to highlight these disconnects. Drawing on these lessons, we sketch a high-level roadmap and guidelines for the community to tackle these to create a more thriving \u201cfuture-proof\u201d data plane ecosystem.", "venue": "ArXiv", "authors": ["Daehyeok  Kim", "Nikita  Lazarev", "Tommy  Tracy", "Farzana  Siddique", "Hun  Namkung", "James C. Hoe", "Vyas  Sekar", "Kevin  Skadron", "Zhiru  Zhang", "Srinivasan  Seshan"], "year": 2021, "n_citations": 0}
{"id": 1971725, "s2_id": "67cd6da17d4f51951d4dbfb3429c82fc77df7c52", "title": "Architectural Techniques for Improving NAND Flash Memory Reliability", "abstract": "Raw bit errors are common in NAND flash memory and will increase in the future. These errors reduce flash reliability and limit the lifetime of a flash memory device. We aim to improve flash reliability with a multitude of low-cost architectural techniques. We show that NAND flash memory reliability can be improved at low cost and with low performance overhead by deploying various architectural techniques that are aware of higher-level application behavior and underlying flash device characteristics. \nWe analyze flash error characteristics and workload behavior through experimental characterization, and design new flash controller algorithms that use the insights gained from our analysis to improve flash reliability at a low cost. We investigate four directions through this approach. (1) We propose a new technique called WARM that improves flash reliability by 12.9 times by managing flash retention differently for write-hot data and write-cold data. (2) We propose a new framework that learns an online flash channel model for each chip and enables four new flash controller algorithms to improve flash reliability by up to 69.9%. (3) We identify three new error characteristics in 3D NAND through a comprehensive experimental characterization of real 3D NAND chips, and propose four new techniques that mitigate these new errors and improve 3D NAND reliability by up to 66.9%. (4) We propose a new technique called HeatWatch that improves 3D NAND reliability by 3.85 times by utilizing self-healing effect to mitigate retention errors in 3D NAND.", "venue": "ArXiv", "authors": ["Yixin  Luo"], "year": 2018, "n_citations": 8}
{"id": 1972374, "s2_id": "e196e912df9513576a4718f7249ad4b68c7c9269", "title": "Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power Management", "abstract": "Modern DRAM architectures allow a number of low-power states on individual memory ranks for advanced power management. Many previous studies have taken advantage of demotions on low-power states for energy saving. However, most of the demotion schemes are statically performed on a limited number of pre-selected low-power states, and are suboptimal for different workloads and memory architectures. Even worse, the idle periods are often too short for effective power state transitions, especially for memory intensive applications. Wrong decisions on power state transition incur significant energy and delay penalties. In this paper, we propose a novel memory system design named RAMZzz with rank-aware energy saving optimizations including dynamic page migrations and adaptive demotions. Specifically, we group the pages with similar access locality into the same rank with dynamic page migrations. Ranks have their hotness: hot ranks are kept busy for high utilization and cold ranks can have more lengthy idle periods for power state transitions. We further develop adaptive state demotions by considering all low-power states for each rank and a prediction model to estimate the power-down timeout among states. We experimentally compare our algorithm with other energy saving policies with cycle-accurate simulation. Experiments with benchmark workloads show that RAMZzz achieves significant improvement on energy-delay2 and energy consumption over other energy saving techniques.", "venue": "IEEE Transactions on Computers", "authors": ["Yanchao  Lu", "Donghong  Wu", "Bingsheng  He", "Xueyan  Tang", "Jianliang  Xu", "Minyi  Guo"], "year": 2016, "n_citations": 9}
{"id": 1973042, "s2_id": "56d78aa8fe9b401d17091d6e6fe279c64a0f3e06", "title": "MC-CIM: Compute-in-Memory with Monte-Carlo Dropouts for Bayesian Edge Intelligence", "abstract": "We propose MC-CIM, a compute-in-memory (CIM) framework for robust, yet low power, Bayesian edge intelligence. Deep neural networks (DNN) with deterministic weights cannot express their prediction uncertainties, thereby pose critical risks for applications where the consequences of mispredictions are fatal such as surgical robotics. To address this limitation, Bayesian inference of a DNN has gained attention. Using Bayesian inference, not only the prediction itself, but the prediction confidence can also be extracted for planning risk-aware actions. However, Bayesian inference of a DNN is computationally expensive, illsuited for real-time and/or edge deployment. An approximation to Bayesian DNN using Monte Carlo Dropout (MC-Dropout) has shown high robustness along with low computational complexity. Enhancing the computational efficiency of the method, we discuss a novel CIM module that can perform in-memory probabilistic dropout in addition to in-memory weight-input scalar product to support the method. We also propose a compute-reuse reformulation of MC-Dropout where each successive instance can utilize the product-sum computations from the previous iteration. Even more, we discuss how the random instances can be optimally ordered to minimize the overall MC-Dropout workload by exploiting combinatorial optimization methods. Application of the proposed CIM-based MC-Dropout execution is discussed for MNIST character recognition and visual odometry (VO) of autonomous drones. The framework reliably gives prediction confidence amidst non-idealities imposed by MC-CIM to a good extent. Proposed MC-CIM with 16\u00d731 SRAM array, 0.85 V supply, 16nm low-standby power (LSTP) technology consumes 27.8 pJ for 30 MC-Dropout instances of probabilistic inference in its most optimal computing and peripheral configuration, saving \u223c43% energy compared to typical execution.", "venue": "ArXiv", "authors": ["Priyesh  Shukla", "Shamma  Nasrin", "Nastaran  Darabi", "Wilfred  Gomes", "Amit Ranjan Trivedi"], "year": 2021, "n_citations": 0}
{"id": 1977477, "s2_id": "6f6ced1670aa448d1eb88011f5667406409f476e", "title": "Feasible methodology for optimization of a novel reversible binary compressor", "abstract": "Now a day reversible logic is an attractive research area due to its low power consumption in the area of VLSI circuit design. The reversible logic gate is utilized to optimize power consumption by a feature of retrieving input logic from an output logic because of bijective mapping between input and output. In this manuscript, we design 4 2 and 5 2 reversible compressor circuits using a new type of reversible gate. In addition, we propose new gate, named as inventive0 gate for optimizing a compressor circuit. The utility of the inventive0 gate is that it can be used as full adder and full subtraction with low value of garbage outputs and quantum cost. An algorithm is shown for designing a compressor structure. The comparative study shows that the proposed compressor structure outperforms the existing ones in terms of garbage outputs, number of gates and quantum cost. The compressor can reduce the effect of carry (Produce from full adder) of the arithmetic frame design. In addition, we implement a basic reversible gate of MOS transistor with less number of MOS transistor count.", "venue": "ArXiv", "authors": ["Neeraj Kumar Misra", "Mukesh Kumar Kushwaha", "Subodh  Wairya", "Amit  Kumar"], "year": 2015, "n_citations": 1}
{"id": 1978763, "s2_id": "03983bc43743f9fb82631cb1eafceb3a4d5295ca", "title": "Novel Reversible Multiplier Architecture Using Reversible TSG Gate", "abstract": "In the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. Recently a 4 * 4 reversible gate called \u201cTSG\u201d is proposed. The most significant aspect of the proposed gate is that it can work singly as a reversible full adder, that is reversible full adder can now be implemented with a single gate only. This paper proposes a NXN reversible multiplier using TSG gate. It is based on two concepts. The partial products can be generated in parallel with a delay of d using Fredkin gates and thereafter the addition can be reduced to log2N steps by using reversible parallel adder designed from TSG gates. A 4x4 architecture of the proposed reversible multiplier is also designed. It is demonstrated that the proposed multiplier architecture using the TSG gate is much better and optimized, compared to its existing counterparts in literature; in terms of number of reversible gates and garbage outputs. Thus, this paper provides the initial threshold to building of more complex system which can execute more complicated operations using reversible logic.", "venue": "IEEE International Conference on Computer Systems and Applications, 2006.", "authors": ["Himanshu  Thapliyal", "M. B. Srinivas"], "year": 2006, "n_citations": 125}
{"id": 1980976, "s2_id": "49532cb16924c06454301f989e2c37c9accc6cd3", "title": "A Workload and Programming Ease Driven Perspective of Processing-in-Memory", "abstract": "Many modern and emerging applications must process increasingly large volumes of data. Unfortunately, prevalent computing paradigms are not designed to efficiently handle such large-scale data: the energy and performance costs to move this data between the memory subsystem and the CPU now dominate the total costs of computation. This forces system architects and designers to fundamentally rethink how to design computers. Processing-in-memory (PIM) is a computing paradigm that avoids most data movement costs by bringing computation to the data. New opportunities in modern memory systems are enabling architectures that can perform varying degrees of processing inside the memory subsystem. However, there are many practical system-level issues that must be tackled to construct PIM architectures, including enabling workloads and programmers to easily take advantage of PIM. This article examines three key domains of work towards the practical construction and widespread adoption of PIM architectures. First, we describe our work on systematically identifying opportunities for PIM in real applications, and quantify potential gains for popular emerging applications (e.g., machine learning, data analytics, genome analysis). Second, we aim to solve several key issues on programming these applications for PIM architectures. Third, we describe challenges that remain for the widespread adoption of PIM.", "venue": "ArXiv", "authors": ["Saugata  Ghose", "Amirali  Boroumand", "Jeremie S. Kim", "Juan  G\u00f3mez-Luna", "Onur  Mutlu"], "year": 2019, "n_citations": 5}
{"id": 1985310, "s2_id": "0c79d6054cf345c316aa570ed4e21c9ccda9a096", "title": "Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing", "abstract": "Matrix-matrix multiplication is a key computational kernel for numerous applications in science and engineering, with ample parallelism and data locality that lends itself well to high-performance implementations. Many matrix multiplication-dependent applications can use reduced-precision integer or fixed-point representations to increase their performance and energy efficiency while still offering adequate quality of results. However, precision requirements may vary between different application phases or depend on input data, rendering constant-precision solutions ineffective. BISMO, a vectorized bit-serial matrix multiplication overlay for reconfigurable computing, previously utilized the excellent binary-operation performance of FPGAs to offer a matrix multiplication performance that scales with required precision and parallelism. We show how BISMO can be scaled up on Xilinx FPGAs using an arithmetic architecture that better utilizes six-input LUTs. The improved BISMO achieves a peak performance of 15.4 binary TOPS on the Ultra96 board with a Xilinx UltraScale+ MPSoC.", "venue": "ACM Trans. Reconfigurable Technol. Syst.", "authors": ["Yaman  Umuroglu", "Davide  Conficconi", "Lahiru  Rasnayake", "Thomas B. Preu\u00dfer", "Magnus  Sj\u00e4lander"], "year": 2019, "n_citations": 6}
{"id": 1989730, "s2_id": "f58babc873f629090f213499f3dee18a2c507329", "title": "Intelligent Architectures for Intelligent Computing Systems", "abstract": "Computing is bottlenecked by data. Large amounts of application data overwhelm storage capability, communication capability, and computation capability of the modern machines we design today. As a result, many key applications' performance, efficiency and scalability are bottlenecked by data movement. In this invited special session talk, we describe three major shortcomings of modern architectures in terms of 1) dealing with data, 2) taking advantage of the vast amounts of data, and 3) exploiting different semantic properties of application data. We argue that an intelligent architecture should be designed to handle data well. We show that handling data well requires designing architectures based on three key principles: 1) data-centric, 2) data-driven, 3) data-aware. We give several examples for how to exploit each of these principles to design a much more efficient and high performance computing system. We especially discuss recent research that aims to fundamentally reduce memory latency and energy, and practically enable computation close to data, with at least two promising novel directions: 1) processing using memory, which exploits analog operational properties of memory chips to perform massively-parallel operations in memory, with low-cost changes, 2) processing near memory, which integrates sophisticated additional processing capability in memory controllers, the logic layer of 3D-stacked memory technologies, or memory chips to enable high memory bandwidth and low memory latency to near-memory logic. We discuss how to enable adoption of such fundamentally more intelligent architectures, which we believe are key to efficiency, performance, and sustainability. We conclude with some guiding principles for future computing architecture and system designs. This accompanying short paper provides a summary of the invited talk and points the reader to further work that may be beneficial to examine.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Onur  Mutlu"], "year": 2021, "n_citations": 4}
{"id": 1991249, "s2_id": "e205c692f5b115cee8e4911e71d1740d7e4e46a2", "title": "Reliability-Aware Overlay Architectures for FPGAs: Features and Design Challenges", "abstract": "The FPGA overlay architectures have been mainly proposed to improve design productivity, circuit portability and system debugging. In this paper, we address the use of overlay architectures for building fault tolerant SRAM-based FPGA systems and discuss the main features and design challenges of a reliability-aware overlay architecture.", "venue": "ArXiv", "authors": ["Mihalis  Psarakis"], "year": 2016, "n_citations": 3}
{"id": 1991253, "s2_id": "bbb34e63378bf5bb8db19400bf0c1606ab6e73c2", "title": "Hamming Distance Tolerant Content-Addressable Memory (HD-CAM) for Approximate Matching Applications", "abstract": "We propose a novel Hamming distance tolerant content-addressable memory (HD-CAM) for energy-efficient in memory approximate matching applications. HD-CAM implements approximate search using matchline charge redistribution rather than its rise or fall time, frequently employed in state of-the-art solutions. HD-CAM was designed in a 65 nm 1.2 V CMOS technology and evaluated through extensive Monte Carlo simulations. Our analysis shows that HD-CAM supports robust operation under significant process variations and changes in the design parameters, enabling a wide range of mismatch threshold (tolerable Hamming distance) levels and pattern lengths. HD-CAM was functionally evaluated for virus DNA classification, which makes HD-CAM suitable for hardware acceleration of genomic surveillance of viral outbreaks such as Covid-19 pandemics.", "venue": "ArXiv", "authors": ["Esteban  Garz'on", "Roman  Golman", "Zuher  Jahshan", "Robert  Hanhan", "Natan  Vinshtok-Melnik", "Marco  Lanuzza", "Adam  Teman", "Leonid  Yavits"], "year": 2021, "n_citations": 0}
{"id": 1993067, "s2_id": "a3d19dc7b3cbd9cfc2c08c5f555b82dd127e98fb", "title": "Improving phase change memory performance with data content aware access", "abstract": "Phase change memory (PCM) is a scalable non-volatile memory technology that has low access latency (like DRAM) and high capacity (like Flash). Writing to PCM incurs significantly higher latency and energy penalties compared to reading its content. A prominent characteristic of PCM\u2019s write operation is that its latency and energy are sensitive to the data to be written as well as the content that is overwritten. We observe that overwriting unknown memory content can incur significantly higher latency and energy compared to overwriting known all-zeros or all-ones content. This is because all-zeros or all-ones content is overwritten by programming the PCM cells only in one direction, i.e., using either SET or RESET operations, not both. In this paper, we propose data content aware PCM writes (DATACON), a new mechanism that reduces the latency and energy of PCM writes by redirecting these requests to overwrite memory locations containing all-zeros or all-ones. DATACON operates in three steps. First, it estimates how much a PCM write access would benefit from overwriting known content (e.g., all-zeros, or all-ones) by comprehensively considering the number of set bits in the data to be written, and the energy-latency trade-offs for SET and RESET operations in PCM. Second, it translates the write address to a physical address within memory that contains the best type of content to overwrite, and records this translation in a table for future accesses. We exploit data access locality in work- loads to minimize the address translation overhead. Third, it re-initializes unused memory locations with known all- zeros or all-ones content in a manner that does not interfere with regular read and write accesses. DATACON overwrites unknown content only when it is absolutely necessary to do so. We evaluate DATACON with workloads from state- of-the-art machine learning applications, SPEC CPU2017, and NAS Parallel Benchmarks. Results demonstrate that DATACON improves the effective access latency by 31%, overall system performance by 27%, and total memory system energy consumption by 43% compared to the best of performance-oriented state-of-the-art techniques.", "venue": "ISMM", "authors": ["Shihao  Song", "Anup  Das", "Onur  Mutlu", "Nagarajan  Kandasamy"], "year": 2020, "n_citations": 23}
{"id": 1993676, "s2_id": "2170198077dec3b0b5679001dc4fed2a7158df1d", "title": "Synthesis of Fault Tolerant Reversible Logic Circuits", "abstract": "Reversible logic is emerging as an important research area having its application in diverse fields such as low power CMOS design, digital signal processing, cryptography, quantum computing and optical information processing. This paper presents a new 4*4 universal reversible logic gate, IG. It is a parity preserving reversible logic gate, that is, the parity of the inputs matches the parity of the outputs. The proposed parity preserving reversible gate can be used to synthesize any arbitrary Boolean function. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. Finally, it is shown how a fault tolerant reversible full adder circuit can be realized using only two IGs. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.", "venue": "2009 IEEE Circuits and Systems International Conference on Testing and Diagnosis", "authors": ["Md. Saiful Islam", "Muhammad Mahbubur Rahman", "Zerina  Begum", "Mohd. Zulfiquar Hafiz", "Abdullah Al Mahmud"], "year": 2009, "n_citations": 77}
{"id": 1995226, "s2_id": "8a6685afcbb40f224ca5485a7f9d245f8610fffd", "title": "Model-Based Development of Distributed Embedded Systems by the Example of the Scicos/SynDEx Framework", "abstract": "The embedded systems engineering industry faces increasing demands for more functionality, rapidly evolving components, and shrinking schedules. Abilities to quickly adapt to changes, develop products with safe design, minimize project costs, and deliver timely are needed. Model-based development (MBD) follows a separation of concerns by abstracting systems with an appropriate intensity. MBD promises higher comprehension by modeling on several abstraction-levels, formal verification, and automated code generation. This thesis demonstrates MBD with the Scicos/SynDEx framework on a distributed embedded system. Scicos is a modeling and simulation environment for hybrid systems. SynDEx is a rapid prototyping integrated development environment for distributed systems. Performed examples implement well-known control algorithms on a target system containing several networked microcontrollers, sensors, and actuators. The addressed research question tackles the feasibility of MBD for medium-sized embedded systems. In the case of single-processor applications experiments show that the comforts of tool-provided simulation, verification, and code-generation have to be weighed against an additional memory consumption in dynamic and static memory compared to a hand-written approach. Establishing a near-seamless modeling-framework with Scicos/SynDEx is expensive. An increased development effort indicates a high price for developing single applications, but might pay off for product families. A further drawback was that the distributed code generated with SynDEx could not be adapted to microcontrollers without a significant alteration of the scheduling tables. The Scicos/SynDEx framework forms a valuable tool set that, however, still needs many improvements. Therefore, its usage is only recommended for experimental purposes.", "venue": "ArXiv", "authors": ["Bernhard  Fischer"], "year": 2010, "n_citations": 0}
{"id": 1995891, "s2_id": "0778d4c9a7f673f7ecbe78a66ef818c15fba7855", "title": "BurstLink: Techniques for Energy-Efficient Conventional and Virtual Reality Video Display", "abstract": "Conventional planar video streaming is the most popular application in mobile systems, and the rapid growth of 360\u25e6 video content and virtual reality (VR) devices are accelerating the adoption of VR video streaming. Unfortunately, video streaming consumes significant system energy due to high power consumption of the system components (e.g., DRAM, display interfaces, and display panel) involved in this process. We propose BurstLink, a novel system-level technique that improves the energy efficiency of planar and VR video streaming. BurstLink is based on two key ideas. First, BurstLink directly transfers a decoded video frame from the host system (i.e., the video decoder or GPU) to the display panel, bypassing the host DRAM. To this end, we extend the display panel with a double remote frame buffer (DRFB), instead of the DRAM\u2019s double frame buffer, so that the system can directly update the DRFB with a new frame while updating the panel\u2019s pixels with the current frame stored in the DRFB. Second, BurstLink transfers a complete decoded frame to the display panel in a single burst, using the maximum bandwidth of modern display interfaces. Unlike conventional systems where the frame transfer rate is limited by the pixel-update throughput of the display panel, BurstLink can always take full advantage of the high bandwidth of modern display interfaces by decoupling the frame transfer from the pixel update as enabled by the DRFB. This direct and burst frame transfer of BurstLink significantly reduces energy consumption in video display by 1) reducing accesses to the host DRAM, 2) increasing the system\u2019s residency at idle power states, and 3) enabling temporal power gating of several system components after quickly transferring each frame into the DRFB. BurstLink can be easily implemented in modern mobile systems with minimal changes to the video display pipeline. We evaluate BurstLink using an analytical power model that we rigorously validate on a real modern mobile system. Our evaluation shows that BurstLink reduces system energy consumption for 4K planar and VR video streaming by 41% and 33%, respectively, and provides an even higher reduction as display resolution and/or display refresh rate increases.", "venue": "ArXiv", "authors": ["Jawad  Haj-Yahya", "Jisung  Park", "Rahul  Bera", "Juan  G\u00f3mez-Luna", "Efraim  Rotem", "Taha  Shahroodi", "Jeremie S. Kim", "Onur  Mutlu"], "year": 2021, "n_citations": 1}
{"id": 1996050, "s2_id": "fd4d6529991ab7b209acc7be46f4abca11f27884", "title": "Automatic Microprocessor Performance Bug Detection", "abstract": "Processor design validation and debug is a difficult and complex task, which consumes the lion\u2019s share of the design process. Design bugs that affect processor performance rather than its functionality are especially difficult to catch, particularly in new microarchitectures. This is because, unlike functional bugs, the correct processor performance of new microarchitectures on complex, long-running benchmarks is typically not deterministically known. Thus, when performance benchmarking new microarchitectures, performance teams may assume that the design is correct when the performance of the new microarchitecture exceeds that of the previous generation, despite significant performance regressions existing in the design. In this work we present a two-stage, machine learning-based methodology that is able to detect the existence of performance bugs in microprocessors. Our results show that our best technique detects 91.5% of microprocessor core performance bugs whose average IPC impact across the studied applications is greater than 1% versus a bug-free design with zero false positives. When evaluated on memory system bugs, our technique achieves 100% detection with zero false positives. Moreover, the detection is automatic, requiring very little performance engineer time.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Erick Carvajal Barboza", "Sara  Jacob", "Mahesh  Ketkar", "Michael  Kishinevsky", "Paul  Gratz", "Jiang  Hu"], "year": 2021, "n_citations": 0}
{"id": 2002235, "s2_id": "adb2b08ca47813983f54dc71ee82e224e667b156", "title": "SparCE: Sparsity Aware General-Purpose Core Extensions to Accelerate Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have emerged as the method of choice for solving a wide range of machine learning tasks. The enormous computational demand posed by DNNs is a key challenge for computing system designers and has most commonly been addressed through the design of DNN accelerators. However, these specialized accelerators utilize large quantities of multiply-accumulate units and on-chip memory and are prohibitive in area and cost constrained systems such as wearable devices and IoT sensors. In this work, we take a complementary approach and improve the performance of DNNs on general-purpose processor (GPP) cores. We do so by exploiting a key attribute of DNNs, viz. sparsity or the prevalence of zero values. We propose Sparsity-aware Core Extensions (SparCE) - a set of low-overhead micro-architectural and ISA extensions that dynamically detect whether an operand (e.g., the result of a load instruction) is zero and subsequently skip a set of future instructions that use it. To maximize performance benefits, SparCE ensures that the instructions to be skipped are prevented from even being fetched, as squashing instructions comes with a penalty (e.g., a pipeline stall). SparCE consists of 2 key micro-architectural enhancements. First, a Sparsity Register File (SpRF) is utilized to track registers that are zero. Next, a Sparsity-Aware Skip Address (SASA) Table is used to indicate instruction sequences that can be skipped, and to specify conditions on SpRF registers that trigger instruction skipping. When an instruction is fetched, SparCE dynamically pre-identifies whether the following instruction(s) can be skipped, and if so appropriately modifies the program counter, thereby skipping the redundant instructions and improving performance. We model SparCE using the gem5 architectural simulator, and evaluate our approach on 6 state-of-the-art image-recognition DNNs in the context of both training and inference using the Caffe deep learning framework. On a scalar microprocessor, SparCE achieves 1.11\u00d7-1.96\u00d7 speedups across both convolution and fully-connected layers that exhibit 10-90 percent sparsity. These speedups translate to 19-31 percent reduction in execution time at the overall application-level. We also evaluate SparCE on a 4-way SIMD ARMv8 processor using the OpenBLAS library, and demonstrate that SparCE achieves 8-15 percent reduction in the application-level execution time.", "venue": "IEEE Transactions on Computers", "authors": ["Sanchari  Sen", "Shubham  Jain", "Swagath  Venkataramani", "Anand  Raghunathan"], "year": 2019, "n_citations": 18}
{"id": 2008367, "s2_id": "7eec96cff72fb7ab821ea7c7a4e8ae315c487b21", "title": "Optimizing Graph Processing and Preprocessing with Hardware Assisted Propagation Blocking", "abstract": "Extensive prior research has focused on alleviating the characteristic poor cache locality of graph analytics workloads. However, graph pre-processing tasks remain relatively unexplored. In many important scenarios, graph pre-processing tasks can be as expensive as the downstream graph analytics kernel. We observe that Propagation Blocking (PB), a software optimization designed for SpMV kernels, generalizes to many graph analytics kernels as well as common pre-processing tasks. In this work, we identify the lingering inefficiencies of a PB execution on conventional multicores and propose architecture support to eliminate PB's bottlenecks, further improving the performance gains from PB. Our proposed architecture -- COBRA -- optimizes the PB execution of both graph processing and pre-processing alike to provide end-to-end speedups of up to 4.6x (3.5x on average).", "venue": "ArXiv", "authors": ["Vignesh  Balaji", "Brandon  Lucia"], "year": 2020, "n_citations": 0}
{"id": 2009086, "s2_id": "9669eee8f9f5e6a60fb12de3bca88ddbb7b3ebfe", "title": "MAC: a novel systematically multilevel cache replacement policy for PCM memory", "abstract": "The rapid development of multi-core system and increase of data-intensive application in recent years call for larger main memory. Traditional DRAM memory can increase its capacity by reducing the feature size of storage cell. Now further scaling of DRAM faces great challenge, and the frequent refresh operations of DRAM can bring a lot of energy consumption. As an emerging technology, Phase Change Memory (PCM) is promising to be used as main memory. It draws wide attention due to the advantages of low power consumption, high density and nonvolatility, while it incurs finite endurance and relatively long write latency. To handle the problem of write, optimizing the cache replacement policy to protect dirty cache block is an efficient way. In this paper, we construct a systematically multilevel structure, and based on it propose a novel cache replacement policy called MAC. MAC can effectively reduce write traffic to PCM memory with low hardware overhead. We conduct simulation experiments on GEM5 to evaluate the performances of MAC and other related works. The results show that MAC performs best in reducing the amount of writes (averagely 25.12%) without increasing the program execution time.", "venue": "ArXiv", "authors": ["Shenchen  Ruan", "Haixia  Wang", "Dongsheng  Wang"], "year": 2016, "n_citations": 0}
{"id": 2009117, "s2_id": "49a1c56e169d9a7d4113290d8d010182f16d7a7c", "title": "A Dual-Port 8-T CAM-Based Network Intrusion Detection Engine for IoT", "abstract": "This letter presents an energy- and memory-efficient pattern-matching engine for a network intrusion detection system (NIDS) in the Internet of Things. Tightly coupled architecture and circuit co-designs are proposed to fully exploit the statistical behaviors of NIDS pattern matching. The proposed engine performs pattern matching in three phases, where the phase-1 prefix matching employs reconfigurable pipelined automata processing to minimize memory footprint without loss of throughput and efficiency. The processing elements utilize 8-T content-addressable memory (CAM) cells for dual-port search by leveraging proposed fixed-1s encoding. A 65-nm prototype demonstrates best-in-class 1.54-fJ energy per search per pattern byte and 0.9-byte memory usage per pattern byte.", "venue": "IEEE Solid-State Circuits Letters", "authors": ["Dai  Li", "Kaiyuan  Yang"], "year": 2020, "n_citations": 1}
{"id": 2010554, "s2_id": "b705d71ba1e05d19b55de1062103b5a19f4a00b1", "title": "AI Accelerator Survey and Trends", "abstract": "Over the past several years, new machine learning accelerators were being announced and released every month for a variety of applications from speech recognition, video object detection, assisted driving, and many data center applications. This paper updates the survey of AI accelerators and processors from past two years. This paper collects and summarizes the cur-rent commercial accelerators that have been publicly announced with peak performance and power consumption numbers. The performance and power values are plotted on a scatter graph, and a number of dimensions and observations from the trends on this plot are again discussed and analyzed. This year, we also compile a list of benchmarking performance results and compute the computational efficiency with respect to peak performance.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Albert  Reuther", "Peter  Michaleas", "Michael  Jones", "Vijay  Gadepally", "Siddharth  Samsi", "Jeremy  Kepner"], "year": 2021, "n_citations": 1}
{"id": 2012715, "s2_id": "5f77fa4e90bdd01d7100079b192478892d684f56", "title": "Enabling Design Methodologies and Future Trends for Edge AI: Specialization and Codesign", "abstract": "Editor\u2019s notes: This work is an introduction and a survey for the Special Issue on Machine Intelligence at the Edge. The authors argue that workloads that were formerly performed in the cloud are increasingly moving to resource-limited edge computing systems, which raises a new set of challenges for machine learning as well as new opportunities. The topic is introduced by means of building blocks ranging from edge fundamentals to edge AI enabling methodologies as well as future trends and challenges. \u2014J\u00f6rg Henkel, Karlsruhe Institute of Technology", "venue": "IEEE Design & Test", "authors": ["Cong  Hao", "Jordan  Dotzel", "Jinjun  Xiong", "Luca  Benini", "Zhiru  Zhang", "Deming  Chen"], "year": 2021, "n_citations": 2}
{"id": 2014685, "s2_id": "d21411cd9b4511e34552e86896aacb3b790a6117", "title": "A Simple Hybrid Model for Accurate Delay Modeling of a Multi-Input Gate", "abstract": "Faithfully representing small gate delay variations caused by input switchings on different inputs in close temporal proximity is a very challenging task for digital delay models. In this paper, we use the example of a 2-input NOR gate to show that a simple hybrid model leads to a surprisingly accurate digital delay model. Our model utilizes simple first-order ordinary differential equations (ODEs) in all modes, resulting from considering transistors as ideal switches in a simple RC model of the gate. By analytically solving the resulting ODEs, we derive expressions for the gate delays, as well as formulas that facilitate model parametrization. It turns out that our model almost faithfully captures the Charlie effect, except in just one specific situation. In addition, we experimentally compare our model\u2019s predictions both to SPICE simulations, using some 15 nm technology, and to some existing delay models. Our results show a significant improvement of the achievable modeling accuracy.", "venue": "ArXiv", "authors": ["Arman  Ferdowsi", "Jurgen  Maier", "Daniel  Ohlinger", "Ulrich  Schmid"], "year": 2021, "n_citations": 0}
{"id": 2015494, "s2_id": "ac95ec8f7d53365e3a03d88d77cce81a02193f5b", "title": "Hardware Acceleration of Monte-Carlo Sampling for Energy Efficient Robust Robot Manipulation", "abstract": "Algorithms based on Monte-Carlo sampling have been widely adapted in robotics and other areas of engineering due to their performance robustness. However, these sampling-based approaches have high computational requirements, making them unsuitable for real-time applications with tight energy constraints. In this paper, we investigate 6 degree-of-freedom (6DoF) pose estimation for robot manipulation using this method, which uses rendering combined with sequential Monte-Carlo sampling. While potentially very accurate, the significant computational complexity of the algorithm makes it less attractive for mobile robots, where runtime and energy consumption are tightly constrained. To address these challenges, we develop a novel hardware implementation of Monte-Carlo sampling on an FPGA with lower computational complexity and memory usage, while achieving high parallelism and modularization. Our results show 12X\u201321X improvements in energy efficiency over low-power and high-end GPU implementations, respectively. Moreover, we achieve real time performance without compromising accuracy.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Yanqi  Liu", "Giuseppe  Calderoni", "R. Iris Bahar"], "year": 2020, "n_citations": 1}
{"id": 2016633, "s2_id": "d0ecffd78981d6bb85dec8b6ed27fc55a01179bf", "title": "Non-Relational Databases on FPGAs: Survey, Design Decisions, Challenges", "abstract": "Non-relational database systems (NRDS), such as graph, document, key-value, and wide-column, have gained much attention in various trending (business) application domains like smart logistics, social network analysis, and medical applications, due to their data model variety and scalability. The broad data variety and sheer size of datasets pose unique challenges for the system design and runtime (incl. power consumption). While CPU performance scaling becomes increasingly more difficult, we argue that NRDS can benefit from adding field programmable gate arrays (FPGAs) as accelerators. However, FPGA-accelerated NRDS have not been systematically studied, yet. \nTo facilitate understanding of this emerging domain, we explore the fit of FPGA acceleration for NRDS with a focus on data model variety. We define the term NRDS class as a group of non-relational database systems supporting the same data model. This survey describes and categorizes the inherent differences and non-trivial trade-offs of relevant NRDS classes as well as their commonalities in the context of common design decisions when building such a system with FPGAs. For example, we found in the literature that for key-value stores the FPGA should be placed into the system as a smart network interface card (SmartNIC) to benefit from direct access of the FPGA to the network. However, more complex data models and processing of other classes (e.g., graph and document) commonly require more elaborate near-data or socket accelerator placements where the FPGA respectively has the only or shared access to main memory. Across the different classes, FPGAs can be used as communication layer or for acceleration of operators and data access. We close with open research and engineering challenges to outline the future of FPGA-accelerated NRDS.", "venue": "ArXiv", "authors": ["Jonas  Dann", "Daniel  Ritter", "Holger  Fr\u00f6ning"], "year": 2020, "n_citations": 1}
{"id": 2022446, "s2_id": "3ba18f5999527dc4dfd8e99c34e3ecfb73c7ab53", "title": "Coprocessors: failures and successes", "abstract": "The appearance and disappearance of coprocessors by integration into the CPU, the success or failure of coprocessors are examined by summarizing their characteristics from the mainframes of the 1960s. The coprocessors most particularly reviewed are the IBM 360 and CDC-6600 I/O processors, the Intel 8087 math coprocessor, the Cell processor, the Intel Xeon Phi coprocessors, the GPUs, the FPGAs, and the coprocessors of manycores SW26010 and Pezy SC-2 used in high-ranked supercomputers in the TOP500 or Green500. The conditions for a coprocessor to be viable in the medium or long-term are defined.", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2019, "n_citations": 0}
{"id": 2026582, "s2_id": "f835e54450463440d0f9e21b1c1874db086fc44c", "title": "RowHammer: A Retrospective", "abstract": "This retrospective paper describes the RowHammer problem in dynamic random access memory (DRAM), which was initially introduced by Kim et al. at the ISCA 2014 Conference. RowHammer is a prime (and perhaps the first) example of how a circuit-level failure mechanism can cause a practical and widespread system security vulnerability. It is the phenomenon that repeatedly accessing a row in a modern DRAM chip causes bit flips in physically adjacent rows at consistently predictable bit locations. RowHammer is caused by a hardware failure mechanism called DRAM disturbance errors, which is a manifestation of circuit-level cell-to-cell interference in a scaled memory technology. Researchers from Google Project Zero demonstrated in 2015 that this hardware failure mechanism can be effectively exploited by user-level programs to gain kernel privileges on real systems. Many other follow-up works demonstrated other practical attacks exploiting RowHammer. In this paper, we comprehensively survey the scientific literature on RowHammer-based attacks as well as mitigation techniques to prevent RowHammer. We also discuss what other related vulnerabilities may be lurking in DRAM and other types of memories, e.g., NAND flash memory or phase change memory, that can potentially threaten the foundations of secure systems, as the memory technologies scale to higher densities. We conclude by describing and advocating a principled approach to memory reliability and security research that can enable us to better anticipate and prevent such vulnerabilities.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Onur  Mutlu", "Jeremie S. Kim"], "year": 2020, "n_citations": 70}
{"id": 2027590, "s2_id": "3b10d2d9944f0a5ba1a479021d4c9590bea9ef21", "title": "MGPU-TSM: A Multi-GPU System with Truly Shared Memory", "abstract": "The sizes of GPU applications are rapidly growing. They are exhausting the compute and memory resources of a single GPU, and are demanding the move to multiple GPUs. However, the performance of these applications scales sub-linearly with GPU count because of the overhead of data movement across multiple GPUs. Moreover, a lack of hardware support for coherency exacerbates the problem because a programmer must either replicate the data across GPUs or fetch the remote data using high-overhead off-chip links. To address these problems, we propose a multi-GPU system with truly shared memory (MGPU-TSM), where the main memory is physically shared across all the GPUs. We eliminate remote accesses and avoid data replication using an MGPU-TSM system, which simplifies the memory hierarchy. Our preliminary analysis shows that MGPU-TSM with 4 GPUs performs, on average, 3.9x? better than the current best performing multi-GPU configuration for standard application benchmarks.", "venue": "ArXiv", "authors": ["Saiful A. Mojumder", "Yifan  Sun", "Leila  Delshadtehrani", "Yenai  Ma", "Trinayan  Baruah", "Jos'e L. Abell'an", "John  Kim", "David  Kaeli", "Ajay  Joshi"], "year": 2020, "n_citations": 2}
{"id": 2027764, "s2_id": "efbff9c471f6403de87353793de622b263da56ad", "title": "Smart temperature sensor for thermal testing of cell-based ICs", "abstract": "In this paper we present a simple and efficient built-in temperature sensor for thermal monitoring of standard-cell based VLSI circuits. The proposed smart temperature sensor uses a ring-oscillator composed of complex gates instead of inverters to optimize their linearity. Simulation results from a 0.18-/spl mu/m CMOS technology show that the nonlinearity error of the sensor can be reduced when an adequate set of standard logic gates is selected.", "venue": "Design, Automation and Test in Europe", "authors": ["Sebasti\u00e0 A. Bota", "M.  Rosales", "Jos\u00e9 Luis Rossell\u00f3", "Jaume  Segura"], "year": 2005, "n_citations": 18}
{"id": 2030263, "s2_id": "d10f6b9a30fe77de373f8be7f33f304a5a6efa1d", "title": "Intelligent Reflecting Surface Aided Wireless Energy and Information Transmission: An Overview", "abstract": "Intelligent reflecting surface (IRS) is a promising technology for achieving spectrum and energy efficient wireless networks cost-effectively. While most existing works focus on exploiting IRS to enhance the performance of wireless information transmission (WIT), the high beamforming gain achieved by passive IRS is also appealing for boosting the efficiency of radio-frequency (RF) based wireless power transfer (WPT) and improving its service coverage. Although IRS-aided WPT shares certain similar characteristics with IRS-aided WIT, they also differ significantly in terms of the design objectives, transmitter/receiver architectures and hardware/power constraints, and so on, which thus may lead to distinct solutions in practice. In this paper, we provide a tutorial overview on IRS-aided WPT and its more complicated integrations with WIT, namely IRS-aided simultaneous wireless information and power transfer (SWIPT) and IRS-aided wireless powered communication networks (WPCNs), from the communication and signal processing perspective. In particular, we identify their fundamental challenges in e.g. passive reflection optimization and channel estimation, propose potential solutions and draw useful insights for practical design. Furthermore, we point out important directions worthy of further investigation in future work.", "venue": "Proceedings of the IEEE", "authors": ["Qingqing  Wu", "Xinrong  Guan", "Rui  Zhang"], "year": 2021, "n_citations": 5}
{"id": 2035379, "s2_id": "4ef4845f77733dea286b58a3aa142549703a5900", "title": "Circuit Design for A Measurement-Based Quantum Carry-Lookahead Adder", "abstract": "We present the design and evaluation of a quantum carry-lookahead adder (QCLA) using measurement-based quantum computation (MBQC), called MBQCLA. QCLA was originally designed for an abstract, concurrent architecture supporting long-distance communication, but most realistic architectures heavily constrain communication distances. The quantum carry-lookahead adder is faster than a quantum ripple-carry adder; QCLA has logarithmic depth while ripple adders have linear depth. MBQCLA utilizes MBQC's ability to transfer quantum states in unit time to accelerate addition. MBQCLA breaks the latency limit of addition circuits in nearest neighbor-only architectures : compared to the $\\Theta(n)$ limit on circuit depth for linear nearest-neighbor architectures, it can reach $\\Theta(log n)$ depth. MBQCLA is an order of magnitude faster than a ripple-carry adder when adding registers longer than 100 qubits, but requires a cluster state that is an order of magnitude larger. The cluster state resources can be classified as computation and communication; for the unoptimized form, $\\approx$ 88 % of the resources are used for communication. Hand optimization of horizontal communication costs results in a $\\approx$ 12% reduction in spatial resources for the in-place MBQCLA circuit. For comparison, a graph state quantum carry-lookahead adder (GSQCLA) uses only $\\approx$ 9 % of the spatial resources of the MBQCLA.", "venue": "ArXiv", "authors": ["Agung  Trisetyarso", "Rodney Van Meter"], "year": 2009, "n_citations": 23}
{"id": 2040701, "s2_id": "c396abbd3cdf3cf41d1b053eaf469c155fda0174", "title": "Architecture, Dataflow and Physical Design Implications of 3D-ICs for DNN-Accelerators", "abstract": "The everlasting demand for higher computing power for deep neural networks (DNNs) drives the development of parallel computing architectures. 3D integration, in which chips are integrated and connected vertically, can further increase performance because it introduces another level of spatial parallelism. Therefore, we analyze dataflows, performance, area, power and temperature of such 3D-DNN-accelerators. Monolithic and TSV-based stacked 3D-ICs are compared against 2D-ICs. We identify workload properties and architectural parameters for efficient 3D-ICs and achieve up to 9. 14x speedup of 3Dvs.2D. We discuss area-performance trade-offs. We demonstrate applicability as the 3D-IC draws similar power as 2D-ICs and is not thermal limited.", "venue": "2021 22nd International Symposium on Quality Electronic Design (ISQED)", "authors": ["Jan Moritz Joseph", "Anand  Samajdar", "Lingjun  Zhu", "Rainer  Leupers", "Syun-Kun  Lim", "Thilo  Pionteck", "Tushar  Krishna"], "year": 2021, "n_citations": 1}
{"id": 2042217, "s2_id": "32800db08a3cb48291641a7544674a2783ce7473", "title": "A Highly Configurable Hardware/Software Stack for DNN Inference Acceleration", "abstract": "This work focuses on an efficient design methodology for domain-specific accelerators. We employ an Agile development approach, with feature-by-feature enhancement of a vertical development stack. This development methodology has been applied to the TVM/VTA inference accelerator. Along the way, we have enhanced the VTA design space and enabled end-to-end support for additional workloads. This has been accomplished by augmenting the VTA micro-architecture and instruction set architecture (ISA), as well as by enhancing the TVM compilation stack to support a wide range of VTA configurations. The VTA tsim implementation (CHISEL-based) has been enhanced with fully pipelined versions of the ALU and GEMM execution units. In tsim, memory width is now parameterized to range between 8-64 bytes per cycle. Field widths and ISA encoding have been made more flexible across multiple targets to support larger addressable scratchpads. A handful of new instructions have been added to enable new or faster functionality. These include: element-wise 8-bit multiplication to support depthwise convolution, load with a choice of pad values to support max pooling, and a clip instruction to support faster execution of a common pattern in ResNets. Support for additional layers, enhanced double buffering allowing for greater scratchpad utilization, and runtime enhancements to lower uop count have also been added. A significant increase in performance is seen for the tsim target just using the fully pipelined versions of ALU and GEMM: \u223c4.9x fewer cycles with minimal area increase to run ResNet-18 under the default configuration. By varying existing and newly added parameters, configurations featuring a further \u223c11.5x decrease in cycle count at a cost of \u223c12x greater area can be instantiated. Tens of intermediate points on the area-performance pareto curve are shown, showcasing the balance of execution unit sizing, memory interface width, and scratchpad sizing which is required to extract good performance from this relatively simple microarchitecture. Finally, VTA is now able to run Mobilenet 1.0 and all layers for ResNets, including the previously disabled pooling and fully connected layers. The TVM/VTA architecture has always featured end-to-end workload evaluation on RTL in a matter of minutes. With our modifications, it now offers a much greater number of feasible configurations with a wide range of cost vs. performance. All capabilities mentioned here, and more, are available in opensource forks of the \u2018tvm\u2019 and \u2018tvm-vta\u2019 repositories. A subset of these capabilities have already been upstreamed.", "venue": "ArXiv", "authors": ["Suvadeep  Banerjee", "Steve  Burns", "Pasquale  Cocchini", "Abhijit  Davare", "Shweta  Jain", "Desmond  Kirkpatrick", "Anton  Sorokin", "Jin  Yang", "Zhenkun  Yang"], "year": 2021, "n_citations": 0}
{"id": 2052086, "s2_id": "3f117def2c5f9fc780d2a5f314dc412a627ff64f", "title": "An Experimental Microarchitecture for a Superconducting Quantum Processor", "abstract": "Quantum computers promise to solve certain problems that are intractable for classical computers, such as factoring large numbers and simulating quantum systems. To date, research in quantum computer engineering has focused primarily at opposite ends of the required system stack: devising high-level programming languages and compilers to describe and optimize quantum algorithms, and building reliable low-level quantum hardware. Relatively little attention has been given to using the compiler output to fully control the operations on experimental quantum processors. Bridging this gap, we propose and build a prototype of a flexible control microarchitecture supporting quantum-classical mixed code for a superconducting quantum processor. The microarchitecture is based on three core elements: (i) a codeword-based event control scheme, (ii) queue-based precise event timing control, and (iii) a flexible multilevel instruction decoding mechanism for control. We design a set of quantum microinstructions that allows flexible control of quantum operations with precise timing. We demonstrate the microarchitecture and microinstruction set by performing a standard gate-characterization experiment on a transmon qubit. CCS CONCEPTS. \u2022 General and reference \u2192 General conference proceedings; \u2022 Computer systems organization \u2192 Quantum computing; \u2022 Hardware \u2192 Quantum technologies;", "venue": "2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["X.  Fu", "M. A. Rol", "C. C. Bultink", "J. van Someren", "Nader  Khammassi", "Imran  Ashraf", "R. F. L. Vermeulen", "J. C. de Sterke", "W. J. Vlothuizen", "R. N. Schouten", "Carmen G. Almud\u00e9ver", "L.  DiCarlo", "Koen  Bertels"], "year": 2017, "n_citations": 60}
{"id": 2056660, "s2_id": "c8b2c219dc83e53395b7383b0fa092ace6f252bc", "title": "High Performance GNR Power Gating for Low-Voltage CMOS Circuits", "abstract": "A robust power gating design using Graphene Nano-Ribbon Field Effect Transistors (GNRFET) is proposed using 16nm technology. The Power Gating (PG) structure is composed of GNRFET as a power switch and MOS power gated module. The proposed structure resolves the main drawbacks of the traditional PG design from the point of view increasing the propagation delay and wake-up time in low voltage regions. GNRFET/MOSFET Conjunction (GMC) is employed to build various structures of PG, GMCPG-SS and GMCPG-NS. In addition to exploiting it to build two multi-mode PG structures. Circuit analysis for CMOS power gated logic modules ISCAS85 benchmark of 16nm technology is used to evaluate the performance of the proposed GNR power switch is compared to the traditional MOS one. Leakage power, wake-up time and power delay product are used as performance circuit parameters for the evaluation.", "venue": "ArXiv", "authors": ["Hader E. El-hmaily", "Rabab  Ezz-Eldin", "A. I. A. Galal", "Hesham F. A. Hamed"], "year": 2019, "n_citations": 0}
{"id": 2058491, "s2_id": "242749187aa4e0d497e252715b1d2f652b8866a2", "title": "SparkXD: A Framework for Resilient and Energy-Efficient Spiking Neural Network Inference using Approximate DRAM", "abstract": "Spiking Neural Networks (SNNs) have the potential for achieving low energy consumption due to their biologically sparse computation. Several studies have shown that the off-chip memory (DRAM) accesses are the most energy-consuming operations in SNN processing. However, state-of-the-art in SNN systems do not optimize the DRAM energy-per-access, thereby hindering achieving high energy-efficiency. To substantially minimize the DRAM energy-per-access, a key knob is to reduce the DRAM supply voltage but this may lead to DRAM errors (i.e., the so-called approximate DRAM). Towards this, we propose SparkXD, a novel framework that provides a comprehensive conjoint solution for resilient and energy-efficient SNN inference using low-power DRAMs subjected to voltage-induced errors. The key mechanisms of SparkXD are: (1) improving the SNN error tolerance through fault-aware training that considers bit errors from approximate DRAM, (2) analyzing the error tolerance of the improved SNN model to find the maximum tolerable bit error rate (BER) that meets the targeted accuracy constraint, and (3) energy-efficient DRAM data mapping for the resilient SNN model that maps the weights in the appropriate DRAM location to minimize the DRAM access energy. Through these mechanisms, SparkXD mitigates the negative impact of DRAM (approximation) errors, and provides the required accuracy. The experimental results show that, for a target accuracy within 1% of the baseline design (i.e., SNN without DRAM errors), SparkXD reduces the DRAM energy by ca. 40% on average across different network sizes.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Abdullah Hanif", "Muhammad  Shafique"], "year": 2021, "n_citations": 6}
{"id": 2060203, "s2_id": "7948ef2dd24aac811f3177eb3b6bad2de6ba8703", "title": "Low-Precision Hardware Architectures Meet Recommendation Model Inference at Scale", "abstract": "Tremendous success of machine learning (ML) and the unabated growth in model complexity motivated many ML-specific designs in hardware architectures to speed up the model inference. While these architectures are diverse, highly optimized low-precision arithmetic is a component shared by most. Nevertheless, recommender systems important to Facebook\u2019s personalization services are demanding and complex: They must serve billions of users per month responsively with low latency while maintaining high prediction accuracy. Do these low-precision architectures work well with our production recommendation systems? They do. But not without significant effort. In this article, we share our search strategies to adapt reference recommendation models to low-precision hardware, our optimization of low-precision compute kernels, and the tool chain to maintain our models\u2019 accuracy throughout their lifespan. We believe our lessons from the trenches can promote better codesign between hardware architecture and software engineering, and advance the state of the art of ML in industry.", "venue": "IEEE Micro", "authors": ["Zhaoxia  Deng", "Jongsoo  Park", "Ping Tak Peter Tang", "Haixin  Liu", "Jie  Yang", "Hector  Yuen", "Jianyu  Huang", "Daya  Khudia", "Xiaohan  Wei", "Ellie  Wen", "Dhruv  Choudhary", "Raghuraman  Krishnamoorthi", "Carole-Jean  Wu", "Satish  Nadathur", "Changkyu  Kim", "Maxim  Naumov", "Sam  Naghshineh", "Mikhail  Smelyanskiy"], "year": 2021, "n_citations": 1}
{"id": 2061199, "s2_id": "37d3c2742b41c311edfb5e7b33789bae22cdb56b", "title": "Scalability of spin FPGA: A Reconfigurable Architecture based on spin MOSFET", "abstract": "Scalability of Field Programmable Gate Array (FPGA) using spin MOSFET (spin FPGA) with magnetocurrent (MC) ratio in the range of 100% to 1000% is discussed for the first time. Area and speed of million-gate spin FPGA are numerically benchmarked with CMOS FPGA for 22nm, 32nm and 45nm technologies including 20% transistor size variation. We show that area is reduced and speed is increased in spin FPGA owing to the nonvolatile memory function of spin MOSFET.", "venue": "ArXiv", "authors": ["Tetsufumi  Tanamoto", "Hideyuki  Sugiyama", "Tomoaki  Inokuchi", "Takao  Marukame", "Mizue  Ishikawa", "Kazutaka  Ikegami", "Yoshiaki  Saito"], "year": 2011, "n_citations": 0}
{"id": 2062436, "s2_id": "274b72c6d2b9d744f3dff90aafeb109dc442cbe8", "title": "The Blockchain Based Auditor on Secret key Life Cycle in Reconfigurable Platform", "abstract": "The growing sophistication of cyber attacks, vulnerabilities in high computing systems and increasing dependency on cryptography to protect our digital data make it more important to keep secret keys safe and secure. Few major issues on secret keys like incorrect use of keys, inappropriate storage of keys, inadequate protection of keys, insecure movement of keys, lack of audit logging, insider threats and non-destruction of keys can compromise the whole security system dangerously. In this article, we have proposed and implemented an isolated secret key memory which can log life cycle of secret keys cryptographically using blockchain (BC) technology. We have also implemented a special custom bus interconnect which receives custom crypto instruction from Processing Element (PE). During the execution of crypto instructions, the architecture assures that secret key will never come in the processor area and the movement of secret keys to various crypto core is recorded cryptographically after the proper authentication process controlled by proposed hardware based BC. To the best of our knowledge, this is the first work which uses blockchain based solution to address the issues of the life cycle of the secret keys in hardware platform. The additional cost of resource usage and timing complexity we spent to implement the proposed idea is very nominal. We have used Xilinx Vivado EDA tool and Artix 7 FPGA board.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Nimisha  Ghosh", "Amlan  Chakrabarti", "Prasant  Mahapatra"], "year": 2020, "n_citations": 0}
{"id": 2064059, "s2_id": "cc557a8b361445db05d5b7211fec4ad5aa7f97b3", "title": "FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review", "abstract": "Due to recent advances in digital technologies, and availability of credible data, an area of artificial intelligence, deep learning, has emerged and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. In particular, convolutional neural networks (CNNs) have demonstrated their effectiveness in the image detection and recognition applications. However, they require intensive CPU operations and memory bandwidth that make general CPUs fail to achieve the desired performance levels. Consequently, hardware accelerators that use application-specific integrated circuits, field-programmable gate arrays (FPGAs), and graphic processing units have been employed to improve the throughput of CNNs. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism and their energy efficiency. In this paper, we review the recent existing techniques for accelerating deep learning networks on FPGAs. We highlight the key features employed by the various techniques for improving the acceleration performance. In addition, we provide recommendations for enhancing the utilization of FPGAs for CNNs acceleration. The techniques investigated in this paper represent the recent trends in the FPGA-based accelerators of deep learning networks. Thus, this paper is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researchers.", "venue": "IEEE Access", "authors": ["Ahmad  Shawahna", "Sadiq M. Sait", "Aiman  El-Maleh"], "year": 2019, "n_citations": 141}
{"id": 2071428, "s2_id": "350a5f1b9620b4a1e8a6d98d1142dbf5c8ec04c1", "title": "Not All Fabrics Are Created Equal: Exploring eFPGA Parameters For IP Redaction", "abstract": "Semiconductor design houses rely on third-party foundries to manufacture their integrated circuits (IC). While this trend allows them to tackle fabrication costs, it introduces security concerns as external (and potentially malicious) parties can access critical parts of the designs and steal or modify the Intellectual Property (IP). Embedded FPGA (eFPGA) redaction is a promising technique to protect critical IPs of an ASIC by redacting (i.e., removing) critical parts and mapping them onto a custom reconfigurable fabric. Only trusted parties will receive the correct bitstream to restore the redacted functionality. While previous studies imply that using an eFPGA is a sufficient condition to provide security against IP threats like reverseengineering, whether this truly holds for all eFPGA architectures is unclear, thus motivating the study in this paper. We examine the security of eFPGA fabrics generated by varying different FPGA design parameters. We characterize the power, performance, and area (PPA) characteristics and evaluate each fabric\u2019s resistance to SAT-based bitstream recovery. Our results encourage designers to work with custom eFPGA fabrics rather than off-the-shelf commercial FPGAs and reveals that only considering a redaction fabric\u2019s bitstream size is inadequate for gauging security.", "venue": "ArXiv", "authors": ["Jitendra  Bhandari", "Abdul Khader Thalakkattu Moosa", "Benjamin  Tan", "Christian  Pilato", "Ganesh  Gore", "Xifan  Tang", "Scott  Temple", "Pierre-Emmanuel  Gaillardon", "Ramesh  Karri"], "year": 2021, "n_citations": 0}
{"id": 2075978, "s2_id": "3302ef1fe288d0741b8886021271fb76a56ff09a", "title": "Efficiency-driven Hardware Optimization for Adversarially Robust Neural Networks", "abstract": "With a growing need to enable intelligence in embedded devices in the Internet of Things (IoT) era, secure hardware implementation of Deep Neural Networks (DNNs) has become imperative. We will focus on how to address adversarial robustness for DNNs through efficiency-driven hardware optimizations. Since memory (specifically, dot-product operations) is a key energy-spending component for DNNs, hardware approaches in the past have focused on optimizing the memory. One such approach is approximate digital CMOS memories with hybrid 6T-8T SRAM cells that enable supply voltage (Vdd) scaling yielding low-power operation, without significantly affecting the performance due to read/write failures incurred in the 6T cells. In this paper, we show how the bit-errors in the 6T cells of hybrid 6T-8T memories minimize the adversarial perturbations in a DNN. Essentially, we find that for different configurations of 8T-6T ratios and scaled Vdd operation, noise incurred in the hybrid memory architectures is bound within specific limits. This hardware noise can potentially interfere in the creation of adversarial attacks in DNNs yielding robustness. Another memory optimization approach involves using analog memristive crossbars that perform Matrix- Vector-Multiplications (MVMs) efficiently with low energy and area requirements. However, crossbars generally suffer from intrinsic non-idealities that cause errors in performing MVMs, leading to degradation in the accuracy of the DNNs. We will show how the intrinsic hardware variations manifested through crossbar non-idealities yield adversarial robustness to the mapped DNNs without any additional optimization.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Abhiroop  Bhattacharjee", "Abhishek  Moitra", "Priyadarshini  Panda"], "year": 2021, "n_citations": 1}
{"id": 2076229, "s2_id": "271719638bcf19a9e245cedbe13b4b9c8042e373", "title": "MIPS-Core Application Specific Instruction-Set Processor for IDEA Cryptography - Comparison between Single-Cycle and Multi-Cycle Architectures", "abstract": "A single-cycle processor completes the execution of an instruction in only one clock cycle. However, its clock period is usually rather long. On the contrary, although clock frequency is higher in a multi-cycle processor, it takes several clock cycles to finish an instruction. Therefore, their runtime efficiencies depend on which program is executed. This paper presents a new processor for International Data Encryption Algorithm (IDEA) cryptography. The new design is an Application Specific Instruction-set Processor (ASIP) in which both general-purpose and special instructions are supported. It is a single-cycle MIPS-core architecture, whose average Clocks Per Instruction (CPI) is 1. Furthermore, a comparison is provided in this paper to show the differences between the proposed single-cycle processor and another comparable multi-cycle crypto processor. FPGA implementation results show that both architectures have almost the same encoding/decoding throughput. However, the previous processor consumes nearly twice as many resources as the new one does.", "venue": "ArXiv", "authors": ["Ahmad  Ahmadi", "Reza Faghih Mirzaee"], "year": 2019, "n_citations": 0}
{"id": 2091589, "s2_id": "437d3bab7da11a3cdef8284cbac98f6e8f7522dc", "title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead", "abstract": "Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and finance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a significant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely computeand energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efficient DL execution arises. This paper first introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efficient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design flexibility and bear the potential of high energy-efficiency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them. INDEX TERMS Machine Learning, ML, Artificial intelligence, AI, Deep Learning, Deep Neural Networks, DNNs, Convolutional Neural Networks, CNNs, Capsule Networks, Spiking Neural Networks, VLSI, Computer Architecture, Hardware Accelerator, Adversarial Attacks, Data Flow, Optimization, Efficiency, Performance, Power Consumption, Energy, Area, Latency", "venue": "IEEE Access", "authors": ["Maurizio  Capra", "Beatrice  Bussolino", "Alberto  Marchisio", "Guido  Masera", "Maurizio  Martina", "Muhammad  Shafique"], "year": 2020, "n_citations": 17}
{"id": 2094365, "s2_id": "3591a1bf291534c72d80c962d86860a7e4117eac", "title": "Reliability-Aware Quantization for Anti-Aging NPUs", "abstract": "Transistor aging is one of the major concerns that challenges designers in advanced technologies. It profoundly degrades the reliability of circuits during its lifetime as it slows down transistors resulting in errors due to timing violations unless large guardbands are included, which leads to considerable performance losses. When it comes to Neural Processing Units (NPUs), where increasing the inference speed is the primary goal, such performance losses cannot be tolerated. In this work, we are the first to propose a reliability-aware quantization to eliminate aging effects in NPUs while completely removing guardbands. Our technique delivers a graceful inference accuracy degradation over time while compensating for the aging-induced delay increase of the NPU. Our evaluation, over ten state-of-the-art neural network architectures trained on the ImageNet dataset, demonstrates that for an entire lifetime of 10 years, the average accuracy loss is merely 3%. In the meantime, our technique achieves 23% higher performance due to the elimination of the aging guardband.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Sami  Salamin", "Georgios  Zervakis", "Ourania  Spantidi", "Iraklis  Anagnostopoulos", "Jorg  Henkel", "Hussam  Amrouch"], "year": 2021, "n_citations": 0}
{"id": 2097136, "s2_id": "eda58f2222e960bf1a9651d797a9f599d1c52ad2", "title": "Work-in-Progress: A Simulation Framework for Domain-Specific System-on-Chips", "abstract": "Homogeneous general purpose processors provide flexibility to implement a variety of applications and facilitate programmability. In contrast, heterogeneous system-on-chips (SoCs) that combine general purpose and specialized processors offer great potential to achieve higher efficiency while maintaining programming flexibility. In particular, domain-specific SoCs (DSSoC), a class of heterogeneous architectures, tailor the architecture and processing elements (PE) to a specific domain. Hence, they can provide superior energy-efficiency compared to general purpose processors by exploiting the characteristics of target applications.", "venue": "2019 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)", "authors": ["Samet E. Arda", "Anish  Krishnakumar", "A. Alper Goksoy", "Joshua  Mack", "Nirmal  Kumbhare", "Anderson L. Sartor", "Ali  Akoglu", "Radu  Marculescu", "\u00dcmit Y. Ogras"], "year": 2019, "n_citations": 0}
{"id": 2100565, "s2_id": "0cd401229f27ee3b61dc3ddda67f8373c9c74f28", "title": "Data Criticality in Multi-Threaded Applications: An Insight for Many-Core Systems", "abstract": "Multi-threaded applications are capable of exploiting the full potential of many-core systems. However, Networkon-Chip (NoC) based inter-core communication in many-core systems is responsible for 60-75% of the miss latency experienced by multi-threaded applications. Delay in the arrival of critical data at the requesting core severely hampers performance. This brief presents some interesting insights about how critical data is requested from the memory by multi-threaded applications. Then it investigates the cause of delay in NoC and how it affects the performance. Finally, this brief shows how NoC-aware memory access optimisations can significantly improve performance. Our experimental evaluation considers Early Restart memory access optimisation and demonstrates that by exploiting available NoC resources, critical data can be prioritised to reduce miss penalty by 11% and improve overall system performance by 9%.", "venue": "ArXiv", "authors": ["Abhijit  Das", "John  Jose", "Prabhat  Mishra"], "year": 2021, "n_citations": 0}
{"id": 2103947, "s2_id": "d62cfc0b92cfc4ab43ff90dc676511987855b266", "title": "Timing Driven C-Slow Retiming on RTL for MultiCores on FPGAs", "abstract": "In this paper C-Slow Retiming (CSR) on RTL is discussed. CSR multiplies the functionality of cores by adding the same number of registers into each path. The technique is ideal for FPGAs with their already existing registers. Previously publications are limited to adding registers on netlist level, which generates a lot of system verification problems and which is assumed to be the major drawback to use this technology in the modern multicore times. The paper shows how CSR can efficiently be done with timing driven automatic RTL modification. The methodology provided with this paper can be used as guidance for using CSR in high level synthesis (HLS). The paper shows the results of a CSR-ed complex RISC core on RTL implemented on FPGAs.", "venue": "PARCO", "authors": ["Tobias  Strauch"], "year": 2013, "n_citations": 6}
{"id": 2105006, "s2_id": "c9e6e11de725263745bfd134ae89ec459fb44ebc", "title": "LazyPIM: Efficient Support for Cache Coherence in Processing-in-Memory Architectures", "abstract": "Processing-in-memory (PIM) architectures have seen an increase in popularity recently, as the high internal bandwidth available within 3D-stacked memory provides greater incentive to move some computation into the logic layer of the memory. To maintain program correctness, the portions of a program that are executed in memory must remain coherent with the portions of the program that continue to execute within the processor. Unfortunately, PIM architectures cannot use traditional approaches to cache coherence due to the high off-chip traffic consumed by coherence messages, which, as we illustrate in this work, can undo the benefits of PIM execution for many data-intensive applications. We propose LazyPIM, a new hardware cache coherence mechanism designed specifically for PIM. Prior approaches for coherence in PIM are ill-suited to applications that share a large amount of data between the processor and the PIM logic. LazyPIM uses a combination of speculative cache coherence and compressed coherence signatures to greatly reduce the overhead of keeping PIM coherent with the processor, even when a large amount of sharing exists.We find that LazyPIM improves average performance across a range of data-intensive PIM applications by 19.6%, reduces off-chip traffic by 30.9%, and reduces energy consumption by 18.0%, over the best prior approaches to PIM coherence.", "venue": "ArXiv", "authors": ["Amirali  Boroumand", "Saugata  Ghose", "Minesh  Patel", "Hasan  Hassan", "Brandon  Lucia", "Nastaran  Hajinazar", "Kevin  Hsieh", "Krishna T. Malladi", "Hongzhong  Zheng", "Onur  Mutlu"], "year": 2017, "n_citations": 12}
{"id": 2107418, "s2_id": "e23f6813db510aff57e359881f032393153202e4", "title": "Improving Reliability, Security, and Efficiency of Reconfigurable Hardware Systems", "abstract": "In this treatise,\u00a0 my research on methods to improve efficiency, reliability, and security of reconfigurable hardware systems, i.e., FPGAs, through partial dynamic reconfiguration is outlined. The efficiency of reconfigurable systems can be improved by loading optimized data paths on-the-fly on an FPGA fabric. This technique was applied to the acceleration of SQL queries\u00a0 for large database applications as well as for image and signal processing applications. The focus was not only on performance improvements and resource efficiency, but also the energy efficiency has been significantly improved. In the area of reliability, countermeasures against radiation-induced faults and\u00a0 aging effects for long mission times were investigated and applied to SRAM-FPGA-based satellite systems. Finally, to increase the security of cryptographic FPGA-based implementations against physical attacks, i.e., side-channel and fault injection analysis as well as reverse engineering, it is proposed to transform static circuit structures into dynamic ones by applying dynamic partial reconfiguration.", "venue": "ArXiv", "authors": ["Daniel  Ziener"], "year": 2018, "n_citations": 1}
{"id": 2110238, "s2_id": "62bc74c87efc71004c8b0c7e29ecc40d51c68cfd", "title": "A Deeper Look into RowHammer\u2019s Sensitivities: Experimental Analysis of Real DRAM Chips and Implications on Future Attacks and Defenses", "abstract": "RowHammer is a circuit-level DRAM vulnerability where repeatedly accessing (i.e., hammering) a DRAM row can cause bit flips in physically nearby rows. The RowHammer vulnerability worsens as DRAM cell size and cell-to-cell spacing shrink. Recent studies demonstrate that modern DRAM chips, including chips previously marketed as RowHammer-safe, are even more vulnerable to RowHammer than older chips such that the required hammer count to cause a bit flip has reduced by more than 10X in the last decade. Therefore, it is essential to develop a better understanding and in-depth insights into the RowHammer vulnerability of modern DRAM chips to more effectively secure current and future systems. Our goal in this paper is to provide insights into fundamental properties of the RowHammer vulnerability that are not yet rigorously studied by prior works, but can potentially be i) exploited to develop more effective RowHammer attacks or ii) leveraged to design more effective and efficient defense mechanisms. To this end, we present an experimental characterization using 248 DDR4 and 24 DDR3 modern DRAM chips from four major DRAM manufacturers demonstrating how the RowHammer effects vary with three fundamental properties: 1) DRAM chip temperature, 2) aggressor row active time, and 3) victim DRAM cell\u2019s physical location. Among our 16 new observations, we highlight that a RowHammer bit flip 1) is very likely to occur in a bounded range, specific to each DRAM cell (e.g., 5.4% of the vulnerable DRAM cells exhibit errors in the range to ), 2) is more likely to occur if the aggressor row is active for longer time (e.g., RowHammer vulnerability increases by 36% if we keep a DRAM row active for 15 column accesses), and 3) is more likely to occur in certain physical regions of the DRAM module under attack (e.g., 5% of the rows are 2x more vulnerable than the remaining 95% of the rows). Our study has important practical implications on future RowHammer attacks and defenses. We describe and analyze the implications of our new findings by proposing three future RowHammer attack and five future RowHammer defense improvements.", "venue": "MICRO", "authors": ["Lois  Orosa", "Abdullah Giray Yaglik\u00e7i", "Haocong  Luo", "Ataberk  Olgun", "Jisung  Park", "Hasan  Hassan", "Minesh  Patel", "Jeremie S. Kim", "Onur  Mutlu"], "year": 2021, "n_citations": 5}
{"id": 2110941, "s2_id": "d646fb780b8c72f23db38f5f112164c62ad5a7a9", "title": "On the Off-Chip Memory Latency of Real-Time Systems: Is DDR DRAM Really the Best Option?", "abstract": "Predictable execution time upon accessing shared memories in multi-core real-time systems is a stringent requirement. A plethora of existing works focus on the analysis of Double Data Rate Dynamic Random Access Memories (DDR DRAMs), or redesigning its memory to provide predictable memory behavior. In this paper, we show that DDR DRAMs by construction suffer inherent limitations associated with achieving such predictability. These limitations lead to 1) highly variable access latencies that fluctuate based on various factors such as access patterns and memory state from previous accesses, and 2) overly pessimistic latency bounds. As a result, DDR DRAMs can be ill-suited for some real-time systems that mandate a strict predictable performance with tight timing constraints. Targeting these systems, we promote an alternative off-chip memory solution that is based on the emerging Reduced Latency DRAM (RLDRAM) protocol, and propose a predictable memory controller (RLDC) managing accesses to this memory. Comparing with the state-of-the-art predictable DDR controllers, the proposed solution provides up to 11\u00d7 less timing variability and 6.4\u00d7 reduction in the worst case memory latency.", "venue": "2018 IEEE Real-Time Systems Symposium (RTSS)", "authors": ["Mohamed  Hassan"], "year": 2018, "n_citations": 15}
{"id": 2111205, "s2_id": "465a44fd22c4cd9a3a07da200cb0b8b68c301737", "title": "PERI: A Posit Enabled RISC-V Core", "abstract": "Owing to the failure of Dennard's scaling the last decade has seen a steep growth of prominent new paradigms leveraging opportunities in computer architecture. Two technologies of interest are Posit and RISC-V. Posit was introduced in mid-2017 as a viable alternative to IEEE 754-2008. Posit promises more accuracy, higher dynamic range, and fewer unused states along with simpler hardware designs as compared to IEEE 754-2008. RISC-V, on the other hand, provides a commercial-grade open-source ISA. It is not only elegant and simple but also highly extensible and customizable, thereby facilitating novel micro-architectural research and exploration. In this paper, we bring these two technologies together and propose the first Posit Enabled RISC-V core. The paper provides insights on how the current 'F' extension and the custom op-code space of RISC-V can be leveraged/modified to support Posit arithmetic. We also present implementation details of a parameterized and feature-complete Posit FPU which is integrated with the RISC-V compliant SHAKTI C-class core either as an execution unit or as an accelerator. To fully leverage the potential of Posit, we further enhance our Posit FPU, with minimal overheads, to support two different exponent sizes (with posit-size being 32-bits). This allows applications to switch from high-accuracy computation mode to a mode with higher dynamic-range at run-time. In the absence of viable software tool-chain to enable porting of applications in the Posit domain, we present a workaround on how certain applications can be modified minimally to exploit the existing RISC-V tool-chain. We also provide examples of applications which can perform better with Posit as compared to IEEE 754-2008. The proposed Posit FPU consumes 3507 slice LUTs and 1294 slice registers on an Artix-7-100T Xilinx FPGA while capable of operating at 100 MHz.", "venue": "ArXiv", "authors": ["Sugandha  Tiwari", "Neel  Gala", "Chester  Rebeiro", "V.  Kamakoti"], "year": 2019, "n_citations": 7}
{"id": 2115524, "s2_id": "b2fe2dce433bf11ccc233b809bc9399bfb5392eb", "title": "Synthesis of Low-Power Digital Circuits Derived from Binary Decision Diagrams", "abstract": "This paper introduces a novel method for synthesizing digital circuits derived from Binary Decision Diagrams (BDDs) that can yield to reduction in power dissipation. The power reduction is achieved by decreasing the switching activity in a circuit while paying close attention to information measures as an optimization criterion. We first present the technique of efficient BDD-based computation of information measures which are used to guide the power optimization procedures. Using this technique, we have developed an algorithm of BDD reordering which leads to reducing the power consumption of the circuits derived from BDDs. Results produced by the synthesis on the ISCAS benchmark circuits are very encouraging.", "venue": "ArXiv", "authors": ["Denis V. Popel"], "year": 2002, "n_citations": 1}
{"id": 2116820, "s2_id": "5280db4b412538c81a2529846a8e480008e06b9b", "title": "A way memoization technique for reducing power consumption of caches in application specific integrated processors", "abstract": "This paper presents a technique for eliminating redundant cache-tag and cache-way accesses to reduce power consumption. The basic idea is to keep a small number of most recently used (MRU) addresses in a memory address buffer (MAB) and to omit redundant tag and way accesses when there is a MAB-hit. Since the approach keeps only tag and set-index values in the MAB, the energy and area overheads are relatively small even for a MAB with a large number of entries. Furthermore, the approach does not sacrifice the performance. In other words, neither the cycle time nor the number of executed cycles increases. The proposed technique has been applied to the Fujitsu VLIW processor (FR-V) and its power saving has been estimated using NanoSim. Experiments for 32 kB 2-way set associative caches show the power consumption of I-cache and D-cache can be reduced by 40% and 50%, respectively.", "venue": "Design, Automation and Test in Europe", "authors": ["Tohru  Ishihara", "Farzan  Fallah"], "year": 2005, "n_citations": 24}
{"id": 2120288, "s2_id": "226e5081e88cb6938105e17e8a1a90f110c586ac", "title": "MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks", "abstract": "Convolutional neural networks (CNNs) play a key role in deep learning applications. However, the large storage overheads and the substantial computation cost of CNNs are problematic in hardware accelerators. Computing-in-memory (CIM) architecture has demonstrated great potential to effectively compute large-scale matrix-vector multiplication. However, the intensive multiply and accumulation (MAC) operations executed at the crossbar array and the limited capacity of CIM macros remain bottlenecks for further improvement of energy efficiency and throughput. To reduce computation costs, network pruning and quantization are two widely studied compression methods to shrink the model size. However, most of the model compression algorithms can only be implemented in digital-based CNN accelerators. For implementation in a static random access memory (SRAM) CIM-based accelerator, the model compression algorithm must consider the hardware limitations of CIM macros, such as the number of word lines and bit lines that can be turned on at the same time, as well as how to map the weight to the SRAM CIM macro. In this study, a software and hardware co-design approach is proposed to design an SRAM CIM-based CNN accelerator and an SRAM CIM-aware model compression algorithm. To lessen the high-precision MAC required by batch normalization (BN), a quantization algorithm that can fuse BN into the weights is proposed. Furthermore, to reduce the number of network parameters, a sparsity algorithm that considers a CIM architecture is proposed. Last, MARS, a CIM-based CNN accelerator that can utilize multiple SRAM CIM macros as processing units and support a sparsity neural network, is proposed.", "venue": "ArXiv", "authors": ["Syuan-Hao  Sie", "Jye-Luen  Lee", "Yi-Ren  Chen", "Chih-Cheng  Lu", "Chih-Cheng  Hsieh", "Meng-Fan  Chang", "Kea-Tiong  Tang"], "year": 2020, "n_citations": 0}
{"id": 2121636, "s2_id": "dbd05866399e98b6acc2f6d0cfb12f0662a8d9e2", "title": "A Case for Lifetime Reliability-Aware Neuromorphic Computing", "abstract": "Neuromorphic computing with non-volatile memory (NVM) can significantly improve performance and lower energy consumption of machine learning tasks implemented using spikebased computations and bio-inspired learning algorithms. High voltages required to operate certain NVMs such as phase-change memory (PCM) can accelerate aging in a neuron\u2019s CMOS circuit, thereby reducing the lifetime of neuromorphic hardware. In this work, we evaluate the long-term, i.e., lifetime reliability impact of executing state-of-the-art machine learning tasks on a neuromorphic hardware, considering failure models such as negative bias temperature instability (NBTI) and time-dependent dielectric breakdown (TDDB). Based on such formulation, we show the reliability-performance trade-off obtained due to periodic relaxation of neuromorphic circuits, i.e., a stop-and-go style of neuromorphic computing.", "venue": "2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["Shihao  Song", "Anup  Das"], "year": 2020, "n_citations": 16}
{"id": 2122509, "s2_id": "2ae70d22391692a609ed92c463d5f43c86088c3d", "title": "Benchmarking High Bandwidth Memory on FPGAs", "abstract": "FPGAs are starting to be enhanced with High Bandwidth Memory (HBM) as a way to reduce the memory bandwidth bottleneck encountered in some applications and to give the FPGA more capacity to deal with application state. However, the performance characteristics of HBM are still not well specified, especially in the context of FPGAs. In this paper, we bridge the gap between nominal specifications and actual performance by benchmarkingHBM on a state-of-the-art FPGA, i.e., a Xilinx Alveo U280 featuring a two-stack HBM subsystem. To this end, we propose Shuhai, a benchmarking tool that allows us to demystify all the underlying details of HBM on an FPGA. FPGA-based benchmarking should also provide a more accurate picture of HBM than doing so on CPUs/GPUs, since CPUs/GPUs are noisier systems due to their complex control logic and cache hierarchy. Since the memory itself is complex, leveraging custom hardware logic to benchmark inside an FPGA provides more details as well as accurate and deterministic measurements. We observe that 1) HBM is able to provide up to 425GB/s memory bandwidth, and 2) how HBM is used has a significant impact on performance, which in turn demonstrates the importance of unveiling the performance characteristics of HBM so as to select the best approach. As a yardstick, we also applyShuhaito DDR4to show the differences between HBM and DDR4.Shuhai can be easily generalized to other FPGA boards or other generations of memory, e.g., HBM3, and DDR3. We will makeShuhaiopen-source, benefiting the community", "venue": "ArXiv", "authors": ["Zeke  Wang", "Hongjing  Huang", "Jie  Zhang", "Gustavo  Alonso"], "year": 2020, "n_citations": 5}
{"id": 2124977, "s2_id": "1a25b0bc6285d23114ab9e9e7adf011d762969bd", "title": "Randomized Last-Level Caches Are Still Vulnerable to Cache Side-Channel Attacks! But We Can Fix It", "abstract": "Cache randomization has recently been revived as a promising defense against conflict-based cache side-channel attacks. As two of the latest implementations, CEASER-S and ScatterCache both claim to thwart conflict-based cache side-channel attacks using randomized skewed caches. Unfortunately, our experiments show that an attacker can easily find a usable eviction set within the chosen remap period of CEASER-S and increasing the number of partitions without dynamic remapping, such as ScatterCache, cannot eliminate the threat. By quantitatively analyzing the access patterns left by various attacks in the LLC, we have newly discovered several problems with the hypotheses and implementations of randomized caches, which are also overlooked by the research on conflict-based cache side-channel attacks.However, cache randomization is not a false hope and it is an effective defense that should be widely adopted in future processors. The newly discovered problems are corresponding to flaws associated with the existing implementation of cache randomization and are fixable. Several new defense ideas are proposed in this paper. Our experiments show that all the newly discovered problems are fixed within the current performance budget. We also argue that randomized set-associative caches can be sufficiently strengthened and possess a better chance to be actually adopted in commercial processors than their skewed counterparts because they introduce less overhaul to the existing cache structure.", "venue": "2021 IEEE Symposium on Security and Privacy (SP)", "authors": ["Wei  Song", "Boya  Li", "Zihan  Xue", "Zhenzhen  Li", "Wenhao  Wang", "Peng  Liu"], "year": 2021, "n_citations": 6}
{"id": 2127410, "s2_id": "8fe6298019752284a0f1eeb9ced7c8fd5dcbc251", "title": "LiteX: an open-source SoC builder and library based on Migen Python DSL", "abstract": "LiteX is a GitHub-hosted SoC builder / IP library and utilities that can be used to create SoCs and full FPGA designs. Besides being open-source and BSD licensed, its originality lies in the fact that its IP components are entirely described using Migen Python internal DSL, which simplifies its design in depth. LiteX already supports various softcores CPUs and essential peripherals, with no dependencies on proprietary IP blocks or generators. This paper provides an overview of LiteX: two real SoC designs on FPGA are presented. They both leverage the LiteX approach in terms of design entry, libraries and integration capabilities. The first one is based on RISC-V core, while the second is based on a LM32 core. In the second use case, we further demonstrate the use of a fully open-source toolchain coupled with LiteX.", "venue": "ArXiv", "authors": ["Florent  Kermarrec", "S'ebastien  Bourdeauducq", "Jean-Christophe Le Lann", "Hannah  Badier"], "year": 2020, "n_citations": 4}
{"id": 2128648, "s2_id": "a10710c4eb60013555f6548a2e6d2f84dfcefaa7", "title": "An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks", "abstract": "Edge TPUs are a domain of accelerators for low-power, edge devices and are widely used in various Google products such as Coral and Pixel devices. In this paper, we first discuss the major microarchitectural details of Edge TPUs. Then, we extensively evaluate three classes of Edge TPUs, covering different computing ecosystems, that are either currently deployed in Google products or are the product pipeline, across 423K unique convolutional neural networks. Building upon this extensive study, we discuss critical and interpretable microarchitectural insights about the studied classes of Edge TPUs. Mainly, we discuss how Edge TPU accelerators perform across convolutional neural networks with different structures. Finally, we present our ongoing efforts in developing high-accuracy learned machine learning models to estimate the major performance metrics of accelerators such as latency and energy consumption. These learned models enable significantly faster (in the order of milliseconds) evaluations of accelerators as an alternative to time-consuming cycle-accurate simulators and establish an exciting opportunity for rapid hardware/software co-design.", "venue": "ArXiv", "authors": ["Amir  Yazdanbakhsh", "Kiran  Seshadri", "Berkin  Akin", "James  Laudon", "Ravi  Narayanaswami"], "year": 2021, "n_citations": 8}
{"id": 2131432, "s2_id": "ae37bf4aa6ba0a081ddb043e8e443f5b02eca629", "title": "TransForm: Formally Specifying Transistency Models and Synthesizing Enhanced Litmus Tests", "abstract": "Memory consistency models (MCMs) specify the legal ordering and visibility of shared memory accesses in a parallel program. Traditionally, instruction set architecture (ISA) MCMs assume that relevant program-visible memory ordering behaviors only result from shared memory interactions that take place between user-level program instructions. This assumption fails to account for virtual memory (VM) implementations that may result in additional shared memory interactions between user-level program instructions and both 1) system-level operations (e.g., address remappings and translation lookaside buffer invalidations initiated by system calls) and 2) hardware-level operations (e.g., hardware page table walks and dirty bit updates) during a user-level program\u2019 s execution. These additional shared memory interactions can impact the observable memory ordering behaviors of user-level programs. Thus, memory transistency models (MTMs) have been coined as a superset of MCMs to additionally articulate VM-aware consistency rules. However, no prior work has enabled formal MTM specifications, nor methods to support their automated analysis.To fill the above gap, this paper presents the TransForm framework. First, TransForm features an axiomatic vocabulary for formally specifying MTMs. Second, TransForm includes a synthesis engine to support the automated generation of litmus tests enhanced with MTM features (i.e., enhanced litmus tests, or ELTs) when supplied with a TransForm MTM specification. As a case study, we formally define an estimated MTM for Intel x86 processors, called x86t_elt, that is based on observations made by an ELT-based evaluation of an Intel x86 MTM implementation from prior work and available public documentation [23], [29]. Given x86t_elt and a synthesis bound (on program size) as input, TransForm\u2019 s synthesis engine successfully produces a complete set of ELTs (within a 9-instruction bound) including relevant hand-curated ELTs from prior work, plus 100 more.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Naorin  Hossain", "Caroline  Trippel", "Margaret  Martonosi"], "year": 2020, "n_citations": 1}
{"id": 2132124, "s2_id": "fb253e7cda6ca2a7fcc0ad10c3fc06472643468a", "title": "Learned Hardware/Software Co-Design of Neural Accelerators", "abstract": "The use of deep learning has grown at an exponential rate, giving rise to numerous specialized hardware and software systems for deep learning. Because the design space of deep learning software stacks and hardware accelerators is diverse and vast, prior work considers software optimizations separately from hardware architectures, effectively reducing the search space. Unfortunately, this bifurcated approach means that many profitable design points are never explored. This paper instead casts the problem as hardware/software co-design, with the goal of automatically identifying desirable points in the joint design space. The key to our solution is a new constrained Bayesian optimization framework that avoids invalid solutions by exploiting the highly constrained features of this design space, which are semi-continuous/semi-discrete. We evaluate our optimization framework by applying it to a variety of neural models, improving the energy-delay product by 18% (ResNet) and 40% (DQN) over hand-tuned state-of-the-art systems, as well as demonstrating strong results on other neural network architectures, such as MLPs and Transformers.", "venue": "ArXiv", "authors": ["Zhan  Shi", "Chirag  Sakhuja", "Milad  Hashemi", "Kevin  Swersky", "Calvin  Lin"], "year": 2020, "n_citations": 4}
{"id": 2137942, "s2_id": "a71b876ff440cad47ef3f39ec6c02de423b16761", "title": "An Application-Specific VLIW Processor with Vector Instruction Set for CNN Acceleration", "abstract": "In recent years, neural networks have surpassed classical algorithms in areas such as object recognition, e.g. in the well-known ImageNet challenge. As a result, great effort is being put into developing fast and efficient accelerators, especially for Convolutional Neural Networks (CNNs). In this work we present ConvAix, a fully C-programmable processor, which \u2014 contrary to many existing architectures \u2014 does not rely on a hard-wired array of multiply-and-accumulate (MAC) units. Instead it maps computations onto independent vector lanes making use of a carefully designed vector instruction set. The presented processor is targeted towards latency-sensitive applications and is capable of executing up to 192 MAC operations per cycle. ConvAix operates at a target clock frequency of 400 MHz in 28nm CMOS, thereby offering state-of-the-art performance with proper flexibility within its target domain. Simulation results for several 2D convolutional layers from well known CNNs (AlexNet, VGG-16) show an average ALU utilization of 72.5% using vector instructions with 16 bit fixed-point arithmetic. Compared to other well-known designs which are less flexible, ConvAix offers competitive energy efficiency of up to 497 GOP/s/W while even surpassing them in terms of area efficiency and processing speed.", "venue": "2019 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Andreas  Bytyn", "Rainer  Leupers", "Gerd  Ascheid"], "year": 2019, "n_citations": 4}
{"id": 2142093, "s2_id": "fab9471df273c8a6235e200587d06d393705a6f0", "title": "Development of SyReC based expandable reversible logic circuits", "abstract": "Reversible computing is gaining high interest from researchers due to its various promises. One of the prominent advantages perceived from reversible logic is that of reduced power dissipation with many reversible gates at hand, designing a reversible circuit (combinational) has received due attention and achievement. A proposed language for description of reversible circuit, namely SyReC, is also in place. What remain are the software tools which would help in reversible circuit synthesis through simulation. Beginning with the smallest reversible circuit realizations the SyReC statements and expressions, we employ a hierarchal approach to develop a complete reversible circuit, entirely from its SyReC code. We implement this as a software tool. The tool allows a user to expand a reversible circuit of choice in terms of bit width of its inputs. The background approach of expansion of a reversible circuit has also been proposed as a part of this dissertation. Also, a user can use the tool to observe the effect of expansion on incurred costs, in terms of increase in number of lines, number of gates and quantum cost. The importance of observing the change in costs with respect to scale of expansion is important not only from analysis point of view, but also because the cost depends on the approach used for expansion. This dissertation also proposes a reversible circuit design for elevator controller (combinational) and the related costs. The aim is to emphasize use of the proposed approach is designing customized circuits.", "venue": "ArXiv", "authors": ["Vandana  Maheshwari"], "year": 2014, "n_citations": 0}
{"id": 2146285, "s2_id": "0fb81d1567d54d064293a1ab06ddfe124463a7eb", "title": "RP-Rewriter: An Optimized Rewriter for Large Terms in ACL2", "abstract": "RP-Rewriter (Retain-Property) is a verified clause processor that can use some of the existing ACL2 rewrite rules to prove conjectures through term rewriting. Optimized for conjectures that can expand into large terms, the rewriter tries to mimic some of the ACL2 rewriting heuristics but also adds some extra features. It can attach side-conditions to terms that help the rewriter retain properties about them and prevent possibly some very expensive backchaining. The rewriter supports user-defined complex meta rules that can return a special structure to prevent redundant rewriting. Additionally, it can store fast alists even when values are not quoted. RP-Rewriter is utilized for two applications, multiplier design proofs and SVEX simplification, which involve very large terms.", "venue": "ACL2", "authors": ["Mertcan  Temel"], "year": 2020, "n_citations": 0}
{"id": 2151252, "s2_id": "fe689a7393f52aa198f88c32eb718912eda048f7", "title": "Sensitivity Analysis of Core Specialization Techniques", "abstract": "The instruction footprint of OS-intensive workloads such as web servers, database servers, and file servers typically exceeds the size of the instruction cache (32 KB). Consequently, such workloads incur a lot of i-cache misses, which reduces their performance drastically. Several papers have proposed to improve the performance of such workloads using core specialization. In this scheme, tasks with different instruction footprints are executed on different cores. In this report, we study the performance of five state of the art core specialization techniques: SelectiveOffload [6], FlexSC [8], DisAggregateOS [5], SLICC [2], and SchedTask [3] for different system parameters. Our studies show that for a suite of 8 popular OS-intensive workloads, SchedTask performs best for all evaluated configurations.", "venue": "ArXiv", "authors": ["Prathmesh  Kallurkar", "Smruti R. Sarangi"], "year": 2017, "n_citations": 1}
{"id": 2152065, "s2_id": "d3d3dfaed6d6d985e822ff5bb8b2aad5f5adfbcb", "title": "Design of a Compact Reversible Read-Only-Memory with MOS Transistors", "abstract": "Energy conservative devices are the need of the modern technology which leads to the development of reversible logic. The synthesis of reversible logic has become an intensely studied area as it overcomes the problem of power dissipation associated with irreversibility. Storage device such as Read-Only-Memory (ROM) can be realized in a reversible way with low power dissipation. The reversibility of ROM has not been yet realized in literature and hence, this paper presents a novel reversible ROM with its Complementary Metal Oxide Semiconductor (CMOS) realization. On the way to present the architecture of reversible ROM, we propose a new reversible gate named as Nowrin Papiya (NP) gate. All the proposed circuits and gates are realized with CMOS based pass transistor logic. Finally, an algorithm as well as several theorems on the numbers of gates, transistors and garbage outputs have been presented to show the optimality of the reversible ROM. Simulations using Microwind DSCH software has been shown to verify the correctness of the proposed design. The comparative results prove that the proposed designs are efficient and optimized in terms of numbers of gates, transistors, garbage outputs, quantum cost and delay.", "venue": "ArXiv", "authors": ["Sadia  Nowrin", "Papiya  Nazneen", "Lafifa  Jamal"], "year": 2016, "n_citations": 5}
{"id": 2160188, "s2_id": "dacaa974f52abb15713cc7114db4460bf30133d1", "title": "METRO: A Software-Hardware Co-Design of Interconnections for Spatial DNN Accelerators", "abstract": "Tiled spatial architectures have proved to be an effective solution to build large-scale DNN accelerators. In particular, interconnections between tiles are critical for high performance in these tile-based architectures. In this work, we identify the inefficiency in the widely used traditional on-chip networks and the opportunity of software-hardware co-design. We propose METRO with the basic idea of decoupling the traffic scheduling policies from hardware fabrics and moving them to the software level. METRO contains two modules working in synergy: METRO software scheduling framework to coordinate the traffics, and METRO hardware facilities to deliver the data based on software configurations. We evaluate the co-design using different flit sizes for synthetic study, illustrating its effectiveness under various hardware resource constraints, in addition to a wide range of DNN models selected from real-world workloads. The results show that METRO achieves 56.3% communication speedup on average and up to 73.6% overall processing time reduction compared with traditional on-chip network designs.", "venue": "ArXiv", "authors": ["Zhao  Wang", "Guangyu  Sun", "Jingchen  Zhu", "Zhe  Zhou", "Yijiang  Guo", "Zhihang  Yuan"], "year": 2021, "n_citations": 0}
{"id": 2162634, "s2_id": "5c83cf52c1fa5bf7d80d5c79682c12b5abb8b35a", "title": "An Ensemble Learning Approach for In-Situ Monitoring of FPGA Dynamic Power", "abstract": "As field-programmable gate arrays (FPGAs) become prevalent in critical application domains, their power consumption is of high concern. In this paper, we present and evaluate a power monitoring scheme capable of accurately estimating the runtime dynamic power of FPGAs in a fine-grained timescale, in order to support emerging power management techniques. In particular, we describe a novel and specialized ensemble model which can be decomposed into multiple customized decision-tree-based base learners. To aid in model synthesis, a generic computer-aided design flow is proposed to generate samples, select features, tune hyperparameters, and train the ensemble estimator. Besides this, a hardware realization of the trained ensemble estimator is presented for on-chip real-time power estimation. In the experiments, we first show that a single decision tree model can achieve prediction error within 4.51% of a commercial gate-level power estimation tool, which is  $2.41\\times$ \u2013 $6.07 \\times $  lower than provided by the commonly used linear model. More importantly, we study the extra gains in inference accuracy using the proposed ensemble model. Experimental results reveal that the ensemble monitoring method can further improve the accuracy of power predictions to within a maximum error of 1.90%. Moreover, the lookup table overhead of the ensemble monitoring hardware employing up to 64 base learners is within 1.22% of the target FPGA, indicating its light-weight and scalable characteristics.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Zhe  Lin", "Sharad  Sinha", "Wei  Zhang"], "year": 2019, "n_citations": 7}
{"id": 2163162, "s2_id": "989fa897fdb002a9a7cc4175f3b011ef5f9480cb", "title": "TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning", "abstract": "Recent studies from several hyperscalars pinpoint to embedding layers as the most memory-intensive deep learning (DL) algorithm being deployed in today's datacenters. This paper addresses the memory capacity and bandwidth challenges of embedding layers and the associated tensor operations. We present our vertically integrated hardware/software co-design, which includes a custom DIMM module enhanced with near-memory processing cores tailored for DL tensor operations. These custom DIMMs are populated inside a GPU-centric system interconnect as a remote memory pool, allowing GPUs to utilize for scalable memory bandwidth and capacity expansion. A prototype implementation of our proposal on real DL systems shows an average 6.2-17.6\u00d7 performance improvement on state-of-the-art DNN-based recommender systems.", "venue": "MICRO", "authors": ["Youngeun  Kwon", "Yunjae  Lee", "Minsoo  Rhu"], "year": 2019, "n_citations": 56}
{"id": 2166077, "s2_id": "0663fa95000c5bba44392c6c6cfac5d93e674765", "title": "Soft GPGPUs for Embedded FPGAs: An Architectural Evaluation", "abstract": "We present a customizable soft architecture which allows for the execution of GPGPU code on an FPGA without the need to recompile the design. Issues related to scaling the overlay architecture to multiple GPGPU multiprocessors are considered along with application-class architectural optimizations. The overlay architecture is optimized for FPGA implementation to support efficient use of embedded block memories and DSP blocks. This architecture supports direct CUDA compilation of integer computations to a binary which is executable on the FPGA-based GPGPU. The benefits of our architecture are evaluated for a collection of five standard CUDA benchmarks which are compiled using standard GPGPU compilation tools. Speedups of 44x, on average, versus a MicroBlaze microprocessor are achieved. We show dynamic energy savings versus a soft-core processor of 80% on average. Application-customized versions of the soft GPGPU can be used to further reduce dynamic energy consumption by an average of 14%.", "venue": "ArXiv", "authors": ["Kevin  Andryc", "Tedy  Thomas", "Russell  Tessier"], "year": 2016, "n_citations": 4}
{"id": 2169320, "s2_id": "e1e14fc15dd52b411886bdcf173e9bd7d6e04ab8", "title": "RC-RNN: Reconfigurable Cache Architecture for Storage Systems Using Recurrent Neural Networks", "abstract": "Solid-State Drives (SSDs) have significant performance advantages over traditional Hard Disk Drives (HDDs) such as lower latency and higher throughput. Significantly higher price per capacity and limited lifetime, however, prevents designers to completely substitute HDDs by SSDs in enterprise storage systems. SSD-based caching has recently been suggested for storage systems to benefit from higher performance of SSDs while minimizing the overall cost. While conventional caching algorithms such as Least Recently Used (LRU) provide high hit ratio in processors, due to the highly random behavior of Input/Output (I/O) workloads, they hardly provide the required performance level for storage systems. In addition to poor performance, inefficient algorithms also shorten SSD lifetime with unnecessary cache replacements. Such shortcomings motivate us to benefit from more complex non-linear algorithms to achieve higher cache performance and extend SSD lifetime. In this paper, we propose RC-RNN, the first reconfigurable SSD-based cache architecture for storage systems that utilizes machine learning to identify performance-critical data pages for I/O caching. The proposed architecture uses Recurrent Neural Networks (RNN) to characterize ongoing workloads and optimize itself towards higher cache performance while improving SSD lifetime. RC-RNN attempts to learn characteristics of the running workload to predict its behavior and then uses the collected information to identify performancecritical data pages to fetch into the cache. We implement the proposed architecture on a physical server equipped with a Core-i7 CPU, 256GB SSD, and a 2TB HDD running Linux kernel 4.4.0. Experimental results show that RC-RNN characterizes workloads with an accuracy up to 94.6% for SNIA I/O workloads. RC-RNN can perform similarly to the optimal cache algorithm by an accuracy of 95% on average, and outperforms previous SSD caching architectures by providing up to 7x higher hit ratio and decreasing cache replacements", "venue": "IEEE Transactions on Emerging Topics in Computing", "authors": ["Shahriar  Ebrahimi", "Reza  Salkhordeh", "Seyed Ali Osia", "Ali  Taheri", "Hamid R. Rabiee", "Hossen  Asadi"], "year": 2021, "n_citations": 0}
{"id": 2171442, "s2_id": "c5a9dfd2f8387be4735759d809d1a0fcc94f5b78", "title": "Reconfigurable computing for Monte Carlo simulations: Results and prospects of the Janus project", "abstract": "We describe Janus, a massively parallel FPGA-based computer optimized for the simulation of spin glasses, theoretical models for the behavior of glassy materials. FPGAs (as compared to GPUs or many-core processors) provide a complementary approach to massively parallel computing. In particular, our model problem is formulated in terms of binary variables, and floating-point operations can be (almost) completely avoided. The FPGA architecture allows us to run many independent threads with almost no latencies in memory access, thus updating up to 1024 spins per cycle. We describe Janus in detail and we summarize the physics results obtained in four years of operation of this machine; we discuss two types of physics applications: long simulations on very large systems (which try to mimic and provide understanding about the experimental non-equilibrium dynamics), and low-temperature equilibrium simulations using an artificial parallel tempering dynamics. The time scale of our non-equilibrium simulations spans eleven orders of magnitude (from picoseconds to a tenth of a second). On the other hand, our equilibrium simulations are unprecedented both because of the low temperatures reached and for the large systems that we have brought to equilibrium. A finite-time scaling ansatz emerges from the detailed comparison of the two sets of simulations. Janus has made it possible to perform spin-glass simulations that would take several decades on more conventional architectures. The paper ends with an assessment of the potential of possible future versions of the Janus architecture, based on state-of-the-art technology.", "venue": "ArXiv", "authors": ["Marco  Baity-Jesi", "Rachel A. Ba\u00f1os", "Andr\u00e9s Cruz Flor", "Luis Antonio Fern\u00e1ndez", "Jos\u00e9 Miguel Gil-Narvi\u00f3n", "Antonio  Gordillo", "Marco  Guidetti", "David  I\u00f1iguez", "Andrea  Maiorano", "Filippo  Mantovani", "Enzo  Marinari", "Victor  Martin-Mayor", "Jorge  Monforte-Garcia", "Antonio Mu\u00f1oz Sudupe", "Denis  Navarro", "Giorgio  Parisi", "Marcello  Pivanti", "Sergio Perez Gaviro", "Federico  Ricci-Tersenghi", "Juan Jesus Ruiz-Lorenzo", "Sebastiano Fabio Schifano", "Beatriz  Seoane", "Alfonso  Taranc\u00f3n", "Pedro  Tellez", "Raffaele  Tripiccione", "David  Yllanes"], "year": 2012, "n_citations": 21}
{"id": 2171660, "s2_id": "45a36cc64d11838a230af7916d2da19860fc9ffc", "title": "Hard Data on Soft Errors: A Large-Scale Assessment of Real-World Error Rates in GPGPU", "abstract": "Graphics processing units (GPUs) are gaining widespread use in high-performance computing because of their performance advantages relative to CPUs. However, the reliability of GPUs is largely unproven. In particular, current GPUs lack error checking and correcting (ECC) in their memory subsystems. The impact of this design has not been previously measured at a large enough scale to quantify soft error events. We present MemtestG80, our software for assessing memory error rates on NVIDIA graphics cards. Furthermore, we present a large-scale assessment of GPU error rate, conducted by running MemtestG80 on over 50,000 hosts on the Folding@home distributed computing network. Our control experiments on consumer-grade and dedicated-GPGPU hardware in a controlled environment found no errors. However, our survey on Folding@home finds that, in their installed environments, two-thirds of tested GPUs exhibit a detectable, pattern-sensitive rate of memory soft errors. We show that these errors persist after controlling for over clocking and environmental proxies for temperature, but depend strongly on board architecture.", "venue": "2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing", "authors": ["Imran S. Haque", "Vijay S. Pande"], "year": 2010, "n_citations": 109}
{"id": 2172104, "s2_id": "e1f4c736e7a9d255caf4b0cb65ac0ccd06c84777", "title": "DLS: Directoryless Shared Last-level Cache", "abstract": "Directory-based protocols have been the de facto solution for maintaining cache coherence in shared-memory parallel systems comprising multi/many cores, where each store instruction is eagerly made globally visible by invalidating the private cache (PC) backups of other cores. Consequently, the directory not only consumes large chip area, but also incurs considerable energy consumption and performance degradation, due to the large number of Invalidation/Ack messages transferred in the interconnection network and resulting network congestion. In this paper, we reveal the interesting fact that the directory is actually an unnecessary luxury for practical parallel systems. Because of widely deployed software/hardware techniques involving instruction reordering, most (if not all) parallel systems work under the weak consistency model, where a remote store instruction is allowed to be invisible to a core before the next synchronization of the core, instead of being made visible eagerly by invalidating PC backups of other cores. Based on this key observation, we propose a lightweight novel scheme called {\\em DLS (DirectoryLess Shared last-level cache)}, which completely removes the directory and Invalidation/Ack messages, and efficiently maintains cache coherence using a novel {\\em self-suspicion + speculative execution} mechanism. Experimental results over SPLASH-2 benchmarks show that on a 16-core processor, DLS not only completely removes the chip area cost of the directory, but also improves processor performance by 11.08%, reduces overall network traffic by 28.83%, and reduces energy consumption of the network by 15.65% on average (compared with traditional MESI protocol with full directory). Moreover, DLS does not involve any modification to programming languages and compilers, and hence is seamlessly compatible with legacy codes.", "venue": "ArXiv", "authors": ["Daofu  Liu", "Yunji  Chen", "Qi  Guo", "Tianshi  Chen", "Ling  Li", "Qunfeng  Dong", "Weiwu  Hu"], "year": 2012, "n_citations": 2}
{"id": 2175214, "s2_id": "12b7b2a5674fe9077e2854eb6fb5c9e16913cf24", "title": "ShortcutFusion: From Tensorflow to FPGA-based accelerator with reuse-aware memory allocation for shortcut data", "abstract": "Residual block is a very common component in recent state-of-the art CNNs such as EfficientNet or EfficientDet. Shortcut data accounts for nearly 40% of feature-maps access in ResNet152 [8]. Most of the previous DNN compilers, accelerators ignore the shortcut data optimization. This paper presents ShortcutFusion, an optimization tool for FPGA-based accelerator with a reuse-aware static memory allocation for shortcut data, to maximize on-chip data reuse given resource constraints. From TensorFlow DNN models, the proposed design generates instruction sets for a group of nodes which uses an optimized data reuse for each residual block. The accelerator design implemented on the Xilinx KCU1500 FPGA card 2.8x faster and 9.9x more power efficient than NVIDIA RTX 2080 Ti for 256x256 input size. . Compared to the result from baseline, in which the weights, inputs, and outputs are accessed from the off-chip memory exactly once per each layer, ShortcutFusion reduces the DRAM access by 47.8-84.8% for RetinaNet, Yolov3, ResNet152, and EfficientNet. Given a similar buffer size to ShortcutMining [8], which also mine the shortcut data in hardware, the proposed work reduces off-chip access for feature-maps 5.27x while accessing weight from off-chip memory exactly once.", "venue": "ArXiv", "authors": ["Duy Thanh Nguyen", "Hyeonseung  Je", "Tuan Nghia Nguyen", "Soojung  Ryu", "Kyujung  Lee", "Hyuk-Jae  Lee"], "year": 2021, "n_citations": 0}
{"id": 2176108, "s2_id": "c2239940a0a986a968e9ad9657a90636758a8e1f", "title": "A fixed latency ORBGRAND decoder architecture with LUT-aided error-pattern scheduling", "abstract": "Guessing Random Additive Noise Decoding (GRAND) is a universal decoding algorithm that has been recently proposed as a practical way to perform maximum likelihood decoding. It generates a sequence of possible error patterns and applies them to the received vector, checking if the result is a valid codeword. Ordered reliability bits GRAND (ORBGRAND) improves on GRAND by considering soft information received from the channel. Both GRAND and ORBGRAND have been implemented in hardware, focusing on average performance, sacrificing worst case throughput and latency. In this work, an improved pattern schedule for ORBGRAND is proposed. It provides > 0.5dB gain over the standard schedule at a block error rate \u2264 10\u22125, and outperforms more complex GRAND flavors with a fraction of the complexity. The proposed schedule is used within a novel code-agnositic decoder architecture: the decoder guarantees fixed high throughput and low latency, making it attractive for latency-constrained applications. It outperforms the worstcase performance of decoders by orders of magnitude, and outperforms many best-case figures. Decoding a code of length 128, it achieves a throughput of 79.21Gb/s with 58.49ns latency, and of 69.61Gb/s with 40.58ns latency, yielding better energy efficiency and comparable area efficiency with respect to the state of the art.", "venue": "ArXiv", "authors": ["Carlo  Condo"], "year": 2021, "n_citations": 0}
{"id": 2177035, "s2_id": "ddbe7cb11105c44a97b6060254b9ab99163e7cee", "title": "SAWL: A Self-adaptive Wear-leveling NVM Scheme for High Performance Storage Systems", "abstract": "In order to meet the needs of high performance computing (HPC) in terms of large memory, high throughput and energy savings, the non-volatile memory (NVM) has been widely studied due to its salient features of high density, near-zero standby power, byte-addressable and non-volatile properties. In HPC systems, the multi-level cell (MLC) technique is used to significantly increase device density and decrease the cost, which however leads to much weaker endurance than the single-level cell (SLC) counterpart. Although wear-leveling techniques can mitigate this weakness in MLC, the improvements upon MLC-based NVM become very limited due to not achieving uniform write distribution before some cells are really worn out. To address this problem, our paper proposes a self-adaptive wear-leveling (SAWL) scheme for MLC-based NVM. The idea behind SAWL is to dynamically tune the wear-leveling granularities and balance the writes across the cells of entire memory, thus achieving suitable tradeoff between the lifetime and cache hit rate. Moreover, to reduce the size of the address-mapping table, SAWL maintains a few recently-accessed mappings in a small on-chip cache. Experimental results demonstrate that SAWL significantly improves the NVM lifetime and the performance for HPC systems, compared with state-of-the-art schemes.", "venue": "ArXiv", "authors": ["Jianming  Huang", "Yu  Hua", "Pengfei  Zuo", "Wen  Zhou", "Fangting  Huang"], "year": 2019, "n_citations": 0}
{"id": 2177634, "s2_id": "e9d485e70e3aab6cc5d4ee4daf2bd91fbe014356", "title": "Making Memristive Processing-in-Memory Reliable", "abstract": "Processing-in-memory (PIM) solutions vastly accelerate systems by reducing data transfer between computation and memory. Memristors possess a unique property that enables storage and logic within the same device, which is exploited in the memristive Memory Processing Unit (mMPU). The mMPU expands fundamental stateful logic techniques, such as IMPLY, MAGIC and FELIX, to high-throughput parallel logic and arithmetic operations within the memory. Unfortunately, memristive processing-in-memory is highly vulnerable to soft errors and this massive parallelism is not compatible with traditional reliability techniques, such as error-correcting-code (ECC). In this paper, we discuss reliability techniques that efficiently support the mMPU by utilizing the same principles as the mMPU computation. We detail ECC techniques that are based on the unique properties of the mMPU to efficiently utilize the massive parallelism. Furthermore, we present novel solutions for efficiently implementing triple modular redundancy (TMR). The short-term and long-term reliability of large-scale applications, such as neural-network acceleration, are evaluated. The analysis clearly demonstrates the importance of high-throughput reliability mechanisms for memristive processing-in-memory.", "venue": "ArXiv", "authors": ["Orian  Leitersdorf", "Ronny  Ronen", "Shahar  Kvatinsky"], "year": 2021, "n_citations": 0}
{"id": 2180990, "s2_id": "0c020d0be3573757ab778ce3ada0a2b18e2b4cb8", "title": "NMPO: Near-Memory Computing Profiling and Offloading", "abstract": "Real-world applications are now processing big-data sets, often bottlenecked by the data movement between the compute units and the main memory. Near-memory computing (NMC), a modern data-centric computational paradigm, can alleviate these bottlenecks, thereby improving the performance of applications. The lack of NMC system availability makes simulators the primary evaluation tool for performance estimation. However, simulators are usually time-consuming, and methods that can reduce this overhead would accelerate the early-stage design process of NMC systems. This work proposes Near-Memory computing Profiling and Offloading (NMPO), a high-level framework capable of predicting NMC offloading suitability employing an ensemble machine learning model. NMPO predicts NMC suitability with an accuracy of 85.6% and, compared to prior works, can reduce the prediction time by using hardware-dependent applications features by up to 3 order of magnitude.", "venue": "2021 24th Euromicro Conference on Digital System Design (DSD)", "authors": ["Stefano  Corda", "Madhurya  Kumaraswamy", "Ahsan Javed Awan", "Roel  Jordans", "Akash  Kumar", "Henk  Corporaal"], "year": 2021, "n_citations": 0}
{"id": 2184583, "s2_id": "b45752eca90c57a0a2373258abfd4ef884457071", "title": "Hardware Accelerator for Adversarial Attacks on Deep Learning Neural Networks", "abstract": "Recent studies identify that Deep learning Neural Networks (DNNs) are vulnerable to subtle perturbations, which are not perceptible to human visual system but can fool the DNN models and lead to wrong outputs. A class of adversarial attack network algorithms has been proposed to generate robust physical perturbations under different circumstance. These algorithms are the first efforts to move forward secure deep learning by providing an avenue to train future defense networks, however, the intrinsic complexity of them prevents their broader usage.In this paper, we propose the first hardware accelerator for adversarial attacks based on memristor crossbar arrays. Our design significantly improves the throughput of a visual adversarial perturbation system, which can further improve the robustness and security of future deep learning systems. Based on the algorithm uniqueness, we propose four implementations for the adversarial attack accelerator (A3) to improve the throughput, energy efficiency, and computational efficiency.", "venue": "2019 Tenth International Green and Sustainable Computing Conference (IGSC)", "authors": ["Haoqiang  Guo", "Lu  Peng", "Jian  Zhang", "Fang  Qi", "Lide  Duan"], "year": 2019, "n_citations": 3}
{"id": 2185865, "s2_id": "c91e84121a63aa833073026698e3f2b525e57e1d", "title": "Static Quantized Radix-2 FFT/IFFT Processor for Constraints Analysis", "abstract": "This research work focuses on the design of a high-resolution fast Fourier transform (FFT) /inverse fast Fourier transform (IFFT) processors for constraints analysis purpose. Amongst the major setbacks associated with such high resolution, FFT processors are the high power consumption resulting from the structural complexity and computational inefficiency of floating-point calculations. As such, a parallel pipelined architecture was proposed to statically scale the resolution of the processor to suite adequate trade-off constraints. The quantization was applied to provide an approximation to address the finite word-length constraints of digital signal processing (DSP). An optimum operating mode was proposed, based on the signal-to-quantization-noise ratio (SQNR) as well as the statistical theory of quantization, to minimize the tradeoff issues associated with selecting the most application-efficient floating-point processing capability in contrast to their resolution quality.", "venue": "ArXiv", "authors": ["Rozita  Teymourzadeh", "Mometo Jim Abigo", "Mok Vee Hoong"], "year": 2018, "n_citations": 1}
{"id": 2193346, "s2_id": "4ce6b6e569c262f440fc3125abcc4557cf3e37d2", "title": "C for a tiny system", "abstract": "We have implemented support for Padauk microcontrollers, tiny 8-Bit devices with 60 B to 256 B of RAM, in the Small Device C Compiler (SDCC), showing that the use of (mostly) standard C to program such minimal devices is feasible. We report on our experience and on the difficulties in supporting the hardware multithreading present on some of these devices. To make the devices a better target for C, we propose various enhancements of the architecture, and empirically evaluated their impact on code size.", "venue": "ArXiv", "authors": ["Philipp Klaus Krause", "Nicolas  Lesser"], "year": 2020, "n_citations": 0}
{"id": 2196200, "s2_id": "d3ce271550555af4d9c20fc3143f103dbd6ca74c", "title": "A Survey of Aging Monitors and Reconfiguration Techniques", "abstract": "CMOS technology scaling makes aging effects an important concern for the design and fabrication of integrated circuits. Aging deterioration reduces the useful life of a circuit, making it fail earlier. This deterioration can affect all portions of a circuit and impacts its performance and reliability. Contemporary literature shows solutions to monitor and mitigate aging using hardware and software monitoring mechanisms and reconfiguration techniques. The goal of this review of the state-of-the-art is to identify existing monitoring and reconfiguration solutions for aging. This survey evaluates the aging research, focusing the years from 2012 to 2019, and proposes a classification for monitors and reconfiguration techniques. Results show that the most common monitor type used for aging detection is to monitor timing errors, and the most common reconfiguration technique used to deal with aging is voltage scaling. Furthermore, most of the literature contributions are in the digital field, using hardware solutions for monitoring aging in circuits. There are few literature contributions in the analog area, being the scope of this survey in the digital domain. By scrutinizing these solutions, this survey points directions for further research and development of aging monitors and reconfiguration techniques", "venue": "ArXiv", "authors": ["Leonardo Rezende Juracy", "Matheus Trevisan Moreira", "Alexandre de Morais Amory", "Fernando Gehm Moraes"], "year": 2020, "n_citations": 0}
{"id": 2198752, "s2_id": "4773107a41a73aaf60d58d41085317da2108f054", "title": "Evaluating Spatial Accelerator Architectures with Tiled Matrix-Matrix Multiplication", "abstract": "There is a growing interest in custom spatial accelerators for machine learning applications. These accelerators employ a spatial array of processing elements (PEs) interacting via custom buffer hierarchies and networks-on-chip. The efficiency of these accelerators comes from employing optimized dataflow (i.e., spatial/temporal partitioning of data across the PEs and fine-grained scheduling) strategies to optimize data reuse. The focus of this work is to evaluate these accelerator architectures using a tiled general matrix-matrix multiplication (GEMM) kernel. To do so, we develop a framework that finds optimized mappings (dataflow and tile sizes) for a tiled GEMM for a given spatial accelerator and workload combination, leveraging an analytical cost model for runtime and energy. Our evaluations over five spatial accelerators demonstrate that the tiled GEMM mappings systematically generated by our framework achieve high performance on various GEMM workloads and accelerators.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Gordon Euhyun Moon", "Hyoukjun  Kwon", "Geonhwa  Jeong", "Prasanth  Chatarasi", "Sivasankaran  Rajamanickam", "Tushar  Krishna"], "year": 2022, "n_citations": 2}
{"id": 2200797, "s2_id": "15ad98d48e735ebfc681e686ff66ed1e0f545524", "title": "Fast Parallel I/O on Cluster Computers", "abstract": "Today\u2019s cluster computers suffer from slow I/O, which slows down I/O-intensive applications. We show that fast disk I/O can be achieved by operating a parallel file system over fast networks such as Myrinet or Gigabit Ethernet. In this paper, we demonstrate how the ParaStation3 communication system helps speed-up the performance of parallel I/O on clusters using the open source parallel virtual file system (PVFS) as testbed and production system. We will describe the set-up of PVFS on the Alpha-Linux-Cluster-Engine (ALiCE) located at Wuppertal University, Germany. Benchmarks on ALiCE achieve write-performances of up to 1 GB/s from a 32-processor compute-partition to a 32-processor PVFS I/Opartition, outperforming known benchmark results for PVFS on the same network by more than a factor of 2. Read-performance from buffer-cache reaches up to 2.2 GB/s. Our benchmarks are giant, I/O-intensive eigenmode problems from lattice quantum chromodynamics, demonstrating stability and performance of PVFS over Parastation in large-scale production runs.", "venue": "ArXiv", "authors": ["Thomas  D\u00fcssel", "Norbert  Eicker", "Florin  Isaila", "Thomas  Lippert", "Thomas  Moschny", "Hartmut  Neff", "Klaus  Schilling", "Walter F. Tichy"], "year": 2003, "n_citations": 1}
{"id": 2202403, "s2_id": "ccb881870207e4e6f49b59d6d92ac6fa64771946", "title": "AccSS3D: Accelerator for Spatially Sparse 3D DNNs", "abstract": "Semantic understanding and completion of real world scenes is a foundational primitive of 3D Visual perception widely used in high-level applications such as robotics, medical imaging, autonomous driving and navigation. Due to the curse of dimensionality, compute and memory requirements for 3D scene understanding grow in cubic complexity with voxel resolution, posing a huge impediment to realizing real-time energy efficient deployments. The inherent spatial sparsity present in the 3D world due to free space is fundamentally different from the channel-wise sparsity that has been extensively studied. We present ACCELERATOR FOR SPATIALLY SPARSE 3D DNNs (AccSS3D), the first end-to-end solution for accelerating 3D scene understanding by exploiting the ample spatial sparsity. As an algorithm-dataflow-architecture co-designed system specialized for spatially-sparse 3D scene understanding, AccSS3D includes novel spatial locality-aware metadata structures, a near-zero latency and spatial sparsity-aware dataflow optimizer, a surface orientation aware pointcloud reordering algorithm and a codesigned hardware accelerator for spatial sparsity that exploits data reuse through systolic and multicast interconnects. The SSpNNA accelerator core together with the 64 KB of L1 memory requires 0.92 mm2 of area in 16nm process at 1 GHz. Overall, AccSS3D achieves 16.8x speedup and a 2232x energy efficiency improvement for 3D sparse convolution compared to an Intel-i7-8700K 4-core CPU, which translates to a 11.8x end-to-end 3D semantic segmentation speedup and a 24.8x energy efficiency improvement (iso technology node)", "venue": "ArXiv", "authors": ["Om Ji Omer", "Prashant  Laddha", "Gurpreet S Kalsi", "Anirud  Thyagharajan", "Kamlesh R Pillai", "Abhimanyu  Kulkarni", "Anbang  Yao", "Yurong  Chen", "Sreenivas  Subramoney"], "year": 2020, "n_citations": 0}
{"id": 2203066, "s2_id": "2ea5432d537a207b0948c2e44b07f19a716c342d", "title": "Security Assessment of Interposer-based Chiplet Integration", "abstract": "With transistor scaling reaching its limits, interposer-based integration of dies (chiplets) is gaining traction. Such an interposer-based integration enables finer and tighter interconnect pitch than traditional system-on-packages and offers two key benefits: 1. It reduces design-to-market time by bypassing the time-consuming process of verification and fabrication. 2. It reduces the design cost by reusing chiplets. While black-boxing of the slow design stages cuts down the design time, it raises significant security concerns. We study the security implications of the emerging interposer-based integration methodology. The black-boxed design stages deploy security measures against hardware Trojans, reverse engineering, and intellectual property piracy in traditional systems-on-chip (SoC) designs and hence are not suitable for interposer-based integration. We propose using functionally diverse chiplets to detect and thwart hardware Trojans and use the inherent logic redundancy to shore up anti-piracy measures. Our proposals do not rely on access to the black-box design stages. We evaluate the security, time and cost benefits of our plan by implementing a MIPS processor, a DCT core, and an AES core using various IPs from the Xilinx CORE GENERATOR IP catalog, on an interposer-based Xilinx FPGA.", "venue": "ArXiv", "authors": ["Mohammed  Shayan", "Kanad  Basu", "Ramesh  Karri"], "year": 2020, "n_citations": 0}
{"id": 2204621, "s2_id": "3aad8051386ab85d8b60d76c1b7347244b3d947e", "title": "Exploring Modern GPU Memory System Design Challenges through Accurate Modeling", "abstract": "This paper explores the impact of simulator accuracy on architecture design decisions in the general-purpose graphics processing unit (GPGPU) space. We perform a detailed, quantitative analysis of the most popular publicly available GPU simulator, GPGPU-Sim, against our enhanced version of the simulator, updated to model the memory system of modern GPUs in more detail. Our enhanced GPU model is able to describe the NVIDIA Volta architecture in sufficient detail to reduce error in memory system even counters by as much as 66X. The reduced error in the memory system further reduces execution time error versus real hardware by 2.5X. To demonstrate the accuracy of our enhanced model against a real machine, we perform a counter-by-counter validation against an NVIDIA TITAN V Volta GPU, demonstrating the relative accuracy of the new simulator versus the publicly available model. \nWe go on to demonstrate that the simpler model discounts the importance of advanced memory system designs such as out-of-order memory access scheduling, while overstating the impact of more heavily researched areas like L1 cache bypassing. Our results demonstrate that it is important for the academic community to enhance the level of detail in architecture simulators as system complexity continues to grow. As part of this detailed correlation and modeling effort, we developed a new Correlator toolset that includes a consolidation of applications from a variety of popular GPGPU benchmark suites, designed to run in reasonable simulation times. The Correlator also includes a database of hardware profiling results for all these applications on NVIDIA cards ranging from Fermi to Volta and a toolchain that enables users to gather correlation statistics and create detailed counter-by-counter hardware correlation plots with minimal effort.", "venue": "ArXiv", "authors": ["Mahmoud  Khairy", "Akshay  Jain", "Tor M. Aamodt", "Timothy G. Rogers"], "year": 2018, "n_citations": 12}
{"id": 2205079, "s2_id": "f61e0db2fc01a020af1387d1303c0c10c6ca15af", "title": "Scaling up HBM Efficiency of Top-K SpMV for Approximate Embedding Similarity on FPGAs", "abstract": "Top-K SpMV is a key component of similarity-search on sparse embeddings. This sparse workload does not perform well on general-purpose NUMA systems that employ traditional caching strategies. Instead, modern FPGA accelerator cards have a few tricks up their sleeve. We introduce a Top-KSpMV FPGA design that leverages reduced precision and a novel packet-wise CSR matrix compression, enabling custom data layouts and delivering bandwidth efficiency often unreachable even in architectures with higher peak bandwidth. With HBM-based boards, we are 100x faster than a multi-threaded CPU implementation and 2x faster than a GPU with 20% higher bandwidth, with 14.2x higher power-efficiency.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Alberto  Parravicini", "Luca Giuseppe Cellamare", "Marco  Siracusa", "Marco Domenico Santambrogio"], "year": 2021, "n_citations": 3}
{"id": 2210598, "s2_id": "a6b48dfe8e52adc26da682152dececf633c7e322", "title": "A Flexible LDPC code decoder with a Network on Chip as underlying interconnect architecture", "abstract": "LDPC (Low Density Parity Check) codes are among the most powerful and widely adopted modern error correcting codes. The iterative decoding algorithms required for these codes involve high computational complexity and high processing throughput is achieved by allocating a sufficient number of processing elements (PEs). Supporting multiple heterogeneous LDPC codes on a parallel decoder poses serious problems in the design of the interconnect structure for such PEs. The aim of this work is to explore the feasibility of NoC (Network on Chip) based decoders, where full flexibility in terms of supported LDPC codes is obtained resorting to an NoC to connect PEs. NoC based LDPC decoders have been previously considered unfeasible because of the cost overhead associated to packet management and routing. On the contrary, the designed NoC adopts a low complexity routing, which introduces a very limited cost overhead with respect to architectures dedicated to specific classes of codes. Moreover the paper proposes an efficient configuration technique, which allows for fast on--the--fly switching among different codes. The decoder architecture is scalable and VLSI synthesis results are presented for several cases of study, including the whole set of WiMAX LDPC codes, WiFi codes and DVB-S2 standard.", "venue": "ArXiv", "authors": ["Carlo  Condo", "Guido  Masera"], "year": 2011, "n_citations": 3}
{"id": 2211430, "s2_id": "b87b6abc7323ac7bd1fa4184f6c1b661efdeda89", "title": "Analysis and Design of Cost-Effective, High-Throughput LDPC Decoders", "abstract": "This paper introduces a new approach to cost-effective, high-throughput hardware designs for low-density parity-check (LDPC) decoders. The proposed approach, called nonsurjective finite alphabet iterative decoders (NS-FAIDs), exploits the robustness of message-passing LDPC decoders to inaccuracies in the calculation of exchanged messages, and it is shown to provide a unified framework for several designs previously proposed in the literature. NS-FAIDs are optimized by density evolution for regular and irregular LDPC codes, and are shown to provide different tradeoffs between hardware complexity and decoding performance. Two hardware architectures targeting high-throughput applications are also proposed, integrating both Min-Sum (MS) and NS-FAID decoding kernels. ASIC post synthesis implementation results on 65-nm CMOS technology show that NS-FAIDs yield significant improvements in the throughput to area ratio, by up to 58.75% with respect to the MS decoder, with even better or only slightly degraded error correction performance.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Thien Truong Nguyen-Ly", "Valentin  Savin", "Khoa  Le", "David  Declercq", "Fakhreddine  Ghaffari", "Oana  Boncalo"], "year": 2018, "n_citations": 23}
{"id": 2211849, "s2_id": "3a85855c08893c351082001785d9be4ceb7c883d", "title": "Memristive fuzzy edge detector", "abstract": "Fuzzy inference systems always suffer from the lack of efficient structures or platforms for their hardware implementation. In this paper, we tried to overcome this difficulty by proposing a new method for the implementation of the fuzzy rule-based inference systems. To achieve this goal, we have designed a multi-layer neuro-fuzzy computing system based on the memristor crossbar structure by introducing a new concept called the fuzzy minterm. Although many applications can be realized through the use of our proposed system, in this study we only show how the fuzzy XOR function can be constructed and how it can be used to extract edges from grayscale images. One main advantage of our memristive fuzzy edge detector (implemented in analog form) compared to other commonly used edge detectors is it can be implemented in parallel form, which makes it a powerful device for real-time applications.", "venue": "Journal of Real-Time Image Processing", "authors": ["Farnood  Merrikh-Bayat", "Saeed Bagheri Shouraki", "Farshad  Merrikh-Bayat"], "year": 2012, "n_citations": 15}
{"id": 2213439, "s2_id": "81f47ecfd3603b933b7456b122287f7143e8c3be", "title": "HCM: Hardware-Aware Complexity Metric for Neural Network Architectures", "abstract": "Convolutional Neural Networks (CNNs) have become common in many fields including computer vision, speech recognition, and natural language processing. Although CNN hardware accelerators are already included as part of many SoC architectures, the task of achieving high accuracy on resource-restricted devices is still considered challenging, mainly due to the vast number of design parameters that need to be balanced to achieve an efficient solution. Quantization techniques, when applied to the network parameters, lead to a reduction of power and area and may also change the ratio between communication and computation. As a result, some algorithmic solutions may suffer from lack of memory bandwidth or computational resources and fail to achieve the expected performance due to hardware constraints. Thus, the system designer and the micro-architect need to understand at early development stages the impact of their high-level decisions (e.g., the architecture of the CNN and the amount of bits used to represent its parameters) on the final product (e.g., the expected power saving, area, and accuracy). Unfortunately, existing tools fall short of supporting such decisions. \nThis paper introduces a hardware-aware complexity metric that aims to assist the system designer of the neural network architectures, through the entire project lifetime (especially at its early stages) by predicting the impact of architectural and micro-architectural decisions on the final product. We demonstrate how the proposed metric can help evaluate different design alternatives of neural network models on resource-restricted devices such as real-time embedded systems, and to avoid making design mistakes at early stages.", "venue": "ArXiv", "authors": ["Alex  Karbachevsky", "Chaim  Baskin", "Evgenii  Zheltonozshkii", "Yevgeny  Yermolin", "Freddy  Gabbay", "Alex M. Bronstein", "Avi  Mendelson"], "year": 2020, "n_citations": 0}
{"id": 2216747, "s2_id": "58ef12b3a7be59b435f0763c3ace85f6538137f3", "title": "An Improved Majority-Logic Decoder Offering Massively Parallel Decoding for Real-Time Control in Embedded Systems", "abstract": "We propose an easy-to-implement hard-decision majority-logic decoding algorithm for Reed-Muller codes RM(r,m) with m \u2265 3, m/2 \u2265 r \u2265 1. The presented algorithm outperforms the best known majority-logic decoding algorithms and offers highly parallel decoding. The result is of special importance for safety- and time-critical applications in embedded systems. A simple combinational circuit can perform the proposed decoding. In particular, we show how our decoder for the three-error-correcting code RM(2, 5) of dimension 16 and length 32 can be realized on hardware level.", "venue": "IEEE Transactions on Communications", "authors": ["Juliane  Bertram", "Peter  Hauck", "Michael  Huber"], "year": 2013, "n_citations": 5}
{"id": 2218642, "s2_id": "f1490488ec0b0eebdf3d00203a62c38247353abc", "title": "New schemes for self-testing RAM", "abstract": "This paper gives an overview of a new technique, named pseudo-ring testing (PRT). PRT can be applied for testing a wide type of random access memories (RAM): bit-or word-oriented and single- or dual-port RAM. An essential particularity of the proposed methodology is the emulation of a linear automaton over Galois field by memory own components.", "venue": "Design, Automation and Test in Europe", "authors": ["Ghenadie  Bodean", "Diana  Bodean", "A.  Labunetz"], "year": 2005, "n_citations": 4}
{"id": 2222902, "s2_id": "ea38618a79c3295ee03594de1a347907389db48f", "title": "Memos: Revisiting Hybrid Memory Management in Modern Operating System", "abstract": "The emerging hybrid DRAM-NVM architecture is challenging the existing memory management mechanism in operating system. In this paper, we introduce memos, which can schedule memory resources over the entire memory hierarchy including cache, channels, main memory comprising DRAM and NVM simultaneously. Powered by our newly designed kernel-level monitoring module and page migration engine, memos can dynamically optimize the data placement at the memory hierarchy in terms of the on-line memory patterns, current resource utilization and feature of memory medium. Our experimental results show that memos can achieve high memory utilization, contributing to system throughput by 19.1% and QoS by 23.6% on average. Moreover, memos can reduce the NVM side memory latency by 3~83.3%, energy consumption by 25.1~99%, and benefit the NVM lifetime significantly (40X improvement on average).", "venue": "ArXiv", "authors": ["Lei  Liu", "Mengyao  Xie", "Hao  Yang"], "year": 2017, "n_citations": 4}
{"id": 2224471, "s2_id": "fa3c7d267b5742fea83253cd3ba69710f4b956e6", "title": "An Energy-Efficient Mixed-Signal Neuron for Inherently Error-Resilient Neuromorphic Systems", "abstract": "This work presents the design and analysis of a mixed-signal neuron (MS-N) for convolutional neural networks (CNN) and compares its performance with a digital neuron (Dig-N) in terms of operating frequency, power and noise. The circuit- level implementation of the MS-N in 65 nm CMOS technology exhibits 2-3 orders of magnitude better energy-efficiency over Dig-N for neuromorphic computing applications - especially at low frequencies due to the high leakage currents from many transistors in Dig-N. The inherent error- resiliency of CNN is exploited to handle the thermal and flicker noise of MS-N. A system-level analysis using a cohesive circuit-algorithmic framework on MNIST and CIFAR-10 datasets demonstrate an increase of 3% in worst-case classification error for MNIST when the integrated noise power in the bandwidth is ~ 1 \u03bcV\u00b2.", "venue": "2017 IEEE International Conference on Rebooting Computing (ICRC)", "authors": ["Baibhab  Chatterjee", "Priyadarshini  Panda", "Shovan  Maity", "Kaushik  Roy", "Shreyas  Sen"], "year": 2017, "n_citations": 4}
{"id": 2229141, "s2_id": "0ff4138f9429b7a4f1c5e75b3bf3c696ca2e1241", "title": "Adaptive 3D-IC TSV Fault Tolerance Structure Generation", "abstract": "In 3-D integrated circuits (3D-ICs), through silicon via (TSV) is a critical technique in providing vertical connections. However, the yield is one of the key obstacles to adopt the TSV-based 3D-ICs technology in industry. Various fault-tolerance structures using spare TSVs to repair faulty functional TSVs have been proposed in literature for yield and reliability enhancement, but a valid structure cannot always be found due to the lack of effective generation methods for fault-tolerance structures. In this paper, we focus on the problem of adaptive fault-tolerance structure (AFTS) generation. Given the relations between functional TSVs and spare TSVs, we first calculate the maximum number of tolerant faults in each TSV group. Then we propose an integer linear programming-based model to construct the AFTS with minimal multiplexer delay overhead and hardware cost. We further develop a speed-up technique through an efficient min-cost-max-flow model. All the proposed methodologies are embedded in a top-down TSV planning framework to form functional TSV groups and generate AFTSs. Experimental results show that, compared with state-of-the-art, the number of spare TSVs used for fault tolerance can be effectively reduced.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Song  Chen", "Qi  Xu", "Bei  Yu"], "year": 2019, "n_citations": 3}
{"id": 2230894, "s2_id": "b35f2441051d2e42a8926af2757c7be635cd717c", "title": "MARS: Nano-Power Battery-free Wireless Interfaces for Touch, Swipe and Speech Input", "abstract": "Augmenting everyday surfaces with interaction sensing capability that is maintenance-free, low-cost (\u223c $1), and in an appropriate form factor is a challenge with current technologies. MARS (Multi-channel Ambiently-powered Realtime Sensing) enables battery-free sensing and wireless communication of touch, swipe, and speech interactions by combining a nanowatt programmable oscillator with frequency-shifted analog backscatter communication. A zero-threshold voltage field-effect transistor (FET) is used to create an oscillator with a low startup voltage (\u223c 500 mV) and current (< 2uA), whose frequency can be affected through changes in inductance or capacitance from the user interactions. Multiple MARS systems can operate in the same environment by tuning each oscillator circuit to a different frequency range. The nanowatt power budget allows the system to be powered directly through ambient energy sources like photodiodes or thermoelectric generators. We differentiate MARS from previous systems based on power requirements, cost, and part count and explore different interaction and activity sensing scenarios suitable for indoor environments.", "venue": "UIST", "authors": ["Nivedita  Arora", "Ali  Mirzazadeh", "Injoo  Moon", "Charles  Ramey", "Yuhui  Zhao", "Daniela C. Rodriguez", "Gregory D. Abowd", "Thad E. Starner"], "year": 2021, "n_citations": 0}
{"id": 2237402, "s2_id": "50d69d564aeaec482cc40f6bb8795b9fbb9f261c", "title": "A Novel Compaction Approach for SBST Test Programs", "abstract": "In-field test of processor-based devices is a must when considering safety-critical systems (e.g., in robotics, aerospace, and automotive applications). During in-field testing, different solutions can be adopted, depending on the specific constraints of each scenario. In the last years, Self-Test Libraries (STLs) developed by IP or semiconductor companies became widely adopted. Given the strict constraints of in-field test, the size and time duration of a STL is a crucial parameter. This work introduces a novel approach to compress functional test programs belonging to an STL. The proposed approach is based on analyzing (via logic simulation) the interaction between the micro-architectural operation performed by each instruction and its capacity to propagate fault effects on any observable output, reducing the required fault simulations to only one. The proposed compaction strategy was validated by resorting to a RISC-V processor and several test programs stemming from diverse generation strategies. Results showed that the proposed compaction approach can reduce the length of test programs by up to 93.9% and their duration by up to 95%, with minimal effect on fault coverage. Keywords\u2014Functional Testing, Software-Based Self-Test (SBST), Test Compaction", "venue": "ArXiv", "authors": ["Juan-David  Guerrero-Balaguera", "Josie E. Rodriguez Condia", "Matteo Sonza Reorda"], "year": 2021, "n_citations": 0}
{"id": 2241036, "s2_id": "428636726561eda98803f9e4e590312c34f21713", "title": "Delay Monitor Circuit for Sensitive Nodes in SRAM-Based FPGA", "abstract": "This paper presents a novel monitor circuit architecture and experiments performed for detection of extra combinational delays in a high frequency SRAM-Based FPGA on delay sensitive nodes due to transient ionizing radiation.", "venue": "ArXiv", "authors": ["Mostafa  Darvishi", "Yves  Audet", "Yves  Blaqui\u00e8re"], "year": 2018, "n_citations": 0}
{"id": 2242375, "s2_id": "4c44fc158d8e062933c964e7d5b9c447d135ff98", "title": "Design of Reconfigurable Multi-Operand Adder for Massively Parallel Processing", "abstract": "The paper presents a systematic study and implementation of a reconfigurable combinatorial multi-operand adder for use in Deep Learning systems. The size of carry changes with the number of operands and hence a reliable algorithm to estimate exact number of carry bits is needed for optimal implementation of a reconfigurable multi-operand adder. A combinatorial multi-operand adder can be faster compared to a sequential implementation using a two operand adder. Use cases for such adders occur in modern processors for deep neural networks. Such processors require massively parallel computing resources on chip. This paper presents a method to estimate the upper bound on the size of carry. A method to compute the exact number of carry bits required for a multi-operand addition operation. A fast combinatorial parallel 4-operand adder module is presented. An algorithm to reconfigure these adder modules to implement larger adders is also described. Further, the paper presents two compact but slower iterative structures that implement multi-operand addition, iterating with one column at a time till the entire word is covered. Such serial/iterative operations are slow but occupy small space while parallel operations are fast but use large silicon area on chip. Interestingly, the area-to-throughput ratio of two architectures can tilt in favor of slower, smaller and large number units instead of the fewer numbers of fast and large compute units. A lemma presented in the paper may be used to identify the condition when such tilt occurs. Potentially, this can save silicon space and increase the throughput of chips for high performance computing. Simulation results of a 16 operand adder and using an set of 4-operand adders for use in neural networks have been presented. Simulation results show that performance gain improves as the number of operations or operands increases.", "venue": "ArXiv", "authors": ["Shilpa  Mayannavar", "Uday  Wali"], "year": 2020, "n_citations": 0}
{"id": 2243810, "s2_id": "57684f81bc77aa0109468729ee91cad03a7527d5", "title": "Accelerate Cycle-Level Full-System Simulation of Multi-Core RISC-V Systems with Binary Translation", "abstract": "It has always been difficult to balance the accuracy and performance of ISSs. RTL simulators or systems such as gem5 are used to execute programs in a cycle-accurate manner but are often prohibitively slow. In contrast, functional simulators such as QEMU can run large benchmarks to completion in a reasonable time yet capture few performance metrics and fail to model complex interactions between multiple cores. \nThis paper presents a novel multi-purpose simulator that exploits binary translation to offer fast cycle-level full-system simulations. Its functional simulation mode outperforms QEMU and, if desired, it is possible to switch between functional and timing modes at run-time. Cycle-level simulations of RISC-V multi-core processors are possible at more than 20 MIPS, a useful middle ground in terms of accuracy and performance with simulation speeds nearly 100 times those of more detailed cycle-accurate models.", "venue": "ArXiv", "authors": ["Xuan  Guo", "Robert  Mullins"], "year": 2020, "n_citations": 2}
{"id": 2250239, "s2_id": "eb796f18efa8202b46e275757138c81bbe511eb9", "title": "Adaptive Domain Model: Dealing With Multiple Attributes of Self-Managing Distributed Object Systems", "abstract": "Self-managing software has emerged as modern systems have become more complex. Some of the distributed object systems may contain thousands of objects deployed on tens or even hundreds hosts. Development and support of such systems often costs a lot. To solve this issue the systems, which are capable supporting multiple self-managing attributes, should be created. In the paper, the Adaptive domain concept is introduced as an extension to the basic domain concept to support a generic adaptation environment for building distributed object systems with multiple self-managing attributes.", "venue": "ISICT", "authors": ["Pavel  Motuzenko"], "year": 2003, "n_citations": 10}
{"id": 2258218, "s2_id": "2bcab8ba9a1fbaa52313a282b49c8ec24ed017d6", "title": "JuxtaPiton: Enabling Heterogeneous-ISA Research with RISC-V and SPARC FPGA Soft-cores", "abstract": "Energy efficiency has become an increasingly important concern in computer architecture due to the end of Dennard scaling. Heterogeneity has been explored as a way to achieve better energy efficiency and heterogeneous microarchitecture chips have become common in the mobile setting. Recent research has explored using heterogeneous-ISA, heterogeneous microarchitecture, general-purpose cores to achieve further energy efficiency gains. However, there is no open-source hardware implementation of a heterogeneous-ISA processor available for research, and effective research on heterogeneous-ISA processors necessitates the emulation speed provided by FPGA prototyping. This work describes our experiences creating JuxtaPiton by integrating a small RISC-V core into the OpenPiton framework, which uses a modified OpenSPARC T1 core. This is the first time a new core has been integrated with the OpenPiton framework, and JuxtaPiton is the first open-source, general-purpose, heterogeneous-ISA processor. JuxtaPiton inherits all the capabilities of OpenPiton, including vital FPGA emulation infrastructure which can boot full-stack Debian Linux. Using this infrastructure, we investigate area and timing effects of using the new RISC-V core on FPGA and the performance of the new core running microbenchmarks.", "venue": "FPGA", "authors": ["Katie  Lim", "Jonathan  Balkind", "David  Wentzlaff"], "year": 2019, "n_citations": 9}
{"id": 2261923, "s2_id": "c2687157e7dc95f20c691b73caac56b2c847cc87", "title": "A Survey of FPGA Based Neural Network Accelerator", "abstract": "Recent researches on neural network have shown significant advantage in machine learning over traditional algorithms based on handcrafted features and models. Neural network is now widely adopted in regions like image, speech and video recognition. But the high computation and storage complexity of neural network inference poses great difficulty on its application. CPU platforms are hard to offer enough computation capacity. GPU platforms are the first choice for neural network process because of its high computation capacity and easy to use development frameworks. \nOn the other hand, FPGA-based neural network inference accelerator is becoming a research topic. With specifically designed hardware, FPGA is the next possible solution to surpass GPU in speed and energy efficiency. Various FPGA-based accelerator designs have been proposed with software and hardware optimization techniques to achieve high speed and energy efficiency. In this paper, we give an overview of previous work on neural network inference accelerators based on FPGA and summarize the main techniques used. An investigation from software to hardware, from circuit level to system level is carried out to complete analysis of FPGA-based neural network inference accelerator design and serves as a guide to future work.", "venue": "ArXiv", "authors": ["Kaiyuan  Guo", "Shulin  Zeng", "Jincheng  Yu", "Yu  Wang", "Huazhong  Yang"], "year": 2017, "n_citations": 110}
{"id": 2265761, "s2_id": "3e7eceb3dfa37c5f215c395d6c377ef8f6d7094e", "title": "An Architecture for Memory Centric Active Storage (MCAS)", "abstract": "The advent of CPU-attached persistent memory technology, such as Intel\u2019s Optane Persistent Memory Modules (PMM), has brought with it new opportunities for storage. In 2018, IBM Research Almaden began investigating and developing a new enterprise-grade storage solution directly aimed at this emerging technology. MCAS (Memory Centric Active Storage) defines an \u201cevolved\u201d network-attached key-value store that offers both near-data compute and the ability to layer enterprise-grade data management services on shared persistent memory. As a converged memory-storage tier, MCAS moves towards eliminating the traditional separation of compute and storage, and thereby unifying the data space. This paper provides an in-depth review of the MCAS architecture and implementation, as well as general performance results.", "venue": "ArXiv", "authors": ["Daniel  Waddington", "Clem  Dickey", "Moshik  Hershcovitch", "Sangeetha  Seshadri"], "year": 2021, "n_citations": 4}
{"id": 2274902, "s2_id": "5902bee57fa1d3229548d43f6f22eb51d8ab9f75", "title": "Compiling Spiking Neural Networks to Mitigate Neuromorphic Hardware Constraints", "abstract": "Spiking Neural Networks (SNNs) are efficient computation models to perform spatio-temporal pattern recognition on resource- and power-constrained platforms. SNNs executed on neuromorphic hardware can further reduce energy consumption of these platforms. With increasing model size and complexity, mapping SNN-based applications to tile-based neuromorphic hardware is becoming increasingly challenging. This is attributed to the limitations of neuro-synaptic cores, viz. a crossbar, to accommodate only a fixed number of pre-synaptic connections per post-synaptic neuron. For complex SNN-based models that have many neurons and pre-synaptic connections per neuron, (1) connections may need to be pruned after training to fit onto the crossbar resources, leading to a loss in model quality, e.g., accuracy, and (2) the neurons and synapses need to be partitioned and placed on the neuro-sypatic cores of the hardware, which could lead to increased latency and energy consumption. In this work, we propose (1) a novel unrolling technique that decomposes a neuron function with many pre-synaptic connections into a sequence of homogeneous neural units to significantly improve the crossbar utilization and retain all pre-synaptic connections, and (2) SpiNeMap, a novel methodology to map SNNs on neuromorphic hardware with an aim to minimize energy consumption and spike latency.", "venue": "2020 11th International Green and Sustainable Computing Workshops (IGSC)", "authors": ["Adarsha  Balaji", "Anup  Das"], "year": 2020, "n_citations": 10}
{"id": 2275760, "s2_id": "08616dcdcb75b5ed3fa99829e1b078505a3495b1", "title": "Automatic Generation of Multi-Precision Multi-Arithmetic CNN Accelerators for FPGAs", "abstract": "Modern deep Convolutional Neural Networks (CNNs) are computationally demanding, yet real applications often require high throughput and low latency. To help tackle these problems, we propose Tomato, a framework designed to automate the process of generating efficient CNN accelerators. The generated design is pipelined and each convolution layer uses different arithmetics at various precisions. Using Tomato, we showcase state-of-the-art multi-precision multi-arithmetic networks, including MobileNet-V1, running on FPGAs. To our knowledge, this is the first multi-precision multi-arithmetic autogeneration framework for CNNs. In software, Tomato fine-tunes pretrained networks to use a mixture of short powers-of-2 and fixed-point weights with a minimal loss in classification accuracy. The fine-tuned parameters are combined with the templated hardware designs to automatically produce efficient inference circuits in FPGAs. We demonstrate how our approach significantly reduces model sizes and computation complexities, and permits us to pack a complete ImageNet network onto a single FPGA without accessing off-chip memories for the first time. Furthermore, we show how Tomato produces implementations of networks with various sizes running on single or multiple FPGAs. To the best of our knowledge, our automatically generated accelerators outperform closest FPGA-based competitors by at least 2-4\u00d7 for lantency and throughput; the generated accelerator runs ImageNet classification at a rate of more than 3000 frames per second.", "venue": "2019 International Conference on Field-Programmable Technology (ICFPT)", "authors": ["Yiren  Zhao", "Xitong  Gao", "Xuan  Guo", "Junyi  Liu", "Erwei  Wang", "Robert  Mullins", "Peter Y. K. Cheung", "George  Constantinides", "Cheng-Zhong  Xu"], "year": 2019, "n_citations": 9}
{"id": 2276946, "s2_id": "a61e184cda4765bba76b4d0b04cedc1bea8627a2", "title": "A Novel RTL ATPG Model Based on Gate Inherent Faults (GIF-PO) of Complex Gates", "abstract": "This paper starts with a comprehensive survey on RTL ATPG. It then proposes a novel RTL ATPG model based on \"Gate Inherent Faults\" (GIF). These GIF are extracted from each complex gate (adder, case-statement, etc.) of the RTL source code individually. They are related to the internal logic paths of a complex gate. They are not related to any net/signal in the RTL design. It is observed, that when all GIF on RTL are covered (100%) and the same stimulus is applied, then all gate level stuck-at faults of the netlist are covered (100%) as well. The proposed RTL ATPG model is therefore synthesis independent. This is shown on ITC'99 testcases. The applied semi-automatic test pattern generation process is based on functional simulation.", "venue": "ArXiv", "authors": ["Tobias  Strauch"], "year": 2016, "n_citations": 0}
{"id": 2280775, "s2_id": "7dac874cb0cc81f1c5bc4dc2021dbd0130aaf0c5", "title": "SARA: Self-Aware Resource Allocation for Heterogeneous MPSoCs", "abstract": "In modern heterogeneous MPSoCs, the management of shared memory resources is crucial in delivering end-to-end QoS. Previous frameworks have either focused on singular QoS targets or the allocation of partitionable resources among CPU applications at relatively slow timescales. However, heterogeneous MPSoCs typically require instant response from the memory system where most resources cannot be partitioned. Moreover, the health of different cores in a heterogeneous MPSoC is often measured by diverse performance objectives. In this work, we propose the Self-Aware Resource Allocation (SARA) framework for heterogeneous MPSoCs. Priority-based adaptation allows cores to use different target performance and self-monitor their own intrinsic health. In response, the system allocates non-partitionable resources based on priorities. The proposed framework meets a diverse range of QoS demands from heterogeneous cores.", "venue": "2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)", "authors": ["Yang  Song", "Olivier  Alavoine", "Bill  Lin"], "year": 2018, "n_citations": 2}
{"id": 2281170, "s2_id": "2087aa3006a526c5887bf822961ce4f263b27811", "title": "AXES: Approximation Manager for Emerging Memory Architectures", "abstract": "Memory approximation techniques are commonly limited in scope, targeting individual levels of the memory hierarchy. Existing approximation techniques for a full memory hierarchy determine optimal configurations at design-time provided a goal and application. Such policies are rigid: they cannot adapt to unknown workloads and must be redesigned for different memory configurations and technologies. We propose AXES: the first self-optimizing runtime manager for coordinating configurable approximation knobs across all levels of the memory hierarchy. AXES continuously updates and optimizes its approximation management policy throughout runtime for diverse workloads. AXES optimizes the approximate memory configuration to minimize power consumption without compromising the quality threshold specified by application developers. AXES can (1) learn a policy at runtime to manage variable application quality of service (QoS) constraints, (2) automatically optimize for a target metric within those constraints, and (3) coordinate runtime decisions for interdependent knobs and subsystems. We demonstrate AXES' ability to efficiently provide functions 1-3 on a RISC-V Linux platform with approximate memory segments in the on-chip cache and main memory. We demonstrate AXES' ability to save up to 37% energy in the memory subsystem without any design-time overhead. We show AXES' ability to reduce QoS violations by 75% with $<5\\%$ additional energy.", "venue": "ArXiv", "authors": ["Biswadip  Maity", "Bryan  Donyanavard", "Anmol  Surhonne", "Amir  Rahmani", "Andreas  Herkersdorf", "Nikil  Dutt"], "year": 2020, "n_citations": 0}
{"id": 2286492, "s2_id": "9fc35d1f41c2158f77f1317dcd8cac8b54daed57", "title": "Tactics to Directly Map CNN Graphs on Embedded FPGAs", "abstract": "Deep convolutional neural networks (CNNs) are the state-of-the-art in image classification. Since CNN feed forward propagation involves highly regular parallel computation, it benefits from a significant speed-up when running on fine grain parallel programmable logic devices. As a consequence, several studies have proposed field-programmable gate array (FPGA)-based accelerators for CNNs. However, because of the large computational power required by CNNs, none of the previous studies has proposed a direct mapping of the CNN onto the physical resources of an FPGA, allocating each processing actor to its own hardware instance. In this letter, we demonstrate the feasibility of the so called direct hardware mapping (DHM) and discuss several tactics we explore to make DHM usable in practice. As a proof of concept, we introduce the HADDOC2 open source tool, that automatically transforms a CNN description into a synthesizable hardware description with platform-independent DHM.", "venue": "IEEE Embedded Systems Letters", "authors": ["K.  Abdelouahab", "M.  Pelcat", "J.  S\u00e9rot", "C.  Bourrasset", "F.  Berry"], "year": 2017, "n_citations": 44}
{"id": 2287727, "s2_id": "06291ef7a3f23ac00380c55c9ef0456e5ea7011c", "title": "Variable Instruction Fetch Rate to Reduce Control Dependent Penalties", "abstract": "In order to overcome the branch execution penalties of hard-to-predict instruction branches, two new instruction fetch micro-architectural methods are proposed in this paper. In addition, to compare performance of the two proposed methods, different instruction fetch policy schemes of existing multi-branch path architectures are evaluated. An improvement in Instructions Per Cycle (IPC) of 29.4% on average over single-thread execution with gshare branch predictor on SPEC 2000/2006 benchmark is shown. In this paper, wide pipeline machines are simulated for evaluation purposes. The methods discussed in this paper can be extended to High Performance Scientific Computing needs, if the demands of IPC improvement are far more critical than $cost.", "venue": "ArXiv", "authors": ["Aswin  Ramachandran", "Louis  Johnson"], "year": 2017, "n_citations": 0}
{"id": 2290148, "s2_id": "d65712a7162694a21d241584e3e921c86f17d3f1", "title": "CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for Embedded Vision Systems and Applications", "abstract": "The field of view (FOV) of convolutional neural networks is highly related to the accuracy of inference. Dilated convolutions are known as an effective solution to the problems which require large FOVs. However, for general-purpose hardware or dedicated hardware, it usually takes extra time to handle dilated convolutions compared with standard convolutions. In this paper, we propose a network module, Cascaded and Separable Structure of Dilated (CASSOD) Convolution, and a special hardware system to handle the CAS-SOD networks efficiently. A CASSOD-Net includes multiple cascaded 2 \u00d7 2 dilated filters, which can be used to replace the traditional 3 \u00d7 3 dilated filters without decreasing the accuracy of inference. Two example applications, face detection and image segmentation, are tested with dilated convolutions and the proposed CASSOD modules. The new network for face detection achieves higher accuracy than the previous work with only 47% of filter weights in the dilated convolution layers of the context module. Moreover, the proposed hardware system can accelerate the computations of dilated convolutions, and it is 2.78 times faster than traditional hardware systems when the filter size is 3 \u00d7 3.", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": ["Tse-Wei  Chen", "Deyu  Wang", "Wei  Tao", "Dongchao  Wen", "Lingxiao  Yin", "Tadayuki  Ito", "Kinya  Osa", "Masami  Kato"], "year": 2021, "n_citations": 0}
{"id": 2293112, "s2_id": "304585f8ebc9c062915661ec29d0ff286e6390b0", "title": "Streaming Architecture for Large-Scale Quantized Neural Networks on an FPGA-Based Dataflow Platform", "abstract": "Deep neural networks (DNNs) are used by different applications that are executed on a range of computer architectures, from IoT devices to supercomputers. The footprint of these networks is huge as well as their computational and communication needs. In order to ease the pressure on resources, research indicates that in many cases a low precision representation (1-2 bit per parameter) of weights and other parameters can achieve similar accuracy while requiring less resources. Using quantized values enables the use of FPGAs to run NNs, since FPGAs are well fitted to these primitives; e.g., FPGAs provide efficient support for bitwise operations and can work with arbitrary-precision representation of numbers. This paper presents a new streaming architecture for running QNNs on FPGAs. The proposed architecture scales out better than alternatives, allowing us to take advantage of systems with multiple FPGAs. We also included support for skip connections, that are used in state-of-the art NNs, and shown that our architecture allows to add those connections almost for free. All this allowed us to implement an 18-layer ResNet for 224\u00d7224 images classification, achieving 57.5% top-1 accuracy. In addition, we implemented a full-sized quantized AlexNet. In contrast to previous works, we use 2-bit activations instead of 1-bit ones, which improves AlexNet's top-1 accuracy from 41.8% to 51.03% for the ImageNet classification. Both AlexNet and ResNet can handle 1000-class real-time classification on an FPGA. Our implementation of ResNet-18 consumes 5\u00d7 less power and is 4\u00d7 slower for ImageNet, when compared to the same NN on the latest Nvidia GPUs. Smaller NNs, that fit a single FPGA, are running faster then on GPUs on small (32\u00d732) inputs, while consuming up to 20\u00d7 less energy and power.", "venue": "2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Chaim  Baskin", "Natan  Liss", "Evgenii  Zheltonozhskii", "Alex M.  Bronstein", "Avi  Mendelson"], "year": 2018, "n_citations": 22}
{"id": 2299746, "s2_id": "cf6df5a001f5058016ba2e0e234bfb539ee051da", "title": "The anatomy of the grid: enabling scalable virtual organizations", "abstract": "\"Grid\" computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high-performance orientation. In this article, we define this new field. First, we review the \"Grid problem,\" which we define as flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources-what we refer to as virtual organizations. In such settings, we encounter unique authentication, authorization, resource access, resource discovery, and other challenges. It is this class of problem that is addressed by Grid technologies. Next, we present an extensible and open Grid architecture, in which protocols, services, application programming interfaces, and software development kits are categorized according to their roles in enabling resource sharing. We describe requirements that we believe any such mechanisms must satisfy, and we discuss the central role played by the intergrid protocols that enable interoperability among different Grid systems. Finally, we discuss how Grid technologies relate to other contemporary technologies, including enterprise integration, application service provider, storage service provider, and peer-to-peer computing. We maintain that Grid concepts and technologies complement and have much to contribute to these other approaches.", "venue": "Proceedings First IEEE/ACM International Symposium on Cluster Computing and the Grid", "authors": ["Ian T. Foster"], "year": 2001, "n_citations": 4031}
{"id": 2301252, "s2_id": "6f537c85b5160a6375306f6eca1a3e8558e7dbd9", "title": "Reconfigurable Hardware Accelerators: Opportunities, Trends, and Challenges", "abstract": "With the emerging big data applications of Machine Learning, Speech Recognition, Artificial Intelligence, and DNA Sequencing in recent years, computer architecture research communities are facing the explosive scale of various data explosion. To achieve high efficiency of data-intensive computing, studies of heterogeneous accelerators which focus on latest applications, have become a hot issue in computer architecture domain. At present, the implementation of heterogeneous accelerators mainly relies on heterogeneous computing units such as Application-specific Integrated Circuit (ASIC), Graphics Processing Unit (GPU), and Field Programmable Gate Array (FPGA). Among the typical heterogeneous architectures above, FPGA-based reconfigurable accelerators have two merits as follows: First, FPGA architecture contains a large number of reconfigurable circuits, which satisfy requirements of high performance and low power consumption when specific applications are running. Second, the reconfigurable architectures of employing FPGA performs prototype systems rapidly and features excellent customizability and reconfigurability. Nowadays, in top-tier conferences of computer architecture, emerging a batch of accelerating works based on FPGA or other reconfigurable architectures. To better review the related work of reconfigurable computing accelerators recently, this survey reserves latest high-level research products of reconfigurable accelerator architectures and algorithm applications as the basis. In this survey, we compare hot research issues and concern domains, furthermore, analyze and illuminate advantages, disadvantages, and challenges of reconfigurable accelerators. In the end, we prospect the development tendency of accelerator architectures in the future, hoping to provide a reference for computer architecture researchers.", "venue": "ArXiv", "authors": ["Chao  Wang", "Wenqi  Lou", "Lei  Gong", "Lihui  Jin", "Luchao  Tan", "Yahui  Hu", "Xi  Li", "Xuehai  Zhou"], "year": 2017, "n_citations": 8}
{"id": 2303461, "s2_id": "3b23c39f21156f9ea86ad8bb2ca53b2cf56b4181", "title": "Predictable Performance and Fairness Through Accurate Slowdown Estimation in Shared Main Memory Systems", "abstract": "This paper summarizes the ideas and key concepts in MISE (Memory Interference-induced Slowdown Estimation), which was published in HPCA 2013 [97], and examines the work's significance and future potential. Applications running concurrently on a multicore system interfere with each other at the main memory. This interference can slow down different applications differently. Accurately estimating the slowdown of each application in such a system can enable mechanisms that can enforce quality-of-service. While much prior work has focused on mitigating the performance degradation due to inter-application interference, there is little work on accurately estimating slowdown of individual applications in a multi-programmed environment. Our goal is to accurately estimate application slowdowns, towards providing predictable performance. \nTo this end, we first build a simple Memory Interference-induced Slowdown Estimation (MISE) model, which accurately estimates slowdowns caused by memory interference. We then leverage our MISE model to develop two new memory scheduling schemes: 1) one that provides soft quality-of-service guarantees, and 2) another that explicitly attempts to minimize maximum slowdown (i.e., unfairness) in the system. Evaluations show that our techniques perform significantly better than state-of-the-art memory scheduling approaches to address the same problems. \nOur proposed model and techniques have enabled significant research in the development of accurate performance models [35, 59, 98, 110] and interference management mechanisms [66, 99, 100, 108, 119, 120].", "venue": "ArXiv", "authors": ["Lavanya  Subramanian", "Vivek  Seshadri", "Yoongu  Kim", "Ben  Jaiyen", "Onur  Mutlu"], "year": 2018, "n_citations": 6}
{"id": 2304538, "s2_id": "a75a9aa15e079fda9175aa1a63dd36a113bcb33f", "title": "A Scalable Near-Memory Architecture for Training Deep Neural Networks on Large In-Memory Datasets", "abstract": "Most investigations into near-memory hardware accelerators for deep neural networks have primarily focused on inference, while the potential of accelerating training has received relatively little attention so far. Based on an in-depth analysis of the key computational patterns in state-of-the-art gradient-based training methods, we propose an efficient near-memory acceleration engine called NTX that can be used to train state-of-the-art deep convolutional neural networks at scale. Our main contributions are: (i) a loose coupling of RISC-V cores and NTX co-processors reducing offloading overhead by <inline-formula><tex-math notation=\"LaTeX\">$7\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>7</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"schuiki-ieq1-2876312.gif\"/></alternatives></inline-formula> over previously published results; (ii) an optimized IEEE 754 compliant data path for fast high-precision convolutions and gradient propagation; (iii) evaluation of near-memory computing with NTX embedded into residual area on the Logic Base die of a Hybrid Memory Cube; and (iv) a scaling analysis to meshes of HMCs in a data center scenario. We demonstrate a <inline-formula><tex-math notation=\"LaTeX\">$2.7\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>7</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"schuiki-ieq2-2876312.gif\"/></alternatives></inline-formula> energy efficiency improvement of NTX over contemporary GPUs at <inline-formula><tex-math notation=\"LaTeX\">$4.4\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>4</mml:mn><mml:mo>.</mml:mo><mml:mn>4</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"schuiki-ieq3-2876312.gif\"/></alternatives></inline-formula> less silicon area, and a compute performance of 1.2 Tflop/s for training large state-of-the-art networks with full floating-point precision. At the data center scale, a mesh of NTX achieves above 95 percent parallel and energy efficiency, while providing <inline-formula><tex-math notation=\"LaTeX\">$2.1\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>2</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"schuiki-ieq4-2876312.gif\"/></alternatives></inline-formula> energy savings or <inline-formula><tex-math notation=\"LaTeX\">$3.1\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"schuiki-ieq5-2876312.gif\"/></alternatives></inline-formula> performance improvement over a GPU-based system.", "venue": "IEEE Transactions on Computers", "authors": ["Fabian  Schuiki", "Michael  Schaffner", "Frank K. G\u00fcrkaynak", "Luca  Benini"], "year": 2019, "n_citations": 42}
{"id": 2309410, "s2_id": "27e2990a580a216393f4b7686fb20c91d20d4ca1", "title": "TinyML: Analysis of Xtensa LX6 microprocessor for Neural Network Applications by ESP32 SoC", "abstract": "In recent decades, Machine Learning (ML) has become extremely important for many computing applications. The pervasiveness of ultra-low-power embedded devices such as ESP32 or ESP32 Cam with tiny Machine Learning (tinyML) applications will enable the mass proliferation of Artificial Intelligent powered Embedded IoT Devices. In the last few years, the microcontroller device (Espressif ESP32) became powerful enough to be used for small/tiny machine learning (tinyML) tasks. The ease of use of platforms like Arduino IDE, MicroPython and TensorFlow Lite (TF) with tinyML application make it an indispensable topic of research for mobile robotics, modern computer science and electrical engineering. The goal of this paper is to analyze the speed of the Xtensa dual core 32-bit LX6 microprocessor by running a neural network application. The different number of inputs (9, 36, 144 and 576) inputted through the different number of neurons in neural networks with one and two hidden layers. Xtensa LX6 microprocessor has been analyzed because it comes inside with Espressif ESP32 and ESP32 Cam which are very easy to use, plug and play IoT device. In this paper speed of the Xtensa LX6 microprocessor in feed-forward mode has been analyzed. Keywords\u2014 TinyML, Xtensa LX6 microprocessor, Machine Learning, Neural Network, Embedded IoT Device, Espressif ESP32 and ESP32 Cam.", "venue": "ArXiv", "authors": ["Md Ziaul Haque Zim"], "year": 2021, "n_citations": 0}
{"id": 2313374, "s2_id": "d7a0de77a7a25943cb99d8301a839cfe54bc6e7b", "title": "A Scalable Multicore Architecture With Heterogeneous Memory Structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)", "abstract": "Neuromorphic computing systems comprise networks of neurons that use asynchronous events for both computation and communication. This type of representation offers several advantages in terms of bandwidth and power consumption in neuromorphic electronic systems. However, managing the traffic of asynchronous events in large scale systems is a daunting task, both in terms of circuit complexity and memory requirements. Here, we present a novel routing methodology that employs both hierarchical and mesh routing strategies and combines heterogeneous memory structures for minimizing both memory requirements and latency, while maximizing programming flexibility to support a wide range of event-based neural network architectures, through parameter configuration. We validated the proposed scheme in a prototype multicore neuromorphic processor chip that employs hybrid analog/digital circuits for emulating synapse and neuron dynamics together with asynchronous digital circuits for managing the address-event traffic. We present a theoretical analysis of the proposed connectivity scheme, describe the methods and circuits used to implement such scheme, and characterize the prototype chip. Finally, we demonstrate the use of the neuromorphic processor with a convolutional neural network for the real-time classification of visual symbols being flashed to a dynamic vision sensor\u00a0(DVS) at high speed.", "venue": "IEEE Transactions on Biomedical Circuits and Systems", "authors": ["Saber  Moradi", "Ning  Qiao", "Fabio  Stefanini", "Giacomo  Indiveri"], "year": 2018, "n_citations": 226}
{"id": 2314498, "s2_id": "4fa1e128b6b56396bff6da73ec64812757eaf7e1", "title": "Mitigating Edge Machine Learning Inference Bottlenecks: An Empirical Study on Accelerating Google Edge Models", "abstract": "As the need for edge computing grows, many modern consumer devices now contain edge machine learning (ML) accelerators that can compute a wide range of neural network (NN) models while still fitting within tight resource constraints. We analyze a commercial Edge TPU using 24 Google edge NN models (including CNNs, LSTMs, transducers, and RCNNs), and find that the accelerator suffers from three shortcomings, in terms of computational throughput, energy efficiency, and memory access handling. We comprehensively study the characteristics of each NN layer in all of the Google edge models, and find that these shortcomings arise from the one-size-fits-all approach of the accelerator, as there is a high amount of heterogeneity in key layer characteristics both across different models and across different layers in the same model. We propose a new acceleration framework called Mensa. Mensa incorporates multiple heterogeneous ML edge accelerators (including both on-chip and near-data accelerators), each of which caters to the characteristics of a particular subset of models. At runtime, Mensa schedules each layer to run on the best-suited accelerator, accounting for both efficiency and inter-layer dependencies. As we analyze the Google edge NN models, we discover that all of the layers naturally group into a small number of clusters, which allows us to design an efficient implementation of Mensa for these models with only three specialized accelerators. Averaged across all 24 Google edge models, Mensa improves energy efficiency and throughput by 3.0x and 3.1x over the Edge TPU, and by 2.4x and 4.3x over Eyeriss v2, a state-of-the-art accelerator.", "venue": "ArXiv", "authors": ["Amirali  Boroumand", "Saugata  Ghose", "Berkin  Akin", "Ravi  Narayanaswami", "Geraldo F. Oliveira", "Xiaoyu  Ma", "Eric  Shiu", "Onur  Mutlu"], "year": 2021, "n_citations": 6}
{"id": 2316189, "s2_id": "739200fcd79e00cc48159320614dc89e9e2a2be7", "title": "How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures", "abstract": "The unprecedented performance of deep neural networks (DNNs) has led to large strides in various Artificial Intelligence (AI) inference tasks, such as object and speech recognition. Nevertheless, deploying such AI models across commodity devices faces significant challenges: large computational cost, multiple performance objectives, hardware heterogeneity and a common need for high accuracy, together pose critical problems to the deployment of DNNs across the various embedded and mobile devices in the wild. As such, we have yet to witness the mainstream usage of state-of-the-art deep learning algorithms across consumer devices. In this paper, we provide preliminary answers to this potentially game-changing question by presenting an array of design techniques for efficient AI systems. We start by examining the major roadblocks when targeting both programmable processors and custom accelerators. Then, we present diverse methods for achieving real-time performance following a cross-stack approach. These span model-, system- and hardware-level techniques, and their combination. Our findings provide illustrative examples of AI systems that do not overburden mobile hardware, while also indicating how they can improve inference accuracy. Moreover, we showcase how custom ASIC- and FPGA-based accelerators can be an enabling factor for next-generation AI applications, such as multi-DNN systems. Collectively, these results highlight the critical need for further exploration as to how the various cross-stack solutions can be best combined in order to bring the latest advances in deep learning close to users, in a robust and efficient manner.", "venue": "2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Stylianos I. Venieris", "Ioannis  Panopoulos", "Ilias  Leontiadis", "Iakovos S. Venieris"], "year": 2021, "n_citations": 1}
{"id": 2316608, "s2_id": "9a32f635334b06407a8caa79b3ef9a86c152253d", "title": "Real-time Detection and Adaptive Mitigation of Power-based Side-Channel Leakage in SoC", "abstract": "Power-based side-channel is a serious security threat to the System on Chip (SoC). The secret information is leaked from the power profile of the system while a cryptographic algorithm is running. The mitigation requires efforts from both the software level and hardware level. Currently, there is no comprehensive solution that can guarantee the whole complex system is free of leakage and can generically protect all cryptographic algorithms. In this paper, we propose a real-time leakage detection and mitigation system which enables the system to monitor the side-channel leakage effects of the hardware. Our proposed system has extensions that provide a real-time monitor of power consumption, detection of side-channel leakage, and real-time adaptive mitigation of detected side-channel leakage. Our proposed system is generic and can protect any algorithm running on it.", "venue": "ArXiv", "authors": ["Pantea  Kiaei", "Yuan  Yao", "Patrick  Schaumont"], "year": 2021, "n_citations": 1}
{"id": 2318130, "s2_id": "e3695e70c01ecea526ecf30f4d2c4e71d7930e24", "title": "Efficient Neuromorphic Signal Processing with Loihi 2", "abstract": "The biologically inspired spiking neurons used in neuromorphic computing are nonlinear filters with dynamic state variables\u2014very different from the stateless neuron models used in deep learning. The next version of Intel's neuromorphic research processor, Loihi 2, supports a wide range of stateful spiking neuron models with fully programmable dynamics. Here we showcase advanced spiking neuron models that can be used to efficiently process streaming data in simulation experiments on emulated Loihi 2 hardware. In one example, Resonate-and-Fire (RF) neurons are used to compute the Short Time Fourier Transform (STFT) with similar computational complexity but 47x less output bandwidth than the conventional STFT. In another example, we describe an algorithm for optical flow estimation using spatiotemporal RF neurons that requires over 90x fewer operations than a conventional DNN-based solution. We also demonstrate promising preliminary results using backpropagation to train RF neurons for audio classification tasks. Finally, we show that a cascade of Hopf resonators\u2014a variant of the RF neuron\u2014replicates novel properties of the cochlea and motivates an efficient spike-based spectrogram encoder.", "venue": "2021 IEEE Workshop on Signal Processing Systems (SiPS)", "authors": ["Garrick  Orchard", "E. Paxon Frady", "Daniel Ben Dayan Rubin", "Sophia  Sanborn", "Sumit Bam Shrestha", "Friedrich T. Sommer", "Mike  Davies"], "year": 2021, "n_citations": 0}
{"id": 2319617, "s2_id": "b857641b712644eb44d4befb90bbee0b0b2ffcdb", "title": "SAPA: Self-Aware Polymorphic Architecture", "abstract": "In this work, we introduce a Self-Aware Polymorphic Architecture (SAPA) design approach to support emerging context-aware applications and mitigate the programming challenges caused by the ever-increasing complexity and heterogeneity of high performance computing systems. Through the SAPA design, we examined the salient software-hardware features of adaptive computing systems that allow for (1) the dynamic allocation of computing resources depending on program needs (e.g., the amount of parallelism in the program) and (2) automatic approximation to meet program and system goals (e.g., execution time budget, power constraints and computation resiliency) without the programming complexity of current multicore and many-core systems. The proposed adaptive computer architecture framework applies machine learning algorithms and control theory techniques to the application execution based on information collected about the system runtime performance trade-offs. It has heterogeneous reconfigurable cores with fast hardware-level migration capability, self-organizing memory structures and hierarchies, an adaptive application-aware network-on-chip, and a built-in hardware layer for dynamic, autonomous resource management. Our prototyped architecture performs extremely well on a large pool of applications.", "venue": "ArXiv", "authors": ["Michel A. Kinsy", "Mihailo  Isakov", "Alan  Ehret", "Donato  Kava"], "year": 2018, "n_citations": 0}
{"id": 2320271, "s2_id": "510d38eafeb19fa7321d40b28e9ded2eca0003b6", "title": "The nanoPU: Redesigning the CPU-Network Interface to Minimize RPC Tail Latency", "abstract": "The nanoPU is a new networking-optimized CPU designed to minimize tail latency for RPCs. By bypassing the cache and memory hierarchy, the nanoPU directly places arriving messages into the CPU register file. The wire-to-wire latency through the application is just 65ns, about 13x faster than the current state-of-the-art. The nanoPU moves key functions from software to hardware: reliable network transport, congestion control, core selection, and thread scheduling. It also supports a unique feature to bound the tail latency experienced by high-priority applications. Our prototype nanoPU is based on a modified RISC-V CPU; we evaluate its performance using cycle-accurate simulations of 324 cores on AWS FPGAs, including real applications (MICA and chain replication).", "venue": "ArXiv", "authors": ["Stephen  Ibanez", "Alex  Mallery", "Serhat  Arslan", "Theo  Jepsen", "Muhammad  Shahbaz", "Nick  McKeown", "Changhoon  Kim"], "year": 2020, "n_citations": 7}
{"id": 2321889, "s2_id": "0375b459c046b6f1bd935c5b4d0f9fad8c02273e", "title": "Assertion-based design exploration of DVS in network processor architectures", "abstract": "With the scaling of technology and higher requirements on performance and functionality, power dissipation is becoming one of the major design considerations in the development of network processors. We use an assertion-based methodology for system-level power/performance analysis to study two dynamic voltage scaling (DVS) techniques, traffic-based DVS and execution-based DVS, in a network processor model. Using the automatically generated distribution analyzers, we analyze the power and performance distributions and study their trade-offs for the two DVS policies with different parameter settings, such as threshold values and window sizes. We discuss the optimal configurations of the two DVS policies under different design requirements. By a set of experiments, we show that the assertion-based trace analysis methodology is an efficient tool that can help a designer easily compare and study optimal architectural configurations in a large design space.", "venue": "Design, Automation and Test in Europe", "authors": ["Jia  Yu", "Wei  Wu", "Xi  Chen", "Harry  Hsieh", "Jun  Yang", "Felice  Balarin"], "year": 2005, "n_citations": 1}
{"id": 2324918, "s2_id": "f04719ae0819f6c29e49a541ab49356a56345d3a", "title": "Mining Secure Behavior of Hardware Designs", "abstract": "Specification mining offers a solution by automating security specification for hardware. Specification miners use a form of machine learning to specify behaviors of a system by studying a system in execution. However, specification mining was first developed for use with software. Complex hardware designs offer unique challenges for this technique. Further, specification miners traditionally capture functional specifications without a notion of security, and may not use the specification logics necessary to describe some security requirements. This work demonstrates specification mining for hardware security. On CISC architectures such as x86, I demonstrate that a miner partitioning the design state space along control signals discovers a specification that includes manually defined properties and, if followed, would secure CPU designs against Memory Sinkhole and SYSRET privilege escalation. For temporal properties, I demonstrate that a miner using security specific linear temporal logic (LTL) templates for specification detection may find properties that, if followed, would secure designs against historical documented security vulnerabilities and against potential future attacks targeting system initialization. For information--flow hyperproperties, I demonstrate that a miner may use Information Flow Tracking (IFT) to develop output properties containing designer specified information--flow security properties as well as properties that demonstrate a design does not contain certain Common Weakness Enumerations (CWEs).", "venue": "ArXiv", "authors": ["Calvin  Deutschbein"], "year": 2021, "n_citations": 0}
{"id": 2325556, "s2_id": "f90f526b101cb8a0260f5165a3875928c58ae48a", "title": "NATSA: A Near-Data Processing Accelerator for Time Series Analysis", "abstract": "Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, and more. Matrix profile, the state-of-the-art algorithm to perform time series analysis, computes the most similar subsequence for a given query subsequence within a sliced time series. Matrix profile has low arithmetic intensity, but it typically operates on large amounts of time series data. In current computing systems, this data needs to be moved between the off-chip memory units and the on-chip computation units for performing matrix profile. This causes a major performance bottleneck as data movement is extremely costly in terms of both execution time and energy. In this work, we present NATSA, the first Near-Data Processing accelerator for time series analysis. The key idea is to exploit modern 3D-stacked High Bandwidth Memory (HBM) to enable efficient and fast specialized matrix profile computation near memory, where time series data resides. NATSA provides three key benefits: 1) quickly computing the matrix profile for a wide range of applications by building specialized energy-efficient floating-point arithmetic processing units close to HBM, 2) improving the energy efficiency and execution time by reducing the need for data movement over slow and energy-hungry buses between the computation units and the memory units, and 3) analyzing time series data at scale by exploiting low-latency, high-bandwidth, and energy-efficient memory access provided by HBM. Our experimental evaluation shows that NATSA improves performance by up to 14.2\u00d7 (9.9\u00d7 on average) and reduces energy by up to 27.2 \u00d7 (19.4 \u00d7 on average), over the state-of-the-art multi-core implementation. NATSA also improves performance by 6.3 \u00d7 and reduces energy by 10.2 \u00d7 over a general-purpose NDP platform with 64 in-order cores.", "venue": "2020 IEEE 38th International Conference on Computer Design (ICCD)", "authors": ["Ivan  Fernandez", "Ricardo  Quislant", "Christina  Giannoula", "Mohammed  Alser", "Juan  G'omez-Luna", "Eladio  Guti'errez", "Oscar  Plata", "Onur  Mutlu"], "year": 2020, "n_citations": 16}
{"id": 2330476, "s2_id": "2c8158a360fb10fbb71d3c3cc618b10cb78a8095", "title": "Transient Execution of Non-Canonical Accesses", "abstract": "Recent years have brought microarchitectural security into the spotlight, proving that modern CPUs are vulnerable to several classes of microarchitectural attacks. These attacks bypass the basic isolation primitives provided by the CPUs: process isolation, memory permissions, access checks, and so on. Nevertheless, most of the research was focused on Intel CPUs, with only a few exceptions. As a result, few vulnerabilities have been found in other CPUs, leading to speculations about their immunity to certain types of microarchitectural attacks. In this paper, we provide a black-box analysis of one of these under-explored areas. Namely, we investigate the flaw of AMD CPUs which may lead to a transient execution hijacking attack. Contrary to nominal immunity, we discover that AMDZen family CPUs exhibit transient execution patterns similar for Meltdown/MDS. Our analysis of exploitation possibilities shows that AMDs design decisions indeed limit the exploitability scope comparing to Intel CPUs, yet it may be possible to use them to amplify other microarchitectural attacks.", "venue": "ArXiv", "authors": ["Saidgani  Musaev", "Christof  Fetzer"], "year": 2021, "n_citations": 1}
{"id": 2331988, "s2_id": "33001234d9fe7e230e3ce9d8371347e935dc46d7", "title": "Solving Large Top-K Graph Eigenproblems with a Memory and Compute-optimized FPGA Design", "abstract": "Large-scale eigenvalue computations on sparse matrices are a key component of graph analytics techniques based on spectral methods. In such applications, an exhaustive computation of all eigenvalues and eigenvectors is impractical and unnecessary, as spectral methods can retrieve the relevant properties of enormous graphs using just the eigenvectors associated with the Top-K largest eigenvalues.In this work, we propose a hardware-optimized algorithm to approximate a solution to the Top-K eigenproblem on sparse matrices representing large graph topologies. We prototype our algorithm through a custom FPGA hardware design that exploits HBM, Systolic Architectures, and mixed-precision arithmetic. We achieve a speedup of 6.22x compared to the highly optimized ARPACK library running on an 80-thread CPU, while keeping high accuracy and 49x better power efficiency.", "venue": "2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Francesco  Sgherzi", "Alberto  Parravicini", "Marco  Siracusa", "Marco D. Santambrogio"], "year": 2021, "n_citations": 0}
{"id": 2346065, "s2_id": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning", "abstract": "The attention mechanism is becoming increasingly popular in Natural Language Processing (NLP) applications, showing superior performance than convolutional and recurrent architectures. However, general-purpose platforms such as CPUs and GPUs are inefficient when performing attention inference due to complicated data movement and low arithmetic intensity. Moreover, existing NN accelerators mainly focus on optimizing convolutional or recurrent models, and cannot efficiently support attention. In this paper, we present SpAtten, an efficient algorithm-architecture co-design that leverages token sparsity, head sparsity, and quantization opportunities to reduce the attention computation and memory access. Inspired by the high redundancy of human languages, we propose the novel cascade token pruning to prune away unimportant tokens in the sentence. We also propose cascade head pruning to remove unessential heads. Cascade pruning is fundamentally different from weight pruning since there is no trainable weight in the attention mechanism, and the pruned tokens and heads are selected on the fly. To efficiently support them on hardware, we design a novel top-k engine to rank token and head importance scores with high throughput. Furthermore, we propose progressive quantization that first fetches MSBs only and performs the computation; if the confidence is low, it fetches LSBs and recomputes the attention outputs, trading computation for memory reduction.Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces DRAM access by 10.0\u00d7 with no accuracy loss, and achieves 1.6\u00d7, 3.0\u00d7, 162\u00d7, 347\u00d7 speedup, and 1.4\u00d7, 3.2\u00d7, 1193\u00d7, 4059\u00d7 energy savings over A3 accelerator, MNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Hanrui  Wang", "Zhekai  Zhang", "Song  Han"], "year": 2021, "n_citations": 21}
{"id": 2346671, "s2_id": "120a91f496eeefe6f5300aa6e691a2f5713d6b65", "title": "S4oC: A Self-Optimizing, Self-Adapting Secure System-on-Chip Design Framework to Tackle Unknown Threats \u2014 A Network Theoretic, Learning Approach", "abstract": "We propose a framework for the design and optimization of a secure self-optimizing, self-adapting system-on-chip (S4oC) architecture. The goal is to minimize the impact of attacks such as hardware Trojan and side-channel, by making real-time adjustments. S4oC learns to reconfigure itself, subject to various security measures and attacks, some of which possibly unknown at design time. Furthermore, the data types and patterns of the target applications, environmental conditions, and sources of variations are incorporated. S4oC is a manycore system, modeled as a four-layer graph, representing the model of computation (MoCp), model of connection (MoCn), model of memory (MoM) and model of storage (MoS), with a large number of elements including heterogeneous reconfigurable processing elements in MoCp, and memory elements in the MoM layer. Security driven community detection, and neural networks are utilized for application task clustering, and distributed reinforcement learning (RL) for task mapping.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Shahin  Nazarian", "Paul  Bogdan"], "year": 2020, "n_citations": 1}
{"id": 2347702, "s2_id": "be167b7999e4fdffde98e0ba7e35c77a06dfebaa", "title": "An Ultra-Efficient Memristor-Based DNN Framework with Structured Weight Pruning and Quantization Using ADMM", "abstract": "The high computation and memory storage of large deep neural networks (DNNs) models pose intensive challenges to the conventional Von-Neumann architecture, incurring sub-stantial data movements in the memory hierarchy. The memristor crossbar array has emerged as a promising solution to mitigate the challenges and enable low-power acceleration of DNNs. Memristor-based weight pruning and weight quantization have been seperately investigated and proven effectiveness in reducing area and power consumption compared to the original DNN model. However, there has been no systematic investigation of memristor-based neuromorphic computing (NC) systems considering both weight pruning and weight quantization. In this paper, we propose an unified and systematic memristor-based framework considering both structured weight pruning and weight quantization by incorporating alternating direction method of multipliers (ADMM) into DNNs training. We consider hardware constraints such as crossbar blocks pruning, conductance range, and mismatch between weight value and real devices, to achieve high accuracy and low power and small area footprint. Our framework is mainly integrated by three steps, i.e., memristor-based ADMM regularized optimization, masked mapping and retraining. Experimental results show that our proposed framework achieves 29.81\u00d7 (20.88\u00d7) weight compression ratio, with 98.38% (96.96%) and 98.29% (97.47%) power and area reduction on VGG-16 (ResNet-18) network where only have 0.5% (0.76%) accuracy loss, compared to the original DNN models. We share our models at anonymous link http://bit.ly/2Jp5LHJ.", "venue": "2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)", "authors": ["Geng  Yuan", "Xiaolong  Ma", "Caiwen  Ding", "Sheng  Lin", "Tianyun  Zhang", "Zeinab S. Jalali", "Yilong  Zhao", "Li  Jiang", "Sucheta  Soundarajan", "Yanzhi  Wang"], "year": 2019, "n_citations": 15}
{"id": 2347788, "s2_id": "1231e59b04a025357d2cbc73c9887960024b02e9", "title": "Reducing Performance Impact of DRAM Refresh by Parallelizing Refreshes with Accesses", "abstract": "1. Summary 1.1. The Problem DRAM requires periodic refresh to prevent data loss from charge leakage. There exists two main refresh methods employed in the majority of today\u2019s DRAM systems. The first method is to carry out refresh operations at the rank level, called all-bank refresh (REFab), which is mainly used by commodity DDR DRAM [6]. Because all-bank refresh prevents all banks within an entire DRAM rank from serving memory requests, it significantly degrades performance. The second method is to perform refreshes at the bank level, called per-bank refresh (REFpb), which is currently supported in LPDDR DRAM used in mobile platforms [7]. In contrast to REFab, REFpb enables a bank to be accessed while another bank is being refreshed, alleviating part of the negative performance impact of refresh. Unfortunately, there are two shortcomings of per-bank refresh. First, refreshes to different banks are scheduled in a strict round-robin order as specified by the LPDDR standard [7]. Using this static policy may force a busy bank to be refreshed, delaying the memory requests queued in that bank, while other idle banks are available to be refreshed. Second, refreshing banks cannot concurrently serve memory requests. Furthermore, the negative performance impact of DRAM refresh becomes exacerbated as DRAM density increases in the future. Figure 1 shows the average performance degradation of allbank/per-bank refresh compared to ideal baseline without any refreshes. 1 Although REFpb performs slightly better than REFab, the performance loss is still significant, especially as the density grows (16.6% loss at 32Gb). Therefore, the goal of our paper [1] is to provide practical mechanisms to overcome these two shortcomings to mitigate the performance overhead of DRAM refresh.", "venue": "ArXiv", "authors": ["Kevin Kai-Wei Chang", "Donghyuk  Lee", "Zeshan  Chishti", "Alaa R. Alameldeen", "Chris  Wilkerson", "Yoongu  Kim", "Onur  Mutlu"], "year": 2016, "n_citations": 0}
{"id": 2348411, "s2_id": "d71c6301354676928b1c38dcdab0238e8a554e7c", "title": "Cross-Layer Optimization for Power-Efficient and Robust Digital Circuits and Systems", "abstract": "With the increasing digital services demand, performance and power-efficiency become vital requirements for digital circuits and systems. However, the enabling CMOS technology scaling has been facing significant challenges of device uncertainties, such as process, voltage, and temperature variations. To ensure system reliability, worst-case corner assumptions are usually made in each design level. However, the over-pessimistic worst-case margin leads to unnecessary power waste and performance loss as high as 2.2x. Since optimizations are traditionally confined to each specific level, those safe margins can hardly be properly exploited. \nTo tackle the challenge, it is therefore advised in this Ph.D. thesis to perform a cross-layer optimization for digital signal processing circuits and systems, to achieve a global balance of power consumption and output quality. \nTo conclude, the traditional over-pessimistic worst-case approach leads to huge power waste. In contrast, the adaptive voltage scaling approach saves power (25% for the CORDIC application) by providing a just-needed supply voltage. The power saving is maximized (46% for CORDIC) when a more aggressive voltage over-scaling scheme is applied. These sparsely occurred circuit errors produced by aggressive voltage over-scaling are mitigated by higher level error resilient designs. For functions like FFT and CORDIC, smart error mitigation schemes were proposed to enhance reliability (soft-errors and timing-errors, respectively). Applications like Massive MIMO systems are robust against lower level errors, thanks to the intrinsically redundant antennas. This property makes it applicable to embrace digital hardware that trades quality for power savings.", "venue": "ArXiv", "authors": ["Yanxiang  Huang"], "year": 2017, "n_citations": 0}
{"id": 2348907, "s2_id": "a39b5a163b0544ec382540839685d8ebfc64849b", "title": "SPA-GCN: Efficient and Flexible GCN Accelerator with an Application for Graph Similarity Computation", "abstract": "While there have been many studies on hardware acceleration for deep learning on images, there has been a rather limited focus on accelerating deep learning applications involving graphs. The unique characteristics of graphs, such as the irregular memory access and dynamic parallelism, impose several challenges when the algorithm is mapped to a CPU or GPU. To address these challenges while exploiting all the available sparsity, we propose a flexible architecture called SPA-GCN for accelerating Graph Convolutional Networks (GCN), the core computation unit in deep learning algorithms on graphs. The architecture is specialized for dealing with many small graphs since the graph size has a significant impact on design considerations. In this context, we use SimGNN, a neural-network-based graph matching algorithm, as a case study to demonstrate the effectiveness of our architecture. The experimental results demonstrate that SPA-GCN can deliver a high speedup compared to a multi-core CPU implementation and a GPU implementation, showing the efficiency of our design.", "venue": "ArXiv", "authors": ["Atefeh  Sohrabizadeh", "Yuze  Chi", "Jason  Cong"], "year": 2021, "n_citations": 0}
{"id": 2351786, "s2_id": "4c4fa5251d28a1fd90686886e983bca433545195", "title": "Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into Cognizance", "abstract": "In recent years, researchers have focused on reducing the model size and number of computations (measured as \u201cmultiply-accumulate\u201d or MAC operations) of DNNs. The energy consumption of a DNN depends on both the number of MAC operations and the energy efficiency of each MAC operation. The former can be estimated at design time; however, the latter depends on the intricate data reuse patterns and underlying hardware architecture. Hence, estimating it at design time is challenging. This article shows that the conventional approach to estimate the data reuse, viz. arithmetic intensity, does not always correctly estimate the degree of data reuse in DNNs since it gives equal importance to all the data types. We propose a novel model, termed \u201cdata type aware weighted arithmetic intensity\u201d (<inline-formula><tex-math notation=\"LaTeX\">$DI$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>D</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"jha-ieq1-3015531.gif\"/></alternatives></inline-formula>), which accounts for the unequal importance of different data types in DNNs. We evaluate our model on 25 state-of-the-art DNNs on two GPUs. We show that our model accurately models data-reuse for all possible data reuse patterns for different types of convolution and different types of layers. We show that our model is a better indicator of the energy efficiency of DNNs. We also show its generality using the central limit theorem.", "venue": "IEEE Transactions on Computers", "authors": ["Nandan Kumar Jha", "Sparsh  Mittal"], "year": 2021, "n_citations": 9}
{"id": 2353438, "s2_id": "c5eaa115790879bace056b924b71cae21a7599c1", "title": "Cognitive Computing in Data-centric Paradigm", "abstract": "Knowledge is the most precious asset of humankind. People extract the experience from the data that provide for us the reality through the feelings. Generally speaking, it is possible to see the analogy of knowledge elaboration between humankind\u2019s way and the artificial system\u2019s way. Digital data are the \"feelings\" of an artificial system, and it needs to invent a method of extraction of knowledge from the Universe of data. The cognitive computing paradigm implies that a system should be able to extract the knowledge from raw data without any human-made algorithm. The first step of the paradigm is analysis of raw data streams through the discovery of repeatable patterns of data. The knowledge of relationships among the patterns provides a way to see the structures and to generalize the concepts with the goal to synthesize new statements. The cognitive computing paradigm is capable of mimicking the human\u2019s ability to generalize the notions. It is possible to say that the generalization step provides the basis for discovering the abstract notions, revealing the abstract relations of patterns and general rules of structure synthesis. If anyone continues the process of structure generalization, then it is possible to build the multi-level hierarchy of abstract notions. Moreover, discovering the generalized classes of notions is the first step towards a paradigm of artificial analytical thinking. The most critical possible responsibility of cognitive computing could be the classification of data and recognition of input data stream\u2019s states. The synthesis of new statements creates the foundation for the foreseeing the possible data states and elaboration of knowledge about new data classes by employing synthesis and checking the hypothesis.", "venue": "ArXiv", "authors": ["Viacheslav  Dubeyko"], "year": 2020, "n_citations": 0}
{"id": 2355543, "s2_id": "a29c2ecd113242f7103413d86f29560feab10a61", "title": "Structured Weight Matrices-Based Hardware Accelerators in Deep Neural Networks: FPGAs and ASICs", "abstract": "Both industry and academia have extensively investigated hardware accelerations. To address the demands in increasing computational capability and memory requirement, in this work, we propose the structured weight matrices (SWM)-based compression technique for both Field Programmable Gate Array (FPGA) and application-specific integrated circuit (ASIC) implementations. In the algorithm part, the SWM-based framework adopts block-circulant matrices to achieve a fine-grained tradeoff between accuracy and compression ratio. The SWM-based technique can reduce computational complexity from O(n2) to O(nlog n) and storage complexity from O(n2) to O(n) for each layer and both training and inference phases. For FPGA implementations on deep convolutional neural networks (DCNNs), we achieve at least 152X and 72X improvement in performance and energy efficiency, respectively using the SWM-based framework, compared with the baseline of IBM TrueNorth processor under same accuracy constraints using the data set of MNIST, SVHN, and CIFAR-10. For FPGA implementations on long short term memory (LSTM) networks, the proposed SWM-based LSTM can achieve up to 21X enhancement in performance and 33.5X gains in energy efficiency compared with the ESE accelerator. For ASIC implementations, the proposed SWM-based ASIC design exhibits impressive advantages in terms of power, throughput, and energy efficiency. Experimental results indicate that this method is greatly suitable for applying DNNs onto both FPGAs and mobile/IoT devices.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Caiwen  Ding", "Ao  Ren", "Geng  Yuan", "Xiaolong  Ma", "Jiayu  Li", "Ning  Liu", "Bo  Yuan", "Yanzhi  Wang"], "year": 2018, "n_citations": 16}
{"id": 2358887, "s2_id": "687262b855a5b65d1abbc3a6f8029b7d0a11be31", "title": "SIAM: Chiplet-based Scalable In-Memory Acceleration with Mesh for Deep Neural Networks", "abstract": "In-memory computing (IMC) on a monolithic chip for deep learning faces dramatic challenges on area, yield, and on-chip interconnection cost due to the ever-increasing model sizes. 2.5D integration or chiplet-based architectures interconnect multiple small chips (i.e., chiplets) to form a large computing system, presenting a feasible solution beyond a monolithic IMC architecture to accelerate large deep learning models. This paper presents a new benchmarking simulator, SIAM, to evaluate the performance of chiplet-based IMC architectures and explore the potential of such a paradigm shift in IMC architecture design. SIAM integrates device, circuit, architecture, network-on-chip (NoC), network-on-package (NoP), and DRAM access models to realize an end-to-end system. SIAM is scalable in its support of a wide range of deep neural networks (DNNs), customizable to various network structures and configurations, and capable of efficient design space exploration. We demonstrate the flexibility, scalability, and simulation speed of SIAM by benchmarking different state-of-the-art DNNs with CIFAR-10, CIFAR-100, and ImageNet datasets. We further calibrate the simulation results with a published silicon result, SIMBA. The chiplet-based IMC architecture obtained through SIAM shows 130 and 72 improvement in energy-efficiency for ResNet-50 on the ImageNet dataset compared to Nvidia V100 and T4 GPUs.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Gokul  Krishnan", "Sumit K. Mandal", "Manvitha  Pannala", "Chaitali  Chakrabarti", "Jae-sun  Seo", "\u00dcmit Y. Ogras", "Yu  Cao"], "year": 2021, "n_citations": 0}
{"id": 2358919, "s2_id": "10f6d31fd607b0986f628e106f27aee8f2e5cbe5", "title": "A Linearization Technique for Self-Interference Cancellation in Full-Duplex Radios", "abstract": "The fundamental problem in the design of a full-duplex radio is the cancellation of the self-interference (SI) signal generated by the transmitter.Current techniques for suppressing SI rely on generating a copy of the SI signal and subtracting it partly in the RF (radio frequency) and digital domains. A critical step in replicating the self-interference is the estimation of the multi-path channel through which the transmitted signal propagates to the antenna. Since there is no prior model on the number of multipath reflections, current techniques assume a tap delay line filter (in the RF and digital domain) with a large number of taps, and estimate the taps in the analog and the digital domain. Assuming such a model leads to a large form-factor for the analog and RF circuits and increased complexity in the digital domain. \nIn this paper, using a linearization technique, we show that the self-interference channel in an indoor environment can be effectively modelled as $H(f)=C_0 + C_1f$ in the frequency domain. Thus, the effective self-interference channel can be represented by two parameters $C_0$ and $C_1$, irrespective of the multipath environment. We also provide experimental evidence to verify the above channel model and propose novel low-complexity designs for self-interference cancellation. Linearization not only aids in the practicality of analog cancellation by reducing the form factor, but also results in a simpler SI filter model in the digital domain due to dimensionality reduction of the channel parameters. Therefore this method can enable the widespread adoption of full-duplex techniques to portable devices in addition to infrastructure base-stations.", "venue": "ArXiv", "authors": ["Arjun  Nadh", "Samuel  Joseph", "Ankit  Sharma", "Sankaran  Aniruddhan", "Radha Krishna Ganti"], "year": 2016, "n_citations": 7}
{"id": 2359227, "s2_id": "f14275042014a00201edd43d1fbecd20a4917d19", "title": "The DEEP-ER Project: I/O and Resiliency Extensions for the Cluster-Booster Architecture", "abstract": "The recently completed research project DEEP-ER has developed a variety of hardware and software technologies to improve the I/O capabilities of next generation high-performance computers, and to enable applications recovering from the larger hardware failure rates expected on these machines. The heterogeneous Cluster-Booster architecture - first introduced in the predecessor DEEP project - has been extended by a multi-level memory hierarchy employing non-volatile and network-attached memory devices. Based on this hardware infrastructure, an I/O and resiliency software stack has been implemented combining and extending well established libraries and software tools, and sticking to standard user-interfaces. Real-world scientific codes have tested the projects' developments and demonstrated the improvements achieved without compromising the portability of the applications.", "venue": "2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)", "authors": ["Anke  Kreuzer", "Jorge  Amaya", "Norbert  Eicker", "Raphael  Leger", "Estela  Suarez"], "year": 2018, "n_citations": 1}
{"id": 2359480, "s2_id": "f983904d21012d3d5aaa3078f92652ef71283e4b", "title": "Machine Learning\u2013enabled Scalable Performance Prediction of Scientific Codes", "abstract": "Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage. PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times.", "venue": "ACM Trans. Model. Comput. Simul.", "authors": ["Gopinath  Chennupati", "Nandakishore  Santhi", "Phill  Romero", "Stephan  Eidenbenz"], "year": 2021, "n_citations": 1}
{"id": 2359565, "s2_id": "8680c46da46ad70d11207895e384783518081cbb", "title": "Impact of Power Supply Noise on Image Sensor Performance in Automotive Applications", "abstract": "Vision Systems are quickly becoming a large component of Active Automotive Safety Systems. In order to be effective in critical safety applications these systems must produce high quality images in both daytime and night-time scenarios in order to provide the large informational content required for software analysis in applications such as lane departure, pedestrian detection and collision detection. The challenge in capturing high quality images in low light scenarios is that the signal to noise ratio is greatly reduced, which can result in noise becoming the dominant factor in a captured image, thereby making these safety systems less effective at night. Research has been undertaken to develop a systematic method of characterising image sensor performance in response to electrical noise in order to improve the design and performance of automotive cameras in low light scenarios. The root cause of image row noise has been established and a mathematical algorithm for determining the magnitude of row noise in an image has been devised. An automated characterisation method has been developed to allow performance characterisation in response to a large frequency spectrum of electrical noise on the image sensor power supply. Various strategies of improving image sensor performance for low light applications have also been proposed from the research outcomes.", "venue": "ArXiv", "authors": ["Shane  Gilroy"], "year": 2020, "n_citations": 1}
{"id": 2360698, "s2_id": "9b8a3f4ec27c8c33ca4ad6c101aa72a1fff523a3", "title": "PolyAdd: Polynomial Formal Verification of Adder Circuits", "abstract": "Only by formal verification approaches functional correctness can be ensured. While for many circuits fast verification is possible, in other cases the approaches fail. In general no efficient algorithms can be given, since the underlying verification problem is NP-complete. In this paper we prove that for different types of adder circuits polynomial verification can be ensured based on BDDs. While it is known that the output functions for addition are polynomially bounded, we show in the following that the entire construction process can be carried out in polynomial time. This is shown for the simple Ripple Carry Adder, but also for fast adders like the Conditional Sum Adder and the Carry Look Ahead Adder. Properties about the adder function are proven and the core principle of polynomial verification is described that can also be extended to other classes of functions and circuit realizations.", "venue": "2021 24th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)", "authors": ["Rolf  Drechsler"], "year": 2021, "n_citations": 5}
{"id": 2360702, "s2_id": "686562c5c7ad4764910a01dfcc4f749614e95b1c", "title": "A Low-Latency List Successive-Cancellation Decoding Implementation for Polar Codes", "abstract": "Due to their provably capacity-achieving performance, polar codes have attracted a lot of research interest recently. For a good error-correcting performance, list successive-cancellation decoding (LSCD) with large list size is used to decode polar codes. However, as the complexity and delay of the list management operation rapidly increase with the list size, the overall latency of LSCD becomes large and limits the applicability of polar codes in high-throughput and latency-sensitive applications. Therefore, in this work, the low-latency implementation for LSCD with large list size is studied. Specifically, at the system level, a selective expansion method is proposed such that some of the reliable bits are not expanded to reduce the computation and latency. At the algorithmic level, a double thresholding scheme is proposed as a fast approximate-sorting method for the list management operation to reduce the LSCD latency for large list size. A VLSI architecture of the LSCD implementing the selective expansion and double thresholding scheme is then developed, and implemented using a UMC 90 nm CMOS technology. Experimental results show that, even for a large list size of 16, the proposed LSCD achieves a decoding throughput of 460 Mbps at a clock frequency of 658 MHz.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["YouZhe  Fan", "ChenYang  Xia", "Ji  Chen", "Chi-Ying  Tsui", "Jie  Jin", "Hui  Shen", "Bin  Li"], "year": 2016, "n_citations": 37}
{"id": 2362343, "s2_id": "690f681e19cafa4938f9246b79ac9192eb971bce", "title": "Pyramid: Machine Learning Framework to Estimate the Optimal Timing and Resource Usage of a High-Level Synthesis Design", "abstract": "The emergence of High-Level Synthesis (HLS) tools shifted the paradigm of hardware design by making the process of mapping high-level programming languages to hardware design such as C to VHDL/Verilog feasible. HLS tools offer a plethora of techniques to optimize designs for both area and performance, but resource usage and timing reports of HLS tools mostly deviate from the post-implementation results. In addition, to evaluate a hardware design performance, it is critical to determine the maximum achievable clock frequency. Obtaining such information using static timing analysis provided by CAD tools is difficult, due to the multitude of tool options. Moreover, a binary search to find the maximum frequency is tedious, time-consuming, and often does not obtain the optimal result. To address these challenges, we propose a framework, called Pyramid, that uses machine learning to accurately estimate the optimal performance and resource utilization of an HLS design. For this purpose, we first create a database of C-to- FPGA results from a diverse set of benchmarks. To find the achievable maximum clock frequency, we use Minerva, which is an automated hardware optimization tool. Minerva determines the close-to-optimal settings of tools, using static timing analysis and a heuristic algorithm, and targets either optimal throughput or throughput-to-area. Pyramid uses the database to train an ensemble machine learning model to map the HLS-reported features to the results of Minerva. To this end, Pyramid recalibrates the results of HLS to bridge the accuracy gap, and enable developers to estimate the throughput or throughputto- area of hardware design with more than 95% accuracy and alleviates the need to perform actual implementation for estimation.", "venue": "2019 29th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Hosein Mohammadi Makrani", "Farnoud  Farahmand", "Hossein  Sayadi", "Sara  Bondi", "Sai Manoj Pudukotai Dinakarrao", "Liang  Zhao", "Avesta  Sasan", "Houman  Homayoun", "Setareh  Rafatirad"], "year": 2019, "n_citations": 14}
{"id": 2365170, "s2_id": "ff594346f977498e1518a05fba76c3dd5e68e35a", "title": "System-Level Optimization of Network-on-Chips for Heterogeneous 3D System-on-Chips", "abstract": "For a system-level design of Networks-on-Chip for 3D heterogeneous System-on-Chip (SoC), the locations of components, routers and vertical links are determined from an application model and technology parameters. In conventional methods, the two inputs are accounted for separately; here, we define an integrated problem that considers both application model and technology parameters. We show that this problem does not allow for exact solution in reasonable time, as common for many design problems. Therefore, we contribute a heuristic by proposing design steps, which are based on separation of intralayer and interlayer communication. The advantage is that this new problem can be solved with well-known methods. We use 3D Vision SoC case studies to quantify the advantages and the practical usability of the proposed optimization approach. We achieve up to 18.8% reduced white space and up to 12.4% better network performance in comparison to conventional approaches.", "venue": "2019 IEEE 37th International Conference on Computer Design (ICCD)", "authors": ["Jan Moritz Joseph", "Dominik  Ermel", "Lennart  Bamberg", "Alberto Garc\u00eda Ortiz", "Thilo  Pionteck"], "year": 2019, "n_citations": 2}
{"id": 2365717, "s2_id": "8635a61bbb3ac3c6c16cc26c73cc2ffc71fa7ed7", "title": "ADIC: Anomaly Detection Integrated Circuit in 65-nm CMOS Utilizing Approximate Computing", "abstract": "In this article, we present a low-power (LP) anomaly detection integrated circuit (ADIC) based on a one-class classifier (OCC) neural network. The ADIC achieves LP operation through a combination of: 1) careful choice of algorithm for online learning and 2) approximate computing techniques to lower average energy. In particular, online pseudoinverse update method (OPIUM) is used to train a randomized neural network for quick and resource-efficient learning. An additional 42% energy saving can be achieved when a lighter version of OPIUM method is used for training with the same number of data samples lead to no significant compromise on the quality of inference. Instead of a single classifier with large number of neurons, an ensemble of <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula> base learner (BL) approach is chosen to reduce learning memory by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula>. This also enables approximate computing by dynamically varying the neural network size based on anomaly detection. Fabricated in 65-nm CMOS, the ADIC has <inline-formula> <tex-math notation=\"LaTeX\">$K=7$ </tex-math></inline-formula> BLs with 32 neurons in each BL and dissipates 11.87 and 3.35 pJ/OP during learning and inference, respectively, at <inline-formula> <tex-math notation=\"LaTeX\">$V_{\\text {dd}}=0.75\\,\\,\\text {V}$ </tex-math></inline-formula> when all seven BLs are enabled. Furthermore, evaluated on the NASA bearing data set, approximately 80% of the chip can be shut down for 99% of the lifetime leading to an energy efficiency of 0.48 pJ/OP, an <inline-formula> <tex-math notation=\"LaTeX\">$18.5 \\times $ </tex-math></inline-formula> reduction over full-precision computing running at <inline-formula> <tex-math notation=\"LaTeX\">$V_{\\text {dd}}=1.2 \\,\\, \\text {V}$ </tex-math></inline-formula> throughout the lifetime.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Bapi  Kar", "Pradeep Kumar Gopalakrishnan", "Sumon Kumar Bose", "Mohendra  Roy", "Arindam  Basu"], "year": 2020, "n_citations": 0}
{"id": 2365871, "s2_id": "fac205ab07c9a21bc46710168e0f85e5d7b08411", "title": "Domain-specific Genetic Algorithm for Multi-tenant DNNAccelerator Scheduling", "abstract": "As Deep Learning continues to drive a variety of applications in datacenters and HPC, there is a growing trend towards building large accelerators with several sub-accelerator cores/chiplets. This work looks at the problem of supporting multi-tenancy on such accelerators. In particular, we focus on the problem of mapping layers from several DNNs simultaneously on an accelerator. Given the extremely large search space, we formulate the search as an optimization problem and develop a specialized genetic algorithm called G# with custom operators to enable structured sample-efficient exploration. We quantitatively compare G# with several common heuristics, state-of-the-art optimization methods, and reinforcement learning methods across different accelerator settings (large/small accelerators) and different sub-accelerator configurations (homogeneous/heterogeneous), and observe G# can consistently find better solutions. Further, to enable real-time scheduling, we also demonstrate a method to generalize the learnt schedules and transfer them to the next batch of jobs, reducing schedule compute time to \u223czero.", "venue": "ArXiv", "authors": ["Sheng-Chun  Kao", "Tushar  Krishna"], "year": 2021, "n_citations": 1}
{"id": 2366307, "s2_id": "b81ef9ccbc31bc7aa2823c43e8d5a63cca14a592", "title": "On the Approximation of Accuracy-configurable Sequential Multipliers via Segmented Carry Chains", "abstract": "In this paper, we present a multiplier based on a sequence of approximated accumulations. According to a given splitting point of the carry chains, the technique herein introduced allows varying the quality of the accumulations and, consequently, the overall product. Our approximate multiplier trades-off accuracy for a reduced latency\u2014with respect to an accurate sequential multiplier\u2014and exploits the inherent area savings of sequential over combinatorial approaches. We implemented multiple versions with different bit-width and accuracy configurations, targeting an FPGA and a 45nm ASIC to estimate resources, power consumption, and latency. We also present two error analyses of the proposed design based on closed-form analysis and simulations.", "venue": "ArXiv", "authors": ["Jorge  Echavarria", "Stefan  Wildermann", "Oliver  Keszocze", "Faramarz  Khosravi", "Andreas  Becher", "Jurgen  Teich"], "year": 2021, "n_citations": 0}
{"id": 2367299, "s2_id": "80005a6253008ac55d27c002f11d4f0646a73e7d", "title": "Pipelined Parallel FFT Architecture", "abstract": "In this paper, an optimized efficient VLSI architecture of a pipeline Fast Fourier transform (FFT) processor capable of producing the reverse output order sequence is presented. Paper presents Radix-2 multipath delay architecture for FFT calculation. The implementation of FFT in hardware is very critical because for calculation of FFT number of butterfly operations i.e. number of multipliers requires due to which hardware gets increased means indirectly cost of hardware is automatically gets increased. Also multiplier operations are slow that's why it limits the speed of operation of architecture. The optimized VLSI implementation of FFT algorithm is presented in this paper. Here architecture is pipelined to optimize it and to increase the speed of operation. Also to increase the speed of operation 2 levels parallel processing is used.", "venue": "ArXiv", "authors": ["Tanaji U. Kamble", "B. G. Patil", "Rakhee S. Bhojakar"], "year": 2017, "n_citations": 0}
{"id": 2367986, "s2_id": "8e8a59c73b8e8e19ff3cccb0a83e54647d2d8dfb", "title": "Crossing the architectural barrier: Evaluating representative regions of parallel HPC applications", "abstract": "Exascale computing will get mankind closer to solving important social, scientific and engineering problems. Due to high prototyping costs, High Performance Computing (HPC) system architects make use of simulation models for design space exploration and hardware-software co-design. However, as HPC systems reach exascale proportions, the cost of simulation increases, since simulators themselves are largely single-threaded. Tools for selecting representative parts of parallel applications to reduce running costs are widespread, e.g., BarrierPoint achieves this by analysing, in simulation, abstract characteristics such as basic blocks and reuse distances. However, architectures new to HPC have a limited set of tools available. In this work, we provide an independent cross-architectural evaluation on real hardware \u2014 across Intel and ARM \u2014 of the BarrierPoint methodology, when applied to parallel HPC proxy applications. We present both cases: when the methodology can be applied and when it cannot. In the former case, results show that we can predict the performance of full application execution by running shorter representative sections. In the latter case, we dive into the underlying issues and suggest improvements. We demonstrate a total simulation time reduction of up to 178x, whilst keeping the error below 2.3% for both cycles and instructions.", "venue": "2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Alexandra  Ferreron", "Radhika  Jagtap", "Sascha  Bischoff", "Roxana  Rusitoru"], "year": 2017, "n_citations": 3}
{"id": 2368466, "s2_id": "dde652f5b02a5e9d28ff5a7ee08f9aaf508d1dd2", "title": "Newton: Gravitating Towards the Physical Limits of Crossbar Acceleration", "abstract": "Many recent works take advantage of highly parallel analog in-situ computation in memristor crossbars to accelerate the many vector-matrix multiplication operations in deep neural networks (DNNs). However, these in-situ accelerators have two significant shortcomings: The ADCs account for a large fraction of chip power and area, and these accelerators adopt a homogeneous design in which every resource is provisioned for the worst case. By addressing both problems, the new architecture, called Newton, moves closer to achieving optimal energy per neuron for crossbar accelerators. We introduce new techniques that apply at different levels of the tile hierarchy, some leveraging heterogeneity and others relying on divide-and-conquer numeric algorithms to reduce computations and ADC pressure. Finally, we place constraints on how a workload is mapped to tiles, thus helping reduce resource-provisioning in tiles. For many convolutional-neural-network (CNN) dataflows and structures, Newton achieves a 77-percent decrease in power, 51-percent improvement in energy-efficiency, and 2.1\u00d7 higher throughput/area, relative to the state-of-the-art In-Situ Analog Arithmetic in Crossbars (ISAAC) accelerator.", "venue": "IEEE Micro", "authors": ["Anirban  Nag", "Ali  Shafiee", "Rajeev  Balasubramonian", "Vivek  Srikumar", "Naveen  Muralimanohar"], "year": 2018, "n_citations": 18}
{"id": 2371902, "s2_id": "a8c559f171859a7d40cbb5e7bc612579c1aaf7da", "title": "Polystore++: Accelerated Polystore System for Heterogeneous Workloads", "abstract": "Modern real-time business analytic consist of heterogeneous workloads (e.g., database queries, graph processing, and machine learning). These analytic applications need programming environments that can capture all aspects of the constituent workloads (including data models they work on and movement of data across processing engines). Polystore systems suit such applications; however, these systems currently execute on CPUs and the slowdown of Moore's Law means they cannot meet the performance and efficiency requirements of modern workloads. We envision Polystore++, an architecture to accelerate existing polystore systems using hardware accelerators (e.g., FPGAs, CGRAs, and GPUs). Polystore++ systems can achieve high performance at low power by identifying and offloading components of a polystore system that are amenable to acceleration using specialized hardware. Building a Polystore++ system is challenging and introduces new research problems motivated by the use of hardware accelerators (e.g., optimizing and mapping query plans across heterogeneous computing units and exploiting hardware pipelining and parallelism to improve performance). In this paper, we discuss these challenges in detail and list possible approaches to address these problems.", "venue": "2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)", "authors": ["Rekha  Singhal", "Nathan  Zhang", "Luigi  Nardi", "Muhammad  Shahbaz", "Kunle  Olukotun"], "year": 2019, "n_citations": 3}
{"id": 2375652, "s2_id": "6911dc4c2fff43a5a4acc189b0d55eeaeaccc318", "title": "Near Data Acceleration with Concurrent Host Access", "abstract": "Near-data accelerators (NDAs) that are integrated with the main memory have the potential for significant power and performance benefits. Fully realizing these benefits requires the large available memory capacity to be shared between the host and NDAs in a way that permits both regular memory access by some applications and accelerating others with an NDA, avoids copying data, enables collaborative processing, and simultaneously offers high performance for both host and NDA. We identify and solve new challenges in this context: mitigating row-locality interference from host to NDAs, reducing read/write-turnaround overhead caused by fine-grain interleaving of host and NDA requests, architecting a memory layout that supports the locality required for NDAs and sophisticated address interleaving for host performance, and supporting both packetized and traditional memory interfaces. We demonstrate our approach in a simulated system that consists of a multi-core CPU and NDA-enabled DDR4 memory modules. We show that our mechanisms enable effective and efficient concurrent access using a set of microbenchmarks, then demonstrate the potential of the system for the important stochastic variance-reduced gradient (SVRG) algorithm.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Benjamin Y. Cho", "Yongkee  Kwon", "Sangkug  Lym", "Mattan  Erez"], "year": 2020, "n_citations": 8}
{"id": 2378043, "s2_id": "89b08fcb2aed804609124f737ceb175ebcb6e6ad", "title": "In-memory multiplication engine with SOT-MRAM based stochastic computing", "abstract": "Processing-in-memory (PIM) turns out to be a promising solution to breakthrough the memory wall and the power wall. While prior PIM designs yield successful implementation of bitwise Boolean logic operations locally in memory, it is difficult to accomplish the multiplication (MUL) instruction in a fast and efficient manner. In this paper, we propose a new stochastic computing (SC) design to perform MUL with in-memory operations. Instead of using the stochastic number generators (SNGs), we harness the inherent stochasticity in the memory write behavior of the magnetic random access memory (MRAM). Each memory bit serves as an SC engine, performs MUL on operands in the form of write voltage pulses, and stores the MUL outcome in-situ. The proposed design provides up to 4x improvement in performance compared with conversational SC approaches, and achieves 18x speedup over implementing MUL with only in-memory bitwise Boolean logic operations.", "venue": "ArXiv", "authors": ["Xin  Ma", "Liang  Chang", "Shuangchen  Li", "Lei  Deng", "Yufei  Ding", "Yuan  Xie"], "year": 2018, "n_citations": 2}
{"id": 2380514, "s2_id": "36ae6103d018cf64c934173406b8bfe40b0eec07", "title": "Automated Design Space Exploration of CGRA Processing Element Architectures using Frequent Subgraph Analysis", "abstract": "The architecture of a coarse-grained reconfigurable array (CGRA) processing element (PE) has a significant effect on the performance and energy efficiency of an application running on the CGRA. This paper presents an automated approach for generating specialized PE architectures for an application or an application domain. Frequent subgraphs mined from a set of applications are merged to form a PE architecture specialized to that application domain. For the image processing and machine learning domains, we generate specialized PEs that are up to 10.5\u00d7 more energy efficient and consume 9.1\u00d7 less area than a baseline PE.", "venue": "ArXiv", "authors": ["Jackson  Melchert", "Kathleen  Feng", "Caleb  Donovick", "Ross  Daly", "Clark  Barrett", "Mark  Horowitz", "Pat  Hanrahan", "Priyanka  Raina"], "year": 2021, "n_citations": 0}
{"id": 2381976, "s2_id": "be75cb665c516a117d51f761206dab4df440dfed", "title": "Dynamic partitioning of physical memory among virtual machines: ASMI:architectural support for memory isolation", "abstract": "It is an open challenge for virtualization technology architects to provide security to Virtual Machine (VM), in the presence of an infected hypervisor, without much compromise on performance. A few hardware modifications have been introduced by manufactures like Intel and AMD to provide a secure VM environment with low performance degradation. These solutions are unable to provide VM isolation in the presence of an infected hypervisor. In this paper we propose a novel memory architecture model, that can achieve a secure physical memory region to each VM without performance degradation.", "venue": "SAC", "authors": ["R.  Jithin", "Priya  Chandran"], "year": 2016, "n_citations": 3}
{"id": 2382990, "s2_id": "82312666fdfe23850a6ea2e955bf4e0a7c9a43a7", "title": "Accelerator Codesign as Non-Linear Optimization", "abstract": "We propose an optimization approach for determining both hardware and software parameters for the efficient implementation of a (family of) applications called dense stencil computations on programmable GPGPUs. We first introduce a simple, analytical model for the silicon area usage of accelerator architectures and a workload characterization of stencil computations. We combine this characterization with a parametric execution time model and formulate a mathematical optimization problem. That problem seeks to maximize a common objective function of 'all the hardware and software parameters'. The solution to this problem, therefore \"solves\" the codesign problem: simultaneously choosing software-hardware parameters to optimize total performance. \nWe validate this approach by proposing architectural variants of the NVIDIA Maxwell GTX-980 (respectively, Titan X) specifically tuned to a predetermined workload of four common 2D stencils (Heat, Jacobi, Laplacian, and Gradient) and two 3D ones (Heat and Laplacian). Our model predicts that performance would potentially improve by 28% (respectively, 33%) with simple tweaks to the hardware parameters such as adapting coarse and fine-grained parallelism by changing the number of streaming multiprocessors and the number of compute cores each contains. We propose a set of Pareto-optimal design points to exploit the trade-off between performance and silicon area and show that by additionally eliminating GPU caches, we can get a further 2-fold improvement.", "venue": "ArXiv", "authors": ["Nirmal  Prajapati", "Sanjay V. Rajopadhye", "Hristo  Djidjev", "Nandakishore  Santhi", "Tobias  Grosser", "Rumen  Andonov"], "year": 2017, "n_citations": 1}
{"id": 2392980, "s2_id": "7f6774a2ef51081648eacb3bcd03568096415bb1", "title": "Revising the classic computing paradigm and its technological implementations", "abstract": "Today\u2019s computing is based on the classic paradigm proposed by John von Neumann, three-quarters of a century ago. That paradigm, however, was justified for (the timing relations of) vacuum tubes only. The technological development invalidated the classic paradigm (but not the model!). It led to catastrophic performance losses in computing systems, from the operating gate level to large networks, including the neuromorphic ones. The model is perfect, but the paradigm is applied outside of its range of validity. The classic paradigm is completed here by providing the \u201cprocedure\u201d missing from the \u201cFirst Draft\u201d that enables computing science to work with cases where the transfer time is not negligible apart from the processing time. The paper reviews whether we can describe the implemented computing processes by using the accurate interpretation of the computing model, and whether we can explain the issues experienced in different fields of today\u2019s computing by omitting the wrong omissions. Furthermore, it discusses some of the consequences of improper technological implementations, from shared media to parallelized operation, suggesting ideas on how computing performance could be improved to meet the growing societal demands.", "venue": "Informatics", "authors": ["J'anos  V'egh"], "year": 2021, "n_citations": 2}
{"id": 2393069, "s2_id": "24ae0201c2c94135c3ed5ff7485dc23ceb0d53ec", "title": "Exploring NoC mapping strategies: an energy and timing aware technique", "abstract": "Complex applications implemented as systems on chip (SoC) demand extensive use of system level modeling and validation. Their implementation gathers a large number of complex IP cores and advanced interconnection schemes, such as hierarchical bus architectures or networks on chip (NoC). Modeling applications involves capturing its computation and communication characteristics. Previously proposed communication weighted models (CWM) consider only the application communication aspects. This work proposes a communication dependence and computation model (CDCM) that can simultaneously consider both aspects of an application. It presents a solution to the problem of mapping applications on regular NoC while considering execution time and energy consumption. The use of CDCM is shown to provide estimated average reductions of 40% in execution time, and 20% in energy consumption, for current technologies.", "venue": "Design, Automation and Test in Europe", "authors": ["C\u00e9sar A. M. Marcon", "Ney Laert Vilar Calazans", "Fernando Gehm Moraes", "Altamiro Amadeu Susin", "Igor M. Reis", "Fabiano  Hessel"], "year": 2005, "n_citations": 93}
{"id": 2399121, "s2_id": "5fc0e5798b3626164d037f9fb982fc80ab626d32", "title": "Quantum Circuit Designs of Integer Division Optimizing T-count and T-depth", "abstract": "Quantum circuits for mathematical functions such as division are necessary to use quantum computers for scientific computing. Quantum circuits based on Clifford+T gates can easily be made fault-tolerant but the T gate is very costly to implement. The small number of qubits available in existing quantum computers adds another constraint on quantum circuits. As a result, reducing T-count and qubit cost have become important optimization goals. The design of quantum circuits for integer division has caught the attention of researchers and designs have been proposed in the literature. However, these designs suffer from excessive T gate and qubit costs. Many of these designs also produce significant garbage output resulting in additional qubit and T gate costs to eliminate these outputs. In this work, we propose two quantum integer division circuits. The first proposed quantum integer division circuit is based on the restoring division algorithm and the second proposed design implements the non-restoring division algorithm. Both proposed designs are optimized in terms of T-count, T-depth and qubits. Both proposed quantum circuit designs are based on (i) a quantum subtractor, (ii) a quantum adder-subtractor circuit, and (iii) a novel quantum conditional addition circuit. Our proposed restoring division circuit achieves average T-count savings from 79.03 to 91.69 percent compared to the existing works. Our proposed non-restoring division circuit achieves average T-count savings from 49.22 to 90.03 percent compared to the existing works. Further, both our proposed designs have linear T-depth. We also illustrate the application of the proposed quantum division circuits in quantum image processing with a case study of quantum bilinear interpolation.", "venue": "IEEE Transactions on Emerging Topics in Computing", "authors": ["Himanshu  Thapliyal", "Edgard  Mu\u00d1oz-Coreas", "T. S. S. Varun", "Travis S. Humble"], "year": 2021, "n_citations": 8}
{"id": 2403037, "s2_id": "090fa6856c4c245b31abfa9b2cf04ed53d49aa68", "title": "Analysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA Device", "abstract": "Unlike traditional PCIe-based FPGA accelerators, heterogeneous SoC-FPGA devices provide tighter integrations between software running on CPUs and hardware accelerators. Modern heterogeneous SoC-FPGA platforms support multiple I/O cache coherence options between CPUs and FPGAs, but these options can have inadvertent effects on the achieved bandwidths depending on applications and data access patterns. To provide the most efficient communications between CPUs and accelerators, understanding the data transaction behaviors and selecting the right I/O cache coherence method is essential. In this paper, we use Xilinx Zynq UltraScale+ as the SoC platform to show how certain I/O cache coherence method can perform better or worse in different situations, ultimately affecting the overall accelerator performances as well. Based on our analysis, we further explore possible software and hardware modifications to improve the I/O performances with different I/O cache coherence options. With our proposed modifications, the overall performance of SoC design can be averagely improved by 20%.", "venue": "2019 29th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Seungwon  Min", "Sitao  Huang", "Mohamed  El-Hadedy", "Jinjun  Xiong", "Deming  Chen", "Wen-Mei  Hwu"], "year": 2019, "n_citations": 4}
{"id": 2403992, "s2_id": "101aa1ae346b6ef8068e5ede6e9afb4496a947ee", "title": "Manticore: A 4096-Core RISC-V Chiplet Architecture for Ultraefficient Floating-Point Computing", "abstract": "Data-parallel problems demand ever growing floating-point (FP) operations per second under tight area- and energy-efficiency constraints. In this work, we present Manticore, a general-purpose, ultraefficient chiplet-based architecture for data-parallel FP workloads. We have manufactured a prototype of the chiplet\u2019s computational core in Globalfoundries 22FDX process and demonstrate more than 5x improvement in energy efficiency on FP intensive workloads compared to CPUs and GPUs. The compute capability at high energy and area efficiency is provided in \u201cSnitch: A tiny pseudo dual-issue processor for area and energy efficient execution of floating-point intensive workloads,\u201d IEEE Trans. Comput., containing eight small integer cores, each controlling a large floating-point unit (FPU). The core supports two custom ISA extensions: The SSRs extension elides explicit load and store instructions by encoding them as register reads and writes (\u201cStream semantic registers: A lightweight RISC-V ISA extension achieving full compute utilization in single-issue cores,\u201d IEEE Trans. Comput.). The floating-point repetition extension decouples the integer core from the FPU allowing floating-point instructions to be issued independently. These two extensions allow the single-issue core to minimize its instruction fetch bandwidth and saturate the instruction bandwidth of the FPU, achieving FPU utilization above 90%, with more than 40% of core area dedicated to the FPU.", "venue": "IEEE Micro", "authors": ["Florian  Zaruba", "Fabian  Schuiki", "Luca  Benini"], "year": 2021, "n_citations": 7}
{"id": 2404263, "s2_id": "fecc0e9339bab9d40d7ca62528f050c8d8d39c20", "title": "CREST: Hardware Formal Verification with ANSI-C Reference Specifications", "abstract": "This paper presents CREST, a prototype front-end tool intended as an add-on to commercial EDA formal verifcation environments. CREST is an adaptation of the CBMC bounded model checker for C, an academic tool widely used in industry for software analysis and property verification. It leverages the capabilities of CBMC to process hardware datapath specifications written in arbitrary ANSI-C, without limiting restrictions to a synthesizable subset. We briefly sketch the architecture of our tool and show its use in a range of verification case studies.", "venue": "ArXiv", "authors": ["Andreas  Tiemeyer", "Thomas F. Melham", "Daniel  Kroening", "John  O'Leary"], "year": 2019, "n_citations": 0}
{"id": 2406081, "s2_id": "2a2edb90010159b86b044e29b57ceb4c33ce0e6d", "title": "Improving Multi-Application Concurrency Support Within the GPU Memory System", "abstract": "GPUs exploit a high degree of thread-level parallelism to hide long-latency stalls. Due to the heterogeneous compute requirements of different applications, there is a growing need to share the GPU across multiple applications in large-scale computing environments. However, while CPUs offer relatively seamless multi-application concurrency, and are an excellent fit for multitasking and for virtualized environments, GPUs currently offer only primitive support for multi-application concurrency. Much of the problem in a contemporary GPU lies within the memory system, where multi-application execution requires virtual memory support to manage the address spaces of each application and to provide memory protection. In this work, we perform a detailed analysis of the major problems in state-of-the-art GPU virtual memory management that hinders multi-application execution. Existing GPUs are designed to share memory between the CPU and GPU, but do not handle multi-application support within the GPU well. We find that when multiple applications spatially share the GPU, there is a significant amount of inter-core thrashing on the shared TLB within the GPU. The TLB contention is high enough to prevent the GPU from successfully hiding stall latencies, thus becoming a first-order performance concern. We introduce MASK, a memory hierarchy design that provides low-overhead virtual memory support for the concurrent execution of multiple applications. MASK extends the GPU memory hierarchy to efficiently support address translation through the use of multi-level TLBs, and uses translation-aware memory and cache management to maximize throughput in the presence of inter-application contention.", "venue": "ArXiv", "authors": ["Rachata  Ausavarungnirun", "Christopher J. Rossbach", "Vance  Miller", "Joshua  Landgraf", "Saugata  Ghose", "Jayneel  Gandhi", "Adwait  Jog", "Onur  Mutlu"], "year": 2017, "n_citations": 2}
{"id": 2407669, "s2_id": "f18a6b6937168dc3495b4ecc78e8b91e8c32dd21", "title": "From Research to Proof-of-Concept: Analysis of a Deployment of FPGAs on a Commercial Search Engine", "abstract": "FPGAs are quickly becoming available in data centres and in the cloud as a one more heterogeneous processing element complementing CPUs and GPUs. There are many reports in the research literature showing the potential for FPGAs to accelerate a wide variety of algorithms, which combined with their growing availability, would seem to also indicate a widespread use in many applications. Unfortunately, there is not much published research exploring what it takes to integrate an FPGA into an existing application in a cost-effective way and keeping the algorithmic performance advantages. Building on recent results exploring how to employ FPGAs to improve the search engines used in the travel industry, this paper analyses the end-to-end performance of the search engine when using FPGAs, as well as the necessary changes to the software and the cost of such deployments. The results provide important insights on current FPGA deployments and what needs to be done to make FPGAs more widely used. For instance, the large potential performance gains provided by an FPGA are greatly diminished in practice if the application cannot submit request in the most optimal way for the FPGA, something that is not always possible and might require significant changes to the application. Similarly, some existing cloud deployments turn out to use a very imbalanced architecture: a powerful FPGA connected to a not so powerful CPU. The result is that the CPU cannot generate enough load for the FPGA, which potentially eliminates all performance gains and might even result in a more expensive system. In this paper, we report on an extensive study and development effort to incorporate FPGAs into a search engine and analyse the issues encountered and their practical impact. We expect that these results will inform the development and deployment of FPGAs in the future by providing important insights on the end-to-end integration of FPGAs within existing systems.", "venue": "ArXiv", "authors": ["Fabio  Maschi", "Gustavo  Alonso", "Anthony  Hock-Koon", "Nicolas  Bondoux", "Teddy  Roy", "Mourad  Boudia", "Matteo  Casalino"], "year": 2021, "n_citations": 0}
{"id": 2408443, "s2_id": "4c0d77e7803f30962c3ddb520e1de8c9ec41bf34", "title": "Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian Vector Broadcast Channels", "abstract": "MIMO technology is one of the most significant advances in the past decade to increase channel capacity and has a great potential to improve network capacity for mesh networks. In a MIMO-based mesh network, the links outgoing from each node sharing the common communication spectrum can be modeled as a Gaussian vector broadcast channel. Recently, researchers showed that ``dirty paper coding'' (DPC) is the optimal transmission strategy for Gaussian vector broadcast channels. So far, there has been little study on how this fundamental result will impact the cross-layer design for MIMO-based mesh networks. To fill this gap, we consider the problem of jointly optimizing DPC power allocation in the link layer at each node and multihop/multipath routing in a MIMO-based mesh networks. It turns out that this optimization problem is a very challenging non-convex problem. To address this difficulty, we transform the original problem to an equivalent problem by exploiting the channel duality. For the transformed problem, we develop an efficient solution procedure that integrates Lagrangian dual decomposition method, conjugate gradient projection method based on matrix differential calculus, cutting-plane method, and subgradient method. In our numerical example, it is shown that we can achieve a network performance gain of 34.4% by using DPC.", "venue": "ArXiv", "authors": ["Jia  Liu", "Yiwei Thomas Hou"], "year": 2007, "n_citations": 5}
{"id": 2413493, "s2_id": "ffc13d68b0a28bd60409b15244156351a8ef1401", "title": "3D IC optimal layout design. A parallel and distributed topological approach", "abstract": "The task of 3D ICs layout design involves the assembly of millions of components taking into account many different requirements and constraints such as topological, wiring or manufacturability ones. It is a NP-hard problem that requires new non-deterministic and heuristic algorithms. Considering the time complexity, the commonly applied Fiduccia-Mattheyses partitioning algorithm is superior to any other local search method. Nevertheless, it can often miss to reach a quasi-optimal solution in 3D spaces. The presented approach uses an original 3D layout graph partitioning heuristics implemented with use of the extremal optimization method. The goal is to minimize the total wire-length in the chip. In order to improve the time complexity a parallel and distributed Java implementation is applied. Inside one Java Virtual Machine separate optimization algorithms are executed by independent threads. The work may also be shared among different machines by means of The Java Remote Method Invocation system.", "venue": "ArXiv", "authors": ["Katarzyna  Grzesiak-Kope'c", "Maciej  Ogorzalek"], "year": 2019, "n_citations": 0}
{"id": 2415664, "s2_id": "18a9f1167f92586788d5b37e2812bbef7bde43cb", "title": "High performance reconfigurable computing systems", "abstract": "High performance reconfigurable computer systems exploit the parallel processing benefits of high performance computing in conjunction with the fast, adaptive hardware acceleration associated with reconfigurable computing. We introduce this novel approach to computer architecture by first discussing the current state of HPRC systems and promising HPRC system architectures, particularly with respect to their ability to exploit different types of potential parallelism at multiple levels. Next, we explore the most appropriate software architecture for HPRC systems, along with hardware architectural issues such as hardware virtualization. We then consider runtime issues concerning communications, context switching, and operating system services. Finally, we consider challenges with developing efficient applications on HPRC platforms.", "venue": "Proceedings of the 44th IEEE 2001 Midwest Symposium on Circuits and Systems. MWSCAS 2001 (Cat. No.01CH37257)", "authors": ["Melissa C. Smith", "S. L. Drager", "L.  Pochet", "Gregory D. Peterson"], "year": 2001, "n_citations": 11}
{"id": 2416823, "s2_id": "58563234f082b8732967179171606db5f490ce64", "title": "Aker: A Design and Verification Framework for Safe and Secure SoC Access Control", "abstract": "Modern systems on a chip (SoCs) utilize heterogeneous architectures where multiple IP cores have concurrent access to on-chip shared resources. In security-critical applications, IP cores have different privilege levels for accessing shared resources, which must be regulated by an access control system. Aker is a design and verification framework for SoC access control. Aker builds upon the Access Control Wrapper (ACW) - a high performance and easy-to-integrate hardware module that dynamically manages access to shared resources. To build an SoC access control system, Aker distributes the ACWs throughout the SoC, wrapping controller IP cores, and configuring the ACWs to perform local access control. To ensure the access control system is functioning correctly and securely, Aker provides a property-driven security verification using MITRE common weakness enumerations. Aker verifies the SoC access control at the IP level to ensure the absence of bugs in the functionalities of the ACW module, at the firmware level to confirm the secure operation of the ACW when integrated with a hardware root-of-trust (HRoT), and at the system level to evaluate security threats due to the interactions among shared resources. The performance, resource usage, and security of access control systems implemented through Aker is experimentally evaluated on a Xilinx UltraScale+ programmable SoC, it is integrated with the OpenTitan hardware root-of-trust, and it is used to design an access control system for the OpenPULP multicore architecture.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Francesco  Restuccia", "Andres  Meza", "Ryan  Kastner"], "year": 2021, "n_citations": 1}
{"id": 2417843, "s2_id": "b0d7b62060559cebdd5c4c0605c0f3cf352ce1c3", "title": "Towards a Domain Specific Solution for a New Generation of Wireless Modems", "abstract": "Wireless cellular System on Chip (SoC) are experiencing unprecedented demands on data rate, latency use case variety. 5G wireless technologies require a massive number of antennas and complex signal processing to improve bandwidth and spectral efficiency. The Internet of Things is causing a proliferation in the number of connected devices, and service categories, such as ultra-reliable low latency, which will produce new use cases, such as self-driving cars, robotic factories, and remote surgery. In addressing these challenges, we can no longer rely on faster cores, or even more silicon. Modem software development is becoming increasingly error prone and difficult as the complexity of the applications and the architectures increase. In this report we propose a Wireless Domain Specific Solution that takes a Dataflow acceleration approach and addresses the need of the SoC to support dataflows that change with use case and user activity, while maintaining the Firm Real Time High Availability with low probability of Heisenbugs that is required in cellular modems. We do this by developing a Domain Specific Architecture that describes the requirements in a suitably abstracted dataflow Domain Specific language. A toolchain is described that automates translation of those requirements in an efficient and robust manner and provides formal guarantees against Heisenbugs. The dataflow native DSA supports the toolchain output with specialized processing, data management and control features with high performance and low power, and recovers rapidly from dropped dataflows while continuing to achieve the real time requirements. This report focuses on the dataflow acceleration in the DSA and the part of the automated toolchain that formally checks the performance and correctness of software running on this dataflow hardware. Results are presented and a summary of future work is given.", "venue": "ArXiv", "authors": ["Alan  Gatherer", "Ashish  Shrivastava", "Hao  Luan", "Asheesh  Kashyap", "Zhenguo  Gu", "Miguel  Dajer"], "year": 2020, "n_citations": 0}
{"id": 2424595, "s2_id": "711e6b20bbef7cd369703c30505b87294692632b", "title": "Formal analysis of SEU mitigation for early dependability and performability analysis of FPGA-based space applications", "abstract": "Abstract SRAM-based FPGAs are increasingly popular in the aerospace industry due to their field programmability and low cost. However, they suffer from cosmic radiation induced Single Event Upsets (SEUs). In safety-critical applications, the dependability of the design is a prime concern since failures may have catastrophic consequences. An early analysis of the relationship between dependability metrics, performability-area trade-off, and different mitigation techniques for such applications can reduce the design effort while increasing the design confidence. This paper introduces a novel methodology based on probabilistic model checking, for the analysis of the reliability, availability, safety and performance-area tradeoffs of safety-critical systems for early design decisions. Starting from the high-level description of a system, a Markov reward model is constructed from the Control Data Flow Graph (CDFG) and a component characterization library targeting FPGAs. The proposed model and exhaustive analysis capture all the failure states (based on the fault detection coverage) and repairs possible in the system. We present quantitative results based on an FIR filter circuit to illustrate the applicability of the proposed approach and to demonstrate that a wide range of useful dependability and performability properties can be analyzed using the proposed methodology. The modeling results show the relationship between different mitigation techniques and fault detection coverage, exposing their direct impact on the design for early decisions.", "venue": "J. Appl. Log.", "authors": ["Khaza Anuarul Hoque", "Otmane A\u00eft Mohamed", "Yvon  Savaria"], "year": 2017, "n_citations": 13}
{"id": 2426961, "s2_id": "eb6a81c35b116cc85ce3c1329bfdb51a2ad2343b", "title": "MGSim - Simulation tools for multi-core processor architectures", "abstract": "MGSim is an open source discrete event simulator for on-chip hardware components, developed at the University of Amsterdam. It is intended to be a research and teaching vehicle to study the fine-grained hardware/software interactions on many-core and hardware multithreaded processors. It includes support for core models with dierent instruction sets, a configurable multi-core interconnect, multiple configurable cache and memory models, a dedicated I/O subsystem, and comprehensive monitoring and interaction facilities. The default model configuration shipped with MGSim implements Microgrids, a many-core architecture with hardware concurrency management. MGSim is furthermore written mostly in C++ and uses object classes to represent chip components. It is optimized for architecture models that can be described as process networks.", "venue": "ArXiv", "authors": ["Mike  Lankamp", "Raphael 'kena' Poss", "Qiang  Yang", "Jian  Fu", "M. Irfan Uddin", "Chris R. Jesshope"], "year": 2013, "n_citations": 17}
{"id": 2436259, "s2_id": "017a2651563d860d42a8d8ab14a1ea8afa20d5a3", "title": "An Efficient Communication Protocol for FPGA IP Protection", "abstract": "We introduce a protection-based IP security scheme to protect soft and firm IP cores which are used on FPGA devices. The scheme is based on Finite State Machin (FSM) obfuscation and exploits Physical Unclonable Function (PUF) for FPGA unique identification (ID) generation which help payper-device licensing. We introduce a communication protocol to protect the rights of parties in this market. On standard benchmark circuits, the experimental results show that our scheme is secure, attack-resilient and can be implemented with low area, power and delay overheads. KeywordsField Programmable Gate Array (FPGA), Finite State Machine (FSM), Intellectual Property (IP) Protection, Hardware Security.", "venue": "ArXiv", "authors": ["Farzane  Khajuyi", "Behnam  Ghavami", "Human  Nikmehr"], "year": 2021, "n_citations": 0}
{"id": 2441128, "s2_id": "3d83961bab2a7293f643c6839f60862de1959336", "title": "Using ECC DRAM to Adaptively Increase Memory Capacity", "abstract": "Modern DRAM modules are often equipped with hardware error correction capabilities, especially for DRAM deployed in large-scale data centers, as process technology scaling has increased the susceptibility of these devices to errors. To provide fast error detection and correction, error-correcting codes (ECC) are placed on an additional DRAM chip in a DRAM module. This additional chip expands the raw capacity of a DRAM module by 12.5%, but the applications are unable to use any of this extra capacity, as it is used exclusively to provide reliability for all data. In reality, there are a number of applications that do not need such strong reliability for all their data regions (e.g., some user batch jobs executing on a public cloud), and can instead benefit from using additional DRAM capacity to store extra data. Our goal in this work is to provide the additional capacity within an ECC DRAM module to applications when they do not need the high reliability of error correction. \nIn this paper, we propose Capacity- and Reliability-Adaptive Memory (CREAM), a hardware mechanism that adapts error correcting DRAM modules to offer multiple levels of error protection, and provides the capacity saved from using weaker protection to applications. For regions of memory that do not require strong error correction, we either provide no ECC protection or provide error detection using multibit parity. We evaluate several layouts for arranging the data within ECC DRAM in these reduced-protection modes, taking into account the various trade-offs exposed from exploiting the extra chip. Our experiments show that the increased capacity provided by CREAM improves performance by 23.0% for a memory caching workload, and by 37.3% for a commercial web search workload executing production query traces. In addition, CREAM can increase bank-level parallelism within DRAM, offering further performance improvements.", "venue": "ArXiv", "authors": ["Yixin  Luo", "Saugata  Ghose", "Tianshi  Li", "Sriram  Govindan", "Bikash  Sharma", "Bryan  Kelly", "Amirali  Boroumand", "Onur  Mutlu"], "year": 2017, "n_citations": 13}
{"id": 2445025, "s2_id": "462a6f3c20d55e251b007a8f8f3d84ad4086e6bd", "title": "Recent Advances in Overcoming Bottlenecks in Memory Systems and Managing Memory Resources in GPU Systems", "abstract": "This article features extended summaries and retrospectives of some of the recent research done by our research group, SAFARI, on (1) various critical problems in memory systems and (2) how memory system bottlenecks affect graphics processing unit (GPU) systems. As more applications share a single system, operations from each application can contend with each other at various shared components. Such contention can slow down each application or thread of execution. The compound effect of contention, high memory latency and access overheads, as well as inefficient management of resources, greatly degrades performance, quality-of-service, and energy efficiency. The ten works featured in this issue study several aspects of (1) inter-application interference in multicore systems, heterogeneous systems, and GPUs; (2) the growing overheads and expenses associated with growing memory densities and latencies; and (3) performance, programmability, and portability issues in modern GPUs, especially those related to memory system resources.", "venue": "ArXiv", "authors": ["Onur  Mutlu", "Saugata  Ghose", "Rachata  Ausavarungnirun"], "year": 2018, "n_citations": 1}
{"id": 2446841, "s2_id": "d74bdbc0bce8ff90e815c74368cdac49b0eb4185", "title": "PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference", "abstract": "Memristor crossbars are circuits capable of performing analog matrix-vector multiplications, overcoming the fundamental energy efficiency limitations of digital logic. They have been shown to be effective in special-purpose accelerators for a limited set of neural network applications. We present the Programmable Ultra-efficient Memristor-based Accelerator (PUMA) which enhances memristor crossbars with general purpose execution units to enable the acceleration of a wide variety of Machine Learning (ML) inference workloads. PUMA's microarchitecture techniques exposed through a specialized Instruction Set Architecture (ISA) retain the efficiency of in-memory computing and analog circuitry, without compromising programmability. We also present the PUMA compiler which translates high-level code to PUMA ISA. The compiler partitions the computational graph and optimizes instruction scheduling and register allocation to generate code for large and complex workloads to run on thousands of spatial cores. We have developed a detailed architecture simulator that incorporates the functionality, timing, and power models of PUMA's components to evaluate performance and energy consumption. A PUMA accelerator running at 1 GHz can reach area and power efficiency of 577 GOPS/s/mm 2 and 837~GOPS/s/W, respectively. Our evaluation of diverse ML applications from image recognition, machine translation, and language modelling (5M-800M synapses) shows that PUMA achieves up to 2,446\u00d7 energy and 66\u00d7 latency improvement for inference compared to state-of-the-art GPUs. Compared to an application-specific memristor-based accelerator, PUMA incurs small energy overheads at similar inference latency and added programmability.", "venue": "ASPLOS", "authors": ["Aayush  Ankit", "Izzat El Hajj", "Sai Rahul Chalamalasetti", "Geoffrey  Ndu", "Martin  Foltin", "R. Stanley Williams", "Paolo  Faraboschi", "Wen-Mei  Hwu", "John Paul Strachan", "Kaushik  Roy", "Dejan S. Milojicic"], "year": 2019, "n_citations": 129}
{"id": 2450645, "s2_id": "6aa42303a5bf220022977efa476c26c0d3f8579f", "title": "An Energy-Efficient Low-Voltage Swing Transceiver for mW-Range IoT End-Nodes", "abstract": "As the Internet-of-Things (IoT) applications become more and more pervasive, IoT end nodes are requiring more and more computational power within a few mW of power envelope, coupled with high-speed and energy-efficient inter-chip communication to deal with the growing input/output and memory bandwidth for emerging near-sensor analytics applications. While traditional interfaces such as SPI cannot cope with these tight requirements, low-voltage swing transceivers can tackle this challenge thanks to their capability to achieve several Gbps of bandwidth at extremely low power. However, recent research on high-speed serial links addressed this challenge only partially, proposing only partial or stand-alone designs, and not addressing their integration in real systems and the related implications. In this paper, we present for the first time a complete design and system-level architecture of a low-voltage swing transceiver integrated within a low-power (mW range) IoT end-node processors, and we compare it with existing microcontroller interfaces. The transceiver, implemented in a commercial 65-nm CMOS technology achieves 10.2\u00d7 higher energy efficiency at 15.7\u00d7 higher performance than traditional microcontroller peripherals (single lane).", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Hayate  Okuhara", "Ahmed  Elnaqib", "Davide  Rossi", "Alfio Di Mauro", "Philipp  Mayer", "Pierpaolo  Palestri", "Luca  Benini"], "year": 2020, "n_citations": 1}
{"id": 2451266, "s2_id": "8f4fba35edc162bd2903e33ea6060aece8163e8d", "title": "Dither computing: a hybrid deterministic-stochastic computing framework", "abstract": "Stochastic computing has a long history as an alternative method of performing arithmetic on a computer. While it can be considered an unbiased estimator of real numbers, it has a variance and MSE on the order of $\\displaystyle \\Omega(\\frac{1}{N})$. On the other hand, deterministic variants of stochastic computing remove the stochastic aspect, but cannot approximate arbitrary real numbers with arbitrary precision and are biased estimators. However, they have an asymptotically superior MSE on the order of $O(\\frac{1}{N^{2}})$. Recent results in deep learning with stochastic rounding suggest that the bias in the rounding can degrade performance. We proposed an alternative framework, called dither computing, that combines aspects of stochastic computing and its deterministic variants and that can perform computing with similar efficiency, is unbiased, and with a variance and MSE also on the optimal order of $\\displaystyle \\Theta(\\frac{1}{N^{2}})$. We also show that it can be beneficial in stochastic rounding applications as well. We provide implementation details and give experimental results to comparatively show the benefits of the proposed scheme.", "venue": "2021 IEEE 28th Symposium on Computer Arithmetic (ARITH)", "authors": ["Chai Wah Wu"], "year": 2021, "n_citations": 0}
{"id": 2465325, "s2_id": "8d59795702e41708ecd676b9df2b15d9a0e7741e", "title": "A bi-directional Address-Event transceiver block for low-latency inter-chip communication in neuromorphic systems", "abstract": "Neuromorphic systems typically use the Address-Event Representation (AER) to transmit signals among nodes, cores, and chips. Communication of Address-Events (AEs) between neuromorphic cores/chips typically requires two parallel digital signal buses for Input/Output (I/O) operations. This requirement can become very expensive for large-scale systems in terms of both dedicated I/O pins and power consumption. In this paper we present a compact fully asynchronous event-driven transmitter/receiver block that is both power efficient and I/O efficient. This block implements high-throughput low-latency bi-directional communication through a parallel AER bus. We show that by placing the proposed AE transceiver block in two separate chips and linking them by a single AER bus, we can drive the communication and switch the transmission direction of the shared bus on a single event basis, from either side with low-latency. We present experimental results that validate the circuits proposed and demonstrate reliable bi-directional event transmission with high-throughput. The proposed AE block, integrated in a neuromorphic chip fabricated using a 28 nm FDSOI process, occupies a silicon die area of 140 \u03bcm \u00d7 70 \u03bcm. The experimental measurements show that the event-driven AE block combined with standard digital I/Os has a direction switch latency of 5 ns and can achieve a worst-case bi-directional event transmission throughput of 28.6M Events/second while consuming 11 pJ per event (26-bit) delivery.", "venue": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Ning  Qiao", "Giacomo  Indiveri"], "year": 2018, "n_citations": 1}
{"id": 2465777, "s2_id": "c60072959863d7063e849141467f90d2754942e7", "title": "Sparse Systolic Tensor Array for Efficient CNN Hardware Acceleration", "abstract": "Convolutional neural network (CNN) inference on mobile devices demands efficient hardware acceleration of low-precision (INT8) general matrix multiplication (GEMM). Exploiting data sparsity is a common approach to further accelerate GEMM for CNN inference, and in particular, structural sparsity has the advantages of predictable load balancing and very low index overhead. In this paper, we address a key architectural challenge with structural sparsity: how to provide support for a range of sparsity levels while maintaining high utilization of the hardware. We describe a time unrolled formulation of variable density-bound block (VDBB) sparsity that allows for a configurable number of non-zero elements per block, at constant utilization. We then describe a systolic array microarchitecture that implements this scheme, with two data reuse optimizations. Firstly, we increase reuse in both operands and partial products by increasing the number of MACs per PE. Secondly, we introduce a novel approach of moving the IM2COL transform into the hardware, which allows us to achieve a 3x data bandwidth expansion just before the operands are consumed by the datapath, reducing the SRAM power consumption. The optimizations for weight sparsity, activation sparsity and data reuse are all interrelated and therefore the optimal combination is not obvious. Therefore, we perform an design space evaluation to find the pareto-optimal design characteristics. The resulting design achieves 16.8 TOPS/W in 16nm with modest 50% model sparsity and scales with model sparsity up to 55.7TOPS/W at 87.5%. As well as successfully demonstrating the variable DBB technique, this result significantly outperforms previously reported sparse CNN accelerators.", "venue": "ArXiv", "authors": ["Zhi-Gang  Liu", "Paul N. Whatmough", "Matthew  Mattina"], "year": 2020, "n_citations": 2}
{"id": 2474115, "s2_id": "8e5a30011f16715d3ca58a81cf0b2f96a2f26534", "title": "Architectural Implications of Graph Neural Networks", "abstract": "Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This letter tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.", "venue": "IEEE Computer Architecture Letters", "authors": ["Zhihui  Zhang", "Jingwen  Leng", "Lingxiao  Ma", "Youshan  Miao", "Chao  Li", "Minyi  Guo"], "year": 2020, "n_citations": 9}
{"id": 2474380, "s2_id": "2cb0af20ab6b43e99795142fd76c92e907a1c96b", "title": "Speeding-up Logic Design and Refining Hardware EDA Flow by Exploring Chinese Character based Graphical Representation", "abstract": "Electrical design automation (EDA) techniques have deeply influenced the computer hardware design, especially in the field of very large scale Integration (VLSI) circuits. Particularly, the popularity of FPGA, ASIC and SOC applications have been dramatically increased due to the well developed EDA tool chains. Over decades, improving EDA tool in terms of functionality, efficiency, accuracy and intelligence is not only the academic research hot spot, but the industry attempting goal as well. \nIn this paper, a novel perspective is taken to review current mainstream EDA working flow and design methods, aiming to shorten the EDA design periods and simplify the logic design overload significantly. Specifically, three major contributions are devoted. First, a Chinese character based representation system (CCRS), which is used for presenting logical abstract syntax tree, is proposed. Second, the register-transfer-level (RTL) level symbolic description technique for CCRS are introduced to replace traditional text-based programming methods. Finally, the refined EDA design flow based on CCRS is discussed. It is convincing that the graphic non-pure-english based EDA flow could lower the design cost and complexity. As a fundamental trial in this new field, it is confirmative that a lot of following works will make the related EDA development prosperous.", "venue": "ArXiv", "authors": ["Shuangbai  Xue", "Yuan  Xue"], "year": 2020, "n_citations": 0}
{"id": 2475977, "s2_id": "c7d73455e21424744a2f9725f7831afdfd55ecec", "title": "Partial sums generation architecture for successive cancellation decoding of polar codes", "abstract": "Polar codes are a new family of error correction codes for which efficient hardware architectures have to be defined for the encoder and the decoder. Polar codes are decoded using the successive cancellation decoding algorithm that includes partial sums computations. We take advantage of the recursive structure of polar codes to introduce an efficient partial sums computation unit that can also implements the encoder. The proposed architecture is synthesized for several code-lengths in 65nm ASIC technology. The area of the resulting design is reduced up to 26% and the maximum working frequency is improved by 25%.", "venue": "SiPS 2013 Proceedings", "authors": ["Guillaume  Berhault", "Camille  Leroux", "Christophe  J\u00e9go", "Dominique  Dallet"], "year": 2013, "n_citations": 28}
{"id": 2476005, "s2_id": "ea7ddd0c4f050514ccb288b8fd4c0408805cdf00", "title": "An Imitation Learning Approach for Cache Replacement", "abstract": "Program execution speed critically depends on increasing cache hits, as cache hits are orders of magnitude faster than misses. To increase cache hits, we focus on the problem of cache replacement: choosing which cache line to evict upon inserting a new line. This is challenging because it requires planning far ahead and currently there is no known practical solution. As a result, current replacement policies typically resort to heuristics designed for specific common access patterns, which fail on more diverse and complex access patterns. In contrast, we propose an imitation learning approach to automatically learn cache access patterns by leveraging Belady's, an oracle policy that computes the optimal eviction decision given the future cache accesses. While directly applying Belady's is infeasible since the future is unknown, we train a policy conditioned only on past accesses that accurately approximates Belady's even on diverse and complex access patterns, and call this approach Parrot. When evaluated on 13 of the most memory-intensive SPEC applications, Parrot increases cache miss rates by 20% over the current state of the art. In addition, on a large-scale web search benchmark, Parrot increases cache hit rates by 61% over a conventional LRU policy. We release a Gym environment to facilitate research in this area, as data is plentiful, and further advancements can have significant real-world impact.", "venue": "ICML", "authors": ["Evan Zheran Liu", "Milad  Hashemi", "Kevin  Swersky", "Parthasarathy  Ranganathan", "Junwhan  Ahn"], "year": 2020, "n_citations": 14}
{"id": 2476815, "s2_id": "1a3bd7ceef9a6a4dd25f5454b30f408f1520b77f", "title": "S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNNs) have achieved great success in performing cognitive tasks. However, execution of CNNs requires a large amount of computing resources and generates heavy memory traffic, which imposes a severe challenge on computing system design. Through optimizing parallel executions and data reuse in convolution, systolic architecture demonstrates great advantages in accelerating CNN computations. However, regular internal data transmission path in traditional systolic architecture prevents the systolic architecture from completely leveraging the benefits introduced by neural network sparsity. Deployment of fine-grained sparsity on the existing systolic architectures is greatly hindered by the incurred computational overheads. In this work, we propose S2 Engine \u2013 a novel systolic architecture that can fully exploit the sparsity in CNNs with maximized data reuse. S2 Engine transmits compressed data internally and allows each processing element to dynamically select an aligned data from the compressed dataflow in convolution. Compared to the na\u0131\u0308ve systolic array, S2 Engine achieves about 3.2\u00d7 and about 3.0\u00d7 improvements on speed and energy efficiency, respectively.", "venue": "ArXiv", "authors": ["Jianlei  Yang", "Wenzhi  Fu", "Xingzhou  Cheng", "Xucheng  Ye", "Pengcheng  Dai", "Weisheng  Zhao"], "year": 2021, "n_citations": 1}
{"id": 2488216, "s2_id": "f4d73402126dfb217ecbef5f93edf21a3ea5bec4", "title": "A Machine Learning Accelerator In-Memory for Energy Harvesting", "abstract": "There is increasing demand to bring machine learning capabilities to low power devices. By integrating the computational power of machine learning with the deployment capabilities of low power devices, a number of new applications become possible. In some applications, such devices will not even have a battery, and must rely solely on energy harvesting techniques. This puts extreme constraints on the hardware, which must be energy efficient and capable of tolerating interruptions due to power outages. Here, as a representative example, we propose an in-memory support vector machine learning accelerator utilizing non-volatile spintronic memory. The combination of processing-in-memory and non-volatility provides a key advantage in that progress is effectively saved after every operation. This enables instant shut down and restart capabilities with minimal overhead. Additionally, the operations are highly energy efficient leading to low power consumption.", "venue": "ArXiv", "authors": ["Salonik  Resch", "S. Karen Khatamifard", "Zamshed Iqbal Chowdhury", "Masoud  Zabihi", "Zhengyang  Zhao", "Jian-Ping  Wang", "Sachin S. Sapatnekar", "Ulya R. Karpuzcu"], "year": 2019, "n_citations": 1}
{"id": 2492517, "s2_id": "51d945b2b2a3533688e193d42b7ec9450b5a561a", "title": "OCCAM: Optimal Data Reuse for Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNNs) are emerging as powerful tools for image processing in important commercial applications. We focus on the important problem of improving the latency of image recognition. While CNNs are amenable highly to prefetching and multithreading to avoid memory latency issues, CNNs\u2019 large data \u2013 each layer\u2019s input, filters, and output \u2013 poses a memory bandwidth problem. While previous work captures only some of the enormous data reuse, full reuse implies that the initial input image and filters are read once from off chip and the final output is written once off chip without spilling the intermediate layers\u2019 data to off-chip. We propose Occam to capture full reuse via four contributions. First, we identify the necessary condition for full reuse. Second, we identify the dependence closure as the sufficient condition to capture full reuse using the least on-chip memory. Third, because the dependence closure is often too large to fit in on-chip memory, we propose a dynamic programming algorithm that optimally partitions a given CNN to guarantee the least off-chip traffic at the partition boundaries for a given on-chip capacity. While tiling is well-known, our contribution is determining the optimal cross-layer tiles. Occam\u2019s partitions reside on different chips forming a pipeline so that a partition\u2019s filters and dependence closure remain on-chip as different images pass through (i.e., each partition incurs off-chip traffic only for its inputs and outputs). Finally, because the optimal partitions may result in an unbalanced pipeline, we propose staggered asynchronous pipelines (STAP) which replicates the bottleneck stages to improve throughput by staggering the mini-batches across the replicas. Importantly, STAP achieves balanced pipelines without changing Occam\u2019s optimal partitioning. Our simulations show that, on average, Occam cuts off-chip transfers by 21x and achieves 2.06x and 1.36x better performance, and 33% and 24% better energy than the base case and Layer Fusion, respectively. Using an FPGA implementation, Occam performs 5.1x better, on average, than the base case.", "venue": "ArXiv", "authors": ["Ashish  Gondimalla", "Jianqiao  Liu", "T. N. Vijaykumar", "Mithuna  Thottethodi"], "year": 2021, "n_citations": 0}
{"id": 2495356, "s2_id": "c221c91bef8f219dccb973d105b27ceaec7daaf6", "title": "FantastIC4: A Hardware-Software Co-Design Approach for Efficiently Running 4Bit-Compact Multilayer Perceptrons", "abstract": "With the growing demand for deploying Deep Learning models to the \u201cedge\u201d, it is paramount to develop techniques that allow to execute state-of-the-art models within very tight and limited resource constraints. In this work we propose a software-hardware optimization paradigm for obtaining a highly efficient execution engine of deep neural networks (DNNs) that are based on fully-connected layers. The work\u2019s approach is centred around compression as a means for reducing the area as well as power requirements of, concretely, multilayer perceptrons (MLPs) with high predictive performances. Firstly, we design a novel hardware architecture named FantastIC4, which (1) supports the efficient on-chip execution of multiple compact representations of fully-connected layers and (2) minimizes the required number of multipliers for inference down to only 4 (thus the name). Moreover, in order to make the models amenable for efficient execution on FantastIC4, we introduce a novel entropy-constrained training method that renders them to be robust to 4bit quantization and highly compressible in size simultaneously. The experimental results show that we can achieve throughputs of 2.45 TOPS with a total power consumption of 3.6W on a Virtual Ultrascale FPGA XCVU440 device implementation, and achieve a total power efficiency of 20.17 TOPS/W on a 22nm process ASIC version. When compared to other state-of-the-art accelerators designed for the Google Speech Command (GSC) dataset, FantastIC4 is better by $51\\times $ in terms of throughput and $145\\times $ in terms of area efficiency (GOPS/mm2).", "venue": "IEEE Open Journal of Circuits and Systems", "authors": ["Simon  Wiedemann", "Suhas  Shivapakash", "Daniel  Becking", "Pablo  Wiedemann", "Wojciech  Samek", "Friedel  Gerfers", "Thomas  Wiegand"], "year": 2021, "n_citations": 4}
{"id": 2498991, "s2_id": "465c39d310e672bba7a735207e71b97201ec1803", "title": "Analog and digital circuit design in 65 nm CMOS: end of the road?", "abstract": "This introductory embedded tutorial gives an overview of the design problems at hand when designing integrated electronic systems in nanometer-scale CMOS technologies. First, some general problems that affect circuit design are addressed, such as the increased leakage and variability with scaling technologies. Next, the impact of this on digital circuit design and embedded memories is discussed. Finally, problems bothering embedded analog circuits are presented, such as reducing supply voltages, poor design productivity and signal integrity troubles. Addressing these problems will determine whether the design road ends at CMOS technology marker \"65 nm \" or not.", "venue": "Design, Automation and Test in Europe", "authors": ["Georges G. E. Gielen", "Wim  Dehaene", "Phillip  Christie", "Dieter  Draxelmayr", "Edmond  Janssens", "Karen  Maex", "Ted  Vucurevich"], "year": 2005, "n_citations": 68}
{"id": 2499363, "s2_id": "f2f3045dd63ace1837a2f3fb4764dc35fd532397", "title": "WinoCNN: Kernel Sharing Winograd Systolic Array for Efficient Convolutional Neural Network Acceleration on FPGAs", "abstract": "The combination of Winograd\u2019s algorithm and systolic array architecture has demonstrated the capability of improving DSP efficiency in accelerating convolutional neural networks (CNNs) on FPGA platforms. However, handling arbitrary convolution kernel sizes in FPGA-based Winograd processing elements and supporting efficient data access remain underexplored. In this work, we are the first to propose an optimized Winograd processing element (WinoPE), which can naturally support multiple convolution kernel sizes with the same amount of computing resources and maintains high runtime DSP efficiency. Using the proposed WinoPE, we construct a highly efficient systolic array accelerator, termed WinoCNN. We also propose a dedicated memory subsystem to optimize the data access. Based on the accelerator architecture, we build accurate resource and performance modeling to explore optimal accelerator configurations under different resource constraints. We implement our proposed accelerator on multiple FPGAs, which outperforms the state-of-the-art designs in terms of both throughput and DSP efficiency. Our implementation achieves DSP efficiency up to 1.33 GOPS/DSP and throughput up to 3.1 TOPS with the Xilinx ZCU102 FPGA. These are 29.1% and 20.0% better than the best solutions reported previously, respectively.", "venue": "2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Xinheng  Liu", "Yao  Chen", "Cong  Hao", "Ashutosh  Dhar", "Deming  Chen"], "year": 2021, "n_citations": 1}
{"id": 2500915, "s2_id": "cfcba3a6e17c7524668deb489480f779ec66f86e", "title": "Using Libraries of Approximate Circuits in Design of Hardware Accelerators of Deep Neural Networks", "abstract": "Approximate circuits have been developed to provide good tradeoffs between power consumption and quality of service in error resilient applications such as hardware accelerators of deep neural networks (DNN). In order to accelerate the approximate circuit design process and to support a fair benchmarking of circuit approximation methods, libraries of approximate circuits have been introduced. For example, EvoApprox8b contains hundreds of 8-bit approximate adders and multipliers. By means of genetic programming we generated an extended version of the library in which thousands of 8- to 128-bit approximate arithmetic circuits are included. These circuits form Pareto fronts with respect to several error metrics, power consumption and other circuit parameters. In our case study we show how a large set of approximate multipliers can be used to perform a resilience analysis of a hardware accelerator of ResNet DNN and to select the most suitable approximate multiplier for a given application. Results are reported for various instances of the ResNet DNN trained on CIFAR-10 benchmark problem.", "venue": "2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)", "authors": ["Vojtech  Mrazek", "Lukas  Sekanina", "Zdenek  Vasicek"], "year": 2020, "n_citations": 5}
{"id": 2504930, "s2_id": "32d37f7aae3e5f04d7fbd0f5e6bafe5f1b2ca214", "title": "Timing Aware Dummy Metal Fill Methodology", "abstract": "In this paper, we analyzed parasitic coupling capacitance coming from dummy metal fill and its impact on timing. Based on the modeling, we proposed two approaches to minimize the timing impact from dummy metal fill. The first approach applies more spacing between critical nets and metal fill, while the second approach leverages the shielding effects of reference nets. Experimental results show consistent improvement compared to traditional metal fill method.", "venue": "ArXiv", "authors": ["Luis  Charre", "Bruno  Gravano", "R\u00e9mi  P\u00f4ssas", "Chen  Zheng"], "year": 2017, "n_citations": 2}
{"id": 2511359, "s2_id": "0be7faada57093ad1ec2e81b4e0c5aee4a55f394", "title": "An ECG-on-Chip With 535 nW/Channel Integrated Lossless Data Compressor for Wireless Sensors", "abstract": "This paper presents a low-power ECG recording system-on-chip (SoC) with on-chip low-complexity lossless ECG compression for data reduction in wireless/ambulatory ECG sensor devices. The chip uses a linear slope predictor for data compression, and incorporates a novel low-complexity dynamic coding-packaging scheme to frame the prediction error into fixed-length 16 bit format. The proposed technique achieves an average compression ratio of 2.25\u00d7 on MIT/BIH ECG database. Implemented in a standard 0.35 \u03bcm process, the compressor uses 0.565 K gates/channel occupying 0.4 mm2 for four channels, and consumes 535 nW/channel at 2.4 V for ECG sampled at 512 Hz. Small size and ultra-low-power consumption makes the proposed technique suitable for wearable ECG sensor applications.", "venue": "IEEE Journal of Solid-State Circuits", "authors": ["Chacko John Deepu", "Xiaoyang  Zhang", "Wen-Sin  Liew", "David Liang Tai Wong", "Yong  Lian"], "year": 2014, "n_citations": 60}
{"id": 2512032, "s2_id": "157b5116a42a14718fd521bc9e17efcabba57551", "title": "Enabling Fine-Grain Restricted Coset Coding Through Word-Level Compression for PCM", "abstract": "Phase change memory (PCM) has recently emerged as a promising technology to meet the fast growing demand for large capacity memory in computer systems, replacing DRAM that is impeded by physical limitations. Multi-level cell (MLC) PCM offers high density with low per-byte fabrication cost. However, despite many advantages, such as scalability and low leakage, the energy for programming intermediate states is considerably larger than programing single-level cell PCM. In this paper, we study encoding techniques to reduce write energy for MLC PCM when the encoding granularity is lowered below the typical cache line size. We observe that encoding data blocks at small granularity to reduce write energy actually increases the write energy because of the auxiliary encoding bits. We mitigate this adverse effect by 1) designing suitable codeword mappings that use fewer auxiliary bits and 2) proposing a new Word-Level Compression (WLC) which compresses more than 91% of the memory lines and provides enough room to store the auxiliary data using a novel restricted coset encoding applied at small data block granularities. Experimental results show that the proposed encoding at 16-bit data granularity reduces the write energy by 39%, on average, versus the leading encoding approach for write energy reduction. Furthermore, it improves endurance by 20% and is more reliable than the leading approach. Hardware synthesis evaluation shows that the proposed encoding can be implemented on-chip with only a nominal area overhead.", "venue": "2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Seyed Mohammad Seyedzadeh", "Alex K. Jones", "Rami G. Melhem"], "year": 2018, "n_citations": 10}
{"id": 2513467, "s2_id": "4bcf63b73d5f464f2c8f194e5adef6d1d06e686e", "title": "Dynamic Priority Queue: An SDRAM Arbiter With Bounded Access Latencies for Tight WCET Calculation", "abstract": "This report introduces a shared resource arbitration scheme \\DPQ - Dynamic Priority Queue\" which provides bandwidth guarantees and low worst case latency to each master in an MPSoC. Being a non-trivial candidate for timing analysis, SDRAM has been chosen as a showcase, but the approach is valid for any shared resource arbitration. Due to its signicant cost, data rate and physical size advantages, SDRAM is a potential candidate for cost sensitive, safety critical and space conserving systems. The variable access latency is a major drawback of SDRAM that induces largely over estimated Worst Case Execution Time (WCET) bounds of applications. In this report we present the DPQ together with an algorithm to predict the shared SDRAM\u2019s worst case latencies. We use the approach to calculate WCET bounds of six hardware tasks executing on an Altera Cyclone III FPGA with shared DDR2 memory. The results show that the DPQ is a fair arbitration scheme and produces low WCET bounds.", "venue": "ArXiv", "authors": ["Hardik  Shah", "Andreas  Raabe", "Alois  Knoll"], "year": 2012, "n_citations": 3}
{"id": 2514533, "s2_id": "a1758555278b547280fa5e355eaddd380d5a1796", "title": "Power-performance trade-offs in nanometer-scale multi-level caches considering total leakage", "abstract": "In this paper, we investigate the impact of T/sub ox/ and V/sub th/ on power performance trade-offs for on-chip caches. We start by examining the optimization of the various components of a single level cache and then extend this to two level cache systems. In addition to leakage, our studies also account for the dynamic power expended as a result of cache misses. Our results show that one can often reduce overall power by increasing the size of the L2 cache if we only allow one pair of V/sub th//T/sub ox/ in L2. However, if we allow the memory cells and the peripherals to have their own V/sub th/ and T/sub ox/, we show that a two-level cache system with smaller L2s will yield less total leakage. We further show that two V/sub th/ and two T/sub ox/ are sufficient to get close to an optimal solution, and that V/sub th/ is generally a better design knob than T/sub ox/ for leakage optimization, thus it is better to restrict the number of T/sub ox/ rather than V/sub th/ if cost is a concern.", "venue": "Design, Automation and Test in Europe", "authors": ["Robert  Bai", "Nam Sung Kim", "Taeho  Kgil", "Dennis  Sylvester", "Trevor N. Mudge"], "year": 2005, "n_citations": 8}
{"id": 2516424, "s2_id": "849212ad7901fbb8df038672564eafaa590bf3f1", "title": "Resistive Neural Hardware Accelerators", "abstract": "Deep Neural Networks (DNNs), as a subset of Machine Learning (ML) techniques, entail that real-world data can be learned and that decisions can be made in real time. However, their wide adoption is hindered by a number of software and hardware limitations. The existing general-purpose hardware platforms used to accelerate DNNs are facing new challenges associated with the growing amount of data and are exponentially increasing the complexity of computations. An emerging non-volatile memory (NVM) devices and processing-in-memory (PIM) paradigm is creating a new hardware architecture generation with increased computing and storage capabilities. In particular, the shift towards ReRAM-based in-memory computing has great potential in the implementation of area and power efficient inference and in training large-scale neural network architectures. These can accelerate the process of the IoT-enabled AI technologies entering our daily life. In this survey, we review the state-of-the-art ReRAM-based DNN many-core accelerators, and their superiority compared to CMOS counterparts was shown. The review covers different aspects of hardware and software realization of DNN accelerators, their present limitations, and future prospectives. In particular, comparison of the accelerators shows the need for the introduction of new performance metrics and benchmarking standards. In addition, the major concerns regarding the efficient design of accelerators include a lack of accuracy in simulation tools for software and hardware co-design.", "venue": "ArXiv", "authors": ["Kamilya  Smagulova", "Mohammed E. Fouda", "Fadi  Kurdahi", "Khaled  Salama", "Ahmed  Eltawil"], "year": 2021, "n_citations": 0}
{"id": 2519757, "s2_id": "6273cf20ea40882453af3364bcb276b04d985a61", "title": "Low-Latency VLSI Architectures for Modular Polynomial Multiplication via Fast Filtering and Applications to Lattice-Based Cryptography", "abstract": "This paper presents a low-latency hardware accelerator for modular polynomial multiplication for lattice-based post-quantum cryptography and homomorphic encryption applications. The proposed novel modular polynomial multiplier exploits the fast finite impulse response (FIR) filter architecture to reduce the computational complexity for the schoolbook modular polynomial multiplication. We also extend this structure to fast M -parallel architectures while achieving low-latency, high-speed, and full hardware utilization. We comprehensively evaluate the performance of the proposed architectures under various polynomial settings as well as in the Saber scheme for post-quantum cryptography as a case study. The experimental results show that our design reduces the computational time and area-time product by 61% and 32%, respectively, compared to the state-of-the-art designs.", "venue": "ArXiv", "authors": ["Weihang  Tan", "Antian  Wang", "Yingjie  Lao", "Xinmiao  Zhang", "Keshab K. Parhi"], "year": 2021, "n_citations": 0}
{"id": 2526493, "s2_id": "774295eec4466aff34247e6ba0e06682817d7ded", "title": "Post-Training Sparsity-Aware Quantization", "abstract": "Quantization is a technique used in deep neural networks (DNNs) to increase execution performance and hardware efficiency. Uniform post-training quantization (PTQ) methods are common, since they can be implemented efficiently in hardware and do not require extensive hardware resources or a training set. Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise. In this paper, we propose a sparsity-aware quantization (SPARQ) method, in which the unstructured and dynamic activation sparsity is leveraged in different representation granularities. 4-bit quantization, for example, is employed by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, while first skipping zero-value bits. Moreover, instead of quantizing activation-by-activation to 4 bits, we focus on pairs of 8-bit activations and examine whether one of the two is equal to zero. If one is equal to zero, the second can opportunistically use the other's 4-bit budget; if both do not equal zero, then each is dynamically quantized to 4 bits, as described. SPARQ achieves minor accuracy degradation, 2x speedup over widely used hardware architectures, and a practical hardware implementation. The code is available at this https URL.", "venue": "ArXiv", "authors": ["Gil  Shomron", "Freddy  Gabbay", "Samer  Kurzum", "Uri  Weiser"], "year": 2021, "n_citations": 2}
{"id": 2527635, "s2_id": "0db3ce23bcd1a86cdcb9cb83f0c5d4003d685549", "title": "Characteristic analysis of 1024-point quantized Radix-2 FFT/IFFT processor", "abstract": "The precise analysis and accurate measurement of harmonic provides a reliable scientific industrial application. However, the high performance DSP processor is the important method of electrical harmonic analysis. Hence, in this research work, the effort was taken to design a novel high-resolution single 1024-point fast Fourier transform (FFT) and inverse fast Fourier transform (IFFT) processors for improvement of the harmonic measurement techniques. Meanwhile the project is started with design and simulation to demonstrate the benefit that is achieved by the proposed 1024-point FFT/IFFT processor. Pipelined structure is incorporated in order to enhance the system efficiency. As such, a pipelined architecture was proposed to statically scale the resolution of the processor to suite adequate trade-off constraints. The proposed FFT makes use of programmable fixed-point/floating-point to realize higher precision FFT.", "venue": "2012 10th IEEE International Conference on Semiconductor Electronics (ICSE)", "authors": ["Rozita  Teymourzadeh", "Memtode  Jim", "Mok Vee Hong"], "year": 2012, "n_citations": 1}
{"id": 2543461, "s2_id": "ec7cd4b6427f2ed0eb88f13fc11f5dc0ffa36e59", "title": "Design of High Performance MIPS Cryptography Processor Based on T-DES Algorithm", "abstract": "ABSTRACT The paper describes the design of high performance MIPS Cryptography processor based on triple data encryption standard. The organization of pipeline stages in such a way that pipeline can be clocked at high frequency. Encryption and Decryption blocks of triple data encryption standard (T-DES) crypto system and dependency among themselves are explained in detail with the help of block diagram. In order to increase the processor functionality and performance, especially for security applications we include three new 32-bit instructions LKLW, LKUW and CRYPT. The design has been synthesized at 40nm process technology targeting using Xilinx Virtex-6 device. The overall MIPS Crypto processor works at 209MHz. Keywords ALU, register file, pipeline, memory, T-DES, throughput 1. INTRODUCTION oday\u2019s digital world, Cryptography is the art and science that deals with the principles and methods for keeping message secure. Encryption is emerging as a disintegrable part of all communication networks and information processing systems, involving transmission of data. Encryption is the transformation of plain data (known as plaintext) into inintengible data (known as cipher text) through an algorithm referred to as cipher. MIPS architecture employs a wide range of applications. The architecture remains the same for all MIPS based processors while the implementations may differ [1]. The proposed design has the feature of 32-bit asymmetric and symmetric cryptography system as a security application. There is a 16- bit RSA cryptography MIPS cryptosystem have been previously designed [2]. There are the small adjustments and minor improvement in the MIPS pipelined architecture design to protect data transmission over insecure medium using authenticating devices such as data encryption standard [DES], Triple-DES and advanced encryption standard [AES] [3]. These cryptographic devices use an identical key for the receiver side and sender side. Our design mainly includes the symmetric cryptosystem into MIPS pipeline stages. That is suitable to encrypt large amount data with high speed. The MIPS is simply known as Millions of instructions per second and is one of the best RISC (Reduced Instruction Set Computer) processor ever designed. High speed MIPS processor possessed Pipeline architecture for speed up processing, increase the frequency and performance of the processor. A MIPS based RISC processor was described in [4]. It consist of basic five stages of pipelining that are pipelined processor is shown in Fig.1 which containInstruction Fetch, Instruction Decode, Instruction Execution, Memory access, write back. These five pipeline stages generate 5 clock cycles processing delay and several Hazard during the operation [2]. These pipelining Hazard are eliminates by inserting NOP (No Operation Performed) instruction which generate some delays for the proper execution of instruction [4]. The pipelining Hazards are of three type\u2019s data, structural and control hazard. These hazards are handled in the MIPS processor by the implementation of forwarding unit, Pre-fetching or Hazard detection unit, branch and jump prediction unit [2]. Forwarding unit is used for preventing data hazards which detects the dependencies and forward the required data from the running instruction to the dependent instructions [5]. Stall are occurred in the pipelined architecture when the consecutive instruction uses the same operand of the instruction and that require more clock cycles for execution and reduces performance. To overcome this situation, instruction pre-fetching unit is used which reduces the stalls and improve performance. The control hazard are occurs when a branch prediction is mistaken or in general, when the system has no mechanism for handling the control hazards [5]. The control hazard is handled by two mechanisms: Flush mechanism and Delayed jump mechanism. The branch and jump prediction unit uses these two mechanisms for preventing control hazards. The flush mechanism runs instruction after a branch and flushes the pipe after the misprediction [5]. Frequent flushing may increase the clock cycles and reduce performance. In the delayed jump mechanism, to handle the control hazard is to fill the pipe after the jump instruction with specific numbers of NOP\u2019s [5]. The branch and jump prediction unit placement in the pipelining architecture may affect the critical or longest path. To detecting the longest path and improving the hardware that resulting minimum clock period and is the standard method of increasing the performance of the processor. To further speed up processor and minimize clock period, the design incorporates a high speed hybrid adder which employs both carry skip and carry select techniques with in the ALU unit to handle the additions. This paper is organized as follows. The system architecture hardware design and implementation are explained in Section II. Instruction set of MIPS including new instructions in detail with corresponding diagrams shown in sub-sections. Hardware implementation design methodology is explained in section III. The experimental results of pipeline stages are shown in section IV. Simulation results of encrypted MIPS pipeline processor and their Verification & synthesis report are describes in sub sections. The conclusions of paper are described in section V.", "venue": "ArXiv", "authors": ["Kirat Pal Singh", "Shivani  Parmar"], "year": 2015, "n_citations": 144}
{"id": 2543851, "s2_id": "a4e047366d1eab5fc8b78e8b023e4b17a48e839a", "title": "Hardware Implementation of Keyless Encryption Scheme for Internet of Things Based on Image of Memristors", "abstract": "The Internet of Things (IoT) is rapidly increasing the number of connected devices. This causes new concerns towards solutions for authenticating numerous IoT devices. Most of these devices are resource-constrained. Therefore, the use of long-secret keys, in traditional cryptography schemes can be hard to implement. Also, the key generation, distribution, and storage are very complex. Moreover, the goal of many reported cyber-attacks is accessing the key. Therefore, researchers have shown an increased interest in designing keyless encryption schemes recently. In this report, we are going to explain the details of the implementation of the keyless protocol by taking advantage of known technology modules such as microcontrollers (MCU), and hash functions. Physical Unclonable Functions (PUFs) have been used in many cryptographic applications such as Password Management Systems, key exchange, Key Generation. In this report, we are going to explain the details of the hardware implementation of keyless encryption in the MCU. Different kinds of memristors have been used in the past. In this work, a look-up-table containing memristor cells value at the various current levels is used since the physical component is unavailable yet. The hardware that is used to implement the system is an evaluation-board of SAMV71 MCU, which is used to implement the control system and hardware hashing.", "venue": "ArXiv", "authors": ["Mohammad  Mohammadinodoushan"], "year": 2020, "n_citations": 2}
{"id": 2544455, "s2_id": "43d38d95ccd41211bc2943519d99382062e5e24b", "title": "Proceedings of the First International Workshop on FPGAs for Software Programmers (FSP 2014)", "abstract": "This volume contains the papers accepted at the First International Workshop on FPGAs for Software Programmers (FSP 2014), held in Munich, Germany, September 1st, 2014. FSP 2014 was co-located with the International Conference on Field Programmable Logic and Applications (FPL).", "venue": "ArXiv", "authors": ["Frank  Hannig", "Dirk  Koch", "Daniel  Ziener"], "year": 2014, "n_citations": 1}
{"id": 2545558, "s2_id": "4857110cb00f9877077400dddfeb5b0150cf9df4", "title": "Neurostream: Scalable and Energy Efficient Deep Learning with Smart Memory Cubes", "abstract": "High-performance computing systems are moving towards 2.5D and 3D memory hierarchies, based on High Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) to mitigate the main memory bottlenecks. This trend is also creating new opportunities to revisit near-memory computation. In this paper, we propose a flexible processor-in-memory (PIM) solution for scalable and energy-efficient execution of deep convolutional networks (ConvNets), one of the fastest-growing workloads for servers and high-end embedded systems. Our co-design approach consists of a network of Smart Memory Cubes (modular extensions to the standard HMC) each augmented with a many-core PIM platform called NeuroCluster. NeuroClusters have a modular design based on NeuroStream coprocessors (for Convolution-intensive computations) and general-purpose RISC-V cores. In addition, a DRAM-friendly tiling mechanism and a scalable computation paradigm are presented to efficiently harness this computational capability with a very low programming effort. NeuroCluster occupies only 8 percent of the total logic-base (LoB) die area in a standard HMC and achieves an average performance of 240\u00a0GFLOPS for complete execution of full-featured state-of-the-art (SoA) ConvNets within a power budget of 2.5\u00a0W. Overall 11\u00a0W is consumed in a single SMC device, with 22.5\u00a0GFLOPS/W energy-efficiency which is 3.5X better than the best GPU implementations in similar technologies. The minor increase in system-level power and the negligible area increase make our PIM system a cost-effective and energy efficient solution, easily scalable to 955\u00a0GFLOPS with a small network of just four SMCs.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Erfan  Azarkhish", "Davide  Rossi", "Igor  Loi", "Luca  Benini"], "year": 2018, "n_citations": 66}
{"id": 2547907, "s2_id": "c33a94bbe0ec4cf7faf28642c313a19f59e885bd", "title": "Reversible logic based concurrent error detection methodology for emerging nanocircuits", "abstract": "Reversible logic has promising applications in emerging nanotechnologies, such as quantum computing, quantum dot cellular automata and optical computing, etc. Faults in reversible logic circuits that result in multi-bit error at the outputs are very tough to detect, and thus in literature, researchers have only addressed the problem of online testing of faults that result single-bit error at the outputs based on parity preserving logic. In this work, we propose a methodology for the concurrent error detection in reversible logic circuits to detect faults that can result in multi-bit error at the outputs. The methodology is based on the inverse property of reversible logic and is termed as \u2018inverse and compare\u2019 method. By using the inverse property of reversible logic, all the inputs can be regenerated at the outputs. Thus, by comparing the original inputs with the regenerated inputs, the faults in reversible circuits can be detected. Minimizing the garbage outputs is one of the main goals in reversible logic design and synthesis. We show that the proposed methodology results in \u2018garbageless\u2019 reversible circuits. A design of reversible full adder that can be concurrently tested for multi-bit error at the outputs is illustrated as the application of the proposed scheme. Finally, we showed the application of the proposed scheme of concurrent error detection towards fault detection in quantum dot cellular automata (QCA) emerging nanotechnology.", "venue": "10th IEEE International Conference on Nanotechnology", "authors": ["Himanshu  Thapliyal", "N.  Ranganathan"], "year": 2010, "n_citations": 22}
{"id": 2548222, "s2_id": "c792f7a17ea1b8bb5611513c9a926dcc8d70d5b0", "title": "Achieving Multi-port Memory Performance on Single-Port Memory with Coding Techniques", "abstract": "Many performance critical systems today must rely on performance enhancements, such as multi-port memories, to keep up with the increasing demand of memory-access capacity. However, the large area footprints and complexity of existing multi-port memory designs limit their applicability. This paper explores a coding theoretic framework to address this problem. In particular, this paper introduces a framework to encode data across multiple single-port memory banks in order to algorithmically realize the functionality of multi-port memory. This paper proposes three code designs with significantly less storage overhead compared to the existing replication based emulations of multi-port memories. To further improve performance, we also demonstrate a memory controller design that utilizes redundancy across coded memory banks to more efficiently schedule read and write requests sent across multiple cores. Furthermore, guided by DRAM traces, the paper explores dynamic coding techniques to improve the efficiency of the coding based memory design. We then show significant performance improvements in critical word read and write latency in the proposed coded-memory design when compared to a traditional uncoded-memory design.", "venue": "2020 3rd International Conference on Information and Computer Technologies (ICICT)", "authors": ["Hardik  Jain", "Matthew  Edwards", "Ethan  Elenberg", "Ankit Singh Rawat", "Sriram  Vishwanath"], "year": 2020, "n_citations": 0}
{"id": 2549664, "s2_id": "87a494fcc23aa9cea191f75e3b599e63d3bdab35", "title": "SMASH: Co-designing Software Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations", "abstract": "Important workloads, such as machine learning and graph analytics applications, heavily involve sparse linear algebra operations. These operations use sparse matrix compression as an effective means to avoid storing zeros and performing unnecessary computation on zero elements. However, compression techniques like Compressed Sparse Row (CSR) that are widely used today introduce significant instruction overhead and expensive pointer-chasing operations to discover the positions of the non-zero elements. In this paper, we identify the discovery of the positions (i.e., indexing) of non-zero elements as a key bottleneck in sparse matrix-based workloads, which greatly reduces the benefits of compression. We propose SMASH, a hardware-software cooperative mechanism that enables highly-efficient indexing and storage of sparse matrices. The key idea of SMASH is to explicitly enable the hardware to recognize and exploit sparsity in data. To this end, we devise a novel software encoding based on a hierarchy of bitmaps. This encoding can be used to efficiently compress any sparse matrix, regardless of the extent and structure of sparsity. At the same time, the bitmap encoding can be directly interpreted by the hardware. We design a lightweight hardware unit, the Bitmap Management Unit (BMU), that buffers and scans the bitmap hierarchy to perform highly-efficient indexing of sparse matrices. SMASH exposes an expressive and rich ISA to communicate with the BMU, which enables its use in accelerating any sparse matrix computation. We demonstrate the benefits of SMASH on four use cases that include sparse matrix kernels and graph analytics applications. Our evaluations show that SMASH provides average performance improvements of 38% for Sparse Matrix Vector Multiplication and 44% for Sparse Matrix Matrix Multiplication, over a state-of-the-art CSR implementation, on a wide variety of matrices with different characteristics. SMASH incurs a very modest hardware area overhead of up to 0.076% of an out-of-order CPU core.", "venue": "MICRO", "authors": ["Konstantinos  Kanellopoulos", "Nandita  Vijaykumar", "Christina  Giannoula", "Roknoddin  Azizi", "Skanda  Koppula", "Nika  Mansouri-Ghiasi", "Taha  Shahroodi", "Juan  G\u00f3mez-Luna", "Onur  Mutlu"], "year": 2019, "n_citations": 29}
{"id": 2551412, "s2_id": "8aa448ebf280d48d321d356a5d47c3e1e2460abb", "title": "Limit on the Addressability of Fault-Tolerant Nanowire Decoders", "abstract": "Although prone to fabrication error, the nanowire crossbar is a promising candidate compoent for next generation nanometer-scale circuits. In the nanowire crossbar architecture, nanowires are addressed by controlling voltages on the mesowires. For area efficiency, we are interested in the maximum number of nanowires N(m,e) that can be addressed by m mesowires, in the face of up to e fabrication errors. Asymptotically tight bounds on N(m,e) are established in this paper. In particular, it is shown that N(m,e) = Theta(2m / mepsiv+1/2). Interesting observations are made on the equivalence between this problem and the problem of constructing optimal EC/AUED codes, superimposed distance codes, pooling designs, and diffbounded set systems. Results in this paper also improve upon those in the EC/AUEC codes literature.", "venue": "IEEE Transactions on Computers", "authors": ["Yeow Meng Chee", "Alan C. H. Ling"], "year": 2009, "n_citations": 8}
{"id": 2552487, "s2_id": "560595948dde20705eb4340d2323707e6d4e1f00", "title": "A Design Methodology for Folded, Pipelined Architectures in VLSI Applications using Projective Space Lattices", "abstract": "Semi-parallel, or folded, VLSI architectures are used whenever hardware resources need to be saved at design time. Most recent applications that are based on Projective Geometry (PG) based balanced bipartite graph also fall in this category. In this paper, we provide a high-level, top-down design methodology to design optimal semi-parallel architectures for applications, whose Data Flow Graph (DFG) is based on PG bipartite graph. Such applications have been found e.g. in error-control coding and matrix computations. Unlike many other folding schemes, the topology of connections between physical elements does not change in this methodology. Another advantage is the ease of implementation. To lessen the throughput loss due to folding, we also incorporate a multi-tier pipelining strategy in the design methodology. The design methodology has been verified by implementing a synthesis tool in C++, which has been verified as well. The tool is publicly available. Further, a complete decoder was manually protototyped before the synthesis tool design, to verify all the algorithms evolved in this paper, towards various steps of refinement. Another specific high-performance design of an LDPC decoder based on this methodology was worked out in past, and has been patented as well.", "venue": "ArXiv", "authors": ["Hrishikesh  Sharma", "Sachin  Patkar"], "year": 2011, "n_citations": 5}
{"id": 2554182, "s2_id": "6914ddd27aac300a88d346dc7d028e8de218adfb", "title": "Decoding the Golden Code: A VLSI Design", "abstract": "The recently proposed Golden code is an optimal space-time block code for 2 times 2 multiple-input-multiple-output (MIMO) systems. The aim of this work is the design of a VLSI decoder for a MIMO system coded with the Golden code. The architecture is based on a rearrangement of the sphere decoding algorithm that achieves maximum-likelihood (ML) decoding performance. Compared to other approaches, the proposed solution exhibits an inherent flexibility in terms of QAM modulation size and this makes our architecture particularly suitable for adaptive modulation schemes. Relying on the flexibility of this approach two different architectures are proposed: a parametric one able to achieve high decoding throughputs (> 165 Mb/s) while keeping low overall decoder complexity (45 KGates), a flexible implementation able to dynamically adapt to the modulation scheme (4-,16-,64-QAM) retaining the low complexity and high throughput features.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Barbara  Cerato", "Guido  Masera", "Emanuele  Viterbo"], "year": 2009, "n_citations": 17}
{"id": 2554442, "s2_id": "7e601556ee5a539dccff0b4ea71347e1ef8ebf29", "title": "Hierarchical Roofline Analysis: How to Collect Data using Performance Tools on Intel CPUs and NVIDIA GPUs", "abstract": "This paper surveys a range of methods to collect necessary performance data on Intel CPUs and NVIDIA GPUs for hierarchical Roofline analysis. As of mid-2020, two vendor performance tools, Intel Advisor and NVIDIA Nsight Compute, have integrated Roofline analysis into their supported feature set. This paper fills the gap for when these tools are not available, or when users would like a more customized workflow for certain analysis. Specifically, we will discuss how to use Intel Advisor, RRZE LIKWID, Intel SDE and Intel Amplifier on Intel architectures, and nvprof, Nsight Compute metrics, and Nsight Compute section files on NVIDIA architectures. These tools will be used to collect information for as many memory/cache levels in the memory hierarchy as possible in order to provide insights into application's data reuse and cache locality characteristics.", "venue": "ArXiv", "authors": ["Charlene  Yang"], "year": 2020, "n_citations": 7}
{"id": 2555053, "s2_id": "ec4d28972a798c1bbd3ba3201c6b6ec1a865e526", "title": "GraVF-M: Graph Processing System Generation for Multi-FPGA Platforms", "abstract": "Due to the irregular nature of connections in most graph datasets, partitioning graph analysis algorithms across multiple computational nodes that do not share a common memory inevitably leads to large amounts of interconnect traffic. Previous research has shown that FPGAs can outcompete software-based graph processing in shared memory contexts, but it remains an open question if this advantage can be maintained in distributed systems.\n In this work, we present GraVF-M, a framework designed to ease the implementation of FPGA-based graph processing accelerators for multi-FPGA platforms with distributed memory. Based on a lightweight description of the algorithm kernel, the framework automatically generates optimized RTL code for the whole multi-FPGA design. We exploit an aspect of the programming model to present a familiar message-passing paradigm to the user, while under the hood implementing a more efficient architecture that can reduce the necessary inter-FPGA network traffic by a factor equal to the average degree of the input graph. A performance model based on a theoretical analysis of the factors influencing performance serves to evaluate the efficiency of our implementation. With a throughput of up to 5.8GTEPS (billions of traversed edges per second) on a 4-FPGA system, the designs generated by GraVF-M compare favorably to state-of-the-art frameworks from the literature and reach 94% of the projected performance limit of the system.", "venue": "TRETS", "authors": ["Nina  Engelhardt", "Hayden K.-H. So"], "year": 2019, "n_citations": 1}
{"id": 2556543, "s2_id": "217d1b66d45adfe73c9324a6751890d651e99dd5", "title": "Multiplying Matrices Without Multiplying", "abstract": "Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning. Consequently, there has been significant work on efficiently approximating matrix multiplies. We introduce a learning-based algorithm for this task that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs 100\u00d7 faster than exact matrix products and 10\u00d7 faster than current approximate methods. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling\u2014\u2013the core operations of our method\u2014\u2013could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have recently been the focus of substantial research and hardware investment.", "venue": "ICML", "authors": ["Davis  Blalock", "John  Guttag"], "year": 2021, "n_citations": 1}
{"id": 2557435, "s2_id": "1b60b7626a68e8be1aea923e7a4e62018bb2279c", "title": "SONIC: A Sparse Neural Network Inference Accelerator with Silicon Photonics for Energy-Efficient Deep Learning", "abstract": "Sparse neural networks can greatly facilitate the deployment of neural networks on resource-constrained platforms as they offer compact model sizes while retaining inference accuracy. Because of the sparsity in parameter matrices, sparse neural networks can, in principle, be exploited in accelerator architectures for improved energy-efficiency and latency. However, to realize these improvements in practice, there is a need to explore sparsity-aware hardware-software co-design. In this paper, we propose a novel silicon photonics-based sparse neural network inference accelerator called SONIC. Our experimental analysis shows that SONIC can achieve up to 5.8\u00d7 better performance-per-watt and 8.4\u00d7 lower energy-per-bit than state-ofthe-art sparse electronic neural network accelerators; and up to 13.8\u00d7 better performance-per-watt and 27.6\u00d7 lower energy-per-bit than the best known photonic neural network accelerators.", "venue": "ArXiv", "authors": ["Febin  Sunny", "Mahdi  Nikdast", "Sudeep  Pasricha"], "year": 2021, "n_citations": 1}
{"id": 2558062, "s2_id": "fd861a78c9db26dc4668a4fdfc73dfff58dadc8f", "title": "Polynesia: Enabling Effective Hybrid Transactional/Analytical Databases with Specialized Hardware/Software Co-Design", "abstract": "An exponential growth in data volume, combined with increasing demand for real-time analysis (i.e., using the most recent data), has resulted in the emergence of database systems that concurrently support transactions and data analytics. These hybrid transactional and analytical processing (HTAP) database systems can support real-time data analysis without the high costs of synchronizing across separate single-purpose databases. Unfortunately, for many applications that perform a high rate of data updates, state-of-the-art HTAP systems incur significant drops in transactional (up to 74.6%) and/or analytical (up to 49.8%) throughput compared to performing only transactions or only analytics in isolation, due to (1) data movement between the CPU and memory, (2) data update propagation, and (3) consistency costs. We propose Polynesia, a hardware\u2013software co-designed system for in-memory HTAP databases. Polynesia (1) divides the HTAP system into transactional and analytical processing islands, (2) implements custom algorithms and hardware to reduce the costs of update propagation and consistency, and (3) exploits processing-in-memory for the analytical islands to alleviate data movement. Our evaluation shows that Polynesia outperforms three state-of-the-art HTAP systems, with average transactional/analytical throughput improvements of 1.70X/3.74X, and reduces energy consumption by 48% over the prior lowest-energy system.", "venue": "ArXiv", "authors": ["Amirali  Boroumand", "Saugata  Ghose", "Geraldo F. Oliveira", "Onur  Mutlu"], "year": 2021, "n_citations": 4}
{"id": 2560188, "s2_id": "86578c79bb8b13a997d9a456f788b82bf45ceb7c", "title": "Accelerating Viterbi Algorithm using Custom Instruction Approach", "abstract": "In recent years, the decoding algorithms in communication networks are becoming increasingly complex aiming to achieve high reliability in correctly decoding received messages. These decoding algorithms involve computationally complex operations requiring high performance computing hardware, which are generally expensive. A cost-effective solution is to enhance the Instruction Set Architecture (ISA) of the processors by creating new custom instructions for the computational parts of the decoding algorithms. In this paper, we propose to utilize the custom instruction approach to efficiently implement the widely used Viterbi decoding algorithm by adding the assembly language instructions to the ISA of DLX, PicoJava II and NIOS II processors, which represent RISC, stack and FPGA-based soft-core processor architectures, respectively. By using the custom instruction approach, the execution time of the Viterbi algorithm is significantly improved by approximately 3 times for DLX and PicoJava II, and by 2 times for NIOS II.", "venue": "2018 14th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)", "authors": ["Waqar  Ahmad", "Imran Hafeez Abbassi", "Usman  Sanwal", "Hasan  Mahmood"], "year": 2018, "n_citations": 0}
{"id": 2564024, "s2_id": "6bafe45a5852f2877a8b2f36e9568b87dc7414d0", "title": "Low-Energy and CPA-Resistant Adiabatic CMOS/MTJ Logic for IoT Devices", "abstract": "The tremendous growth in the number of Internet of Things (IoT) devices has increased focus on the energy efficiency and security of an IoT device. In this paper, we will present a design level, non-volatile adiabatic architecture for low-energy and Correlation Power Analysis (CPA) resistant IoT devices. IoT devices constructed with CMOS integrated circuits suffer from high dynamic energy and leakage power. To solve this, we look at both adiabatic logic and STT-MTJs (Spin Transfer Torque Magnetic Tunnel Junctions) to reduce both dynamic energy and leakage power. Furthermore, CMOS integrated circuits suffer from side-channel leakage making them insecure against power analysis attacks. We again look to adiabatic logic to design secure circuits with uniform power consumption, thus, defending against power analysis attacks. We have developed a hybrid adiabatic-MTJ architecture using two-phase adiabatic logic. We show that hybrid adiabatic-MTJ circuits are both low energy and secure when compared with CMOS circuits. As a case study, we have constructed one round of PRESENT and have shown energy savings of 64.29% at a frequency of 25 MHz. Furthermore, we have performed a correlation power analysis attack on our proposed design and determined that the key was kept hidden.", "venue": "2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Zachary  Kahleifeh", "Himanshu  Thapliyal"], "year": 2021, "n_citations": 1}
{"id": 2564570, "s2_id": "e7cebf9cc8d2af11d0be908371924290aa8888a0", "title": "Using Cache-coloring to Mitigate Inter-set Write Variation in Non-volatile Caches", "abstract": "In recent years, researchers have explored use of non-volatile devices such as STT-RAM (spin torque transfer RAM) for designing on-chip caches, since they provide high density and consume low leakage power. A common limitation of all non-volatile devices is their limited write endurance. Further, since existing cache management policies are write-variation unaware, excessive writes to a few blocks may lead to a quick failure of the whole cache. We propose an architectural technique for wear-leveling of non-volatile last level caches (LLCs). Our technique uses cache-coloring approach which adds a software-controlled mapping layer between groups of physical pages and cache sets. Periodically the mapping is altered to ensure that write-traffic can be spread uniformly to different sets of the cache to achieve wear-leveling. Simulations performed with an x86-64 simulator and SPEC2006 benchmarks show that our technique reduces the worst-case writes to cache blocks and thus improves the cache lifetime by 4.07X.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 12}
{"id": 2567058, "s2_id": "b9ba5c670c1c6a0b41cb982b00bd20ace168ec79", "title": "MAVIREC: ML-Aided Vectored IR-Drop Estimation and Classification", "abstract": "Vectored IR drop analysis is a critical step in chip signoff that checks the power integrity of an on-chip power delivery network. Due to the prohibitive runtimes of dynamic IR drop analysis, the large number of test patterns must be whittled down to a small subset of worst-case IR vectors. Unlike the traditional slow heuristic method that select a few vectors with incomplete coverage, MAVIREC uses machine learning techniques-3D convolutions and regression-like layers-for accurately recommending a larger subset of test patterns that exercise worst-case scenarios. In under 30 minutes, MAVIREC profiles 100K-cycle vectors and provides better coverage than a state-of-the-art industrial flow. Further, MAVIREC's IR drop predictor shows 10X speedup with under 4mV Rmse relative to an industrial flow.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Vidya A. Chhabria", "Yanqing  Zhang", "Haoxing  Ren", "Ben  Keller", "Brucek  Khailany", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 3}
{"id": 2567637, "s2_id": "0f98311df6ba70701fda0af9082c8883f1a67684", "title": "SpikeDyn: A Framework for Energy-Efficient Spiking Neural Networks with Continual and Unsupervised Learning Capabilities in Dynamic Environments", "abstract": "Spiking Neural Networks (SNNs) bear the potential of efficient unsupervised and continual learning capabilities because of their biological plausibility, but their complexity still poses a serious research challenge to enable their energy-efficient design for resource-constrained scenarios (like embedded systems, IoT-Edge, etc.). We propose SpikeDyn, a comprehensive framework for energy-efficient SNNs with continual and unsupervised learning capabilities in dynamic environments, for both the training and inference phases. It is achieved through the following multiple diverse mechanisms: 1) reduction of neuronal operations, by replacing the inhibitory neurons with direct lateral inhibitions; 2) a memory- and energy-constrained SNN model search algorithm that employs analytical models to estimate the memory footprint and energy consumption of different candidate SNN models and selects a Pareto-optimal SNN model; and 3) a lightweight continual and unsupervised learning algorithm that employs adaptive learning rates, adaptive membrane threshold potential, weight decay, and reduction of spurious updates. Our experimental results show that, for a network with 400 excitatory neurons, our SpikeDyn reduces the energy consumption on average by 51% for training and by 37% for inference, as compared to the state-of-the-art. Due to the improved learning algorithm, SpikeDyn provides on avg. 21% accuracy improvement over the state-of-the-art, for classifying the most recently learned task, and by 8% on average for the previously learned tasks.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad  Shafique"], "year": 2021, "n_citations": 5}
{"id": 2567794, "s2_id": "967e9cbe42175f3216ce869e2af44668f3ba1ac3", "title": "Concept for a CMOS Image Sensor Suited for Analog Image Pre-Processing", "abstract": "A concept for a novel CMOS image sensor suited for analog image pre-processing is presented in this paper. As an example, an image restoration algorithm for reducing image noise is applied as image pre-processing in the analog domain. To supply low-latency data input for analog image preprocessing, the proposed concept for a CMOS image sensor offers a new sensor signal acquisition method in 2D. In comparison to image pre-processing in the digital domain, the proposed analog image pre-processing promises an improved image quality. Furthermore, the image noise at the stage of analog sensor signal acquisition can be used to select the most effective restoration algorithm applied to the analog circuit due to image processing prior to the A/D converter.", "venue": "ArXiv", "authors": ["Lan  Shi", "Christopher  Soell", "Andreas  Baenisch", "Robert  Weigel", "J\u00fcrgen  Seiler", "Thomas  Ussm\u00fcller"], "year": 2015, "n_citations": 6}
{"id": 2575025, "s2_id": "974074f651f06ffe3c6bafb1aa624609a754a376", "title": "Hardware Implementation of Fano Decoder for Polarization-adjusted Convolutional (PAC) Codes", "abstract": "This brief proposes a hardware implementation architecture for Fano decoding of polarization-adjusted convolutional (PAC) codes. This architecture uses a novel branch metric unit specific to PAC codes. The proposed decoder is tested on FPGA, and its performance is evaluated on ASIC using TSMC 28 nm 0.72 V library. The decoder can be clocked at 500 MHz and reach an average information throughput of 38 Mb/s at 3.5 dB signal-to-noise ratio for a block length of 128 and a code rate of 1/2.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Amir  Mozammel"], "year": 2021, "n_citations": 0}
{"id": 2576497, "s2_id": "0a1fd7d7d409654be073818e46964394870b072f", "title": "Dark Memory and Accelerator-Rich System Optimization in the Dark Silicon Era", "abstract": "Unlike traditional dark silicon works that attack the computing logic, this article puts a focus on the memory part, which dissipates most of the energy for memory-bound CPU applications. This article discusses the dark memory state and present Pareto curves for compute units, accelerators, and on-chip memory, and motivates the need for HW/SW codesign for parallelism and locality. \u2013Muhammad Shafique, Vienna University of Technology", "venue": "IEEE Design & Test", "authors": ["Ardavan  Pedram", "Stephen  Richardson", "Mark  Horowitz", "Sameh  Galal", "Shahar  Kvatinsky"], "year": 2017, "n_citations": 83}
{"id": 2577564, "s2_id": "1e2b14f5e6df5be380a172e9cebc85fb49976b25", "title": "Reliability Assessment and Quantitative Evaluation of Soft-Error Resilient 3D Network-on-Chip Systems", "abstract": "Three-Dimensional Networks-on-Chips (3D-NoCs) have been proposed as an auspicious solution, merging the high parallelism of the Network-on-Chip (NoC) paradigm with the high-performance and low-power cost of 3D-ICs. However, as technology scales down, the reliability issues are becoming more crucial, especially for complex 3D-NoC which provides the communication requirements of multi and many-core systems-on-chip. Reliability assessment is prominent for early stages of the manufacturing process to prevent costly redesigns of a target system. In this paper, we present an accurate reliability assessment and quantitative evaluation of a soft-error resilient 3D-NoC based on a soft-error resilient mechanism. The system can recover from transient errors occurring in different pipeline stages of the router. Based on this analysis, the effects of failures in the network's principal components are determined.", "venue": "2016 IEEE 25th Asian Test Symposium (ATS)", "authors": ["Khanh N. Dang", "Michael Conrad Meyer", "Yuichi  Okuyama", "Ben A. Abderazek"], "year": 2016, "n_citations": 8}
{"id": 2589657, "s2_id": "5efb3395cd4b8749c50dba2df46d0ccc62cd1a76", "title": "Structured Model Pruning of Convolutional Networks on Tensor Processing Units", "abstract": "The deployment of convolutional neural networks is often hindered by high computational and storage requirements. Structured model pruning is a promising approach to alleviate these requirements. Using the VGG-16 model as an example, we measure the accuracy-efficiency trade-off for various structured model pruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units (TPUs). To measure the actual performance of models, we develop a structured model pruning library for TensorFlow2 to modify models in place (instead of adding mask layers). We show that structured model pruning can significantly improve model memory usage and speed on TPUs without losing accuracy, especially for small datasets (e.g., CIFAR-10).", "venue": "ArXiv", "authors": ["Kongtao  Chen", "Ken  Franko", "Ruoxin  Sang"], "year": 2021, "n_citations": 1}
{"id": 2590944, "s2_id": "76938f756d382f00a24047aa30d0e3f328ccf75a", "title": "Hardware Implementation of Deep Network Accelerators Towards Healthcare and Biomedical Applications", "abstract": "The advent of dedicated Deep Learning (DL) accelerators and neuromorphic processors has brought on new opportunities for applying both Deep and Spiking Neural Network (SNN) algorithms to healthcare and biomedical applications at the edge. This can facilitate the advancement of medical Internet of Things (IoT) systems and Point of Care (PoC) devices. In this paper, we provide a tutorial describing how various technologies including emerging memristive devices, Field Programmable Gate Arrays (FPGAs), and Complementary Metal Oxide Semiconductor (CMOS) can be used to develop efficient DL accelerators to solve a wide variety of diagnostic, pattern recognition, and signal processing problems in healthcare. Furthermore, we explore how spiking neuromorphic processors can complement their DL counterparts for processing biomedical signals. The tutorial is augmented with case studies of the vast literature on neural network and neuromorphic hardware as applied to the healthcare domain. We benchmark various hardware platforms by performing a sensor fusion signal processing task combining electromyography (EMG) signals with computer vision. Comparisons are made between dedicated neuromorphic processors and embedded AI accelerators in terms of inference latency and energy. Finally, we provide our analysis of the field and share a perspective on the advantages, disadvantages, challenges, and opportunities that various accelerators and neuromorphic processors introduce to healthcare and biomedical domains.", "venue": "IEEE Transactions on Biomedical Circuits and Systems", "authors": ["Mostafa Rahimi Azghadi", "Corey  Lammie", "Jason K. Eshraghian", "Melika  Payvand", "Elisa  Donati", "Bernabe  Linares-Barranco", "Giacomo  Indiveri"], "year": 2020, "n_citations": 21}
{"id": 2593462, "s2_id": "3060eea581b53c728e2d6c3161ce2048d0326c45", "title": "SERAD: Soft Error Resilient Asynchronous Design Using a Bundled Data Protocol", "abstract": "The risk of soft errors due to radiation continues to be a significant challenge for engineers trying to build systems that can handle harsh environments. Building systems that are Radiation Hardened by Design (RHBD) is the preferred approach, but existing techniques are expensive in terms of performance, power, and/or area. This paper introduces a novel soft-error resilient asynchronous bundled-data design template, SERAD, which uses a combination of temporal and spatial redundancy to mitigate Single Event Transients (SETs) and upsets (SEUs). SERAD uses Error Detecting Logic (EDL) to detect SETs at the inputs of sequential elements and correct them via re-sampling. Because SERAD only pays the delay penalty in the presence of an SET, which rarely occurs, its average performance is comparable to the baseline synchronous design. We tested the SERAD design using a combination of Spice and Verilog simulations and evaluated its impact on area, frequency, and power on an open-core MIPS-like processor using a NCSU 45nm cell library. Our post-synthesis results show that the SERAD design consumes less than half of the area of the Triple Modular Redundancy (TMR), exhibits significantly less performance degradation than Glitch Filtering (GF), and consumes no more total power than the baseline unhardened design.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Sai Aparna Aketi", "Smriti  Gupta", "Huimei  Cheng", "Joycee  Mekie", "Peter A. Beerel"], "year": 2020, "n_citations": 4}
{"id": 2596277, "s2_id": "b4711cbf932ac4f9574bee7ac1fe897788493dfc", "title": "Sensitivity of Standard Library Cells to Optical Fault Injection Attacks in IHP 250 nm Technology", "abstract": "The IoT consists of a lot of devices such as embedded systems, wireless sensor nodes (WSNs), control systems, etc. It is essential for some of these devices to protect information that they process and transmit. The issue is that an adversary may steal these devices to gain a physical access to the device. There is a variety of ways that allows to reveal cryptographic keys. One of them are optical Fault Injection attacks. We performed successful optical Fault Injections into different type of gates, in particular INV, NAND, NOR, FF. In our work we concentrate on the selection of the parameters configured by an attacker and their influence on the success of the Fault Injections.", "venue": "2020 9th Mediterranean Conference on Embedded Computing (MECO)", "authors": ["Dmytro  Petryk", "Zoya  Dyka", "Peter  Langend\u00f6rfer"], "year": 2020, "n_citations": 1}
{"id": 2598294, "s2_id": "e4d94c89fcaae176b853057291b06cfd104c043f", "title": "Design of a Near-Ideal Fault-Tolerant Routing Algorithm for Network-on-Chip-Based Multicores", "abstract": "With relentless CMOS technology downsizing Networks-on-Chips (NoCs) are inescapably experiencing escalating susceptibility to wearout and reduced reliability. While faults in processors and memories may be masked via redundancy, or mitigated via techniques such as task migration, NoCs are especially vulnerable to hardware faults as a single link breakdown may cause inter-tile communication to halt indefinitely, rendering the whole multicore chip inoperable. As such, NoCs impose the risk of becoming the pivotal point of failure in chip multicores that utilize them. Aiming towards seamless NoC operation in the presence of faulty links we propose Hermes, a near-ideal fault-tolerant routing algorithm that meets the objectives of exhibiting high levels of robustness, operating in a distributed mode, guaranteeing freedom from deadlocks, and evening-out traffic, among many. Hermes is a limited-overhead deadlock-free hybrid routing algorithm, utilizing load-balancing routing on fault-free paths to sustain high-throughput, while providing pre-reconfigured escape path selection in the vicinity of faults. Under such online mechanisms, Hermes's performance degrades gracefully with increasing faulty link counts, a crucially desirable response lacking in prior-art. Additionally, Hermes identifies non-communicating network partitions in scenarios where faulty links are topologically densely distributed such that packets being routed to physically isolated regions cause no network stagnation due to indefinite chained blockages starting at sub-network boundaries. An extensive experimental evaluation, including utilizing traffic workloads gathered from full-system chip multi-processor simulations, shows that Hermes improves network throughput by up to $3\\times$ when compared against the state-of-the-art. Further, hardware synthesis results prove Hermes's efficacy.", "venue": "ArXiv", "authors": ["Costas  Iordanou", "Vassos  Soteriou", "Konstantinos  Aisopos"], "year": 2020, "n_citations": 0}
{"id": 2599189, "s2_id": "d10ff1ac5fb54609564d614ba8a87a2ca11e99e3", "title": "Reservoir Computing with Thin-film Ferromagnetic Devices", "abstract": "Advances in artificial intelligence are driven by technologies inspired by the brain, but these technologies are orders of magnitude less powerful and energy efficient than biological systems. Inspired by the nonlinear dynamics of neural networks, new unconventional computing hardware has emerged with the potential for extreme parallelism and ultra-low power consumption. Physical reservoir computing demonstrates this with a variety of unconventional systems from optical-based to spintronic [1]. Reservoir computers provide a nonlinear projection of the task input into a highdimensional feature space by exploiting the system\u2019s internal dynamics. A trained readout layer then combines features to perform tasks, such as pattern recognition and time-series analysis. Despite progress, achieving state-of-the-art performance without external signal processing to the reservoir remains challenging. Here we show, through simulation, that magnetic materials in thin-film geometries can realise reservoir computers with greater than or similar accuracy to digital recurrent neural networks. Our results reveal that basic spin properties of magnetic films generate the required nonlinear dynamics and memory to solve machine learning tasks. Furthermore, we show that neuromorphic hardware can be reduced in size by removing the need for discrete neural components and external processing. The natural dynamics and nanoscale size of magnetic thin-films present a new path towards fast energy-efficient computing with the potential to innovate portable smart devices, self driving vehicles, and robotics.", "venue": "ArXiv", "authors": ["Matthew  Dale", "Richard F. L. Evans", "Sarah  Jenkins", "Simon  O'Keefe", "Angelika  Sebald", "Susan  Stepney", "Fernando  Torre", "Martin  Trefzer"], "year": 2021, "n_citations": 0}
{"id": 2600845, "s2_id": "f44bc4867d01e080a6c22aaef069aa5f466adb58", "title": "Achieving Efficient Realization of Kalman Filter on CGRA through Algorithm-Architecture Co-design", "abstract": "In this paper, we present efficient realization of Kalman Filter (KF) that can achieve up to 65% of the theoretical peak performance of underlying architecture platform. KF is realized using Modified Faddeeva Algorithm (MFA) as a basic building block due to its versatility and REDEFINE Coarse Grained Reconfigurable Architecture (CGRA) is used as a platform for experiments since REDEFINE is capable of supporting realization of a set algorithmic compute structures at run-time on a Reconfigurable Data-path (RDP). We perform several hardware and software based optimizations in the realization of KF to achieve 116% improvement in terms of Gflops over the first realization of KF. Overall, with the presented approach for KF, 4-105x performance improvement in terms of Gflops/watt over several academically and commercially available realizations of KF is attained. In REDEFINE, we show that our implementation is scalable and the performance attained is commensurate with the underlying hardware resources", "venue": "ARC", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan"], "year": 2018, "n_citations": 3}
{"id": 2603745, "s2_id": "00a61fea581ad61195a1779a5c163ed7da996126", "title": "Optimizing Routerless Network-on-Chip Designs: An Innovative Learning-Based Framework", "abstract": "Machine learning applied to architecture design presents a promising opportunity with broad applications. Recent deep reinforcement learning (DRL) techniques, in particular, enable efficient exploration in vast design spaces where conventional design strategies may be inadequate. This paper proposes a novel deep reinforcement framework, taking routerless networks-on-chip (NoC) as an evaluation case study. The new framework successfully resolves problems with prior design approaches being either unreliable due to random searches or inflexible due to severe design space restrictions. The framework learns (near-)optimal loop placement for routerless NoCs with various design constraints. A deep neural network is developed using parallel threads that efficiently explore the immense routerless NoC design space with a Monte Carlo search tree. Experimental results show that, compared with conventional mesh, the proposed deep reinforcement learning (DRL) routerless design achieves a 3.25x increase in throughput, 1.6x reduction in packet latency, and 5x reduction in power. Compared with the state-of-the-art routerless NoC, DRL achieves a 1.47x increase in throughput, 1.18x reduction in packet latency, and 1.14x reduction in average hop count albeit with slightly more power overhead.", "venue": "ArXiv", "authors": ["Ting-Ru  Lin", "Drew  Penney", "Massoud  Pedram", "Lizhong  Chen"], "year": 2019, "n_citations": 5}
{"id": 2604965, "s2_id": "7a6b9e9c9a21b451a5cf8a9282d63dda2de0a486", "title": "An Improved Structure Of Reversible Adder And Subtractor", "abstract": "In today's world everyday a new technology which is faster, smaller and more complex than its predecessor is being developed. The increased number of transistors packed onto a chip of a conventional system results in increased power consumption that is why Reversible logic has drawn attention of Researchers due to its less heat dissipating characteristics. Reversible logic can be imposed over applications such as quantum computing, optical computing, quantum dot cellular automata, low power VLSI circuits, DNA computing. This paper presents the reversible combinational circuit of adder, subtractor and parity preserving subtractor. The suggested circuit in this paper are designed using Feynman, Double Feynman and MUX gates which are better than the existing one in literature in terms of Quantum cost, Garbage output and Total logical calculations.", "venue": "ArXiv", "authors": ["Aakash  Gupta", "Pradeep  Singla", "Jitendra  Gupta", "Nitin  Maheshwari"], "year": 2013, "n_citations": 10}
{"id": 2606646, "s2_id": "5f6986547cb4f5c12108ed31244b628028424d3f", "title": "Accelerating Recurrent Neural Networks for Gravitational Wave Experiments", "abstract": "This paper presents novel reconfigurable architectures for reducing the latency of recurrent neural networks (RNNs) that are used for detecting gravitational waves. Gravitational interferometers such as the LIGO detectors capture cosmic events such as black hole mergers which happen at unknown times and of varying durations, producing time-series data. We have developed a new architecture capable of accelerating RNN inference for analyzing time-series data from LIGO detectors. This architecture is based on optimizing the initiation intervals (II) in a multi-layer LSTM (Long Short-Term Memory) network, by identifying appropriate reuse factors for each layer. A customizable template for this architecture has been designed, which enables the generation of low-latency FPGA designs with efficient resource utilization using high-level synthesis tools. The proposed approach has been evaluated based on two LSTM models, targeting a ZYNQ 7045 FPGA and a U250 FPGA. Experimental results show that with balanced II, the number of DSPs can be reduced up to 42% while achieving the same IIs. When compared to other FPGA-based LSTM designs, our design can achieve about 4.92 to 12.4 times lower latency.", "venue": "2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Zhiqiang  Que", "Erwei  Wang", "Umar  Marikar", "Eric  Moreno", "Jennifer  Ngadiuba", "Hamza  Javed", "Bartlomiej  Borzyszkowski", "Thea  Aarrestad", "Vladimir  Loncar", "Sioni  Summers", "Maurizio  Pierini", "Peter Y Cheung", "Wayne  Luk"], "year": 2021, "n_citations": 1}
{"id": 2607544, "s2_id": "6f1ef15dea74488868b6ab5c60e97e4cc92f9394", "title": "Reconfiguration Strategies for Online Hardware Multitasking in Embedded Systems", "abstract": "An intensive use of reconfigurable hardware is expected in future embedded systems. This means that the system has to decide which tasks are more suitable for hardware execution. In order to make an efficient use of the FPGA it is convenient to choose one that allows hardware multitasking, which is implemented by using partial dynamic reconfiguration. One of the challenges for hardware multitasking in embedded systems is the online management of the only reconfiguration port of present FPGA devices. This paper presents different online reconfiguration scheduling strategies which assign the reconfiguration interface resource using different criteria: workload distribution or task deadline. The online scheduling strategies presented take efficient and fast decisions based on the information available at each moment. Experiments have been made in order to analyze the performance and convenience of these reconfiguration strategies.", "venue": "ArXiv", "authors": ["Marcos  Sanchez-Elez", "Sara  Roman"], "year": 2013, "n_citations": 2}
{"id": 2607599, "s2_id": "5be258f4b4d30fc0b3c915ff5180ee089c115080", "title": "EdgeDRNN: Recurrent Neural Network Accelerator for Edge Inference", "abstract": "Low-latency, low-power portable recurrent neural network (RNN) accelerators offer powerful inference capabilities for real-time applications such as IoT, robotics, and human-machine interaction. We propose a lightweight Gated Recurrent Unit (GRU)-based RNN accelerator called EdgeDRNN that is optimized for low-latency edge RNN inference with batch size of 1. EdgeDRNN adopts the spiking neural network inspired delta network algorithm to exploit temporal sparsity in RNNs. Weights are stored in inexpensive DRAM which enables EdgeDRNN to compute large multi-layer RNNs on the most inexpensive FPGA. The sparse updates reduce DRAM weight memory access by a factor of up to 10x and the delta can be varied dynamically to trade-off between latency and accuracy. EdgeDRNN updates a 5 million parameter 2-layer GRU-RNN in about 0.5ms. It achieves latency comparable with a 92W Nvidia 1080 GPU. It outperforms NVIDIA Jetson Nano, Jetson TX2 and Intel Neural Compute Stick 2 in latency by 5X. For a batch size of 1, EdgeDRNN achieves a mean effective throughput of 20.2GOp/s and a wall plug power efficiency that is over 4X higher than the commercial edge AI platforms.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Chang  Gao", "Antonio  Rios-Navarro", "Xi  Chen", "Shih-Chii  Liu", "Tobi  Delbruck"], "year": 2020, "n_citations": 8}
{"id": 2609094, "s2_id": "2eb173fa8a8fcbe2af7195f65813e7f5c1a18b50", "title": "What Your DRAM Power Models Are Not Telling You: Lessons from a Detailed Experimental Study", "abstract": "Main memory (DRAM) consumes as much as half of the total system power in a computer today, due to the increasing demand for memory capacity and bandwidth. There is a growing need to understand and analyze DRAM power consumption, which can be used to research new DRAM architectures and systems that consume less power. A major obstacle against such research is the lack of detailed and accurate information on the power consumption behavior of modern DRAM devices. Researchers have long relied on DRAM power models that are predominantly based off of a set of standardized current measurements provided by DRAM vendors, called IDD values. Unfortunately, we find that state-of-the-art DRAM power models are often highly inaccurate when compared with the real power consumed by DRAM. This is because existing DRAM power models (1) are based off of the worst-case power consumption of devices, as vendor specifications list the current consumed by the most power-hungry device sold; (2) do not capture variations in DRAM power consumption due to different data value patterns; and (3) do not account for any variation across different devices or within a device.", "venue": "Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems", "authors": ["Saugata  Ghose", "Abdullah Giray Yaglik\u00e7i", "Raghav  Gupta", "Donghyuk  Lee", "Kais  Kudrolli", "William X. Liu", "Hasan  Hassan", "Kevin K. Chang", "Niladrish  Chatterjee", "Aditya  Agrawal", "Mike  O'Connor", "Onur  Mutlu"], "year": 2018, "n_citations": 52}
{"id": 2611551, "s2_id": "f8fee4252a739c64675a67edb11c84b11fc6ebe5", "title": "Advanced datapath synthesis using graph isomorphism", "abstract": "This paper presents an advanced DAG-based algorithm for datapath synthesis that targets area minimization using logic-level resource sharing. The problem of identifying common specification logic is formulated using unweighted graph isomorphism problem, in contrast to a weighted graph isomorphism using AIGs. In the context of gate-level datapath circuits, our algorithm solves the unweighted graph isomorphism problem in linear time. The experiments are conducted within an industrial synthesis flow that includes the complete high-level synthesis, logic synthesis and placement and route procedures. Experimental results show a significant runtime improvements compared to the existing datapath synthesis algorithms.", "venue": "2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Cunxi  Yu", "Mihir  Choudhury", "Andrew  Sullivan", "Maciej J. Ciesielski"], "year": 2017, "n_citations": 5}
{"id": 2618551, "s2_id": "bb473893a5724e611b32ea9c492040d12e5568c5", "title": "GLOW: A global router for low-power thermal-reliable interconnect synthesis using photonic wavelength multiplexing", "abstract": "In this paper, we examine the integration potential and explore the design space of low power thermal reliable on-chip interconnect synthesis featuring nanophotonics Wavelength Division Multiplexing (WDM). With the recent advancements, it is foreseen that nanophotonics holds the promise to be employed for future on-chip data signalling due to its unique power efficiency, signal delay and huge multiplexing potential. However, there are major challenges to address before feasible on-chip integration could be reached. In this paper, we present GLOW, a hybrid global router to provide low power opto-electronic interconnect synthesis under the considerations of thermal reliability and various physical design constraints such as optical power, delay and signal quality. GLOW is evaluated with testing cases derived from ISPD07-08 global routing benchmarks. Compared with a greedy approach, GLOW demonstrates around 23%-50% of total optical power reduction, revealing great potential of on-chip WDM interconnect synthesis.", "venue": "17th Asia and South Pacific Design Automation Conference", "authors": ["Duo  Ding", "Bei  Yu", "David Z. Pan"], "year": 2012, "n_citations": 33}
{"id": 2623678, "s2_id": "bccf3b8d1bc18b953787b3002b1dc555fc3e989a", "title": "Decision tree based hardware power monitoring for run time dynamic power management in FPGA", "abstract": "Fine-grained runtime power management techniques could be promising solutions for power reduction. Therefore, it is essential to establish accurate power monitoring schemes to obtain dynamic power variation in a short period (i.e., tens or hundreds of clock cycles). In this paper, we leverage a decision-tree-based power modeling approach to establish finegrained hardware power monitoring on FPGA platforms. A generic and complete design flow is developed to implement the decision tree power model which is capable of precisely estimating dynamic power in a fine-grained manner. A flexible architecture of the hardware power monitoring is proposed, which can be instrumented in any RTL design for runtime power estimation, dispensing with the need for extra power measurement devices. Experimental results of applying the proposed model to benchmarks with different resource types reveal an average error up to 4% for dynamic power estimation. Moreover, the overheads of area, power and performance incurred by the power monitoring circuitry are extremely low. Finally, we apply our power monitoring technique to the power management using phase shedding with an on-chip multi-phase regulator as a proof of concept and the results demonstrate 14% efficiency enhancement for the power supply of the FPGA internal logic.", "venue": "2017 27th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Zhe  Lin", "Wei  Zhang", "Sharad  Sinha"], "year": 2017, "n_citations": 4}
{"id": 2624763, "s2_id": "38cf9bb1495202e5dd99981c9f9fb81eae5efa84", "title": "The Case for Approximate Intermittent Computing", "abstract": "We present the concept of approximate intermittent computing and concretely demonstrate its application. Intermittent computations stem from the erratic energy patterns caused by energy harvesting: computations unpredictably terminate whenever energy is insufficient and the application state is lost. Existing solutions maintain equivalence to continuous executions by creating persistent state on non-volatile memory, enabling stateful computations to cross power failures. The performance penalty is massive: system throughput reduces while energy consumption increases. In contrast, approximate intermittent computations trade the accuracy of the results for sparing the entire overhead to maintain equivalence to a continuous execution. This is possible as we use approximation to limit the extent of stateful computations to the single power cycle, enabling the system to completely shift the energy budget for managing persistent state to useful computations towards an immediate approximate result. To this end, we effectively reverse the regular formulation of approximate computing problems. First, we apply approximate intermittent computing to human activity recognition. We design an anytime variation of support vector machines able to improve the accuracy of the classification as energy is available. We build a hw/sw prototype using kinetic energy and show a 7x improvement in system throughput compared to state-of-the-art system support for intermittent computing, while retaining 83% accuracy in a setting where the best attainable accuracy is 88%. Next, we apply approximate intermittent computing in a sharply different scenario, that is, embedded image processing, using loop perforation. Using a different hw/sw prototype we build and diverse energy traces, we show a 5x improvement in system throughput compared to state-of-the-art system support for intermittent computing, while providing an equivalent output in 84% of the cases.", "venue": "ArXiv", "authors": ["Fulvio  Bambusi", "Francesco  Cerizzi", "Yamin  Lee", "Luca  Mottola"], "year": 2021, "n_citations": 0}
{"id": 2624994, "s2_id": "80188f55c575105613bf058555937e16c1c494f6", "title": "Tiny-CFA: A Minimalistic Approach for Control-Flow Attestation Using Verified Proofs of Execution", "abstract": "The design of tiny trust anchors has received significant attention over the past decade, to secure low-end MCU-s that cannot afford expensive security mechanisms. In particular, hardware/software (hybrid) co-designs offer low hardware cost, while retaining similar security guarantees as (more expensive) hardware-based techniques. Hybrid trust anchors support security services, such as remote attestation, proofs of software update/erasure/reset, proofs of remote software execution, in resource-constrained MCU-s, e.g., MSP430 and AVR AtMega32. Despite these advances, detection of control-flow attacks in low-end MCU-s remains a challenge, since hardware requirements of the cheapest related architectures are often more expensive than the MCU-s themselves. In this work, we tackle this challenge by designing Tiny-CFA - a control-flow attestation (CFA) technique with a single hardware requirement - the ability to generate proofs of remote software execution (PoX). In turn, PoX can be implemented very efficiently and securely in low-end MCU-s. Consequently, our design achieves the lowest hardware overhead of any CFA architecture (i.e., two orders of magnitude cheaper), while relying on a formally verified PoX architecture as its sole hardware requirement. With respect to runtime overhead, Tiny-CFA also achieves better performance than prior CFA techniques based on code instrumentation. We implement and evaluate Tiny-CFA, analyze its security, and demonstrate its practicality using real-world publicly available applications.", "venue": "ArXiv", "authors": ["Ivan De Oliveira Nunes", "Sashidhar  Jakkamsetti", "Gene  Tsudik"], "year": 2020, "n_citations": 5}
{"id": 2626238, "s2_id": "2d38be51659b6e1a4c1ef134de4cb74601bf6824", "title": "Layout Decomposition for Triple Patterning Lithography", "abstract": "As minimum feature size and pitch spacing further scale down, triple patterning lithography is a likely 193 nm extension along the paradigm of double patterning lithography for 14-nm technology node. Layout decomposition, which divides input layout into several masks to minimize the conflict and stitch numbers, is a crucial design step for double/triple patterning lithography. In this paper, we present a systematic study on triple patterning layout decomposition problem, which is shown to be NP-hard. Because of the NP-hardness, the runtime required to exactly solve it increases dramatically with the problem size. We first propose a set of graph division techniques to reduce the problem size. Then, we develop integer linear programming (ILP) to solve it. For large layouts, even with the graph-division techniques, ILP may still suffer from serious runtime overhead. To achieve better trade-off between runtime and performance, we present a novel semidefinite programming (SDP)-based algorithm. Followed by a mapping process, we can translate the SDP solutions into the final decomposition solutions. Experimental results show that the graph division can reduce runtime dramatically. In addition, SDP-based algorithm can achieve great speedup even compared with accelerated ILP, with very comparable results in terms of the stitch number and the conflict number.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Bei  Yu", "Kun  Yuan", "Duo  Ding", "David Z. Pan"], "year": 2015, "n_citations": 9}
{"id": 2627059, "s2_id": "4ca5508d233b2bfdc121749f0a5a58ef45bcbf8d", "title": "A Survey of Novel Cache Hierarchy Designs for High Workloads", "abstract": "Traditional on-die, three-level cache hierarchy design is very commonly used but is also prone to latency, especially at the Level 2 (L2) cache. We discuss three distinct ways of improving this design in order to have better performance. Performance is especially important for systems with high workloads. The first method proposes to eliminate L2 altogether while proposing a new prefetching technique, the second method suggests increasing the size of L2, while the last method advocates the implementation of optical caches. After carefully contemplating results in performance gains and the advantages and disadvantages of each method, we found the last method to be the best of the three.", "venue": "ArXiv", "authors": ["Pranjal Singh Rajput", "Sonnya  Dellarosa", "Kanya  Satis"], "year": 2021, "n_citations": 0}
{"id": 2627643, "s2_id": "d6a7144d778f6f5c3249b187081d27ae16017269", "title": "ECI-Cache: A High-Endurance and Cost-Efficient I/O Caching Scheme for Virtualized Platforms", "abstract": "In recent years, high interest in using Virtual Machines (VMs) in data centers and cloud computing has significantly increased the demand for high-performance data storage systems. A straightforward approach to providing a high-performance storage system is using Solid-State Drives (SSDs). Inclusion of SSDs in storage systems, however, imposes significantly higher cost compared to Hard Disk Drives (HDDs). Recent studies suggest using SSDs as a caching layer for HDD-based storage subsystems in virtualized platforms. Such studies neglect to address the endurance and cost of SSDs, which can significantly affect the efficiency of I/O caching. Moreover, previous studies only configure the cache size to provide the required performance level for each VM, while neglecting other important parameters such as cache write policy and request type, which can adversely affect both performance-per-cost and endurance. In this paper, we propose a new high-Endurance and Cost-efficient I/O caching (ECI-Cache) scheme for virtualized platforms in large-scale data centers, which improves both performance-per-cost and endurance of the SSD cache. ECI-Cache dynamically assigns 1) an efficient cache size for each VM, to maximize the overall performance of the running VMs and 2) an effective write policy for each VM, to enhance the endurance and performance-per-cost of the storage subsystem.", "venue": "Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems", "authors": ["Saba  Ahmadian", "Onur  Mutlu", "Hossein  Asadi"], "year": 2018, "n_citations": 12}
{"id": 2628413, "s2_id": "a32ae470939042a75511e417fae23d3baf2c81d7", "title": "DNN-Life: An Energy-Efficient Aging Mitigation Framework for Improving the Lifetime of On-Chip Weight Memories in Deep Neural Network Hardware Architectures", "abstract": "Negative Biased Temperature Instability (NBTI)-induced aging is one of the critical reliability threats in nano-scale devices. This paper makes the first attempt to study the NBTI aging in the on-chip weight memories of deep neural network (DNN) hardware accelerators, subjected to complex DNN workloads. We propose DNN-Life, a specialized aging analysis and mitigation framework for DNNs, which jointly exploits hardware- and software-level knowledge to improve the lifetime of a DNN weight memory with reduced energy overhead. At the software-level, we analyze the effects of different DNN quantization methods on the distribution of the bits of weight values. Based on the insights gained from this analysis, we propose a micro-architecture that employs low-cost memory-write (and read) transducers to achieve an optimal duty-cycle at run time in the weight memory cells, thereby balancing their aging. As a result, our DNN-Life framework enables efficient aging mitigation of weight memory of the given DNN hardware at minimal energy overhead during the inference process.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Muhammad Abdullah Hanif", "Muhammad  Shafique"], "year": 2021, "n_citations": 2}
{"id": 2629382, "s2_id": "e3dea8151f6f07fc99b7a84d9296704e29b12538", "title": "Reconfigurable Low-latency Memory System for Sparse Matricized Tensor Times Khatri-Rao Product on FPGA", "abstract": "Tensor decomposition has become an essential tool in many applications in various domains, including machine learning. Sparse Matricized Tensor Times Khatri-Rao Product (MTTKRP) is one of the most computationally expensive kernels in tensor computations. Despite having significant computational parallelism, MTTKRP is a challenging kernel to optimize due to its irregular memory access characteristics. This paper focuses on a multi-faceted memory system, which explores the spatial and temporal locality of the data structures of MTTKRP. Further, users can reconfigure our design depending on the behavior of the compute units used in the FPGA accelerator. Our system efficiently accesses all the MTTKRP data structures while reducing the total memory access time, using a distributed cache and Direct Memory Access (DMA) subsystem. Moreover, our work improves the memory access time by 3.5\u00d7 compared with commercial memory controller IPs. Also, our system shows 2\u00d7 and 1.26\u00d7 speedups compared with cache-only and DMA-only memory systems, respectively.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Sasindu  Wijeratne", "Rajgopal  Kannan", "Viktor  Prasanna"], "year": 2021, "n_citations": 0}
{"id": 2629677, "s2_id": "6de9f6a061752688fb9a32d8c38de912e8fbaf0b", "title": "Indirection Stream Semantic Register Architecture for Efficient Sparse-Dense Linear Algebra", "abstract": "Sparse-dense linear algebra is crucial in many domains, but challenging to handle efficiently on CPUs, GPUs, and accelerators alike; multiplications with sparse formats like CSR and CSF require indirect memory lookups. In this work, we enhance a memory-streaming RISC-V ISA extension to accelerate sparse-dense products through streaming indirection. We present efficient dot, matrix-vector, and matrix-matrix product kernels using our hardware, enabling single-core FPU utilizations of up to 80% and speedups of up to 7.2x over an optimized baseline without extensions. A matrix-vector implementation on a multicore cluster is up to 5.8x faster and 2.7x more energy-efficient with our kernels than an optimized baseline. We propose further uses for our indirection hardware, such as scatter-gather operations and codebook decoding, and compare our work to state-of-the-art CPU, GPU, and accelerator approaches, measuring a 2.8x higher peak FP64 utilization in CSR matrix-vector multiplication than a GTX 1080 Ti GPU running a cuSPARSE kernel.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Paul  Scheffler", "Florian  Zaruba", "Fabian  Schuiki", "Torsten  Hoefler", "Luca  Benini"], "year": 2021, "n_citations": 2}
{"id": 2631002, "s2_id": "dd403fb03f64a4a224946e5c0a419548395222b5", "title": "Ratatoskr: An open-source framework for in-depth power, performance and area analysis in 3D NoCs", "abstract": "We introduce ratatoskr, an open-source framework for in-depth power, performance and area (PPA) analysis in NoCs for 3D-integrated and heterogeneous System-on-Chips (SoCs). It covers all layers of abstraction by providing a NoC hardware implementation on RT level, a NoC simulator on cycle-accurate level and an application model on transaction level. By this comprehensive approach, ratatoskr can provide the following specific PPA analyses: Dynamic power of links can be measured within 2.4% accuracy of bit-level simulations while maintaining cycle-accurate simulation speed. Router power is determined from RT level synthesis combined with cycle-accurate simulations. The performance of the whole NoC can be measured both via cycle-accurate and RT level simulations. The performance of individual routers is obtained from RT level including gate-level verification. The NoC area is calculated from RT level. Despite these manifold features, ratatoskr offers easy two-step user interaction: First, a single point-of-entry that allows to set design parameters and second, PPA reports are generated automatically. For both the input and the output, different levels of abstraction can be chosen for high-level rapid network analysis or low-level improvement of architectural details. The synthesize NoC model reduces up to 32% total router power and 3% router area in comparison to a conventional standard router. As a forward-thinking and unique feature not found in other NoC PPA-measurement tools, ratatoskr supports heterogeneous 3D integration that is one of the most promising integration paradigms for upcoming SoCs. Thereby, ratatoskr lies the groundwork to design their communication architectures.", "venue": "ArXiv", "authors": ["Jan Moritz Joseph", "Lennart  Bamberg", "Imad  Hajjar", "Behnam Razi Perjikolaei", "Alberto Garc\u00eda Ortiz", "Thilo  Pionteck"], "year": 2019, "n_citations": 5}
{"id": 2637513, "s2_id": "9a95e04d3c893465122579b8cd5142fe0b0b0d06", "title": "Proactive Aging Mitigation in CGRAs through Utilization-Aware Allocation", "abstract": "Resource balancing has been effectively used to mitigate the long-term aging effects of Negative Bias Temperature Instability (NBTI) in multi-core and Graphics Processing Unit (GPU) architectures. In this work, we investigate this strategy in Coarse-Grained Reconfigurable Arrays (CGRAs) with a novel application-to-CGRA allocation approach. By introducing important extensions to the reconfiguration logic and the datapath, we enable the dynamic movement of configurations throughout the fabric and allow overutilized Functional Units (FUs) to recover from stress-induced NBTI aging. Implementing the approach in a resource-constrained state-of-the-art CGRA reveals 2.2\u00d7 lifetime improvement with negligible performance overheads and less than 10% increase in area.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Marcelo  Brandalero", "Bernardo Neuhaus Lignati", "Antonio Carlos Schneider Beck", "Muhammad  Shafique", "Michael  H\u00fcbner"], "year": 2020, "n_citations": 0}
{"id": 2639875, "s2_id": "b66c6921e3ff492e9e6fb7c8996423a4284abb45", "title": "Secure Boot from Non-Volatile Memory for Programmable SoC Architectures", "abstract": "In modern embedded systems, the trust in comprehensive security standards all along the product life cycle has become an increasingly important access-to-market requirement. However, these security standards rely on mandatory immunity assumptions such as the integrity and authenticity of an initial system configuration typically loaded from Non-Volatile Memory (NVM). This applies especially to FPGA-based Programmable System-on-Chip (PSoC) architectures, since object codes as well as configuration data easily exceed the capacity of a secure boot ROM. In this context, an attacker could try to alter the content of the NVM device in order to manipulate the system. The PSoC therefore relies on the integrity of the NVM particularly at boot-time. In this paper, we propose a methodology for securely booting from an NVM in a potentially unsecure environment by exploiting the reconfigurable logic of the FPGA. Here, the FPGA serves as a secure anchor point by performing required integrity and authenticity verifications prior to the configuration and execution of any user application loaded from the NVM on the PSoC. The proposed secure boot process is based on the following assumptions and steps: 1) The boot configuration is stored on a fully encrypted Secure Digital memory card (SD card) or alternatively Flash acting as NVM. 2) At boot time, a hardware design called Trusted Memory-Interface Unit (TMIU) is loaded to verify first the authenticity of the deployed NVM and then after decryption the integrity of its content. To demonstrate the practicability of our approach, we integrated the methodology into the vendor-specific secure boot process of a Xilinx Zynq PSoC and evaluated the design objectives performance, power and resource costs.", "venue": "2020 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)", "authors": ["Franz-Josef  Streit", "Florian  Fritz", "Andreas  Becher", "Stefan  Wildermann", "Stefan  Werner", "Martin  Schmidt-Korth", "Michael  Pschyklenk", "Jurgen  Teich"], "year": 2020, "n_citations": 1}
{"id": 2652052, "s2_id": "789e45fad51af8a10295af2d6d43e37e264779ec", "title": "Cross-Point Architecture for Spin-Transfer Torque Magnetic Random Access Memory", "abstract": "Spin-transfer torque magnetic random access memory (STT-MRAM) is considered as one of the most promising candidates to build up a true universal memory thanks to its fast write/read speed, infinite endurance, and nonvolatility. However, the conventional access architecture based on 1 transistor + 1 memory cell limits its storage density as the selection transistor should be large enough to ensure the write current higher than the critical current for the STT operation. This paper describes a design of cross-point architecture for STT-MRAM. The mean area per word corresponds to only two transistors, which are shared by a number of bits (e.g., 64). This leads to significant improvement of data density (e.g., 1.75 F2/bit). Special techniques are also presented to address the sneak currents and low-speed issues of conventional cross-point architecture, which are difficult to surmount and few efficient design solutions have been reported in the literature. By using an STT-MRAM SPICE model including precise experimental parameters and STMicroelectronics 65 nm technology, some chip characteristic results such as cell area, data access speed, and power have been calculated or simulated to demonstrate the expected performances of this new memory architecture.", "venue": "IEEE Transactions on Nanotechnology", "authors": ["Weisheng  Zhao", "Sumanta  Chaudhuri", "Celso  Accoto", "Jacques-Olivier  Klein", "Claude  Chappert", "Pascale  Mazoyer"], "year": 2012, "n_citations": 32}
{"id": 2653047, "s2_id": "7fe0afcc5d99d770f2f45541873b514ca3393abb", "title": "Design space exploration tools for the ByoRISC configurable processor family", "abstract": "In this paper, the ByoRISC (Build your own RISC) configurable application-specific instruction-set processor (ASIP) family is presented. ByoRISCs, as vendor-independent cores, provide extensive architectural parameters over a baseline processor, which can be customized by application-specific hardware extensions (ASHEs). Such extensions realize multi-input multi-output (MIMO) custom instructions with local state and load/store accesses to the data memory. ByoRISCs incorporate a true multi-port register file, zero-overhead custom instruction decoding, and scalable data forwarding mechanisms. Given these design decisions, ByoRISCs provide a unique combination of features that allow their use as architectural testbeds and the seamless and rapid development of new high-performance ASIPs. \nThe performance characteristics of ByoRISCs, implemented as vendor-independent cores, have been evaluated for both ASIC and FPGA implementations, and it is proved that they provide a viable solution in FPGA-based system-on-a-chip design. A case study of an image processing pipeline is also presented to highlight the process of utilizing a ByoRISC custom processor. A peak performance speedup of up to 8.5$\\times$ can be observed, whereas an average performance speedup of 4.4$\\times$ on Xilinx Virtex-4 targets is achieved. In addition, ByoRISC outperforms an experimental VLIW architecture named VEX even in its 16-wide configuration for a number of data-intensive application kernels.", "venue": "ArXiv", "authors": ["Nikolaos  Kavvadias", "Spiridon  Nikolaidis"], "year": 2014, "n_citations": 0}
{"id": 2653422, "s2_id": "e7bd207144c0cde2773401650b664c1e2936d297", "title": "Low Latency CMOS Hardware Acceleration for Fully Connected Layers in Deep Neural Networks", "abstract": "We present a novel low latency CMOS hardware accelerator for fully connected (FC) layers in deep neural networks (DNNs). The FC accelerator, FC-ACCL, is based on 128 8x8 or 16x16 processing elements (PEs) for matrix-vector multiplication, and 128 multiply-accumulate (MAC) units integrated with 128 High Bandwidth Memory (HBM) units for storing the pretrained weights. Micro-architectural details for CMOS ASIC implementations are presented and simulated performance is compared to recent hardware accelerators for DNNs for AlexNet and VGG 16. When comparing simulated processing latency for a 4096-1000 FC8 layer, our FC-ACCL is able to achieve 48.4 GOPS (with a 100 MHz clock) which improves on a recent FC8 layer accelerator quoted at 28.8 GOPS with a 150 MHz clock. We have achieved this considerable improvement by fully utilizing the HBM units for storing and reading out column-specific FClayer weights in 1 cycle with a novel colum-row-column schedule, and implementing a maximally parallel datapath for processing these weights with the corresponding MAC and PE units. When up-scaled to 128 16x16 PEs, for 16x16 tiles of weights, the design can reduce latency for the large FC6 layer by 60 % in AlexNet and by 3 % in VGG16 when compared to an alternative EIE solution which uses compression.", "venue": "ArXiv", "authors": ["Nick  Iliev", "Amit Ranjan Trivedi"], "year": 2020, "n_citations": 2}
{"id": 2656906, "s2_id": "806de7596cf4d4f74e2e1bb8ace788df54fb09a9", "title": "CapsAcc: An Efficient Hardware Accelerator for CapsuleNets with Data Reuse", "abstract": "Recently, CapsuleNets have overtaken traditional Deep Convolutional Neural Networks (CNNs), because of their improved generalization ability due to the multi-dimensional capsules, in contrast to the single-dimensional neurons. Consequently, CapsuleNets also require extremely intense matrix computations, making it a gigantic challenge to achieve high performance. In this paper, we propose CapsAcc, the first specialized CMOS-based hardware architecture to perform CapsuleNets inference with high performance and energy efficiency. State-of-the- art convolutional CNN accelerators would not work efficiently for CapsuleNets, as their designs do not account for unique processing nature of CapsuleNets involving multi-dimensional matrix processing, squashing and dynamic routing. Our architecture exploits the massive parallelism by flexibly feeding the data to a specialized systolic array according to the operations required in different layers. It also avoids extensive load and store operations on the on-chip memory, by reusing the data when possible. We synthesized the complete CapsAcc architecture in a 32nm CMOS technology using Synopsys design tools, and evaluated it for the MNIST benchmark (as also done by the original CapsuleNet paper) to ensure consistent and fair comparisons. This work enables highly-efficient CapsuleNets inference on embedded platforms.", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Alberto  Marchisio", "Muhammad Abdullah Hanif", "Muhammad  Shafique"], "year": 2019, "n_citations": 18}
{"id": 2657330, "s2_id": "ca96c5e7efb083721d1145aaf7e1e6a2befc0d27", "title": "Data Streaming and Traffic Gathering in Mesh-based NoC for Deep Neural Network Acceleration", "abstract": "The increasing popularity of deep neural network (DNN) applications demands high computing power and efficient hardware accelerator architecture. DNN accelerators use a large number of processing elements (PEs) and on-chip memory for storing weights and other parameters. As the communication backbone of a DNN accelerator, networks-on-chip (NoC) play an important role in supporting various dataflow patterns and enabling processing with communication parallelism in a DNN accelerator. However, the widely used mesh-based NoC architectures inherently cannot support the efficient one-to-many and many-to-one traffic largely existing in DNN workloads. In this paper, we propose a modified mesh architecture with a one-way/two-way streaming bus to speedup one-tomany (multicast) traffic, and the use of gather packets to support many-to-one (gather) traffic. The analysis of the runtime latency of a convolutional layer shows that the two-way streaming architecture achieves better improvement than the one-way streaming architecture for an Output Stationary (OS) dataflow architecture. The simulation results demonstrate that the gather packets can help to reduce the runtime latency up to 1.8 times and network power consumption up to 1.7 times, compared with the repetitive unicast method on modified mesh architectures supporting two-way streaming.", "venue": "ArXiv", "authors": ["Binayak  Tiwari", "Mei  Yang", "Xiaohang  Wang", "Yingtao  Jiang"], "year": 2021, "n_citations": 0}
{"id": 2657435, "s2_id": "96a054ed37f205c9004ac6adb5afd59c9471a055", "title": "Decanting the Contribution of Instruction Types and Loop Structures in the Reuse of Traces", "abstract": "Reuse has been proposed as a microarchitecture-level mechanism to reduce the amount of executed instructions, collapsing dependencies and freeing resources for other instructions. Previous works have used reuse domains such as memory accesses, integer or not floating point, based on the reusability rate. However, these works have not studied the specific contribution of reusing different subsets of instructions for performance. In this work, we analysed the sensitivity of trace reuse to instruction subsets, comparing their efficiency to their complementary subsets. We also studied the amount of reuse that can be extracted from loops. Our experiments show that disabling trace reuse outside loops does not harm performance but reduces in 12% the number of accesses to the reuse table. Our experiments with reuse subsets show that most of the speedup can be retained even when not reusing all types of instructions previously found in the reuse domain.", "venue": "ArXiv", "authors": ["Andrey M. Coppieters", "Sheila de Oliveira", "Felipe Maia Galv\u00e3o Fran\u00e7a", "Maur\u00edcio L. Pilla", "Amarildo T. da Costa"], "year": 2017, "n_citations": 0}
{"id": 2657601, "s2_id": "709f5b598e8bd4c26dc6d9582da4f9f8dfcd747f", "title": "EBPC: Extended Bit-Plane Compression for Deep Neural Network Inference and Training Accelerators", "abstract": "In the wake of the success of convolutional neural networks in image classification, object recognition, speech recognition, etc., the demand for deploying these compute-intensive ML models on embedded and mobile systems with tight power and energy constraints at low cost, as well as for boosting throughput in data centers, is growing rapidly. This has sparked a surge of research into specialized hardware accelerators. Their performance is typically limited by I/O bandwidth, power consumption is dominated by I/O transfers to off-chip memory, and on-chip memories occupy a large part of the silicon area. We introduce and evaluate a novel, hardware-friendly, and lossless compression scheme for the feature maps present within convolutional neural networks. We present hardware architectures and synthesis results for the compressor and decompressor in 65 nm. With a throughput of one 8-bit word/cycle at 600 MHz, they fit into 2.8 kGE and 3.0 kGE of silicon area, respectively\u2014together the size of less than seven 8-bit multiply-add units at the same throughput. We show that an average compression ratio of <inline-formula> <tex-math notation=\"LaTeX\">$5.1\\times $ </tex-math></inline-formula> for AlexNet, <inline-formula> <tex-math notation=\"LaTeX\">$4\\times $ </tex-math></inline-formula> for VGG-16, <inline-formula> <tex-math notation=\"LaTeX\">$2.4\\times $ </tex-math></inline-formula> for ResNet-34 and <inline-formula> <tex-math notation=\"LaTeX\">$2.2\\times $ </tex-math></inline-formula> for MobileNetV2 can be achieved\u2014a gain of 45\u201370% over existing methods. Our approach also works effectively for various number formats, has a low frame-to-frame variance on the compression ratio, and achieves compression factors for gradient map compression during training that are even better than for inference.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Lukas  Cavigelli", "Georg  Rutishauser", "Luca  Benini"], "year": 2019, "n_citations": 12}
{"id": 2658471, "s2_id": "7f11a68c97ca4e8722814201d3972e03e69bc7fe", "title": "An orthogonal 16-point approximate DCT for image and video compression", "abstract": "A low-complexity orthogonal multiplierless approximation for the 16-point discrete cosine transform (DCT) was introduced. The proposed method was designed to possess a very low computational cost. A fast algorithm based on matrix factorization was proposed requiring only 60 additions. The proposed architecture outperforms classical and state-of-the-art algorithms when assessed as a tool for image and video compression. Digital VLSI hardware implementations were also proposed being physically realized in field programmable gate array technology and implemented in 45\u00a0nm up to synthesis and place-route levels. Additionally, the proposed method was embedded into a high efficiency video coding (HEVC) reference software for actual proof-of-concept. Obtained results show negligible video degradation when compared to Chen DCT algorithm in HEVC.", "venue": "Multidimens. Syst. Signal Process.", "authors": ["Thiago L. T. da Silveira", "F\u00e1bio M. Bayer", "Renato J. Cintra", "Sunera  Kulasekera", "Arjuna  Madanayake", "Alice J. Kozakevicius"], "year": 2016, "n_citations": 23}
{"id": 2660774, "s2_id": "ecda25f911b319c550be653d64fb7539a8206404", "title": "Skybridge: 3-D Integrated Circuit Technology Alternative to CMOS", "abstract": "Continuous scaling of CMOS has been the major catalyst in miniaturization of integrated circuits (ICs) and crucial for global socio-economic progress. However, scaling to sub-20nm technologies is proving to be challenging as MOSFETs are reaching their fundamental limits and interconnection bottleneck is dominating IC operational power and performance. Migrating to 3-D, as a way to advance scaling, has eluded us due to inherent customization and manufacturing requirements in CMOS that are incompatible with 3-D organization. Partial attempts with die-die and layer-layer stacking have their own limitations. We propose a 3-D IC fabric technology, Skybridge[TM], which offers paradigm shift in technology scaling as well as design. We co-architect Skybridge's core aspects, from device to circuit style, connectivity, thermal management, and manufacturing pathway in a 3-D fabric-centric manner, building on a uniform 3-D template. Our extensive bottom-up simulations, accounting for detailed material system structures, manufacturing process, device, and circuit parasitics, carried through for several designs including a designed microprocessor, reveal a 30-60x density, 3.5x performance per watt benefits, and 10X reduction in interconnect lengths vs. scaled 16-nm CMOS. Fabric-level heat extraction features are shown to successfully manage IC thermal profiles in 3-D. Skybridge can provide continuous scaling of integrated circuits beyond CMOS in the 21st century.", "venue": "ArXiv", "authors": ["Mostafizur  Rahman", "Santosh  Khasanvis", "Jiajun  Shi", "Mingyu  Li", "Csaba Andras Moritz"], "year": 2014, "n_citations": 21}
{"id": 2665137, "s2_id": "8e177c64bf337ccfb14d8328a273c0cd641e7d13", "title": "Tucker Tensor Decomposition on FPGA", "abstract": "Tensor computation has emerged as a powerful mathematical tool for solving high-dimensional and/or extreme-scale problems in science and engineering. The last decade has witnessed tremendous advancement of tensor computation and its applications in machine learning and big data. However, its hardware optimization on resource-constrained devices remains an (almost) unexplored field. This paper presents an hardware accelerator for a classical tensor computation framework, Tucker decomposition. We study three modules of this architecture: tensor-times-matrix (TTM), matrix singular value decomposition (SVD), and tensor permutation, and implemented them on Xilinx FPGA for prototyping. In order to further reduce the computing time, a warm-start algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator is used to evaluate the performance of our design. Some synthetic data sets and a real MRI data set are used to validate the design and evaluate its performance. We compare our work with state-of-the-art software toolboxes running on both CPU and GPU, and our work shows 2.16 \u2013 30.2\u00d7 speedup on the cardiac MRI data set.", "venue": "2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Kaiqi  Zhang", "Xiyuan  Zhang", "Zheng  Zhang"], "year": 2019, "n_citations": 7}
{"id": 2670765, "s2_id": "99d886df9d0d391d48c8c0bcff144b8234ac9b82", "title": "Dependable Neural Networks Through Redundancy, A Comparison of Redundant Architectures", "abstract": "With edge-AI finding an increasing number of real-world applications, especially in industry, the question of functionally safe applications using AI has begun to be asked. In this body of work, we explore the issue of achieving dependable operation of neural networks. We discuss the issue of dependability in general implementation terms before examining lockstep solutions. We intuit that it is not necessarily a given that two similar neural networks generate results at precisely the same time and that synchronization between the platforms will be required. We perform some preliminary measurements that may support this intuition and introduce some work in implementing lockstep neural network engines. Keywords\u2014edge-AI, neural networks, lockstep processing, functional safety, dependability", "venue": "ArXiv", "authors": ["Hans Dermot Doran", "Gianluca  Ielpo", "David  Ganz", "Michael  Zapke"], "year": 2021, "n_citations": 0}
{"id": 2672258, "s2_id": "37bc6182a54d644cf747025f552c6c68bb2053ba", "title": "BEANNA: A Binary-Enabled Architecture for Neural Network Acceleration", "abstract": "Modern hardware design trends have shifted towards specialized hardware acceleration for computationally intensive tasks like machine learning and computer vision. While these complex workloads can be accelerated by commercial GPUs, domain-specific hardware is far more optimal when needing to meet the stringent memory, throughput, and power constraints of mobile and embedded devices. This paper proposes and evaluates a Binary-Enabled Architecture for Neural Network Acceleration (BEANNA), a neural network hardware accelerator capable of processing both floating point and binary network layers. Through the use of a novel 16x16 systolic array based matrix multiplier with processing elements that compute both floating point and binary multiply-adds, BEANNA seamlessly switches between high precision floating point and binary neural network layers. Running at a clock speed of 100MHz, BEANNA achieves a peak throughput of 52.8 GigaOps/second when operating in high precision mode, and 820 GigaOps/second when operating in binary mode. Evaluation of BEANNA was performed by comparing a hybrid network with floating point outer layers and binary hidden layers to a network with only floating point layers. The hybrid network accelerated using BEANNA achieved a 194% throughput increase, a 68% memory usage decrease, and a 66% energy consumption decrease per inference, all this at the cost of a mere 0.23% classification accuracy decrease on the MNIST dataset.", "venue": "ArXiv", "authors": ["Caleb  Terrill", "Fred  Chu"], "year": 2021, "n_citations": 0}
{"id": 2672294, "s2_id": "20362f4cc4f6318b754f04708a2027a725146007", "title": "Energy Saving Techniques for Phase Change Memory (PCM)", "abstract": "In recent years, the energy consumption of computing systems has increased and a large fraction of this energy is consumed in main memory. Towards this, researchers have proposed use of non-volatile memory, such as phase change memory (PCM), which has low read latency and power; and nearly zero leakage power. However, the write latency and power of PCM are very high and this, along with limited write endurance of PCM present significant challenges in enabling wide-spread adoption of PCM. To address this, several architecture-level techniques have been proposed. In this report, we review several techniques to manage power consumption of PCM. We also classify these techniques based on their characteristics to provide insights into them. The aim of this work is encourage researchers to propose even better techniques for improving energy efficiency of PCM based main memory.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 13}
{"id": 2673149, "s2_id": "c3e810c7818a26290866fe1dcd851495890be775", "title": "Byzantine-Robust and Privacy-Preserving Framework for FedML", "abstract": "Federated learning has emerged as a popular paradigm for collaboratively training a model from data distributed among a set of clients. This learning setting presents, among others, two unique challenges: how to protect privacy of the clients' data during training, and how to ensure integrity of the trained model. We propose a two-pronged solution that aims to address both challenges under a single framework. First, we propose to create secure enclaves using a trusted execution environment (TEE) within the server. Each client can then encrypt their gradients and send them to verifiable enclaves. The gradients are decrypted within the enclave without the fear of privacy breaches. However, robustness check computations in a TEE are computationally prohibitive. Hence, in the second step, we perform a novel gradient encoding that enables TEEs to encode the gradients and then offloading Byzantine check computations to accelerators such as GPUs. Our proposed approach provides theoretical bounds on information leakage and offers a significant speed-up over the baseline in empirical evaluation.", "venue": "ArXiv", "authors": ["Hanieh  Hashemi", "Yongqin  Wang", "Chuan  Guo", "Murali  Annavaram"], "year": 2021, "n_citations": 5}
{"id": 2675659, "s2_id": "dfdea0f436999dbecfd26d48e1701463ef05a4d5", "title": "Fast Polar Decoders: Algorithm and Implementation", "abstract": "Polar codes provably achieve the symmetric capacity of a memoryless channel while having an explicit construction. The adoption of polar codes however, has been hampered by the low throughput of their decoding algorithm. This work aims to increase the throughput of polar decoding hardware by an order of magnitude relative to successive-cancellation decoders and is more than 8 times faster than the current fastest polar decoder. We present an algorithm, architecture, and FPGA implementation of a flexible, gigabit-per-second polar decoder.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Gabi  Sarkis", "Pascal  Giard", "Alexander  Vardy", "Claude  Thibeault", "Warren J. Gross"], "year": 2014, "n_citations": 275}
{"id": 2676096, "s2_id": "7459aabd79238fddba5c3df236b73afd8a6bf438", "title": "Efficient Approaches for Designing Fault Tolerant Reversible Carry Look-Ahead and Carry-Skip Adders", "abstract": "Combinational or Classical logic circuits dissipate heat for every bit of information that is lost. Information is lost when the input vector cannot be recovered from its corresponding output vector. Reversible logic circuit implements only the functions having one-to-one mapping between its input and output vectors and therefore naturally takes care of heating. Reversible logic design becomes one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient approaches for designing fault tolerant reversible fast adders that implement carry look-ahead and carry-skip logic. The proposed high speed reversible adders include MIG gates for the realization of its basic building block. The MIG gate is universal and parity preserving. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.", "venue": "ArXiv", "authors": ["Md. Saiful Islam", "Muhammad Mahbubur Rahman", "Zerina  Begum", "Mohd. Zulfiquar Hafiz"], "year": 2010, "n_citations": 29}
{"id": 2679219, "s2_id": "ff335e1a87c791757f294544ae3b87dfc7a202c8", "title": "AutoSVA: Democratizing Formal Verification of RTL Module Interactions", "abstract": "Modern SoC design relies on the ability to separately verify IP blocks relative to their own specifications. Formal verification (FV) using SystemVerilog Assertions (SVA) is an effective method to exhaustively verify blocks at unit-level. Unfortunately, FV has a steep learning curve and requires engineering effort that discourages hardware designers from using it during RTL module development. We propose AutoSVA, a framework to automatically generate FV testbenches that verify liveness and safety of control logic involved in module interactions. We demonstrate AutoSVA\u2019s effectiveness and efficiency on deadlock-critical modules of widely-used open-source hardware projects.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Marcelo  Orenes-Vera", "Aninda  Manocha", "David  Wentzlaff", "Margaret  Martonosi"], "year": 2021, "n_citations": 0}
{"id": 2681067, "s2_id": "65fec44db0d1b425fd46a8f01a35ea767dc36337", "title": "Improving 3D NAND Flash Memory Lifetime by Tolerating Early Retention Loss and Process Variation", "abstract": "Compared to planar (i.e., two-dimensional) NAND flash memory, 3D NAND flash memory uses a new flash cell design, and vertically stacks dozens of silicon layers in a single chip. This allows 3D NAND flash memory to increase storage density using a much less aggressive manufacturing process technology than planar NAND flash memory. The circuit-level and structural changes in 3D NAND flash memory significantly alter how different error sources affect the reliability of the memory. In this paper, through experimental characterization of real, state-of-the-art 3D NAND flash memory chips, we find that 3D NAND flash memory exhibits three new error sources that were not previously observed in planar NAND flash memory: (1) layer-to-layer process variation, a new phenomenon specific to the 3D nature of the device, where the average error rate of each 3D-stacked layer in a chip is significantly different; (2) early retention loss, a new phenomenon where the number of errors due to charge leakage increases quickly within several hours after programming; and (3) retention interference, a new phenomenon where the rate at which charge leaks from a flash cell is dependent on the data value stored in the neighboring cell. Based on our experimental results, we develop new analytical models of layer-to-layer process variation and retention loss in 3D NAND flash memory. Motivated by our new findings and models, we develop four new techniques to mitigate process variation and early retention loss in 3D NAND flash memory. Our first technique, Layer Variation Aware Reading (LaVAR), reduces the effect of layer-to-layer process variation by fine-tuning the read reference voltage separately for each layer. Our second technique, Layer-Interleaved Redundant Array of Independent Disks (LI-RAID), uses information about layer-to-layer process variation to intelligently group pages under the RAID error recovery technique in a manner that reduces the likelihood that the recovery of a group fails significantly earlier than the recovery of other groups. Our third technique, Retention Model Aware Reading (ReMAR), reduces retention errors in 3D NAND flash memory by tracking the retention time of the data using our new retention model and adapting the read reference voltage to data age. Our fourth technique, Retention Interference Aware Neighbor-Cell Assisted Correction (ReNAC), adapts the read reference voltage to the amount of retention interference a page has experienced, in order to re-read the data after a read operation fails. These four techniques are complementary, and can be combined together to significantly improve flash memory reliability. Compared to a state-of-the-art baseline, our techniques, when combined, improve flash memory lifetime by 1.85\u00d7. Alternatively, if a NAND flash vendor wants to keep the lifetime of the 3D NAND flash memory device constant, our techniques reduce the storage overhead required to hold error correction information by 78.9%.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Yixin  Luo", "Saugata  Ghose", "Yu  Cai", "Erich F. Haratsch", "Onur  Mutlu"], "year": 2018, "n_citations": 6}
{"id": 2683393, "s2_id": "50741e1e838b8292e2d730f89f766c4c735e3678", "title": "A Competitive Edge: Can FPGAs Beat GPUs at DCNN Inference Acceleration in Resource-Limited Edge Computing Applications?", "abstract": "When trained as generative models, Deep Learning algorithms have shown exceptional performance on tasks involving high dimensional data such as image denoising and super-resolution. In an increasingly connected world dominated by mobile and edge devices, there is surging demand for these algorithms to run locally on embedded platforms. FPGAs, by virtue of their reprogrammability and low-power characteristics, are ideal candidates for these edge computing applications. As such, we design a spatio-temporally parallelized hardware architecture capable of accelerating a deconvolution algorithm optimized for power-efficient inference on a resource-limited FPGA. We propose this FPGA-based accelerator to be used for Deconvolutional Neural Network (DCNN) inference in low-power edge computing applications. To this end, we develop methods that systematically exploit micro-architectural innovations, design space exploration, and statistical analysis. Using a Xilinx PYNQ-Z2 FPGA, we leverage our architecture to accelerate inference for two DCNNs trained on the MNIST and CelebA datasets using the Wasserstein GAN framework. On these networks, our FPGA design achieves a higher throughput to power ratio with lower run-to-run variation when compared to the NVIDIA Jetson TX1 edge computing GPU.", "venue": "ArXiv", "authors": ["Ian  Colbert", "Jake  Daly", "Ken  Kreutz-Delgado", "Srinjoy  Das"], "year": 2021, "n_citations": 2}
{"id": 2684796, "s2_id": "f0a32e442ef706e5e57d14e044b3d4b568a51146", "title": "Louvre: Light-weight Ordering Using Versioning for Release Consistency", "abstract": "Fence instructions are fundamental primitives that ensure consistency in a weakly consistent shared memory multi-core processor. The execution cost of these instructions is significant and adds a non-trivial overhead to parallel programs. In a naive architecture implementation, we track the ordering constraints imposed by a fence by its entry in the reorder buffer and its execution overhead entails stalling the processor's pipeline until the store buffer is drained and also conservatively invalidating speculative loads. These actions create a cascading effect of increased overhead on the execution of the following instructions in the program. We find these actions to be overly restrictive and that they can be further relaxed thereby allowing aggressive optimizations. \nThe current work proposes a lightweight mechanism in which we assign ordering tags, called versions, to load and store instructions when they reside in the load/store queues and the write buffer. The version assigned to a memory access allows us to fully exploit the relaxation allowed by the weak consistency model and restricts its execution in such a way that the ordering constraints by the model are satisfied. We utilize the information captured through the assigned versions to reduce stalls caused by waiting for the store buffer to drain and to avoid unnecessary squashing of speculative loads, thereby minimizing the re-execution penalty. This method is particularly effective for the release consistency model that employs uni-directional fence instructions. We show that this mechanism reduces the ordering instruction latency by 39.6% and improves program performance by 11% on average over the baseline implementation.", "venue": "ArXiv", "authors": ["Pranith  Kumar", "Prasun  Gera", "Hyojong  Kim", "Hyesoon  Kim"], "year": 2017, "n_citations": 0}
{"id": 2685105, "s2_id": "4d49e11e2e87a7b43f43091d348220419c4fd84e", "title": "Channel Tiling for Improved Performance and Accuracy of Optical Neural Network Accelerators", "abstract": "Low latency, high throughput inference on Convolution Neural Networks (CNNs) remains a challenge, especially for applications requiring large input or large kernel sizes. 4F optics provides a solution to accelerate CNNs by converting convolutions into Fourier-domain point-wise multiplications that are computationally 'free' in optical domain. However, existing 4F CNN systems suffer from the all-positive sensor readout issue which makes the implementation of a multi-channel, multi-layer CNN not scalable or even impractical. In this paper we propose a simple channel tiling scheme for 4F CNN systems that utilizes the high resolution of 4F system to perform channel summation inherently in optical domain before sensor detection, so the outputs of different channels can be correctly accumulated. Compared to state of the art, channel tiling gives similar accuracy, significantly better robustness to sensing quantization (33\\% improvement in required sensing precision) error and noise (10dB reduction in tolerable sensing noise), 0.5X total filters required, 10-50X+ throughput improvement and as much as 3X reduction in required output camera resolution/bandwidth. Not requiring any additional optical hardware, the proposed channel tiling approach addresses an important throughput and precision bottleneck of high-speed, massively-parallel optical 4F computing systems.", "venue": "ArXiv", "authors": ["Shurui  Li", "Mario  Miscuglio", "Volker J. Sorger", "Puneet  Gupta"], "year": 2020, "n_citations": 1}
{"id": 2685284, "s2_id": "4a9240b44e907205e2bd56e2839d09a2a67083ae", "title": "Ara: A 1-GHz+ Scalable and Energy-Efficient RISC-V Vector Processor With Multiprecision Floating-Point Support in 22-nm FD-SOI", "abstract": "In this article, we present Ara, a 64-bit vector processor based on the version 0.5 draft of RISC-V\u2019s vector extension, implemented in GlobalFoundries 22FDX fully depleted silicon-on-insulator (FD-SOI) technology. Ara\u2019s microarchitecture is scalable, as it is composed of a set of identical lanes, each containing part of the processor\u2019s vector register file and functional units. It achieves up to 97% floating-point unit (FPU) utilization when running a <inline-formula> <tex-math notation=\"LaTeX\">$256\\times256$ </tex-math></inline-formula> double-precision matrix multiplication on 16 lanes. Ara runs at more than 1 GHz in the typical corner (TT/0.80 V/25 \u00b0C), achieving a performance up to 33 DP\u2013GFLOPS. In terms of energy efficiency, Ara achieves up to 41 DP\u2013GFLOPS <inline-formula> <tex-math notation=\"LaTeX\">$\\text {W}^{-1}$ </tex-math></inline-formula> under the same conditions, which is slightly superior to similar vector processors found in the literature. An analysis on several vectorizable linear algebra computation kernels for a range of different matrix and vector sizes gives insight into performance limitations and bottlenecks for vector processors and outlines directions to maintain high energy efficiency even for small matrix sizes where the vector architecture achieves suboptimal utilization of the available FPUs.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Matheus  Cavalcante", "Fabian  Schuiki", "Florian  Zaruba", "Michael  Schaffner", "Luca  Benini"], "year": 2020, "n_citations": 21}
{"id": 2686176, "s2_id": "2be0e4e23cbfa13d0d8f52930cff25f906e70d7f", "title": "An Energy-Efficient Quad-Camera Visual System for Autonomous Machines on FPGA Platform", "abstract": "In our past few years\u2019 of commercial deployment experiences, we identify localization as a critical task in autonomous machine applications, and a great acceleration target. In this paper, based on the observation that the visual frontend is a major performance and energy consumption bottleneck, we present our design and implementation of an energy-efficient hardware architecture for ORB (Oriented-Fast and Rotated-BRIEF) based localization system on FPGAs. To support our multi-sensor autonomous machine localization system, we present hardware synchronization, frame-multiplexing, and parallelization techniques, which are integrated in our design. Compared to Nvidia TX1 and Intel i7, our FPGA-based implementation achieves $5.6\\times$ and $3.4\\times$ speedup, as well as $3.0\\times$ and $34.6\\times$ power reduction, respectively.", "venue": "2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)", "authors": ["Zishen  Wan", "Yuyang  Zhang", "Arijit  Raychowdhury", "Bo  Yu", "Yanjun  Zhang", "Shaoshan  Liu"], "year": 2021, "n_citations": 3}
{"id": 2687230, "s2_id": "74d6c95b5ab3f77c343a205fde9d9b7f7e540465", "title": "A brief experience on journey through hardware developments for image processing and its applications on Cryptography", "abstract": "The importance of embedded applications on image and video processing,communication and cryptography domain has been taking a larger space in current research era. Improvement of pictorial information for betterment of human perception like deblurring, de-noising in several fields such as satellite imaging, medical imaging etc are renewed research thrust. Specifically we would like to elaborate our experience on the significance of computer vision as one of the domains where hardware implemented algorithms perform far better than those implemented through software. So far embedded design engineers have successfully implemented their designs by means of Application Specific Integrated Circuits (ASICs) and/or Digital Signal Processors (DSP), however with the advancement of VLSI technology a very powerful hardware device namely the Field Programmable Gate Array (FPGA) combining the key advantages of ASICs and DSPs was developed which have the possibility of reprogramming making them a very attractive device for rapid prototyping.Communication of image and video data in multiple FPGA is no longer far away from the thrust of secured transmission among them, and then the relevance of cryptography is indeed unavoidable. This paper shows how the Xilinx hardware development platform as well Mathworks Matlab can be used to develop hardware based computer vision algorithms and its corresponding crypto transmission channel between multiple FPGA platform from a system level approach, making it favourable for developing a hardware-software co-design environment.", "venue": "ArXiv", "authors": ["Sangeet  Saha", "Chandrajit  Pal", "Rourab  Paul", "Satyabrata  Maity", "Suman  Sau"], "year": 2012, "n_citations": 5}
{"id": 2690730, "s2_id": "4805fd529fe58798d01689b397fe6dfcc65c03e2", "title": "Noise Limited Computational Speed", "abstract": "In modern transistor based logic gates, the impact of noise on computation has become increasingly relevant since the voltage scaling strategy aimed at decreasing the dissipated power, has increased the probability of error due to the reduced switching threshold voltages. In this paper, we discuss the role of noise in a two state model that mimic the dynamics of standard logic gates and show that the presence of the noise sets a fundamental limit to the computing speed. An optimal idle time interval that minimizes the error probability is derived.", "venue": "ArXiv", "authors": ["Luca  Gammaitoni"], "year": 2007, "n_citations": 18}
{"id": 2690845, "s2_id": "a220608766e864441e0b48ab1164146555e5aaa7", "title": "An Internet-enabled technology to support evolutionary design", "abstract": "Abstract This paper discusses the systematic use of product feedback information to support life cycle design approaches and provides guidelines for developing a design at both the product and the system levels. Design activities are surveyed in the light of the product life cycle, and the design information flow is interpreted from a semiotic perspective. The natural evolution of a design is considered, the notion of design expectations is introduced and the importance of evaluation of these expectations in dynamic environments is argued. Possible strategies for reconciliation of the expectations and environmental factors are described. An Internet-enabled technology is proposed to monitor product functionality, use and operational environment and to supply the designer with relevant information. A pilot study of assessing design expectations of a refrigerator is outlined, and conclusions are drawn.", "venue": "ArXiv", "authors": ["Victor V. Kryssanov", "H.  Tamaki", "K.  Ueda"], "year": 2006, "n_citations": 1}
{"id": 2695377, "s2_id": "8c8f83ae02a364d6b4bc755bee4b4a115682b3ee", "title": "DeepScaleTool: A Tool for the Accurate Estimation of Technology Scaling in the Deep-Submicron Era", "abstract": "The estimation of classical CMOS \"constant-field\" or \"Dennard\" scaling methods that define scaling factors for various dimensional and electrical parameters have become less accurate in the deep-submicron regime, which drives the need for better estimation approaches especially in the educational and research domains. We present DeepScaleTool, a tool for the accurate estimation of deep-submicron technology scaling by modeling and curve fitting published data by a leading commercial fabrication company for silicon fabrication technology generations from 130 nm to 7 nm for the key parameters of area, delay, and energy. Compared to 10 nm-7 nm scaling data published by a leading foundry, the DeepScaleTool achieves an error of 1.7% in area, 2.5% in delay, and 5% in power. This compares favorably with another leading academic estimation method that achieves an error of 24% in area, 9.1% in delay, and 24.9% in power.", "venue": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Satyabrata  Sarangi", "Bevan  Baas"], "year": 2021, "n_citations": 1}
{"id": 2698505, "s2_id": "d94a70d042accfed211e9f58590a9135969ae4d8", "title": "ThreadPoolComposer - An Open-Source FPGA Toolchain for Software Developers", "abstract": "This extended abstract presents ThreadPoolComposer, a high-level synthesis-based development framework and meta-toolchain that provides a uniform programming interface for FPGAs portable across multiple platforms.", "venue": "ArXiv", "authors": ["Jens  Korinth", "David de la Chevallerie", "Andreas  Koch"], "year": 2015, "n_citations": 1}
{"id": 2698537, "s2_id": "6d462cfd5b06cf84bbf4ccb72997886f2776ff18", "title": "Enabling Incremental Training with Forward Pass for Edge Devices", "abstract": "Deep Neural Networks (DNNs) are commonly deployed on end devices that exist in constantly changing environments. In order for the system to maintain it\u2019s accuracy, it is critical that it is able to adapt to changes and recover by retraining parts of the network. However, end devices have limited resources making it challenging to train on the same device. Moreover, training deep neural networks is both memory and compute intensive due to the backpropagation algorithm. In this paper we introduce a method using evolutionary strategy (ES) that can partially retrain the network enabling it to adapt to changes and recover after an error has occurred. This technique enables training on an inference-only hardware without the need to use backpropagation and with minimal resource overhead. We demonstrate the ability of our technique to retrain a quantized MNIST neural network after injecting noise to the input. Furthermore, we present the micro-architecture required to enable training on HLS4ML (an inference hardware architecture) and implement it in Verilog. We synthesize our implementation for a Xilinx Kintex Ultrascale Field Programmable Gate Array (FPGA) resulting in less than 1% resource utilization required to implement the incremental training.", "venue": "ArXiv", "authors": ["Dana  AbdulQader", "Shoba  Krishnan", "Claudionor N. Coelho"], "year": 2021, "n_citations": 0}
{"id": 2703682, "s2_id": "923bcfe2fefd9e667fd76b93361e55b183351568", "title": "Quad-Core RSA Processor with Countermeasure Against Power Analysis Attacks", "abstract": "Rivest-Shamir-Adleman (RSA) cryptosystem uses modular multiplication for encryption and decryption. So, performance of RSA can be drastically improved by optimizing modular multiplication. This paper proposes a new parallel, high-radix Montgomery multiplier for 1024 bits multi-core RSA processor. Each computation step operates in radix 4. The computation speed is increased by more than 4 times. We also implement a True Random Number Generator based resilience block to protect the coprocessor against power attacks.", "venue": "ArXiv", "authors": ["Javad  Bagherzadeh", "Vishishtha  Bothra", "Disha  Gujar", "Sugandha  Gupta", "Jinal  Shah"], "year": 2020, "n_citations": 0}
{"id": 2704175, "s2_id": "7a0608a274bdef1f948dee74be047a3ad62a00c8", "title": "Runtime Mitigation of Packet Drop Attacks in Fault-tolerant Networks-on-Chip", "abstract": "Fault-tolerant routing (FTR) in Networks-on-Chip (NoCs) has become a common practice to sustain the performance of multi-core systems with an increasing number of faults on a chip. On the other hand, usage of third-party intellectual property blocks has made security a primary concern in modern day designs. This article presents a mechanism to mitigate a denial-of-service attack, namely packet drop attack, which may arise due to the hardware Trojans (HTs) in NoCs that adopt FTR algorithms. HTs, associated with external kill switches, are conditionally triggered to enable the attack scenario. Security modules, such as authentication unit, buffer shuffler, and control unit, have been proposed to thwart the attack in runtime and restore secure packet flow in the NoC. These units work together as a shield to safeguard the packets from proceeding towards the output ports with faulty links. Synthesis results show that the proposed secure FT router, when compared with a baseline FT router, has area and power overheads of at most 4.04% and 0.90%, respectively. Performance evaluation shows that SeFaR has acceptable overheads in the execution time, energy consumption, average packet latency, and power-latency product metrics when compared with a baseline FT router while running real benchmarks, as well as synthetic traffic. Further, a possible design of a comprehensive secure router has been presented with a view to addressing and mitigating multiple attacks that can arise in the NoC routers.", "venue": "ArXiv", "authors": ["N  Prasad", "Navonil  Chatterjee", "Santanu  Chattopadhyay", "Indrajit  Chakrabarti"], "year": 2019, "n_citations": 0}
{"id": 2704306, "s2_id": "01cf3d3b91ef610dbeb2723667d00cea1b74b362", "title": "Performance Optimization of SU3_Bench on Xeon and Programmable Integrated Unified Memory Architecture", "abstract": "SU3_Bench is a microbenchmark developed to explore performance portability across multiple programming models/methodologies using a simple, but nontrivial, mathematical kernel. This kernel has been derived from the MILC lattice quantum chromodynamics (LQCD) code. SU3_Bench is bandwidth bound and generates regular compute and data access patterns. Therefore, on most traditional CPU and GPU-based systems, its performance is mainly determined by the achievable memory bandwidth. Although SU3_Bench is a simple kernel, experience says its subtleties require a certain amount of tweaking to achieve peak performance for a given programming model and hardware, making performance portability challenging. In this paper, we share some of the challenges in obtaining the peak performance for SU3_Bench on a state-of-the-art Intel Xeon machine, due to the nuances of variable definition, the nature of compiler-provided default constructors, how memory is accessed at object creation time, and the NUMA effects on the machine. We discuss how to tackle those challenges to improve SU3_Bench\u2019s performance by 2\u00d7 compared to the original OpenMP implementation available at Github. This provides a valuable lesson for other similar kernels. Expanding on the performance portability aspects, we also show early results obtained porting SU3_Bench to the new Intel Programmable Integrated UnifiedMemory Architecture (PIUMA), characterized by a more balanced flops-to-byte ratio. This paper shows that it is not the usual bandwidth or flops, rather the pipeline throughput, that determines SU3_Bench\u2019s performance on PIUMA. Finally, we show how to improve performance on PIUMA and how that compares with the performance on Xeon, which has around one order of magnitude more flops-per-byte. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, July 2017, Washington, DC, USA \u00a9 2021 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn CCS CONCEPTS \u2022Hardware\u2192 Emerging technologies; \u2022Computingmethodologies \u2192 Shared memory algorithms.", "venue": "ArXiv", "authors": ["Jesmin Jahan Tithi", "Fabio  Checconi", "Douglas  Doerfler", "Fabrizio  Petrini"], "year": 2021, "n_citations": 1}
{"id": 2705746, "s2_id": "9bd9fa72bcd400b7a8374c96272e40e314cb9e55", "title": "SMART: A Heterogeneous Scratchpad Memory Architecture for Superconductor SFQ-based Systolic CNN Accelerators", "abstract": "Ultra-fast & low-power superconductor single-flux-quantum (SFQ)-based CNN systolic accelerators are built to enhance the CNN inference throughput. However, shift-register (SHIFT)-based scratchpad memory (SPM) arrays prevent a SFQ CNN accelerator from exceeding 40% of its peak throughput, due to the lack of random access capability. This paper first documents our study of a variety of cryogenic memory technologies, including Vortex Transition Memory (VTM), Josephson-CMOS SRAM, MRAM, and Superconducting Nanowire Memory, during which we found that none of the aforementioned technologies made a SFQ CNN accelerator achieve high throughput, small area, and low power simultaneously. Second, we present a heterogeneous SPM architecture, SMART, composed of SHIFT arrays and a random access array to improve the inference throughput of a SFQ CNN systolic accelerator. Third, we propose a fast, low-power and dense pipelined random access CMOS-SFQ array by building SFQ passive-transmission-line-based H-Trees that connect CMOS sub-banks. Finally, we create an ILP-based compiler to deploy CNN models on SMART. Experimental results show that, with the same chip area overhead, compared to the latest SHIFT-based SFQ CNN accelerator, SMART improves the inference throughput by 3.9 \u00d7 (2.2 \u00d7), and reduces the inference energy by 86% (71%) when inferring a single image (a batch of images).", "venue": "MICRO", "authors": ["Farzaneh  Zokaee", "Lei  Jiang"], "year": 2021, "n_citations": 0}
{"id": 2711974, "s2_id": "2b8a95fa19f0b5ac8a6e87c389edd7121ac44ada", "title": "HADES: Microprocessor Hazard Analysis via Formal Verification of Parameterized Systems", "abstract": "HADES is a fully automated verification tool for pipeline-based microprocessors that aims at flaws caused by improperly handled data hazards. It focuses on single-pipeline microprocessors designed at the register transfer level (RTL) and deals with read-after-write, write-after-write, and write-after-read hazards. HADES combines several techniques, including data-flow analysis, error pattern matching, SMT solving, and abstract regular model checking. It has been successfully tested on several microprocessors for embedded applications.", "venue": "MEMICS", "authors": ["Luk\u00e1s  Charv\u00e1t", "Ales  Smrcka", "Tom\u00e1s  Vojnar"], "year": 2016, "n_citations": 1}
{"id": 2716503, "s2_id": "b729a5a1e4147146052b48cba54d70fc3a9b6baf", "title": "Irregular Accesses Reorder Unit: Improving GPGPU Memory Coalescing for Graph-Based Workloads", "abstract": "GPGPU architectures have become established as the dominant parallelization and performance platform achieving exceptional popularization and empowering domains such as regular algebra, machine learning, image detection and self-driving cars. However, irregular applications struggle to fully realize GPGPU performance as a result of control flow divergence and memory divergence due to irregular memory access patterns. \nTo ameliorate these issues, programmers are obligated to carefully consider architecture features and devote significant efforts to modify the algorithms with complex optimization techniques, which shift programmers priorities yet struggle to quell the shortcomings. We show that in graph-based GPGPU irregular applications these inefficiencies prevail, yet we find that it is possible to relax the strict relationship between thread and data processed to empower new optimizations. \nBased on this key idea, we propose the Irregular accesses Reorder Unit (IRU), a novel hardware extension tightly integrated in the GPGPU pipeline. The IRU reorders data processed by the threads on irregular accesses which significantly improves memory coalescing, and allows increased performance and energy efficiency. Additionally, the IRU is capable of filtering and merging duplicated irregular access which further improves graph-based irregular applications. Programmers can easily utilize the IRU with a simple API, or compiler optimized generated code with the extended ISA instructions provided. \nWe evaluate our proposal for state-of-the-art graph-based algorithms and a wide selection of applications. Results show that the IRU achieves a memory coalescing improvement of 1.32x and a 46% reduction in the overall traffic in the memory hierarchy, which results in 1.33x and 13% improvement in performance and energy savings respectively, while incurring in a small 5.6% area overhead.", "venue": "ArXiv", "authors": ["Albert  Segura", "Jose-Maria  Arnau", "Antonio  Gonzalez"], "year": 2020, "n_citations": 0}
{"id": 2722735, "s2_id": "b958b82b85690edfc3ad3a1aeb1532d2df275b6b", "title": "Brightening the Optical Flow through Posit Arithmetic", "abstract": "As new technologies are invented, their commercial viability needs to be carefully examined along with their technical merits and demerits. The ${posit}^{TM}$ data format, proposed as a drop-in replacement for IEEE $754 ^{TM}$ float format, is one such invention that requires extensive theoretical and experimental study to identify products that can benefit from the advantages of posits for specific market segments. In this paper, we present an extensive empirical study of posit-based arithmetic vis-\u00e0-vis IEEE 754 compliant arithmetic for the optical flow estimation method called Lucas-Kanade (LuKa). First, we use SoftPosit and SoftFloat format emulators to perform an empirical error analysis of the LuKa method. Our study shows that the average error in LuKa with SoftPosit is an order of magnitude lower than LuKa with SoftFloat. We then present the integration of the hardware implementation of a posit adder and multiplier in a RISC-V open-source platform. We make several recommendations, along with the analysis of LuKa in the RISC-V context, for future generation platforms incorporating posit arithmetic units.", "venue": "2021 22nd International Symposium on Quality Electronic Design (ISQED)", "authors": ["Vinay  Saxena", "Ankitha  Reddy", "Jonathan  Neudorfer", "John  Gustafson", "Sangeeth  Nambiar", "Rainer  Leupers", "Farhad  Merchant"], "year": 2021, "n_citations": 2}
{"id": 2725471, "s2_id": "d4aa753378e3396a0dd30d8a2e2f4758cc9b0ea3", "title": "On Architecture to Architecture Mapping for Concurrency", "abstract": "Mapping programs from one architecture to another plays a key role in technologies such as binary translation, decompilation, emulation, virtualization, and application migration. Although multicore architectures are ubiquitous, the state-of-the-art translation tools do not handle concurrency primitives correctly. Doing so is rather challenging because of the subtle differences in the concurrency models between architectures. \nIn response, we address various aspects of the challenge. First, we develop correct and efficient translations between the concurrency models of two mainstream architecture families: x86 and ARM (versions 7 and 8). We develop direct mappings between x86 and ARMv8 and ARMv7, and fence elimination algorithms to eliminate redundant fences after direct mapping. Although our mapping utilizes ARMv8 as an intermediate model for mapping between x86 and ARMv7, we argue that it should not be used as an intermediate model in a decompiler because it disallows common compiler transformations. \nSecond, we propose and implement a technique for inserting memory fences for safely migrating programs between different architectures. Our technique checks robustness against x86 and ARM, and inserts fences upon robustness violations. Our experiments demonstrate that in most of the programs both our techniques introduce significantly fewer fences compared to naive schemes for porting applications across these architectures.", "venue": "ArXiv", "authors": ["Soham  Chakraborty"], "year": 2020, "n_citations": 0}
{"id": 2726388, "s2_id": "8882713b94bc22ff9615a140e4b17cc821a79cb1", "title": "A Digital Hardware Fast Algorithm and FPGA-based Prototype for a Novel 16-point Approximate DCT for Image Compression Applications", "abstract": "The discrete cosine transform (DCT) is the key step in many image and video coding standards. The eight-point DCT is an important special case, possessing several low-complexity approximations widely investigated. However, the 16-point DCT transform has energy compaction advantages. In this sense, this paper presents a new 16-point DCT approximation with null multiplicative complexity. The proposed transform matrix is orthogonal and contains only zeros and ones. The proposed transform outperforms the well-known Walsh?Hadamard transform and the current state-of-the-art 16-point approximation. A fast algorithm for the proposed transform is also introduced. This fast algorithm is experimentally validated using hardware implementations that are physically realized and verified on a 40?nm CMOS Xilinx Virtex-6 XC6VLX240T FPGA chip for a maximum clock rate of 342?MHz. Rapid prototypes on FPGA for a 8-bit input word size show significant improvement in compressed image quality by up to 1?2?dB at the cost of only eight adders compared to the state-of-art 16-point DCT approximation algorithm in the literature (Bouguezel et al 2010 Proc. 53rd IEEE Int. Midwest Symp. on Circuits and Systems).", "venue": "ArXiv", "authors": ["F\u00e1bio M. Bayer", "Renato J. Cintra", "Amila  Edirisuriya", "Arjuna  Madanayake"], "year": 2017, "n_citations": 26}
{"id": 2728411, "s2_id": "540d4384ec0a6b30a168afc9d33c4879bf186fe5", "title": "CMOS Ising Machines with Coupled Bistable Nodes", "abstract": "Ising machines use physics to naturally guide a dynamical system towards an optimal state which can be read out as a heuristical solution to a combinatorial optimization problem. Such designs that use nature as a computing mechanism can lead to higher performance and/or lower operation costs. Quantum annealers are a prominent example of such efforts. However, existing Ising machines are generally bulky and energy intensive. Such disadvantages might lead to intrinsic advantages at some larger scale in the future. But for now, integrated electronic designs allow more immediate applications. We propose one such design that uses bistable nodes, coupled with programmable and variable strengths. The design is fully CMOS compatible for on-chip applications and demonstrates competitive solution quality and significantly superior execution time and energy.", "venue": "ArXiv", "authors": ["Richard  Afoakwa", "Yiqiao  Zhang", "Uday Kumar Reddy Vengalam", "Zeljko  Ignjatovic", "Michael  Huang"], "year": 2020, "n_citations": 0}
{"id": 2729627, "s2_id": "892e7af049e182ba0dc172e22f16730150f1a1cb", "title": "On the Impact of Partial Sums on Interconnect Bandwidth and Memory Accesses in a DNN Accelerator", "abstract": "Dedicated accelerators are being designed to address the huge resource requirement of the deep neural network (DNN) applications. The power, performance and area (PPA) constraints limit the number of MACs available in these accelerators. The convolution layers which require huge number of MACs are often partitioned into multiple iterative sub-tasks. This puts huge pressure on the available system resources such as interconnect and memory bandwidth. The optimal partitioning of the feature maps for these sub-tasks can reduce the bandwidth requirement substantially. Some accelerators avoid off-chip or interconnect transfers by implementing local memories; however, the memory accesses are still performed and a reduced bandwidth can help in saving power in such architectures. In this paper, we propose a first order analytical method to partition the feature maps for optimal bandwidth and evaluate the impact of such partitioning on the bandwidth. This bandwidth can be saved by designing an active memory controller which can perform basic arithmetic operations. It is shown that the optimal partitioning and active memory controller can achieve up to 40% bandwidth reduction.", "venue": "2020 IEEE 15th International Conference on Industrial and Information Systems (ICIIS)", "authors": ["Mahesh  Chandra"], "year": 2020, "n_citations": 0}
{"id": 2733681, "s2_id": "86736393554a9461dfc7066b0d4ed10e64d59762", "title": "Resource sharing and pipelining in coarse-grained reconfigurable architecture for domain-specific optimization", "abstract": "Coarse-grained reconfigurable architectures aim to achieve goals of both high performance and flexibility. However, existing reconfigurable array architectures require many resources without considering the specific application domain. Functional resources that take long latency and/or large area can be pipelined and/or shared among the processing elements. Therefore, the hardware cost and the delay can be effectively reduced without any performance degradation for some application domains. We suggest such a reconfigurable array architecture template and a design space exploration flow for domain-specific optimization. Experimental results show that our approach is much more efficient, in both performance and area, compared to existing reconfigurable architectures.", "venue": "Design, Automation and Test in Europe", "authors": ["Yoonjin  Kim", "Mary  Kiemb", "Chulsoo  Park", "Jinyong  Jung", "Kiyoung  Choi"], "year": 2005, "n_citations": 98}
{"id": 2734961, "s2_id": "892eb24c10cc79482013c0d30500b01ab53e4b2a", "title": "Development of routing algorithms in networks-on-chip based on ring circulant topologies", "abstract": "This work is devoted to the study of communication subsystem of networks-on-chip (NoCs) development with an emphasis on their topologies. The main characteristics of NoC topologies and the routing problem in NoCs with various topologies are considered. It is proposed to use two-dimensional circulant topologies for NoC design, since they have significantly better characteristics than most common mesh and torus topologies, and, in contrast to many other approaches to improving topologies, have a regular structure. The emphasis is on using ring circulants which although in some cases have somewhat worse characteristics than the optimal circulants, compensate by one-length first generatrix in such graphs that greatly facilitate routing in them. The paper considers three different approaches to routing in NoCs with ring circulant topology: Table routing, Clockwise routing, and Adaptive routing. The algorithms of routing are proposed, the results of synthesis of routers, based on them, are presented, and the cost of chip resources for the implementation of such communication subsystems in NoCs is estimated.", "venue": "Heliyon", "authors": ["Aleksandr Yu. Romanov"], "year": 2019, "n_citations": 8}
{"id": 2735221, "s2_id": "862bb48c7ada86a97daadb330f045085f9cafa1c", "title": "ERSFQ 8-Bit Parallel Arithmetic Logic Unit", "abstract": "We have designed and tested a parallel 8-bit ERSFQ arithmetic logic unit (ALU). The ALU design employs wave-pipelined instruction execution and features modular bit-slice architecture that is easily extendable to any number of bits and adaptable to current recycling. A carry signal synchronized with an asynchronous instruction propagation provides the wave-pipeline operation of the ALU. The ALU instruction set consists of 14 arithmetical and logical instructions. It has been designed and simulated for operation up to a 10 GHz clock rate at the 10-kA/cm2 fabrication process. The ALU is embedded into a shift\u2013register\u2013based high-frequency testbed with on-chip clock generator to allow for comprehensive high frequency testing for all possible operands. The 8-bit ERSFQ ALU, comprising 6840 Josephson junctions, has been fabricated with MIT Lincoln Lab's 10-kA/cm2 SFQ5ee fabrication process featuring eight Nb wiring layers and a high-kinetic inductance layer needed for ERSFQ technology. We evaluated the bias margins for all instructions and various operands at both low and high frequency clock. At low frequency, clock and all instruction propagation through ALU were observed with bias margins of +/\u221211% and +/\u22129%, respectively. Also at low speed, the ALU exhibited correct functionality for all arithmetical and logical instructions with +/-6% bias margins. We tested the 8-bit ALU for all instructions up to 2.8 GHz clock frequency.", "venue": "IEEE Transactions on Applied Superconductivity", "authors": ["Alex F. Kirichenko", "Igor V. Vernik", "Michael Y. Kamkar", "Jason  Walter", "Maximilian  Miller", "Lucian Remus Albu", "Oleg A. Mukhanov"], "year": 2019, "n_citations": 16}
{"id": 2737447, "s2_id": "5a77877689e44e3ed48250f53e05b0d37bd901d7", "title": "Morph: Flexible Acceleration for 3D CNN-Based Video Understanding", "abstract": "The past several years have seen both an explosion in the use of Convolutional Neural Networks (CNNs) and accelerators to make CNN inference practical. In the architecture community, the lion share of effort has targeted CNN inference for image recognition. The closely related problem of video recognition has received far less attention as an accelerator target. This is surprising, as video recognition is more computationally intensive than image recognition, and video traffic is predicted to be the majority of internet traffic in the coming years. This paper fills the gap between algorithmic and hardware advances for video recognition by providing a design space exploration and flexible architecture for accelerating 3D Convolutional Neural Networks (3D CNNs)\u2014the core kernel in modern video understanding. When compared to (2D) CNNs used for image recognition, efficiently accelerating 3D CNNs poses a significant engineering challenge due to their large (and variable over time) memory footprint and higher dimensionality. To address these challenges, we design a novel accelerator called \"Morph,\" that can adaptively support different spatial and temporal tiling strategies depending on the needs of each layer of each target 3D CNN. We codesign a software infrastructure alongside the Morph hardware to find good-fit parameters to control the hardware. Evaluated on state-of-the-art 3D CNNs, Morph achieves up to 2.7\u00d7 (1.9\u00d7 average) reduction in energy consumption and improves performance/watt up to 4.4\u00d7 (3\u00d7 average) compared to a baseline 3D CNN accelerator, with an area overhead of 2%. Morph further achieves a 11.6\u00d7 average energy reduction on 3D CNNs when compared to Eyeriss, a popular 2D CNN accelerator, while reducing efficiency compared to Eyeriss on a 2D CNN by 71%.", "venue": "2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Kartik  Hegde", "Rohit  Agrawal", "Yulun  Yao", "Christopher W. Fletcher"], "year": 2018, "n_citations": 38}
{"id": 2742206, "s2_id": "43b0466b26986eb6b180e2a46c82a7ef94977873", "title": "Temperature-aware Dynamic Optimization of Embedded Systems", "abstract": "Due to embedded systems` stringent design constraints, much prior work focused on optimizing energy consumption and/or performance. Since embedded systems typically have fewer cooling options, rising temperature, and thus temperature optimization, is an emergent concern. Most embedded systems only dissipate heat by passive convection, due to the absence of dedicated thermal management hardware mechanisms. The embedded system`s temperature not only affects the system`s reliability, but could also affect the performance, power, and cost. Thus, embedded systems require efficient thermal management techniques. However, thermal management can conflict with other optimization objectives, such as execution time and energy consumption. In this paper, we focus on managing the temperature using a synergy of cache optimization and dynamic frequency scaling, while also optimizing the execution time and energy consumption. This paper provides new insights on the impact of cache parameters on efficient temperature-aware cache tuning heuristics. In addition, we present temperature-aware phase-based tuning, TaPT, which determines Pareto optimal clock frequency and cache configurations for fine-grained execution time, energy, and temperature tradeoffs. TaPT enables autonomous system optimization and also allows designers to specify temperature constraints and optimization priorities. Experiments show that TaPT can effectively reduce execution time, energy, and temperature, while imposing minimal hardware overhead.", "venue": "ArXiv", "authors": ["Tosiron  Adegbija", "Ann  Gordon-Ross"], "year": 2016, "n_citations": 0}
{"id": 2743987, "s2_id": "bff1c796ee9b1f53e235d89ae7f3bdffb43c053e", "title": "Automatic Conversion from Flip-flop to 3-phase Latch-based Designs", "abstract": "Latch-based designs have many benefits over their flip-flop based counterparts but have limited use partially because most RTL specifications are flop-centric and automatic conversion of FF to latch-based designs is challenging. Conventional conversion algorithms target master-slave latch-based designs with two non-overlapping clocks. This paper presents a novel automated design flow that converts flip-flop to 3-phase latch-based designs. The resulting circuits have the same performance as the master-slave based designs but require significantly less latches. Our experimental results demonstrate the potential for savings in the number of latches (21.3%), area (5.8%), and power (16.3%) on a variety of ISCAS, CEP, and CPU benchmark circuits, compared to the master-slave conversions.", "venue": "ArXiv", "authors": ["Huimei  Cheng", "Yichen  Gu", "Peter A. Beerel"], "year": 2019, "n_citations": 1}
{"id": 2745265, "s2_id": "64208b38e111f9358f03eaf8d5c83f8354cd5c84", "title": "SEALing Neural Network Models in Secure Deep Learning Accelerators", "abstract": "Deep learning (DL) accelerators are increasingly deployed on edge devices to support fast local inferences. However, they suffer from a new security problem, i.e., being vulnerable to physical access based attacks. An adversary can easily obtain the entire neural network (NN) model by physically snooping the GDDR memory bus that connects the accelerator chip with DRAM memory. Therefore, memory encryption becomes important for DL accelerators on edge devices to improve the security of NN models. Nevertheless, we observe that traditional memory encryption solutions that have been efficiently used in CPU systems cause significant performance degradation when directly used in DL accelerators. The main reason comes from the big bandwidth gap between the GDDR memory bus and the encryption engine. To address this problem, our paper proposes SEAL, a Secure and Efficient Accelerator scheme for deep Learning. SEAL enhances the performance of the encrypted DL accelerator from two aspects, i.e., improving the data access bandwidth and the efficiency of memory encryption. Specifically, to improve the data access bandwidth, SEAL leverages a criticality-aware smart encryption scheme which identifies partial data that have no impact on the security of NN models and allows them to bypass the encryption engine, thus reducing the amount of data to be encrypted. To improve the efficiency of memory encryption, SEAL leverages a colocation mode encryption scheme to eliminate memory accesses from counters used for encryption by co-locating data and their counters. Our experimental results demonstrate that, compared with traditional memory encryption solutions, SEAL achieves 1.4 ~ 1.6 times IPC improvement and reduces the inference latency by 39% ~ 60%. Compared with a baseline accelerator without memory encryption, SEAL compromises only 5% ~ 7% IPC for significant security improvement.", "venue": "ArXiv", "authors": ["Pengfei  Zuo", "Yu  Hua", "Ling  Liang", "Xinfeng  Xie", "Xing  Hu", "Yuan  Xie"], "year": 2020, "n_citations": 1}
{"id": 2746763, "s2_id": "4ea33217f6d927f22331bc23eee8daef7038e01c", "title": "High-Performance and Energy-Effcient Memory Scheduler Design for Heterogeneous Systems", "abstract": "When multiple processor cores (CPUs) and a GPU integrated together on the same chip share the off-chip DRAM, requests from the GPU can heavily interfere with requests from the CPUs, leading to low system performance and starvation of cores. Unfortunately, state-of-the-art memory scheduling algorithms are ineffective at solving this problem due to the very large amount of GPU memory traffic, unless a very large and costly request buffer is employed to provide these algorithms with enough visibility across the global request stream. \nPreviously-proposed memory controller (MC) designs use a single monolithic structure to perform three main tasks. First, the MC attempts to schedule together requests to the same DRAM row to increase row buffer hit rates. Second, the MC arbitrates among the requesters (CPUs and GPU) to optimize for overall system throughput, average response time, fairness and quality of service. Third, the MC manages the low-level DRAM command scheduling to complete requests while ensuring compliance with all DRAM timing and power constraints. This paper proposes a fundamentally new approach, called the Staged Memory Scheduler (SMS), which decouples the three primary MC tasks into three significantly simpler structures that together improve system performance and fairness. Our evaluation shows that SMS provides 41.2% performance improvement and fairness improvement compared to the best previous state-of-the-art technique, while enabling a design that is significantly less complex and more power-efficient to implement.", "venue": "ArXiv", "authors": ["Rachata  Ausavarungnirun", "Gabriel H. Loh", "Lavanya  Subramanian", "Kevin K. Chang", "Onur  Mutlu"], "year": 2018, "n_citations": 1}
{"id": 2751652, "s2_id": "b93dbfbaae539ba3874329ac2b91759259489283", "title": "C-LSTM: Enabling Efficient LSTM using Structured Compression Techniques on FPGAs", "abstract": "Recently, significant accuracy improvement has been achieved for acoustic recognition systems by increasing the model size of Long Short-Term Memory (LSTM) networks. Unfortunately, the ever-increasing size of LSTM model leads to inefficient designs on FPGAs due to the limited on-chip resources. The previous work proposes to use a pruning based compression technique to reduce the model size and thus speedups the inference on FPGAs. However, the random nature of the pruning technique transforms the dense matrices of the model to highly unstructured sparse ones, which leads to unbalanced computation and irregular memory accesses and thus hurts the overall performance and energy efficiency. In contrast, we propose to use a structured compression technique which could not only reduce the LSTM model size but also eliminate the irregularities of computation and memory accesses. This approach employs block-circulant instead of sparse matrices to compress weight matrices and reduces the storage requirement from $\\mathcalO (k^2)$ to $\\mathcalO (k)$. Fast Fourier Transform algorithm is utilized to further accelerate the inference by reducing the computational complexity from $\\mathcalO (k^2)$ to $\\mathcalO (k\\textlog k)$. The datapath and activation functions are quantized as 16-bit to improve the resource utilization. More importantly, we propose a comprehensive framework called C-LSTM to automatically optimize and implement a wide range of LSTM variants on FPGAs. According to the experimental results, C-LSTM achieves up to 18.8X and 33.5X gains for performance and energy efficiency compared with the state-of-the-art LSTM implementation under the same experimental setup, and the accuracy degradation is very small.", "venue": "FPGA", "authors": ["Shuo  Wang", "Zhe  Li", "Caiwen  Ding", "Bo  Yuan", "Qinru  Qiu", "Yanzhi  Wang", "Yun  Liang"], "year": 2018, "n_citations": 119}
{"id": 2753164, "s2_id": "df8061cd204197cb607875a1fb8a36ec8a956d23", "title": "Design Methodologies for Reliable and Energy-efficient PCM Systems", "abstract": "Phase-change memory (PCM) is a scalable and low latency non-volatile memory (NVM) technology that has been proposed to serve as storage class memory (SCM), providing low access latency similar to DRAM and often approaching or exceeding the capacity of SSD. The multilevel property of PCM also enables its adoption in neuromorphic systems to build high-density synaptic storage. We investigate and describe two significant bottlenecks of a PCM system. First, writing to PCM cells incurs significantly higher latency and energy penalties than reading its content. Second, high operating voltages of PCM impacts its reliable operations. In this work, we propose methodologies to tackle the bottlenecks, improving performance, reliability, energy consumption, and sustainability for a PCM system.", "venue": "2020 11th International Green and Sustainable Computing Workshops (IGSC)", "authors": ["Shihao  Song", "Anup  Das"], "year": 2020, "n_citations": 9}
{"id": 2755160, "s2_id": "80d8fcb6c4b85723a1cae6a36e338f994c38f962", "title": "A Wrapper of PCI Express with FIFO Interfaces Based on FPGA", "abstract": "This paper proposes a PCI Express (PCIE) Wrapper core named PWrapper with FIFO interfaces. Compared with other PCIE solutions, PWrapper has several advantages such as flexibility, isolation of clock domain, etc. PWrapper is implemented and verified on Vertex-5-FX70T which is a development board provided by Xilinx Inc. Architecture of PWrapper and design of two key modules are illustrated, which timing optimization methods have been adopted. Then we explained the advantages and challenges of on-chip interfaces technology based on FIFOs. The verification results show that PWrapper can achieve the speed of 1.8Gbps (Giga bits per second).", "venue": "2012 International Conference on Industrial Control and Electronics Engineering", "authors": ["Hu  Li", "Yuanan  Liu", "Dongming  Yuan", "Hefei  Hu"], "year": 2012, "n_citations": 4}
{"id": 2756448, "s2_id": "e986ddd7dc95008e2296196ad3b0a093c7f81bae", "title": "Towards Fast and Energy-Efficient Binarized Neural Network Inference on FPGA", "abstract": "Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN by using a single bit (-1/+1) for network parameters and intermediate representations, which has greatly reduced the off-chip data transfer and storage overhead. However, a large amount of computation redundancy still exists in BNN inference. By analyzing local properties of images and the learned BNN kernel weights, we observe an average of ~78% input similarity and ~59% weight similarity among weight kernels, measured by our proposed metric in common network architectures. Thus there does exist redundancy that can be exploited to further reduce the amount of on-chip computations. Motivated by the observation, in this paper, we proposed two types of fast and energy-efficient architectures for BNN inference. We also provide analysis and insights to pick the better strategy of these two for different datasets and network models. By reusing the results from previous computation, much cycles for data buffer access and computations can be skipped. By experiments, we demonstrate that 80% of the computation and 40% of the buffer access can be skipped by exploiting BNN similarity. Thus, our design can achieve 17% reduction in total power consumption, 54% reduction in on-chip power consumption and 2.4\u00d7 maximum speedup, compared to the baseline without applying our reuse technique. Our design also shows 1.9\u00d7 more area-efficiency compared to state-of-the-art BNN inference design. We believe our deployment of BNN on FPGA leads to a promising future of running deep learning models on mobile devices.", "venue": "FPGA", "authors": ["Cheng  Fu", "Shilin  Zhu", "Hao  Su", "Ching-En  Lee", "Jishen  Zhao"], "year": 2019, "n_citations": 13}
{"id": 2756528, "s2_id": "83faa0dcb8d054f1df2a42c844ce4161c808d0a7", "title": "SneakySnake: A Fast and Accurate Universal Genome Pre-Alignment Filter for CPUs, GPUs, and FPGAs", "abstract": "MOTIVATION\nWe introduce SneakySnake, a highly parallel and highly accurate pre-alignment filter that remarkably reduces the need for computationally costly sequence alignment. The key idea of SneakySnake is to reduce the approximate string matching (ASM) problem to the single net routing (SNR) problem in VLSI chip layout. In the SNR problem, we are interested in finding the optimal path that connects two terminals with the least routing cost on a special grid layout that contains obstacles. The SneakySnake algorithm quickly solves the SNR problem and uses the found optimal path to decide whether or not performing sequence alignment is necessary. Reducing the ASM problem into SNR also makes SneakySnake efficient to implement on CPUs, GPUs, and FPGAs.\n\n\nRESULTS\nSneakySnake significantly improves the accuracy of pre-alignment filtering by up to four orders of magnitude compared to the state-of-the-art pre-alignment filters, Shouji, GateKeeper, and SHD. For short sequences, SneakySnake accelerates Edlib (state-of-the-art implementation of Myers's bit-vector algorithm) and Parasail (state-of-the-art sequence aligner with a configurable scoring function), by up to 37.7\u00d7 and 43.9\u2009\u00d7\u2009(>12\u00d7 on average), respectively, with its CPU implementation, and by up to 413\u00d7 and 689\u2009\u00d7\u2009(>400\u00d7 on average), respectively, with FPGA and GPU acceleration. For long sequences, the CPU implementation of SneakySnake accelerates Parasail and KSW2 (sequence aligner of minimap2) by up to 979\u2009\u00d7\u2009(276.9\u00d7 on average) and 91.7\u2009\u00d7\u2009(31.7\u00d7 on average), respectively. As SneakySnake does not replace sequence alignment, users can still obtain all capabilities (e.g., configurable scoring functions) of the aligner of their choice, unlike existing acceleration efforts that sacrifice some aligner capabilities.\n\n\nAVAILABILITY\nhttps://github.com/CMU-SAFARI/SneakySnake.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data is available at Bioinformatics online.", "venue": "Bioinform.", "authors": ["Mohammed  Alser", "Taha  Shahroodi", "Juan  G\u00f3mez-Luna", "Can  Alkan", "Onur  Mutlu"], "year": 2021, "n_citations": 15}
{"id": 2756981, "s2_id": "ec4435586f94778fc32798afae3e11f3812d48ec", "title": "Cain: Automatic Code Generation for Simultaneous Convolutional Kernels on Focal-plane Sensor-processors", "abstract": "Focal-plane Sensor-processors (FPSPs) are a camera technology that enable low power, high frame rate computation, making them suitable for edge computation. Unfortunately, these devices\u2019 limited instruction sets and registers make developing complex algorithms difficult. In this work, we present Cain \u2013 a compiler that targets SCAMP-5, a general-purpose FPSP \u2013 which generates code from multiple convolutional kernels. As an example, given the convolutional kernels for an MNIST digit recognition neural network, Cain produces code that is half as long, when compared to the other available compilers for SCAMP-5.", "venue": "ArXiv", "authors": ["Edward  Stow", "Riku  Murai", "Sajad  Saeedi", "Paul H. J. Kelly"], "year": 2021, "n_citations": 0}
{"id": 2760425, "s2_id": "b866b38bec23ff6fe85436c30134c06c01bfdc9f", "title": "Hardware Efficient WiMAX Deinterleaver Capable of Address Generation for Random Interleaving Depths", "abstract": "The variation in the prescribed modulation schemes and code rates for WiMAX interleaver design, as defined by IEEE 802.16 standard, demands a plethora of hardware if all the modulation schemes and code rates have to be unified into a single electronic device. Add to this the complexities involved with the algorithms and permutations of the WiMAX standard, invariably dependent on floor function which is extremely hardware inefficient. This paper is an attempt towards removing the complexities and excess hardware involvement in the implementation of the permutations involved in Deinterleaver designs as defined by IEEE 802.16", "venue": "ArXiv", "authors": ["Omar  Rafique", "L  GangadharaiahS."], "year": 2014, "n_citations": 2}
{"id": 2760551, "s2_id": "ab0f9ac0643a5fed3cf49bc8b724541c80c5ac54", "title": "Routing in Networks on Chip with Multiplicative Circulant Topology", "abstract": "The development of multi-core processor systems is a demanded branch of science and technology. The appearance of processors with dozens and hundreds of cores poses to the developers the question of choosing the optimal topology capable to provide efficient routing in a network with a large number of nodes. In this paper, we consider the possibility of using multiplicative circulants as a topology for networks-on-chip. A specialized routing algorithm for networks with multiplicative circulant topology, taking into account topology features and having a high scalability, has been developed.", "venue": "Journal of Physics: Conference Series", "authors": ["Shchegoleva  M.A.", "Romanov  A.Yu.", "Lezhnev  E.V.", "Amerikanov  A.A"], "year": 2019, "n_citations": 0}
{"id": 2760759, "s2_id": "1c16237aa82ca05de92e1f809c11c726932917ca", "title": "Accelerating CNN inference on FPGAs: A Survey", "abstract": "Convolutional Neural Networks (CNNs) are currently adopted to solve an ever greater number of problems, ranging from speech recognition to image classification and segmentation. The large amount of processing required by CNNs calls for dedicated and tailored hardware support methods. Moreover, CNN workloads have a streaming nature, well suited to reconfigurable hardware architectures such as FPGAs. The amount and diversity of research on the subject of CNN FPGA acceleration within the last 3 years demonstrates the tremendous industrial and academic interest. This paper presents a state-of-the-art of CNN inference accelerators over FPGAs. The computational workloads, their parallelism and the involved memory accesses are analyzed. At the level of neurons, optimizations of the convolutional and fully connected layers are explained and the performances of the different methods compared. At the network level, approximate computing and datapath optimization methods are covered and state-of-the-art approaches compared. The methods and tools investigated in this survey represent the recent trends in FPGA CNN inference accelerators and will fuel the future advances on effcient hardware deep learning.", "venue": "ArXiv", "authors": ["Kamel  Abdelouahab", "Maxime  Pelcat", "Jocelyn  S\u00e9rot", "Fran\u00e7ois  Berry"], "year": 2018, "n_citations": 84}
{"id": 2761519, "s2_id": "295c539f6ef049b5b9ee4cf1913066b4e8ec3883", "title": "Evaluating Row Buffer Locality in Future Non-Volatile Main Memories", "abstract": "DRAM-based main memories have read operations that destroy the read data, and as a result, must buffer large amounts of data on each array access to keep chip costs low. Unfortunately, system-level trends such as increased memory contention in multi-core architectures and data mapping schemes that improve memory parallelism may cause only a small amount of the buffered data to be accessed. This makes buffering large amounts of data on every memory array access energy-inefficient. Emerging non-volatile memories (NVMs) such as PCM, STT-RAM, and RRAM, however, do not have destructive read operations, opening up opportunities for employing small row buffers without incurring additional area penalty and/or design complexity. In this work, we discuss architectural changes to enable small row buffers at a low cost in NVMs. We provide a memory access protocol, energy model, and timing model to enable further system-level evaluation. We evaluate the system-level tradeoffs of employing different row buffer sizes in NVM main memories in terms of energy, performance, and endurance, with different data mapping schemes. We find that on a multi-core CMP system, reducing the row buffer size can greatly reduce main memory dynamic energy compared to a DRAM baseline with large row sizes, without greatly affecting endurance, and for some memories, leads to improved performance.", "venue": "ArXiv", "authors": ["Justin  Meza", "Jing  Li", "Onur  Mutlu"], "year": 2018, "n_citations": 22}
{"id": 2762706, "s2_id": "6439b90b637ab62f309e066b3420e8e7f2eb8776", "title": "Fast and Scalable Computation of the Forward and Inverse Discrete Periodic Radon Transform", "abstract": "The discrete periodic radon transform (DPRT) has extensively been used in applications that involve image reconstructions from projections. Beyond classic applications, the DPRT can also be used to compute fast convolutions that avoids the use of floating-point arithmetic associated with the use of the fast Fourier transform. Unfortunately, the use of the DPRT has been limited by the need to compute a large number of additions and the need for a large number of memory accesses. This paper introduces a fast and scalable approach for computing the forward and inverse DPRT that is based on the use of: a parallel array of fixed-point adder trees; circular shift registers to remove the need for accessing external memory components when selecting the input data for the adder trees; an image block-based approach to DPRT computation that can fit the proposed architecture to available resources; and fast transpositions that are computed in one or a few clock cycles that do not depend on the size of the input image. As a result, for an N \u00d7 N image (N prime), the proposed approach can compute up to N2 additions per clock cycle. Compared with the previous approaches, the scalable approach provides the fastest known implementations for different amounts of computational resources. For example, for a 251\u00d7251 image, for approximately 25% fewer flip-flops than required for a systolic implementation, we have that the scalable DPRT is computed 36 times faster. For the fastest case, we introduce optimized just 2N + \u2308log2 N\u2309 + 1 and 2N + 3 \u2308log2 N\u2309 + B + 2 cycles, architectures that can compute the DPRT and its inverse in respectively, where B is the number of bits used to represent each input pixel. On the other hand, the scalable DPRT approach requires more 1-b additions than for the systolic implementation and provides a tradeoff between speed and additional 1-b additions. All of the proposed DPRT architectures were implemented in VHSIC Hardware Description Language (VHDL) and validated using an Field-Programmable Gate Array (FPGA) implementation.", "venue": "IEEE Transactions on Image Processing", "authors": ["Cesar  Carranza", "Daniel  Llamocca", "Marios S. Pattichis"], "year": 2016, "n_citations": 16}
{"id": 2766418, "s2_id": "a5db4d248176995539010fdf2ac6d0f378b3bd2f", "title": "The implementation of a Deep Recurrent Neural Network Language Model on a Xilinx FPGA", "abstract": "Recently, FPGA has been increasingly applied to problems such as speech recognition, machine learning, and cloud computation such as the Bing search engine used by Microsoft. This is due to FPGAs great parallel computation capacity as well as low power consumption compared to general purpose processors. However, these applications mainly focus on large scale FPGA clusters which have an extreme processing power for executing massive matrix or convolution operations but are unsuitable for portable or mobile applications. This paper describes research on single-FPGA platform to explore the applications of FPGAs in these fields. In this project, we design a Deep Recurrent Neural Network (DRNN) Language Model (LM) and implement a hardware accelerator with AXI Stream interface on a PYNQ board which is equipped with a XILINX ZYNQ SOC XC7Z020 1CLG400C. The PYNQ has not only abundant programmable logic resources but also a flexible embedded operation system, which makes it suitable to be applied in the natural language processing field. We design the DRNN language model with Python and Theano, train the model on a CPU platform, and deploy the model on a PYNQ board to validate the model with Jupyter notebook. Meanwhile, we design the hardware accelerator with Overlay, which is a kind of hardware library on PYNQ, and verify the acceleration effect on the PYNQ board. Finally, we have found that the DRNN language model can be deployed on the embedded system smoothly and the Overlay accelerator with AXI Stream interface performs at 20 GOPS processing throughput, which constitutes a 70.5X and 2.75X speed up compared to the work in Ref.30 and Ref.31 respectively.", "venue": "ArXiv", "authors": ["Yufeng  Hao", "Steven  Quigley"], "year": 2017, "n_citations": 10}
{"id": 2766644, "s2_id": "9f1de5dfebc90836e45e2324299520466dc035ea", "title": "An ECG-on-Chip for Wearable Cardiac Monitoring Devices", "abstract": "This paper describes a highly integrated, low power chip solution for ECG signal processing in wearable devices. The chip contains an instrumentation amplifier with programmable gain, a band-pass filter, a 12-bit SAR ADC, a novel QRS detector, 8K on-chip SRAM, and relevant control circuitry and CPU interfaces. The analog front end circuits accurately senses and digitizes the raw ECG signal, which is then filtered to extract the QRS. The sampling frequency used is 256 Hz. ECG samples are buffered locally on an asynchronous FIFO and is read out using a faster clock, as and when it is required by the host CPU via an SPI interface. The chip was designed and implemented in 0.35\u00b5m standard CMOS process. The analog core operates at 1V while the digital circuits and SRAM operate at 3.3V. The chip total core area is 5.74 mm2 and consumes 9.6\u00b5W. Small size and low power consumption make this design suitable for usage in wearable heart monitoring devices.", "venue": "2010 Fifth IEEE International Symposium on Electronic Design, Test & Applications", "authors": ["Chacko John Deepu", "Xiaoyuan  Xu", "Xiaodan  Zou", "Libin  Yao", "Yong  Lian"], "year": 2010, "n_citations": 40}
{"id": 2771434, "s2_id": "bc11732a7dded597e7ad296c8c01f0d19e5063e4", "title": "Hardware/Software Codesign for Training/Testing Multiple Neural Networks on Multiple FPGAs", "abstract": "Most neural network designs for FPGAs are inflexible. In this paper, we propose a flexible VHDL structure that would allow any neural network to be implemented on multiple FPGAs. Moreover, the VHDL structure allows for testing as well as training multiple neural networks. The VHDL design consists of multiple processor groups. There are two types of processor groups: Mini Vector Machine Processor Group and Activation Processor Group. Each processor group consists of individual Mini Vector Machines and Activation Processor. The Mini Vector Machines apply vector operations to the data, while the Activation Processors apply activation functions to the data. A ring buffer was implemented to connect the various processor groups.", "venue": "ArXiv", "authors": ["Brosnan  Yuen"], "year": 2019, "n_citations": 0}
{"id": 2773589, "s2_id": "06771d4e264337b8483c3a963ddf26610f78a2de", "title": "RAPPER: Ransomware Prevention via Performance Counters", "abstract": "Ransomware can produce direct and controllable economic loss, which makes it one of the most prominent threats in cyber security. As per the latest statistics, more than half of malwares reported in Q1 of 2017 are ransomware and there is a potent threat of a novice cybercriminals accessing rasomware-as-a-service. The concept of public-key based data kidnapping and subsequent extortion was introduced in 1996. Since then, variants of ransomware emerged with different cryptosystems and larger key sizes though, the underlying techniques remained same. Though there are works in literature which proposes a generic framework to detect the crypto ransomwares, we present a two step unsupervised detection tool which when suspects a process activity to be malicious, issues an alarm for further analysis to be carried in the second step and detects it with minimal traces. The two step detection framework- RAPPER uses Artificial Neural Network and Fast Fourier Transformation to develop a highly accurate, fast and reliable solution to ransomware detection using minimal trace points.", "venue": "ArXiv", "authors": ["Manaar  Alam", "Sarani  Bhattacharya", "Debdeep  Mukhopadhyay", "Anupam  Chattopadhyay"], "year": 2018, "n_citations": 30}
{"id": 2777632, "s2_id": "73106d4cded22af88c9d7990c36a98bc734f52ec", "title": "In-memory Multi-valued Associative Processor", "abstract": "In-memory associative processor architectures are offered as a great candidate to overcome memory-wall bottleneck and to enable vector/parallel arithmetic operations. In this paper, we extend the functionality of the associative processor to multi-valued arithmetic. To allow for in-memory compute implementation of arithmetic or logic functions, we propose a structured methodology enabling the automatic generation of the corresponding look-up tables (LUTs). We propose two approaches to build the LUTs: a first approach that formalizes the intuition behind LUT pass ordering and a more optimized approach that reduces the number of required write cycles. To demonstrate these methodologies, we present a novel ternary associative processor (TAP) architecture that is employed to implement efficient ternary vector in-place addition. A SPICEMATLAB co-simulator is implemented to test the functionality of the TAP and to evaluate the performance of the proposed AP ternary in-place adder implementations in terms of energy, delay, and area. Results show that compared to the binary AP adder, the ternary AP adder results in a 12.25% and 6.2% reduction in energy and area, respectively. The ternary AP also demonstrates a 52.64% reduction in energy and a delay that is up to 9.5x smaller when compared to a state-of-art ternary carry-lookahead adder.", "venue": "ArXiv", "authors": ["Mira  Hout", "Mohammed E. Fouda", "Rouwaida  Kanj", "Ahmed M. Eltawil"], "year": 2021, "n_citations": 0}
{"id": 2781101, "s2_id": "61b08bdd07e71a82f9cf5f7066e61cee393dc8c6", "title": "The Anatomy of the Grid: Enabling Scalable Virtual Organizations", "abstract": "The term \"the Grid\" was coined in the mid-1990s to denote a proposed distributed computing infrastructure for advanced science and engineering [4]. Considerable progress has since been made on the construction of such an infrastructure (e.g., [1,6,7]) but the term \"Grid\" has also been conflated, at least in popular perception, to embrace everything from advanced networking to artificial intelligence. One might wonder whether the term has any real substance and meaning. Is there really a distinct \"Grid problem\" and hence a need for new \"Grid technologies\"? If so, what is the nature of these technologies, and what is their domain of applicability? While numerous groups have interest in Grid concepts and share, to a significant extent, a common vision of Grid architecture, we do not see consensus on the answers to these questions.", "venue": "Euro-Par", "authors": ["Ian T. Foster"], "year": 2001, "n_citations": 142}
{"id": 2781178, "s2_id": "f0a1b25605edfbbdf6dc2575de9d5f1f19b86eaf", "title": "A 97mW 110MS/s 12b Pipeline ADC Implemented in 0.18mum Digital CMOS", "abstract": "A 12 bit Pipeline ADC fabricated in a 0.18 $\\mu$m pure digital CMOS technology is presented. Its nominal conversion rate is 110MS/s and the nominal supply voltage is 1.8V. The effective number of bits is 10.4 when a 10MHz input signal with 2V_{P-P} signal swing is applied. The occupied silicon area is 0.86mm^2 and the power consumption equals 97mW. A switched capacitor bias current circuit scale the bias current automatically with the conversion rate, which gives scaleable power consumption and full performance of the ADC from 20 to 140MS/s.", "venue": "DATE", "authors": ["Terje N. Andersen", "Atle  Briskemyr", "Frode  Telst\u00f8", "Johnny  Bj\u00f8rnsen", "Thomas E. Bonnerud", "Bj\u00f8rnar  Hernes", "\u00d8ystein  Moldsvor"], "year": 2004, "n_citations": 6}
{"id": 2785047, "s2_id": "1cb0257a617de318ca168f8b58ab50f505b4d274", "title": "An Application-Specific Design Methodology for STbus Crossbar Generation", "abstract": "As the communication requirements of current and future Multiprocessor Systems on Chips (MPSoCs) continue to increase, scalable communication architectures are needed to support the heavy communication demands of the system. This is reflected in the recent trend that many of the standard bus products such as STbus, have now introduced the capability of designing a crossbar with multiple buses operating in parallel. The crossbar configuration should be designed to closely match the application traffic characteristics and performance requirements. In this work we address this issue of application-specific design of optimal crossbar (using STbus crossbar architecture), satisfying the performance requirements of the application and optimal binding of cores onto the crossbar resources. We present a simulation based design approach that is based on analysis of actual traffic trace of the application, considering local variations in traffic rates, temporal overlap among traffic streams and criticality of traffic streams. Our methodology is applied to several MPSoC designs and the resulting crossbar platforms are validated for performance by cycle-accurate SystemC simulation of the designs. The experimental case studies show large reduction in packet latencies (up to 7\u00d7) and large crossbar component savings (up to 3.5\u00d7) compared to traditional design approaches.", "venue": "Design, Automation and Test in Europe", "authors": ["Srinivasan  Murali", "Giovanni De Micheli"], "year": 2005, "n_citations": 85}
{"id": 2785463, "s2_id": "cf3dc0442a3c1fead4516d60f0b81de8b6bdfa0b", "title": "Energy Efficient and High Performance Current-Mode Neural Network Circuit using Memristors and Digitally Assisted Analog CMOS Neurons", "abstract": "Emerging nano-scale programmable Resistive-RAM (RRAM) has been identified as a promising technology for implementing brain-inspired computing hardware. Several neural network architectures, that essentially involve computation of scalar products between input data vectors and stored network weights can be efficiently implemented using high density cross-bar arrays of RRAM integrated with CMOS. In such a design, the CMOS interface may be responsible for providing input excitations and for processing the RRAM output. In order to achieve high energy efficiency along with high integration density in RRAM based neuromorphic hardware, the design of RRAM-CMOS interface can therefore play a major role. In this work we propose design of high performance, current mode CMOS interface for RRAM based neural network design. The use of current mode excitation for input interface and design of digitally assisted current-mode CMOS neuron circuit for the output interface is presented. The proposed technique achieve 10x energy as well as performance improvement over conventional approaches employed in literature. Network level simulations show that the proposed scheme can achieve 2 orders of magnitude lower energy dissipation as compared to a digital ASIC implementation of a feed-forward neural network.", "venue": "ArXiv", "authors": ["Aranya  Goswamy", "Sagar  Kumashi", "Vikash  Sehwag", "Siddharth  Singh", "Manny  Jain", "Kaushik  Roy", "Mrigank  Sharad"], "year": 2015, "n_citations": 2}
{"id": 2788251, "s2_id": "b10e6ec0f30e1982b3215ef3de223c295750ba21", "title": "Architecting Non-Volatile Main Memory to Guard Against Persistence-based Attacks", "abstract": "DRAM-based main memory and its associated components increasingly account for a significant portion of application performance bottlenecks and power budget demands inside the computing ecosystem. To alleviate the problems of storage density and power constraints associated with DRAM, system architects are investigating alternative non-volatile memory technologies such as Phase Change Memory (PCM) to either replace or be used alongside DRAM memory. While such alternative memory types offer many promises to overcome the DRAM-related issues, they present a significant security threat to the users due to persistence of memory data even after power down. \nIn this paper, we investigate smart mechanisms to obscure the data left in non-volatile memory after power down. In particular, we analyze the effect of using a single encryption algorithm versus differentiated encryption based on the security needs of the application phases. We also explore the effect of encryption on a hybrid main memory that has a DRAM buffer cache plus PCM main memory. Our mechanism takes into account the limited write endurance problem associated with several non-volatile memory technologies including PCM, and avoids any additional writes beyond those originally issued by the applications. We evaluate using Gem5 simulator and SPEC 2006 applications, and show the performance and power overheads of our proposed design.", "venue": "ArXiv", "authors": ["Fan  Yao", "Guru  Venkataramani"], "year": 2019, "n_citations": 1}
{"id": 2788263, "s2_id": "86466441fb2d63750fac0d63e33758fd76753098", "title": "iELAS: An ELAS-Based Energy-Efficient Accelerator for Real-Time Stereo Matching on FPGA Platform", "abstract": "Stereo matching is a critical task for robot navigation and autonomous vehicles, providing the depth estimation of surroundings. Among all stereo matching algorithms, Efficient Large-scale Stereo (ELAS) offers one of the best tradeoffs between efficiency and accuracy. However, due to the inherent iterative process and unpredictable memory access pattern, ELAS can only run at 1.5-3 fps on high-end CPUs and difficult to achieve real-time performance on low-power platforms. In this paper, we propose an energy-efficient architecture for real-time ELAS-based stereo matching on FPGA platform. Moreover, the original computational-intensive and irregular triangulation module is reformed in a regular manner with points interpolation, which is much more hardware-friendly. optimizations, including memory management, parallelism, and pipelining, are further utilized to reduce memory footprint and improve throughput. Compared with Intel i7 CPU and the state-of-the-art $\\mathrm{C}\\mathrm{P}\\mathrm{U}+$FPGA implementation, our FPGA realization achieves up to $ 38.4\\times$ and $ 3.32\\times$ frame rate improvement, and up to $ 27.1\\times$ and $ 1.13\\times$ energy efficiency improvement, respectively.", "venue": "2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)", "authors": ["Tian  Gao", "Zishen  Wan", "Yuyang  Zhang", "Bo  Yu", "Yanjun  Zhang", "Shaoshan  Liu", "Arijit  Raychowdhury"], "year": 2021, "n_citations": 5}
{"id": 2789522, "s2_id": "fe8dec68ed7ddaf2357cc98bead68312b8f1a061", "title": "NEURAghe: Exploiting CPU-FPGA Synergies for Efficient and Flexible CNN Inference Acceleration on Zynq SoCs", "abstract": "Deep convolutional neural networks (CNNs) obtain outstanding results in tasks that require human-level understanding of data, like image or speech recognition. However, their computational load is significant, motivating the development of CNN-specialized accelerators. This work presents NEURAghe, a flexible and efficient hardware/software solution for the acceleration of CNNs on Zynq SoCs. NEURAghe leverages the synergistic usage of Zynq ARM cores and of a powerful and flexible Convolution-Specific Processor deployed on the reconfigurable logic. The Convolution-Specific Processor embeds both a convolution engine and a programmable soft core, releasing the ARM processors from most of the supervision duties and allowing the accelerator to be controlled by software at an ultra-fine granularity. This methodology opens the way for cooperative heterogeneous computing: While the accelerator takes care of the bulk of the CNN workload, the ARM cores can seamlessly execute hard-to-accelerate parts of the computational graph, taking advantage of the NEON vector engines to further speed up computation. Through the companion NeuDNN SW stack, NEURAghe supports end-to-end CNN-based classification with a peak performance of 169GOps/s, and an energy efficiency of 17GOps/W. Thanks to our heterogeneous computing model, our platform improves upon the state-of-the-art, achieving a frame rate of 5.5 frames per second (fps) on the end-to-end execution of VGG-16 and 6.6fps on ResNet-18.", "venue": "ACM Trans. Reconfigurable Technol. Syst.", "authors": ["Paolo  Meloni", "Alessandro  Capotondi", "Gianfranco  Deriu", "Michele  Brian", "Francesco  Conti", "Davide  Rossi", "Luigi  Raffo", "Luca  Benini"], "year": 2018, "n_citations": 42}
{"id": 2791747, "s2_id": "c10ac1da715dedcdff94a53f3d54b94d8b69abdd", "title": "Improving the Performance of a NoC-based CNN Accelerator with Gather Support", "abstract": "The increasing application of deep learning technology drives the need for an efficient parallel computing architecture for Convolutional Neural Networks (CNNs). A significant challenge faced when designing a many-core CNN accelerator is to handle the data movement between the processing elements. The CNN workload introduces many-to-one traffic in addition to one-to-one and one-to-many traffic. As the de-facto standard for on-chip communication, Network-on-Chip (NoC) can support various unicast and multicast traffic. For many-to-one traffic, repetitive unicast is employed which is not an efficient way. In this paper, we propose to use the gather packet on mesh-based NoCs employing output stationary systolic array in support of many-to-one traffic. The gather packet will collect the data from the intermediate nodes eventually leading to the destination efficiently. This method is evaluated using the traffic traces generated from the convolution layer of AlexNet and VGG-16 with improvement in the latency and power than the repetitive unicast method.", "venue": "2020 IEEE 33rd International System-on-Chip Conference (SOCC)", "authors": ["Binayak  Tiwari", "Mei  Yang", "Xiaohang  Wang", "Yingtao  Jiang", "Venkatesan  Muthukumar"], "year": 2020, "n_citations": 1}
{"id": 2793224, "s2_id": "4f48a1c728cbb621206bb39770144dcbadb4aa28", "title": "Holistic Hardware Security Assessment Framework: A Microarchitectural Perspective", "abstract": "Our goal is to enable holistic hardware security evaluation from the microarchitectural point of view. To achieve this, we propose a framework that categorizes threat models based on the microarchitectural components being targeted, and provides a generic security metric that can be used to assess the vulnerability of components, as well as the system as a whole.", "venue": "ArXiv", "authors": ["Tochukwu  Idika", "Ismail  Akturk"], "year": 2021, "n_citations": 0}
{"id": 2793511, "s2_id": "7a2c7fee4f8f036e94c4346104c1a5ed509a31ab", "title": "Metal-Gated Junctionless Nanowire Transistors", "abstract": "Junctionless Nanowire Field-Effect Transistors (JNFETs), where the channel region is uniformly doped without the need for source-channel and drain-channel junctions or lateral doping abruptness, are considered an attractive alternative to conventional CMOS FETs. Previous theoretical and experimental works [1][2] on JNFETs have considered polysilicon gates and silicon-dioxide dielectric. However, with further scaling, JNFETs will suffer from deleterious effects of doped polysilicon such as high resistance, additional capacitance due to gate-oxide interface depletion, and incompatibility with high-k dielectrics[3][4]. In this paper, novel metal- gated high-k JNFETs are investigated through detailed process and device simulations. These MJNFETs are also ideally suited for new types of nano-architectures such as N3ASICs [5] which utilize regular nanowire arrays with limited customization. In such nano- systems, the simplified device geometry in conjunction with a single-type FET circuit style [6] would imply that logic arrays could be patterned out of pre-doped SOI wafers without the need for any additional ion implantation.", "venue": "ArXiv", "authors": ["Mostafizur  Rahman", "Pritish  Narayanan", "Csaba Andras Moritz"], "year": 2014, "n_citations": 1}
{"id": 2796368, "s2_id": "019298652531a16e90515ee5fbc9b7b0c2c98026", "title": "In-memory Implementation of On-chip Trainable and Scalable ANN for AI/ML Applications", "abstract": "Traditional von Neumann architecture based processors become inefficient in terms of energy and throughput as they involve separate processing and memory units, also known as~\\textit{memory wall}. The memory wall problem is further exacerbated when massive parallelism and frequent data movement are required between processing and memory units for real-time implementation of artificial neural network (ANN) that enables many intelligent applications. One of the most promising approach to address the memory wall problem is to carry out computations inside the memory core itself that enhances the memory bandwidth and energy efficiency for extensive computations. This paper presents an in-memory computing architecture for ANN enabling artificial intelligence (AI) and machine learning (ML) applications. The proposed architecture utilizes deep in-memory architecture based on standard six transistor (6T) static random access memory (SRAM) core for the implementation of a multi-layered perceptron. Our novel on-chip training and inference in-memory architecture reduces energy cost and enhances throughput by simultaneously accessing the multiple rows of SRAM array per precharge cycle and eliminating the frequent access of data. The proposed architecture realizes backpropagation which is the keystone during the network training using newly proposed different building blocks such as weight updation, analog multiplication, error calculation, signed analog to digital conversion, and other necessary signal control units. The proposed architecture was trained and tested on the IRIS dataset which exhibits $\\approx46\\times$ more energy efficient per MAC (multiply and accumulate) operation compared to earlier classifiers.", "venue": "ArXiv", "authors": ["Abhash  Kumar", "Jawar  Singh", "Sai Manohar Beeraka", "Bharat  Gupta"], "year": 2020, "n_citations": 0}
{"id": 2799134, "s2_id": "4216ab6439f94248b25a284a40dcf273ff7e6454", "title": "Multi Core SSL/TLS Security Processor Architecture Prototype Design with automated Preferential Algorithm in FPGA", "abstract": "In this paper a pipelined architecture of a high speed network security processor (NSP) for SSL/TLS protocol is implemented on a system on chip (SOC) where hardware information of all encryption, hashing and key exchange algorithms are stored in flash memory in terms of bit files, in contrary to related works where all are actually implemented in hardware. The NSP finds applications in e-commerce, virtual private network (VPN) and in other fields that require data confidentiality. The mot ivation of the present work is to dynamically execute applications with stipulated throughput within budgeted hardware resource and power. A preferential algorithm choosing an appropriate cipher suite is proposed, which is based on Effi cient System Index (ESI) budget comprising of power, throughput and resource given by the user. The bit files of the chosen security algorithms are downlo aded from the flash memory to the partial region of field programmable ga te array (FPGA). The proposed SOC controls data communication between an application running in a system through a PCI and the Ethernet interface of a network. Partial configuration fe ature is used in ISE14.4 suite with ZYNQ 7z020-clg484 FPGA platform. The performances of the implemented crypto algorithms are considerably better than the existing works reported in lit eratures. c 2014 Manuscript in Elsevier Ltd format.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Amlan  Chakrabarti", "Ranjan  Ghosh"], "year": 2014, "n_citations": 0}
{"id": 2799538, "s2_id": "b5d3c7818ae5de168d7eaca67f7fd50541aa1eea", "title": "Efficient Floating-Point Givens Rotation Unit", "abstract": "High-throughput QR decomposition is a key operation in many advanced signal processing and communication applications. For some of these applications, using floating-point computation is becoming almost compulsory. However, there are scarce works in hardware implementations of floating-point QR decomposition for embedded systems. In this paper, we propose a very efficient high-throughput floating-point Givens rotation unit for QR decomposition. Moreover, the initial proposed design for conventional number formats is enhanced by using the new Half-Unit Biased format. The provided error analysis shows the effectiveness of our proposals and the trade-off of different implementation parameters. FPGA implementation results are also presented and a thorough comparison between both approaches. These implementation results also reveal outstanding improvements compared to other previous similar designs in terms of area, latency, and throughput.", "venue": "Circuits Syst. Signal Process.", "authors": ["Javier  Hormigo", "Sergio D. Munoz"], "year": 2021, "n_citations": 1}
{"id": 2811448, "s2_id": "2384c8b981e9354c426e950d08ff8d1665aabe32", "title": "Low-Complexity Scaling Methodsfor DCT-II Approximations", "abstract": "This paper introduces a collection of scaling methods for generating <inline-formula><tex-math notation=\"LaTeX\">$\\text{2}\\,N$</tex-math></inline-formula>-point DCT-II approximations based on <inline-formula><tex-math notation=\"LaTeX\">$N$</tex-math></inline-formula>-point low-complexity transformations. Such scaling is based on the Hou recursive matrix factorization of the exact <inline-formula><tex-math notation=\"LaTeX\">$\\text{2}\\,N$</tex-math></inline-formula>-point DCT-II matrix. Encompassing the widely employed Jridi-Alfalou-Meher scaling method, the proposed techniques are shown to produce DCT-II approximations that outperform the transforms resulting from the JAM scaling method according to total error energy and mean squared error. Orthogonality conditions are derived and an extensive error analysis based on statistical simulation demonstrates the good performance of the introduced scaling methods. A hardware implementation is also provided demonstrating the competitiveness of the proposed methods when compared to the JAM scaling method.", "venue": "IEEE Transactions on Signal Processing", "authors": ["Diego F. G. Coelho", "Renato J. Cintra", "Arjuna  Madanayake", "Sirani M. Perera"], "year": 2021, "n_citations": 1}
{"id": 2814199, "s2_id": "5d6e055cb289e047c97e7023bffb320f07bb7aa8", "title": "A Technique for Write-endurance aware Management of Resistive RAM Last Level Caches", "abstract": "Due to increasing cache sizes and large leakage consumption of SRAM device, conventional SRAM caches contribute significantly to the processor power consumption. Recently researchers have used non-volatile memory devices to design caches, since they provide high density, comparable read latency and low leakage power dissipation. However, their high write latency may increase the execution time and hence, leakage energy consumption. Also, since their write endurance is small, a conventional energy saving technique may further aggravate the problem of write-variations, thus reducing their lifetime. In this paper, we present a cache energy saving technique for non-volatile caches, which also attempts to improve their lifetime by making writes equally distributed to the cache. Our technique uses dynamic cache reconfiguration to adjust the cache size to meet program requirement and turns off the remaining cache to save energy. Microarchitectural simulations performed using an x86-64 simulator, SPEC2006 benchmarks and a resistive-RAM LLC (last level cache) show that over an 8MB baseline cache, our technique saves 17.55% memory subsystem (last level cache + main memory) energy and improves the lifetime by 1.33X. Over the same resistive-RAM baseline, an SRAM of similar area with no cache reconfiguration leads to an energy loss of 186.13%.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 0}
{"id": 2814763, "s2_id": "f1dc2b5b542e2bf98d521167837a9388fdaf9d1a", "title": "Dataflow-Architecture Co-Design for 2.5D DNN Accelerators using Wireless Network-on-Package", "abstract": "Deep neural network (DNN) models continue to grow in size and complexity, demanding higher computational power to enable real-time inference. To efficiently deliver such computational demands, hardware accelerators are being developed and deployed across scales. This naturally requires an efficient scale-out mechanism for increasing compute density as required by the application. 2.5D integration over interposer has emerged as a promising solution, but as we show in this work, the limited interposer bandwidth and multiple hops in the Network-on-Package (NoP) can diminish the benefits of the approach. To cope with this challenge, we propose WIENNA, a wireless NoP-based 2.5D DNN accelerator. In WIENNA, the wireless NoP connects an array of DNN accelerator chiplets to the global buffer chiplet, providing high-bandwidth multicasting capabilities. Here, we also identify the dataflow style that most efficiency exploits the wireless NoP\u2019s high-bandwidth multicasting capability on each layer. With modest area and power overheads, WIENNA achieves 2.2X-5.1X higher throughput and 38.2% lower energy than an interposer-based NoP design.", "venue": "2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Robert  Guirado", "Hyoukjun  Kwon", "Sergi  Abadal", "Eduard  Alarc'on", "Tushar  Krishna"], "year": 2021, "n_citations": 0}
{"id": 2822100, "s2_id": "f03ddae1dd5f89df1deca4356bdf74f9ed26f200", "title": "User-aware power management for mobile devices", "abstract": "The power management techniques to extend battery lifespan is becoming increasingly important due to longer user applications' running time in mobile devices. Even when users do not use any applications, battery lifespan decreases continually. It occurs because of service daemons of mobile platform and network-based data synchronization operations. In this paper, we propose a new power management system that recognizes the idle time of the device to reduce the battery consumption of mobile devices.", "venue": "2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE)", "authors": ["Geunsik  Lim", "Changwoo  Min", "Donghyun  Kang", "Young Ik Eom"], "year": 2013, "n_citations": 5}
{"id": 2822316, "s2_id": "f3d0524e9413c675694aa429c1f803a48ec8f928", "title": "PieceTimer: A holistic timing analysis framework considering setup/hold time interdependency using a piecewise model", "abstract": "In static timing analysis, clock-to-q delays of flip-flops are considered as constants. Setup times and hold times are characterized separately and also used as constants. The characterized delays, setup times and hold times, are applied in timing analysis independently to verify the performance of circuits. In reality, however, clock-to-q delays of flip-flops depend on both setup and hold times. Instead of being constants, these delays change with respect to different setup/hold time combinations. Consequently, the simple abstraction of setup/hold times and constant clock-to-q delays introduces inaccuracy in timing analysis. In this paper, we propose a holistic method to consider the relation between clock-to-q delays and setup/hold time combinations with a piecewise linear model. The result is more accurate than that of traditional timing analysis, and the incorporation of the interdependency between clock-to-q delays, setup times and hold times may also improve circuit performance.", "venue": "2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Grace Li Zhang", "Bing  Li", "Ulf  Schlichtmann"], "year": 2016, "n_citations": 15}
{"id": 2824070, "s2_id": "2e50675439a2bf21e5f2b80be26af4b66d2d8fce", "title": "A Survey on RISC-V Security: Hardware and Architecture", "abstract": "The Internet of Things (IoT) is an ongoing technological revolution. Embedded processors are the processing engines of smart IoT devices. For decades, these processors were mainly based on the Arm instruction set architecture (ISA). In recent years, the free and open RISC-V ISA standard has attracted the attention of industry and academia and is becoming the mainstream. Many companies have already owned or are designing RISC-V processors. Many important operating systems and major tool chains have supported RISC-V. Data security and user privacy protection are common challenges faced by all IoT devices. In order to deal with foreseeable security threats, the RISC-V community is studying security solutions aimed at achieving a root of trust (RoT) and ensuring that sensitive information on RISC-V devices is not tampered with or leaked. Many RISC-V security research projects are underway, but the academic community has not yet conducted a comprehensive survey of RISC-V security solutions. The latest technology and future development direction of RICS-V security research are still unclear. In order to fill this research gap, this paper presents an in-depth survey on RISC-V security technologies. This paper summarizes the representative security mechanisms of RISC-V hardware and architecture. Specifically, we first briefly introduce the background and development status of RISC-V, and compare the RISC-V mechanisms with the most relevant Arm mechanisms, highlighting their similarities and differences. Then, we investigate the security research of RISC-V around the theme of hardware and architecture security. Our survey covers hardware and physical access security, hardware-assisted security units, ISA security extensions, memory protection, cryptographic primitives, and side-channel attack protection. Based on our survey, we predict the future research and development directions of RISC-V security. We hope that our research can inspire RISC-V researchers and developers.", "venue": "ArXiv", "authors": ["Tao  Lu"], "year": 2021, "n_citations": 0}
{"id": 2827694, "s2_id": "be4595013d2fe85140b5cb465da9bd53ffe9a949", "title": "Mitigating Power Attacks through Fine-Grained Instruction Reordering", "abstract": "Side-channel attacks are a security exploit that take advantage of information leakage. They use measurement and analysis of physical parameters to reverse engineer and extract secrets from a system. Power analysis attacks in particular, collect a set of power traces from a computing device and use statistical techniques to correlate this information with the attacked application data and source code. Countermeasures like just-in-time compilation, random code injection and instruction descheduling obfuscate the execution of instructions to reduce the security risk. Unfortunately, due to the randomness and excess instructions executed by these solutions, they introduce large overheads in performance, power and area. In this work we propose a scheduling algorithm that dynamically reorders instructions in an out-of-order processor to provide obfuscated execution and mitigate power analysis attacks with little-to-no effect on the performance, power or area of the processor. We exploit the time between operand availability of critical instructions (slack) to create high-performance random schedules without requiring additional instructions or static prescheduling. Further, we perform an extended security analysis using different attacks. We highlight the dangers of using incorrect adversarial assumptions, which can often lead to a false sense of security. In that regard, our advanced security metric demonstrates improvements of 34\u00d7, while our basic security evaluation shows results up to 261\u00d7. Moreover, our system achieves performance within 96% on average, of the baseline unprotected processor.", "venue": "ArXiv", "authors": ["Yun  Chen", "Ali  Hajiabadi", "Romain  Poussier", "Andreas  Diavastos", "Shivam  Bhasin", "Trevor E. Carlson"], "year": 2021, "n_citations": 0}
{"id": 2829940, "s2_id": "a5bd15d203c6aa740aba16776b422db010e66b58", "title": "Buddy-RAM: Improving the Performance and Efficiency of Bulk Bitwise Operations Using DRAM", "abstract": "Bitwise operations are an important component of modern day programming. Many widely-used data structures (e.g., bitmap indices in databases) rely on fast bitwise operations on large bit vectors to achieve high performance. Unfortunately, in existing systems, regardless of the underlying architecture (e.g., CPU, GPU, FPGA), the throughput of such bulk bitwise operations is limited by the available memory bandwidth. \nWe propose Buddy, a new mechanism that exploits the analog operation of DRAM to perform bulk bitwise operations completely inside the DRAM chip. Buddy consists of two components. First, simultaneous activation of three DRAM rows that are connected to the same set of sense amplifiers enables us to perform bitwise AND and OR operations. Second, the inverters present in each sense amplifier enables us to perform bitwise NOT operations, with modest changes to the DRAM array. These two components make Buddy functionally complete. Our implementation of Buddy largely exploits the existing DRAM structure and interface, and incurs low overhead (1% of DRAM chip area). \nOur evaluations based on SPICE simulations show that, across seven commonly-used bitwise operations, Buddy provides between 10.9X---25.6X improvement in raw throughput and 25.1X---59.5X reduction in energy consumption. We evaluate three real-world data-intensive applications that exploit bitwise operations: 1) bitmap indices, 2) BitWeaving, and 3) bitvector-based implementation of sets. Our evaluations show that Buddy significantly outperforms the state-of-the-art.", "venue": "ArXiv", "authors": ["Vivek  Seshadri", "Donghyuk  Lee", "Thomas  Mullins", "Hasan  Hassan", "Amirali  Boroumand", "Jeremie  Kim", "Michael A. Kozuch", "Onur  Mutlu", "Phillip B. Gibbons", "Todd C. Mowry"], "year": 2016, "n_citations": 39}
{"id": 2831749, "s2_id": "5d5f818b0abc2f26f0253e4d7c3325cf5aa59460", "title": "ApproxFPGAs: Embracing ASIC-Based Approximate Arithmetic Components for FPGA-Based Systems", "abstract": "There has been abundant research on the development of Approximate Circuits (ACs) for ASICs. However, previous studies have illustrated that ASIC-based ACs offer asymmetrical gains in FPGA-based accelerators. Therefore, an AC that might be pareto-optimal for ASICs might not be pareto-optimal for FPGAs. In this work, we present the ApproxFPGAs methodology that uses machine learning models to reduce the exploration time for analyzing the state-of-the-art ASIC-based ACs to determine the set of pareto-optimal FPGA-based ACs. We also perform a case-study to illustrate the benefits obtained by deploying these pareto-optimal FPGA-based ACs in a state-of-the-art automation framework to systematically generate pareto-optimal approximate accelerators that can be deployed in FPGA-based systems to achieve high performance or low-power consumption.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Bharath Srinivas Prabakaran", "Vojtech  Mrazek", "Zdenek  Vasicek", "Lukas  Sekanina", "Muhammad  Shafique"], "year": 2020, "n_citations": 4}
{"id": 2836803, "s2_id": "c4350acaabe5fe6433333b27fcb03694211bed9a", "title": "Rise of the Autonomous Machines", "abstract": "After decades of uninterrupted progress and growth, information technology has so evolved that it can be said we are entering the age of autonomous machines, but there exist many roadblocks in the way of making this a reality. In this article, we make a preliminary attempt at recognizing and categorizing the technical and non-technical challenges of autonomous machines; for each of the ten areas we have identified, we review current status, roadblocks, and potential research directions. It is hoped that this will help the community define clear, effective, and more formal development goalposts for the future. The Age of Autonomous Machines: The Sixth Layer of Information", "venue": "ArXiv", "authors": ["Shaoshan  Liu", "Jean-Luc  Gaudiot"], "year": 2021, "n_citations": 1}
{"id": 2837235, "s2_id": "1d7a922bac8a263b8e8fcba9c01d0fa9d562be1c", "title": "PRINS: Resistive CAM Processing in Storage", "abstract": "Near-data in-storage processing research has been gaining momentum in recent years. Typical processing-in-storage architecture places a single or several processing cores inside the storage and allows data processing without transferring it to the host CPU. Since this approach replicates von Neumann architecture inside storage, it is exposed to the problems faced by von Neumann architecture, especially the bandwidth wall. We present PRINS, a novel in-data processing-in-storage architecture based on Resistive Content Addressable Memory (RCAM). PRINS functions simultaneously as a storage and a massively parallel associative processor. PRINS alleviates the bandwidth wall faced by conventional processing-in-storage architectures by keeping the computing inside the storage arrays, thus implementing in-data, rather than near-data, processing. We show that PRINS may outperform a reference computer architecture with a bandwidth-limited external storage. The performance of PRINS Euclidean distance, dot product and histogram implementation exceeds the attainable performance of a reference architecture by up to four orders of magnitude, depending on the dataset size. The performance of PRINS SpMV may exceed the attainable performance of such reference architecture by more than two orders of magnitude.", "venue": "ArXiv", "authors": ["Leonid  Yavits", "Roman  Kaplan", "Ran  Ginosar"], "year": 2018, "n_citations": 3}
{"id": 2842395, "s2_id": "f9d7433c7deb93448bcbd92097ac3fb1641e8092", "title": "A Microbenchmark Characterization of the Emu Chick", "abstract": "The Emu Chick is a prototype system designed around the concept of migratory memory-side processing. Rather than transferring large amounts of data across power-hungry, high-latency interconnects, the Emu Chick moves lightweight thread contexts to near-memory cores before the beginning of each memory read. The current prototype hardware uses FPGAs to implement cache-less \"Gossamer cores for doing computational work and a stationary core to run basic operating system functions and migrate threads between nodes. In this multi-node characterization of the Emu Chick, we extend an earlier single-node investigation (Hein, et al. AsHES 2018) of the the memory bandwidth characteristics of the system through benchmarks like STREAM, pointer chasing, and sparse matrix-vector multiplication. We compare the Emu Chick hardware to architectural simulation and an Intel Xeon-based platform. Our results demonstrate that for many basic operations the Emu Chick can use available memory bandwidth more efficiently than a more traditional, cache-based architecture although bandwidth usage suffers for computationally intensive workloads like SpMV. Moreover, the Emu Chick provides stable, predictable performance with up to 65% of the peak bandwidth utilization on a random-access pointer chasing benchmark with weak locality.", "venue": "Parallel Comput.", "authors": ["Jeffrey  Young", "Eric R. Hein", "Srinivas  Eswar", "Patrick  Lavin", "Jiajia  Li", "E. Jason Riedy", "Richard W. Vuduc", "Tom  Conte"], "year": 2019, "n_citations": 7}
{"id": 2843162, "s2_id": "aaab691323c942eb2634ade28e98c7ddb8bfcd5f", "title": "Efficient and scalable barrier over Quadrics and Myrinet with a new NIC-based collective message passing protocol", "abstract": "Summary form only given. Modern interconnects often have programmable processors in the network interface that can be utilized to offload communication processing from host CPU. We explore different schemes to support collective operations at the network interface and propose a new collective protocol. With barrier as an initial case study, we have demontrated that much of the communication processing can be greatly simplified with this collective protocol. Accordingly, we have designed and implemented efficient and scalable NIC-based barrier operations over two high performance interconnects, Quadrics and Myrinet. Our evaluation shows that, over a Quadrics cluster of 8 nodes with ELan3 network, the NIC-based barrier operation achieves a barrier latency of only 5.60/spl mu/s. This result is a 2.48 factor of improvement over the Elanlib tree-based barrier operation. Over a Myrinet cluster of 8 nodes with LANai-XP NIC cards, a barrier latency of 14.20/spl mu/s over 8 nodes is achieved. This is a 2.64 factor of improvement over the host-based barrier algorithm. Furthermore, an analytical model developed for the proposed scheme indicates that a NIC-based barrier operation on a 1024-node cluster can be performed with only 22.13/spl mu/s latency over Quadrics and with 38.94/spl mu/s latency over Myrinet. These results indicate the potential for developing high performance communication subsystems for next generation clusters.", "venue": "18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.", "authors": ["Weikuan  Yu", "Darius  Buntinas", "Richard L. Graham", "Dhabaleswar K. Panda"], "year": 2004, "n_citations": 28}
{"id": 2844586, "s2_id": "1fe228f196c837345c41ef00ae018a0b26dbe5ca", "title": "Large Scale Low Power Computing System - Status of Network Design in ExaNeSt and EuroExa Projects", "abstract": "The deployment of the next generation computing platform at ExaFlops scale requires to solve new technological challenges mainly related to the impressive number (up to 10^6) of compute elements required. This impacts on system power consumption, in terms of feasibility and costs, and on system scalability and computing efficiency. In this perspective analysis, exploration and evaluation of technologies characterized by low power, high efficiency and high degree of customization is strongly needed. Among the various European initiative targeting the design of ExaFlops system, ExaNeSt and EuroExa are EU-H2020 funded initiatives leveraging on high end MPSoC FPGAs. Last generation MPSoC FPGAs can be seen as non-mainstream but powerful HPC Exascale enabling components thanks to the integration of embedded multi-core, ARM-based low power CPUs and a huge number of hardware resources usable to co-design application oriented accelerators and to develop a low latency high bandwidth network architecture. In this paper we introduce ExaNet the FPGA-based, scalable, direct network architecture of ExaNeSt system. ExaNet allow us to explore different interconnection topologies, to evaluate advanced routing functions for congestion control and fault tolerance and to design specific hardware components for acceleration of collective operations. After a brief introduction of the motivations and goals of ExaNeSt and EuroExa projects, we will report on the status of network architecture design and its hardware/software testbed adding preliminary bandwidth and latency achievements.", "venue": "PARCO", "authors": ["Roberto  Ammendola", "Andrea  Biagioni", "Fabrizio  Capuani", "Paolo  Cretaro", "Giulia De Bonis", "Francesca Lo Cicero", "Alessandro  Lonardo", "Michele  Martinelli", "Pier Stanislao Paolucci", "Elena  Pastorelli", "Luca  Pontisso", "Francesco  Simula", "Piero  Vicini"], "year": 2017, "n_citations": 2}
{"id": 2847779, "s2_id": "d6ca36b25bcb8c094b584504f734b1bf869d5c1f", "title": "UNIT: Unifying Tensorized Instruction Compilation", "abstract": "Because of the increasing demand for intensive computation in deep neural networks, researchers have developed both hardware and software mechanisms to reduce the compute and memory burden. A widely adopted approach is to use mixed precision data types. However, it is hard to benefit from mixed precision without hardware specialization because of the overhead of data casting. Recently, hardware vendors offer tensorized instructions specialized for mixed-precision tensor operations, such as Intel VNNI, Nvidia Tensor Core, and ARM DOT. These instructions involve a new computing idiom, which reduces multiple low precision elements into one high precision element. The lack of compilation techniques for this emerging idiom makes it hard to utilize these instructions. In practice, one approach is to use vendor-provided libraries for computationally-intensive kernels, but this is inflexible and prevents further optimizations. Another approach is to manually write hardware intrinsics, which is error-prone and difficult for programmers. Some prior works tried to address this problem by creating compilers for each instruction. This requires excessive efforts when it comes to many tensorized instructions. In this work, we develop a compiler framework, UNIT, to unify the compilation for tensorized instructions. The key to this approach is a unified semantics abstraction which makes the integration of new instructions easy, and the reuse of the analysis and transformations possible. Tensorized instructions from different platforms can be compiled via UNIT with moderate effort for favorable performance. Given a tensorized instruction and a tensor operation, UNIT automatically detects the applicability of the instruction, transforms the loop organization of the operation, and rewrites the loop body to take advantage of the tensorized instruction. According to our evaluation, UNIT is able to target various mainstream hardware platforms. The generated end-to-end inference model achieves 1.3 x speedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an Nvidia GPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on an ARM CPU.", "venue": "2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)", "authors": ["Jian  Weng", "Animesh  Jain", "Jie  Wang", "Leyuan  Wang", "Yida  Wang", "Tony  Nowatzki"], "year": 2021, "n_citations": 3}
{"id": 2849415, "s2_id": "6edbbdab1511195904fb3ee60eb8f9282687ffda", "title": "Tackling Variabilities in Autonomous Driving", "abstract": "The state-of-the-art driving automation system demands extreme computational resources to meet rigorous accuracy and latency requirements. Though emerging driving automation computing platforms are based on ASIC to provide better performance and power guarantee, building such an acceleratorbased computing platform for driving automation still present challenges. First, the workloads mix and performance requirements exposed to driving automation system present significant variability. Second, with more cameras/sensors integrated in a future fully autonomous driving vehicle, a heterogeneous multi-accelerator architecture substrate is needed that requires a design space exploration for a new form of parallelism. In this work, we aim to extensively explore the above system design challenges and these challenges motivate us to propose a comprehensive framework that synergistically handles the heterogeneous hardware accelerator design principles, system design criteria, and task scheduling mechanism. Specifically, we propose a novel heterogeneous multi-core AI accelerator (HMAI) to provide the hardware substrate for the driving automation tasks with variability. We also define system design criteria to better utilize hardware resources and achieve increased throughput while satisfying the performance and energy restrictions. Finally, we propose a deep reinforcement learning (RL)-based task scheduling mechanism FlexAI, to resolve task mapping issue. Experimental results show that with FlexAI scheduling, basically 100% tasks in each driving route can be processed by HMAI within their required period to ensure safety, and FlexAI can also maximally reduce the breaking distance up to 96% as compared to typical heuristics and guided random-searchbased algorithms.", "venue": "ArXiv", "authors": ["Yuqiong  Qi", "Yang  Hu", "Haibin  Wu", "Shen  Li", "Haiyu  Mao", "Xiaochun  Ye", "Dongrui  Fan", "Ninghui  Sun"], "year": 2021, "n_citations": 0}
{"id": 2851351, "s2_id": "d9af256258075f2096ac7064337457425bc50844", "title": "BayesPerf: minimizing performance monitoring errors using Bayesian statistics", "abstract": "Hardware performance counters (HPCs) that measure low-level architectural and microarchitectural events provide dynamic contextual information about the state of the system. However, HPC measurements are error-prone due to non determinism (e.g., undercounting due to event multiplexing, or OS interrupt-handling behaviors). In this paper, we present BayesPerf, a system for quantifying uncertainty in HPC measurements by using a domain-driven Bayesian model that captures microarchitectural relationships between HPCs to jointly infer their values as probability distributions. We provide the design and implementation of an accelerator that allows for low-latency and low-power inference of the BayesPerf model for x86 and ppc64 CPUs. BayesPerf reduces the average error in HPC measurements from 40.1% to 7.6% when events are being multiplexed. The value of BayesPerf in real-time decision-making is illustrated with a simple example of scheduling of PCIe transfers.", "venue": "ASPLOS", "authors": ["Subho S. Banerjee", "Saurabh  Jha", "Zbigniew T. Kalbarczyk", "Ravishankar K. Iyer"], "year": 2021, "n_citations": 1}
{"id": 2852499, "s2_id": "267b9391b8f5bce5a43a6525cb0b846321783efb", "title": "On the Optimal Refresh Power Allocation for Energy-Efficient Memories", "abstract": "Refresh is an important operation to prevent loss of data in dynamic random-access memory (DRAM). However, frequent refresh operations incur considerable power consumption and degrade system performance. Refresh power cost is especially significant in high-capacity memory devices and battery-powered edge/mobile applications. In this paper, we propose a principled approach to optimizing the refresh power allocation. Given a model for the bit error rate dependence on power, we formulate a convex optimization problem to minimize the word mean squared error for a refresh power constraint; hence we can guarantee the optimality of the obtained refresh power allocations. In addition, we provide an integer programming problem to optimize the discrete refresh interval assignments. For an 8-bit accessed word, numerical results show that the optimized nonuniform refresh intervals reduce the refresh power by 29% at a peak signal-to-noise ratio of 50dB compared to the uniform assignment.", "venue": "2019 IEEE Global Communications Conference (GLOBECOM)", "authors": ["Yongjune  Kim", "Won Ho Choi", "Cyril  Guyot", "Yuval  Cassuto"], "year": 2019, "n_citations": 2}
{"id": 2853211, "s2_id": "7aa12eb945e2be5c2b1e7a0aeeb5392ba12bbe62", "title": "NOP - A Simple Experimental Processor for Parallel Deployment", "abstract": "The design of a parallel computing system using several thousands or even up to a million processors asks for processing units that are simple and thus small in space, to make as many processing units as possible fit on a single die. \nThe design presented herewith is far from being optimised, it is not meant to compete with industry performance devices. Its main purpose is to allow for a prototypical implementation of a dynamic software system as a proof of concept.", "venue": "ArXiv", "authors": ["Oskar  Schirmer"], "year": 2016, "n_citations": 1}
{"id": 2863146, "s2_id": "20933691c21735285714228fc3d64bbb4b61e4f5", "title": "FPGA-based Binocular Image Feature Extraction and Matching System", "abstract": "Image feature extraction and matching is a fundamental but computation intensive task in machine vision. This paper proposes a novel FPGA-based embedded system to accelerate feature extraction and matching. It implements SURF feature point detection and BRIEF feature descriptor construction and matching. For binocular stereo vision, feature matching includes both tracking matching and stereo matching, which simultaneously provide feature point correspondences and parallax information. Our system is evaluated on a ZYNQ XC7Z045 FPGA. The result demonstrates that it can process binocular video data at a high frame rate (640 x 480 @ 162fps). Moreover, an extensive test proves our system has robustness for image compression, blurring and illumination.", "venue": "ICMSSP 2019", "authors": ["Qi  Ni", "Fei  Wang", "Ziwei  Zhao", "Peng  Gao"], "year": 2019, "n_citations": 3}
{"id": 2867349, "s2_id": "7e5fbbead3f2d0bcc0f9a3eeaae8591b1356ea13", "title": "Using Multi-Core HW/SW Co-design Architecture for Accelerating K-means Clustering Algorithm", "abstract": "The capability of classifying and clustering a desired set of data is an essential part of building knowledge from data. However, as the size and dimensionality of input data increases, the run-time for such clustering algorithms is expected to grow superlinearly, making it a big challenge when dealing with BigData. K-mean clustering is an essential tool for many big data applications including data mining, predictive analysis, forecasting studies, and machine learning. However, due to large size (volume) of Big-Data, and large dimensionality of its data points, even the application of a simple k-mean clustering may become extremely time and resource demanding. Specially when it is necessary to have a fast and modular dataset analysis flow. In this paper, we demonstrate that using a two-level filtering algorithm based on binary kd-tree structure is able to decrease the time of convergence in K-means algorithm for large datasets. The two-level filtering algorithm based on binary kd-tree structure evolves the SW to naturally divide the classification into smaller data sets, based on the number of available cores and size of logic available in a target FPGA. The empirical result on this two-level structure over multi-core FPGA-based architecture provides 330X speed-up compared to a conventional software-only solution.", "venue": "ArXiv", "authors": ["Hadi Mardani Kamali"], "year": 2018, "n_citations": 3}
{"id": 2867789, "s2_id": "c36e11536f8a70d80083cec9e089ed3cc7019f51", "title": "GhostMinion: A Strictness-Ordered Cache System for Spectre Mitigation", "abstract": "Out-of-order speculation, a technique ubiquitous since the early 1990s, remains a fundamental security flaw. Via attacks such as Spectre and Meltdown, an attacker can trick a victim, in an otherwise entirely correct program, into leaking its secrets through the effects of misspeculated execution, in a way that is entirely invisible to the programmer\u2019s model. This has serious implications for application sandboxing and inter-process communication. Designing efficient mitigations that preserve the performance of out-of-order execution has been a challenge. The speculation-hiding techniques in the literature have been shown to not close such channels comprehensively, allowing adversaries to redesign attacks. Strong, precise guarantees are necessary, but mitigations must achieve high performance to be adopted. We present Strictness Ordering, a new constraint system that shows how we can comprehensively eliminate transient side channel attacks, while still allowing complex speculation and data forwarding between speculative instructions. We then present GhostMinion, a cache modification built using a variety of new techniques designed to provide Strictness Order at only 2.5% overhead.", "venue": "MICRO", "authors": ["Sam  Ainsworth"], "year": 2021, "n_citations": 1}
{"id": 2870884, "s2_id": "ef75284e14c8af479396db98a1356914c849ce95", "title": "Improving the GPU space of computation under triangular domain problems", "abstract": "There is a stage in the GPU computing pipeline where a grid of thread-blocks is mapped to the problem domain. Normally, this grid is a k-dimensional bounding box that covers a k-dimensional problem no matter its shape. Threads that fall inside the problem domain perform computations, otherwise they are discarded at runtime. For problems with non-square geometry, this is not always the best idea because part of the space of computation is executed without any practical use. Two- dimensional triangular domain problems, alias td-problems, are a particular case of interest. Problems such as the Euclidean distance map, LU decomposition, collision detection and simula- tions over triangular tiled domains are all td-problems and they appear frequently in many areas of science. In this work, we propose an improved GPU mapping function g(lambda), that maps any lambda block to a unique location (i, j) in the triangular domain. The mapping is based on the properties of the lower triangular matrix and it works at a block level, thus not compromising thread organization within a block. The theoretical improvement from using g(lambda) is upper bounded as I < 2 and the number of wasted blocks is reduced from O(n^2) to O(n). We compare our strategy with other proposed methods; the upper-triangular mapping (UTM), the rectangular box (RB) and the recursive partition (REC). Our experimental results on Nvidias Kepler GPU architecture show that g(lambda) is between 12% and 15% faster than the bounding box (BB) strategy. When compared to the other strategies, our mapping runs significantly faster than UTM and it is as fast as RB in practical use, with the advantage that thread organization is not compromised, as in RB. This work also contributes at presenting, for the first time, a fair comparison of all existing strategies running the same experiments under the same hardware.", "venue": "ArXiv", "authors": ["Cristobal A. Navarro", "Nancy  Hitschfeld-Kahler"], "year": 2013, "n_citations": 3}
{"id": 2872332, "s2_id": "aa45b423620697856a4fcd23e11d36652f2787cb", "title": "Lithography hotspot detection and mitigation in nanometer VLSI", "abstract": "With continued feature size scaling, even state of the art semiconductor manufacturing processes will often run into layouts with poor printability and yield. Identifying lithography hotspots is important at both physical verification and early physical design stages. While detailed lithography simulations can be very accurate, they may be too computationally expensive for full-chip scale and physical design inner loops. Meanwhile, pattern matching and machine learning based hotspot detection methods can provide acceptable quality and yet fast turn-around-time for full-chip scale physical verification and design. In this paper, we discuss some key issues and recent results on lithography hotspot detection and mitigation in nanometer VLSI.", "venue": "2013 IEEE 10th International Conference on ASIC", "authors": ["Jhih-Rong  Gao", "Bei  Yu", "Duo  Ding", "David Z. Pan"], "year": 2013, "n_citations": 3}
{"id": 2872891, "s2_id": "e5d2b364140071f2ab20a942b855d4599775faea", "title": "The Processing Using Memory Paradigm: In-DRAM Bulk Copy, Initialization, Bitwise AND and OR", "abstract": "In existing systems, the off-chip memory interface allows the memory controller to perform only read or write operations. Therefore, to perform any operation, the processor must first read the source data and then write the result back to memory after performing the operation. This approach consumes high latency, bandwidth, and energy for operations that work on a large amount of data. Several works have proposed techniques to process data near memory by adding a small amount of compute logic closer to the main memory chips. In this article, we describe two techniques proposed by recent works that take this approach of processing in memory further by exploiting the underlying operation of the main memory technology to perform more complex tasks. First, we describe RowClone, a mechanism that exploits DRAM technology to perform bulk copy and initialization operations completely inside main memory. We then describe a complementary work that uses DRAM to perform bulk bitwise AND and OR operations inside main memory. These two techniques significantly improve the performance and energy efficiency of the respective operations.", "venue": "ArXiv", "authors": ["Vivek  Seshadri", "Onur  Mutlu"], "year": 2016, "n_citations": 14}
{"id": 2873593, "s2_id": "582873fe216f12fdcda403e2595207501f8fd75c", "title": "On Value Recomputation to Accelerate Invisible Speculation", "abstract": "Recent architectural approaches that address speculative side-channel attacks aim to prevent software from exposing the microarchitectural state changes of transient execution. The Delay-on-Miss technique is one such approach, which simply delays loads that miss in the L1 cache until they become non-speculative, resulting in no transient changes in the memory hierarchy. However, this costs performance, prompting the use of value prediction (VP) to regain some of the delay. However, the problem cannot be solved by simply introducing a new kind of speculation (value prediction). Value-predicted loads have to be validated, which cannot be commenced until the load becomes non-speculative. Thus, value-predicted loads occupy the same amount of precious core resources (e.g., reorder buffer entries) as Delay-on-Miss. The end result is that VP only yields marginal benefits over Delay-on-Miss. In this paper, our insight is that we can achieve the same goal as VP (increasing performance by providing the value of loads that miss) without incurring its negative side-effect (delaying the release of precious resources), if we can safely, non-speculatively, recompute a value in isolation (without being seen from the outside), so that we do not expose any information by transferring such a value via the memory hierarchy. Value Recomputation, which trades computation for data transfer was previously proposed in an entirely different context: to reduce energy-expensive data transfers in the memory hierarchy. In this paper, we demonstrate the potential of value recomputation in relation to the Delay-on-Miss approach of hiding speculation, discuss the trade-offs, and show that we can achieve the same level of security, reaching 93% of the unsecured baseline performance (5% higher than Delay-on-miss), and exceeding (by 3%) what even an oracular (100% accuracy and coverage) value predictor could do.", "venue": "ArXiv", "authors": ["Christos  Sakalis", "Zamshed I. Chowdhury", "Shayne  Wadle", "Ismail  Akturk", "Alberto  Ros", "Magnus  Sj\u00e4lander", "Stefanos  Kaxiras", "Ulya R. Karpuzcu"], "year": 2021, "n_citations": 0}
{"id": 2874130, "s2_id": "17e83ba6afeab79e98c04276c02f88772d6c0871", "title": "High-density and Secure Data Transmission via Linear Combinations", "abstract": "Suppose that there are $n$ Senders and $n$ Receivers. Our goal is to send long messages from Sender $i$ to Receiver $i$ such that no other receiver can retrieve the message intended for Receiver $i$. The task can easily be completed using $n$ private channels between the pairs. Solutions, using one channel needs either encryption or switching elements for routing the messages to their addressee. \nThe main result of the present work is a description of a network in which The Senders and the Receivers are connected with only $n^{o(1)}$ channels; the encoding and de-coding is nothing else just very fast linear combinations of the message-bits; and there are no switching or routing-elements in the network, just linear combinations are computed, with fixed connections (channels or wires). \nIn the proofs we do not use {\\em any} unproven cryptographical or complexity theoretical assumptions.", "venue": "ArXiv", "authors": ["Vince  Grolmusz"], "year": 2003, "n_citations": 0}
{"id": 2875684, "s2_id": "04cc5fc1e98a54cf1f210067cc9fcc5364023fa9", "title": "Soft-error tolerance analysis and optimization of nanometer circuits", "abstract": "Nanometer circuits are becoming increasingly susceptible to soft-errors due to alpha-particle and atmospheric neutron strikes as device scaling reduces node capacitances and supply/threshold voltage scaling reduces noise margins. It is becoming crucial to add soft-error tolerance (SET) estimation and optimization to the design flow to handle the increasing susceptibility. The first part of this paper presents a tool for accurate SET analysis of nm circuits (ASERTA) that can be used to estimate the SET of nm circuits consisting of millions of gates. The tolerance estimates generated by the tool match SPICE generated estimates closely while taking orders of magnitude less computation time. The second part of the paper presents a tool for SET optimization of nm circuits (SERTOPT) using the tolerance estimates generated by ASERTA. The tool finds optimal sizes, channel lengths, supply voltages and threshold voltages to be assigned to gates in a combinational circuit such that the SET is increased while meeting the timing constraints. Experiments on ISCAS'85 benchmark circuits showed that the soft-error rate of the optimized circuit decreased by as much as 47% with marginal increase in circuit delay.", "venue": "Design, Automation and Test in Europe", "authors": ["Yuvraj Singh Dhillon", "Abdulkadir Utku Diril", "Abhijit  Chatterjee"], "year": 2005, "n_citations": 132}
{"id": 2885248, "s2_id": "2fbe4c0d83ee4455703ec28fa2bf7da6898c7d0e", "title": "An efficient cntfet-based 7-input minority gate", "abstract": "Complementary metal oxide semiconductor technology (CMOS) has been faced critical challenges in nano-scale regime. CNTFET (Carbon Nanotube Field effect transistor) technology is a promising alternative for CMOS technology. In this paper, we proposed a novel 7-input minority gate in CNTFET technology that has only 9 CNTFETs. Minority function is utilized in the voting systems for decision making and also it is used in data mining. This proposed 7-input minority gate is utilized less fewer transistors than the conventional CMOS method which utilizes many transistors for implementing sum of products. By means of this proposed 7-input minority gate, a 4-input NAND gate can be implemented, which gets better the conventional design in terms of delay and energy efficiency and has much more deriving power at its output.", "venue": "VLSIC 2013", "authors": ["Samira Shirinabadi Farahani", "Ronak  Zarhoun", "Mohammad Hossein Moaiyeri", "Keivan  Navi"], "year": 2013, "n_citations": 5}
{"id": 2887721, "s2_id": "e4a323b5b35543c966088bb40910d308d963807a", "title": "Shenjing: A low power reconfigurable neuromorphic accelerator with partial-sum and spike networks-on-chip", "abstract": "The next wave of on-device AI will likely require energy-efficient deep neural networks. Brain-inspired spiking neural networks (SNN) has been identified to be a promising candidate. Doing away with the need for multipliers significantly reduces energy. For on-device applications, besides computation, communication also incurs a significant amount of energy and time. In this paper, we propose Shenjing, a configurable SNN architecture which fully exposes all on-chip communications to software, enabling software mapping of SNN models with high accuracy at low power. Unlike prior SNN architectures like TrueNorth, Shenjing does not require any model modification and retraining for the mapping. We show that conventional artificial neural networks (ANN) such as multilayer perceptron, convolutional neural networks, as well as the latest residual neural networks can be mapped successfully onto Shenjing, realizing ANNs with SNN\u2019s energy efficiency. For the MNIST inference problem using a multilayer perceptron, we were able to achieve an accuracy of 96% while consuming just 1.26 mW using 10 Shenjing cores.", "venue": "2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Bo  Wang", "Jun  Zhou", "Weng-Fai  Wong", "Li-Shiuan  Peh"], "year": 2020, "n_citations": 5}
{"id": 2896097, "s2_id": "ef097af64ab290c7dbe8e8752558b37b381b0c08", "title": "Design of PIC12F675 Microcontroller Based Data Acquisition System for Slowly Varying Signals", "abstract": "The present paper describes the design of a cost effective, better resolution data acquisition system (DAS) which is compatible to most of the PC and laptops. A low cost DAS has been designed using PIC12F675 having 4-channel analog input with 10-bit resolution for the monitoring of slowly varying signals. The DAS so designed is interfaced to the serial port of the PC. Firmware is written in Basic using Oshonsoft PIC IDE and burn to the microcontroller by using PICkit2 programmer. An application program is also developed using Visual Basic 6 which allows to display the waveform of the signal(s) and simultaneously the data also can be saved into the hard disk of the computer for future use and analysis.", "venue": "ArXiv", "authors": ["N. Monoranjan Singh", "K. C. Sarma"], "year": 2012, "n_citations": 9}
{"id": 2896099, "s2_id": "4f42f733955bf0b431a67503c39045f06fe0f639", "title": "Criticality Aware Soft Error Mitigation in the Configuration Memory of SRAM Based FPGA", "abstract": "Efficient low complexity error correcting code (ECC) is considered as an effective technique for mitigation of multi-bit upset (MBU) in the configuration memory (CM) of static random access memory (SRAM) based Field Programmable Gate Array (FPGA) devices. Traditional multi-bit ECCs have large overhead and complex decoding circuit to correct adjacent multibit error. In this work, we propose a simple multi-bit ECC which uses Secure Hash Algorithm for error detection and parity based two dimensional Erasure Product Code for error correction. Present error mitigation techniques perform error correction in the CM without considering the criticality or the execution period of the tasks allocated in different portion of CM. In most of the cases, error correction is not done in the right instant, which sometimes either suspends normal system operation or wastes hardware resources for less critical tasks. In this paper, we advocate for a dynamic priority-based hardware scheduling algorithm which chooses the tasks for error correction based on their area, execution period and criticality. The proposed method has been validated in terms of overhead due to redundant bits, error correction time and system reliability", "venue": "2019 32nd International Conference on VLSI Design and 2019 18th International Conference on Embedded Systems (VLSID)", "authors": ["Swagata  Mandal", "Sreetama  Sarkar", "Ming Ming Wong", "Anupam  Chattopadhyay", "Amlan  Chakrabarti"], "year": 2019, "n_citations": 4}
{"id": 2899002, "s2_id": "be72f33e3e787ef44c1b85637a029460cfeabfeb", "title": "Formally Verifying WARP-V, an Open-Source TL-Verilog RISC-V Core Generator", "abstract": "Timing-abstract and transaction-level design using TL-Verilog have shown significant productivity gains for logic design. In this work, we explored the natural extension of transaction-level design methodology into formal verification. \nWARP-V is a CPU core generator written in TL-Verilog. Our primary verification vehicle for WARP-V was a formal verification framework for RISC-V, called riscv-formal. The timing-abstract and transaction-level logic modeling techniques of TL-Verilog greatly simplified the task of creating a harness connecting the WARP-V model to the verification interface of riscv-formal. Furthermore, the same harness works across all RISC-V configurations of WARP-V.", "venue": "ArXiv", "authors": ["Steven F. Hoover", "\u00c1kos  Hadnagy"], "year": 2018, "n_citations": 0}
{"id": 2899523, "s2_id": "80bd81d94ee5b75940c28d14bb81082715146569", "title": "First-Generation Inference Accelerator Deployment at Facebook", "abstract": "In this paper, we provide a deep dive into the deployment of inference accelerators at Facebook. Many of our ML workloads have unique characteristics, such as sparse memory accesses, large model sizes, as well as high compute, memory and network bandwidth requirements. We co-designed a high-performance, energy-efficient inference accelerator platform based on these requirements. We describe the inference accelerator platform ecosystem we developed and deployed at Facebook: both hardware, through Open Compute Platform (OCP), and software framework and tooling, through Pytorch/Caffe2/Glow. A characteristic of this ecosystem from the start is its openness to enable a variety of AI accelerators from different vendors. This platform, with six low-power accelerator cards alongside a singlesocket host CPU, allows us to serve models of high complexity that cannot be easily or efficiently run on CPUs. We describe various performance optimizations, at both platform and accelerator level, which enables this platform to serve production traffic at Facebook. We also share deployment challenges, lessons learned during performance optimization, as well as provide guidance for future inference hardware co-design.", "venue": "ArXiv", "authors": ["Michael J. Anderson", "Benny  Chen", "Stephen  Chen", "Summer  Deng", "Jordan  Fix", "Michael  Gschwind", "Aravind  Kalaiah", "Changkyu  Kim", "Jaewon  Lee", "Jason  Liang", "Haixin  Liu", "Yinghai  Lu", "Jack  Montgomery", "Arun  Moorthy", "Nadathur  Satish", "Sam  Naghshineh", "Avinash  Nayak", "Jongsoo  Park", "Chris  Petersen", "Martin  Schatz", "Narayanan  Sundaram", "Bangsheng  Tang", "Peter  Tang", "Amy  Yang", "Jiecao  Yu", "Hector  Yuen", "Ying  Zhang", "Aravind  Anbudurai", "Vandana  Balan", "Harsha  Bojja", "Joe  Boyd", "Matthew  Breitbach", "Claudio  Caldato", "Anna  Calvo", "Garret  Catron", "Sneh  Chandwani", "Panos  Christeas", "Brad  Cottel", "Brian  Coutinho", "Arun  Dalli", "Abhishek  Dhanotia", "Oniel  Duncan", "Roman  Dzhabarov", "Simon  Elmir", "Chunli  Fu", "Wenyin  Fu", "Michael  Fulthorp", "Adi  Gangidi", "Nick  Gibson", "Sean  Gordon", "Beatriz Padilla Hernandez", "Daniel  Ho", "Yu-Cheng  Huang", "Olof  Johansson", "Shishir  Juluri", "et  al."], "year": 2021, "n_citations": 4}
{"id": 2900086, "s2_id": "49083298fa76747cbd6c8157e5276c861e4de1a2", "title": "Effect of Thread Level Parallelism on the Performance of Optimum Architecture for Embedded Applications", "abstract": "According to the increasing complexity of network application and internet traffic, network processor as a subset of embedded processors have to process more computation intensive tasks. By scaling down the feature size and emersion of chip multiprocessors (CMP) that are usually multi-thread processors, the performance requirements are somehow guaranteed. As multithread processors are the heir of uni-thread processors and there isn't any general design flow to design a multithread embedded processor, in this paper we perform a comprehensive design space exploration for an optimum uni-thread embedded processor based on the limited area and power budgets. Finally we run multiple threads on this architecture to find out the maximum thread level parallelism (TLP) based on performance per power and area optimum uni-thread architecture.", "venue": "ArXiv", "authors": ["Mehdi  Alipour", "Hojjat  Taghdisi"], "year": 2012, "n_citations": 3}
{"id": 2903431, "s2_id": "666d9d7397e3f67606c0ffb333f4e156720750e3", "title": "Analysis of energy consumption in a precision beekeeping system", "abstract": "Honey bees have been domesticated by humans for several thousand years and mainly provide honey and pollination, which is fundamental for plant reproduction. Nowadays, the work of beekeepers is constrained by external factors that stress their production (parasites and pesticides among others). Taking care of large numbers of beehives is time-consuming, so integrating sensors to track their status can drastically simplify the work of beekeepers. Precision beekeeping complements beekeepers' work thanks to the Internet of Things (IoT) technology. If used correctly, data can help to make the right diagnosis for honey bees colony, increase honey production and decrease bee mortality. Providing enough energy for on-hive and in-hive sensors is a challenge. Some solutions rely on energy harvesting, others target usage of large batteries. Either way, it is mandatory to analyze the energy usage of embedded equipment in order to design an energy efficient and autonomous bee monitoring system. This paper relies on a fully autonomous IoT framework that collects environmental and image data of a beehive. It consists of a data collecting node (environmental data sensors, camera, Raspberry Pi and Arduino) and a solar energy supplying node. Supported services are analyzed task by task from an energy profiling and efficiency standpoint, in order to identify the highly pressured areas of the framework. This first step will guide our goal of designing a sustainable precision beekeeping system, both technically and energy-wise.", "venue": "IOT", "authors": ["Hugo  Hadjur", "Doreid  Ammar", "Laurent  Lef\u00e8vre"], "year": 2020, "n_citations": 1}
{"id": 2904618, "s2_id": "0374a4e82470188cb4a05e9277eac2c574eec84f", "title": "DRMap: A Generic DRAM Data Mapping Policy for Energy-Efficient Processing of Convolutional Neural Networks", "abstract": "Many convolutional neural network (CNN) accelerators face performance- and energy-efficiency challenges which are crucial for embedded implementations, due to high DRAM access latency and energy. Recently, some DRAM architectures have been proposed to exploit subarray-level parallelism for decreasing the access latency. Towards this, we present a design space exploration methodology to study the latency and energy of different mapping policies on different DRAM architectures, and identify the pareto-optimal design choices. The results show that the energy-efficient DRAM accesses can be achieved by a mapping policy that orderly prioritizes to maximize the row buffer hits, bank- and subarray-level parallelism.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Abdullah Hanif", "Muhammad  Shafique"], "year": 2020, "n_citations": 10}
{"id": 2905018, "s2_id": "8d6f72c56282c29bfe62f271fad6826dfcd8a30f", "title": "Systematic transaction level modeling of embedded systems with SystemC", "abstract": "The paper gives an overview of a transaction level modeling (TLM) design flow for straightforward embedded system design with SystemC. The goal is to develop systematically both the HW and SW application-specific components of an embedded system using the TLM approach, thus allowing for fast communication architecture exploration, rapid prototyping and early embedded SW development. To this end, we specify a lightweight transaction-based communication protocol, SHIP (SystemC high-level interface protocol), and present a methodology for automatic mapping of the communication part of a system to a given architecture, including HW/SW interfaces.", "venue": "Design, Automation and Test in Europe", "authors": ["Wolfgang  Klingauf"], "year": 2005, "n_citations": 22}
{"id": 2906723, "s2_id": "5412b687108fe8dd7d2a0fe75ba99f518820bb56", "title": "Bridging the Gap: FPGAs as Programmable Switches", "abstract": "The emergence of P4, a domain specific language, coupled to PISA, a domain specific architecture, is revolutionizing the networking field. P4 allows to describe how packets are processed by a programmable data plane, spanning ASICs and CPUs, implementing PISA. Because the processing flexibility can be limited on ASICs, while the CPUs performance for networking tasks lag behind, recent works have proposed to implement PISA on FPGAs. However, little effort has been dedicated to analyze whether FPGAs are good candidates to implement PISA.In this work, we take a step back and evaluate the micro-architecture efficiency of various PISA blocks. We demonstrate, supported by a theoretical and experimental analysis, that the performance of a few PISA blocks is severely limited by the current FPGA architectures. Specifically, we show that match tables and programmable packet schedulers represent the main performance bottlenecks for FPGA-based programmable switches. Thus, we explore two avenues to alleviate these shortcomings. First, we identify network applications well tailored to current FPGAs. Second, to support a wider range of networking applications, we propose modifications to the FPGA architectures which can also be of interest out of the networking field.", "venue": "2020 IEEE 21st International Conference on High Performance Switching and Routing (HPSR)", "authors": ["Thomas  Luinaud", "Thibaut  Stimpfling", "Jeferson Santiago da Silva", "Yvon  Savaria", "J. M. Pierre Langlois"], "year": 2020, "n_citations": 2}
{"id": 2907533, "s2_id": "c545f0f6de47ebb0bc78579cf26b45f4ffe44770", "title": "Comparing quaternary and binary multipliers", "abstract": "We compare the implementation of a 8x8 bit multiplier with two different implementations of a 4x4 quaternary digit multiplier. Interfacing this binary multiplier with quaternary to binary decoders and binary to quaternary encoders leads to a 4x4 multiplier that outperforms the best direct implementation of a 4x4 quaternary multiplier. The far greater complexity of the 1-digit multipliers and 1-digit adders used in this direct implementation compared to the binary 1-bit multipliers and full adders cannot be compensated by the reduced count of quaternary operators. As the best quaternary multiplier includes the corresponding binary one, it means that there is no opportunity to get less interconnects, less chip area, less power dissipation with the quaternary multiplier.", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2020, "n_citations": 2}
{"id": 2912240, "s2_id": "746a4c213367861164d60cc7b2f76d695e4e3223", "title": "A Brief Survey of Non-Residue Based Computational Error Correction", "abstract": "The idea of computational error correction has been around for over half a century. The motivation has largely been to mitigate unreliable devices, manufacturing defects or harsh environments, primarily as a mandatory measure to preserve reliability, or more recently, as a means to lower energy by allowing soft errors to occasionally creep. While residue codes have shown great promise for this purpose, there have been several orthogonal non-residue based techniques. In this article, we provide a high level outline of some of these non-residual approaches.", "venue": "ArXiv", "authors": ["Sriseshan  Srikanth", "Bobin  Deng", "Thomas M. Conte"], "year": 2016, "n_citations": 2}
{"id": 2912919, "s2_id": "e56c84700f939c240c8937f7cf44959a39222d15", "title": "VLSI Architectures for WIMAX Channel Decoders", "abstract": "This chapter describes the main architectures proposed in the literature to implement the channel decoders required by the WiMax standard, namely convolutional codes, turbo codes (both block and convolutional) and LDPC. Then it shows a complete design of a convolutional turbo code encoder/decoder system for WiMax.", "venue": "ArXiv", "authors": ["Maurizio  Martina", "Guido  Masera"], "year": 2010, "n_citations": 1}
{"id": 2915758, "s2_id": "7afddf4c3aa354c6539252b332d2593533cb51c5", "title": "Understanding Cache Boundness of ML Operators on ARM Processors", "abstract": "Machine Learning (ML) compilers like TVM allow a fast and flexible deployment on embedded CPUs. This enables the use of non-standard operators, which are common in ML compression techniques. However, it is necessary to understand the limitations of typical compute-intense operators in ML workloads to design a proper solution. This is the first indetail analysis of dense and convolution operators, generated with TVM, that compares to the fundamental hardware limits of embedded ARM processors. Thereby it explains the gap between computational peak performance, theoretical and measured, and real-world state-of-the-art results, created with TVM and openBLAS. Instead, one can see that single-precision general matrix multiply (GEMM) and convolutions are bound by L1-cacheread bandwidth. Explorations of 8-bit and bit-serial quantized operators show that quantization can be used to achieve relevant speedups compared to cache-bound floating-point operators. However, the performance of quantized operators highly depends on the interaction between data layout and bit packing.", "venue": "ArXiv", "authors": ["Bernhard  Klein", "Christoph  Gratl", "Manfred  M\u00fccke", "Holger  Fr\u00f6ning"], "year": 2021, "n_citations": 0}
{"id": 2916690, "s2_id": "4b17dcd4546168675bcf7308e6d8f8d78b15191a", "title": "Accelerating Encrypted Computing on Intel GPUs", "abstract": "Homomorphic Encryption (HE) is an emerging encryption scheme that allows computations to be performed directly on encrypted messages. This property provides promising applications such as privacy-preserving deep learning and cloud computing. Prior works have been proposed to enable practical privacy-preserving applications with architectural-aware optimizations on CPUs, GPUs and FPGAs. However, there is no systematic optimization for the whole HE pipeline on Intel GPUs. In this paper, we present the first-ever SYCL-based GPU backend for Microsoft SEAL APIs. We perform optimizations from instruction level, algorithmic level and application level to accelerate our HE library based on the Cheon, Kim, Kim and Song (CKKS) scheme on Intel GPUs. The performance is validated on two latest Intel GPUs. Experimental results show that our staged optimizations together with optimizations including low-level optimizations and kernel fusion accelerate the Number Theoretic Transform (NTT), a key algorithm for HE, by up to 9.93X compared with the naive GPU baseline. The roofline analysis confirms that our optimized NTT reaches 79.8% and 85.7% of the peak performance on two GPU devices. Through the highly optimized NTT and the assembly-level optimization, we obtain 2.32X 3.05X acceleration for HE evaluation routines. In addition, our all-together systematic optimizations improve the performance of encrypted element-wise polynomial matrix multiplication application by up to 3.10X.", "venue": "ArXiv", "authors": ["Yujia  Zhai", "Mohannad  Ibrahim", "Yiqin  Qiu", "Fabian  Boemer", "Zizhong  Chen", "Alexey  Titov", "Alexander  Lyashevsky"], "year": 2021, "n_citations": 1}
{"id": 2919556, "s2_id": "595101f13b961d69c553ce1ed24f60f3f1085e02", "title": "RecSSD: near data processing for solid state drive based recommendation inference", "abstract": "Neural personalized recommendation models are used across a wide variety of datacenter applications including search, social media, and entertainment. State-of-the-art models comprise large embedding tables that have billions of parameters requiring large memory capacities. Unfortunately, large and fast DRAM-based memories levy high infrastructure costs. Conventional SSD-based storage solutions offer an order of magnitude larger capacity, but have worse read latency and bandwidth, degrading inference performance. RecSSD is a near data processing based SSD memory system customized for neural recommendation inference that reduces end-to-end model inference latency by 2\u00d7 compared to using COTS SSDs across eight industry-representative models.", "venue": "ASPLOS", "authors": ["Mark  Wilkening", "Udit  Gupta", "Samuel  Hsia", "Caroline  Trippel", "Carole-Jean  Wu", "David  Brooks", "Gu-Yeon  Wei"], "year": 2021, "n_citations": 11}
{"id": 2922119, "s2_id": "fa843d89f60a5d8ed5d4edeea61feaddd511e611", "title": "Performance-aware predictive-model-based on-chip body-bias regulation strategy for an ULP multi-core cluster in 28 nm UTBB FD-SOI", "abstract": "The performance and reliability of Ultra-Low-Power (ULP) computing platforms are adversely affected by environmental temperature and process variations. Mitigating the effect of these phenomena becomes crucial when these devices operate near-threshold, due to the magnification of process variations and to the strong temperature inversion effect that affects advanced technology nodes in low-voltage corners, which causes huge overhead due to margining for timing closure. Supporting an extended range of reverse and forward body-bias, UTBB FD-SOI technology provides a powerful knob to compensate for such variations. In this work we propose a methodology to maximize energy efficiency at run-time exploiting body biasing on a ULP platform operating near-threshold. The proposed method relies on on-line performance measurements by means of Process Monitoring Blocks (PMBs) coupled with an on-chip low-power body bias generator. We correlate the measurement performed by the PMBs to the maximum achievable frequency of the system, deriving a predictive model able to estimate it with an error of 9.7% at 0.7V. To minimize the effect of process variations we propose a calibration procedure that allows to use a PMB model affected by only the temperature-induced error, which reduces the frequency estimation error by 2.4x (from 9.7% to 4%). We finally propose a controller architecture relying on the derived models to automatically regulate at run-time the body bias voltage. We demonstrate that adjusting the body bias voltage against environmental temperature variations leads up to 2X reduction in the leakage power and a 15% improvement on the global energy consumption when the system operates at 0.7V and 170MHz", "venue": "Integr.", "authors": ["Alfio Di Mauro", "Davide  Rossi", "Antonio  Pullini", "Philippe  Flatresse", "Luca  Benini"], "year": 2020, "n_citations": 1}
{"id": 2926298, "s2_id": "f62b5dc3cc82667f7542c90a0c682f8b76578cfb", "title": "Approximate Early Output Asynchronous Adders Based on Dual-Rail Data Encoding and 4-Phase Return-to-Zero and Return-to-One Handshaking", "abstract": "Approximate computing is emerging as an alternative to accurate computing due to its potential for realizing digital circuits and systems with low power dissipation, less critical path delay, and less area occupancy for an acceptable trade-off in the accuracy of results. In the domain of computer arithmetic, several approximate adders and multipliers have been designed and their potential have been showcased versus accurate adders and multipliers for practical digital signal processing applications. Nevertheless, in the existing literature, almost all the approximate adders and multipliers reported correspond to the synchronous design method. In this work, we consider robust asynchronous i.e. quasi-delay-insensitive realizations of approximate adders by employing delay-insensitive codes for data representation and processing, and the 4-phase handshake protocols for data communication. The 4-phase handshake protocols used are the return-to-zero and the return-to-one protocols. Specifically, we consider the implementations of 32-bit approximate adders based on the return-to-zero and return-to-one handshake protocols by adopting the delay-insensitive dual-rail code for data encoding. We consider a range of approximations varying from 4-bits to 20-bits for the least significant positions of the accurate 32-bit asynchronous adder. The asynchronous adders correspond to early output (i.e. early reset) type, which are based on the well-known ripple carry adder architecture. The experimental results show that approximate asynchronous adders achieve reductions in the design metrics such as latency, cycle time, average power dissipation, and silicon area compared to the accurate asynchronous adders. Further, the reductions in the design metrics are greater for the return-to-one protocol compared to the return-to-zero protocol. The design metrics were estimated using a 32/28nm CMOS technology.", "venue": "ArXiv", "authors": ["P.  Balasubramanian"], "year": 2018, "n_citations": 3}
{"id": 2942478, "s2_id": "d9f06ceedf04f787e56e10c269b204a23cacb82e", "title": "Enabling Virtual Memory Research on RISC-V with a Configurable TLB Hierarchy for the Rocket Chip Generator", "abstract": "The Rocket Chip Generator uses a collection of parameterized processor components to produce RISC-V-based SoCs. It is a powerful tool that can produce a wide variety of processor designs ranging from tiny embedded processors to complex multi-core systems. In this paper we extend the features of the Memory Management Unit of the Rocket Chip Generator and specifically the TLB hierarchy. TLBs are essential in terms of performance because they mitigate the overhead of frequent Page Table Walks, but may harm the critical path of the processor due to their size and/or associativity. In the original Rocket Chip implementation the L1 Instruction/Data TLB is fully-associative and the shared L2 TLB is direct-mapped. We lift these restrictions and design and implement configurable, set-associative L1 and L2 TLB templates that can create any organization from direct-mapped to fully-associative to achieve the desired ratio of performance and resource utilization, especially for larger TLBs. We evaluate different TLB configurations and present performance, area, and frequency results of our design using benchmarks from the SPEC2006 suite on the Xilinx ZCU102 FPGA.", "venue": "ArXiv", "authors": ["Nikolaos Charalampos Papadopoulos", "Vasileios  Karakostas", "Konstantinos  Nikas", "Nectarios  Koziris", "Dionisios N. Pnevmatikatos"], "year": 2020, "n_citations": 0}
{"id": 2943942, "s2_id": "c5fc1c58e95e4544552084729a40adf21592a271", "title": "Flat ORAM: A Simplified Write-Only Oblivious RAM Construction for Secure Processors", "abstract": "Oblivious RAM (ORAM) is a cryptographic primitive which obfuscates the access patterns to a storage, thereby preventing privacy leakage. So far in the current literature, only \u2018fully functional\u2019 ORAMs are widely studied which can protect, at a cost of considerable performance penalty, against the strong adversaries who can monitor all read and write operations. However, recent research has shown that information can still be leaked even if only the write access pattern (not reads) is visible to the adversary. For such weaker adversaries, a fully functional ORAM turns out to be an overkill, causing unnecessary overheads. Instead, a simple \u2018write-only\u2019 ORAM is sufficient, and, more interestingly, is preferred as it can offer far better performance and energy efficiency than a fully functional ORAM. In this work, we present Flat ORAM: an efficient write-only ORAM scheme which outperforms the closest existing write-only ORAM called HIVE. HIVE suffers from performance bottlenecks while managing the memory occupancy information vital for correctness of the protocol. Flat ORAM introduces a simple idea of Occupancy Map (OccMap) to efficiently manage the memory occupancy information resulting in far better performance. Our simulation results show that, compared to HIVE, Flat ORAM offers \n \n \n \n 50\n %\n \n \n \n performance gain on average and up to \n \n \n \n 80\n %\n \n \n \n energy savings.", "venue": "Cryptogr.", "authors": ["Syed Kamran Haider", "Marten van Dijk"], "year": 2019, "n_citations": 8}
{"id": 2948617, "s2_id": "3bf377193523166e9f2c828545326bbf59f24e4f", "title": "TaxoNN: A Light-Weight Accelerator for Deep Neural Network Training", "abstract": "Emerging intelligent embedded devices rely on Deep Neural Networks (DNNs) to be able to interact with the real-world environment. This interaction comes with the ability to retrain DNNs, since environmental conditions change continuously in time. Stochastic Gradient Descent (SGD) is a widely used algorithm to train DNNs by optimizing the parameters over the training data iteratively. In this work, first we present a novel approach to add the training ability to a baseline DNN accelerator (inference only) by splitting the SGD algorithm into simple computational elements. Then, based on this heuristic approach we propose TaxoNN, a light-weight accelerator for DNN training. TaxoNN can easily tune the DNN weights by reusing the hardware resources used in the inference process using a time-multiplexing approach and low-bitwidth units. Our experimental results show that TaxoNN delivers, on average, 0.97% higher misclassification rate compared to a full-precision implementation. Moreover, TaxoNN provides 2.1\u00d7 power saving and 1.65\u00d7 area reduction over the state-of-the-art DNN training accelerator.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Reza  Hojabr", "Kamyar  Givaki", "Kossar  Pourahmadi", "Parsa  Nooralinejad", "Ahmad  Khonsari", "Dara  Rahmati", "M. Hassan Najafi"], "year": 2020, "n_citations": 0}
{"id": 2948779, "s2_id": "a1b3eb0da6efbe55975753d3a92e109b62c28305", "title": "2.5D Root of Trust: Secure System-Level Integration of Untrusted Chiplets", "abstract": "For the first time, we leverage the 2.5D interposer technology to establish system-level security in the face of hardware- and software-centric adversaries. More specifically, we integrate chiplets (i.e., third-party hard intellectual property of complex functionality, like microprocessors) using a security-enforcing interposer. Such hardware organization provides a robust 2.5D root of trust for trustworthy, yet powerful and flexible, computation systems. The security paradigms for our scheme, employed firmly by design and construction, are: 1) stringent physical separation of trusted from untrusted components and 2) runtime monitoring. The system-level activities of all untrusted commodity chiplets are checked continuously against security policiesvia physically separated security features. Aside from the security promises, the good economics of outsourced supply chains are still maintained; the system vendor is free to procure chiplets from the open market, while only producing the interposer and assembling the 2.5D system oneself. We showcase our scheme using the Cortex-M0 core and the AHB-Lite bus by ARM, building a secure 64-core system with shared memories. We evaluate our scheme through hardware simulation, considering different threat scenarios. Finally, we devise a physical-design flow for 2.5D systems, based on commercial-grade design tools, to demonstrate and evaluate our 2.5D root of trust.", "venue": "IEEE Transactions on Computers", "authors": ["Mohammed  Nabeel", "Mohammed  Ashraf", "Satwik  Patnaik", "Vassos  Soteriou", "Ozgur  Sinanoglu", "Johann  Knechtel"], "year": 2020, "n_citations": 3}
{"id": 2951624, "s2_id": "5518a2e59b4878d08c2a857e6f40cb018501eac7", "title": "GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching", "abstract": "Analysis engines based on Graph Neural Networks (GNNs) are vital for many real-world problems that model relationships using large graphs. Challenges for a GNN hardware platform include the ability to (a) host a variety of GNNs, (b) handle high sparsity in input node feature vectors and the graph adjacency matrix and the accompanying random memory access patterns, and (c) maintain load-balanced computation in the face of uneven workloads induced by high sparsity and power-law vertex degree distributions in real datasets. The proposes GNNIE, an accelerator designed to run a broad range of GNNs. It tackles workload imbalance by (i) splitting node feature operands into blocks, (ii) reordering and redistributing computations, and (iii) using a flexible MAC architecture with low communication overheads among the processing elements. In addition, it adopts a graph partitioning scheme and a graph-specific caching policy that efficiently uses off-chip memory bandwidth that is well suited to the characteristics of real-world graphs. Random memory access effects are mitigated by partitioning and degree-aware caching to enable the reuse of high-degree vertices. GNNIE achieves average speedups of over 8890\u00d7 over a CPU and 295\u00d7 over a GPU over multiple datasets on graph attention networks (GATs), graph convolutional networks (GCNs), GraphSAGE, GINConv, and DiffPool, Compared to prior approaches, GNNIE achieves an average speedup of 9.74\u00d7 over HyGCN for GCN, GraphSAGE, and GINConv; HyGCN cannot implement GATs. GNNIE achieves an average speedup of 2.28\u00d7 over AWB-GCN (which runs only GCNs), despite using 3.4\u00d7 fewer processing units.", "venue": "ArXiv", "authors": ["Sudipta  Mondal", "Susmita Dey Manasi", "Kishor  Kunal", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 0}
{"id": 2952128, "s2_id": "2012e5549a60e0e7d969088a96f2780745e10b54", "title": "Parametric Estimation of the Ultimate Size of Hypercomputers", "abstract": "The performance of the emerging petaflops-scale supercomputers of the nearest future (hypercomputers) will be governed not only by the clock frequency of the processing nodes or by the width of the system bus, but also by such factors as the overall power consumption and the geometric size. In this paper, we study the influence of such parameters on one of the most important characteristics of a general purpose computer - on the degree of multithreading that must be present in an application to make the use of the hypercomputer justifiable. Our major finding is that for the class of applications with purely random memory access patterns \"super-fast computing\" and \"high-performance computing\" are essentially synonyms for \"massively-parallel computing.\"", "venue": "ArXiv", "authors": ["Dmitry  Zinoviev"], "year": 2011, "n_citations": 0}
{"id": 2956638, "s2_id": "4f2b84f7c1a9b98bf7da976d09c43f06d4741aaa", "title": "Effective Pre-Silicon Verification of Processor Cores by Breaking the Bounds of Symbolic Quick Error Detection", "abstract": "We present a novel approach to pre-silicon verification of processor designs. The purpose of pre-silicon verification is to find logic bugs in a design at an early stage and thus avoid timeand cost-intensive post-silicon debugging. Our approach relies on symbolic quick error detection (Symbolic QED, or SQED). SQED is targeted at finding logic bugs in a symbolic representation of a design by combining bounded model checking (BMC) with QED tests. QED tests are powerful in generating short sequences of instructions (traces) that trigger bugs. We extend an existing SQED approach with symbolic starting states. This way, we enable the BMC tool to select starting states arbitrarily when generating a trace. To avoid false positives, (e.g., traces starting in unreachable states that may not behave in accordance with the processor instructionset architecture), we define constraints to restrict the set of possible starting states. We demonstrate that these constraints, together with reasonable assumptions about the system behavior, allow us to avoid false positives. Using our approach, we discovered previously unknown bugs in open-source RISC-V processor cores that existing methods cannot detect. Moreover, our novel approach outperforms existing ones in the detection of bugs having long traces and in the detection of hardware Trojans, i.e., unauthorized modifications of a design.", "venue": "ArXiv", "authors": ["Karthik  Ganesan", "Florian  Lonsing", "Srinivasa Shashank Nuthakki", "Eshan  Singh", "Mohammad Rahmani Fadiheh", "Wolfgang  Kunz", "Dominik  Stoffel", "Clark  Barrett", "Subhasish  Mitra"], "year": 2021, "n_citations": 1}
{"id": 2965606, "s2_id": "12ca0dd2ce2c0c0a282bf583200d99a046999bb5", "title": "Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory", "abstract": "Realizing today\u2019s cloud-level artificial intelligence (AI) functionalities directly on devices distributed at the edge of the internet calls for edge hardware capable of processing multiple modalities of sensory data (e.g. video, audio) at unprecedented energy-efficiency. AI hardware architectures today cannot meet the demand due to a fundamental \u201cmemory wall\u201d: data movement between separate compute and memory units consumes large energy and incurs long latency. Resistive random-access memory (RRAM) based compute-in-memory (CIM) architectures promise to bring orders of magnitude energy-efficiency improvement by performing computation directly within memory, using intrinsic physical properties of RRAM devices. However, conventional approaches to CIM hardware design limit its functional flexibility necessary for processing diverse AI workloads, and must overcome hardware imperfections that degrade inference accuracy. Such trade-offs between efficiency, versatility and accuracy cannot be addressed by isolated improvements on any single level of the design. By co-optimizing across all hierarchies of the design from algorithms and architecture to circuits and devices, we present NeuRRAM the first multimodal edge AI chip using RRAM CIM to simultaneously deliver a high degree of versatility in reconfiguring a single chip for diverse model architectures, record energy-efficiency 5\uf0b4 8\uf0b4 better than prior art across various computational bit-precisions, and inference accuracy comparable to software models with 4-bit weights on all measured standard AI benchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image classification, 84.7% accuracy on Google speech command recognition, and a 70% reduction in image reconstruction error on a Bayesian image recovery task. This work paves a way towards building highly efficient and reconfigurable edge AI hardware platforms for the more demanding and heterogeneous AI applications of the future.", "venue": "ArXiv", "authors": ["Weier  Wan", "Rajkumar  Kubendran", "Clemens J. S. Schaefer", "Sukru Burc Eryilmaz", "Wenqiang  Zhang", "Dabin  Wu", "Stephen R. Deiss", "Priyanka  Raina", "He  Qian", "Bin  Gao", "Siddharth  Joshi", "Huaqiang  Wu", "H.-S. Philip Wong", "Gert  Cauwenberghs"], "year": 2021, "n_citations": 0}
{"id": 2966209, "s2_id": "2d0189942ee43874d7382b8f4e8409fb3075676f", "title": "Object-oriented approach to rapid custom instruction design", "abstract": "Due to continuous evolution of Systems-on-Chip (SoC), the complexity of their design and development has augmented exponentially. To deal with the ever-growing complexity of such embedded systems, we introduce, in this paper, an object-oriented approach to rapid SoC design using auto-generation of hardware custom instructions to simplify and accelerate the SoC design process. In our approach, a Data Flow Graph (DFG) is adopted as a representation of the arithmetic operation to convert it to a custom instruction. Then VHDL code will be automatically generated. The input C code is automatically updated for calling the new hardware components. To prove the effectiveness of the proposed approach, a Java source code framework named Automatic Custom Architecture generator (ACAgen) is developed. Experimental results on 3D sample application validate our approach and demonstrate how the proposed framework facilitates and accelerates the SoC design process at low costs.", "venue": "2012 IEEE Faible Tension Faible Consommation", "authors": ["Emna  Kallel", "Yassine  Aoudni", "Mohamed  Abid"], "year": 2012, "n_citations": 0}
{"id": 2966862, "s2_id": "b7aef11e8672a92615b5afec522bd10efa4a863c", "title": "3D-aCortex: An Ultra-Compact Energy-Efficient Neurocomputing Platform Based on Commercial 3D-NAND Flash Memories", "abstract": "The first contribution of this paper is the development of extremely dense, energy-efficient mixed-signal vector-by-matrix-multiplication (VMM) circuits based on the existing 3D-NAND flash memory blocks, without any need for their modification. Such compatibility is achieved using time-domain-encoded VMM design. Our detailed simulations have shown that, for example, the 5-bit VMM of 200-element vectors, using the commercially available 64-layer gate-all-around macaroni-type 3D-NAND memory blocks designed in the 55-nm technology node, may provide an unprecedented area efficiency of 0.14 um2/byte and energy efficiency of ~10 fJ/Op, including the input/output and other peripheral circuitry overheads. Our second major contribution is the development of 3D-aCortex, a multi-purpose neuromorphic inference processor that utilizes the proposed 3D-VMM blocks as its core processing units. We have performed rigorous performance simulations of such a processor on both circuit and system levels, taking into account non-idealities such as drain-induced barrier lowering, capacitive coupling, charge injection, parasitics, process variations, and noise. Our modeling of the 3D-aCortex performing several state-of-the-art neuromorphic-network benchmarks has shown that it may provide the record-breaking storage efficiency of 4.34 MB/mm2, the peak energy efficiency of 70.43 TOps/J, and the computational throughput up to 10.66 TOps/s. The storage efficiency can be further improved seven-fold by aggressively sharing VMM peripheral circuits at the cost of slight decrease in energy efficiency and throughput.", "venue": "Neuromorph. Comput. Eng.", "authors": ["Mohammad  Bavandpour", "Shubham  Sahay", "Mohammad Reza Mahmoodi", "Dmitri B. Strukov"], "year": 2021, "n_citations": 9}
{"id": 2969004, "s2_id": "2ca150a579d0564268d6c3ab2fcd79f549754433", "title": "Topics in asynchronous systems", "abstract": "In the paper we define and characterize the asynchronous systems from the point of view of their autonomy, determinism, order, non-anticipation, time invariance, symmetry, stability and other important properties. The study is inspired by the models of the asynchronous circuits.", "venue": "ArXiv", "authors": ["Serban E. Vlad"], "year": 2004, "n_citations": 3}
{"id": 2969230, "s2_id": "8002a5c3f115d85cce190d688390aee6c3b4c20e", "title": "RVCoreP-32IC: A high-performance RISC-V soft processor with an efficient fetch unit supporting the compressed instructions", "abstract": "In this paper, we propose a high-performance RISC-V soft processor with an efficient fetch unit supporting the compressed instructions targeting on FPGA. The compressed instruction extension in RISC-V can reduce the program size by about 25%. But it needs a complicated logic for the instruction fetch unit and has a significant impact on performance. We propose an instruction fetch unit that supports the compressed instructions while exhibiting high performance. Furthermore, we propose a RISC-V soft processor using this unit. We implement this proposed processor in Verilog HDL and verify the behavior using Verilog simulation and an actual Xilinx Atrix-7 FPGA board. We compare the results of some benchmarks and the amount of hardware with related works. DMIPS, CoreMark value, and Embench value of the proposed processor achieved 42.5%, 41.1% and 21.3% higher performance than the related work, respectively.", "venue": "ArXiv", "authors": ["Takuto  Kanamori", "Hiromu  Miyazaki", "Kenji  Kise"], "year": 2020, "n_citations": 0}
{"id": 2973970, "s2_id": "61a0f2d0743d2d8b8e6c99891f80e8afe5b2aafb", "title": "Fixed-Posit: A Floating-Point Representation for Error-Resilient Applications", "abstract": "Today, almost all computer systems use IEEE-754 floating point to represent real numbers. Recently, posit was proposed as an alternative to IEEE-754 floating point as it has better accuracy and a larger dynamic range. The configurable nature of posit, with varying number of regime and exponent bits, has acted as a deterrent to its adoption. To overcome this shortcoming, we propose fixed-posit representation where the number of regime and exponent bits are fixed, and present the design of a fixed-posit multiplier. We evaluate the fixed-posit multiplier on error-resilient applications of AxBench and OpenBLAS benchmarks as well as neural networks. The proposed fixed-posit multiplier has 47%, 38.5%, 22% savings for power, area and delay respectively when compared to posit multipliers and up to 70%, 66%, 26% savings in power, area and delay respectively when compared to 32-bit IEEE-754 multiplier. These savings are accompanied with minimal output quality loss (1.2% average relative error) across OpenBLAS and AxBench workloads. Further, for neural networks like ResNet-18 on ImageNet we observe a negligible accuracy loss (0.12%) on using the fixed-posit multiplier.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Varun  Gohil", "Sumit  Walia", "Joycee  Mekie", "Manu  Awasthi"], "year": 2021, "n_citations": 0}
{"id": 2974940, "s2_id": "5e0b9af562d8c29984f779b0708f079181f658a9", "title": "Latency of Concatenating Unlicensed LPWAN with Cellular IoT: An Experimental QoE Study", "abstract": "Developing low-power wide-area network (LPWAN) solutions that are efficient to adopt, deploy and maintain are vital for smart cities. The poor quality-of-service of unlicensed LPWAN, and the high service cost of LTE-M/NB-IoT are key disadvantages of these technologies. Concatenating unlicensed with licensed LPWANs can overcome these limitations and harness their benefits. However, a concatenated LPWAN architecture will inevitably result in excess latency which may impact users\u2019 quality-of-experience (QoE). To evaluate the real-life feasibility of this system, we first propose a concatenated LPWAN architecture and experimentally measure the statistics of end-to-end (E2E) latencies. The concatenated delay margin is determined by benchmarking the latencies with different LPWAN architecture schemes, namely with unlicensed IoT (standalone LoRa), cellular IoT (standalone LTE-M), and concatenated IoT (LoRa interfaced with LTE-M). Through extensive experimental measurement campaigns of 30,000 data points of E2E latencies, we show that the excess delay due to LPWAN interfacing introduces on average less than 300 milliseconds. With a users\u2019 QoE satisfaction of 95%, we also found that concatenated LPWAN outperforms unlicensed IoT by roughly 1.5 s. Overall, the result suggests that a concatenated LPWAN is technically feasible and offers an affordable alternative for real-world smart city deployment.", "venue": "2021 IEEE 94th Vehicular Technology Conference (VTC2021-Fall)", "authors": ["Alvin  Ramoutar", "Zohreh  Motamedi", "Mouhamed  Abdulla"], "year": 2021, "n_citations": 0}
{"id": 2977083, "s2_id": "0d10d86e82981be60e9230fd06a337e240b28843", "title": "Opportunities and Challenges for Next Generation Computing", "abstract": "Computing has dramatically changed nearly every aspect of our lives, from business and agriculture to communication and entertainment. As a nation, we rely on computing in the design of systems for energy, transportation and defense; and computing fuels scientific discoveries that will improve our fundamental understanding of the world and help develop solutions to major challenges in health and the environment. Computing has changed our world, in part, because our innovations can run on computers whose performance and cost-performance has improved a million-fold over the last few decades. A driving force behind this has been a repeated doubling of the transistors per chip, dubbed Moore's Law. A concomitant enabler has been Dennard Scaling that has permitted these performance doublings at roughly constant power, but, as we will see, both trends face challenges. Consider for a moment the impact of these two trends over the past 30 years. A 1980's supercomputer (e.g. a Cray 2) was rated at nearly 2 Gflops and consumed nearly 200 KW of power. At the time, it was used for high performance and national-scale applications ranging from weather forecasting to nuclear weapons research. A computer of similar performance now fits in our pocket and consumes less than 10 watts. What would be the implications of a similar computing/power reduction over the next 30 years - that is, taking a petaflop-scale machine (e.g. the Cray XK7 which requires about 500 KW for 1 Pflop (=1015 operations/sec) performance) and repeating that process? What is possible with such a computer in your pocket? How would it change the landscape of high capacity computing? In the remainder of this paper, we articulate some opportunities and challenges for dramatic performance improvements of both personal to national scale computing, and discuss some \"out of the box\" possibilities for achieving computing at this scale.", "venue": "ArXiv", "authors": ["Gregory D. Hager", "Mark D. Hill", "Katherine  Yelick"], "year": 2020, "n_citations": 1}
{"id": 2977690, "s2_id": "7e0bb72bb3e4ce893088d8619e2bb476ecac7f33", "title": "Mixed-level identification of fault redundancy in microprocessors", "abstract": "A new high-level implementation independent functional fault model for control faults in microprocessors is introduced. The fault model is based on the instruction set, and is specified as a set of data constraints to be satisfied by test data generation. We show that the high-level test, which satisfies these data constraints, will be sufficient to guarantee the detection of all non-redundant low level faults. The paper proposes a simple and fast simulation based method of generating test data, which satisfy the constraints prescribed by the proposed fault model, and a method of evaluating the high-level control fault coverage for the proposed fault model and for the given test. A method is presented for identification of the high-level redundant faults, and it is shown that a test, which provides 100% coverage of non-redundant high-level faults, will also guarantee 100% non-redundant SAF coverage, whereas all gate-level SAF not covered by the test are identified as redundant. Experimental results of test generation for the execution part of a microprocessor support the results presented in the paper.", "venue": "2019 IEEE Latin American Test Symposium (LATS)", "authors": ["Adeboye Stephen Oyeniran", "Raimund  Ubar", "Maksim  Jenihhin", "Cemil Cem G\u00fcrsoy", "Jaan  Raik"], "year": 2019, "n_citations": 2}
{"id": 2979212, "s2_id": "3dd7af3b02e3decb908960e4399e25c3e049c41f", "title": "Inner Loop Optimizations in Mapping Single Threaded Programs to Hardware", "abstract": "In the context of mapping high-level algorithms to hardware, we consider the basic problem of generating an efficient hardware implementation of a single threaded program, in particular, that of an inner loop. We describe a control-flow mechanism which provides dynamic loop-pipelining capability in hardware, so that multiple iterations of an arbitrary inner loop can be made simultaneously active in the generated hardware, We study the impact of this loop-pipelining scheme in conjunction with source-level loop-unrolling. In particular, we apply this technique to some common loop kernels: regular kernels such as the fast-fourier transform and matrix multiplication, as well as an example of an inner loop whose body has branching. The resulting resulting hardware descriptions are synthesized to an FPGA target, and then characterized for performance and resource utilization. We observe that the use of dynamic loop-pipelining mechanism alone typically results in a significant improvements in the performance of the hardware. If the loop is statically unrolled and if loop-pipelining is applied to the unrolled program, then the performance improvement is still substantial. When dynamic loop pipelining is used in conjunction with static loop unrolling, the improvement in performance ranges from 6X to 20X (in terms of number of clock cycles needed for the computation) across the loop kernels that we have studied. These optimizations do have a hardware overhead, but, in spite of this, we observe that the joint use of these loop optimizations not only improves performance, but also the performance/cost ratio of the resulting hardware.", "venue": "ArXiv", "authors": ["Madhav  Desai"], "year": 2014, "n_citations": 0}
{"id": 2979396, "s2_id": "e8df11fa36a3eeb42df09c6b14d24a180abbb801", "title": "STBPU: A Reasonably Safe Branch Predictor Unit", "abstract": "Modern processors have suffered a deluge of dangerous side channel and speculative execution attacks that exploit vulnerabilities rooted in branch predictor units (BPU). Many such attacks exploit the shared use of the BPU between unrelated processes, which allows malicious processes to retrieve sensitive data or enable speculative execution attacks. Attacks that exploit collisions between different branch instructions inside the BPU are among the most dangerous. Various protections and mitigations are proposed such as CPU microcode updates, secured cache designs, fencing mechanisms, invisible speculations. While some effectively mitigate speculative execution attacks, they overlook BPU as an attack vector, leaving BPU prone to malicious collisions and resulting critical penalty such as advanced micro-op cache attacks. Furthermore, some mitigations severely hamper the accuracy of the BPU resulting in increased CPU performance overhead. To address these, we present the secret token branch predictor unit (STBPU), a branch predictor design that mitigates collision-based speculative execution attacks and BPU side channel whilst incurring little to no performance overhead. STBPU achieves this by customizing inside data representations for each software entity requiring isolation. To prevent more advanced attacks, STBPU monitors hardware events and preemptively changes how STBPU data is stored and interpreted.", "venue": "ArXiv", "authors": ["Tao  Zhang", "Timothy  Lesch", "Kenneth  Koltermann", "Dmitry  Evtyushkin"], "year": 2021, "n_citations": 0}
{"id": 2979995, "s2_id": "0da306fab4c3e7031bf6de679275c3957f963a02", "title": "A fast concurrent power-thermal model for sub-100 nm digital ICs", "abstract": "As technology scales down, the static power is expected to become a significant fraction of the total power. The exponential dependence of static power with the operating temperature makes the thermal profile estimation of high-performance IC a key issue to compute the total power dissipated in the next-generation. In this paper we present accurate and compact analytical models to estimate the static power dissipation and the temperature of operation of CMOS gates. The models are the fundamentals of a performance estimation tool in which numerical procedures are avoided for any computation to set a faster estimation and optimization. The models developed are compared to measurements and SPICE simulations for a 0.12 /spl mu/m technology showing excellent results.", "venue": "Design, Automation and Test in Europe", "authors": ["Jos\u00e9 Luis Rossell\u00f3", "Vicent  Canals", "Sebasti\u00e0 A. Bota", "Ali  Keshavarzi", "Jaume  Segura"], "year": 2005, "n_citations": 5}
{"id": 2995117, "s2_id": "6daaa1bc0fe6cb0d1dc9170789ad6de3a2c9035b", "title": "Analog vs. Digital Spatial Transforms: A Throughput, Power, and Area Comparison", "abstract": "Spatial linear transforms that process multiple parallel analog signals to simplify downstream signal processing find widespread use in multi-antenna communication systems, machine learning inference, data compression, audio and ultrasound applications, among many others. In the past, a wide range of mixed-signal as well as digital spatial transform circuits have been proposed\u2014it is, however, a longstanding question whether analog or digital transforms are superior in terms of throughput, power, and area. In this paper, we focus on Hadamard transforms and perform a systematic comparison of state-of-the-art analog and digital circuits implementing spatial transforms in the same 65 nm CMOS technology. We analyze the trade-offs between throughput, power, and area, and we identify regimes in which mixed-signal or digital Hadamard transforms are preferable. Our comparison reveals that (i) there is no clear winner and (ii) analog-to-digital conversion is often dominating area and energy efficiency\u2014and not the spatial transform.", "venue": "2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["Zephan M. Enciso", "Seyed  Hadi Mirfarshbafan", "Oscar  Casta\u00f1eda", "Clemens JS. Schaefer", "Christoph  Studer", "Siddharth  Joshi"], "year": 2020, "n_citations": 2}
{"id": 3000109, "s2_id": "683f0137d0944e07a86c3d9601d92f99fd80811a", "title": "FPGA Implementation of High Speed Baugh-Wooley Multiplier Using Decomposition Logic", "abstract": "The Baugh-Wooley algorithm is a well-known iterative algorithm for performing multiplication in digital signal processing applications. Decomposition logic is used with Baugh-Wooley algorithm to enhance the speed and to reduce the critical path delay. In this paper a high speed multiplier is designed and implemented using decomposition logic and Baugh-Wooley algorithm. The result is compared with booth multiplier. FPGA based architecture is presented and design has been implemented using Xilinx 12.3 device.", "venue": "ArXiv", "authors": ["Ananda  Kiran", "Navdeep  Prashar"], "year": 2015, "n_citations": 3}
{"id": 3011615, "s2_id": "fc27f18b50a95b6c07ccb517cd5c385582d9a4cf", "title": "HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation", "abstract": "Tensor computations overwhelm traditional general-purpose computing devices due to the large amounts of data and operations of the computations. They call for a holistic solution composed of both hardware acceleration and software mapping. Hardware/software (HW/SW) co-design optimizes the hardware and software in concert and produces high-quality solutions. There are two main challenges in the co-design flow. First, multiple methods exist to partition tensor computation and have different impacts on performance and energy efficiency. Besides, the hardware part must be implemented by the intrinsic functions of spatial accelerators. It is hard for programmers to identify and analyze the partitioning methods manually. Second, the overall design space composed of HW/SW partitioning, hardware optimization, and software optimization is huge. The design space needs to be efficiently explored. To this end, we propose an agile co-design approach HASCO that provides an efficient HW/SW solution to dense tensor computation. We use tensor syntax trees as the unified IR, based on which we develop a two-step approach to identify partitioning methods. For each method, HASCO explores the hardware and software design spaces. We propose different algorithms for the explorations, as they have distinct objectives and evaluation costs. Concretely, we develop a multi-objective Bayesian optimization algorithm to explore hardware optimization. For software optimization, we use heuristic and Q-learning algorithms. Experiments demonstrate that HASCO achieves a 1.25X to 1.44X latency reduction through HW/SW co-design compared with developing the hardware and software separately.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Qingcheng  Xiao", "Size  Zheng", "Bingzhe  Wu", "Pengcheng  Xu", "Xuehai  Qian", "Yun  Liang"], "year": 2021, "n_citations": 4}
{"id": 3012195, "s2_id": "efd0e7fbdbccf5e073852fa6c79eded7ac6fb7e0", "title": "Fallout: Reading Kernel Writes From User Space", "abstract": "Recently, out-of-order execution, an important performance optimization in modern high-end processors, has been revealed to pose a significant security threat, allowing information leaks across security domains. In particular, the Meltdown attack leaks information from the operating system kernel to user space, completely eroding the security of the system. To address this and similar attacks, without incurring the performance costs of software countermeasures, Intel includes hardware-based defenses in its recent Coffee Lake R processors. \nIn this work, we show that the recent hardware defenses are not sufficient. Specifically, we present Fallout, a new transient execution attack that leaks information from a previously unexplored microarchitectural component called the store buffer. We show how unprivileged user processes can exploit Fallout to reconstruct privileged information recently written by the kernel. We further show how Fallout can be used to bypass kernel address space randomization. Finally, we identify and explore microcode assists as a hitherto ignored cause of transient execution. \nFallout affects all processor generations we have tested. However, we notice a worrying regression, where the newer Coffee Lake R processors are more vulnerable to Fallout than older generations.", "venue": "ArXiv", "authors": ["Marina  Minkin", "Daniel  Moghimi", "Moritz  Lipp", "Michael  Schwarz", "Jo Van Bulck", "Daniel  Genkin", "Daniel  Gruss", "Frank  Piessens", "Berk  Sunar", "Yuval  Yarom"], "year": 2019, "n_citations": 44}
{"id": 3020995, "s2_id": "3a00bab02bd541ffdf40c40ac8640d855008e898", "title": "Applications and Techniques for Fast Machine Learning in Science", "abstract": "In this community review report, we discuss applications and techniques for fast machine learning (ML) in science -- the concept of integrating power ML methods into the real-time experimental data processing loop to accelerate scientific discovery. The material for the report builds on two workshops held by the Fast ML for Science community and covers three main areas: applications for fast ML across a number of scientific domains; techniques for training and implementing performant and resource-efficient ML algorithms; and computing architectures, platforms, and technologies for deploying these algorithms. We also present overlapping challenges across the multiple scientific domains where common solutions can be found. This community report is intended to give plenty of examples and inspiration for scientific discovery through integrated and accelerated ML solutions. This is followed by a high-level overview and organization of technical advances, including an abundance of pointers to source material, which can enable these breakthroughs.", "venue": "ArXiv", "authors": ["Allison McCarn Deiana", "Nhan  Tran", "Joshua  Agar", "Michaela  Blott", "Giuseppe Di Guglielmo", "Javier  Duarte", "Philip C. Harris", "Scott  Hauck", "Mia  Liu", "Mark S. Neubauer", "Jennifer  Ngadiuba", "Seda Ogrenci Memik", "Maurizio  Pierini", "Thea  Aarrestad", "Steffen  B\u00e4hr", "J\u00fcrgen  Becker", "Anne-Sophie  Berthold", "Richard J. Bonventre", "Tomas E. Muller Bravo", "Markus  Diefenthaler", "Zhen  Dong", "Nick  Fritzsche", "Amir  Gholami", "Ekaterina  Govorkova", "Kyle J. Hazelwood", "Christian  Herwig", "Babar  Khan", "Sehoon  Kim", "Thomas  Klijnsma", "Yaling  Liu", "Kin Ho Lo", "Tri  Nguyen", "Gianantonio  Pezzullo", "Seyedramin  Rasoulinezhad", "Ryan A. Rivera", "Kate  Scholberg", "Justin  Selig", "Sougata  Sen", "Dmitri  Strukov", "William  Tang", "Savannah  Thais", "Kai Lukas Unger", "Ricardo  Vilalta", "Belinavon  Krosigk", "Thomas K. Warburton", "Maria Acosta Flechas", "Anthony  Aportela", "Thomas  Calvet", "Leonardo  Cristella", "Daniel  Diaz", "Caterina  Doglioni", "Maria Domenica Galati", "Elham E Khoda", "Farah  Fahim", "Davide  Giri", "Benjamin  Hawks", "Duc  Hoang", "Burt  Holzman", "Shih-Chieh  Hsu", "Sergo  Jindariani", "Iris  Johnson", "Raghav  Kansal", "Ryan  Kastner", "Erik  Katsavounidis", "Jeffrey  Krupa", "Pan  Li", "Sandeep  Madireddy", "Ethan  Marx", "Patrick  McCormack", "Andres  Meza", "Jovan  Mitrevski", "Mohammed Attia Mohammed", "Farouk  Mokhtar", "Eric  Moreno", "Srishti  Nagu", "Rohin  Narayan", "Noah  Palladino", "Zhiqiang  Que", "Sang Eon Park", "Subramanian  Ramamoorthy", "Dylan  Rankin", "Simon  Rothman", "Ashish  Sharma", "Sioni  Summers", "Pietro  Vischia", "Jean-Roch  Vlimant", "Olivia  Weng"], "year": 2021, "n_citations": 2}
{"id": 3021906, "s2_id": "661d1b49405128853fbd41d25ca51c2cfa8a9c8f", "title": "Effects of VLSI Circuit Constraints on Temporal-Coding Multilayer Spiking Neural Networks", "abstract": "The spiking neural network (SNN) has been attracting considerable attention not only as a mathematical model for the brain, but also as an energy-efficient information processing model for real-world applications. In particular, SNNs based on temporal coding are expected to be much more efficient than those based on rate coding, because the former requires substantially fewer spikes to carry out tasks. As SNNs are continuous-state and continuous-time models, it is favorable to implement them with analog VLSI circuits. However, the construction of the entire system with continuous-time analog circuits would be infeasible when the system size is very large. Therefore, mixed-signal circuits must be employed, and the time discretization and quantization of the synaptic weights are necessary. Moreover, the analog VLSI implementation of SNNs exhibits non-idealities, such as the effects of noise and device mismatches, as well as other constraints arising from the analog circuit operation. In this study, we investigated the effects of the time discretization and/or weight quantization on the performance of SNNs. Furthermore, we elucidated the effects the lower bound of the membrane potentials and the temporal fluctuation of the firing threshold. Finally, we propose an optimal approach for the mapping of mathematical SNN models to analog circuits with discretized time.", "venue": "ArXiv", "authors": ["Yusuke  Sakemi", "Takashi  Morie", "Takeo  Hosomi", "Kazuyuki  Aihara"], "year": 2021, "n_citations": 0}
{"id": 3022549, "s2_id": "c83edee16219e6a9bfa23e54a9276290ee43e826", "title": "Challenges and Obstacles Towards Deploying Deep Learning Models on Mobile Devices", "abstract": "From computer vision and speech recognition to forecasting trajectories in autonomous vehicles, deep learning approaches are at the forefront of so many domains. Deep learning models are developed using plethora of high-level, generic frameworks and libraries. Running those models on the mobile devices require hardware-aware optimizations and in most cases converting the models to other formats or using a third-party framework. In reality, most of the developed models need to undergo a process of conversion, adaptation, and, in some cases, full retraining to match the requirements and features of the framework that is deploying the model on the target platform. Variety of hardware platforms with heterogeneous computing elements, from wearable devices to high-performance GPU clusters are used to run deep learning models. In this paper, we present the existing challenges, obstacles, and practical solutions towards deploying deep learning models on mobile devices.", "venue": "ArXiv", "authors": ["Hamid  Tabani", "Ajay  Balasubramaniam", "Elahe  Arani", "Bahram  Zonooz"], "year": 2021, "n_citations": 0}
{"id": 3023950, "s2_id": "9d6acac70b2d1fdb861a08b00766ef263109cd7f", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks", "abstract": "The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.", "venue": "ArXiv", "authors": ["Torsten  Hoefler", "Dan  Alistarh", "Tal  Ben-Nun", "Nikoli  Dryden", "Alexandra  Peste"], "year": 2021, "n_citations": 37}
{"id": 3027930, "s2_id": "60f1877fa3872afcd5e6ee01312cf7bd2df726fe", "title": "CleaNN: Accelerated Trojan Shield for Embedded Neural Networks", "abstract": "We propose Cleann, the first end-to-end framework that enables online mitigation of Trojans for embedded Deep Neural Network (DNN) applications. A Trojan attack works by injecting a backdoor in the DNN while training; during inference, the Trojan can be activated by the specific backdoor trigger. What differentiates Cleann from the prior work is its lightweight methodology which recovers the ground-truth class of Trojan samples without the need for labeled data, model retraining, or prior assumptions on the trigger or the attack. We leverage dictionary learning and sparse approximation to characterize the statistical behavior of benign data and identify Trojan triggers. Cleann is devised based on algorithm/hardware co-design and is equipped with specialized hardware to enable efficient real-time execution on resource-constrained embedded platforms. Proof of concept evaluations on Cleann for the state-of-the-art Neural Trojan attacks on visual benchmarks demonstrate its competitive advantage in terms of attack resiliency and execution overhead.", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Mojan  Javaheripi", "Mohammad  Samragh", "Gregory  Fields", "Tara  Javidi", "Farinaz  Koushanfar"], "year": 2020, "n_citations": 7}
{"id": 3030031, "s2_id": "50cace21faf3c4cef7ef618b9d0e5e40fce4bcd4", "title": "Generalized Fault-Tolerance Topology Generation for Application-Specific Network-on-Chips", "abstract": "The network-on-chips (NoCs)-based communication architecture is a promising candidate for addressing communication bottlenecks in many-core processors and neural network processors. In this article, we consider the generalized fault-tolerance topology generation problem, where the link (physical channel) or switch failures can happen, for application-specific NoCs (ASNoCs). With a user-defined maximum number of faults, <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula>, we propose an integer linear programming (ILP)-based method to generate ASNoC topologies, which can tolerate at most <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula> faults in switches or links. Given the communication requirements between cores and their floorplan, we first propose a convex-cost-flow-based method to solve a core mapping (CM) problem for building connections between the cores and switches. Second, an ILP-based method is proposed to solve the routing path allocation (PA) problem, where <inline-formula> <tex-math notation=\"LaTeX\">$K+1$ </tex-math></inline-formula> switch-disjoint routing paths are allocated for every communication flow between the cores. Finally, to reduce switch sizes, we propose to share the switch ports for the connections between the cores and switches and formulate the port sharing problem as a clique-partitioning problem, which is solved by iteratively finding a set of the maximum cliques. Additionally, we propose an ILP-based method to simultaneously solve the CM and routing PA problems when only physical link failures are considered. The experimental results show that the power consumption of fault-tolerance topologies increases almost linearly with <inline-formula> <tex-math notation=\"LaTeX\">$K$ </tex-math></inline-formula> because of the routing path redundancy for fault tolerance. When both switch faults and link faults are considered, port sharing can reduce the average power consumption of fault-tolerance topologies with <inline-formula> <tex-math notation=\"LaTeX\">$K=1$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=\"LaTeX\">$K=2$ </tex-math></inline-formula>, and <inline-formula> <tex-math notation=\"LaTeX\">$K=3$ </tex-math></inline-formula> by 18.08%, 28.88%, and 34.20%, respectively. When considering only the physical link faults, the experimental results show that compared to the fault-tolerant topology generation (FTTG) algorithm, the proposed method reduces power consumption and hop count by 10.58% and 6.25%, respectively; compared to the de Bruijn Digraph (DBG)-based method, the proposed method reduces power consumption and hop count by 21.72% and 9.35%, respectively.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Song  Chen", "Mengke  Ge", "Zhigang  Li", "Jinglei  Huang", "Qi  Xu", "Feng  Wu"], "year": 2020, "n_citations": 4}
{"id": 3031421, "s2_id": "6fcfffd9af9a5cd48777132a21ca34e233390ce2", "title": "On the Performance Potential of Speculative Execution based on Branch and Value Prediction", "abstract": "Fluid Stochastic Petri Nets are used to capture the dynamic behavior of an ILP processor, and discrete-event simulation is applied to assess the performance potential of predictions and speculative execution in boosting the performance of ILP processors that fetch, issue, execute and commit a large number of instructions per cycle.", "venue": "ArXiv", "authors": ["Pece  Mitrevski", "Marjan  Gusev"], "year": 2013, "n_citations": 1}
{"id": 3035124, "s2_id": "6132771388b07af51fdbf8de99897eb99df21d89", "title": "An Architectural Approach for Decoding and Distributing Functions in FPUs in a Functional Processor System", "abstract": "The main goal of this research is to develop the concepts of a revolutionary processor system called Functional Processor System. The fairly novel work carried out in this proposal concentrates on decoding of function pipelines and distributing it in FPUs as a part of scheduling approach. As the functional programs are super-level programs that entails requirements only at functional level, decoding of functions and distribution of functions in the heterogeneous functional processor units are a challenge. We explored the possibilities of segregation of the functions from the application program and distributing the functions on the relevant FPUs by using address mapping techniques. Here we pursue the perception of feeding the functions into the processor farm rather than the processor fetching the instructions or functions and executing it. This work is carried out at theoretical levels and it requires a long way to go in the realization of this work in hardware perhaps with a large industrial team with a pragmatic time frame.", "venue": "ArXiv", "authors": ["T. R. Gopalakrishnan Nair", "R.  Selvarani", "H. K. Krutthika"], "year": 2010, "n_citations": 0}
{"id": 3040298, "s2_id": "3c6e9c64f4a013248e3ab65c9b2c1da5fd2b84ce", "title": "Bit Parallel 6T SRAM In-memory Computing with Reconfigurable Bit-Precision", "abstract": "This paper presents 6T SRAM cell-based bit-parallel in-memory computing (IMC) architecture to support various computations with reconfigurable bit-precision. In the proposed technique, bit-line computation is performed with a short WL followed by BL boosting circuits, which can reduce BL computing delays. By per-forming carry-propagation between each near-memory circuit, bit-parallel complex computations are also enabled by iterating operations with low latency. In addition, reconfigurable bit-precision is also supported based on carry-propagation size. Our 128KB in/near memory computing architecture has been implemented using a 28nm CMOS process, and it can achieve 2.25GHz clock frequency at 0.9V with 5.2% of area overhead. The proposed architecture also achieves 0.68, 8.09 TOPS/W for the parallel addition and multiplication, respectively. In addition, the proposed work also supports a wide range of supply voltage, from 0.6V to 1.1V.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Kyeongho  Lee", "Jinho  Jeong", "Sungsoo  Cheon", "Woong  Choi", "Jongsun  Park"], "year": 2020, "n_citations": 3}
{"id": 3045445, "s2_id": "5cd05b92dfd8ba18c7af987b81634f1f64649676", "title": "Architectural-Space Exploration of Heterogeneous Reliability and Checkpointing Modes for Out-of-Order Superscalar Processors", "abstract": "State-of-the-art reliability techniques and mechanisms deploy full-scale redundancy, like double or triple modular redundancy (DMR, TMR), on different layers of the computing stack to detect and/or correct such transient faults. However, the techniques relying on full-scale redundancy incur significant area, performance, and/or power overheads, which might not always be feasible/practical due to system constraints such as deadlines and available power budget for the full chip (or a processor core). In this work, we propose a novel design methodology to generate and explore the architectural-space of heterogeneous reliability modes for out-of-order superscalar multi-core processors. These heterogeneous modes enable varying reliability and power/area trade-offs, from which an optimal configuration can be chosen at run time to meet the reliability requirements of a given system while reducing the corresponding power overheads (or solving the inverse problem, i.e., maximizing the reliability under a given power constraint). Our experimental results show that a pareto-optimal heterogeneous reliability mode reduces the core vulnerability by 87%, on average, across multiple application workloads, with area and power overheads of 10% and 43%, respectively. To further enhance the design space of heterogeneous reliability modes, we investigate the effectiveness of combining different processor state compression techniques like Distributed Multi-threaded Checkpointing (DMTCP), Hash-based Incremental Checkpointing (HBICT) and GNU zip, such that the correct processor state can be recovered once a fault is detected. We reduced the checkpoint sizes by a factor of $\\sim 6\\times $ using a unique combination of different state compression techniques.", "venue": "IEEE Access", "authors": ["Bharath Srinivas Prabakaran", "Mihika  Dave", "Florian  Kriebel", "Semeen  Rehman", "Muhammad  Shafique"], "year": 2019, "n_citations": 1}
{"id": 3049791, "s2_id": "434a33d60f997b4a59411c8cbed3d777c2ec9098", "title": "HALF: Holistic Auto Machine Learning for FPGAs", "abstract": "Deep Neural Networks (DNNs) are capable of solving complex problems in domains related to embedded systems, such as image and natural language processing. To efficiently implement DNNs on a specific FPGA platform for a given cost criterion, e.g., energy efficiency, an enormous amount of design parameters must be considered from the topology down to the final hardware implementation. Interdependencies between the different design layers must be taken into account and explored efficiently, making it hardly possible to find optimized solutions manually. An automatic, holistic design approach can improve the quality of DNN implementations on FPGA significantly. To this end, we present a cross-layer design space exploration methodology. It comprises optimizations starting from a hardware-aware topology search for DNNs down to the final optimized implementation for a given FPGA platform. The methodology is implemented in our Holistic Auto machine Learning for FPGAs (HALF) framework, which combines an evolutionary search algorithm, various optimization steps, and a library of parametrizable hardware DNN modules. HALF automates both the exploration process and the implementation of optimized solutions on a target FPGA platform for various applications. We demonstrate the performance of HALF on a medical use case for arrhythmia detection for three different design goals, i.e., low-energy, low-power, and high-throughput. Our FPGA implementation outperforms a TensorRT optimized model on an Nvidia Jetson platform in both throughput and energy consumption.", "venue": "2021 31st International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Jonas  Ney", "Dominik Marek Loroch", "Vladimir  Rybalkin", "Nico  Weber", "Jens  Kr\u00fcger", "Norbert  Wehn"], "year": 2021, "n_citations": 0}
{"id": 3050397, "s2_id": "120eb1aeba39e4942275f4aa0df0de5a19e68ab5", "title": "SSR: A Stall Scheme Reducing Bubbles in Load-Use Hazard of RISC-V Pipeline", "abstract": "Modern processors usually adopt pipeline structure and often load data from memory. At that point, the load-use hazard will inevitably occur, which usually stall the pipeline and reduce performance. This paper introduces and compares two schemes to solve load-use hazard. One is the traditional scheme that detect hazard between ID stage and EXE stage, which stalls the pipeline and insert bubbles between the two instructions. In the scheme we proposed, we add a simple bypass unit between EXE and MEM stage that disables the stall of load-use hazard caused by the traditional scheme, which can reduce the bubble between the two instructions. It's quite a considerable benefit in eliminating bubbles especially in the long pipeline or programs of plenty load instructions. The scheme was implemented in the open source RISC-V SoC generator Rocket-chip and synthesized in SMIC 130-nm technology. The results show that the performance of the latter scheme is increased by 6.9% in the Dhrystone benchmark with the reasonable cost of area and power.", "venue": "AINA", "authors": ["Dongchu  Su", "Yong  Li", "Bo  Yuan"], "year": 2020, "n_citations": 0}
{"id": 3051634, "s2_id": "519377ce0b9b71d163553b68d48b6e99ce1acb64", "title": "P4-Compatible High-Level Synthesis of Low Latency 100 Gb/s Streaming Packet Parsers in FPGAs", "abstract": "Packet parsing is a key step in SDN-aware devices. Packet parsers in SDN networks need to be both reconfigurable and fast, to support the evolving network protocols and the increasing multi-gigabit data rates. The combination of packet processing languages with FPGAs seems to be the perfect match for these requirements. In this work, we develop an open-source FPGA-based configurable architecture for arbitrary packet parsing to be used in SDN networks. We generate low latency and high-speed streaming packet parsers directly from a packet processing program. Our architecture is pipelined and entirely modeled using templated \\textttC++ classes. The pipeline layout is derived from a parser graph that corresponds to a P4 code after a series of graph transformation rounds. The RTL code is generated from the \\textttC++ description using Xilinx Vivado HLS and synthesized with Xilinx Vivado. Our architecture achieves a \\SI100 \\giga\\bit/\\second data rate in a Xilinx Virtex-7 FPGA while reducing the latency by 45% and the LUT usage by 40% compared to the state-of-the-art.", "venue": "FPGA", "authors": ["Jeferson Santiago da Silva", "Fran\u00e7ois R. Boyer", "J. M. Pierre Langlois"], "year": 2018, "n_citations": 23}
{"id": 3055644, "s2_id": "2edca04e97180ad7dc08a130a7ed0670af32d700", "title": "3U-EdgeAI: Ultra-Low Memory Training, Ultra-Low Bitwidth Quantization, and Ultra-Low Latency Acceleration", "abstract": "The deep neural network (DNN) based AI applications on the edge require both low-cost computing platforms and high-quality services. However, the limited memory, computing resources, and power budget of the edge devices constrain the effectiveness of the DNN algorithms. Developing edge-oriented AI algorithms and implementations (e.g., accelerators) is challenging. In this paper, we summarize our recent efforts for efficient on-device AI development from three aspects, including both training and inference. First, we present on-device training with ultra-low memory usage. We propose a novel rank-adaptive tensor-based tensorized neural network model, which offers orders-of-magnitude memory reduction during training. Second, we introduce an ultra-low bitwidth quantization method for DNN model compression, achieving the state-of-the-art accuracy under the same compression ratio. Third, we introduce an ultra-low latency DNN accelerator design, practicing the software/hardware co-design methodology. This paper emphasizes the importance and efficacy of training, quantization and accelerator design, and calls for more research breakthroughs in the area for AI on the edge.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Yao  Chen", "Cole  Hawkins", "Kaiqi  Zhang", "Zheng  Zhang", "Cong  Hao"], "year": 2021, "n_citations": 0}
{"id": 3058470, "s2_id": "db40c5677c8e853d0de727e0cb750bb7ed2a9077", "title": "Automatic latency balancing in VHDL-implemented complex pipelined systems", "abstract": "Balancing (equalization) of latency in parallel paths in the pipelined data processing system is an important problem. Without that the data from different paths arrive at the processing blocks in different clock cycles, and incorrect results are produced. Manual correction of latencies is a tedious and error-prone work. This paper presents an automatic method of latency equalization in systems described in VHDL. The method is based on simulation and is portable between different simulation and synthesis tools. The method does not increase the complexity of the synthesized design comparing to the solution based on manual latency adjustment. The example implementation of the proposed methodology together with a simple design demonstrating its use is available as an open source project under BSD license.", "venue": "ArXiv", "authors": ["Wojciech M. Zabolotny"], "year": 2015, "n_citations": 3}
{"id": 3062928, "s2_id": "cb14b610e404252093d55bf2349544e28b245489", "title": "Design & Simulation of 128x Interpolator Filter", "abstract": "This paper presents the design consideration and simulation of interpolator of OSR 128. The proposed structure uses the half band filers & Comb/Sinc filter. Experimental result shows that proposed interpolator achieves the design specification, and also has good noise rejection capabilities. The interpolator accepts the input at 44.1 kHz for applications like CD & DVD audio. The interpolation filter can be applied to the delta sigma DAC. The related work is done with the MATLAB & XILINX ISE simulators. The maximum operating frequency is achieved as 34.584 MHz.", "venue": "ArXiv", "authors": ["Rahul  Sinha", "Sonika  Arora"], "year": 2012, "n_citations": 3}
{"id": 3063921, "s2_id": "269b35999a89dc97cc238b28b5bf8ad1e9b28b22", "title": "From DNNs to GANs: Review of efficient hardware architectures for deep learning", "abstract": "In recent times, the trend in very large scale integration (VLSI) industry is multi-dimensional, e.g. reduction of energy consumption, occupancy of less space, precise result, less power dissipation, faster response etc. To meet these needs, the hardware architecture should be reliable and robust to these problems. Recently, neural network and deep learning has been started to impact the present research paradigm significantly which consists of parameters in the order of millions, nonlinear function for activation, convolutional operation for feature extraction, softmax regression for classification, generative adversarial networks, etc. These operations involve huge calculation and memory overhead. Presently available DSP processors are incapable of performing these operations and they most face the problems e.g. memory overhead, performance drop and compromised accuracy. Moreover, if a huge silicon area is powered to accelerate the operation using parallel computation, the IC\u2019s will be having significant chance of burning out due to the considerable generation of heat. Hence, novel dark silicon constraint is developed to reduce the heat dissipation without sacrificing the accuracy. Similarly, different algorithms have been adapted to design a DSP processor compatible for fast performance in neural network, activation function, convolutional neural network and generative adversarial network. In this review, we illustrate the recent developments in hardware for accelerating the efficient implementation of deep learning networks with enhanced performance. The techniques investigated in this review are expected to direct future research challenges of hardware optimization for high-performance computations.", "venue": "ArXiv", "authors": ["Gaurab  Bhattacharya"], "year": 2021, "n_citations": 0}
{"id": 3066283, "s2_id": "8cd833d6cfddffbae975e1f10ea9ddb1d5efc81c", "title": "MoRS: An Approximate Fault Modelling Framework for Reduced-Voltage SRAMs", "abstract": "On-chip memory (usually based on Static RAMsSRAMs) are crucial components for various computing devices including heterogeneous devices, e.g, GPUs, FPGAs, ASICs to achieve high performance. Modern workloads such as Deep Neural Networks (DNNs) running on these heterogeneous fabrics are highly dependent on the on-chip memory architecture for efficient acceleration. Hence, improving the energy-efficiency of such memories directly leads to an efficient system. One of the common methods to save energy is undervolting i.e., supply voltage underscaling below the nominal level. Such systems can be safely undervolted without incurring faults down to a certain voltage limit. This safe range is also called voltage guardband. However, reducing voltage below the guardband level without decreasing frequency causes timing-based faults. In this paper, we propose MoRS, a framework that generates the first approximate undervolting fault model using real faults extracted from experimental undervolting studies on SRAMs to build the model. We inject the faults generated by MoRS into the on-chip memory of the DNN accelerator to evaluate the resilience of the system under the test. MoRS has the advantage of simplicity without any need for high-time overhead experiments while being accurate enough in comparison to a fully randomlygenerated fault injection approach. We evaluate our experiment in popular DNN workloads by mapping weights to SRAMs and measure the accuracy difference between the output of the MoRS and the real data. Our results show that the maximum difference between real fault data and the output fault model of MoRS is 6.21%, whereas the maximum difference between real data and random fault injection model is 23.2%. In terms of average proximity to the real data, the output of MoRS outperforms the random fault injection approach by 3.21x.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Ismail Emir Y\u00fcksel", "Behzad  Salami", "Oguz  Ergin", "Osman S. Unsal", "Adri\u00e1n  Cristal"], "year": 2021, "n_citations": 0}
{"id": 3069100, "s2_id": "da7e44b2990721dcc681326afa57b086adea18ae", "title": "The BaseJump Manycore Accelerator Network", "abstract": "The BaseJump Manycore Accelerator-Network is an open source mesh-based On-Chip-Network which is designed leveraging the Bespoke Silicon Group's 20+ years of experience in designing manycore architectures. It has been used in the 16nm 511-core RISC-V compatible Celerity chip Davidson et al. (2018), forming the basis of both a 1 GHz 496-core RISC-V manycore and a 10-core always-on low voltage complex. It was also used in the 180nm BSG Ten chip, which featured ten cores and a mesh that extends over off-chip links to an FPGA. To facilitate use by the open source community of the BaseJump Manycore network, we explain the ideas, protocols, interfaces and potential uses of the mesh network. We also show an example with source code that demonstrates how to integrate user designs into the mesh network.", "venue": "ArXiv", "authors": ["Shaolin  Xie", "Michael Bedford Taylor"], "year": 2018, "n_citations": 3}
{"id": 3069328, "s2_id": "bcc07372c2a6e84df97f84c66ec72e6a1e09fddc", "title": "On the Accuracy of Analog Neural Network Inference Accelerators", "abstract": "Specialized accelerators have recently garnered attention as a method to reduce the power consumption of neural network inference. A promising category of accelerators utilizes nonvolatile memory arrays to both store weights and perform in situ analog computation inside the array. While prior work has explored the design space of analog accelerators to optimize performance and energy efficiency, there is seldom a rigorous evaluation of the accuracy of these accelerators. This work shows how architectural design decisions, particularly in mapping neural network parameters to analog memory cells, influence inference accuracy. When evaluated using ResNet50 on ImageNet, the resilience of the system to analog non-idealities\u2014cell programming errors, analog-to-digital converter resolution, and array parasitic resistances\u2014all improve when analog quantities in the hardware are made proportional to the weights in the network. Moreover, contrary to the assumptions of prior work, nearly equivalent resilience to cell imprecision can be achieved by fully storing weights as analog quantities, rather than spreading weight bits across multiple devices, often referred to as bit slicing. By exploiting proportionality, analog system designers have the freedom to match the precision of the hardware to the needs of the algorithm, rather than attempting to guarantee the same level of precision in the intermediate results as an equivalent digital accelerator. This ultimately results in an analog accelerator that is more accurate, more robust to analog errors, and more energy-efficient.", "venue": "ArXiv", "authors": ["T. Patrick Xiao", "Ben  Feinberg", "Christopher H. Bennett", "Venkatraman  Prabhakar", "Prashant  Saxena", "Vineet  Agrawal", "Sapan  Agarwal", "Matthew J. Marinella"], "year": 2021, "n_citations": 0}
{"id": 3070752, "s2_id": "4d07bd25816f1f8aa1fc5e9a8934ef59a436862d", "title": "Mitigating Write Disturbance Errors of Phase-Change Memory as In-Module Approach", "abstract": "With the growing demand for technology scaling and storage capacity in server systems to support high-performance computing, phase-change memory (PCM) has garnered attention as the next-generation non-volatile memory to satisfy these requirements. However, write disturbance error (WDE) appears as a serious reliability problem preventing PCM from general commercialization. WDE occurs on the neighboring cells of a written cell due to heat dissipation. Previous studies for the prevention of WDEs are based on the write cache or verify-n-correction while they often suffer from significant area overhead and performance degradation, making it unsuitable for high-performance computing. Therefore, an on-demand correction is required to minimize the performance overhead. In this paper, an in-module disturbance barrier (IMDB) mitigating WDEs is proposed. IMDB includes two sets of SRAMs into two levels and evicts entries with a policy that leverages the characteristics of WDE. In this work, the comparator dedicated to the replacement policy requires significant hardware resources and latency. Thus, an approximate comparator is designed to reduce the area and latency considerably. Furthermore, the exploration of architecture parameters is conducted to obtain cost-effective design. The proposed work significantly reduces WDEs without a noticeable speed degradation and additional energy consumption compared to previous methods.", "venue": "ArXiv", "authors": ["Hyokeun  Lee", "Seungyong  Lee", "Byeongki  Song", "Moonsoo  Kim", "Seokbo  Shim", "Hyun  Kim", "Hyuk-Jae  Lee"], "year": 2020, "n_citations": 0}
{"id": 3071177, "s2_id": "9cb5b8a2e5cc7e2aade248f3bd600f622207a7a9", "title": "An Area Efficient 2D Fourier Transform Architecture for FPGA Implementation", "abstract": "Two-dimensional Fourier transform plays a significant role in a variety of image processing problems, such as medical image processing, digital holography, correlation pattern recognition, hybrid digital optical processing, optical computing etc. 2D spatial Fourier transformation involves large number of image samples and hence it requires huge hardware resources of field programmable gate arrays (FPGA). In this paper, we present an area efficient architecture of 2D FFT processor that reuses the butterfly units multiple times. This is achieved by using a control unit that sends back the previous computed data of N/2 butterfly units to itself for {log_2(N) - 1} times. A RAM controller is used to synchronize the flow of data samples between the functional blocks.The 2D FFT processor is simulated by VHDL and the results are verified on a Virtex-6 FPGA. The proposed method outperforms the conventional NxN point 2D FFT in terms of area which is reduced by a factor of log_N(2) with negligible increase in computation time.", "venue": "ArXiv", "authors": ["Atin  Mukherjee", "Debesh  Choudhury"], "year": 2018, "n_citations": 1}
{"id": 3074791, "s2_id": "1397b68a98f52bf40fc999a17d486943a6b01ffe", "title": "Building the Case for Temperature Awareness in Energy Consumption Models: an Application of the Energy-Frequency Convexity Rule", "abstract": "Optimizing computing and communication systems that host energy-critical applications is becoming a key issue for software developers. In previous work, we introduced and validated the Energy/Frequency Convexity Rule for CPU-bound benchmarks on recent ARM platforms. This rule states that there exists an optimal clock frequency that minimizes the CPU's energy consumption for non-performance-critical programs. We showed that the Energy/Frequency Convexity Rule is related to the non-linearity of power with respect to frequency and is not dependent on the supply voltage. Here, we discuss the application of an analytical energy consumption model proposed previously to our target board, a TI AM572x EVM. We show that this non-linear analytical model can, for our experimental settings, be approximated by a frequency-linear variant, as our voltage is maintained constant. This, however, does not fit the measurements on the board, suggesting that a parameter is currently missing in the analytical model. We conjecture that accounting for temperature in the model would yield more accurate results that are in-line with our measurements. This builds the case for the inclusion of this important parameter in our energy models.", "venue": "ArXiv", "authors": ["Kameswar Rao Vaddina", "Florian  Brandner", "G\u00e9rard  Memmi", "Pierre  Jouvelot"], "year": 2018, "n_citations": 0}
{"id": 3075149, "s2_id": "289e029dc7cd5c5db6ebe33be0576fb1f018d5c1", "title": "Decomposition of transition systems into sets of synchronizing state machines", "abstract": "Transition systems (TS) and Petri nets (PN) are important models of computation ubiquitous in formal methods for modeling systems. An important problem is how to extract from a given TS a PN whose reachability graph is equivalent (with a suitable notion of equivalence) to the original TS.This paper addresses the decomposition of transition systems into synchronizing state machines (SMs), which are a class of Petri nets where each transition has one incoming and one outgoing arc and all markings have exactly one token. This is an important case of the general problem of extracting a PN from a TS. The decomposition is based on the theory of regions, and it is shown that a property of regions called excitation-closure is a sufficient condition to guarantee the equivalence between the original TS and a decomposition into SMs.An efficient algorithm is provided which solves the problem by reducing its critical steps to the maximal independent set problem (to compute a minimal set of irredundant SMs) or to satisfiability (to merge the SMs). We report experimental results that show a good trade-off between quality of results vs. computation time.", "venue": "2021 24th Euromicro Conference on Digital System Design (DSD)", "authors": ["Viktor  Teren", "Jordi  Cortadella", "Tiziano  Villa"], "year": 2021, "n_citations": 0}
{"id": 3081506, "s2_id": "aba0621a287a1aa2161d577ba81281864f3fcf3d", "title": "Analysis of Intel's Haswell Microarchitecture Using the ECM Model and Microbenchmarks", "abstract": "This paper presents an in-depth analysis of Intel's Haswell microarchitecture for streaming loop kernels. Among the new features examined are the dual-ring Uncore design, Cluster-on-Die mode, Uncore Frequency Scaling, enhancements such as new and improved execution units, as well as improvements throughout the memory hierarchy. The Execution-Cache-Memory diagnostic performance model is used together with a generic set of microbenchmarks to quantify the efficiency of the microarchitecture. The set of microbenchmarks is chosen in a way that it can serve as a blueprint for other streaming loop kernels.", "venue": "ARCS", "authors": ["Johannes  Hofmann", "Dietmar  Fey", "Jan  Eitzinger", "Georg  Hager", "Gerhard  Wellein"], "year": 2016, "n_citations": 15}
{"id": 3082344, "s2_id": "1c0b1815ec08ad75bffb85f08a21b6931a004d16", "title": "Logic design for on-chip test clock generation - implementation details and impact on delay test quality", "abstract": "This paper addresses delay test for SOC devices with high frequency clock domains. A logic design for on-chip high-speed clock generation, implemented to avoid expensive test equipment, is described in detail. Techniques for on-chip clock generation, meant to reduce test vector count and to increase test quality, are discussed. ATPG results for the proposed techniques are given.", "venue": "Design, Automation and Test in Europe", "authors": ["Matthias  Beck", "Olivier  Barondeau", "Martin  Kaibel", "Frank  Poehl", "Xijiang  Lin", "Ron  Press"], "year": 2005, "n_citations": 58}
{"id": 3083213, "s2_id": "9a5c3d04cde121acd808b8a7d6ef7a4014b2cb72", "title": "Dependability Analysis of Data Storage Systems in Presence of Soft Errors", "abstract": "In recent years, high availability and reliability of data storage systems (DSS) have been significantly threatened by soft errors occurring in storage controllers. Due to their specific functionality and hardware\u2013software stack, error propagation and manifestation in DSS is quite different from general-purpose computing architectures. To the best of our knowledge, no previous study has examined the system-level effects of soft errors on the availability and reliability of DSS. In this paper, we first analyze the effects of soft errors occurring in the server processors of storage controllers on the entire storage system dependability. To this end, we implement the major functions of a typical data storage system controller, running on a full stack of storage system operating system, and develop a framework to perform fault injection experiments using a full system simulator. We then propose a new metric, storage system vulnerability factor (SSVF), to accurately capture the impact of soft errors in storage systems. By conducting extensive experiment, it is revealed that depending on the controller configuration, up to 40% of cache memory contains end-user data in which any unrecoverable soft errors will result in data loss (DL) in an irreversible manner. However, soft errors in the rest of cache memory filled by operating system and storage applications will result in data unavailability (DU) at the storage system level. Our analysis also shows that detectable unrecoverable errors on the cache data field are the major cause of DU in storage systems, while silent data corruptions in the cache tag and data fields are mainly the cause of DL in storage systems.", "venue": "IEEE Transactions on Reliability", "authors": ["Mostafa  Kishani", "Mehdi  Tahoori", "Hossein  Asadi"], "year": 2019, "n_citations": 12}
{"id": 3084178, "s2_id": "8675ee2a4bbb42b55f324fd5495751dc5415c50a", "title": "FPGA-based ORB feature extraction for real-time visual SLAM", "abstract": "Simultaneous Localization And Mapping (SLAM) is the problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it. How to enable SLAM robustly and durably on mobile, or even IoT grade devices, is the main challenge faced by the industry today. The main problems we need to address are: 1.) how to accelerate the SLAM pipeline to meet real-time requirements; and 2.) how to reduce SLAM energy consumption to extend battery life. After delving into the problem, we found out that feature extraction is indeed the bottleneck of performance and energy consumption. Hence, in this paper, we design, implement, and evaluate a hardware ORB feature extractor and prove that our design is a great balance between performance and energy consumption compared with ARM Krait and Intel Core i5.", "venue": "2017 International Conference on Field Programmable Technology (ICFPT)", "authors": ["Weikang  Fang", "Yanjun  Zhang", "Bo  Yu", "Shaoshan  Liu"], "year": 2017, "n_citations": 35}
{"id": 3087073, "s2_id": "fc57db85a5ed5affa31d4f41dfde201cc8cd0b1d", "title": "Memory-Efficient CNN Accelerator Based on Interlayer Feature Map Compression", "abstract": "Existing deep convolutional neural networks (CNNs) generate massive interlayer feature data during network inference. To maintain real-time processing in embedded systems, large onchip memory is required to buffer the interlayer feature maps. In this paper, we propose an efficient hardware accelerator with an interlayer feature compression technique to significantly reduce the required on-chip memory size and off-chip memory access bandwidth. The accelerator compresses interlayer feature maps through transforming the stored data into frequency domain using hardware-implemented 8\u00d78 discrete cosine transform (DCT). The high-frequency components are removed after the DCT through quantization. Sparse matrix compression is utilized to further compress the interlayer feature maps. The on-chip memory allocation scheme is designed to support dynamic configuration of the feature map buffer size and scratch pad size according to different network-layer requirements. The hardware accelerator combines compression, decompression, and CNN acceleration into one computing stream, achieving minimal compressing and processing delay. A prototype accelerator is implemented on an FPGA platform and also synthesized in TSMC 28-nm COMS technology. It achieves 403GOPS peak throughput and 1.4x~3.3x interlayer feature map reduction by adding light hardware area overhead, making it a promising hardware accelerator for intelligent IoT devices.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Zhuang  Shao", "Xiaoliang  Chen", "Li  Du", "Lei  Chen", "Yuan  Du", "Wei  Zhuang", "Huadong  Wei", "Chenjia  Xie", "Zhongfeng  Wang"], "year": 2021, "n_citations": 0}
{"id": 3087279, "s2_id": "3051e53cf1a82fb1b1fc9d65ee76d4444ffce6ca", "title": "CORDIC-based Architecture for Powering Computation in Fixed-Point Arithmetic", "abstract": "We present a fixed point architecture (source VHDL code is provided) for powering computation. The fully customized architecture, based on the expanded hyperbolic CORDIC algorithm, allows for design space exploration to establish trade-offs among design parameters (numerical format, number of iterations), execution time, resource usage and accuracy. We also generate Pareto-optimal realizations in the resource-accuracy space: this approach can produce optimal hardware realizations that simultaneously satisfy resource and accuracy requirements.", "venue": "ArXiv", "authors": ["Nia  Simmonds", "Joshua  Mack", "Sam  Bellestri", "Daniel  Llamocca"], "year": 2016, "n_citations": 1}
{"id": 3087542, "s2_id": "6045e8507fea133b0ece895bd57d8d9039f88f1f", "title": "Design of fault-tolerant and dynamically-reconfigurable microfluidic biochips", "abstract": "Microfluidics-based biochips are soon expected to revolutionize clinical diagnosis, DNA sequencing, and other laboratory procedures involving molecular biology. Most microfluidic biochips are based on the principle of continuous fluid flow and they rely on permanently-etched microchannels, micropumps, and microvalves. We focus here on the automated design of \"digital\" droplet-based microfluidic biochips. In contrast to continuous-flow systems, digital microfluidics offers dynamic reconfigurability; groups of cells in a microfluidics array can be reconfigured to change their functionality during the concurrent execution of a set of bioassays. We present a simulated annealing-based technique for module placement in such biochips. The placement procedure not only addresses chip area, but it also considers fault tolerance, which allows a microfluidic module to be relocated elsewhere in the system when a single cell is detected to be faulty. Simulation results are presented for a case study involving the polymerase chain reaction.", "venue": "Design, Automation and Test in Europe", "authors": ["Fei  Su", "Krishnendu  Chakrabarty"], "year": 2005, "n_citations": 57}
{"id": 3088315, "s2_id": "903a678035f37fc25e3d05946f5f4809db6861b1", "title": "PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks", "abstract": "Convolutional neural networks (CNNs) have been widely employed in many applications such as image classification, video analysis and speech recognition. Being compute-intensive, CNN computations are mainly accelerated by GPUs with high power dissipations. Recently, studies were carried out exploiting FPGA as CNN accelerator because of its reconfigurability and energy efficiency advantage over GPU, especially when OpenCL-based high-level synthesis tools are now available providing fast verification and implementation flows. Previous OpenCL-based design only focused on creating a generic framework to identify performance-related hardware parameters, without utilizing FPGA's special capability of pipelining kernel functions to minimize memory bandwidth requirement. In this work, we propose an FPGA accelerator with a new architecture of deeply pipelined OpenCL kernels. Data reuse and task mapping techniques are also presented to improve design efficiency. The proposed schemes are verified by implementing two representative large-scale CNNs, AlexNet and VGG on Altera Stratix-V A7 FPGA. We have achieved a similar peak performance of 33.9 GOPS with a 34% resource reduction on DSP blocks compared to previous work. Our design is openly accessible and thus can be reused to explore new architectures for neural network accelerators.", "venue": "ArXiv", "authors": ["Dong  Wang", "Jianjing  An", "Ke  Xu"], "year": 2016, "n_citations": 28}
{"id": 3089905, "s2_id": "dc154049ae3a6d52c89a67cec8a90067046a2ef7", "title": "Improving the Accuracy-Memory Trade-Off of Random Forests Via Leaf-Refinement", "abstract": "This appendix accompanies the paper \u2018Improving the Accuracy-Memory Trade-Off of Random Forests Via Leaf-Refinement\u2019. It provides results for more experiments which are not given in the paper due to space reasons. 1. Transformation of the Many-Could-Be-Better-Than-All-Theorem", "venue": "ArXiv", "authors": ["Sebastian  Buschj\u00e4ger", "Katharina  Morik"], "year": 2021, "n_citations": 0}
{"id": 3091683, "s2_id": "54f8632ef389cca04eb8b09456fdf0dd539b9d11", "title": "Frequency Analysis of Decoupling Capacitors for Three Voltage Supplies in SoC", "abstract": "Reduction in power consumption has become a major criterion of design in modern ICs. One such scheme to reduce power consumption by an IC is the use of multiple power supplies for critical and non-critical paths. To maintain the impedance of a power distribution system below a specified level, multiple decoupling capacitors are placed at different levels of power grid hierarchy. This paper describes about three-voltage supply power distribution systems. The noise at one power supply can propagate to the other power supply, causing power and signal integrity problems in the overall system. Effects such as anti-resonance and remedies for these effects are studied. Impedance of the three-voltage supply power distribution system is calculated in terms of RLC-model of decoupling capacitors. Further the obtained impedance depends on the frequency; hence brief frequency analysis of impedance is done.", "venue": "ArXiv", "authors": ["Mohd  Abubakr"], "year": 2007, "n_citations": 1}
{"id": 3094653, "s2_id": "73fed847b03242e87e9ac0e3ddd19d3664857ccf", "title": "Optimizing the Write Fidelity of MRAMs", "abstract": "Magnetic random-access memory (MRAM) is a promising memory technology due to its high density, non-volatility, and high endurance. However, achieving high memory fidelity incurs significant write-energy costs, which should be reduced for the large-scale deployment of MRAMs. In this paper, we formulate an optimization problem to maximize the memory fidelity given energy constraints, and propose a biconvex optimization approach to solve it. The basic idea is to allocate non-uniform write pulses depending on the importance of each bit position. We consider the mean squared error (MSE) as a fidelity metric and propose an iterative water-filling algorithm to minimize the MSE. Although the iterative algorithm does not guarantee the global optimality, we can choose a proper starting point that decreases the MSE exponentially and guarantees fast convergence. For an 8-bit accessed word, the proposed algorithm reduces the MSE by a factor of 21.", "venue": "2020 IEEE International Symposium on Information Theory (ISIT)", "authors": ["Yongjune  Kim", "Yoocharn  Jeon", "Cyril  Guyot", "Yuval  Cassuto"], "year": 2020, "n_citations": 2}
{"id": 3097749, "s2_id": "cb642b747e2ef3e1a3f7c7c36adeca0e1d237329", "title": "Using DSP Slices as Content-Addressable Update Queues", "abstract": "Content-Addressable Memory (CAM) is a powerful abstraction for building memory caches, routing tables and hazard detection logic. Without a native CAM structure available on FPGA devices, their functionality must be emulated using the structural primitives at hand. Such an emulation causes significant overhead in the consumption of the underlying resources, typically general-purpose fabric and on-chip block RAM (BRAM). This often motivates mitigating trade-offs, such as the reduction of the associativity of memory caches. This paper describes a technique to implement the hazard resolution in a memory update queue. It hides the readout latency of off-chip memory in read-modify-write cycles while guaranteeing the delivery of the full memory bandwidth. The innovative use of DSP slices allows them to assume and combine the functions of (a) the tag and data storage, (b) the tag matching, and (c) the data update in this key-value mapping scenario. The proposed approach provides designers with extra flexibility by adding this resource type as another option to implement CAM.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Thomas B. Preusser", "Monica  Chiosa", "Alexander  Weiss", "Gustavo  Alonso"], "year": 2020, "n_citations": 0}
{"id": 3106426, "s2_id": "419fc475c41ed914121003d24d0198c9b5fe78ac", "title": "iVAMS 1.0: Polynomial-Metamodel-Integrated Intelligent Verilog-AMS for Fast, Accurate Mixed-Signal Design Optimization", "abstract": "Electronic circuit behavioral models built with hardware description/modeling languages such as Verilog-AMS for system-level simulations are typically functional models. They do not capture the physical design (layout) information of the target design. Numerous iterations of post-layout design adjustments are usually required to ensure that design specifications are met with the presence of layout parasitics. In this paper a paradigm shift of the current trend is presented that integrates layout-level information in Verilog-AMS through metamodels such that system-level simulation of a mixed-signal circuit/system is realistic and as accurate as true parasitic netlist simulation. The simulations performed with these parasitic-aware models can be used to estimate system performance without layout iterations. We call this new form of Verilog-AMS as iVAMS (i.e. Intelligent Verilog-AMS). We call this iVAMS 1.0 as it is simple polynomial-metamodel integrated Intelligent Verilog-AMS. As a specific case study, a voltage-controlled oscillator (VCO) Verilog-AMS behavioral model and design flow are proposed to assist fast PLL design space exploration. The PLL simulation employing quadratic metamodels achieves approximately 10X speedup compared to that employing the layout extracted, parasitic netlist. The simulations using this behavioral model attain high accuracy. The observed error for the simulated lock time and average power dissipation are 0.7% and 3%, respectively. This behavioral metamodel approach bridges the gap between layout-accurate but fast simulation and design space exploration. The proposed method also allows much shorter design verification and optimization to meet stringent time-to-market requirements. Compared to the optimization using the layout netlist, the runtime using the behavioral model is reduced by 88.9%.", "venue": "ArXiv", "authors": ["Saraju P. Mohanty", "Elias  Kougianos"], "year": 2019, "n_citations": 1}
{"id": 3107028, "s2_id": "09bb7390575f8353cc8881c14400c4c830f810fb", "title": "Hardware transactional persistent memory", "abstract": "Emerging Persistent Memory technologies (also pm, Non-Volatile DIMMs, Storage Class Memory or scm) hold tremendous promise for accelerating popular data-management applications like in-memory databases. However, programmers now need to deal with ensuring the atomicity of transactions on Persistent Memory resident data and maintaining consistency between the order in which processors perform stores and that in which the updated values become durable. The problem is specially challenging when high-performance isolation mechanisms like Hardware Transactional Memory (htm) are used for concurrency control. This work shows how htm transactions can be ordered correctly and atomically into PM by the use of a novel software protocol combined with a Persistent Memory Controller, without requiring changes to processor cache hardware or htm protocols. In contrast, previous approaches require significant changes to existing processor microarchitectures. Our approach, evaluated using both micro-benchmarks and the stamp suite compares well with standard (volatile) htm transactions. It also yields significant gains in throughput and latency in comparison with persistent transactional locking.", "venue": "MEMSYS", "authors": ["Ellis  Giles", "Kshitij  Doshi", "Peter J. Varman"], "year": 2018, "n_citations": 8}
{"id": 3107319, "s2_id": "a7e9ba75ba6cd349adc64c159e559b86578ba88e", "title": "Deterministic Memory Abstraction and Supporting Multicore System Architecture", "abstract": "Poor time predictability of multicore processors has been a long-standing challenge in the real-time systems community. In this paper, we make a case that a fundamental problem that prevents efficient and predictable real-time computing on multicore is the lack of a proper memory abstraction to express memory criticality, which cuts across various layers of the system: the application, OS, and hardware. We, therefore, propose a new holistic resource management approach driven by a new memory abstraction, which we call Deterministic Memory. The key characteristic of deterministic memory is that the platform - the OS and hardware - guarantees small and tightly bounded worst-case memory access timing. In contrast, we call the conventional memory abstraction as best-effort memory in which only highly pessimistic worst-case bounds can be achieved. We propose to utilize both abstractions to achieve high time predictability but without significantly sacrificing performance. We present deterministic memory-aware OS and architecture designs, including OS-level page allocator, hardware-level cache, and DRAM controller designs. We implement the proposed OS and architecture extensions on Linux and gem5 simulator. Our evaluation results, using a set of synthetic and real-world benchmarks, demonstrate the feasibility and effectiveness of our approach.", "venue": "ECRTS", "authors": ["Farzad  Farshchi", "Prathap Kumar Valsan", "Renato  Mancuso", "Heechul  Yun"], "year": 2018, "n_citations": 11}
{"id": 3107721, "s2_id": "1a02f5ee36cf9f0cc4b70f76e633ea53a72e4399", "title": "Enabling Automated FPGA Accelerator Optimization Using Graph Neural Networks", "abstract": "High-level synthesis (HLS) has freed the computer architects from developing their designs in a very low-level language and needing to exactly specify how the data should be transferred in registerlevel. With the help of HLS, the hardware designers must describe only a high-level behavioral flow of the design. Despite this, it still can take weeks to develop a high-performance architecture mainly because there are many design choices at a higher level that requires more time to explore. It also takes several minutes to hours to get feedback from the HLS tool on the quality of each design candidate. In this paper, we propose to solve this problem by modeling the HLS tool with a graph neural network (GNN) that is trained to be used for a wide range of applications. The experimental results demonstrate that by employing the GNN-based model, we are able to estimate the quality of design in milliseconds with high accuracy which can help us search through the solution space very quickly.", "venue": "ArXiv", "authors": ["Atefeh  Sohrabizadeh", "Yunsheng  Bai", "Yizhou  Sun", "Jason  Cong"], "year": 2021, "n_citations": 0}
{"id": 3108770, "s2_id": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit", "abstract": "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X\u201330X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X\u201380X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.", "venue": "2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Norman P. Jouppi", "Cliff  Young", "Nishant  Patil", "David A. Patterson", "Gaurav  Agrawal", "Raminder  Bajwa", "Sarah  Bates", "Suresh  Bhatia", "Nan  Boden", "Al  Borchers", "Rick  Boyle", "Pierre-luc  Cantin", "Clifford  Chao", "Chris  Clark", "Jeremy  Coriell", "Mike  Daley", "Matt  Dau", "Jeffrey  Dean", "Ben  Gelb", "Tara Vazir Ghaemmaghami", "Rajendra  Gottipati", "William  Gulland", "Robert  Hagmann", "C. Richard Ho", "Doug  Hogberg", "John  Hu", "Robert  Hundt", "Dan  Hurt", "Julian  Ibarz", "Aaron  Jaffey", "Alek  Jaworski", "Alexander  Kaplan", "Harshit  Khaitan", "Daniel  Killebrew", "Andy  Koch", "Naveen  Kumar", "Steve  Lacy", "James  Laudon", "James  Law", "Diemthu  Le", "Chris  Leary", "Zhuyuan  Liu", "Kyle  Lucke", "Alan  Lundin", "Gordon  MacKean", "Adriana  Maggiore", "Maire  Mahony", "Kieran  Miller", "Rahul  Nagarajan", "Ravi  Narayanaswami", "Ray  Ni", "Kathy  Nix", "Thomas  Norrie", "Mark  Omernick", "Narayana  Penukonda", "Andy  Phelps", "Jonathan  Ross", "Matt  Ross", "Amir  Salek", "Emad  Samadiani", "Chris  Severn", "Gregory  Sizikov", "Matthew  Snelham", "Jed  Souter", "Dan  Steinberg", "Andy  Swing", "Mercedes  Tan", "Gregory  Thorson", "Bo  Tian", "Horia  Toma", "Erick  Tuttle", "Vijay  Vasudevan", "Richard  Walter", "Walter  Wang", "Eric  Wilcox", "Doe Hyun Yoon"], "year": 2017, "n_citations": 2603}
{"id": 3115534, "s2_id": "237962984c60fbaf634f0511b499508b024496eb", "title": "Estimating Silent Data Corruption Rates Using a Two-Level Model", "abstract": "High-performance and safety-critical system architects must accurately evaluate the application-level silent data corruption (SDC) rates of processors to soft errors. Such an evaluation requires error propagation all the way from particle strikes on low-level state up to the program output. Existing approaches that rely on low-level simulations with fault injection cannot evaluate full applications because of their slow speeds, while application-level accelerated fault testing in accelerated particle beams is often impractical. We present a new two-level methodology for application resilience evaluation that overcomes these challenges. The proposed approach decomposes application failure rate estimation into (1) identifying how particle strikes in low-level unprotected state manifest at the architecture-level, and (2) measuring how such architecture-level manifestations propagate to the program output. We demonstrate the effectiveness of this approach on GPU architectures. We also show that using just one of the two steps can overestimate SDC rates and produce different trends---the composition of the two is needed for accurate reliability modeling.", "venue": "ArXiv", "authors": ["Siva Kumar Sastry Hari", "Paolo  Rech", "Timothy  Tsai", "Mark  Stephenson", "Arslan  Zulfiqar", "Michael  Sullivan", "Philip  Shirvani", "Paul  Racunas", "Joel  Emer", "Stephen W. Keckler"], "year": 2020, "n_citations": 2}
{"id": 3117208, "s2_id": "89f8e38050118afdab46e35830f197ddb1f5c745", "title": "RANC: Reconfigurable Architecture for Neuromorphic Computing", "abstract": "Neuromorphic architectures have been introduced as platforms for energy-efficient spiking neural network execution. The massive parallelism offered by these architectures has also triggered interest from nonmachine learning application domains. In order to lift the barriers to entry for hardware designers and application developers, we present RANC: a reconfigurable architecture for neuromorphic computing, an opensource highly flexible ecosystem that enables rapid experimentation with neuromorphic architectures in both software via C++ simulation and hardware via FPGA emulation. We present the utility of the RANC ecosystem by showing its ability to recreate behavior of IBM\u2019s TrueNorth and validate with a direct comparison to IBM\u2019s Compass simulation environment and published literature. RANC allows optimizing architectures based on application insights as well as prototyping future neuromorphic architectures that can support new classes of applications entirely. We demonstrate the highly parameterized and configurable nature of RANC by studying the impact of architectural changes on improving application mapping efficiency with quantitative analysis based on Alveo U250 FPGA. We present post routing resource usage and throughput analysis across implementations of synthetic aperture radar classification and vector matrix multiplication applications, and demonstrate a neuromorphic architecture that scales to emulating 259K distinct neurons and 73.3M distinct synapses.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Joshua  Mack", "Ruben  Purdy", "Kris  Rockowitz", "Michael  Inouye", "Edward  Richter", "Spencer  Valancius", "Nirmal  Kumbhare", "Md Sahil Hassan", "Kaitlin  Fair", "John  Mixter", "Ali  Akoglu"], "year": 2021, "n_citations": 0}
{"id": 3118646, "s2_id": "f8662ad31440678bc8927fa2daf12c4b22cf17e2", "title": "Proceedings of the DATE Friday Workshop on System-level Design Methods for Deep Learning on Heterogeneous Architectures (SLOHA 2021)", "abstract": "This volume contains the papers accepted at the first DATE Friday Workshop on System-level Design Methods for Deep Learning on Heterogeneous Architectures (SLOHA 2021), held virtually on February 5, 2021. SLOHA 2021 was co-located with the Conference on Design, Automation and Test in Europe (DATE).", "venue": "ArXiv", "authors": ["Frank  Hannig", "Paolo  Meloni", "Matteo  Spallanzani", "Matthias  Ziegler"], "year": 2021, "n_citations": 0}
{"id": 3126653, "s2_id": "9ddc8eae7c261ef8809e423102a66d8376cd59fb", "title": "Multi-armed Bandit Algorithms on System-on-Chip: Go Frequentist or Bayesian?", "abstract": "Multi-armed Bandit (MAB) algorithms identify the best arm among multiple arms via exploration-exploitation trade-off without prior knowledge of arm statistics. Their usefulness in wireless radio, IoT, and robotics demand deployment on edge devices, and hence, a mapping on system-on-chip (SoC) is desired. Theoretically, the Bayesian approach-based Thompson Sampling (TS) algorithm offers better performance than the frequentist approach-based Upper Confidence Bound (UCB) algorithm. However, TS is not synthesizable due to \\textit{Beta} function. We address this problem by approximating it via a pseudo-random number generator-based approach and efficiently realize the TS algorithm on Zynq SoC. In practice, the type of arms distribution (e.g., Bernoulli, Gaussian, etc.) is unknown and hence, a single algorithm may not be optimal. We propose a reconfigurable and intelligent MAB (RI-MAB) framework. Here, intelligence enables the identification of appropriate MAB algorithms for a given environment, and reconfigurability allows on-the-fly switching between algorithms on the SoC. This eliminates the need for parallel implementation of algorithms resulting in huge savings in resources and power consumption. We analyze the functional correctness, area, power, and execution time of the proposed and existing architectures for various arm distributions, word-length, and hardware-software co-design approaches. We demonstrate the superiority of the RI-MAB over TS and UCB only architectures.", "venue": "ArXiv", "authors": ["S. V. Sai Santosh", "Sumit J. Darak"], "year": 2021, "n_citations": 0}
{"id": 3127386, "s2_id": "2b2043771db9164b0dbb7e23eed3868a1985e59e", "title": "Profiling-Assisted Decoupled Access-Execute", "abstract": "As energy efficiency became a critical factor in the embedded systems domain, dynamic voltage and frequency scaling (DVFS) techniques have emerged as means to control the system's power and energy efficiency. Additionally, due to the compact design, thermal issues become prominent. State of the art work promotes software decoupled access-execution (DAE) that statically generates code amenable to DVFS techniques. The compiler builds memory-bound access phases, designed to prefetch data in the cache at low frequency, and compute-bound phases, that consume the data and perform computations at high frequency. This work investigates techniques to find the optimal balance between lightweight and efficient access phases. A profiling step guides the selection of loads to be prefetched in the access phase. For applications whose behavior vary significantly with respect to the input data, the profiling can be performed online, accompanied by just-in-time compilation. We evaluated the benefits in energy efficiency and performance for both static and dynamic code generation and showed that precise prefetching of critical loads can result in 20% energy improvements, on average. DAE is particularly beneficial for embedded systems as by alternating access phases (executed at low frequency) and execute phases (at high frequency) DAE proactively reduces the temperature and therefore prevents thermal emergencies.", "venue": "ArXiv", "authors": ["Jonatan  Waern", "Per  Ekemark", "Konstantinos  Koukos", "Stefanos  Kaxiras", "Alexandra  Jimborean"], "year": 2016, "n_citations": 1}
{"id": 3130350, "s2_id": "757867500c97ca81d3280962974d5546b8f1de09", "title": "Real-time Closed Loop Neural Decoding on a Neuromorphic chip", "abstract": "This paper presents for the first time a real-time closed loop neuromorphic decoder chip-driven intra-cortical brain machine interface (iBMI) in a non-human primate (NHP) based experimental setup. Decoded results show trial success rates and mean times to target comparable to those obtained by hand-controlled joystick. Neural control trial success rates of \u2248 96% of those obtained by hand-controlled joystick have been demonstrated. Also, neural control has shown mean target reach speeds of \u2248 85% of those obtained by hand-controlled joystick. These results pave the way for fast and accurate, fully implantable neuromorphic neural decoders in iBMIs.", "venue": "2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)", "authors": ["Shoeb  Shaikh", "Rosa Q. So", "Tafadzwa  Sibindi", "Camilo  Libedinsky", "Arindam  Basu"], "year": 2019, "n_citations": 6}
{"id": 3130422, "s2_id": "92e247b6c18ef5d3c539c281fcf33b27388bb854", "title": "Generic pipelined processor modeling and high performance cycle-accurate simulator generation", "abstract": "Detailed modeling of processors and high performance cycle-accurate simulators are essential for today's hardware and software design. These problems are challenging enough by themselves and have seen many previous research efforts. Addressing both simultaneously is even more challenging, with many existing approaches focusing on one over another. In this paper, we propose the reduced colored Petri net (RCPN) model that has two advantages: first, it offers a very simple and intuitive way of modeling pipelined processors: second, it can generate high performance cycle-accurate simulators. RCPN benefits from all the useful features of colored Petri nets without suffering from their exponential growth in complexity. RCPN processor models are very intuitive since they are a mirror image of the processor pipeline block diagram. Furthermore, in our experiments on the generated cycle-accurate simulators for XScale and StrongArm processor models, we achieved an order of magnitude (/spl sim/15 times) speedup over the popular SimpleScalar ARM simulator.", "venue": "Design, Automation and Test in Europe", "authors": ["Mehrdad  Reshadi", "Nikil D. Dutt"], "year": 2005, "n_citations": 24}
{"id": 3130744, "s2_id": "5074b2562db8e892ac21453e697152e843d86383", "title": "Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity", "abstract": "Long Short-Term Memory (LSTM) recurrent networks are frequently used for tasks involving time sequential data such as speech recognition. However, it is difficult to deploy these networks on hardware to achieve high throughput and low latency because the fully-connected structure makes LSTM networks a memory-bounded algorithm. Previous work in LSTM accelerators either exploited weight spatial sparsity or temporal sparsity. In this paper, we present a new accelerator called \u201cSpartus\u201d that exploits spatio-temporal sparsity to achieve ultra-low latency inference. The spatial sparsity was induced using our proposed pruning method called Column-Balanced Targeted Dropout (CBTD) that leads to structured sparse weight matrices benefiting workload balance. It achieved up to 96% weight sparsity with negligible accuracy difference for an LSTM network trained on a TIMIT phone recognition task. To induce temporal sparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU method to the LSTM network. This combined sparsity saves on weight memory access and associated arithmetic operations simultaneously. Spartus was implemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single DeltaLSTM layer of 1024 neurons running on Spartus is 1 \u03bcs. Spartus achieved 9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which are respectively 4X and 7X higher than the previous state-of-the-art.", "venue": "ArXiv", "authors": ["Chang  Gao", "Tobi  Delbr\u00fcck", "Shih-Chii  Liu"], "year": 2021, "n_citations": 1}
{"id": 3131612, "s2_id": "a9f34ef630da7b4c0abc4b7d8d010f78c717056c", "title": "Cross-Stack Workload Characterization of Deep Recommendation Systems", "abstract": "Deep learning based recommendation systems form the backbone of most personalized cloud services. Though the computer architecture community has recently started to take notice of deep recommendation inference, the resulting solutions have taken wildly different approaches - ranging from near memory processing to at-scale optimizations. To better design future hardware systems for deep recommendation inference, we must first systematically examine and characterize the underlying systems-level impact of design decisions across the different levels of the execution stack. In this paper, we characterize eight industry-representative deep recommendation models at three different levels of the execution stack: algorithms and software, systems platforms, and hardware microarchitectures. Through this cross-stack characterization, we first show that system deployment choices (i.e., CPUs or GPUs, batch size granularity) can give us up to 15x speedup. To better understand the bottlenecks for further optimization, we look at both software operator usage breakdown and CPU frontend and backend microarchitectural inefficiencies. Finally, we model the correlation between key algorithmic model architecture features and hardware bottlenecks, revealing the absence of a single dominant algorithmic component behind each hardware bottleneck.", "venue": "2020 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Samuel  Hsia", "Udit  Gupta", "Mark  Wilkening", "Carole-Jean  Wu", "Gu-Yeon  Wei", "David  Brooks"], "year": 2020, "n_citations": 11}
{"id": 3132079, "s2_id": "f5b074d6aee35d0818a81d7789361367f472da68", "title": "Rendering Elimination: Early Discard of Redundant Tiles in the Graphics Pipeline", "abstract": "GPUs are one of the most energy-consuming components for real-time rendering applications, since a large number of fragment shading computations and memory accesses are involved. Main memory bandwidth is especially taxing battery-operated devices such as smartphones. Tile-Based Rendering GPUs divide the screen space into multiple tiles that are independently rendered in on-chip buffers, thus reducing memory bandwidth and energy consumption. We have observed that, in many animated graphics workloads, a large number of screen tiles have the same color across adjacent frames. In this paper, we propose Rendering Elimination (RE), a novel micro-architectural technique that accurately determines if a tile will be identical to the same tile in the preceding frame before rasterization by means of comparing signatures. Since RE identifies redundant tiles early in the graphics pipeline, it completely avoids the computation and memory accesses of the most power consuming stages of the pipeline, which substantially reduces the execution time and the energy consumption of the GPU. For widely used Android applications, we show that RE achieves an average speedup of 1.74x and energy reduction of 43% for the GPU/Memory system, surpassing by far the benefits of Transaction Elimination, a state-of-the-art memory bandwidth reduction technique available in some commercial Tile-Based Rendering GPUs.", "venue": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Marti  Anglada", "Enrique de Lucas", "Joan-Manuel  Parcerisa", "Juan L. Arag\u00f3n", "Pedro  Marcuello", "Antonio  Gonz\u00e1lez"], "year": 2019, "n_citations": 5}
{"id": 3133484, "s2_id": "8c4276c1a4a1f907f5c4c0d02af35c49f6e99277", "title": "Polynomial Circuit Verification using BDDs", "abstract": "Verification is one of the central tasks during circuit design. While most of the approaches have exponential worst-case behaviour, in the following techniques are discussed for proving polynomial circuit verification based on Binary Decision Diagrams (BDDs). It is shown that for circuits with specific structural properties, like e.g. tree-like circuits, and circuits based on multiplexers derived from BDDs complete formal verification can be carried out in polynomial time and space.", "venue": "ArXiv", "authors": ["Rolf  Drechsler"], "year": 2021, "n_citations": 0}
{"id": 3135569, "s2_id": "e76db65bab4304a267fd7700bd7a6825643ee205", "title": "DeepDive: An Integrative Algorithm/Architecture Co-Design for Deep Separable Convolutional Neural Networks", "abstract": "Deep Separable Convolutional Neural Network (DSCNN) has become the emerging paradigm by offering modular networks with structural sparsity to achieve higher accuracy with relatively lower operations and parameters. However, there is a lack of customized architectures that can provide flexible solutions that fit the sparsity of the DSCNNs. This paper introduces DeepDive, a fully-functional vertical co-design framework, for power-efficient implementation of DSCNNs on edge FPGAs. DeepDive's architecture supports crucial heterogeneous Compute Units (CUs) to fully support DSCNNs with various convolutional operators interconnected with structural sparsity. It offers FPGA-aware training and online quantization combined with modular synthesizable C++ CUs, customized for DSCNNs. The execution results on Xilinx's ZCU102 FPGA board demonstrate 47.4 and 233.3 FPS/Watt for MobileNet-V2 and a compact version of EfficientNet, respectively, as two state-of-the-art depthwise separable CNNs. These comparisons showcase how DeepDive improves FPS/Watt by 2.2\u00d7 and 1.51\u00d7 over Jetson Nano high and low power modes, respectively. It also enhances FPS/Watt by about 2.27\u00d7 and 37.25\u00d7 over two other FPGA implementations.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Mohammadreza  Baharani", "Ushma  Sunil", "Kaustubh  Manohar", "Steven  Furgurson", "Hamed  Tabkhi"], "year": 2021, "n_citations": 0}
{"id": 3141833, "s2_id": "9076a0f3df67e65655415c1944b590d4d7d4af7b", "title": "Programmable FPGA-based Memory Controller", "abstract": "Even with generational improvements in DRAM technology, memory access latency still remains the major bottleneck for application accelerators, primarily due to limitations in memory interface IPs which cannot fully account for variations in target applications, the algorithms used, and accelerator architectures. Since developing memory controllers for different applications is time-consuming, this paper introduces a modular and programmable memory controller that can be configured for different target applications on available hardware resources. The proposed memory controller efficiently supports cache-line accesses along with bulk memory transfers. The user can configure the controller depending on the available logic resources on the FPGA, memory access pattern, and external memory specifications. The modular design supports various memory access optimization techniques including, request scheduling, internal caching, and direct memory access. These techniques contribute to reducing the overall latency while maintaining high sustained bandwidth. We implement the system on a state-of-the-art FPGA and evaluate its performance using two widely studied domains: graph analytics and deep learning workloads. We show improved overall memory access time up to 58% on CNN and GCN workloads compared with commercial memory controller IPs.", "venue": "2021 IEEE Symposium on High-Performance Interconnects (HOTI)", "authors": ["Sasindu  Wijeratne", "Sanket  Pattnaik", "Zhiyu  Chen", "Rajgopal  Kannan", "Viktor  Prasanna"], "year": 2021, "n_citations": 1}
{"id": 3143082, "s2_id": "4d9df5d2a3ce62f263861f8e5ccde84d3cde76ec", "title": "Deadlock Recovery Technique in Bus Enhanced NoC Architecture", "abstract": "Increase in the speed of processors has led to crucial role of communication in the performance of systems. As a result, routing is taken into consideration as one of the most important subjects of the Network on Chip architecture. Routing algorithms to deadlock avoidance prevent packets route completely based on network traffic condition by means of restricting the route of packets. This action leads to less performance especially in non-uniform traffic patterns. On the other hand True Fully Adoptive Routing algorithm provides routing of packets completely based on traffic condition. However, deadlock detection and recovery mechanisms are needed to handle deadlocks. Use of global bus beside NoC as a parallel supportive environment, provide platform to offer advantages of both features of bus and NoC. This bus is useful for broadcast and multicast operations, sending delay sensitive signals, system management and other services. In this research, we use this bus as an escaping path for deadlock recovery technique. According to simulation results, this bus is suitable platform for deadlock recovery technique.", "venue": "VLSIC 2012", "authors": ["Saeid Sharifian Nia", "Abbas  Vafaei", "Hamid  Shahimohamadi"], "year": 2012, "n_citations": 3}
{"id": 3150263, "s2_id": "5984a0b6957b0e9ed8b030c3784f3ec367fbfd8c", "title": "SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems", "abstract": "Simple graph algorithms such as PageRank have been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this problem with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10 \u00d7 speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA\u2019s wide applicability.", "venue": "MICRO", "authors": ["Maciej  Besta", "Raghavendra  Kanakagiri", "Grzegorz  Kwasniewski", "Rachata  Ausavarungnirun", "Jakub  Ber\u00e1nek", "Konstantinos  Kanellopoulos", "Kacper  Janda", "Zur  Vonarburg-Shmaria", "Lukas  Gianinazzi", "Ioana  Stefan", "Juan  G\u00f3mez-Luna", "Marcin  Copik", "Lukas  Kapp-Schwoerer", "Salvatore Di Girolamo", "Marek  Konieczny", "Onur  Mutlu", "Torsten  Hoefler"], "year": 2021, "n_citations": 6}
{"id": 3152882, "s2_id": "f5b01d3da13e47a78eb65acabdd015b9034ff209", "title": "Architecture Support for FPGA Multi-tenancy in the Cloud", "abstract": "Cloud deployments now increasingly provision FPGA accelerators as part of virtual instances. While FPGAs are still essentially single-tenant, the growing demand for hardware acceleration will inevitably lead to the need for methods and architectures supporting FPGA multi-tenancy. In this paper, we propose an architecture supporting space-sharing of FPGA devices among multiple tenants in the cloud. The proposed architecture implements a network-on-chip (NoC) designed for fast data movement and low hardware footprint. Prototyping the proposed architecture on a Xilinx Virtex Ultrascale + demonstrated near specification maximum frequency for on-chip data movement and high throughput in virtual instance access to hardware accelerators. We demonstrate similar performance compared to single-tenant deployment while increasing FPGA utilization (we achieved $6 \\times$ higher FPGA utilization with our case study), which is one of the major goals of virtualization. Overall, our NoC interconnect achieved about $2 \\times$ higher maximum frequency than the state-of-the-art and a bandwidth of 25.6 Gbps.", "venue": "2020 IEEE 31st International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Joel Mandebi Mbongue", "Alex  Shuping", "Pankaj  Bhowmik", "Christophe  Bobda"], "year": 2020, "n_citations": 6}
{"id": 3155268, "s2_id": "82fd829ad1b9449ba433d5d5004e507805cb9c1d", "title": "Assertion Based Functional Verification of March Algorithm Based MBIST Controller", "abstract": "The thesis work presents assertion based functional verification of RTL representation of a digital design. The MBIST controller is designed based on a memory testing March algorithm. This March algorithm is a little modified March C algorithm which is modified by adding a paused element to test memory data retention faults. In assertion based functional verification, creation of verification plan, for MBIST controller RTL model and the implementation&simulation of the verification plan using System-Verilog and Synopsys-VCS are done. In ABV, verification plan includes the MBIST controller design and functional specification, functional coverage goals, code coverage goals, and assertions. Assertions are used to check the errors in RTL model of MBIST controller and to provide the functionality coverage. Functional coverage metrics are used to track the level or quality of verification. Most of the functional metrics score approximately reached the planned goal of 100 % which is planned in the verification plan. The designed MBIST controller is verified against the intended features. ABV approach helped to make the verification and design process efficient and less time-consuming by finding the bugs, exercising the corner cases in the design, and using the directed test cases in a small design. ABV helped to write directed and efficient test cases (25) which are approx 32 % less than the use of maximum possible random test cases (88) for designed MBIST controller with 100% assertion coverage and approximately equal total functional coverage, i.e., 97 % approx. In this way, ABV helped to fasten the design and verification process with better quality and assurance of correct functionality of MBIST controller after the integration in MBIST architecture.", "venue": "ArXiv", "authors": ["Ashwani  Kumar"], "year": 2021, "n_citations": 0}
{"id": 3155929, "s2_id": "d808dbdd6a53259bea908266397b3983f246aabc", "title": "Dissecting the Graphcore IPU Architecture via Microbenchmarking", "abstract": "This report focuses on the architecture and performance of the Intelligence Processing Unit (IPU), a novel, massively parallel platform recently introduced by Graphcore and aimed at Artificial Intelligence/Machine Learning (AI/ML) workloads. We dissect the IPU's performance behavior using microbenchmarks that we crafted for the purpose. We study the IPU's memory organization and performance. We study the latency and bandwidth that the on-chip and off-chip interconnects offer, both in point-to-point transfers and in a spectrum of collective operations, under diverse loads. We evaluate the IPU's compute power over matrix multiplication, convolution, and AI/ML primitives. We discuss actual performance in comparison with its theoretical limits. Our findings reveal how the IPU's architectural design affects its performance. Moreover, they offer simple mental models to predict an application's performance on the IPU, on the basis of the computation and communication steps it involves. This report is the natural extension to a novel architecture of a continuing effort of ours that focuses on the microbenchmark-based discovery of massively parallel architectures.", "venue": "ArXiv", "authors": ["Zhe  Jia", "Blake  Tillman", "Marco  Maggioni", "Daniele Paolo Scarpazza"], "year": 2019, "n_citations": 36}
{"id": 3156884, "s2_id": "b4df237615e0b8c59914d286745cc1ec0f899f4f", "title": "Phase distance mapping: a phase-based cache tuning methodology for embedded systems", "abstract": "Networked embedded systems typically leverage a collection of low-power embedded systems (nodes) to collaboratively execute applications spanning diverse application domains (e.g., video, image processing, communication, etc.) with diverse application requirements. The individual networked nodes must operate under stringent constraints (e.g., energy, memory, etc.) and should be specialized to meet varying applications\u2019 requirements in order to adhere to these constraints. Phase-based tuning specializes a system\u2019s tunable parameters to the varying runtime requirements of an application\u2019s different phases of execution to meet optimization goals. Since the design space for tunable systems can be very large, one of the major challenges in phase-based tuning is determining the best configuration for each phase without incurring significant tuning overhead (e.g., energy and/or performance) during design space exploration. In this paper, we propose phase distance mapping, which directly determines the best configuration for a phase, thereby eliminating design space exploration. Phase distance mapping applies the correlation between a known phase\u2019s characteristics and best configuration to determine a new phase\u2019s best configuration based on the new phase\u2019s characteristics. Experimental results verify that our phase distance mapping approach, when applied to cache tuning, determines cache configurations within 1\u00a0% of the optimal configurations on average and yields an energy delay product savings of 27\u00a0% on average.", "venue": "Des. Autom. Embed. Syst.", "authors": ["Tosiron  Adegbija", "Ann  Gordon-Ross", "Arslan  Munir"], "year": 2014, "n_citations": 10}
{"id": 3159920, "s2_id": "b5fa24c86c2f9967015ae310a6b90fd4c3a140be", "title": "Addressing Resiliency of In-Memory Floating Point Computation", "abstract": "In-memory computing (IMC) can eliminate the data movement between processor and memory which is a barrier to the energy-efficiency and performance in Von-Neumann computing. Resistive RAM (RRAM) is one of the promising devices for IMC applications (e.g. integer and Floating Point (FP) operations and random logic implementation) due to low power consumption, fast operation, and small footprint in crossbar architecture. In this paper, we propose FAME, a pipelined FP arithmetic (adder/subtractor) using RRAM crossbar based IMC. A novel shift circuitry is proposed to lower the shift overhead during FP operations. Since 96% of the RRAMs used in our architecture are in High Resistance State (HRS), we propose two approaches namely Shift-At-The-Output (SATO) and Force To VDD (FTV) (ground (FTG)) to mitigate Stuck-at-1 (SA1) failures. In both techniques, the fault-free RRAMs are exploited to perform the computation by using an extra clock cycle. Although performance degrades by 50%, SATO can handle 50% of the faults whereas FTV can handle 99% of the faults in the RRAM-based compute array at low power and area overhead. Simulation results show that the proposed single precision FP adder consumes 335 pJ and 322 pJ for NAND-NAND and NOR-NOR based implementations, respectively. The area overheads of SATO and FTV are 28.5% and 9.5%, respectively.", "venue": "ArXiv", "authors": ["Sina Sayyah Ensan", "Swaroop  Ghosh", "Seyedhamidreza  Motaman", "Derek  Weast"], "year": 2020, "n_citations": 0}
{"id": 3160116, "s2_id": "f21e5f1bf5b5e3d7f22f5baa966e4c5bf343ab9c", "title": "IChannels: Exploiting Current Management Mechanisms to Create Covert Channels in Modern Processors", "abstract": "To operate efficiently across a wide range of workloads with varying power requirements, a modern processor applies different current management mechanisms, which briefly throttle instruction execution while they adjust voltage and frequency to accommodate for power-hungry instructions (PHIs) in the instruction stream. Doing so 1) reduces the power consumption of non-PHI instructions in typical workloads and 2) optimizes system voltage regulators\u2019 cost and area for the common use case while limiting current consumption when executing PHIs.However, these mechanisms may compromise a system\u2019s confidentiality guarantees. In particular, we observe that multilevel side-effects of throttling mechanisms, due to PHI-related current management mechanisms, can be detected by two different software contexts (i.e., sender and receiver) running on 1) the same hardware thread, 2) co-located Simultaneous Multi-Threading (SMT) threads, and 3) different physical cores.Based on these new observations on current management mechanisms, we develop a new set of covert channels, IChannels, and demonstrate them in real modern Intel processors (which span more than 70% of the entire client and server processor market). Our analysis shows that IChannels provides more than 24\u00d7 the channel capacity of state-of-the-art power management covert channels. We propose practical and effective mitigations to each covert channel in IChannels by leveraging the insights we gain through a rigorous characterization of real systems.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Jawad  Haj-Yahya", "Jeremie S. Kim", "Abdullah Giray Yaglikci", "Ivan  Puddu", "Lois  Orosa", "Juan  G\u00f3mez-Luna", "Mohammed  Alser", "Onur  Mutlu"], "year": 2021, "n_citations": 2}
{"id": 3165071, "s2_id": "d9ecd118292a6fb255c759ee638d0c68f502c35a", "title": "A new embedded measurement structure for eDRAM capacitor", "abstract": "The embedded DRAM (eDRAM) is more and more used in system-on-chip (SOC). It is challenging to integrate the DRAM capacitor process into a logic process to get satisfactory yields. The specific process of DRAM capacitor and the low capacitance value (/spl sim/30 fF) of this device induce problems of process monitoring and failure analysis. We propose a new test structure to measure the capacitance value of each DRAM cell capacitor in a DRAM array. This concept has been validated by simulation on a 0.18 /spl mu/m eDRAM technology.", "venue": "Design, Automation and Test in Europe", "authors": ["Laurent  Lopez", "Jean Michel Portal", "Didier  N\u00e9e"], "year": 2005, "n_citations": 2}
{"id": 3166120, "s2_id": "901ed4bd732bc9590dc90234dab870d9944b1c40", "title": "FPGA Based Emulation Environment for Neuromorphic Architectures", "abstract": "Neuromorphic architectures such as IBM\u2019s TrueNorth and Intel\u2019s Loihi have been introduced as platforms for energy efficient spiking neural network execution. However, there is no framework that allows for rapidly experimenting with neuromorphic architectures and studying the trade space on hardware performance and network accuracy. Fundamentally, this creates a barrier to entry for hardware designers looking to explore neuromorphic architectures. In this paper we present an open-source FPGA based emulation environment for neuromorphic computing research. We prototype IBM\u2019s TrueNorth architecture as a reference design and discuss FPGA specific design decisions made when implementing and integrating it\u2019s core components. We conduct resource utilization analysis and realize a streaming-enabled TrueNorth architecture on the Zynq UltraScale+ MPSoC. We then perform functional verification by implementing networks for MNIST dataset and vector matrix multiplication (VMM) in our emulation environment and present an accuracy-based comparison based on the same networks generated using IBM\u2019s Compass simulation environment. We demonstrate the utility of our emulation environment for hardware designers and application engineers by altering the neuron behavior for VMM mapping, which is, to the best of our knowledge, not feasible with any other tool including IBM\u2019s Compass environment. The proposed parameterized and configurable emulation platform serves as a basis for expanding its features to support emerging architectures, studying hypothetical neuromorphic architectures, or rapidly converging to hardware configuration through incremental changes based on bottlenecks as they become apparent during application mapping process.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Spencer  Valancius", "Edward  Richter", "Ruben  Purdy", "Kris  Rockowitz", "Michael  Inouye", "Joshua  Mack", "Nirmal  Kumbhare", "Kaitlin  Fair", "John  Mixter", "Ali  Akoglu"], "year": 2020, "n_citations": 4}
{"id": 3177276, "s2_id": "7ef0669f09e3b1ae356faec906abb14d71c4b525", "title": "Maintaining Virtual Areas on FPGAs using Strip Packing with Delays", "abstract": "Every year, the computing resources available on dynamically partially reconfigurable devices increase enormously. In the near future, we expect many applications to run on a single reconfigurable device. In this paper, we present a concept for multitasking on dynamically partially reconfigurable systems called virtual area management. We explain its advantages, show its challenges, and discuss possible solutions. Furthermore, we investigate one problem in more detail: Packing modules with time-varying resource requests. This problem from the reconfigurable computing field results in a completely new optimization problem not tackled before. ILP-based and heuristic approaches are compared in an experimental study and the drawbacks and benefits discussed.", "venue": "ArXiv", "authors": ["Josef  Angermeier", "S\u00e1ndor P. Fekete", "Tom  Kamphans", "Nils  Schweer", "J\u00fcrgen  Teich"], "year": 2010, "n_citations": 2}
{"id": 3177893, "s2_id": "a5c1b1672b87ff582a332c90bb13d79605247e77", "title": "Demystifying the characteristics of 3D-stacked memories: A case study for Hybrid Memory Cube", "abstract": "Three-dimensional (3D)-stacking technology, which enables the integration of DRAM and logic dies, offers high bandwidth and low energy consumption. This technology also empowers new memory designs for executing tasks not traditionally associated with memories. A practical 3D-stacked memory is Hybrid Memory Cube (HMC), which provides significant access bandwidth and low power consumption in a small area. Although several studies have taken advantage of the novel architecture of HMC, its characteristics in terms of latency and bandwidth or their correlation with temperature and power consumption have not been fully explored. This paper is the first, to the best of our knowledge, to characterize the thermal behavior of HMC in a real environment using the AC-510 accelerator and to identify temperature as a new limitation for this state-of-the-art design space. Moreover, besides bandwidth studies, we deconstruct factors that contribute to latency and reveal their sources for high- and low-load accesses. The results of this paper demonstrates essential behaviors and performance bottlenecks for future explorations of packet-switched and 3D-stacked memories.", "venue": "2017 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Ramyad  Hadidi", "Bahar  Asgari", "Burhan Ahmad Mudassar", "Saibal  Mukhopadhyay", "Sudhakar  Yalamanchili", "Hyesoon  Kim"], "year": 2017, "n_citations": 37}
{"id": 3178031, "s2_id": "aad59e9d085ef832d4b465950d391f9ac80a9564", "title": "Scalable High Performance SDN Switch Architecture on FPGA for Core Networks", "abstract": "Due to the increasing heterogeneity in network user requirements, dynamically varying day to day network traffic patterns and delay in network service deployment, there is a huge demand for scalability and flexibility in modern networking infrastructure, which in return has paved way for the introduction of Software Defined Networking (SDN) in core networks. In this paper, we present an FPGA-based switch which is fully compliant with OpenFlow; the pioneering protocol for southbound interface of SDN. The switch architecture is completely implemented on hardware. The design consists of an OpenFlow Southbound agent which can process OpenFlow packets at a rate of 10Gbps. The architecture contains a primary pipeline which is capable of achieving core network throughputs and an auxiliary pipeline leading to the Openflow agent. Single clock cycle Content Accessible Memory (CAM) architecture supports the overall design to achieve its throughput and latency requirements. The proposed architecture speed scales up to 400Gbps while it consumes only 60% resources on a Xilinx Virtex-7 featuring XC7VX485T FPGA. Switch fabric is capable of connecting to a control plane running upon a host PC via PCIe which provides an opportunity at research level to explore SDN in core networks. Moreover, the architecture is experimented for different scaled versions using line rates of 10G, 25G and 100G. By using FPGA based embedded platforms which support sufficient number of ports and their line rates, this architecture can be deployed in core networks.", "venue": "FPGA", "authors": ["Sasindu  Wijeratne", "Ashen  Ekanayake", "Sandaruwan  Jayaweera", "Danuka  Ravishan", "Ajith  Pasqual"], "year": 2019, "n_citations": 3}
{"id": 3180025, "s2_id": "c1bf50648dbb1fbf1cc8dd4c934a81871cd9ac42", "title": "On Path Memory in List Successive Cancellation Decoder of Polar Codes", "abstract": "Polar code is a breakthrough in coding theory. Using list successive cancellation decoding with large list size L, polar codes can achieve excellent error correction performance. The L partial decoded vectors are stored in the path memory and updated according to the results of list management. In the state-of-the-art designs, the memories are implemented with registers and a large crossbar is used for copying the partial decoded vectors from one block of memory to another during the update. The architectures are quite area-costly when the code length and list size are large. To solve this problem, we propose two optimization schemes for the path memory in this work. First, a folded path memory architecture is presented to reduce the area cost. Second, we show a scheme that the path memory can be totally removed from the architecture. Experimental results show that these schemes effectively reduce the area of path memory.", "venue": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["ChenYang  Xia", "YouZhe  Fan", "Ji  Chen", "Chi-Ying  Tsui"], "year": 2018, "n_citations": 2}
{"id": 3181782, "s2_id": "7a53427ceee5458a6adc6987e7bb211578efbe74", "title": "FPGA Based Novel High Speed DAQ System Design with Error Correction", "abstract": "Present state of the art applications in the area of high energy physics experiments (HEP), radar communication, satellite communication and bio medical instrumentation require fault resilient data acquisition (DAQ) system with the data rate in the order of Gbps. In order to keep the high speed DAQ system functional in such radiation environment where direct intervention of human is not possible, a robust and error free communication system is necessary. In this work we present an efficient DAQ design and its implementation on field programmable gate array (FPGA). The proposed DAQ system supports high speed data communication (~4.8 Gbps) and achieves multi-bit error correction capabilities. BCH code (named after Raj Boseand D. K. Ray Chaudhuri) has been used for multi-bit error correction. The design has been implemented on Xilinx Kintex-7board and is tested for board to board communication as well as for board to PC using PCIe (Peripheral Component Interconnect express) interface. To the best of our knowledge, the proposed FPGA based high speed DAQ system utilizing optical link and multi-bit error resiliency can be considered first of its kind. Performance estimation of the implemented DAQ system is done based on resource utilization, critical path delay, efficiency and bit error rate (BER).", "venue": "2015 IEEE Computer Society Annual Symposium on VLSI", "authors": ["Swagata  Mandal", "Suman  Sau", "Amlan  Chakrabarti", "Jogendra  Saini", "Sushanta Kumar Pal", "Subhasis  Chattopadhyay"], "year": 2015, "n_citations": 3}
{"id": 3182552, "s2_id": "e553e70a3fc3b1b47f78701d128ad21658ba22ef", "title": "Impact of On-Chip Interconnect on In-Memory Acceleration of Deep Neural Networks", "abstract": "With the widespread use of Deep Neural Networks (DNNs), machine learning algorithms have evolved in two diverse directions \u2013 one with ever-increasing connection density for better accuracy and the other with more compact sizing for energy efficiency. The increase in connection density increases on-chip data movement, which makes efficient on-chip communication a critical function of the DNN accelerator. The contribution of this work is threefold. First, we illustrate that the point-to-point (P2P)-based interconnect is incapable of handling a high volume of on-chip data movement for DNNs. Second, we evaluate P2P and network-onchip (NoC) interconnect (with regular topology) for SRAMand ReRAM-based in-memory computing (IMC) architectures for a range of DNNs. This analysis shows the necessity for the optimal interconnect choice for an IMC DNN accelerator. Finally, we perform an experimental evaluation for different DNNs to empirically obtain the performance of the IMC architecture with both NoC-tree and NoC-mesh. We conclude that, at the tile-level, NoC-tree is appropriate for compact DNNs employed at the edge, and NoC-mesh is necessary to accelerate DNNs with high connection density. Furthermore, we propose a technique to determine the optimal choice of interconnect for any given DNN. In this technique, we use analytical models of NoC to evaluate end-to-end communication latency of any given DNN. We demonstrate that the interconnect optimization in the IMC architecture results in up to 6\u00d7 improvement in energy-delay-area product for VGG-19 inference compared to the state-of-the-art ReRAM-based IMC architectures.", "venue": "ArXiv", "authors": ["Gokul  Krishnan", "Sumit K. Mandal", "Chaitali  Chakrabarti", "Jae-sun  Seo", "\u00dcmit Y. Ogras", "Yu  Cao"], "year": 2021, "n_citations": 0}
{"id": 3185408, "s2_id": "9718cf04bb7095c5e59873b2f9726d9fb439bd49", "title": "Guardian: Symbolic Validation of Orderliness in SGX Enclaves", "abstract": "Modern processors can offer hardware primitives that allow a process to run in isolation. These primitives implement a trusted execution environment (TEE) in which a program can run such that the integrity and confidentiality of its execution are guaranteed. Intel's Software Guard eXtensions (SGX) is an example of such primitives and its isolated processes are called enclaves. These guarantees, however, can be easily thwarted if the enclave has not been properly designed. Its interface with the untrusted software stack is a perhaps the largest attack surface that adversaries can exploit; unintended interactions with untrusted code can expose the enclave to memory corruption attacks, for instance. In this paper, we propose a notion of an orderly enclave which splits its behaviour into the following execution phases: entry, secure, ocall, and exit. Each of them imposes a set of restrictions that enforce a particular policy of access to untrusted memory and, in some cases, sanitisation conditions. A violation of these policies and conditions might indicate an undesired interaction with untrusted data/code or a lack of sanitisation, both of which can be harnessed to perpetrate attacks against the enclave. We also introduce Guardian: an open-source tool that uses symbolic execution to carry out the validation of an enclave against our notion of an orderly enclave; in this process, it also looks for some other typical attack primitives. We discuss how our approach can prevent and flag enclave vulnerabilities that have been identified in the literature. Moreover, we have evaluated how our approach fares in the analysis of some enclave samples. In this process, Guardian identified some security issues previously undetected in some of these samples that were acknowledged and fixed by the corresponding maintainers.", "venue": "ArXiv", "authors": ["Pedro  Antonino", "Wojciech Aleksander Woloszyn", "A. W. Roscoe"], "year": 2021, "n_citations": 0}
{"id": 3188183, "s2_id": "249bd39b323e073df1a837f641d64a2c6c748a46", "title": "UVMBench: A Comprehensive Benchmark Suite for Researching Unified Virtual Memory in GPUs", "abstract": "The recent introduction of Unified Virtual Memory (UVM) in GPUs offers a new programming model that allows GPUs and CPUs to share the same virtual memory space, shifts the complex memory management from programmers to GPU driver/ hardware, and enables kernel execution even when memory is oversubscribed. Meanwhile, UVM may also incur considerable performance overhead due to the tracking and data migration along with the special handling of page faults and page table walk. As UVM is attracting significant attention from the research community to develop innovative solutions to these problems, in this paper, we propose a comprehensive UVM benchmark suite named UVMBench to facilitate future research on this important topic. The proposed UVMBench consists of 34 representative benchmarks from a wide range of application domains. The suite also features unified programming implementation and diverse memory access patterns across benchmarks, thus allowing thorough evaluation and comparison with current state-of-the-art. A set of experiments have been conducted on real GPUs to verify and analyze the benchmark suite behaviors under various scenarios.", "venue": "ArXiv", "authors": ["Yongbin  Gu", "Wenxuan  Wu", "Yunfan  Li", "Lizhong  Chen"], "year": 2020, "n_citations": 2}
{"id": 3188661, "s2_id": "46a9a0a513f6894dafba2dc16d95bc68dc9d61c0", "title": "Efficient Realization of Givens Rotation through Algorithm-Architecture Co-design for Acceleration of QR Factorization", "abstract": "We present efficient realization of Generalized Givens Rotation (GGR) based QR factorization that achieves 3-100x better performance in terms of Gflops/watt over state-of-the-art realizations on multicore, and General Purpose Graphics Processing Units (GPGPUs). GGR is an improvement over classical Givens Rotation (GR) operation that can annihilate multiple elements of rows and columns of an input matrix simultaneously. GGR takes 33% lesser multiplications compared to GR. For custom implementation of GGR, we identify macro operations in GGR and realize them on a Reconfigurable Data-path (RDP) tightly coupled to pipeline of a Processing Element (PE). In PE, GGR attains speed-up of 1.1x over Modified Householder Transform (MHT) presented in the literature. For parallel realization of GGR, we use REDEFINE, a scalable massively parallel Coarse-grained Reconfigurable Architecture, and show that the speed-up attained is commensurate with the hardware resources in REDEFINE. GGR also outperforms General Matrix Multiplication (gemm) by 10% in-terms of Gflops/watt which is counter-intuitive.", "venue": "ArXiv", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan", "Rainer  Leupers"], "year": 2018, "n_citations": 5}
{"id": 3190041, "s2_id": "e3576d513dc4d6ea2d7cd7f0f2e875e0144353e9", "title": "Yield enhancement of digital microfluidics-based biochips using space redundancy and local reconfiguration", "abstract": "As microfluidics-based biochips become more complex, manufacturing yield will have significant influence on production volume and product cost. We propose an interstitial redundancy approach to enhance the yield of biochips that are based on droplet-based microfluidics. In this design method, spare cells are placed in the interstitial sites within the microfluidic array, and they replace neighboring faulty cells via local reconfiguration. The proposed design method is evaluated using a set of concurrent real-life bioassays.", "venue": "Design, Automation and Test in Europe", "authors": ["Fei  Su", "Krishnendu  Chakrabarty", "Vamsee K. Pamula"], "year": 2005, "n_citations": 18}
{"id": 3190418, "s2_id": "695cd1990a775ca741e04af92889093447562fe0", "title": "Design space exploration for image processing architectures on FPGA targets", "abstract": "Due to the emergence of embedded applications in image and video processing, communication and cryptography, improvement of pictorial information for better human perception like deblurring, denoising in several fields such as satellite imaging, medical imaging, mobile applications etc. are gaining importance for renewed research. Behind such developments, the primary responsibility lies with the advancement of semiconductor technology leading to FPGA based programmable logic devices, which combines the advantages of both custom hardware and dedicated DSP resources. In addition, FPGA provides powerful reconfiguration feature and hence is an ideal target for rapid prototyping. We have endeavoured to exploit exceptional features of FPGA technology in respect to hardware parallelism leading to higher computational density and throughput, and have observed better performances than those one can get just merely porting the image processing software algorithms to hardware. In this paper, we intend to present an elaborate review, based on our expertise and experiences, on undertaking necessary transformation to an image processing software algorithm including the optimization techniques that makes its operation in hardware comparatively faster.", "venue": "ArXiv", "authors": ["Chandrajit  Pal", "Avik  Kotal", "Asit  Samanta", "Amlan  Chakrabarti", "Ranjan  Ghosh"], "year": 2014, "n_citations": 4}
{"id": 3192480, "s2_id": "d80402516a55592059d546d50f0f38ac2562f23e", "title": "DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware", "abstract": "Spiking Neural Networks (SNN) are an emerging computation model, which uses event-driven activation and bio-inspired learning algorithms. SNN-based machine-learning programs are typically executed on tilebased neuromorphic hardware platforms, where each tile consists of a computation unit called crossbar, which maps neurons and synapses of the program. However, synthesizing such programs on an off-the-shelf neuromorphic hardware is challenging. This is because of the inherent resource and latency limitations of the hardware, which impact both model performance, e.g., accuracy, and hardware performance, e.g., throughput. We propose DFSynthesizer, an end-to-end framework for synthesizing SNN-based machine learning programs to neuromorphic hardware. The proposed framework works in four steps. First, it analyzes a machine-learning program and generates SNN workload using representative data. Second, it partitions the SNN workload and generates clusters that fit on crossbars of the target neuromorphic hardware. Third, it exploits the rich semantics of Synchronous Dataflow Graph (SDFG) to represent a clustered SNN program, allowing for performance analysis in terms of key hardware constraints such as number of crossbars, dimension of each crossbar, buffer space on tiles, and tile communication bandwidth. Finally, it uses a novel scheduling algorithm to execute clusters on crossbars of the hardware, guaranteeing hardware performance. We evaluate DFSynthesizer with 10 commonly used machine-learning programs. Our results demonstrate that DFSynthesizer provides much tighter performance guarantee compared to current mapping approaches.", "venue": "ArXiv", "authors": ["Shihao  Song", "Harry  Chong", "Adarsha  Balaji", "Anup  Das", "James  Shackleford", "Nagarajan  Kandasamy"], "year": 2021, "n_citations": 4}
{"id": 3196185, "s2_id": "c5d73d3a257b0037d0fa1857c16b1c1f3c960aed", "title": "A hardware time manager implementation for the Xenomai real-time kernel of embedded Linux", "abstract": "Nowadays, the use of embedded operating systems in different embedded projects is subject to a tremendous growth. Embedded Linux is becoming one of those most popular EOSs due to its modularity, efficiency, reliability, and cost. One way to make it hard real-time is to include a real-time kernel like Xenomai. One of the key characteristics of a Real-Time Operating System (RTOS) is its ability to meet execution time deadlines deterministically. So, the more precise and flexible the time management can be, the better it can handle efficiently the determinism for different embedded applications. RTOS time precision is characterized by a specific periodic interrupt service controlled by a software time manager. The smaller the period of the interrupt, the better the precision of the RTOS, the more it overloads the CPU, and though reduces the overall efficiency of the RTOS. In this paper, we propose to drastically reduce these overheads by migrating the time management service of Xenomai into a configurable hardware component to relieve the CPU. The hardware component is implemented in a Field Programmable Gate Array coupled to the CPU. This work was achieved in a Master degree project where students could apprehend many fields of embedded systems: RTOS programming, hardware design, performance evaluation, etc.", "venue": "SIGBED", "authors": ["Pierre  Olivier", "Jalil  Boukhobza"], "year": 2012, "n_citations": 5}
{"id": 3197212, "s2_id": "0c255ee278aec255c684a0648127a02c669e3507", "title": "In-RDBMS Hardware Acceleration of Advanced Analytics", "abstract": "\n The data revolution is fueled by advances in machine learning, databases, and hardware design. Programmable accelerators are making their way into each of these areas independently. As such, there is a void of solutions that enables hardware acceleration at the intersection of these disjoint fields. This paper sets out to be the initial step towards a unifying solution for in-\n D\n atabase\n A\n cceleration of Advanced\n A\n nalytics (DAnA). Deploying specialized hardware, such as FPGAs, for in-database analytics currently requires hand-designing the hardware and manually routing the data. Instead, DAnA automatically maps a high-level specification of advanced analytics queries to an FPGA accelerator. The accelerator implementation is generated for a User Defined Function (UDF), expressed as a part of an SQL query using a Python-embedded Domain-Specific Language (DSL). To realize an efficient in-database integration, DAnA accelerators contain a novel hardware structure,\n Striders\n , that directly interface with the buffer pool of the database.\n Striders\n extract, cleanse, and process the training data tuples that are consumed by a multi-threaded FPGA engine that executes the analytics algorithm. We integrate DAnA with PostgreSQL to generate hardware accelerators for a range of real-world and synthetic datasets running diverse ML algorithms. Results show that DAnA-enhanced PostgreSQL provides, on average, 8.3\u00d7 end-to-end speedup for real datasets, with a maximum of 28.2\u00d7. Moreover, DAnA-enhanced PostgreSQL is, on average, 4.0\u00d7 faster than the multi-threaded Apache MADLib running on Greenplum. DAnA provides these benefits while hiding the complexity of hardware design from data scientists and allowing them to express the algorithm in \u224830-60 lines of Python.\n", "venue": "Proc. VLDB Endow.", "authors": ["Divya  Mahajan", "Joon Kyung Kim", "Jacob  Sacks", "Adel  Ardalan", "Arun  Kumar", "Hadi  Esmaeilzadeh"], "year": 2018, "n_citations": 30}
{"id": 3199525, "s2_id": "ebc4ff5dbb2a2728f9aaf29cb9c8bf35b1ea1e90", "title": "27.5-29.5 GHz Switched Array Sounder for Dynamic Channel Characterization: Design, Implementation and Measurements", "abstract": "A pre-requisite for the design of wireless systems is the understanding of the propagation channel. While a wealth of propagation knowledge exists for bands below 6 GHz, the same can not be said for bands approaching millimeter-wave frequencies. In this paper, we present the design, implementation and measurement-based verification of a re-configurable 27.5-29.5 GHz channel sounder for measuring dynamic directional channels. Based on the switched array principle, our design is capable of characterizing 128\u00d7256 dual-polarized channels with snapshot times of around 600 ms. This is in sharp contrast to measurement times on the order of tens-of-minutes with rotating horn antenna sounders. Our design lends itself to high angular resolution at both link ends with calibrated antenna arrays sampled at 2\u25e6 and 5\u25e6 intervals in the azimuth and elevation domains. This is complemented with a bandwidth of up to 2 GHz, enabling nanosecond-level delay resolution. The short measurement times and stable radio frequency design facilitates real-time processing and averaging of the received wavefronts to gain measurement signal-to-noise ratio and dynamic range. After disclosing the sounder design and implementation, we demonstrate its capabilities by presenting dynamic and static measurements at 28 GHz over a 1 GHz bandwidth in an office corridor environment.", "venue": "ArXiv", "authors": ["Harsh  Tataria", "Erik L. Bengtsson", "Ove  Edfors", "Fredrik  Tufvesson"], "year": 2021, "n_citations": 0}
{"id": 3201519, "s2_id": "ec6ca596b8a74e2a2c6a525fb9ccd3476c20eb3e", "title": "Revisiting RowHammer: An Experimental Analysis of Modern DRAM Devices and Mitigation Techniques", "abstract": "RowHammer is a circuit-level DRAM vulnerability, first rigorously analyzed and introduced in 2014, where repeatedly accessing data in a DRAM row can cause bit flips in nearby rows. The RowHammer vulnerability has since garnered significant interest in both computer architecture and computer security research communities because it stems from physical circuit-level interference effects that worsen with continued DRAM density scaling. As DRAM manufacturers primarily depend on density scaling to increase DRAM capacity, future DRAM chips will likely be more vulnerable to RowHammer than those of the past. Many RowHammer mitigation mechanisms have been proposed by both industry and academia, but it is unclear whether these mechanisms will remain viable solutions for future devices, as their overheads increase with DRAM\u2019s vulnerability to RowHammer.In order to shed more light on how RowHammer affects modern and future devices at the circuit-level, we first present an experimental characterization of RowHammer on 1580 DRAM chips (408$\\times$DDR3, 652$\\times$DDR4, and 520$\\times$LPDDR4) from 300 DRAM modules (60$\\times$DDR3, 110$\\times$DDR4, and 130$\\times$LPDDR4) with RowHammer protection mechanisms disabled, spanning multiple different technology nodes from across each of the three major DRAM manufacturers. Our studies definitively show that newer DRAM chips are more vulnerable to RowHammer: as device feature size reduces, the number of activations needed to induce a RowHammer bit flip also reduces, to as few as 9.6 k (4.8k to two rows each) in the most vulnerable chip we tested.We evaluate five state-of-the-art RowHammer mitigation mechanisms using cycle-accurate simulation in the context of real data taken from our chips to study how the mitigation mechanisms scale with chip vulnerability. We find that existing mechanisms either are not scalable or suffer from prohibitively large performance overheads in projected future devices given our observed trends of RowHammer vulnerability. Thus, it is critical to research more effective solutions to RowHammer.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Jeremie S. Kim", "Minesh  Patel", "A. Giray Yaglikci", "Hasan  Hassan", "Roknoddin  Azizi", "Lois  Orosa", "Onur  Mutlu"], "year": 2020, "n_citations": 45}
{"id": 3209000, "s2_id": "6a0f24b4cf630116d11bf710a5283d08c34dee3a", "title": "Instructional Level Parallelism", "abstract": "This paper is a review of the developments in Instruction level parallelism. It takes into account all the changes made in speeding up the execution. The various drawbacks and dependencies due to pipelining are discussed and various solutions to overcome them are also incorporated. It goes ahead in the last section to explain where is the new research leading us.", "venue": "ArXiv", "authors": ["Taposh  Dutta-Roy"], "year": 2019, "n_citations": 0}
{"id": 3209151, "s2_id": "f113cbe0df270fef2b69e93f10889b82f9d0c9eb", "title": "Hardware Versus Software Fault Injection of Modern Undervolted SRAMs", "abstract": "To improve power efficiency, researchers are experimenting with dynamically adjusting the supply voltage of systems below the nominal operating points. However, production systems are typically not allowed to function on voltage settings that is below the reliable limit. Consequently, existing software fault tolerance studies are based on fault models, which inject faults on random fault locations using fault injection techniques. In this work we study whether random fault injection is accurate to simulate the behavior of undervolted SRAMs. \nOur study extends the Gem5 simulator to support fault injection on the caches of the simulated system. The fault injection framework uses fault maps, which describe the faulty bits of SRAMs, as inputs. To compare random fault injection and hardware guided fault injection, we use two types of fault maps. The first type of maps are created through undervolting real SRAMs and observing the location of the erroneous bits, whereas the second type of maps are created by corrupting random bits of the SRAMs. During our study we corrupt the L1-Dcache of the simulated system and we monitor the behavior of the two types of fault maps on the resiliency of six benchmarks. The difference among the resiliency of a benchmark when tested with the different fault maps can be up to 24%.", "venue": "ArXiv", "authors": ["Muhammet Abdullah Soyturk", "Konstantinos  Parasyris", "Behzad  Salami", "Osman S. Unsal", "Gulay  Yalcin", "Leonardo  Bautista-Gomez"], "year": 2019, "n_citations": 0}
{"id": 3212494, "s2_id": "eae4d649942597b3e07844c84bf805cbeeba6e1a", "title": "Network-on-Chip with load balancing based on interleave of flits technique", "abstract": "This paper presents the evaluation of a Network-on-Chip (NoC) that offers load balancing for Systems-on-Chip (SoCs) dedicated for multimedia applications that require high traffic of variable bitrate communication. The NoC is based on a technique that allows the interleaving of flits from diferente flows in the same communication channel, and keep the load balancing without a centralized control in the network. For this purpose, all flits in the network received extra bits, such that every flit carries routing information. The routers use this extra information to perform arbitration and schedule the flits to the corresponding output ports. Analytic comparisons and experimental data show that the approach adopted in the network keeps average latency lower for variable bitrate flows than a network based on resource reservation when both networks are working over 80% of offered load.", "venue": "ArXiv", "authors": ["Marcelo Daniel Berejuck"], "year": 2015, "n_citations": 1}
{"id": 3213392, "s2_id": "5a801d0f9407dcdd8c8a34b3f0361ad4bc27a03d", "title": "Formal Verification of an Iterative Low-Power x86 Floating-Point Multiplier with Redundant Feedback", "abstract": "We present the formal verification of a low-power x86 floating-point multiplier. The multiplier operates iteratively and feeds back intermediate results in redundant representation. It supports x87 and SSE instructions in various precisions and can block the issuing of new instructions. The design has been optimized for low-power operation and has not been constrained by the formal verification effort. Additional improvements for the implementation were identified through formal verification. The formal verification of the design also incorporates the implementation of clock-gating and control logic. The core of the verification effort was based on ACL2 theorem proving. Additionally, model checking has been used to verify some properties of the floating-point scheduler that are relevant for the correct operation of the unit.", "venue": "ACL2", "authors": ["Peter-Michael  Seidel"], "year": 2011, "n_citations": 3}
{"id": 3213576, "s2_id": "d25af5dcd56695870b1569266f40e37bfad940b3", "title": "ELSA: A Throughput-Optimized Design of an LSTM Accelerator for Energy-Constrained Devices", "abstract": "The next significant step in the evolution and proliferation of artificial intelligence technology will be the integration of neural network (NN) models within embedded and mobile systems. This calls for the design of compact, energy efficient NN models in silicon. In this paper, we present a scalable ASIC design of an LSTM accelerator named ELSA, that is suitable for energy-constrained devices. It includes several architectural innovations to achieve small area and high energy efficiency. To reduce the area and power consumption of the overall design, the compute-intensive units of ELSA employ approximate multiplications and still achieve high performance and accuracy. The performance is further improved through efficient synchronization of the elastic pipeline stages to maximize the utilization. The paper also includes a performance model of ELSA, as a function of the hidden nodes and time steps, permitting its use for the evaluation of any LSTM application. ELSA was implemented in RTL and was synthesized and placed and routed in 65nm technology. Its functionality is demonstrated for language modeling-a common application of LSTM. ELSA is compared against a baseline implementation of an LSTM accelerator with standard functional units and without any of the architectural innovations of ELSA. The paper demonstrates that ELSA can achieve significant improvements in power, area and energy-efficiency when compared to the baseline design and several ASIC implementations reported in the literature, making it suitable for use in embedded systems and real-time applications.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Elham  Azari", "Sarma  Vrudhula"], "year": 2020, "n_citations": 1}
{"id": 3214432, "s2_id": "6f5881499c24bf1e2745096937b872778949110c", "title": "Critical Business Decision Making for Technology Startups: A PerceptIn Case Study", "abstract": "Most business decisions are made with analysis, but some are judgment calls not susceptible to analysis due to time or information constraints. In this article, we present a real-life case study of critical business decision making of PerceptIn, an autonomous driving technology startup: in early years of PerceptIn, PerceptIn had to make a decision on the design of computing systems for its autonomous vehicle products. By providing details on PerceptIn's decision process and the results of the decision, we hope to provide some insights that can be beneficial to entrepreneurs and engineering managers in technology startups.", "venue": "IEEE Engineering Management Review", "authors": ["Shaoshan  Liu"], "year": 2020, "n_citations": 4}
{"id": 3214496, "s2_id": "af520acfa846f4b4e2a4802d15d16fe216f0653a", "title": "Designer-driven topology optimization for pipelined analog to digital converters", "abstract": "The paper suggests a practical \"hybrid\" synthesis methodology which integrates designer-derived analytical models for system-level description with simulation-based models at the circuit level. We show how to optimize stage-resolution to minimize the power in a pipelined ADC. Exploration (via detailed synthesis) of several ADC configurations is used to show that a 4-3-2... resolution distribution uses the least power for a 13-bit 40 MSPS converter in a 0.25 /spl mu/m CMOS process.", "venue": "Design, Automation and Test in Europe", "authors": ["Yu-Tsun  Chien", "Dong  Chen", "Jea-Hong  Lou", "Gin-Kou  Ma", "Rob A. Rutenbar", "Tamal  Mukherjee"], "year": 2005, "n_citations": 6}
{"id": 3222948, "s2_id": "166b7972918149cea4232b8c4c317549171e194b", "title": "Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures", "abstract": "The manuscript describes fast and scalable architectures and associated algorithms for computing convolutions and cross-correlations. The basic idea is to map 2D convolutions and cross-correlations to a collection of 1D convolutions and cross-correlations in the transform domain. This is accomplished through the use of the discrete periodic radon transform for general kernels and the use of singular value decomposition -LU decompositions for low-rank kernels. The approach uses scalable architectures that can be fitted into modern FPGA and Zynq-SOC devices. Based on different types of available resources, for <inline-formula> <tex-math notation=\"LaTeX\">$P\\times P$ </tex-math></inline-formula> blocks, 2D convolutions and cross-correlations can be computed in just <inline-formula> <tex-math notation=\"LaTeX\">$O(P)$ </tex-math></inline-formula> clock cycles up to <inline-formula> <tex-math notation=\"LaTeX\">$O(P^{2})$ </tex-math></inline-formula> clock cycles. Thus, there is a trade-off between performance and required numbers and types of resources. We provide implementations of the proposed architectures using modern programmable devices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required resources, we show that the proposed approaches significantly outperform current methods.", "venue": "IEEE Transactions on Image Processing", "authors": ["Cesar  Carranza", "Daniel  Llamocca", "Marios  Pattichis"], "year": 2017, "n_citations": 11}
{"id": 3231591, "s2_id": "433a3739e6adfaff99a651b5c98e9040f3636f92", "title": "Content Addressable Parallel Processors on a FPGA", "abstract": "In this short article, we report on the implementation of a Content Addressable Parallel Processor using a FPGA. While Content addressable memories have been implemented in FPGAs, to our knowledge this is the first implementation in FPGA of Caxton C. Foster\u2019s vision of parallel processing, particularly the notions of parallel write as well as the combining of output values, which are usually missing in more typical CAM implementations, such as the ones designed for network routing. The resulting CAPP is made accessible to a host computer over a USB/UART interface, using a straightforward serial protocol that is demonstrated using a Python-based driver.", "venue": "ArXiv", "authors": ["Ayush  Salik", "Manor  Askenazi", "Edward  Rietman"], "year": 2021, "n_citations": 0}
{"id": 3236517, "s2_id": "a1a76cc05af359cfa00ca6cdfc03d6bf2aad0497", "title": "Fortifying Vehicular Security Through Low Overhead Physically Unclonable Functions", "abstract": "Within vehicles, the Controller Area Network (CAN) allows efficient communication between the electronic control units (ECUs) responsible for controlling the various subsystems. The CAN protocol was not designed to include much support for secure communication. The fact that so many critical systems can be accessed through an insecure communication network presents a major security concern. Adding security features to CAN is difficult due to the limited resources available to the individual ECUs and the costs that would be associated with adding the necessary hardware to support any additional security operations without overly degrading the performance of standard communication. Replacing the protocol is another option, but it is subject to many of the same problems. The lack of security becomes even more concerning as vehicles continue to adopt smart features. Smart vehicles have a multitude of communication interfaces an attacker could exploit to gain access to the networks. In this work, we propose a security framework that is based on physically unclonable functions (PUFs) and lightweight cryptography (LWC). The framework does not require any modification to the standard CAN protocol while also minimizing the amount of additional message overhead required for its operation. The improvements in our proposed framework result in major reduction in the number of CAN frames that must be sent during operation. For a system with 20 ECUs, for example, our proposed framework only requires 6.5% of the number of CAN frames that is required by the existing approach to successfully authenticate every ECU.", "venue": "ACM Journal on Emerging Technologies in Computing Systems", "authors": ["Carson  Labrado", "Himanshu  Thapliyal", "Saraju P. Mohanty"], "year": 2022, "n_citations": 0}
{"id": 3240886, "s2_id": "d4776335a928099422913c1fcb0b68d21b79cd79", "title": "Adaptable Register File Organization for Vector Processors", "abstract": "Modern scientific applications are getting more diverse, and the vector lengths in those applications vary widely. Contemporary Vector Processors (VPs) are designed either for short vector lengths, e.g., Fujitsu A64FX with 512-bit ARM SVE vector support, or long vectors, e.g., NEC Aurora Tsubasa with 16Kbits Maximum Vector Length (MVL). Unfortunately, both approaches have drawbacks. On the one hand, short vector length VP designs struggle to provide high efficiency for applications featuring long vectors with high Data Level Parallelism (DLP). On the other hand, long vector VP designs waste resources and underutilize the Vector Register File (VRF) when executing low DLP applications with short vector lengths. Therefore, those long vector VP implementations are limited to a specialized subset of applications, where relatively high DLP must be present to achieve excellent performance with high efficiency. To overcome these limitations, we propose an Adaptable Vector Architecture (AVA) that leads to having the best of both worlds. AVA is designed for short vectors (MVL=16 elements) and is thus area and energy-efficient. However, AVA has the functionality to reconfigure the MVL, thereby allowing to exploit the benefits of having a longer vector (up to 128 elements) microarchitecture when abundant DLP is present. We model AVA on the gem5 simulator and evaluate the performance with six applications taken from the RiVEC Benchmark Suite. To obtain area and power consumption metrics, we model AVA on McPAT for 22nm technology. Our results show that by reconfiguring our small VRF (8KB) plus our novel issue queue scheme, AVA yields a 2X speedup over the default configuration for short vectors. Additionally, AVA shows competitive performance when compared to a long vector VP, while saving 50% of area. 1The Maximum Vector Length (MVL) refers to the maximum number of elements held in each vector register. MVL is commonly selected at design time by the computer architect based on the VP target market.", "venue": "ArXiv", "authors": ["Crist'obal Ram'irez Lazo", "Enrico  Reggiani", "Carlos Rojas Morales", "Roger Figueras Bagu'e", "Luis Alfonso Villa Vargas", "Marco Antonio Ram'irez Salinas", "Mateo Valero Cort'es", "Osman Sabri Unsal", "Adri'an  Cristal"], "year": 2021, "n_citations": 0}
{"id": 3243732, "s2_id": "905466f2be5573d4bfff6e8e95e25dcf69b2dfb9", "title": "Evaluating FPGA Accelerator Performance with a Parameterized OpenCL Adaptation of the HPCChallenge Benchmark Suite", "abstract": "FPGAs have found increasing adoption in data center applications since a new generation of high-level tools have become available which noticeably reduce development time for FPGA accelerators and still provide high quality of results. There is however no high-level benchmark suite available which specifically enables a comparison of FPGA architectures, programming tools and libraries for HPC applications. \nTo fill this gap, we have developed an OpenCL-based open source implementation of the HPCC benchmark suite for Xilinx and Intel FPGAs. This benchmark can serve to analyze the current capabilities of FPGA devices, cards and development tool flows, track progress over time and point out specific difficulties for FPGA acceleration in the HPC domain. Additionally, the benchmark documents proven performance optimization patterns. We will continue optimizing and porting the benchmark for new generations of FPGAs and design tools and encourage active participation to create a valuable tool for the community.", "venue": "ArXiv", "authors": ["Marius  Meyer", "Tobias  Kenter", "Christian  Plessl"], "year": 2020, "n_citations": 2}
{"id": 3244913, "s2_id": "e300e6e39feb153647253effe7b64b87dbd27939", "title": "Copernicus: Characterizing the Performance Implications of Compression Formats Used in Sparse Workloads", "abstract": "Sparse matrices are the key ingredients of several application domains, from scientific computation to machine learning. The primary challenge with sparse matrices has been efficiently storing and transferring data, for which many sparse formats have been proposed to significantly eliminate zero entries. Such formats, essentially designed to optimize memory footprint, may not be as successful in performing faster processing. In other words, although they allow faster data transfer and improve memory bandwidth utilization -- the classic challenge of sparse problems -- their decompression mechanism can potentially create a computation bottleneck. Not only is this challenge not resolved, but also it becomes more serious with the advent of domain-specific architectures (DSAs), as they intend to more aggressively improve performance. The performance implications of using various formats along with DSAs, however, has not been extensively studied by prior work. To fill this gap of knowledge, we characterize the impact of using seven frequently used sparse formats on performance, based on a DSA for sparse matrix-vector multiplication (SpMV), implemented on an FPGA using high-level synthesis (HLS) tools, a growing and popular method for developing DSAs. Seeking a fair comparison, we tailor and optimize the HLS implementation of decompression for each format. We thoroughly explore diverse metrics, including decompression overhead, latency, balance ratio, throughput, memory bandwidth utilization, resource utilization, and power consumption, on a variety of real-world and synthetic sparse workloads.", "venue": "ArXiv", "authors": ["Bahar  Asgari", "Ramyad  Hadidi", "Joshua  Dierberger", "Charlotte  Steinichen", "Hyesoon  Kim"], "year": 2020, "n_citations": 0}
{"id": 3248797, "s2_id": "eb483b26d4a9a51e4e57ffa13adb37949b2fd930", "title": "Digital LDO with Time-Interleaved Comparators for Fast Response and Low Ripple", "abstract": "On-chip voltage regulation using distributed Digital Low Drop Out (LDO) voltage regulators has been identified as a promising technique for efficient power-management for emerging multi-core processors. Digital LDOs (DLDO) can offer low voltage operation, faster transient response, and higher current efficiency. Response time as well as output voltage ripple can be reduced by increasing the speed of the dynamic comparators. However, the comparator offset steeply increases for high clock frequencies, thereby leading to enhanced variations in output voltage. In this work we explore the design of digital LDOs with multiple dynamic comparators that can overcome this bottleneck. In the proposed topology, we apply time-interleaved comparators with the same voltage threshold and uniform current step in order to accomplish the aforementioned features. For a load step of 50mA, a DLDO with 8 time-interleaved comparators could achieve an output ripple of less than 5mV, while achieving a settling time of less than 0.5us. Load current dependant dynamic adjustment of clock frequency is proposed to maintain high current efficiency of ~97%.", "venue": "2016 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Sohail  Ahasan", "Saurav  Maji", "Kaushik  Roy", "Mrigank  Sharad"], "year": 2016, "n_citations": 3}
{"id": 3249911, "s2_id": "c6b44600ce61db90f31bf26143b3be890ee38fe3", "title": "Improving the process-variation tolerance of digital circuits using gate sizing and statistical techniques", "abstract": "A new approach for enhancing the process-variation tolerance of digital circuits is described. We extend recent advances in statistical timing analysis into an optimization framework. Our objective is to reduce the performance variance of a technology-mapped circuit where delays across elements are represented by random variables which capture the manufacturing variations. We introduce the notion of statistical critical paths, which account for both means and variances of performance variation. An optimization engine is used to size gates with the goal of reducing the timing variance along the statistical critical paths. We apply a pair of nested statistical analysis methods deploying a slower more accurate approach for tracking statistical critical paths and a fast engine for evaluation of gate size assignments. We derive a new approximation for the max operation on random variables which is deployed for the faster inner engine. Circuit optimization is carried out using a gain-based algorithm that terminates when constraints are satisfied or no further improvements can be made. We show optimization results that demonstrate an average of 72% reduction in performance variation at the expense of average 20% increase in design area.", "venue": "Design, Automation and Test in Europe", "authors": ["Osama  Neiroukh", "Xiaoyu  Song"], "year": 2005, "n_citations": 39}
{"id": 3252265, "s2_id": "fd1d6bfd848bcbc918ca967acfdf7e434d3ca2ff", "title": "A New Doctrine for Hardware Security", "abstract": "In this paper, we promote the idea that recent woes in hardware security are not because of a lack of technical solutions but rather because market forces and incentives prevent those with the ability to fix problems from doing so. At the root of the problem is the fact that hardware security comes at a cost; Present issues in hardware security can be seen as the result of the players in the game of hardware security finding ways of avoiding paying this cost. We formulate this idea into a doctrine of security, namely the Doctrine of Shared Burdens. Three cases studies---Rowhammer, Spectre, and Meltdown---are interpreted though the lens of this doctrine. Our doctrine illuminates why these problems and exist and what can be done about them.", "venue": "ArXiv", "authors": ["Adam  Hastings", "Simha  Sethumadhavan"], "year": 2020, "n_citations": 0}
{"id": 3257555, "s2_id": "aee174182c8233bf868e77345788c51d56105bfa", "title": "An efficient graph accelerator with parallel data conflict management", "abstract": "Graph-specific computing with the support of dedicated accelerator has greatly boosted the graph processing in both efficiency and energy. Nevertheless, their data conflict management is still sequential when certain vertex needs a large number of conflicting updates at the same time, leading to prohibitive performance degradation. This is particularly true and serious for processing natural graphs. In this paper, we have the insight that the atomic operations for the vertex updating of many graph algorithms (e.g., BFS, PageRank, and WCC) are typically incremental and simplex. This hence allows us to parallelize the conflicting vertex updates in an accumulative manner. We architect AccuGraph, a novel graph-specific accelerator that can simultaneously process atomic vertex updates for massive parallelism while ensuring the correctness. A parallel accumulator is designed to remove the serialization in atomic protections for conflicting vertex updates through merging their results in parallel. Our implementation on Xilinx FPGA with a wide variety of typical graph algorithms shows that our accelerator achieves an average throughput by 2.36 GTEPS as well as up to 3.14x performance speedup in comparison with state-of-the-art ForeGraph (with its single-chip version).", "venue": "PACT", "authors": ["Pengcheng  Yao"], "year": 2018, "n_citations": 24}
{"id": 3261836, "s2_id": "ce14009e2c40529fd64072713ba0f1251160fbc2", "title": "Reinforcement learning based interconnection routing for adaptive traffic optimization", "abstract": "Applying Machine Learning (ML) techniques to design and optimize computer architectures is a promising research direction. Optimizing the runtime performance of a Network-on-Chip (NoC) necessitates a continuous learning framework. In this work, we demonstrate the promise of applying reinforcement learning (RL) to optimize NoC runtime performance. We present three RL-based methods for learning optimal routing algorithms. The experimental results show the algorithms can successfully learn a near-optimal solution across different environment states.", "venue": "NOCS", "authors": ["Sheng-Chun  Kao", "Chao-Han Huck Yang", "Pin-Yu  Chen", "Xiaoli  Ma", "Tushar  Krishna"], "year": 2019, "n_citations": 1}
{"id": 3266621, "s2_id": "ee6136c6af582d1e71c126cfa39eaac799c2b7a5", "title": "GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks", "abstract": "Generative Adversarial Networks (GANs) are one of the most recent deep learning models that generate synthetic data from limited genuine datasets. GANs are on the frontier as further extension of deep learning into many domains (e.g., medicine, robotics, content synthesis) requires massive sets of labeled data that is generally either unavailable or prohibitively costly to collect. Although GANs are gaining prominence in various fields, there are no accelerators for these new models. In fact, GANs leverage a new operator, called transposed convolution, that exposes unique challenges for hardware acceleration. This operator first inserts zeros within the multidimensional input, then convolves a kernel over this expanded array to add information to the embedded zeros. Even though there is a convolution stage in this operator, the inserted zeros lead to underutilization of the compute resources when a conventional convolution accelerator is employed. We propose the GANAX architecture to alleviate the sources of inefficiency associated with the acceleration of GANs using conventional convolution accelerators, making the first GAN accelerator design possible. We propose a reorganization of the output computations to allocate compute rows with similar patterns of zeros to adjacent processing engines, which also avoids inconsequential multiply-adds on the zeros. This compulsory adjacency reclaims data reuse across these neighboring processing engines, which had otherwise diminished due to the inserted zeros. The reordering breaks the full SIMD execution model, which is prominent in convolution accelerators. Therefore, we propose a unified MIMD-SIMD design for GANAX that leverages repeated patterns in the computation to create distinct microprograms that execute concurrently in SIMD mode. The interleaving of MIMD and SIMD modes is performed at the granularity of single microprogrammed operation. To amortize the cost of MIMD execution, we propose a decoupling of data access from data processing in GANAX. This decoupling leads to a new design that breaks each processing engine to an access micro-engine and an execute micro-engine. The proposed architecture extends the concept of access-execute architectures to the finest granularity of computation for each individual operand. Evaluations with six GAN models shows, on average, 3.6x speedup and 3.1x energy savings over Eyeriss without compromising the efficiency of conventional convolution accelerators. These benefits come with a mere \u22487.8% area increase. These results suggest that GANAX is an effective initial step that paves the way for accelerating the next generation of deep neural models.", "venue": "2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Amir  Yazdanbakhsh", "Hajar  Falahati", "Philip J. Wolfe", "Kambiz  Samadi", "Nam Sung Kim", "Hadi  Esmaeilzadeh"], "year": 2018, "n_citations": 48}
{"id": 3268958, "s2_id": "842f6cf8a582cec915aa833e9969dda8c647d7e3", "title": "Energy-Efficient Accelerator Design for Deformable Convolution Networks", "abstract": "Deformable convolution networks (DCNs) proposed to address the image recognition with geometric or photometric variations typically involve deformable convolution that convolves on arbitrary locations of input features. The locations change with different inputs and induce considerable dynamic and irregular memory accesses which cannot be handled by classic neural network accelerators (NNAs). Moreover, bilinear interpolation (BLI) operation that is required to obtain deformed features in DCNs also cannot be deployed on existing NNAs directly. Although a general purposed processor (GPP) seated along with classic NNAs can process the deformable convolution, the processing on GPP can be extremely slow due to the lack of parallel computing capability. To address the problem, we develop a DCN accelerator on existing NNAs to support both the standard convolution and deformable convolution. Specifically, for the dynamic and irregular accesses in DCNs, we have both the input and output features divided into tiles and build a tile dependency table (TDT) to track the irregular tile dependency at runtime. With the TDT, we further develop an on-chip tile scheduler to handle the dynamic and irregular accesses efficiently. In addition, we propose a novel mapping strategy to enable parallel BLI processing on NNAs and apply layer fusion techniques for more energy-efficient DCN processing. According to our experiments, the proposed accelerator achieves orders of magnitude higher performance and energy efficiency compared to the typical computing architectures including ARM, ARM+TPU, and GPU with 6.6% chip area penalty to a classic NNA.", "venue": "ArXiv", "authors": ["Dawen  Xu", "Cheng  Chu", "Cheng  Liu", "Ying  Wang", "Huawei  Li", "Xiaowei  Li", "Kwang-Ting  Cheng"], "year": 2021, "n_citations": 0}
{"id": 3270064, "s2_id": "942b74a275ae75c516de184ec0331d972e1ae7d4", "title": "Ultra-Low Power Crypto-Engine Based on Simon 32/64 for Energy- and Area-Constrained Integrated Systems", "abstract": "This paper proposes an ultra-low power crypto-engine achieving sub-pJ/bit energy and sub-1K$\\mu$$m^2$ in 40nm CMOS, based on the Simon cryptographic algorithm. Energy and area efficiency are pursued via microarchitectural exploration, ultra-low voltage operation with high resiliency via latch-based pipelines, and power reduction techniques via multi-bit sequential elements. Overall, the comparison with the state of the art shows best-in-class energy efficiency and area. This makes it well suited for ubiquitous security in tightly-constrained platforms, e.g. RFIDs, low-end sensor nodes.", "venue": "ArXiv", "authors": ["Sachin  Taneja", "Massimo  Alioto"], "year": 2018, "n_citations": 3}
{"id": 3272677, "s2_id": "78bd6f185e24ba7decf501fc2b6f0f8acb6f9e88", "title": "Pointer-Chase Prefetcher for Linked Data Structures", "abstract": "Caches only exploit spatial and temporal locality in a set of address referenced in a program. Due to dynamic construction of linked data-structures, they are difficult to cache as the spatial locality between the nodes is highly dependent on the data layout. Prefetching can play an important role in improving the performance of linked data-structures. In this project, a pointer chase mechanism along with compiler hints is adopted to design a prefetcher for linked data-structures. The design is evaluated against the baseline of processor with cache in terms of performance, area and energy", "venue": "ArXiv", "authors": ["Nitish Kumar Srivastava", "Akshay Dilip Navalakha"], "year": 2018, "n_citations": 1}
{"id": 3273400, "s2_id": "07fa2422258c602c1b06e86f86d0315e933a4ebf", "title": "Near-Memory Computing: Past, Present, and Future", "abstract": "The conventional approach of moving data to the CPU for computation has become a significant performance bottleneck for emerging scale-out data-intensive applications due to their limited data reuse. At the same time, the advancement in 3D integration technologies has made the decade-old concept of coupling compute units close to the memory --- called near-memory computing (NMC) --- more viable. Processing right at the \"home\" of data can significantly diminish the data movement problem of data-intensive applications. \nIn this paper, we survey the prior art on NMC across various dimensions (architecture, applications, tools, etc.) and identify the key challenges and open issues with future research directions. We also provide a glimpse of our approach to near-memory computing that includes i) NMC specific microarchitecture independent application characterization ii) a compiler framework to offload the NMC kernels on our target NMC platform and iii) an analytical model to evaluate the potential of NMC.", "venue": "Microprocess. Microsystems", "authors": ["Gagandeep  Singh", "Lorenzo  Chelini", "Stefano  Corda", "Ahsan Javed Awan", "Sander  Stuijk", "Roel  Jordans", "Henk  Corporaal", "Albert-Jan  Boonstra"], "year": 2019, "n_citations": 25}
{"id": 3273945, "s2_id": "206e5a2b025374a095015aac9f8846e80b56825a", "title": "Improving 3D NAND Flash Memory Lifetime by Tolerating Early Retention Loss and Process Variation", "abstract": "Compared to planar NAND flash memory, 3D NAND flash memory uses a new flash cell design, and vertically stacks dozens of silicon layers in a single chip. This allows 3D NAND flash memory to increase storage density using a much less aggressive manufacturing process technology than planar NAND. The circuit-level and structural changes in 3D NAND flash memory significantly alter how different error sources affect the reliability of the memory. Our goal is to (1)~identify and understand these new error characteristics of 3D NAND flash memory, and (2)~develop new techniques to mitigate prevailing 3D NAND flash errors. \\chIIIn this paper, we perform a rigorous experimental characterization of real, state-of-the-art 3D NAND flash memory chips, and identify three new error characteristics that were not previously observed in planar NAND flash memory, but are fundamental to the new architecture of 3D NAND flash memory. \\beginenumerate [leftmargin=13pt] \u0131tem 3D NAND flash memory exhibits layer-to-layer process variation, a new phenomenon specific to the 3D nature of the device, where the average error rate of each 3D-stacked layer in a chip is significantly different. We are the first to provide detailed experimental characterization results of layer-to-layer process variation in real flash devices in open literature. Our results show that the raw bit error rate in the middle layer can be 6\u00d7 the error rate in the top layer. \u0131tem 3D NAND flash memory experiences \\emphearly retention loss, a new phenomenon where the number of errors due to charge leakage increases quickly within several hours after programming, but then increases at a much slower rate. We are the first to perform an extended-duration observation of early retention loss over the course of 24~days. Our results show that the retention error rate in a 3D NAND flash memory block quickly increases by an order of magnitude within $\\sim$3 hours after programming. \u0131tem 3D NAND flash memory experiences retention interference, a new phenomenon where the rate at which charge leaks from a flash cell is dependent on the amount of charge stored in neighboring flash cells. Our results show that charge leaks at a lower rate (i.e., the retention loss speed is slower) when the neighboring cell is in a state that holds more charge (i.e., a higher-voltage state). \\endenumerate Our experimental observations indicate that we must revisit the error models and error mitigation mechanisms devised for planar NAND flash, as they are no longer accurate for 3D NAND flash behavior. To this end, we develop \\emphnew analytical model\\chIs of (1)~the layer-to-layer process variation in 3D NAND flash memory, and (2)~retention loss in 3D NAND flash memory. Our models estimate the raw bit error rate (RBER), threshold voltage distribution, and the \\emphoptimal read reference voltage (i.e., the voltage at which RBER is minimized when applied during a read operation) for each flash page. Both models are useful for developing techniques to mitigate raw bit errors in 3D NAND flash memory. Motivated by our new findings and models, we develop four new techniques to mitigate process variation and early retention loss in 3D NAND flash memory. Our first technique, LaVAR, reduces process variation by fine-tuning the read reference voltage independently for each layer. Our second technique, LI-RAID, improves reliability by changing how pages are grouped under the RAID (Redundant Array of Independent Disks) error recovery technique, using information about layer-to-layer process variation to reduce the likelihood that the RAID recovery of a group could fail significantly earlier during the flash lifetime than recovery of other groups. Our third technique, ReMAR, reduces retention errors in 3D NAND flash memory by tracking the retention age of the data using our retention model and adapting the read reference voltage to data age. Our fourth technique, ReNAC, adapts the read reference voltage to the amount of retention interference to re-read the data after a read operation fails. These four techniques are complementary, and can be combined together to significantly improve flash memory reliability. Compared to a state-of-the-art baseline, our techniques, when combined, improve flash memory lifetime by 1.85\u00d7. Alternatively, if a NAND flash manufacturer wants to keep the lifetime of the 3D NAND flash memory device constant, our techniques reduce the storage overhead required to hold error correction information by 78.9%. For more information on our new experimental characterization of modern 3D NAND flash memory chips and our proposed models and techniques, please refer to the full version of our paper~\\citeluo.pomacs18.", "venue": "Abstracts of the 2018 ACM International Conference on Measurement and Modeling of Computer Systems", "authors": ["Yixin  Luo", "Saugata  Ghose", "Yu  Cai", "Erich F. Haratsch", "Onur  Mutlu"], "year": 2018, "n_citations": 53}
{"id": 3275056, "s2_id": "af30fd8856c956e78d7dc663c72669cb89d606d3", "title": "NVMExplorer: A Framework for Cross-Stack Comparisons of Embedded Non-Volatile Memories", "abstract": "Repeated off-chip memory access to DRAM drive up operating power for data-intensive applications, and SRAM technology scaling and leakage power limits the efficiency of embedded memories. Future on-chip storage will need higher density and energy efficiency, and the actively expanding field of emerging, embeddable non-volatile memory (eNVM) technologies is providing many potential candidates to satisfy this need. Each technology proposal presents distinct trade-offs in terms of density, read, write, and reliability characteristics, and we present a comprehensive framework for navigating and quantifying these design trade-offs alongside realistic system constraints and application-level impacts. This work evaluates eNVM-based storage for a range of application and system contexts including machine learning on the edge, graph analytics, and general purpose cache hierarchy, in addition to describing a freely available (http://nvmexplorer.seas.harvard.edu/) set of tools for application experts, system designers, and device experts to better understand, compare, and quantify the next generation of embedded memory solutions.", "venue": "ArXiv", "authors": ["Lillian  Pentecost", "Alexander  Hankin", "Marco  Donato", "Mark  Hempstead", "Gu-Yeon  Wei", "David  Brooks"], "year": 2021, "n_citations": 0}
{"id": 3276323, "s2_id": "cbe8417449f19639ed71dbd8387bc02354f72f0b", "title": "A Methodology for Efficient Space-Time Adapter Design Space Exploration: A Case Study of an Ultra Wide Band Interleaver", "abstract": "This paper presents a solution to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall architecture of the system is significantly affected by communication architecture, so the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of input/output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a resource constraints graph (RCG). The RCG properties enable an efficient architecture space exploration in order to synthesize a STAR component. The proposed approach has been tested to design an industrial data mixing block example: an ultra-wideband interleaver.", "venue": "2007 IEEE International Symposium on Circuits and Systems", "authors": ["Cyrille  Chavet", "Philippe  Coussy", "Pascal  Urard", "Eric  Martin"], "year": 2007, "n_citations": 8}
{"id": 3278304, "s2_id": "33bc3086ec39e321f01461ca86dc34e4310c9d62", "title": "A 481pJ/decision 3.4M decision/s Multifunctional Deep In-memory Inference Processor using Standard 6T SRAM Array", "abstract": "This paper describes a multi-functional deep in-memory processor for inference applications. Deep in-memory processing is achieved by embedding pitch-matched low-SNR analog processing into a standard 6T 16KB SRAM array in 65 nm CMOS. Four applications are demonstrated. The prototype achieves up to 5.6X (9.7X estimated for multi-bank scenario) energy savings with negligible (<1%) accuracy degradation in all four applications as compared to the conventional architecture.", "venue": "ArXiv", "authors": ["Mingu  Kang", "Sujan K. Gonugondla", "Ameya  Patil", "Naresh R. Shanbhag"], "year": 2016, "n_citations": 24}
{"id": 3287909, "s2_id": "09d2130c4c35a57626e94689ea3c7a02c9698141", "title": "EffiTest: Efficient delay test and statistical prediction for configuring post-silicon tunable buffers", "abstract": "At nanometer manufacturing technology nodes, process variations significantly affect circuit performance. To combat them, postsilicon clock tuning buffers can be deployed to balance timing budgets of critical paths for each individual chip after manufacturing. The challenge of this method is that path delays should be measured for each chip to configure the tuning buffers properly. Current methods for this delay measurement rely on path-wise frequency stepping. This strategy, however, requires too much time from expensive testers. In this paper, we propose an efficient delay test framework (EffiTest) to solve the post-silicon testing problem by aligning path delays using the already-existing tuning buffers in the circuit. In addition, we only test representative paths and the delays of other paths are estimated by statistical delay prediction. Experimental results demonstrate that the proposed method can reduce the number of frequency stepping iterations by more than 94% with only a slight yield loss.", "venue": "2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)", "authors": ["Grace Li Zhang", "Bing  Li", "Ulf  Schlichtmann"], "year": 2016, "n_citations": 21}
{"id": 3288551, "s2_id": "8bb7d2d98895ad2f53e84251e401121359a8af21", "title": "A Row-Parallel 8$\\,\\times\\,$ 8 2-D DCT Architecture Using Algebraic Integer-Based Exact Computation", "abstract": "An algebraic integer (AI)-based time-multiplexed row-parallel architecture and two final reconstruction step (FRS) algorithms are proposed for the implementation of bivariate AI encoded 2-D discrete cosine transform (DCT). The architecture directly realizes an error-free 2-D DCT without using FRSs between row-column transforms, leading to an 8 \u00d7 8 2-D DCT that is entirely free of quantization errors in AI basis. As a result, the user-selectable accuracy for each of the coefficients in the FRS facilitates each of the 64 coefficients to have its precision set independently of others, avoiding the leakage of quantization noise between channels as is the case for published DCT designs. The proposed FRS uses two approaches based on: 1) optimized Dempster-Macleod multipliers, and 2) expansion factor scaling. This architecture enables low-noise high-dynamic range applications in digital video processing that requires full control of the finite-precision computation of the 2-D DCT. The proposed architectures and FRS techniques are experimentally verified and validated using hardware implementations that are physically realized and verified on field-programmable gate array (FPGA) chip. Six designs, for 4-bit and 8-bit input word sizes, using the two proposed FRS schemes, have been designed, simulated, physically implemented, and measured. The maximum clock rate and block rate achieved among 8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a pixel rate of 8 \u00d7 307.787\u22482.462 GHz if eventually embedded in a real- time video-processing system. The equivalent frame rate is about 1187.35Hz for the image size of 1920 \u00d7 1080. All implementations are functional on a Xilinx Virtex-6 XC6VLX240T FPGA device.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "authors": ["Arjuna  Madanayake", "Renato J. Cintra", "Denis  Onen", "Vassil S. Dimitrov", "Nilanka T. Rajapaksha", "Leonard T. Bruton", "Amila  Edirisuriya"], "year": 2012, "n_citations": 25}
{"id": 3289432, "s2_id": "3077a7436b9be2ba69c1a71246627a4b858d0bb1", "title": "Data-stationary Architecture to Execute Quantum Algorithms Classically", "abstract": "This paper presents a data stationary architecture in which each word has an attached address field. Address fields massively update in parallel to record data interchanges. Words do not move until memory is read for post processing. A sea of such cells can test large-scale quantum algorithms, although other programming is possible.", "venue": "ArXiv", "authors": ["John Robert Burger"], "year": 2004, "n_citations": 3}
{"id": 3298979, "s2_id": "0336201a30c862c7ab7b122ef2ab59d044a1a72f", "title": "Area and throughput trade-offs in the design of pipelined discrete wavelet transform architectures", "abstract": "The JPEG2000 standard defines the discrete wavelet transform (DWT) as a linear space-to frequency transform of the image domain in an irreversible compression. This irreversible discrete wavelet transform is implemented by an FIR filter using 9/7 Daubechies coefficients or a lifting scheme of factorized coefficients from 9/7 Daubechies coefficients. The paper investigates the tradeoffs between area, power and data throughput (or operating frequency) of several implementations of the discrete wavelet transform using the lifting scheme in various pipeline designs. The paper shows the results of five different architectures synthesized and simulated in FPGAs. It concludes that the descriptions with pipelined operators provide the best area-power-operating frequency trade-off over non-pipelined operator descriptions. Those descriptions require around 40% more hardware to increase the maximum operating frequency up to 100% and reduce power consumption to less than 50%. Starting from behavioral HDL descriptions provides the best area-power-operating frequency trade-off, improving hardware cost and maximum operating frequency by around 30% in comparison to structural descriptions for the same power requirement.", "venue": "Design, Automation and Test in Europe", "authors": ["Sandro V. Silva", "Sergio  Bampi"], "year": 2005, "n_citations": 18}
{"id": 3300892, "s2_id": "b8e8d87107d8f7c22cc5c3c8ee1bf21e9f7ae931", "title": "A Streaming Accelerator for Deep Convolutional Neural Networks with Image and Feature Decomposition for Resource-limited System Applications", "abstract": "Deep convolutional neural networks (CNN) are widely used in modern artificial intelligence (AI) and smart vision systems but also limited by computation latency, throughput, and energy efficiency on a resource-limited scenario, such as mobile devices, internet of things (IoT), unmanned aerial vehicles (UAV), and so on. A hardware streaming architecture is proposed to accelerate convolution and pooling computations for state-of-the-art deep CNNs. It is optimized for energy efficiency by maximizing local data reuse to reduce off-chip DRAM data access. In addition, image and feature decomposition techniques are introduced to optimize memory access pattern for an arbitrary size of image and number of features within limited on-chip SRAM capacity. A prototype accelerator was implemented in TSMC 65 nm CMOS technology with 2.3 mm x 0.8 mm core area, which achieves 144 GOPS peak throughput and 0.8 TOPS/W peak energy efficiency.", "venue": "ArXiv", "authors": ["Yuan  Du", "Li  Du", "Yilei  Li", "Junjie  Su", "Mau-Chung Frank Chang"], "year": 2017, "n_citations": 6}
{"id": 3303699, "s2_id": "ca5a9d75ebffa8beff0ffe978795df9ebe0c8382", "title": "Quantum Computing - from NISQ to PISQ", "abstract": "1 Abstract\u2014After spending 9 years in Quantum Computing and given the impeding timeline of developing good quality quantum processing units, it is the moment to rethink the approach to advance quantum computing research. Rather than waiting for quantum hardware technologies to mature, we need to start assessing in tandem the impact of the occurrence of quantum computing in various scientific fields. However, to this purpose, we need to use a complementary but quite different approach than proposed by the NISQ vision, which is heavily focused on and burdened by the engineering challenges. That is why we propose and advocate the PISQ-approach: Perfect Intermediate-Scale Quantum computing based on the already known concept of perfect qubits. This will allow researchers to focus much more on the development of new applications by defining the algorithms in terms of perfect qubits and evaluate them on quantum computing simulators that are executed on supercomputers. It is not the long-term solution but it will allow universities to currently develop research on quantum logic and algorithms and companies can already start developing their internal know-how on quantum solutions.", "venue": "IEEE Micro", "authors": ["Koen  Bertels", "Aritra  Sarkar", "Imran  Ashraf"], "year": 2021, "n_citations": 1}
{"id": 3305010, "s2_id": "00eb88e6bbc3564d4b26c0c5f3a664f7a04dd771", "title": "A formalisation of XMAS", "abstract": "Communication fabrics play a key role in the correctness and performance of modern multi-core processors and systems-on-chip. To enable formal verification, a recent trend is to use high-level micro-architectural models to capture designers\u2019 intent about the communication and processing of messages. Intel proposed the XMAS language to support the formal definition of executable specifications of micro-architectures. We formalise the semantics of XMAS in ACL2. Our formalisation represents the computation of the values of all wires of a design. Our main function computes a set of possible routing targets for each message and whether a message can make progress according to the current network state. We prove several properties on the semantics, including termination, non-emptiness of routing, and correctness of progress conditions. Our current effort focuses on a basic subset of the entire XMAS language, which includes queues, functions, and switches.", "venue": "ACL2", "authors": ["Bernard van Gastel", "Julien  Schmaltz"], "year": 2013, "n_citations": 7}
{"id": 3314256, "s2_id": "80204a8bab8a07c3e1bca8562cfe216bfd7415dc", "title": "FPGA based High Speed Data Acquisition System for High Energy Physics Application", "abstract": "In high energy physics experiments (HEP), high speed and fault resilient data communication is needed between detectors/sensors and the host PC. Transient faults can occur in the communication hardware due to various external effects like presence of charged particles, noise in the environment or radiation effects in HEP experiments and that leads to single/multiple bit error. In order to keep the communication system functional in such a radiation environment where direct intervention of human is not possible, a high speed data acquisition (DAQ) architecture is necessary which supports error recovery. This design presents an efficient implementation of field programmable gate array (FPGA) based high speed DAQ system with optical communication link supported by multi-bit error correcting model. The design has been implemented on Xilinx Kintex-7 board and is tested for board to board communication as well as for PC communication using PCI (Peripheral Component Interconnect express). Data communication speed up to 4.8 Gbps has been achieved in board to board and board to PC communication and estimation of resource utilization and critical path delay are also measured.", "venue": "ArXiv", "authors": ["Swagata  Mandal", "Suman  Sau", "Amlan  Chakrabarti", "Subhasis  Chattopadhyay"], "year": 2015, "n_citations": 0}
{"id": 3317250, "s2_id": "a3917d235a14a4871872eb088f1fdf9c4c53cccb", "title": "Arrow: A RISC-V Vector Accelerator for Machine Learning Inference", "abstract": "In this paper we present Arrow, a configurable hardware accelerator architecture that implements a subset of the RISC-V v0.9 vector ISA extension aimed at edge machine learning inference. Our experimental results show that an Arrow co-processor can execute a suite of vector and matrix benchmarks fundamental to machine learning inference 2 \u2013 78\u00d7 faster than a scalar RISC processor while consuming 20% \u2013 99% less energy when implemented in a Xilinx XC7A200T-1SBG484C FPGA.", "venue": "ArXiv", "authors": ["Imad Al Assir", "Mohamad El Iskandarani", "Hadi Rayan Al Sandid", "Mazen A. R. Saghir"], "year": 2021, "n_citations": 0}
{"id": 3321800, "s2_id": "856487531e32274250221b48989afbaca4fa8084", "title": "Modeling of a reconfigurable OFDM IP block family for an RF system simulator", "abstract": "The idea of designing a domain specific mother model of an IP block family as a basis for modeling of system integration is presented here. A common reconfigurable mother model for ten different standardized digital OFDM transmitters has been developed. By means of a set of parameters, the mother model can be reconfigured to any of the ten selected standards. So far the applicability of the proposed reconfiguration and analog-digital co-modeling methods have been proved by modeling the function of the digital parts of three, IEEE 802.11a, ADSL and DRM, transmitters in an RF system simulator. The model is intended to be used as signal source template in RF system simulations. The concept is not restricted to signal sources, it can be applied to any IP block development. The idea of the mother model is applied in other design domains to prove that in certain application areas, OFDM transceivers in this case, the design process can progress simultaneously in different design domains - mixed signal, system and RTL-architectural - without the need for high-level synthesis. Only the mother models of three design domains are needed to be formally proved to function as specified.", "venue": "Design, Automation and Test in Europe", "authors": ["Hannu  Heusala", "Jussi  Liedes"], "year": 2005, "n_citations": 1}
{"id": 3322275, "s2_id": "c2e3caf690fd5f4d514236a430e6a1370c5080d7", "title": "Pangloss: a novel Markov chain prefetcher", "abstract": "We present Pangloss, an efficient high-performance data prefetcher that approximates a Markov chain on delta transitions. With a limited information scope and space/logic complexity, it is able to reconstruct a variety of both simple and complex access patterns. This is achieved by a highly-efficient representation of the Markov chain to provide accurate values for transition probabilities. In addition, we have added a mechanism to reconstruct delta transitions originally obfuscated by the out-of-order execution or page transitions, such as when streaming data from multiple sources. Our single-level (L2) prefetcher achieves a geometric speedup of 1.7% and 3.2% over selected state-of-the-art baselines (KPCP and BOP). When combined with an equivalent for the L1 cache (L1 & L2), the speedups rise to 6.8% and 8.4%, and 40.4% over non-prefetch. In the multi-core evaluation, there seems to be a considerable performance improvement as well.", "venue": "ArXiv", "authors": ["Philippos  Papaphilippou", "Paul H. J. Kelly", "Wayne  Luk"], "year": 2019, "n_citations": 2}
{"id": 3323876, "s2_id": "70741dca9a231f6866635c0707441e595b99714f", "title": "A Cache Reconfiguration Approach for Saving Leakage and Refresh Energy in Embedded DRAM Caches", "abstract": "In recent years, the size and leakage energy consumption of large last level caches (LLCs) has increased. To address this, embedded DRAM (eDRAM) caches have been considered which have lower leakage energy consumption; however eDRAM caches consume a significant amount of energy in the form of refresh energy. In this paper, we present a technique for saving both leakage and refresh energy in eDRAM caches. We use dynamic cache reconfiguration approach to intelligently turn-off part of the cache to save leakage energy and refresh only valid data of the active (i.e. not turned-off) cache to save refresh energy. We evaluate our technique using an x86-64 simulator and SPEC2006 benchmarks and compare it with a recently proposed technique for saving refresh energy, named Refrint. The experiments have shown that our technique provides better performance and energy efficiency than Refrint. Using our technique, for a 2MB LLC and 40 micro-seconds eDRAM refresh period, the average saving in energy over eDRAM baseline (which periodically refreshes all cache lines) is 22.8%.", "venue": "ArXiv", "authors": ["Sparsh  Mittal"], "year": 2013, "n_citations": 12}
{"id": 3325572, "s2_id": "06ae8b623abcc42626bdad95e32022073bf2e2cf", "title": "Efficient Communication Acceleration for Next-Gen Scale-up Deep Learning Training Platforms", "abstract": "Deep Learning (DL) training platforms are built by interconnecting multiple DL accelerators (e.g., GPU/TPU) via fast, customized interconnects. As the size of DL models and the compute efficiency of the accelerators has continued to increase, there has also been a corresponding steady increase in the bandwidth of these interconnects.Systems today provide 100s of gigabytes (GBs) of inter-connect bandwidth via a mix of solutions such as Multi-Chip packaging modules (MCM) and proprietary interconnects(e.g., NVlink) that together from the scale-up network of accelerators. However, as we identify in this work, a significant portion of this bandwidth goes under-utilized. This is because(i) using compute cores for executing collective operations such as all-reduce decreases overall compute efficiency, and(ii) there is memory bandwidth contention between the accesses for arithmetic operations vs those for collectives, and(iii) there are significant internal bus congestions that increase the latency of communication operations. To address this challenge, we propose a novel microarchitecture, calledAccelerator Collectives Engine(ACE), forDL collective communication offload. ACE is a smart net-work interface (NIC) tuned to cope with the high-bandwidth and low latency requirements of scale-up networks and is able to efficiently drive the various scale-up network systems(e.g. switch-based or point-to-point topologies). We evaluate the benefits of the ACE with micro-benchmarks (e.g. single collective performance) and popular DL models using an end-to-end DL training simulator. For modern DL workloads, ACE on average increases the net-work bandwidth utilization by 1.97X, resulting in 2.71X and 1.44X speedup in iteration time for ResNet-50 and GNMT, respectively.", "venue": "ArXiv", "authors": ["Saeed  Rashidi", "Srinivas  Sridharan", "Sudarshan  Srinivasan", "Matthew  Denton", "Tushar  Krishna"], "year": 2020, "n_citations": 1}
{"id": 3325659, "s2_id": "7e9fa172f7dea4f8749f6cf1cb5e3119e62aa6b5", "title": "Efficient Hardware Realizations of Feedforward Artificial Neural Networks", "abstract": "This article presents design techniques proposed for efficient hardware implementation of feedforward artificial neural networks (ANNs) under parallel and time-multiplexed architectures. To reduce their design complexity, after the weights of ANN are determined in a training phase, we introduce a technique to find the minimum quantization value used to convert the floating-point weight values to integers. For each design architecture, we also propose an algorithm that tunes the integer weights to reduce the hardware complexity avoiding a loss in the hardware accuracy. Furthermore, the multiplications of constant weights by input variables are implemented under the shift-adds architecture using the fewest number of addition/subtraction operations found by prominent previously proposed algorithms. Finally, we introduce a computer-aided design (CAD) tool, called SIMURG, that can describe an ANN design in hardware automatically based on the ANN structure and the solutions of proposed design techniques and algorithms. Experimental results indicate that the tuning techniques can significantly reduce the ANN hardware complexity under a design architecture and the multiplierless design of ANN can lead to a significant reduction in area and energy consumption, increasing the latency slightly.", "venue": "ArXiv", "authors": ["Mohammadreza Esmali Nojehdeh", "Sajjad  Parvin", "Mustafa  Altun"], "year": 2021, "n_citations": 0}
{"id": 3329356, "s2_id": "598f569c0f4ec87a14d0cccae71bcc144fc6467b", "title": "AutoML for Multilayer Perceptron and FPGA Co-design", "abstract": "Optimizing neural network architectures (NNA) is a difficult process in part because of the vast number of hyperparameter combinations that exist. The difficulty in designing performant neural networks has brought a recent surge in interest in the automatic design and optimization of neural networks. The focus of the existing body of research has been on optimizing NNA for accuracy [1] [2] with publications starting to address hardware optimizations [3]. Our focus is to close this gap by using evolutionary algorithms to search an entire design space, including NNA and reconfigurable hardware. Large data-centric companies such as Facebook[4] [5] and Google [6] have published data showing that MLP workloads are the majority of their application base. Facebook cites the use of MLP for tasks such as determining which ads to display, which stories matter to see in a news feed, and which results to present from a search. Park et al. stress the importance of these networks and the current limitations on standard hardware and the call for what this research aims to solve, i.e., software and hardware co-design in [7]. Our research aims to take advantage of the reconfigurable architecture of an FPGA device that is capable of molding to a specific workload and neural network structure. Leveraging evolutionary algorithms to search the entire design space of both MLP and target hardware simultaneously, we find unique solutions that achieve both top accuracy and optimal hardware performance.", "venue": "2020 IEEE 33rd International System-on-Chip Conference (SOCC)", "authors": ["Philip  Colangelo", "Oren  Segal", "Alex  Speicher", "Martin  Margala"], "year": 2020, "n_citations": 1}
{"id": 3334448, "s2_id": "a4d98a482e1ca4de6953f952feedd6c9eaa2bd39", "title": "Memcomputing for Accelerated Optimization", "abstract": "In this work, we introduce the concept of an entirely new circuit architecture based on the novel, physics-inspired computing paradigm: Memcomputing. In particular, we focus on digital memcomputing machines (DMMs) that can be designed leveraging properties of non-linear dynamical systems; ultimate descriptors of electronic circuits. The working principle of these systems relies on the ability of currents and voltages of the circuit to self-organize in order to satisfy mathematical relations. In particular for this work, we discuss self-organizing gates, namely Self-Organizing Algebraic Gates (SOAGs), aimed to solve linear inequalities and therefore used to solve optimization problems in Integer Linear Programming (ILP) format. Unlike conventional IOgates, SOAGs are terminal-agnostic, meaning each terminal handles a superposition of input and output signals. When appropriately assembled to represent a given ILP problem, the corresponding self-organizing circuit converges to the equilibria that express the solutions to the problem at hand. Because DMM's components are non-quantum, the ordinary differential equations describing it can be efficiently simulated on our modern computers in software, as well as be built in hardware with off-of-the-shelf technology. As an example, we show the performance of this novel approach implemented as Software as a Service (MemCPU XPC) to address an ILP problem. Compared to today's best solution found using a world renowned commercial solver, MemCPU XPC brings the time to solution down from 23 hours to less than 2 minutes.", "venue": "ArXiv", "authors": ["John  Aiken", "Fabio L. Traversa"], "year": 2020, "n_citations": 1}
{"id": 3335019, "s2_id": "78c1761d6681256eeb052c1871d1b11c11b0762c", "title": "Slim NoC: A Low-Diameter On-Chip Network Topology for High Energy Efficiency and Scalability", "abstract": "Emerging chips with hundreds and thousands of cores require networks with unprecedented energy/area efficiency and scalability. To address this, we propose Slim NoC (SN): a new on-chip network design that delivers significant improvements in efficiency and scalability compared to the state-of-the-art. The key idea is to use two concepts from graph and number theory, degree-diameter graphs combined with non-prime finite fields, to enable the smallest number of ports for a given core count. SN is inspired by state-of-the-art off-chip topologies; it identifies and distills their advantages for NoC settings while solving several key issues that lead to significant overheads on-chip. SN provides NoC-specific layouts, which further enhance area/energy efficiency. We show how to augment SN with state-of-the-art router microarchitecture schemes such as Elastic Links, to make the network even more scalable and efficient. Our extensive experimental evaluations show that SN outperforms both traditional low-radix topologies (e.g., meshes and tori) and modern high-radix networks (e.g., various Flattened Butterflies) in area, latency, throughput, and static/dynamic power consumption for both synthetic and real workloads. SN provides a promising direction in scalable and energy-efficient NoC topologies.", "venue": "ASPLOS", "authors": ["Maciej  Besta", "Syed Minhaj Hassan", "Sudhakar  Yalamanchili", "Rachata  Ausavarungnirun", "Onur  Mutlu", "Torsten  Hoefler"], "year": 2018, "n_citations": 5}
{"id": 3336431, "s2_id": "a90d4496780a531f653b8f0c9482d1fd2d10efd4", "title": "Mechanical Computing Systems Using Only Links and Rotary Joints", "abstract": "A new model for mechanical computing is demonstrated that requires only two basic parts, links, and rotary joints. These basic parts are combined into two main higher level structures, locks, and balances, and suffice to create all necessary combinatorial and sequential logic required for a Turing-complete computational system. While working systems have yet to be implemented using this new approach, the mechanical simplicity of the systems described may lend themselves better to, e.g., microfabrication, than previous mechanical computing designs. Additionally, simulations indicate that if molecular-scale implementations could be realized, they would be far more energy-efficient than conventional electronic computers.", "venue": "Journal of Mechanisms and Robotics", "authors": ["Ralph C. Merkle", "Robert A. Freitas", "Tad  Hogg", "Thomas E. Moore", "Matthew S. Moses", "James  Ryley"], "year": 2018, "n_citations": 16}
{"id": 3337108, "s2_id": "0c7701fc6129ddc86b2dcce65d5188332eb84d1f", "title": "Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization Framework", "abstract": "Deep Neural Networks (DNNs) have achieved extraordinary performance in various application domains. To support diverse DNN models, efficient implementations of DNN inference on edge-computing platforms, e.g., ASICs, FPGAs, and embedded systems, are extensively investigated. Due to the huge model size and computation amount, model compression is a critical step to deploy DNN models on edge devices. This paper focuses on weight quantization, a hardware-friendly model compression approach that is complementary to weight pruning.Unlike existing methods that use the same quantization scheme for all weights, we propose the first solution that applies different quantization schemes for different rows of the weight matrix. It is motivated by (1) the distribution of the weights in the different rows are not the same; and (2) the potential of achieving better utilization of heterogeneous FPGA hardware resources. To achieve that, we first propose a hardware-friendly quantization scheme named sum-of-power-of-2 (SP2) suitable for Gaussian-like weight distribution, in which the multiplication arithmetic can be replaced with logic shifter and adder, thereby enabling highly efficient implementations with the FPGA LUT resources. In contrast, the existing fixed-point quantization is suitable for Uniform-like weight distribution and can be implemented efficiently by DSP. Then to fully explore the resources, we propose an FPGA-centric mixed scheme quantization (MSQ) with an ensemble of the proposed SP2 and the fixed-point schemes. Combining the two schemes can maintain, or even increase accuracy due to better matching with weight distributions.For the FPGA implementations, we develop a parameterized architecture with heterogeneous Generalized Matrix Multiplication (GEMM) cores\u2014one using LUTs for computations with SP2 quantized weights and the other utilizing DSPs for fixed-point quantized weights. Given the partition ratio among the two schemes based on resource characterization, MSQ quantization training algorithm derives an optimally quantized model for the FPGA implementation. We evaluate our FPGA-centric quantization framework across multiple application domains. With optimal SP2/fixed-point ratios on two FPGA devices, i.e., Zynq XC7Z020 and XC7Z045, we achieve performance improvement of 2.1 \u00d7 -4.1 \u00d7 compared to solely exploiting DSPs for all multiplication operations. In addition, the CNN implementations with the proposed MSQ scheme can achieve higher accuracy and comparable hardware utilization efficiency compared to the state-of-the-art designs.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Sung-En  Chang", "Yanyu  Li", "Mengshu  Sun", "Runbin  Shi", "Hayden K.-H. So", "Xuehai  Qian", "Yanzhi  Wang", "Xue  Lin"], "year": 2021, "n_citations": 9}
{"id": 3340926, "s2_id": "5a1b7b02a554571a4c4ef382c63996da1d5bfc62", "title": "Dynamic hardware system for cascade SVM classification of melanoma", "abstract": "Melanoma is the most dangerous form of skin cancer, which is responsible for the majority of skin cancer-related deaths. Early diagnosis of melanoma can significantly reduce mortality rates and treatment costs. Therefore, skin cancer specialists are using image-based diagnostic tools for detecting melanoma earlier. We aim to develop a handheld device featured with low cost and high performance to enhance early detection of melanoma at the primary healthcare. But, developing this device is very challenging due to the complicated computations required by the embedded diagnosis system. Thus, we aim to exploit the recent hardware technology in reconfigurable computing to achieve a high-performance embedded system at low cost. Support vector machine (SVM) is a common classifier that shows high accuracy for classifying melanoma within the diagnosis system and is considered as the most compute-intensive task in the system. In this paper, we propose a dynamic hardware system for implementing a cascade SVM classifier on FPGA for early melanoma detection. A multi-core architecture is proposed to implement a two-stage cascade classifier using two classifiers with accuracies of 98% and 73%. The hardware implementation results were optimized by using the dynamic partial reconfiguration technology, where very low resource utilization of 1% slices and power consumption of 1.5\u00a0W were achieved. Consequently, the implemented dynamic hardware system meets vital embedded system constraints of high performance and low cost, resource utilization, and power consumption, while achieving efficient classification with high accuracy.", "venue": "Neural Computing and Applications", "authors": ["Shereen  Afifi", "Hamid  GholamHosseini", "Roopak  Sinha"], "year": 2018, "n_citations": 10}
{"id": 3346810, "s2_id": "7821f9d2876e7f804d2e877f14c6f4e6b1ddcbd3", "title": "A Survey of FPGA-Based Robotic Computing", "abstract": "Recent researches on robotics have shown significant improvement, spanning from algorithms, mechanics to hardware architectures. Robotics, including manipulators, legged robots, drones, and autonomous vehicles, are now widely applied in diverse scenarios. However, the high computation and data complexity of robotic algorithms pose great challenges to its applications. On the one hand, CPU platform is flexible to handle multiple robotic tasks. GPU platform has higher computational capacities and easy-to-use development frameworks, so they have been widely adopted in several applications. On the other hand, FPGA-based robotic accelerators are becoming increasingly competitive alternatives, especially in latency-critical and power-limited scenarios. With specialized designed hardware logic and algorithm kernels, FPGA-based accelerators can surpass CPU and GPU in performance and energy efficiency. In this paper, we give an overview of previous work on FPGA-based robotic accelerators covering different stages of the robotic system pipeline. An analysis of software and hardware optimization techniques and main technical issues is presented, along with some commercial and space applications, to serve as a guide for future work.", "venue": "IEEE Circuits and Systems Magazine", "authors": ["Zishen  Wan", "Bo  Yu", "Thomas Yuang Li", "Jie  Tang", "Yuhao  Zhu", "Yu  Wang", "Arijit  Raychowdhury", "Shaoshan  Liu"], "year": 2021, "n_citations": 13}
{"id": 3348454, "s2_id": "c4edcd4f88028040db611dc3070cf646800c138f", "title": "BPLight-CNN: A Photonics-based Backpropagation Accelerator for Deep Learning", "abstract": "\n Training deep learning networks involves continuous weight updates across the various layers of the deep network while using a backpropagation (BP) algorithm. This results in expensive computation overheads during training. Consequently, most deep learning accelerators today employ pretrained weights and focus only on improving the design of the inference phase. The recent trend is to build a complete deep learning accelerator by incorporating the training module. Such efforts require an ultra-fast chip architecture for executing the BP algorithm. In this article, we propose a novel photonics-based backpropagation accelerator for high-performance deep learning training. We present the design for a convolutional neural network (CNN),\n BPLight-CNN\n , which incorporates the silicon photonics-based backpropagation accelerator.\n BPLight-CNN\n is a first-of-its-kind photonic and memristor-based CNN architecture for end-to-end training and prediction. We evaluate\n BPLight-CNN\n using a photonic CAD framework (IPKISS) on deep learning benchmark models, including LeNet and VGG-Net. The proposed design achieves (i) at least 34\u00d7 speedup, 34\u00d7 improvement in computational efficiency, and 38.5\u00d7 energy savings during training; and (ii) 29\u00d7 speedup, 31\u00d7 improvement in computational efficiency, and 38.7\u00d7 improvement in energy savings during inference compared with the state-of-the-art designs. All of these comparisons are done at a 16-bit resolution, and BPLight-CNN achieves these improvements at a cost of approximately 6% lower accuracy compared with the state-of-the-art.\n", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["D.  Dang", "S. V. R. Chittamuru", "S.  Pasricha", "R.  Mahapatra", "D.  Sahoo"], "year": 2021, "n_citations": 1}
{"id": 3356225, "s2_id": "ee104c4236784fc6cd00573856535b31d64ef67c", "title": "CIDAN: Computing in DRAM with Artificial Neurons", "abstract": "Numerous applications such as graph processing, cryptography, databases, bioinformatics, etc., involve the repeated evaluation of Boolean functions on large bit vectors. In-memory architectures which perform processing in memory (PIM) are tailored for such applications. This paper describes a different architecture for in-memory computation called CIDAN, that achieves a 3X improvement in performance and a 2X improvement in energy for a representative set of algorithms over the state-of-the-art in-memory architectures. CIDAN uses a new basic processing element called a TLPE, which comprises a threshold logic gate (TLG) (a.k.a artificial neuron or perceptron). The implementation of a TLG within a TLPE is equivalent to a multi-input, edge-triggered flipflop that computes a subset of threshold functions of its inputs. The specific threshold function is selected on each cycle by enabling/disabling a subset of the weights associated with the threshold function, by using logic signals. In addition to the TLG, a TLPE realizes some non-threshold functions by a sequence of TLG evaluations. An equivalent CMOS implementation of a TLPE requires a substantially higher area and power. CIDAN has an array of TLPE(s) that is integrated with a DRAM, to allow fast evaluation of any one of its set of functions on large bit vectors. Results of running several common in-memory applications in graph processing and cryptography are presented.", "venue": "2021 IEEE 39th International Conference on Computer Design (ICCD)", "authors": ["Gian  Singh", "Ankit  Wagle", "Sarma  Vrudhula", "Sunil  Khatri"], "year": 2021, "n_citations": 0}
{"id": 3357785, "s2_id": "077db2b3875a015d6a9135ae552a49b5c12d8f99", "title": "A Self-Reconfigurable Computing Platform Hardware Architecture", "abstract": "Field Programmable Gate Arrays (FPGAs) have recently been increasingly used for highly-parallel processing of compute intensive tasks. This paper introduces an FPGA hardware platform architecture that is PC-based, allows for fast reconfiguration over the PCI bus, and retains a simple physical hardware design. The design considerations are first discussed, then the resulting system architecture designed is illustrated. Finally, experimental results on the FPGA resources utilized for this design are presented.", "venue": "ArXiv", "authors": ["Andreas  Weisensee", "Darran  Nathan"], "year": 2004, "n_citations": 6}
{"id": 3359895, "s2_id": "aac8f174a81dc5898059ab0d24517c9e417a4d4e", "title": "Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of Absolute Difference", "abstract": "Euclid\u2019s algorithm is widely used in calculating of GCD (Greatest Common Divisor) of two positive numbers. There are various fields where this division is used such as channel coding, cryptography, and error correction codes. This makes the GCD a fundamental algorithm in number theory, so a number of methods have been discovered to efficiently compute it. The main contribution of this paper is to investigate a method that computes the GCD of two 32-bit numbers based on Euclidean algorithm which targets six different Xilinx chips. The complexity of this method that we call Optimized_GCDSAD is achieved by utilizing Sum of Absolute Difference (SAD) block which is based on a fast carry-out generation function. The efficiency of the proposed architecture is evaluated based on criteria such as time (latency), area delay product (ADP) and space (slice number) complexity. The VHDL codes of these architectures have been implemented and synthesized through ISE 14.7. A detailed comparative analysis indicates that the proposed Optimized_GCDSAD method based on SAD block outperforms previously known results. Keywords\u2014FPGA; Euclid\u2019s algorithm; GCD; Sum of Absolute Difference; SAD Block", "venue": "ArXiv", "authors": ["Saeideh  Nabipour", "Masoume  Gholizade", "Nima  Nabipour"], "year": 2021, "n_citations": 0}
{"id": 3361742, "s2_id": "bc29d5193bf8b7746be78dcd52b5f88b24fd2c2b", "title": "Low power-area designs of 1bit full adder in cadence virtuoso platform", "abstract": "Power consumption has emerged as a primary design constraint for integrated circuits (ICs). In the Nano meter technology regime, leakage power has become a major component of total power. Full adder is the basic functional unit of an ALU. The power consumption of a processor is lowered by lowering the power consumption of an ALU, and the power consumption of an ALU can be lowered by lowering the power consumption of Full adder. So the full adder designs with low power characteristics are becoming more popular these days. This proposed work illustrates the design of the low-power less transistor full adder designs using cadence tool and virtuoso platform, the entire simulations have been done on 180nm single n-well CMOS bulk technology, in virtuoso platform of cadence tool with the supply voltage 1.8V and frequency of 100MHz. These circuits consume less power with maximum (6T design)of 93.1% power saving compare to conventional 28T design and 80.2% power saving compare to SERF design without much delay degradation. The proposed circuit exploits the advantage of GDI technique and pass transistor logic", "venue": "VLSIC 2013", "authors": ["G  KarthikReddy"], "year": 2013, "n_citations": 16}
{"id": 3363973, "s2_id": "29f8dfd3db2f5c9ad543dbb86257f3dfc2c280b2", "title": "Low-Precision Training in Logarithmic Number System using Multiplicative Weight Update", "abstract": "Training large-scale deep neural networks (DNNs) currently requires a significant amount of energy, leading to serious environmental impacts. One promising approach to reduce the energy costs is representing DNNs with low-precision numbers. While it is common to train DNNs with forward and backward propagation in low-precision, training directly over low-precision weights, without keeping a copy of weights in high-precision, still remains to be an unsolved problem. This is due to complex interactions between learning algorithms and low-precision number systems. To address this, we jointly design a low-precision training framework involving a logarithmic number system (LNS) and a multiplicative weight update training method, termed LNS-Madam. LNS has a high dynamic range even in a low-bitwidth setting, leading to high energy efficiency and making it relevant for on-board training in energy-constrained edge devices. We design LNS to have the flexibility of choosing different bases for weights and gradients, as they usually require different quantization gaps and dynamic ranges during training. By drawing the connection between LNS and multiplicative update, LNS-Madam ensures low quantization error during weight update, leading to a stable convergence even if the bitwidth is limited. Compared to using a fixed-point or floating-point number system and training with popular learning algorithms such as SGD and Adam, our joint design with LNS and LNS-Madam optimizer achieves better accuracy while requiring smaller bitwidth. Notably, with only 5-bit for gradients, the proposed training framework achieves accuracy comparable to full-precision state-of-the-art models such as ResNet-50 and BERT. To verify the efficiency of our framework, we also conduct energy estimations by analyzing the math datapath units during training. We calculate that our design achieves over 60x energy reduction compared to FP32 on BERT models. For full training of ResNet-50 on ImageNet, our design reduces the carbon emissions by 98% around. \u2217Work done during an internship at NVIDIA Research. Preprint. Under review. ar X iv :2 10 6. 13 91 4v 1 [ cs .L G ] 2 6 Ju n 20 21 1.7B 7.5B 39B 145B 530B 1TB Number of Parameters in GPT Models 1 10 10 10 10 E ne rg y C os t p er It er at io n (m J)", "venue": "ArXiv", "authors": ["Jiawei  Zhao", "Steve  Dai", "Rangharajan  Venkatesan", "Ming-Yu  Liu", "Brucek  Khailany", "Bill  Dally", "Anima  Anandkumar"], "year": 2021, "n_citations": 0}
{"id": 3365438, "s2_id": "4bb93a83d84cc6d69586a56e6f100c8c97e9afe6", "title": "Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference", "abstract": "A rising research challenge is running costly machine learning (ML) networks locally on resource-constrained edge devices. ML networks with large convolutional layers can easily exceed available memory, increasing latency due to excessive swapping. Previous memory reduction techniques such as pruning and quantization reduce model accuracy and often require retraining. Alternatively, distributed methods partition the convolutions into equivalent smaller sub-computations, but the implementations introduce communication costs and require a network of devices. However, a distributed partitioning approach can also be used to run in a reduced memory footprint on a single device by subdividing the network into smaller operations. This report extends prior work on distributed partitioning using tiling and fusing of convolutional layers into a memory-aware execution on a single device. Our approach extends prior fusing strategies to allow for two groups of convolutional layers that are fused and tiled independently. This approach reduces overhead via data reuse, and reduces the memory footprint further. i ar X iv :2 10 7. 06 96 0v 1 [ cs .L G ] 1 4 Ju l 2 02 1 We also propose a memory usage predictor coupled with a search algorithm to provide fusing and tiling configurations for an arbitrary set of convolutional layers. When applied to the YOLOv2 object detection network, results show that our approach can run in less than half the memory, and with a speedup of up to 2.78 under severe memory constraints. Additionally, our algorithm will return a configuration with a latency that is within 6% of the best latency measured in a manual search.", "venue": "ArXiv", "authors": ["Jackson  Farley", "Andreas  Gerstlauer"], "year": 2021, "n_citations": 0}
{"id": 3370923, "s2_id": "c346a92aa857e4021018f7db112b50bdbb84404c", "title": "A Novel Low Power Non-Volatile SRAM Cell with Self Write Termination", "abstract": "A non-volatile SRAM cell is proposed for low power applications using Spin Transfer Torque-Magnetic Tunnel Junction (STT-MTJ) devices. This novel cell offers non-volatile storage, thus allowing selected blocks of SRAM to be switched off during standby operation. To further increase the power savings, a write termination circuit is designed which detects completion of MTJ write and closes the bidirectional current path for the MTJ. A reduction of 25.81% in the number of transistors and reduction of 2.95% in the power consumption is achieved in comparison to prior work on write termination circuits.", "venue": "2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)", "authors": ["Kanika  Monga", "Akul  Malhotra", "Nitin  Chaturvedi", "S.  Gurunayaranan"], "year": 2019, "n_citations": 1}
{"id": 3373704, "s2_id": "386aa6d99359610397cb24ef9847e2885ae97835", "title": "High-Performance Simultaneous Multiprocessing for Heterogeneous System-on-Chip", "abstract": "This paper presents a methodology for simultaneous heterogeneous computing, named ENEAC, where a quad core ARM Cortex-A53 CPU works in tandem with a preprogrammed on-board FPGA accelerator. A heterogeneous scheduler distributes the tasks optimally among all the resources and all compute units run asynchronously, which allows for improved performance for irregular workloads. ENEAC achieves up to 17\\% performance improvement \\ignore{and 14\\% energy usage reduction,} when using all platform resources compared to just using the FPGA accelerators and up to 865\\% performance increase \\ignore{and up to 89\\% energy usage decrease} when using just the CPU. The workflow uses existing commercial tools and C/C++ as a single programming language for both accelerator design and CPU programming for improved productivity and ease of verification.", "venue": "ArXiv", "authors": ["Kris  Nikov", "Mohammad  Hosseinabady", "Rafael  Asenjo", "Andr\u00e9s  Rodr\u00edguez", "Angeles G. Navarro", "Jos\u00e9 L. N\u00fa\u00f1ez-Y\u00e1\u00f1ez"], "year": 2020, "n_citations": 0}
{"id": 3374099, "s2_id": "a4dc99a7dd876efeb4d556d62147d781a36bd257", "title": "A Two-Level Approximate Logic Synthesis Combining Cube Insertion and Removal", "abstract": "Approximate computing is an attractive paradigm for reducing the design complexity of error-resilient systems, therefore improving performance and saving power consumption. In this work, we propose a new two-level approximate logic synthesis method based on cube insertion and removal procedures. Experimental results have shown significant literal count and runtime reduction compared to the state-of-the-art approach. The method scalability is illustrated for a high error threshold over large benchmark circuits. The obtained solutions have presented a literal number reduction up to 38%, 56% and 93% with respect to an error rate of 1%, 3% and 5%, respectively.", "venue": "ArXiv", "authors": ["Gabriel  Ammes", "Walter Lau Neto", "Paulo  Butzen", "Pierre-Emmanuel  Gaillardon", "Renato P. Ribas"], "year": 2021, "n_citations": 0}
{"id": 3374306, "s2_id": "6877de40bb5e1937ef53f20700ddade3c67b0a0a", "title": "A Novel Method for Soft Error Mitigation in FPGA using Adaptive Cross Parity Code", "abstract": "Field Programmable Gate Arrays (FPGAs) are more prone to be affected by transient faults in presence of radiation and other environmental hazards compared to Application Specific Integrated Circuits (ASICs). Hence, error mitigation and recovery techniques are absolutely necessary to protect the FPGA hardware from soft errors arising due to such transient faults. In this paper, a new efficient multi-bit error correcting method for FPGAs is proposed using adaptive cross parity check (ACPC) code. ACPC is easy to implement and the needed decoding circuit is also simple. In the proposed scheme total configuration memory is partitioned into two parts. One part will contain ACPC hardware, which is static and assumed to be unaffected by any kind of errors. Other portion will store the binary file for logic, which is to be protected from transient error and is assumed to be dynamically reconfigurable (Partial reconfigurable area). Binary file from the secondary memory passes through ACPC hardware and the bits for forward error correction (FEC) field are calculated before entering into the reconfigurable portion. In the runtime scenario, the data from the dynamically reconfigurable portion of the configuration memory will be read back and passed through the ACPC hardware. The ACPC hardware will correct the errors before the data enters into the dynamic configuration memory. We propose a first of its kind methodology for novel transient fault correction using ACPC code for FPGAs. To validate the design we have tested the proposed methodology with Kintex FPGA. We have also measured different parameters like critical path, power consumption, overhead resource and error correction efficiency to estimate the performance of our proposed method.", "venue": "ArXiv", "authors": ["Swagata  Mandal", "Rourab  Paul", "Suman  Sau", "Amlan  Chakrabarti", "Subhasis  Chattopadhyay"], "year": 2015, "n_citations": 1}
{"id": 3376522, "s2_id": "93dbd399b2c3d67754ee363250f8976850deef69", "title": "GenASM: A High-Performance, Low-Power Approximate String Matching Acceleration Framework for Genome Sequence Analysis", "abstract": "Genome sequence analysis has enabled significant advancements in medical and scientific areas such as personalized medicine, outbreak tracing, and the understanding of evolution. To perform genome sequencing, devices extract small random fragments of an organism\u2019s DNA sequence (known as reads). The first step of genome sequence analysis is a computational process known as read mapping. In read mapping, each fragment is matched to its potential location in the reference genome with the goal of identifying the original location of each read in the genome. Unfortunately, rapid genome sequencing is currently bottlenecked by the computational power and memory bandwidth limitations of existing systems, as many of the steps in genome sequence analysis must process a large amount of data. A major contributor to this bottleneck is approximate string matching (ASM), which is used at multiple points during the mapping process. ASM enables read mapping to account for sequencing errors and genetic variations in the reads.We propose GenASM, the first ASM acceleration framework for genome sequence analysis. GenASM performs bitvectorbased ASM, which can efficiently accelerate multiple steps of genome sequence analysis. We modify the underlying ASM algorithm (Bitap) to significantly increase its parallelism and reduce its memory footprint. Using this modified algorithm, we design the first hardware accelerator for Bitap. Our hardware accelerator consists of specialized systolic-array-based compute units and on-chip SRAMs that are designed to match the rate of computation with memory capacity and bandwidth, resulting in an efficient design whose performance scales linearly as we increase the number of compute units working in parallel.We demonstrate that GenASM provides significant performance and power benefits for three different use cases in genome sequence analysis. First, GenASM accelerates read alignment for both long reads and short reads. For long reads, GenASM outperforms state-of-the-art software and hardware accelerators by 116\u00d7 and 3.9\u00d7, respectively, while reducing power consumption by 37\u00d7 and 2.7\u00d7. For short reads, GenASM outperforms state-of-the-art software and hardware accelerators by 111\u00d7 and 1.9\u00d7. Second, GenASM accelerates pre-alignment filtering for short reads, with 3.7\u00d7 the performance of a state-of-the-art pre-alignment filter, while reducing power consumption by 1.7\u00d7 and significantly improving the filtering accuracy. Third, GenASM accelerates edit distance calculation, with 22\u201312501\u00d7 and 9.3\u2013400\u00d7 speedups over the state-of-the-art software library and FPGA-based accelerator, respectively, while reducing power consumption by 548\u2013582\u00d7 and 67\u00d7. We conclude that GenASM is a flexible, high-performance, and low-power framework, and we briefly discuss four other use cases that can benefit from GenASM.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Damla Senol Cali", "Gurpreet S. Kalsi", "Z\u00fclal  Bing\u00f6l", "Can  Firtina", "Lavanya  Subramanian", "Jeremie S. Kim", "Rachata  Ausavarungnirun", "Mohammed  Alser", "Juan  G\u00f3mez-Luna", "Amirali  Boroumand", "Anant  Nori", "Allison  Scibisz", "Sreenivas  Subramoney", "Can  Alkan", "Saugata  Ghose", "Onur  Mutlu"], "year": 2020, "n_citations": 20}
{"id": 3376687, "s2_id": "224d6454f09d93ab9e7fd60d68fe9ccd39fff28b", "title": "DRAMDig: A Knowledge-assisted Tool to Uncover DRAM Address Mapping", "abstract": "As recently emerged rowhammer exploits require undocumented DRAM address mapping, we propose a generic knowledge-assisted tool, DRAMDig, which takes domain knowledge into consideration to efficiently and deterministically uncover the DRAM address mappings on any Intel-based machines. We test DRAMDig on a number of machines with different combinations of DRAM chips and microarchitectures ranging from Intel Sandy Bridge to Coffee Lake. Comparing to previous works, DRAMDig deterministically reverse-engineered DRAM address mappings on all the test machines with only 7.8 minutes on average. Based on the uncovered mappings, we perform double-sided rowhammer tests and the results show that DRAMDig induced significantly more bit flips than previous works, justifying the correctness of the uncovered DRAM address mappings.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Minghua  Wang", "Zhi  Zhang", "Yueqiang  Cheng", "Surya  Nepal"], "year": 2020, "n_citations": 3}
{"id": 3378295, "s2_id": "255de52e470f43d892ad94b5902f504b045b0e2e", "title": "High-Level Synthesis of Security Properties via Software-Level Abstractions", "abstract": "High-level synthesis (HLS) is a key component for the hardware acceleration of applications, especially thanks to the diffusion of reconfigurable devices in many domains, from data centers to edge devices. HLS reduces development times by allowing designers to raise the abstraction level and use automated methods for hardware generation. Since security concerns are becoming more and more relevant for data-intensive applications, we investigate how to abstract security properties and use HLS for their integration with the accelerator functionality. We use the case of dynamic information flow tracking, showing how classic software-level abstractions can be efficiently used to hide implementation details to the designers.", "venue": "ArXiv", "authors": ["Christian  Pilato", "Francesco  Regazzoni"], "year": 2021, "n_citations": 0}
{"id": 3384883, "s2_id": "2cf24a2cfbb4d3c05ed4ff83bd7f4cad50cc44a5", "title": "The Cost of Software-Based Memory Management Without Virtual Memory", "abstract": "Virtual memory has been a standard hardware feature for more than three decades. At the price of increased hardware complexity, it has simplified software and promised strong isolation among colocated processes. In modern computing systems, however, the costs of virtual memory have increased significantly. With large memory workloads, virtualized environments, data center computing, and chips with multiple DMA devices, virtual memory can degrade performance and increase power usage. We therefore explore the implications of building applications and operating systems without relying on hardware support for address translation. Primarily, we investigate the implications of removing the abstraction of large contiguous memory segments. Our experiments show that the overhead to remove this reliance is surprisingly small for real programs. We expect this small overhead to be worth the benefit of reducing the complexity and energy usage of address translation. In fact, in some cases, performance can even improve when address translation is avoided.", "venue": "ArXiv", "authors": ["Drew  Zagieboylo", "G. Edward Suh", "Andrew C. Myers"], "year": 2020, "n_citations": 0}
{"id": 3385264, "s2_id": "051ec05ed97669feb163d666e44d8379744f98fc", "title": "Effective Algorithm-Accelerator Co-design for AI Solutions on Edge Devices", "abstract": "High quality AI solutions require joint optimization of AI algorithms, such as deep neural networks (DNNs), and their hardware accelerators. To improve the overall solution quality as well as to boost the design productivity, efficient algorithm and accelerator co-design methodologies are indispensable. In this paper, we first discuss the motivations and challenges for the Algorithm/Accelerator co-design problem, and then provide several effective solutions. Especially, we highlight three leading works of effective co-design methodologies: 1) the first simultaneous DNN/FPGA co-design method; 2) a bi-directional light weight DNN and accelerator co-design method; 3) a differentiable and efficient DNN and accelerator co-search method. We demonstrate the effectiveness of the proposed co-design approaches using extensive experiments on both FPGAs and GPUs, with comparisons to existing works. This paper emphasizes the importance and efficacy of algorithm-accelerator co-design, and calls for more research breakthroughs in this interesting and demanding area.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Cong  Hao", "Yao  Chen", "Xiaofan  Zhang", "Yuhong  Li", "Jinjun  Xiong", "Wen-mei  Hwu", "Deming  Chen"], "year": 2020, "n_citations": 1}
{"id": 3388071, "s2_id": "17cd67ee33973ee5c8d674ad50f48577ecd8830d", "title": "Probabilistic Value-Deviation-Bounded Source-Dependent Bit-Level Channel Adaptation for Approximate Communication", "abstract": "Computing systems that can tolerate effects of errors in their communicated data values can trade this tolerance for improved resource efficiency. Many important applications of computing, such as embedded sensor systems, can tolerate errors that are bounded in their distribution of deviation from correctness (distortion). We present a channel adaptation technique which modulates properties of I/O channels typical in embedded sensor systems, to provide a tradeoff between I/O power dissipation and distortion of communicated data. We provide an efficient-to-compute formulation for the distribution of integer distortion accounting for the distribution of transmitted values. Using this formulation we implement our value-deviation-bounded (VDB) channel adaptation. We experimentally quantify the achieved reduction in power dissipation on a hardware prototype integrated with the required programmable channel modulation circuitry. We augment these experimental measurements with an analysis of the distributions of distortions. We show that our probabilistic VDB channel adaptation can provide up to a 2\u00d7 reduction in I/O power dissipation. When synthesized for a miniature low-power FPGA intended for use in sensor interfaces, a register transfer level implementation of the channel adaptation control logic requires only 106 flip-flops and 224 4-input LUTs for implementing per-bit channel adaptation on serialized streams of 8-bit sensor data.", "venue": "IEEE Transactions on Computers", "authors": ["Bilgesu Arif Bilgin", "Phillip  Stanley-Marbell"], "year": 2021, "n_citations": 0}
{"id": 3391809, "s2_id": "730ced22261cbeffdded1029216815b54eda85d2", "title": "CAMA: Energy and Memory Efficient Automata Processing in Content-Addressable Memories", "abstract": "Accelerating finite automata processing is critical for advancing real-time analytic in pattern matching, data mining, bioinformatics, intrusion detection, and machine learning. Recent in-memory automata accelerators leveraging SRAMs and DRAMs have shown exciting improvements over conventional digital designs. However, the bit-vector representation of state transitions used by all state-of-the-art (SOTA) designs is only optimal in processing worst-case completely random patterns, while a significant amount of memory and energy is wasted in running most real-world benchmarks. We present CAMA, a Content-Addressable Memory (CAM) enabled Automata accelerator for processing homogeneous nondeterministic finite automata (NFA). A radically different state representation scheme, along with co-designed novel circuits and data encoding schemes, greatly reduces energy, memory, and chip area for most realistic NFAs. CAMA is holistically optimized with the following major contributions: (1) a 16\u00d7256 8-transistor (8T) CAM array for state matching, replacing the 256\u00d7256 6T SRAM array or two 16\u00d7256 6T SRAM banks in state-of-theart (SOTA) designs; (2) a novel encoding scheme that enables content searching within 8T SRAMs and adapts to different applications; (3) a reconfigurable and scalable architecture that improves efficiency on all tested benchmarks, without losing support for any NFA that\u2019s compatible with SOTA designs; (4) an optimization framework that automates the choice of encoding schemes and maps a given NFA to the proposed hardware. Two versions of CAMA, one optimized for energy (CAMA-E) and the other for throughput (CAMA-T), are comprehensively evaluated in a 28nm CMOS process, and across 21 real-world and synthetic benchmarks. CAMA-E achieves 2.1\u00d7, 2.8\u00d7, and 2.04\u00d7 lower energy than CA, 2-stride Impala, and eAP. CAMA-T shows 2.68\u00d7, 3.87\u00d7 and 2.62\u00d7 higher average compute density than 2-stride Impala, CA, and eAP. Both versions reduce the chip area required for the largest tested benchmark by 2.48\u00d7 over CA, 1.91\u00d7 over 2-stride Impala, and 1.78\u00d7 over eAP.", "venue": "ArXiv", "authors": ["Yi  Huang", "Zhiyu  Chen", "Dai  Li", "Kaiyuan  Yang"], "year": 2021, "n_citations": 0}
{"id": 3398008, "s2_id": "3c5599f86f8bb01083fd9afa4ed48bef73c28815", "title": "A Novel Reconfigurable Computing Architecture for Image Signal Processing Using Circuit-Switched NoC and Synchronous Dataflow Model", "abstract": "In this paper, a novel reconfigurable architecture is proposed for multifunctional image signal processing systems. A circuit-switched NoC is used to provide interconnection because the non-TMD links ensure fixed throughput, which is a desirable behavior for computational intensive image processing algorithms compared with packet-switched NoC. Image processing algorithms are modeled as synchronous dataflow graphs which provide a unified model for general computing procedure. An image processing system is considered as several temporally mutually exclusive algorithms. Thus, their dataflow graph representations could be considered as a group and a merging algorithm could be applied to generate a union graph while eliminating spatial redundancy for area consumption optimization. After the union graph have been mapped and routed on the NoC, the reconfigurable system could be configured to any of its target image processing algorithms by properly setting the NoC topology. Experiments show the demo reconfigurable system with two image processing applications cost 26.4% less hardware resource, compared with the non-reconfigurable implementations.", "venue": "ArXiv", "authors": ["Feitian  Li", "Fei  Qiao", "Qi  Wei", "Huazhong  Yang"], "year": 2013, "n_citations": 0}
{"id": 3405522, "s2_id": "47f1ca0bc61d8c891180b3f49d8b85784786412d", "title": "A Survey of System Architectures and Techniques for FPGA Virtualization", "abstract": "FPGA accelerators are gaining increasing attention in both cloud and edge computing because of their hardware flexibility, high computational throughput, and low power consumption. However, the design flow of FPGAs often requires specific knowledge of the underlying hardware, which hinders the wide adoption of FPGAs by application developers. Therefore, the virtualization of FPGAs becomes extremely important to create a useful abstraction of the hardware suitable for application developers. Such abstraction also enables the sharing of FPGA resources among multiple users and accelerator applications, which is important because, traditionally, FPGAs have been mostly used in single-user, single-embedded-application scenarios. There are many works in the field of FPGA virtualization covering different aspects and targeting different application areas. In this article, we review the system architectures used in the literature for FPGA virtualization. In addition, we identify the primary objectives of FPGA virtualization, based on which we summarize the techniques for realizing FPGA virtualization. This article helps researchers to efficiently learn about FPGA virtualization research by providing a comprehensive review of the existing literature.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Masudul Hassan Quraishi", "Erfan Bank Tavakoli", "Fengbo  Ren"], "year": 2021, "n_citations": 1}
{"id": 3406859, "s2_id": "cb449c58779129789751f8f7af19eefacd148f3d", "title": "A matrix math facility for Power ISA(TM) processors", "abstract": "Power ISA(TM) Version 3.1 has introduced a new family of matrix math instructions, collectively known as the Matrix-Multiply Assist (MMA) facility. The instructions in this facility implement numerical linear algebra operations on small matrices and are meant to accelerate computation-intensive kernels, such as matrix multiplication, convolution and discrete Fourier transform. These instructions have led to a power- and area-efficient implementation of a high throughput math engine in the future POWER10 processor. Performance per core is 4 times better, at constant frequency, than the previous generation POWER9 processor. We also advocate the use of compiler built-ins as the preferred way of leveraging these instructions, which we illustrate through case studies covering matrix multiplication and convolution.", "venue": "ArXiv", "authors": ["Jos\u00e9 E. Moreira", "Kit  Barton", "Steven  Battle", "Peter  Bergner", "Ramon Bertran Monfort", "Puneeth  Bhat", "Pedro  Caldeira", "David  Edelsohn", "Gordon  Fossum", "Brad  Frey", "Nemanja  Ivanovic", "Chip  Kerchner", "Vincent  Lim", "Shakti  Kapoor", "Tulio Machado Filho", "Silvia Melitta Mueller", "Brett  Olsson", "Satish  Sadasivam", "Baptiste  Saleil", "Bill  Schmidt", "Rajalakshmi  Srinivasaraghavan", "Shricharan  Srivatsan", "Brian W. Thompto", "Andreas  Wagner", "Nelson  Wu"], "year": 2021, "n_citations": 1}
{"id": 3407714, "s2_id": "d6cb1ba5313eb0a6cd349ca9da61333dbff740fb", "title": "CodeAPeel: An Integrated and Layered Learning Technology For Computer Architecture Courses", "abstract": "In this paper, we introduce a versatile, multilayered technology to help support teaching and learning computer architecture concepts. This technology, called CodeAPeel is already implemented in one particular form to describe instruction processing in assembly and machine layers by a comprehensive simulation of fetch-decode-execute process as well as animation of the behavior of CPU registers, RAM, VRAM, STACK memories, and various control registers of a generic instruction set architecture. Unlike most educational CPU simulators, CodeAPeel does not simulate a real processor such as MIPS or RISC-V, but it is designed and implemented as a generic RISC instruction set architecture with both scalar and vector operands, making it a dual architecture, supporting Flynn\u2019s SISD and SIMD instruction set architectures.", "venue": "ArXiv", "authors": ["A. Yavuz Oruc", "A.  Atmaca", "Y. Nevzat Sengun", "A. Semi Yeniyol"], "year": 2021, "n_citations": 0}
{"id": 3407889, "s2_id": "e48410829acffe034615aebb9ed3b8762ebc079a", "title": "Proposal for a High Precision Tensor Processing Unit", "abstract": "This whitepaper proposes the design and adoption of a new generation of Tensor Processing Unit which has the performance of Google's TPU, yet performs operations on wide precision data. The new generation TPU is made possible by implementing arithmetic circuits which compute using a new general purpose, fractional arithmetic based on the residue number system.", "venue": "ArXiv", "authors": ["Eric B. Olsen"], "year": 2017, "n_citations": 1}
{"id": 3413193, "s2_id": "cf20ea946649d0fb5973392c415d604dfd759dc2", "title": "ANDROMEDA: An FPGA Based RISC-V MPSoC Exploration Framework", "abstract": "With the growing demands of consumer electronic products, the computational requirements are increasing exponentially. Due to the applications' computational needs, the computer architects are trying to pack as many cores as possible on a single die for accelerated execution of the application program codes. In a multiprocessor system-on-chip (MPSoC), striking a balance among the number of cores, memory subsystems, and network-on-chip parameters is essential to attain the desired performance. In this paper, we present ANDROMEDA, a RISC-V based framework that allows us to explore the different configurations of an MPSoC and observe the performance penalties and gains. We emulate the various configurations of MPSoC on the Synopsys HAPS-80D Dual FPGA platform. Using STREAM, matrix multiply, and N-body simulations as benchmarks, we demonstrate our framework's efficacy in quickly identifying the right parameters for efficient execution of these benchmarks.", "venue": "2021 34th International Conference on VLSI Design and 2021 20th International Conference on Embedded Systems (VLSID)", "authors": ["Farhad  Merchant", "Dominik  Sisejkovic", "Lennart M. Reimann", "Kirthihan  Yasotharan", "Thomas  Grass", "Rainer  Leupers"], "year": 2021, "n_citations": 0}
{"id": 3416071, "s2_id": "704c338ef76f55cde98454c22fe1fa7e55e09c56", "title": "Phantom: A High-Performance Computational Core for Sparse Convolutional Neural Networks", "abstract": "Sparse convolutional neural networks (CNNs) have gained significant traction over the past few years as sparse CNNs can drastically decrease the model size and computations, if exploited befittingly, as compared to their dense counterparts. Sparse CNNs often introduce variations in the layer shapes and sizes, which can prevent dense accelerators from performing well on sparse CNN models. Recently proposed sparse accelerators like SCNN, Eyeriss v2, and SparTen, actively exploit the two-sided or full sparsity, that is, sparsity in both weights and activations, for performance gains. These accelerators, however, either have inefficient micro-architecture (Eyeriss v2, SCNN), which limits their performance, have no support for non-unit stride convolutions (SCNN) and fully-connected (FC) layers (SCNN, SparTen), or suffer massively from systematic load imbalance (SCNN, Eyeriss v2). To circumvent these issues and support both sparse and dense models, we propose Phantom, a multi-threaded, dynamic, and flexible neural computational core. Phantom uses sparse binary mask representation to actively lookahead into sparse computations, and dynamically schedule its computational threads to maximize the thread utilization and throughput. We also generate a two-dimensional (2D) mesh architecture of Phantom neural computational cores, which we refer to as Phantom-2D accelerator, and propose a novel dataflow that supports all layers of a CNN, including unit and non-unit stride convolutions, and FC layers. In addition, Phantom-2D uses a two-level load balancing strategy to minimize the computational idling, thereby, further improving the hardware utilization. To show support for different types of layers, we evaluate the performance of the Phantom architecture on VGG16 and MobileNet. Our simulations show that the Phantom-2D accelerator attains a performance gain of 12\u00d7, 4.1\u00d7, 1.98\u00d7, and 2.36\u00d7, over dense architectures, SCNN, SparTen, and Eyeriss v2, respectively.", "venue": "ArXiv", "authors": ["Mahmood Azhar Qureshi", "Arslan  Munir"], "year": 2021, "n_citations": 0}
{"id": 3421746, "s2_id": "19753845e35469275bcf5d6b285618437cba8fd0", "title": "PointAcc: Efficient Point Cloud Accelerator", "abstract": "Deep learning on point clouds plays a vital role in a wide range of applications such as autonomous driving and AR/VR. These applications interact with people in real time on edge devices and thus require low latency and low energy. Compared to projecting the point cloud to 2D space, directly processing 3D point cloud yields higher accuracy and lower #MACs. However, the extremely sparse nature of point cloud poses challenges to hardware acceleration. For example, we need to explicitly determine the nonzero outputs and search for the nonzero neighbors (mapping operation), which is unsupported in existing accelerators. Furthermore, explicit gather and scatter of sparse features are required, resulting in large data movement overhead. In this paper, we comprehensively analyze the performance bottleneck of modern point cloud networks on CPU/GPU/TPU. To address the challenges, we then present PointAcc, a novel point cloud deep learning accelerator. PointAcc maps diverse mapping operations onto one versatile ranking-based kernel, streams the sparse computation with configurable caching, and temporally fuses consecutive dense layers to reduce the memory footprint. Evaluated on 8 point cloud models across 4 applications, PointAcc achieves 3.7 \u00d7 speedup and 22 \u00d7 energy savings over RTX 2080Ti GPU. Co-designed with light-weight neural networks, PointAcc rivals the prior accelerator Mesorasi by 100 \u00d7 speedup with 9.1% higher accuracy running segmentation on the S3DIS dataset. PointAcc paves the way for efficient point cloud recognition.", "venue": "MICRO", "authors": ["Yujun  Lin", "Zhekai  Zhang", "Haotian  Tang", "Hanrui  Wang", "Song  Han"], "year": 2021, "n_citations": 0}
{"id": 3426875, "s2_id": "4a265f91a9130708ad86c5a5c846643893b56913", "title": "vlang: Mapping Verilog Netlists to Modern Technologies", "abstract": "Portability of hardware designs between Programmable Logic Devices (PLD) can be accomplished through the use of device-agnostic hardware description languages (HDL) such as Verilog or VHDL. Hardware designers can use HDLs to migrate hardware designs between devices and explore performance, area and power tradeoffs, as well as, port designs to an alternative device. However, if design files are corrupt or missing, the portability of the design is lost. While reverse engineering efforts may be able to recover an HDL-netlist of the original design, HDL-netlists use device-specific primitives, restricting portability. Additionally, the recovered design may benefit from other computational technologies (e.g., \u03bcP, GPGPUs), but is restricted to the domain of PLDs. In this work, we provide a new framework, vlang, which automatically maps Verilog-netlists into LLVM\u2019s intermediate representation (IR). The remapped design can use the LLVM-framework to target many device technologies such as: x86-64 assembly, RISC-V, ARM or to other PLDs with a modern high-level synthesis tool. Our framework is able to preserve the exact functionality of the original design within the software executable. The vlangproduced software executable can be used with other software programs, or to verify the functionality and correctness of the remapped design. We evaluate our work with a suite of hardware designs from OpenCores. We compare our framework against state-of-the-art simulators, thereby outlining our framework\u2019s ability to produce a fully-functional, cycle accurate softwareexecutable. We also explore the usage of vlang as a front-end for high-level synthesis tools.", "venue": "ArXiv", "authors": ["Nicholas V. Giamblanco", "Andrew  Schmidt"], "year": 2021, "n_citations": 0}
{"id": 3429512, "s2_id": "1b6375804931f8ccdac0a6de3f948c88ea00d141", "title": "A Class of Optimal Structures for Node Computations in Message Passing Algorithms", "abstract": "Consider the computations at a node in a message passing algorithm. Assume that the node has incoming and outgoing messages <inline-formula> <tex-math notation=\"LaTeX\">$\\mathbf {x} = (x_{1}, x_{2}, \\ldots, x_{n})$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\mathbf {y} = (y_{1}, y_{2}, \\ldots, y_{n})$ </tex-math></inline-formula>, respectively. In this paper, we investigate a class of structures that can be adopted by the node for computing <inline-formula> <tex-math notation=\"LaTeX\">$\\mathbf {y}$ </tex-math></inline-formula> from <inline-formula> <tex-math notation=\"LaTeX\">$\\mathbf {x}$ </tex-math></inline-formula>, where each <inline-formula> <tex-math notation=\"LaTeX\">$y_{j}, j = 1, 2, \\ldots, n$ </tex-math></inline-formula> is computed via a binary tree with leaves <inline-formula> <tex-math notation=\"LaTeX\">$\\mathbf {x}$ </tex-math></inline-formula> excluding <inline-formula> <tex-math notation=\"LaTeX\">$x_{j}$ </tex-math></inline-formula>. We make three main contributions regarding this class of structures. First, we prove that the minimum complexity of such a structure is <inline-formula> <tex-math notation=\"LaTeX\">$3n - 6$ </tex-math></inline-formula>, and if a structure has such complexity, its minimum latency is <inline-formula> <tex-math notation=\"LaTeX\">$\\delta + \\lceil \\log (n-2^{\\delta }) \\rceil $ </tex-math></inline-formula> with <inline-formula> <tex-math notation=\"LaTeX\">$\\delta = \\lfloor \\log (n/2) \\rfloor $ </tex-math></inline-formula>, where the logarithm always takes base two. Second, we prove that the minimum latency of such a structure is <inline-formula> <tex-math notation=\"LaTeX\">$\\lceil \\log (n-1) \\rceil $ </tex-math></inline-formula>, and if a structure has such latency, its minimum complexity is <inline-formula> <tex-math notation=\"LaTeX\">$n \\log (n-1)$ </tex-math></inline-formula> when <inline-formula> <tex-math notation=\"LaTeX\">$n-1$ </tex-math></inline-formula> is a power of two. Third, given <inline-formula> <tex-math notation=\"LaTeX\">$(n, \\tau)$ </tex-math></inline-formula> with <inline-formula> <tex-math notation=\"LaTeX\">$\\tau \\geq \\lceil \\log (n-1) \\rceil $ </tex-math></inline-formula>, we propose a construction for a structure which we conjecture to have the minimum complexity among structures with latencies at most <inline-formula> <tex-math notation=\"LaTeX\">$\\tau $ </tex-math></inline-formula>. Our construction method runs in <inline-formula> <tex-math notation=\"LaTeX\">$O(n^{3} \\log ^{2}(n))$ </tex-math></inline-formula> time, and the obtained structure has complexity at most (generally much smaller than) <inline-formula> <tex-math notation=\"LaTeX\">$n \\lceil \\log (n) \\rceil - 2$ </tex-math></inline-formula>.", "venue": "IEEE Transactions on Information Theory", "authors": ["Xuan  He", "Kui  Cai", "Liang  Zhou"], "year": 2022, "n_citations": 0}
{"id": 3430994, "s2_id": "9482613a98fe29db6b7a4d2d5e2f3f2f22eb7499", "title": "VersaGNN: a Versatile accelerator for Graph neural networks", "abstract": "Graph Neural Network (GNN) is a promising approach for analyzing graph-structured data that tactfully captures their dependency information via node-level message passing. It has achieved state-of-the-art performances in many tasks, such as node classification, graph matching, clustering, and graph generation. As GNNs operate on non-Euclidean data, their irregular data access patterns cause considerable computational costs and overhead on conventional architectures, such as GPU and CPU. Our analysis show that GNN adopts a hybrid computing mode. The Aggregation (or Message Passing) phase performs vector additions where vectors are fetched with irregular strides. The Transformation (or Node Embedding) phase can be either dense or sparse-dense matrix multiplication. In this work, We propose VersaGNN, an ultra-efficient, systolicarray based versatile hardware accelerator that unifies dense and sparse matrix multiplication. By applying this single optimized systolic-arrays to both aggregation and transformation phases, we have significantly reduced chip sizes and energy consumption. We then divide the computing engine into blocked systolic arrays to support the Strassen\u2019s algorithm for dense matrix multiplication, dramatically scaling down the number of multiplications and enabling high-throughput computation of GNNs. To balance the workload of sparse-dense matrix multiplication, we also introduced a greedy algorithm to combine sparse sub-matrices of compressed format into condensed ones to reduce computational cycles. Compared with current state-of-the-art GNN software frameworks, VersaGNN achieves on average 3712\u00d7 speedup with 1301.25\u00d7 energy reduction on CPU, and 35.4\u00d7 speedup with 17.66\u00d7 energy reduction on GPU.", "venue": "ArXiv", "authors": ["Feng  Shi", "Ahren Yiqiao Jin", "Song-Chun  Zhu"], "year": 2021, "n_citations": 1}
{"id": 3431282, "s2_id": "e3b70106f9bf2f5889e820ae299f8ded2e486e46", "title": "High speed SRT divider for intelligent embedded system", "abstract": "Increasing development in embedded system, VLSI and processor design have given rise to increased demands from the system in terms of power, speed, area, throughput etcetera. Most of the sophisticated embedded system applications consist of processors; which now need an arithmetic unit with the ability to execute complex division operations with maximum efficiency. Hence the speed of the arithmetic unit is critically dependent on division operation. Most of the dividers use the SRT division algorithm for division. In IoT and other embedded applications typically radix 2 and radix 4 division algorithms are used. The proposed algorithm lies on parallel execution of various steps so as to reduce time critical path, use fuzzy logic to solve the overlap problem in quotient selection; hence reducing maximum delay and increasing the accuracy. Every logical circuit has a maximum delay on which the timing of the circuit is dependent and the path, causing the maximum delay is known as the critical path. Our approach uses the previous SRT algorithm methods to make a highly parallel pipelined design and use Mamdani model to determine a solution to the overlapping problem to reduce the overall execution time of radix 4 SRT division on 64 bits double precision floating point numbers to 281ns. The design is made using Bluespec System Verilog, synthesized and simulated using Vivado v.2016.1 and implemented on Xilinx VirtexUltraScale FPGA board.", "venue": "2017 International Conference on Soft Computing and its Engineering Applications (icSoftComp)", "authors": ["Bhavana  Mehta", "Jonti  Talukdar", "Sachin  Gajjar"], "year": 2017, "n_citations": 2}
{"id": 3432102, "s2_id": "9bd809d630b00c541c9c6d771bab02142ed6852a", "title": "Heterogeneous processor pipeline for a product cipher application", "abstract": "Processing data received as a stream is a task commonly performed by modern embedded devices, in a wide range of applications such as multimedia (encoding/decoding/ playing media), networking (switching and routing), digital security, scientific data processing, etc. Such processing normally tends to be calculation intensive and therefore requiring significant processing power. Therefore, hardware acceleration methods to increase the performance of such applications constitute an important area of study. In this paper, we present an evaluation of one such method to process streaming data, namely multi-processor pipeline architecture. The hardware is based on a Multiple-Processor System on Chip (MPSoC), using a data encryption algorithm as a case study. The algorithm is partitioned on a coarse grained level and mapped on to an MPSoC with five processor cores in a pipeline, using specifically configured Xtensa LX3 cores. The system is then selectively optimized by strengthening and pruning the resources of each processor core. The optimized system is evaluated and compared against an optimal single-processor System on Chip (SoC) for the same application. The multiple-processor pipeline system for data encryption algorithms used was observed to provide significant speed ups, up to 4.45 times that of the single-processor system, which is close to the ideal speed up from a five-stage pipeline.", "venue": "2011 6th International Conference on Industrial and Information Systems", "authors": ["I. B. Nawinne", "M. S. Wickramasinghe", "Roshan G. Ragel", "S.  Radhakrishnan"], "year": 2011, "n_citations": 1}
{"id": 3434759, "s2_id": "2e7f540dfc517d61740a62c4afff0f483d76cb7b", "title": "A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation", "abstract": "This paper presents a methodology for using LLVM-based tools to tune the DCA++ (dynamical cluster approximation) application that targets the new ARM A64FX processor. The goal is to describe the changes required for the new architecture and generate efficient single instruction/multiple data (SIMD) instructions that target the new Scalable Vector Extension instruction set. During manual tuning, the authors used the LLVM tools to improve code parallelization by using OpenMP SIMD, refactored the code and applied transformation that enabled SIMD optimizations, and ensured that the correct libraries were used to achieve optimal performance. By applying these code changes, code speed was increased by 1.98\u00d7 and 78 GFlops were achieved on the A64FX processor. The authors aim to automatize parts of the efforts in the OpenMP Advisor tool, which is built on top of existing and newly introduced LLVM tooling.", "venue": "IWOMP", "authors": ["Joseph  Huber", "Weile  Wei", "Giorgis  Georgakoudis", "Johannes  Doerfert", "Oscar  Hernandez"], "year": 2021, "n_citations": 1}
{"id": 3440661, "s2_id": "72f3fe27b93f6d72bb0f7e7ef7e3909ac96add14", "title": "Investigating Warp Size Impact in GPUs", "abstract": "There are a number of design decisions that impact a GPU's performance. Among such decisions deciding the right warp size can deeply influence the rest of the design. Small warps reduce the performance penalty associated with branch divergence at the expense of a reduction in memory coalescing. Large warps enhance memory coalescing significantly but also increase branch divergence. This leaves designers with two choices: use a small warps and invest in finding new solutions to enhance coalescing or use large warps and address branch divergence employing effective control-flow solutions. In this work our goal is to investigate the answer to this question. We analyze warp size impact on memory coalescing and branch divergence. We use our findings to study two machines: a GPU using small warps but equipped with excellent memory coalescing (SW+) and a GPU using large warps but employing an MIMD engine immune from control-flow costs (LW+). Our evaluations show that building coalescing-enhanced small warp GPUs is a better approach compared to pursuing a control-flow enhanced large warp GPU.", "venue": "ArXiv", "authors": ["Ahmad  Lashgar", "Amirali  Baniasadi", "Ahmad  Khonsari"], "year": 2012, "n_citations": 3}
{"id": 3441481, "s2_id": "d34b8e0946789b99bc23b49b8a0bd5f0fc12f7ef", "title": "Adaptive Precision CNN Accelerator Using Radix-X Parallel Connected Memristor Crossbars", "abstract": "Neural processor development is reducing our reliance on remote server access to process deep learning operations in an increasingly edge-driven world. By employing in-memory processing, parallelization techniques, and algorithm-hardware co-design, memristor crossbar arrays are known to efficiently compute large scale matrix-vector multiplications. However, state-of-the-art implementations of negative weights require duplicative column wires, and high precision weights using single-bit memristors further distributes computations. These constraints dramatically increase chip area and resistive losses, which lead to increased power consumption and reduced accuracy. In this paper, we develop an adaptive precision method by varying the number of memristors at each crosspoint. We also present a weight mapping algorithm designed for implementation on our crossbar array. This novel algorithm-hardware solution is described as the radix-X Convolutional Neural Network Crossbar Array, and demonstrate how to efficiently represent negative weights using a single column line, rather than double the number of additional columns. Using both simulation and experimental results, we verify that our radix-5 CNN array achieves a validation accuracy of 90.5% on the CIFAR-10 dataset, a 4.5% improvement over binarized neural networks whilst simultaneously reducing crossbar area by 46% over conventional arrays by removing the need for duplicate columns to represent signed weights.", "venue": "ArXiv", "authors": ["Jaeheum  Lee", "Jason K. Eshraghian", "Kyoungrok  Cho", "Kamran  Eshraghian"], "year": 2019, "n_citations": 14}
{"id": 3443929, "s2_id": "e933af6246d12a2e29294a17188e5acf94bb3dbc", "title": "A System-Level Voltage/Frequency Scaling Characterization Framework for Multicore CPUs", "abstract": "Supply voltage scaling is one of the most effective techniques to reduce the power consumption of microprocessors. However, technology limitations such as aging and process variability enforce microprocessor designers to apply pessimistic voltage guardbands to guarantee correct operation in the field for any foreseeable workload. This worst-case design practice makes energy efficiency hard to scale with technology evolution. Improving energy-efficiency requires the identification of the chip design margins through time-consuming and comprehensive characterization of its operational limits. Such a characterization of state-of-the-art multi-core CPUs fabricated in aggressive technologies is a multi-parameter process, which requires statistically significant information. In this paper, we present an automated framework to support system-level voltage and frequency scaling characterization of Applied Micro\u2019s state-of-the-art ARMv8-based multicore CPUs used in the X-Gene 2 micro-server family. The fully automated framework can provide fine-grained information of the system\u2019s state by monitoring any abnormal behavior that may occur during reduced supply voltage conditions. We also propose a new metric to quantify the behavior of a microprocessor when it operates beyond nominal conditions. Our experimental results demonstrate potential uses of the characterization framework to identify the limits of operation for improved energy efficiency.", "venue": "ArXiv", "authors": ["George  Papadimitriou", "Manolis  Kaliorakis", "Athanasios  Chatzidimitriou", "Dimitris  Gizopoulos", "Greg  Favor", "Kumar  Sankaran", "Shidhartha  Das"], "year": 2021, "n_citations": 5}
{"id": 3444295, "s2_id": "46f5c777b15bf3ca17c9d80a9fd4f614b375623a", "title": "A Hardware-Aware Heuristic for the Qubit Mapping Problem in the NISQ Era", "abstract": "Due to several physical limitations in the realization of quantum hardware, today's quantum computers are qualified as noisy intermediate-scale quantum (NISQ) hardware. NISQ hardware is characterized by a small number of qubits (50 to a few hundred) and noisy operations. Moreover, current realizations of superconducting quantum chips do not have the ideal all-to-all connectivity between qubits but rather at most a nearest-neighbor connectivity. All these hardware restrictions add supplementary low-level requirements. They need to be addressed before submitting the quantum circuit to an actual chip. Satisfying these requirements is a tedious task for the programmer. Instead, the task of adapting the quantum circuit to a given hardware is left to the compiler. In this article, we propose a hardware-aware (HA) mapping transition algorithm that takes the calibration data into account with the aim to improve the overall fidelity of the circuit. Evaluation results on IBM quantum hardware show that our HA approach can outperform the state of the art, both in terms of the number of additional gates and circuit fidelity.", "venue": "IEEE Transactions on Quantum Engineering", "authors": ["Siyuan  Niu", "Adrien  Suau", "Gabriel  Staffelbach", "Aida  Todri-Sanial"], "year": 2020, "n_citations": 11}
{"id": 3450628, "s2_id": "6654a29fb8c26d2572ed9700cc0971f0d785c083", "title": "Compiler Infrastructure for Specializing Domain-Specific Memory Templates", "abstract": "Specialized hardware accelerators are becoming important for more and more applications. Thanks to specialization, they can achieve high performance and energy efficiency but their design is complex and time consuming. This problem is exacerbated when large amounts of data must be processed, like in modern big data and machine learning applications. The designer has not only to optimize the accelerator logic but also produce efficient memory architectures. To simplify this process, we propose a multi-level compilation flow that specializes a domain-specific memory template to match data, application, and technology requirements.", "venue": "ArXiv", "authors": ["Stephanie  Soldavini", "Christian  Pilato"], "year": 2021, "n_citations": 2}
{"id": 3455135, "s2_id": "c8f242a315557bb14764070dec93cdb243fd33b6", "title": "The SpiNNaker 2 Processing Element Architecture for Hybrid Digital Neuromorphic Computing", "abstract": "This paper introduces the processing element architecture of the second generation SpiNNaker chip, implemented in 22nm FDSOI. On circuit level, the chip features adaptive body biasing for near-threshold operation, and dynamic voltage-andfrequency scaling driven by spiking activity. On system level, processing is centered around an ARM M4 core, similar to the processor-centric architecture of the first generation SpiNNaker. To speed operation of subtasks, we have added accelerators for numerical operations of both spiking (SNN) and rate based (deep) neural networks (DNN). PEs communicate via a dedicated, custom-designed network-on-chip. We present three benchmarks showing operation of the whole processor element on SNN, DNN and hybrid SNN/DNN networks.", "venue": "ArXiv", "authors": ["Sebastian  H\u00f6ppner", "Yexin  Yan", "Andreas  Dixius", "Stefan  Scholze", "Johannes  Partzsch", "Marco  Stolba", "Florian  Kelber", "Bernhard  Vogginger", "Felix  Neum\u00e4rker", "Georg  Ellguth", "Stephan  Hartmann", "Stefan  Schiefer", "Thomas  Hocker", "Dennis  Walter", "Gengting  Liu", "Jim D. Garside", "Steve B. Furber", "Christian  Mayr"], "year": 2021, "n_citations": 1}
{"id": 3463337, "s2_id": "8aefc85208074951f4756825305fd5962315b874", "title": "ArSMART: An Improved SMART NoC Design Supporting Arbitrary-Turn Transmission", "abstract": "SMART NoC, which transmits unconflicted flits to distant processing elements (PEs) in one cycle through the express bypass, is a high-performance NoC design proposed recently. However, if contention occurs, flits with low priority would not only be buffered but also could not fully utilize bypass. Although there exist several routing algorithms that decrease contentions by rounding busy routers and links, they cannot be directly applicable to SMART since it lacks the support for arbitrary-turn (i.e., the number and direction of turns are free of constraints) routing. Thus, in this article, to minimize contentions and further utilize bypass, we propose an improved SMART NoC, called ArSMART, in which arbitrary-turn transmission is enabled. Specifically, ArSMART divides the whole NoC into multiple clusters where the route computation is conducted by the cluster controller and the data forwarding is performed by the bufferless reconfigurable router. Since the long-range transmission in SMART NoC needs to bypass the intermediate arbitration, to enable this feature, we directly configure the input and output ports connection rather than apply hop-by-hop table-based arbitration. To further explore the higher communication capabilities, effective adaptive routing algorithms that are compatible with ArSMART are proposed. The route computation overhead, one of the main concerns for adaptive routing algorithms, is hidden by our carefully designed control mechanism. Compared with the state-of-the-art SMART NoC, the experimental results demonstrate an average reduction of 40.7% in application schedule length and 29.7% in energy consumption.", "venue": "ArXiv", "authors": ["Hui  Chen", "Peng  Chen", "Jun  Zhou", "H. K. Duong Luan", "Weichen  Liu"], "year": 2020, "n_citations": 3}
{"id": 3466875, "s2_id": "e23848e967ebff11c21239dce7204741b28ec9c4", "title": "Direct Spatial Implementation of Sparse Matrix Multipliers for Reservoir Computing", "abstract": "Reservoir computing systems rely on the recurrent multiplication of a very large, sparse, fixed matrix. We argue that direct spatial implementation of these fixed matrices minimizes the work performed in the computation, and allows for significant reduction in latency and power through constant propagation and logic minimization. Bit-serial arithmetic enables massive static matrices to be implemented. We present the structure of our bit-serial matrix multiplier, and evaluate using canonical signed digit representation to further reduce logic utilization. We have implemented these matrices on a large FPGA and provide a cost model that is simple and extensible. These FPGA implementations, on average, reduce latency by 50x up to 86x versus GPU libraries. Comparing against a recent sparse DNN accelerator, we measure a 4.1x to 47x reduction in latency depending on matrix dimension and sparsity. Throughput of the FPGA solution is also competitive for a wide range of matrix dimensions and batch sizes. Finally, we discuss ways these techniques could be deployed in ASICs, making them applicable for dynamic sparse matrix computations.", "venue": "ArXiv", "authors": ["Matthew  Denton", "Herman  Schmit"], "year": 2021, "n_citations": 0}
{"id": 3468700, "s2_id": "02ed6039c45bf92926497560d0ace1d29fc9cb24", "title": "Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks", "abstract": "Popular deep learning frameworks require users to fine-tune their memory usage so that the training data of a deep neural network (DNN) fits within the GPU physical memory. Prior work tries to address this restriction by virtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be utilized for memory allocations. Despite its merits, virtualizing memory can incur significant performance overheads when the time needed to copy data back and forth from CPU memory is higher than the latency to perform DNN computations. We introduce a high-performance virtualization strategy based on a \"compressing DMA engine\" (cDMA) that drastically reduces the size of the data structures that are targeted for CPU-side allocations. The cDMA engine offers an average 2.6x (maximum 13.8x) compression ratio by exploiting the sparsity inherent in offloaded data, improving the performance of virtualized DNNs by an average 53% (maximum 79%) when evaluated on an NVIDIA Titan Xp.", "venue": "2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Minsoo  Rhu", "Mike  O'Connor", "Niladrish  Chatterjee", "Jeff  Pool", "Stephen W. Keckler"], "year": 2018, "n_citations": 103}
{"id": 3469223, "s2_id": "37cde825820070ade8f75a31b41aeacd5398efc5", "title": "On whether and how D-RISC and Microgrids can be kept relevant (self-assessment report)", "abstract": "This report lays flat my personal views on D-RISC and Microgrids as of March 2013. It reflects the opinions and insights that I have gained from working on this project during the period 2008-2013. This report is structed in two parts: deconstruction and reconstruction. In the deconstruction phase, I review what I believe are the fundamental motivation and goals of the D-RISC/Microgrids enterprise, and identify what I judge are shortcomings: that the project did not deliver on its expectations, that fundamental questions are left unanswered, and that its original motivation may not even be relevant in scientific research any more in this day and age. In the reconstruction phase, I start by identifying the merits of the current D-RISC/Microgrids technology and know-how taken at face value, re-motivate its existence from a different angle, and suggest new, relevant research questions that could justify continued scientific investment.", "venue": "ArXiv", "authors": ["Raphael 'kena' Poss"], "year": 2013, "n_citations": 1}
{"id": 3469443, "s2_id": "2bcb9599dae1ec6f4ee94027ad87281665828317", "title": "Is spiking logic the route to memristor-based computers?", "abstract": "Memristors have been suggested as a novel route to neuromorphic computing based on the similarity between neurons (synapses and ion pumps) and memristors. The D.C. action of the memristor is a current spike, which we think will be fruitful for building memristor computers. In this paper, we introduce 4 different logical assignations to implement sequential logic in the memristor and introduce the physical rules, summation, `bounce-back', directionality and `diminishing returns', elucidated from our investigations. We then demonstrate how memristor sequential logic works by instantiating a NOT gate, an AND gate and a Full Adder with a single memristor. The Full Adder makes use of the memristor's memory to add three binary values together and outputs the value, the carry digit and even the order they were input in.", "venue": "2013 IEEE 20th International Conference on Electronics, Circuits, and Systems (ICECS)", "authors": ["Ella  Gale", "Ben de Lacy Costello", "Andrew  Adamatzky"], "year": 2013, "n_citations": 12}
{"id": 3471353, "s2_id": "20a6369403a0c8c2d83f7c2be2a18425eeb8445e", "title": "Addressing Variability in Reuse Prediction for Last-Level Caches", "abstract": "Last-Level Cache (LLC) represents the bulk of a modern CPU processor's transistor budget and is essential for application performance as LLC enables fast access to data in contrast to much slower main memory. However, applications with large working set size often exhibit streaming and/or thrashing access patterns at LLC. As a result, a large fraction of the LLC capacity is occupied by dead blocks that will not be referenced again, leading to inefficient utilization of the LLC capacity. To improve cache efficiency, the state-of-the-art cache management techniques employ prediction mechanisms that learn from the past access patterns with an aim to accurately identify as many dead blocks as possible. Once identified, dead blocks are evicted from LLC to make space for potentially high reuse cache blocks. \nIn this thesis, we identify variability in the reuse behavior of cache blocks as the key limiting factor in maximizing cache efficiency for state-of-the-art predictive techniques. Variability in reuse prediction is inevitable due to numerous factors that are outside the control of LLC. The sources of variability include control-flow variation, speculative execution and contention from cores sharing the cache, among others. Variability in reuse prediction challenges existing techniques in reliably identifying the end of a block's useful lifetime, thus causing lower prediction accuracy, coverage, or both. To address this challenge, this thesis aims to design robust cache management mechanisms and policies for LLC in the face of variability in reuse prediction to minimize cache misses, while keeping the cost and complexity of the hardware implementation low. To that end, we propose two cache management techniques, one domain-agnostic and one domain-specialized, to improve cache efficiency by addressing variability in reuse prediction.", "venue": "ArXiv", "authors": ["Priyank  Faldu"], "year": 2020, "n_citations": 0}
{"id": 3472357, "s2_id": "8604aca909c16c5ab1f1e16cae48b3091696f574", "title": "On a composition of digraphs", "abstract": "Many \"good\" topologies for interconnection networks are based on line digraphs of regular digraphs. These digraphs support unitary matrices. We propose the property \"being the digraph of a unitary matrix\" as additional criterion for the design of new interconnection networks. We define a composition of digraphs, which we call diagonal union. Diagonal union can be used to construct digraphs of unitary matrices. We remark that digraphs obtained via diagonal union are state split graphs, as defined in symbolic dynamics. Finally, we list some potential directions for future research.", "venue": "ArXiv", "authors": ["Simone  Severini"], "year": 2003, "n_citations": 0}
{"id": 3472756, "s2_id": "08c1f95921e91c08812f2bf7b6c1c53051e7684d", "title": "Reversible Computing with Fast, Fully Static, Fully Adiabatic CMOS", "abstract": "To advance the energy efficiency of general digital computing far beyond the thermodynamic limits that apply to conventional digital circuits will require utilizing the principles of reversible computing. It has been known since the early 1990s that reversible computing based on adiabatic switching is possible in CMOS, although almost all the \"adiabatic\" CMOS logic families in the literature are not actually fully adiabatic, which limits their achievable energy savings. The first CMOS logic style achieving truly, fully adiabatic operation if leakage was negligible (CRL) was not fully static, which led to practical engineering difficulties in the presence of certain nonidealities. Later, \"static\" adiabatic logic families were described, but they were not actually fully adiabatic, or fully static, and were much slower.In this paper, we describe a new logic family, Static 2-Level Adiabatic Logic (S2LAL), which is, to our knowledge, the first CMOS logic family that is both fully static, and truly, fully adiabatic (modulo leakage). In addition, S2LAL is, we think, the fastest possible such family (among fully pipelined sequential circuits), having a latency per logic stage of one tick (transition time), and a minimum clock period (initiation interval) of 8 ticks. S2LAL requires 8 phases of a trapezoidal power-clock waveform (plus constant power and ground references) to be supplied. We argue that, if implemented in a suitable fabrication process designed to aggressively minimize leakage, S2LAL should be capable of demonstrating a greater level of energy efficiency than any other semiconductor-based digital logic family known today.", "venue": "2020 International Conference on Rebooting Computing (ICRC)", "authors": ["Michael P. Frank", "Robert W. Brocato", "Brian D. Tierney", "Nancy A. Missert", "Alexander H. Hsia"], "year": 2020, "n_citations": 9}
{"id": 3473959, "s2_id": "fb8ed3c88d487db5efd18bca4f83085f8b4b0345", "title": "UNBIAS PUF: A Physical Implementation Bias Agnostic Strong PUF", "abstract": "The Physical Unclonable Function (PUF) is a promising hardware security primitive because of its inherent uniqueness and low cost. To extract the device-specific variation from delay-based strong PUFs, complex routing constraints are imposed to achieve symmetric path delays; and systematic variations can severely compromise the uniqueness of the PUF. In addition, the metastability of the arbiter circuit of an Arbiter PUF can also degrade the quality of the PUF due to the induced instability. In this paper we propose a novel strong UNBIAS PUF that can be implemented purely by Register Transfer Language (RTL), such as verilog, without imposing any physical design constraints or delay characterization effort to solve the aforementioned issues. Efficient inspection bit prediction models for unbiased response extraction are proposed and validated. Our experimental results of the strong UNBIAS PUF show 5.9% intra-Fractional Hamming Distance (FHD) and 45.1% inter-FHD on 7 Field Programmable Gate Array (FPGA) boards without applying any physical layout constraints or additional XOR gates. The UNBIAS PUF is also scalable because no characterization cost is required for each challenge to compensate the implementation bias. The averaged intra-FHD measured at worst temperature and voltage variation conditions is 12%, which is still below the margin of practical Error Correction Code (ECC) with error reduction techniques for PUFs.", "venue": "ArXiv", "authors": ["Wei-Che  Wang", "Zhuoqi  Li", "Joseph P. Skudlarek", "Mario  Larouche", "Michael  Chen", "Puneet  Gupta"], "year": 2017, "n_citations": 0}
{"id": 3479218, "s2_id": "b7d96ef5468b43c44da8f9d6ce6559f99cf97ddc", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference", "abstract": "Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (i) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (ii) replaces most multiply-accumulations with additions, and (iii) reduces the off-chip traffic by amplifying on-chip memory capacity.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Ali Hadi Zadeh", "Andreas  Moshovos"], "year": 2020, "n_citations": 27}
{"id": 3479627, "s2_id": "ff234a70c98eb29abf777e88fcdd988f4a66acc7", "title": "Efficient Implementation of Multi-Channel Convolution in Monolithic 3D ReRAM Crossbar", "abstract": "Convolutional neural networks (CNNs) demonstrate promising accuracy in a wide range of applications. Among all layers in CNNs, convolution layers are the most computation-intensive and consume the most energy. As the maturity of device and fabrication technology, 3D resistive random access memory (ReRAM) receives substantial attention for accelerating large vector-matrix multiplication and convolution due to its high parallelism and energy efficiency benefits. However, implementing multi-channel convolution naively in 3D ReRAM will either produce incorrect results or exploit only partial parallelism of 3D ReRAM. In this paper, we propose a 3D ReRAM-based convolution accelerator architecture, which efficiently maps multi-channel convolution to monolithic 3D ReRAM. Our design has two key principles. First, we exploit the intertwined structure of 3D ReRAM to implement multi-channel convolution by using a state-of-the-art convolution algorithm. Second, we propose a new approach to efficiently implement negative weights by separating them from non-negative weights using configurable interconnects. Our evaluation demonstrates that our mapping scheme in 16-layer 3D ReRAM achieves a speedup of 5.79X, 927.81X, and 36.8X compared with a custom 2D ReRAM baseline and state-of-the-art CPU and GPU. Our design also reduces energy consumption by 2.12X, 1802.64X, and 114.1X compared with the same baseline.", "venue": "ArXiv", "authors": ["Sho  Ko", "Yun Joon Soh", "Jishen  Zhao"], "year": 2020, "n_citations": 1}
{"id": 3480175, "s2_id": "4fe749379067cbbe9ee428ac54a830aa89d8e743", "title": "Sprinkler: Maximizing resource utilization in many-chip solid state disks", "abstract": "Resource utilization is one of the emerging problems in many-chip SSDs. In this paper, we propose Sprinkler, a novel device-level SSD controller, which targets maximizing resource utilization and achieving high performance without additional NAND flash chips. Specifically, Sprinkler relaxes parallelism dependency by scheduling I/O requests based on internal resource layout rather than the order imposed by the device-level queue. In addition, Sprinkler improves flash-level parallelism and reduces the number of transactions (i.e., improves transactionallocality) by over-committing flash memory requests to specific resources. Our extensive experimental evaluation using a cycle-accurate large-scale SSD simulation framework shows that a many-chip SSD equipped with our Sprinkler provides at least 56.6% shorter latency and 1.8 -2.2 times better throughput than the state-of-the-art SSD controllers. Further, it improves overall resource utilization by 68.8% under different I/O request patterns and provides, on average, 80.2% more flash-level parallelism by reducing half of the flash memory requests at runtime.", "venue": "2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Myoungsoo  Jung", "Mahmut T. Kandemir"], "year": 2014, "n_citations": 50}
{"id": 3488504, "s2_id": "79f465795504afef00cc9c8c81bf7b89f3ec89a8", "title": "Mixed precision in Graphics Processing Unit", "abstract": "Modern graphics computing units (GPUs) are designed and optimized to perform highly parallel numerical calculations. This parallelism has enabled (and promises) significant advantages, both in terms of energy performance and calculation. In this document, we take stock of the different applications of mixed precision. We recall the standards currently used in the overwhelming majority of systems in terms of numerical computation. We show that the mixed precision which decreases the precision at the input of an operation does not necessarily decrease the precision of its output. We show that this previous principle allows its transposition into one of the branches that most needs computing power: machine learning. The use of fixed point numbers and half-precision are two very effective ways to increase the learning ability of complex neural networks. Mixed precision still requires the use of suitable hardware, failing which the calculation time could on the contrary be lengthened. The NVIDIA Tensor Core that is found among others in their Tesla V100 range, is an example of implementation at the hardware level of mixed precision. On the other hand, by abandoning the traditional von Neumann model, mixed precision can also be transposed to a lower level of abstraction, using phase change memories.", "venue": "ArXiv", "authors": ["Quentin  Gallou'edec"], "year": 2021, "n_citations": 0}
{"id": 3489058, "s2_id": "76ed7e41fabd91a86f5e5dc58688a58f329d31db", "title": "Recent Advances in DRAM and Flash Memory Architectures", "abstract": "This article features extended summaries and retrospectives of some of the recent research done by our group, SAFARI, on (1) understanding, characterizing, and modeling various critical properties of modern DRAM and NAND flash memory, the dominant memory and storage technologies, respectively; and (2) several new mechanisms we have proposed based on our observations from these analyses, characterization, and modeling, to tackle various key challenges in memory and storage scaling. In order to understand the sources of various bottlenecks of the dominant memory and storage technologies, these works perform rigorous studies of device-level and application-level behavior, using a combination of detailed simulation and experimental characterization of real memory and storage devices.", "venue": "ArXiv", "authors": ["Onur  Mutlu", "Saugata  Ghose", "Rachata  Ausavarungnirun"], "year": 2018, "n_citations": 3}
{"id": 3489191, "s2_id": "258bc4736817faa2d0df09929ef98bb0f2c11565", "title": "Digital Neuron: A Hardware Inference Accelerator for Convolutional Deep Neural Networks", "abstract": "We propose a Digital Neuron, a hardware inference accelerator for convolutional deep neural networks with integer inputs and integer weights for embedded systems. The main idea to reduce circuit area and power consumption is manipulating dot products between input feature and weight vectors by Barrel shifters and parallel adders. The reduced area allows the more computational engines to be mounted on an inference accelerator, resulting in high throughput compared to prior HW accelerators. We verified that the multiplication of integer numbers with 3-partial sub-integers does not cause significant loss of inference accuracy compared to 32-bit floating point calculation. The proposed digital neuron can perform 800 MAC operations in one clock for computation for convolution as well as full-connection. This paper provides a scheme that reuses input, weight, and output of all layers to reduce DRAM access. In addition, this paper proposes a configurable architecture that can provide inference of adaptable feature of convolutional neural networks. The throughput in terms of Watt of the digital neuron is achieved 754.7 GMACs/W.", "venue": "ArXiv", "authors": ["Hyunbin  Park", "Dohyun  Kim", "Shiho  Kim"], "year": 2018, "n_citations": 2}
{"id": 3490533, "s2_id": "bc60ccbdeeb3120b645edd01072304b2b3cd651e", "title": "CIAO: Cache Interference-Aware Throughput-Oriented Architecture and Scheduling for GPUs", "abstract": "A modern GPU aims to simultaneously execute more warps for higher Thread-Level Parallelism (TLP) and performance. When generating many memory requests, however, warps contend for limited cache space and thrash cache, which in turn severely degrades performance. To reduce such cache thrashing, we may adopt cache locality-aware warp scheduling which gives higher execution priority to warps with higher potential of data locality. However, we observe that warps with high potential of data locality often incurs far more cache thrashing or interference than warps with low potential of data locality. Consequently, cache locality-aware warp scheduling may undesirably increase cache interference and/or unnecessarily decrease TLP. In this paper, we propose Cache Interference-Aware throughput-Oriented (CIAO) on-chip memory architecture and warp scheduling which exploit unused shared memory space and take insight opposite to cache locality-aware warp scheduling. Specifically, CIAO on-chip memory architecture can adaptively redirect memory requests of severely interfering warps to unused shared memory space to isolate memory requests of these interfering warps from those of interfered warps. If these interfering warps still incur severe cache interference, CIAO warp scheduling then begins to selectively throttle execution of these interfering warps. Our experiment shows that CIAO can offer 54% higher performance than prior cache locality-aware scheduling at a small chip cost.", "venue": "2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Jie  Zhang", "Shuwen  Gao", "Nam Sung Kim", "Myoungsoo  Jung"], "year": 2018, "n_citations": 3}
{"id": 3494790, "s2_id": "dd5bd899fd939fa056416325d6ad4cef79d7dbaf", "title": "Variability-Aware Design for Energy Efficient Computational Artificial Intelligence Platform", "abstract": "Portable computing devices, which include tablets, smart phones and various types of wearable sensors, experienced a rapid development in recent years. One of the most critical limitations for these devices is the power consumption as they use batteries as the power supply. However, the bottleneck of the power saving schemes in both hardware design and software algorithm is the huge variability in power consumption. The variability is caused by a myriad of factors, including the manufacturing process, the ambient environment (temperature, humidity), the aging effects and etc. As the technology node scaled down to 28nm and even lower, the variability becomes more severe. As a result, a platform for variability characterization seems to be very necessary and helpful.", "venue": "ArXiv", "authors": ["Rhonda P. Zhang"], "year": 2017, "n_citations": 0}
{"id": 3499185, "s2_id": "3af0c863e0e5411fb392dc377cba80cf54a5d125", "title": "ReSpawn: Energy-Efficient Fault-Tolerance for Spiking Neural Networks considering Unreliable Memories", "abstract": "Spiking neural networks (SNNs) have shown a potential for having low energy with unsupervised learning capabilities due to their biologically-inspired computation. However, they may suffer from accuracy degradation if their processing is performed under the presence of hardware-induced faults in memories, which can come from manufacturing defects or voltage-induced approximation errors. Since recent works still focus on the fault-modeling and random fault injection in SNNs, the impact of memory faults in SNN hardware architectures on accuracy and the respective fault-mitigation techniques are not thoroughly explored. Toward this, we propose ReSpawn, a novel framework for mitigating the negative impacts of faults in both the off-chip and on-chip memories for resilient and energy-efficient SNNs. The key mechanisms of ReSpawn are: (1) analyzing the fault tolerance of SNNs; and (2) improving the SNN fault tolerance through (a) fault-aware mapping (FAM) in memories, and (b) fault-aware training-and-mapping (FATM). If the training dataset is not fully available, FAM is employed through efficient bit-shuffling techniques that place the significant bits on the non-faulty memory cells and the insignificant bits on the faulty ones, while minimizing the memory access energy. Meanwhile, if the training dataset is fully available, FATM is employed by considering the faulty memory cells in the data mapping and training processes. The experimental results show that, compared to the baseline SNN without fault-mitigation techniques, ReSpawn with a fault-aware mapping scheme improves the accuracy by up to 70% for a network with 900 neurons without retraining.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Abdullah Hanif", "Muhammad  Shafique"], "year": 2021, "n_citations": 1}
{"id": 3502285, "s2_id": "00d9612ad0e853a40465051a23aaa2ecf0051f8a", "title": "Seeds of SEED: A Side-Channel Resilient Cache Skewed by a Linear Function over a Galois Field", "abstract": "Consider a set-associative cache with pn sets and pn ways where p is prime and n > 0. Furthermore, assume that the cache may be shared among pn mutually distrusting principals that may use the Prime+Probe side-channel attack against one another; architecturally, these principals occupy separate security domains (for example, separate processes, virtual machines, sandboxes, etc.). This paper shows that there exists a linear skewing of cache sets over the Galois field Gp n that exhibits the following property: each cache set of each security domain intersects every cache set of every other security domain exactly once. Therefore, a random eviction from a single cache set in security domain A may be observed via Prime+Probe in any of security domain B\u2019s cache sets. This paper characterizes this linear skewing and describes how it can be implemented efficiently in hardware.", "venue": "2021 International Symposium on Secure and Private Execution Environment Design (SEED)", "authors": ["Scott  Constable", "Thomas  Unterluggauer"], "year": 2021, "n_citations": 0}
{"id": 3502483, "s2_id": "69fd7129613569441b200f3bca726eab3ce23423", "title": "FPGA deep learning acceleration based on convolutional neural network", "abstract": "In view of the large amount of calculation and long calculation time of convolutional neural network (CNN), this paper proposes a convolutional neural network hardware accelerator based on field programmable logic gate array (FPGA). First, through in-depth analysis of the forward operation principle of the convolutional layer and exploration of the parallelism of the convolutional layer operation, a hardware architecture of input channel parallelism, output channel parallelism and convolution window deep pipeline is designed. Then in the above architecture, a fully parallel multiplication-addition tree module is designed to accelerate the convolution operation and an efficient window buffer module to implement the pipeline operation of the convolution window. The final experimental results show that the energy efficiency ratio of the accelerator proposed in this article reaches 32.73 GOPS/W, which is 34% higher than the existing solution, and the performance reaches 317.86 GOPS.", "venue": "ArXiv", "authors": ["Xiong  Jun"], "year": 2020, "n_citations": 1}
{"id": 3503315, "s2_id": "f5a0e99056551a48c86a86405af71a75489e993b", "title": "A Software-defined SoC Memory Bus Bridge Architecture for Disaggregated Computing", "abstract": "Disaggregation and rack-scale systems have the potential of drastically decreasing TCO and increasing utilization of cloud datacenters, while maintaining performance. While the concept of organising resources in separate pools and interconnecting them together on demand is straightforward, its materialisation can be radically different in terms of performance and scale potential. In this paper, we presenta memory bus bridge architecture which enables communication between 100s of masters and slaves in todays complex multiprocessor SoCs, that are physically intregrated in different chips and even different mainboards. The bridge tightly couples serial transceivers and a circuit network for chip-to-chip transfers. A key property of the proposed bridge architecture is that it is software-defined and thus can be configured at runtime, via a software control plane, to prepare and steer memory access transactions to remote slaves. This is particularly important because it enables datacenter orchestration tools to manage the disaggregated resource allocation. Moreover, we evaluate a bridge prototype we have build for ARM AXI4 memory bus interconnect and we discuss application-level observed performance.", "venue": "AISTECS@HiPEAC", "authors": ["Dimitris  Syrivelis", "Andrea  Reale", "Kostas  Katrinis", "Christian  Pinto"], "year": 2018, "n_citations": 4}
{"id": 3503876, "s2_id": "8aa99bb086cb8e1370e60a42aea56eac61cf2eb1", "title": "Evaluation and Design Space Exploration of a Time-Division Multiplexed NoC on FPGA for Image Analysis Applications", "abstract": "The aim of this paper is to present an adaptable Fat Tree NoC architecture for Field Programmable Gate Array (FPGA) designed for image analysis applications. Traditional Network on Chip (NoC) is not optimal for dataflow applications with large amount of data. On the opposite, point-to-point communications are designed from the algorithm requirements but they are expensives in terms of resource and wire. We propose a dedicated communication architecture for image analysis algorithms. This communication mechanism is a generic NoC infrastructure dedicated to dataflow image processing applications, mixing circuit-switching and packet-switching communications. The complete architecture integrates two dedicated communication architectures and reusable IP blocks. Communications are based on the NoC concept to support the high bandwidth required for a large number and type of data. For data communication inside the architecture, an efficient time-division multiplexed (TDM) architecture is proposed. This NoC uses a Fat Tree (FT) topology with Virtual Channels (VCs) and flit packet-switching with fixed routes. Two versions of the NoC are presented in this paper. The results of their implementations and their Design Space Exploration (DSE) on Altera Stratix II are analyzed and compared with a point-to-point communication and illustrated with a multispectral image application. Results show that a point-to-point communication scheme is not efficient for large amount of multispectral image data communications. An NoC architecture uses only 10% of the memory blocks required for a point-to-point architecture but seven times more logic elements. This resource allocation is more adapted to image analysis algorithms as memory elements are a critical point in embedded architectures. An FT NoC-based communication scheme for data transfers provides a more appropriate solution for resource allocation.", "venue": "EURASIP J. Embed. Syst.", "authors": ["Linlin  Zhang", "Virginie  Fresse", "Mohammed  Khalid", "Dominique  Houzet", "Anne-Claire  Legrand"], "year": 2009, "n_citations": 6}
{"id": 3511682, "s2_id": "12b68cb06efdab086a80c638d21a2927e8d82f59", "title": "SoftMC: Practical DRAM Characterization Using an FPGA-Based Infrastructure", "abstract": "This paper summarizes the SoftMC DRAM characterization infrastructure, which was published in HPCA 2017, and examines the work's significance and future potential. \nSoftMC (Soft Memory Controller) is the first publicly-available DRAM testing infrastructure that can flexibly and efficiently test DRAM chips in a manner accessible to both software and hardware developers. SoftMC is an FPGA-based testing platform that can control and test memory modules designed for the commonly-used DDR (Double Data Rate) interface. SoftMC has two key properties: (i) it provides flexibility to thoroughly control memory behavior or to implement a wide range of mechanisms using DDR commands; and (ii) it is easy to use as it provides a simple and intuitive high-level programming interface for users, completely hiding the low-level details of the FPGA. \nWe demonstrate the capability, flexibility, and programming ease of SoftMC with two example use cases. First, we implement a test that characterizes the retention time of DRAM cells. Second, we show that the expected latency reduction of two recently-proposed mechanisms, which rely on accessing recently-refreshed or recently-accessed DRAM cells faster than other DRAM cells, is not observable in existing DRAM chips. \nVarious versions of the SoftMC platform have enabled many of our other DRAM characterization studies. We discuss several other use cases of SoftMC, including the ability to characterize emerging non-volatile memory modules that obey the DDR standard. We hope that our open-source release of SoftMC fills a gap in the space of publicly-available experimental memory testing infrastructures and inspires new studies, ideas, and methodologies in memory system design.", "venue": "ArXiv", "authors": ["Hasan  Hassan", "Nandita  Vijaykumar", "Samira Manabi Khan", "Saugata  Ghose", "Kevin K. Chang", "Gennady  Pekhimenko", "Donghyuk  Lee", "Oguz  Ergin", "Onur  Mutlu"], "year": 2018, "n_citations": 1}
{"id": 3514053, "s2_id": "7731e14abb150467f0e8af519843e172e78efcda", "title": "Branch Prediction Is Not A Solved Problem: Measurements, Opportunities, and Future Directions", "abstract": "Modern branch predictors predict the vast majority of conditional branch instructions with near-perfect accuracy, allowing superscalar, out-of-order processors to maximize speculative efficiency and thus performance. However, this impressive overall effectiveness belies a substantial missed opportunity in single-threaded instructions per cycle (IPC). For example, we show that correcting the mispredictions made by the state-of-the-art TAGE-SC-L branch predictor on SPECint 2017 would improve IPC by margins similar to an advance in process technology node. In this work, we measure and characterize these mispredictions. We find that they categorically arise from either (1) a small number of systematically hard-to-predict (H2P) branches; or (2) rare branches with low dynamic execution counts. Using data from SPECint 2017 and additional large code footprint applications, we quantify the occurrence and IPC impact of these two categories. We then demonstrate that solely increasing the resources afforded to existing branch predictors does not address the root causes of most mispredictions. This leads us to reexamine basic assumptions in branch prediction and to propose new research directions that, for example, deploy machine learning to improve pattern matching for H2Ps, and use on-chip phase learning to track long-term statistics for rare branches.", "venue": "2019 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Chit-Kwan  Lin", "Stephen J. Tarsa"], "year": 2019, "n_citations": 8}
{"id": 3516149, "s2_id": "ade903df1e67fb59069b51a0a8fc227853a4a8dc", "title": "Errors in Flash-Memory-Based Solid-State Drives: Analysis, Mitigation, and Recovery", "abstract": "NAND flash memory is ubiquitous in everyday life today because its capacity has continuously increased and cost has continuously decreased over decades. This positive growth is a result of two key trends: (1) effective process technology scaling; and (2) multi-level (e.g., MLC, TLC) cell data coding. Unfortunately, the reliability of raw data stored in flash memory has also continued to become more difficult to ensure, because these two trends lead to (1) fewer electrons in the flash memory cell floating gate to represent the data; and (2) larger cell-to-cell interference and disturbance effects. Without mitigation, worsening reliability can reduce the lifetime of NAND flash memory. As a result, flash memory controllers in solid-state drives (SSDs) have become much more sophisticated: they incorporate many effective techniques to ensure the correct interpretation of noisy data stored in flash memory cells. \nIn this chapter, we review recent advances in SSD error characterization, mitigation, and data recovery techniques for reliability and lifetime improvement. We provide rigorous experimental data from state-of-the-art MLC and TLC NAND flash devices on various types of flash memory errors, to motivate the need for such techniques. Based on the understanding developed by the experimental characterization, we describe several mitigation and recovery techniques, including (1) cell-tocell interference mitigation; (2) optimal multi-level cell sensing; (3) error correction using state-of-the-art algorithms and methods; and (4) data recovery when error correction fails. We quantify the reliability improvement provided by each of these techniques. Looking forward, we briefly discuss how flash memory and these techniques could evolve into the future.", "venue": "ArXiv", "authors": ["Yu  Cai", "Saugata  Ghose", "Erich F. Haratsch", "Yixin  Luo", "Onur  Mutlu"], "year": 2017, "n_citations": 54}
{"id": 3520255, "s2_id": "c0659ff396a78f6714ca4202f40d76fa60306df3", "title": "A partitioning methodology for accelerating applications in hybrid reconfigurable platforms", "abstract": "In this paper, we propose a methodology for partitioning and mapping computational intensive applications in reconfigurable hardware blocks of different granularity. A generic hybrid reconfigurable architecture is considered so as the methodology can be applicable to a large number of heterogeneous reconfigurable platforms. The methodology mainly consists of two stages, the analysis and the mapping of the application onto fine and coarse-grain hardware resources. A prototype framework consisting of analysis, partitioning and mapping tools has been also developed. For the coarse-grain reconfigurable hardware, we use our previously developed high-performance coarse-grain datapath. In this work, the methodology is validated using two real-world applications, an OFDM transmitter and a JPEG encoder. In the case of the OFDM transmitter, a maximum clock cycle decrease of 82 % relative to the ones in an all fine-grain mapping solution is achieved. The corresponding performance improvement for the JPEG is 43 %.", "venue": "Design, Automation and Test in Europe", "authors": ["Michalis D. Galanis", "Athanasios  Milidonis", "George  Theodoridis", "Dimitrios  Soudris", "Constantinos E. Goutis"], "year": 2005, "n_citations": 13}
{"id": 3520438, "s2_id": "5cd3467cb6377b8e08a56f800e59c4e3ca33b240", "title": "Neuromorphic hardware as a self-organizing computing system", "abstract": "This paper presents the self-organized neuromorphic architecture named SOMA. The objective is to study neural-based self-organization in computing systems and to prove the feasibility of a self-organizing hardware structure. Considering that these properties emerge from large scale and fully connected neural maps, we will focus on the definition of a self-organizing hardware architecture based on digital spiking neurons that offer hardware efficiency. From a biological point of view, this corresponds to a combination of the so-called synaptic and structural plasticities. We intend to define computational models able to simultaneously self-organize at both computation and communication levels, and we want these models to be hardware-compliant, fault tolerant and scalable by means of a neuro-cellular structure.", "venue": "ArXiv", "authors": ["Lyes  Khacef", "Bernard  Girau", "Nicolas P. Rougier", "Andres  Upegui", "Beno\u00eet  Miramond"], "year": 2018, "n_citations": 9}
{"id": 3521205, "s2_id": "79ba9e9ec975d0d366ca2bfa4d8501467ea41ec1", "title": "Arch2030: A Vision of Computer Architecture Research over the Next 15 Years", "abstract": "Application trends, device technologies and the architecture of systems drive progress in information technologies. However, the former engines of such progress - Moore's Law and Dennard Scaling - are rapidly reaching the point of diminishing returns. The time has come for the computing community to boldly confront a new challenge: how to secure a foundational future for information technology's continued progress. The computer architecture community engaged in several visioning exercises over the years. Five years ago, we released a white paper, 21st Century Computer Architecture, which influenced funding programs in both academia and industry. More recently, the IEEE Rebooting Computing Initiative explored the future of computing systems in the architecture, device, and circuit domains. This report stems from an effort to continue this dialogue, reach out to the applications and devices/circuits communities, and understand their trends and vision. We aim to identify opportunities where architecture research can bridge the gap between the application and device domains.", "venue": "ArXiv", "authors": ["Luis  Ceze", "Mark D. Hill", "Thomas F. Wenisch"], "year": 2016, "n_citations": 25}
{"id": 3525289, "s2_id": "224c34f1a0ba77da85f9fbdc674471ea833009d6", "title": "Versa: A Dataflow-Centric Multiprocessor with 36 Systolic ARM Cortex-M4F Cores and a Reconfigurable Crossbar-Memory Hierarchy in 28nm", "abstract": "We present Versa, an energy-efficient processor with 36 systolic ARM Cortex-M4F cores and a runtime-reconfigurable memory hierarchy. Versa exploits algorithm-specific characteristics in order to optimize bandwidth, access latency, and data reuse. Measured on a set of kernels with diverse data access, control, and synchronization characteristics, reconfiguration between different Versa modes yields median energy-efficiency improvements of 11.6\u00d7 and 37.2\u00d7 over mobile CPU and GPU baselines, respectively.", "venue": "2021 Symposium on VLSI Circuits", "authors": ["Sung  Kim", "Morteza  Fayazi", "Alhad  Daftardar", "Kuan-Yu  Chen", "Jielun  Tan", "Subhankar  Pal", "Tutu  Ajayi", "Yan  Xiong", "Trevor N. Mudge", "Chaitali  Chakrabarti", "David  Blaauw", "Ronald G. Dreslinski", "Hun-Seok  Kim"], "year": 2021, "n_citations": 0}
{"id": 3526371, "s2_id": "05b1450d6e9f97ac89bfcc8007a8699a9c56ca5c", "title": "XNOR Neural Engine: A Hardware Accelerator IP for 21.6-fJ/op Binary Neural Network Inference", "abstract": "Binary neural networks (BNNs) are promising to deliver accuracy comparable to conventional deep neural networks at a fraction of the cost in terms of memory and energy. In this paper, we introduce the XNOR neural engine (XNE), a fully digital configurable hardware accelerator IP for BNNs, integrated within a microcontroller unit (MCU) equipped with an autonomous I/O subsystem and hybrid SRAM/standard cell memory. The XNE is able to fully compute convolutional and dense layers in autonomy or in cooperation with the core in the MCU to realize more complex behaviors. We show post-synthesis results in 65- and 22-nm technology for the XNE IP and post-layout results in 22 nm for the full MCU indicating that this system can drop the energy cost per binary operation to 21.6 fJ per operation at 0.4 V, and at the same time is flexible and performant enough to execute state-of-the-art BNN topologies such as ResNet-34 in less than 2.2 mJ per frame at 8.9 frames/s.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Francesco  Conti", "Pasquale Davide Schiavone", "Luca  Benini"], "year": 2018, "n_citations": 50}
{"id": 3528140, "s2_id": "93eed485284a7a89e88bbe3766a48d7d5c5489f3", "title": "Real-Time Refocusing Using an FPGA-Based Standard Plenoptic Camera", "abstract": "Plenoptic cameras are receiving increased attention in scientific and commercial applications because they capture the entire structure of light in a scene, enabling optical transforms (such as focusing) to be applied computationally after the fact, rather than once and for all at the time a picture is taken. In many settings, real-time interactive performance is also desired, which in turn requires significant computational power due to the large amount of data required to represent a plenoptic image. Although GPUs have been shown to provide acceptable performance for real-time plenoptic rendering, their cost and power requirements make them prohibitive for embedded uses (such as in-camera). On the other hand, the computation to accomplish plenoptic rendering is well structured, suggesting the use of specialized hardware. Accordingly, this paper presents an array of switch-driven finite impulse response filters, implemented with field programmable gate array (FPGA) to accomplish high-throughput spatial-domain rendering. The proposed architecture provides a power-efficient rendering hardware design suitable for full-video applications as required in broadcasting or cinematography. A benchmark assessment of the proposed hardware implementation shows that real-time performance can readily be achieved, with a one order of magnitude performance improvement over a GPU implementation and three orders of magnitude performance improvement over a general-purpose CPU implementation.", "venue": "IEEE Transactions on Industrial Electronics", "authors": ["Christopher  Hahne", "Andrew  Lumsdaine", "Amar  Aggoun", "Vladan  Velisavljevic"], "year": 2018, "n_citations": 9}
{"id": 3532475, "s2_id": "976126a3ec00e9bfa373ae7abbae0d26eccfebb5", "title": "NeuroMAX: A High Throughput, Multi-Threaded, Log-Based Accelerator for Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNNs) require high throughput hardware accelerators for real time applications owing to their huge computational cost. Most traditional CNN accelerators rely on single core, linear processing elements (PEs) in conjunction with 1D dataflows for accelerating convolution operations. This limits the maximum achievable ratio of peak throughput per PE count to unity. Most of the past works optimize their dataflows to attain close to a 100% hardware utilization to reach this ratio. In this paper, we introduce a high throughput, multi-threaded, log-based PE core. The designed core provides a 200% increase in peak throughput per PE count while only incurring a 6% increase in area overhead compared to a single, linear multiplier PE core with same output bit precision. We also present a 2D weight broadcast dataflow which exploits the multi-threaded nature of the PE cores to achieve a high hardware utilization per layer for various CNNs. The entire architecture, which we refer to as NeuroMAX, is implemented on Xilinx Zynq 7020 SoC at 200 MHz processing clock. Detailed analysis is performed on throughput, hardware utilization, area and power breakdown, and latency to show performance improvement compared to previous FPGA and ASIC designs.", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Mahmood Azhar Qureshi", "Arslan  Munir"], "year": 2020, "n_citations": 3}
{"id": 3533561, "s2_id": "321ccac8e298a5b53e3df3becff4e79b0b216870", "title": "A Machine Learning Pipeline Stage for Adaptive Frequency Adjustment", "abstract": "A machine learning (ML) design framework is proposed for adaptively adjusting clock frequency based on propagation delay of individual instructions. A random forest model is trained to classify propagation delays in real time, utilizing current operation type, current operands, and computation history as ML features. The trained model is implemented in Verilog as an additional pipeline stage within a baseline processor. The modified system is experimentally tested at the gate level in 45 nm CMOS technology, exhibiting a speedup of 70% and energy reduction of 30% with coarse-grained ML classification. A speedup of 89% is demonstrated with finer granularities with 15.5% reduction in energy consumption.", "venue": "ArXiv", "authors": ["Arash Fouman Ajirlou", "Inna  Partin-Vaisband"], "year": 2020, "n_citations": 2}
{"id": 3541515, "s2_id": "a8652fd1c7806ae4557eac4113dcdd7ea77f1d10", "title": "Symbolic Verification of Quantum Circuits", "abstract": "This short note proposes a symbolic approach for representing and reasoning about quantum circuits using complex, vector or matrix-valued Boolean expressions. A major benefit of this approach is that it allows us to directly borrow the existing techniques and tools for verification of classical logic circuits in reasoning about quantum circuits.", "venue": "ArXiv", "authors": ["Mingsheng  Ying", "Zhengfeng  Ji"], "year": 2020, "n_citations": 1}
{"id": 3543665, "s2_id": "b14e61328589d41db11eb649fedf081b39c5754c", "title": "An Energy-Efficient FPGA-Based Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution", "abstract": "Convolutional neural networks (CNNs) demonstrate excellent performance in various computer vision applications. In recent years, FPGA-based CNN accelerators have been proposed for optimizing performance and power efficiency. Most accelerators are designed for object detection and recognition algorithms that are performed on low-resolution images. However, real-time image super-resolution (SR) cannot be implemented on a typical accelerator because of the long execution cycles required to generate high-resolution (HR) images, such as those used in ultra-high-definition systems. In this paper, we propose a novel CNN accelerator with efficient parallelization methods for SR applications. First, we propose a new methodology for optimizing the deconvolutional neural networks (DCNNs) used for increasing feature maps. Second, we propose a novel method to optimize CNN dataflow so that the SR algorithm can be driven at low power in display applications. Finally, we quantize and compress a DCNN-based SR algorithm into an optimal model for efficient inference using on-chip memory. We present an energy-efficient architecture for SR and validate our architecture on a mobile panel with quad-high-definition resolution. Our experimental results show that, with the same hardware resources, the proposed DCNN accelerator achieves a throughput up to 108 times greater than that of a conventional DCNN accelerator. In addition, our SR system achieves an energy efficiency of 144.9, 293.0, and 500.2 GOPS/W at SR scale factors of 2, 3, and 4, respectively. Furthermore, we demonstrate that our system can restore HR images to a high quality while greatly reducing the data bit-width and the number of parameters compared with conventional SR algorithms.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "authors": ["Jung-Woo  Chang", "Keon-Woo  Kang", "Suk-Ju  Kang"], "year": 2020, "n_citations": 25}
{"id": 3546600, "s2_id": "445ca3a42a3a593c9dfa71a73b042549dcab5a31", "title": "Methodology for standard cell compliance and detailed placement for triple patterning lithography", "abstract": "As the feature size of semiconductor process further scales to sub-16nm technology node, triple patterning lithography (TPL) has been regarded one of the most promising lithography candidates. M1 and contact layers, which are usually deployed within standard cells, are most critical and complex parts for modern digital designs. Traditional design flow that ignores TPL in early stages may limit the potential to resolve all the TPL conflicts. In this paper, we propose a coherent framework, including standard cell compliance and detailed placement to enable TPL friendly design. Considering TPL constraints during early design stages, such as standard cell compliance, improves the layout decomposability. With the pre-coloring solutions of standard cells, we present a TPL aware detailed placement, where the layout decomposition and placement can be resolved simultaneously. Our experimental results show that, with negligible impact on critical path delay, our framework can resolve the conflicts much more easily, compared with the traditional physical design flow and followed layout decomposition.", "venue": "2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Bei  Yu", "Xiaoqing  Xu", "Jhih-Rong  Gao", "David Z. Pan"], "year": 2013, "n_citations": 40}
{"id": 3547753, "s2_id": "54c945d17973276b8de3e75a3c12adc8b25adee6", "title": "Evaluating Built-In ECC of FPGA On-Chip Memories for the Mitigation of Undervolting Faults", "abstract": "Voltage underscaling below the nominal level is an effective solution for improving energy efficiency in digital circuits, e.g., Field Programmable Gate Arrays (FPGAs). However, further undervolting below a safe voltage level and without accompanying frequency scaling leads to timing related faults, potentially undermining the energy savings. Through experimental voltage underscaling studies on commercial FPGAs, we observed that the rate of these faults exponentially increases for on-chip memories, or Block RAMs (BRAMs). To mitigate these faults, we evaluated the efficiency of the built-in Error-Correction Code (ECC) and observed that more than 90 % of the faults are correctable and further 7 % are detectable (but not correctable). This efficiency is the result of the single-bit type of these faults, which are then effectively covered by the Single-Error Correction and Double-Error Detection (SECDED) design of the built-in ECC. Finally, motivated by the above experimental observations, we evaluated an FPGA-based Neural Network (NN) accelerator under low-voltage operations, while built-in ECC is leveraged to mitigate undervolting faults and thus, prevent NN significant accuracy loss. In consequence, we achieve 40 % of the BRAM power saving through undervolting below the minimum safe voltage level, with a negligible NN accuracy loss, thanks to the substantial fault coverage by the built-in ECC.", "venue": "2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)", "authors": ["Behzad  Salami", "Osman S. Unsal", "Adri\u00e1n  Cristal"], "year": 2019, "n_citations": 11}
{"id": 3549047, "s2_id": "2393d2b94fb4950c7c810064b7900761d700b382", "title": "DEW: A fast level 1 cache simulation approach for embedded processors with FIFO replacement policy", "abstract": "Increasing the speed of cache simulation to obtain hit/miss rates enables performance estimation, cache exploration for embedded systems and energy estimation. Previously, such simulations, particularly exact approaches, have been exclusively for caches which utilize the least recently used (LRU) replacement policy. In this paper, we propose a new, fast and exact cache simulation method for the First In First Out(FIFO) replacement policy. This method, called DEW, is able to simulate multiple level 1 cache configurations (different set sizes, associativities, and block sizes) with FIFO replacement policy. DEW utilizes a binomial tree based representation of cache configurations and a novel searching method to speed up simulation over single cache simulators like Dinero IV. Depending on different cache block sizes and benchmark applications, DEW operates around 8 to 40 times faster than Dinero IV. Dinero IV compares 2.17 to 19.42 times more cache ways than DEW to determine accurate miss rates.", "venue": "2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)", "authors": ["Mohammad Shihabul Haque", "Jorgen  Peddersen", "Andhi  Janapsatya", "Sri  Parameswaran"], "year": 2010, "n_citations": 20}
{"id": 3550165, "s2_id": "fe7ceb03b12c0dbd50290be632dacdccac72af77", "title": "Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization", "abstract": "This paper describes a novel approach of packing sparse convolutional neural networks into a denser format for efficient implementations using systolic arrays. By combining multiple sparse columns of a convolutional filter matrix into a single dense column stored in the systolic array, the utilization efficiency of the systolic array can be substantially increased (e.g., 8x) due to the increased density of nonzero weights in the resulting packed filter matrix. In combining columns, for each row, all filter weights but the one with the largest magnitude are pruned. The remaining weights are retrained to preserve high accuracy. We study the effectiveness of this joint optimization for both high utilization efficiency and classification accuracy with ASIC and FPGA designs based on efficient bit-serial implementations of multiplier-accumulators. We demonstrate that in mitigating data privacy concerns the retraining can be accomplished with only fractions of the original dataset (e.g., 10% for CIFAR-10). We present analysis and empirical evidence on the superior performance of our column combining approach against prior arts under metrics such as energy efficiency (3x) and inference latency (12x).", "venue": "ASPLOS", "authors": ["H. T. Kung", "Bradley  McDanel", "Sai Qian Zhang"], "year": 2019, "n_citations": 85}
{"id": 3554188, "s2_id": "31571efa7131c404985bfe5bfad7561814d00a2a", "title": "Functional Failure Rate Due to Single-Event Transients in Clock Distribution Networks", "abstract": "With technology scaling, lower supply voltages, and higher operating frequencies clock distribution networks become more and more vulnerable to transients faults. These faults can cause circuit-wide effects and thus, significantly contribute to the functional failure rate of the circuit. This paper proposes a methodology to analyse how the functional behaviour is affected by Single-Event Transients in the clock distribution network. The approach is based on logic-level simulation and thus, only uses the register-transfer level description of a design. Therefore, a fault model is proposed which implements the main effects due to radiation-induced transients in the clock network. This fault model enables the computation of the functional failure rate caused by Single-Event Transients for each individual clock buffer, as well as the complete network. Further, it allows the identification of the most vulnerable flip-flops related to SingleEvent Transients in the clock network.The proposed methodology is applied in a practical example and a fault injection campaign is performed. In order to evaluate the impact of Single-Event Transients in clock distribution networks, the obtained functional failure rate is compared to the error rate caused by Single-Event Upsets in the sequential logic.", "venue": "2019 14th International Conference on Design & Technology of Integrated Systems In Nanoscale Era (DTIS)", "authors": ["Thomas  Lange", "Maximilien  Glorieux", "Dan  Alexandrescu", "Luca  Sterpone"], "year": 2019, "n_citations": 3}
{"id": 3557001, "s2_id": "d78ba649e5bff580fd18148a831e214fc3647b3e", "title": "A Many-Core Overlay for High-Performance Embedded Computing on FPGAs", "abstract": "In this work, we propose a configurable many-core overlay for high-performance embedded computing. The size of internal memory, supported operations and number of ports can be configured independently for each core of the overlay. The overlay was evaluated with matrix multiplication, LU decomposition and Fast-Fourier Transform (FFT) on a ZYNQ-7020 FPGA platform. The results show that using a system-level many-core overlay avoids complex hardware design and still provides good performance results.", "venue": "ArXiv", "authors": ["M\u00e1rio P. V\u00e9stias", "Hor\u00e1cio C. Neto"], "year": 2014, "n_citations": 2}
{"id": 3557084, "s2_id": "ab6150200f93fb73f0a9c1606cfff7db3390b8bb", "title": "hlslib: Software Engineering for Hardware Design", "abstract": "High-level synthesis (HLS) tools have brought FPGA development into the mainstream, by allowing programmers to design architectures using familiar languages such as C, C++, and OpenCL. While the move to these languages has brought significant benefits, many aspects of traditional software engineering are still unsupported, or not exploited by developers in practice. Furthermore, designing reconfigurable architectures requires support for hardware constructs, such as FIFOs and shift registers, that are not native to CPU-oriented languages. To address this gap, we have developed hlslib, a collection of software tools, plug-in hardware modules, and code samples, designed to enhance the productivity of HLS developers. The goal of hlslib is two-fold: first, create a community-driven arena of bleeding edge development, which can move quicker, and provides more powerful abstractions than what is provided by vendors; and second, collect a wide range of example codes, both minimal proofs of concept, and larger, real-world applications, that can be reused directly or inspire other work. hlslib is offered as an open source library, containing CMake files, C++ headers, convenience scripts, and examples codes, and is receptive to any contribution that can benefit HLS developers, through general functionality or examples.", "venue": "ArXiv", "authors": ["Johannes de Fine Licht", "Torsten  Hoefler"], "year": 2019, "n_citations": 9}
{"id": 3560089, "s2_id": "d790bfdaf72ecfb38ce264d4dfdac0131d8b3b7c", "title": "A general scheme for noise-tolerant logic design based on probabilistic and DCVS approaches", "abstract": "The performance of logic function could be affected significantly by the noise effect as the dimension of CMOS devices scales to nanometers. Thus, many pertinent researches about noise-tolerant logic gate have received growing attention. Considering the randomness as the noise's nature, probabilistic-based approach proves better noise-immunity and three design schemes with the technique of Markov Random Field (MRF) have been proposed in [1]-[3]. In this paper, a general circuit scheme for noise-tolerant logic design based on MRF theory and Differential Cascode Voltage Switch (DCVS) technique has been proposed, which is an extension of the work in [3], [4]. A DCVS block with only four transistors has been successfully inserted to the original circuit scheme from [3] and extensive simulation results based on HSPICE show that our proposed design can operate correctly with the input signal of 1dB SNR. When using the Kullback-Leibler Distance (KLD) [5] as the evaluation parameter, the KLD value of our design decreases by 76.5% on average than [3] which means that superior noise-immunity could be obtained through our work.", "venue": "2015 IEEE 13th International New Circuits and Systems Conference (NEWCAS)", "authors": ["Xinghua  Yang", "Fei  Qiao", "Qi  Wei", "Huazhong  Yang"], "year": 2015, "n_citations": 5}
{"id": 3561992, "s2_id": "44198d4d08129857b04b9834b31684bb1178d832", "title": "Exploring the Mysteries of System-Level Test", "abstract": "System-level test, or SLT, is an increasingly important process step in today\u2019s integrated circuit testing flows. Broadly speaking, SLT aims at executing functional workloads in operational modes. In this paper, we consolidate available knowledge about what SLT is precisely and why it is used despite its considerable costs and complexities. We discuss the types or failures covered by SLT, and outline approaches to quality assessment, test generation and root-cause diagnosis in the context of SLT. Observing that the theoretical understanding for all these questions has not yet reached the level of maturity of the more conventional structural and functional test methods, we outline new and promising directions for methodical developments leveraging on recent findings from software engineering.", "venue": "2020 IEEE 29th Asian Test Symposium (ATS)", "authors": ["Ilia  Polian", "Jens  Anders", "Steffen  Becker", "Paolo  Bernardi", "Krishnendu  Chakrabarty", "Nourhan  Elhamawy", "Matthias  Sauer", "Adit D. Singh", "Matteo Sonza Reorda", "Stefan  Wagner"], "year": 2020, "n_citations": 1}
{"id": 3562509, "s2_id": "56d4438f287b943e6e8a77e62c0e55656ce56f3f", "title": "A Retrospective Recount of Computer Architecture Research with a Data-Driven Study of Over Four Decades of ISCA Publications", "abstract": "This study began with a research project, called DISCvR, conducted at the IBM-ILLINOIS Center for Cognitive Computing Systems Reseach. The goal of DISCvR was to build a practical NLP based AI pipeline for document understanding which will help us better understand the computation patterns and requirements of modern computing systems. While building such a prototype, an early use case came to us thanks to the 2017 IEEE/ACM International Symposium on Microarchitecture (MICRO-50) Program Co-chairs, Drs. Hillery Hunter and Jaime Moreno. They asked us if we can perform some data-driven analysis of the past 50 years of MICRO papers and show some interesting historical perspectives on MICRO's 50 years of publication. We learned two important lessons from that experience: (1) building an AI solution to truly understand unstructured data is hard in spite of the many claimed successes in natural language understanding; and (2) providing a data-driven perspective on computer architecture research is a very interesting and fun project. Recently we decided to conduct a more thorough study based on all past papers of International Symposium on Computer Architecture (ISCA) from 1973 to 2018, which resulted this article. We recognize that we have just scratched the surface of natural language understanding of unstructured data, and there are many more aspects that we can improve. But even with our current study, we felt there were enough interesting findings that may be worthwhile to share with the community. Hence we decided to write this article to summarize our findings so far based only on ISCA publications. Our hope is to generate further interests from the community in this topic, and we welcome collaboration from the community to deepen our understanding both of the computer architecture research and of the challenges of NLP-based AI solutions.", "venue": "ArXiv", "authors": ["Omer  Anjum", "Wen-Mei  Hwu", "Jinjun  Xiong"], "year": 2019, "n_citations": 0}
{"id": 3566228, "s2_id": "814e538b8c3505553c8840cc5a201a6d5a1b0ada", "title": "The Cost of Application-Class Processing: Energy and Performance Analysis of a Linux-Ready 1.7-GHz 64-Bit RISC-V Core in 22-nm FDSOI Technology", "abstract": "The open-source RISC-V instruction set architecture (ISA) is gaining traction, both in industry and academia. The ISA is designed to scale from microcontrollers to server-class processors. Furthermore, openness promotes the availability of various open-source and commercial implementations. Our main contribution in this paper is a thorough power, performance, and efficiency analysis of the RISC-V ISA targeting baseline \u201capplication class\u201d functionality, i.e., supporting the Linux OS and its application environment based on our open-source single-issue in-order implementation of the 64-bit ISA variant (RV64GC) called Ariane. Our analysis is based on a detailed power and efficiency analysis of the RISC-V ISA extracted from silicon measurements and calibrated simulation of an Ariane instance (RV64IMC) taped-out in GlobalFoundries 22FDX technology. Ariane runs at up to 1.7-GHz, achieves up to 40-Gop/sW energy efficiency, which is superior to similar cores presented in the literature. We provide insight into the interplay between functionality required for the application-class execution (e.g., virtual memory, caches, and multiple modes of privileged operation) and energy cost. We also compare Ariane with RISCY, a simpler and a slower microcontroller-class core. Our analysis confirms that supporting application-class execution implies a nonnegligible energy-efficiency loss and that compute performance is more cost-effectively boosted by instruction extensions (e.g., packed SIMD) rather than the high-frequency operation.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Florian  Zaruba", "Luca  Benini"], "year": 2019, "n_citations": 75}
{"id": 3583832, "s2_id": "58fd250188a81f470595a3fac920d1fb46679a21", "title": "SideLine: How Delay-Lines (May) Leak Secrets from your SoC", "abstract": "To meet the ever-growing need for performance in silicon devices, SoC providers have been increasingly relying on software-hardware cooperation. By controlling hardware resources such as power or clock management from the software, developers earn the possibility to build more flexible and power efficient applications. Despite the benefits, these hardware components are now exposed to software code and can potentially be misused as open-doors to jeopardize trusted environments, perform privilege escalation or steal cryptographic secrets. In this work, we introduce SideLine, a novel side-channel vector based on delay-line components widely implemented in high-end SoCs. After providing a detailed method on how to access and convert delay-line data into power consumption information, we demonstrate that these entities can be used to perform remote power side-channel attacks. We report experiments carried out on two SoCs from distinct vendors and we recount several core-vs-core attack scenarios in which an adversary process located in one processor core aims at eavesdropping the activity of a victim process located in another core. For each scenario, we demonstrate the adversary ability to fully recover the secret key of an OpenSSL AES running in the victim core. Even more detrimental, we show that these attacks are still practicable if the victim or the attacker program runs over an operating system.", "venue": "IACR Cryptol. ePrint Arch.", "authors": ["Joseph  Gravellier", "Jean-Max  Dutertre", "Yannick  Teglia", "Philippe  Loubet-Moundi"], "year": 2020, "n_citations": 2}
{"id": 3584952, "s2_id": "381bec98f037d4c6d46d887a6930c56e5e78c5e7", "title": "Noisy Machines: Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation", "abstract": "The success of deep learning has brought forth a wave of interest in computer hardware design to better meet the high demands of neural network inference. In particular, analog computing hardware has been heavily motivated specifically for accelerating neural networks, based on either electronic, optical or photonic devices, which may well achieve lower power consumption than conventional digital electronics. However, these proposed analog accelerators suffer from the intrinsic noise generated by their physical components, which makes it challenging to achieve high accuracy on deep neural networks. Hence, for successful deployment on analog accelerators, it is essential to be able to train deep neural networks to be robust to random continuous noise in the network weights, which is a somewhat new challenge in machine learning. In this paper, we advance the understanding of noisy neural networks. We outline how a noisy neural network has reduced learning capacity as a result of loss of mutual information between its input and output. To combat this, we propose using knowledge distillation combined with noise injection during training to achieve more noise robust networks, which is demonstrated experimentally across different networks and datasets, including ImageNet. Our method achieves models with as much as two times greater noise tolerance compared with the previous best attempts, which is a significant step towards making analog hardware practical for deep learning.", "venue": "ArXiv", "authors": ["Chuteng  Zhou", "Prad  Kadambi", "Matthew  Mattina", "Paul N. Whatmough"], "year": 2020, "n_citations": 6}
{"id": 3589693, "s2_id": "615769c86118b327a65bb4e3eab88b99b2c22b66", "title": "Verification and Design Methods for the BrainScaleS Neuromorphic Hardware System", "abstract": "This paper presents verification and implementation methods that have been developed for the design of the BrainScaleS-2 65nm ASICs. The 2nd generation BrainScaleS chips are mixed-signal devices with tight coupling between full-custom analog neuromorphic circuits and two general purpose microprocessors (PPU) with SIMD extension for on-chip learning and plasticity. Simulation methods for automated analysis and pre-tapeout calibration of the highly parameterizable analog neuron and synapse circuits and for hardware-software co-development of the digital logic and software stack are presented. Accelerated operation of neuromorphic circuits and highly-parallel digital data buses between the full-custom neuromorphic part and the PPU require custom methodologies to close the digital signal timing at the interfaces. Novel extensions to the standard digital physical implementation design flow are highlighted. We present early results from the first full-size BrainScaleS-2 ASIC containing 512 neurons and 130K synapses, demonstrating the successful application of these methods. An application example illustrates the full functionality of the BrainScaleS-2 hybrid plasticity architecture.", "venue": "J. Signal Process. Syst.", "authors": ["Andreas  Gr\u00fcbl", "Sebastian  Billaudelle", "Benjamin  Cramer", "Vitali  Karasenko", "Johannes  Schemmel"], "year": 2020, "n_citations": 10}
{"id": 3593357, "s2_id": "f3c7c272538e0ab2a09f91fa4929f845708d8e0f", "title": "Efficient emotion recognition using hyperdimensional computing with combinatorial channel encoding and cellular automata", "abstract": "In this paper, a hardware-optimized approach to emotion recognition based on the efficient brain-inspired hyperdimensional computing (HDC) paradigm is proposed. Emotion recognition provides valuable information for human-computer interactions, however the large number of input channels (>200) and modalities (>3) involved in emotion recognition are significantly expensive from a memory perspective. To address this, methods for memory reduction and optimization are proposed, including a novel approach that takes advantage of the combinatorial nature of the encoding process, and an elementary cellular automaton. HDC with early sensor fusion is implemented alongside the proposed techniques achieving two-class multi-modal classification accuracies of >76% for valence and >73% for arousal on the multi-modal AMIGOS and DEAP datasets, almost always better than state of the art. The required vector storage is seamlessly reduced by 98% and the frequency of vector requests by at least 1/5. The results demonstrate the potential of efficient hyperdimensional computing for low-power, multi-channeled emotion recognition tasks.", "venue": "ArXiv", "authors": ["Alisha  Menon", "Anirudh  Natarajan", "Reva  Agashe", "Daniel  Sun", "Melvin  Aristio", "Harrison  Liew", "Yakun Sophia Shao", "Jan M. Rabaey"], "year": 2021, "n_citations": 0}
{"id": 3593818, "s2_id": "e3eb3af7aea4b1306b3f59f458ca0bcb44b4de65", "title": "How to extend the Single-Processor Paradigm to the Explicitly Many-Processor Approach", "abstract": "The computing paradigm invented for processing a small amount of data on a single segregated processor cannot meet the challenges set by the present-day computing demands. The paper proposes a new computing paradigm (extending the old one to use several processors explicitly) and discusses some questions of its possible implementation. Some advantages of the implemented approach, illustrated with the results of a loosely-timed simulator, are presented.", "venue": "ArXiv", "authors": ["J'anos  V'egh"], "year": 2020, "n_citations": 4}
{"id": 3596570, "s2_id": "c71f9c29cacb39e72cf90997800b20593735df00", "title": "SoC software components diagnosis technology", "abstract": "A novel approach to evaluation of hardware and software testability, represented in the form of register transfer graph, is proposed. Instances of making of software graph models for their subsequent testing and diagnosis are shown.", "venue": "Proceedings of IEEE East-West Design & Test Symposium (EWDTS'08)", "authors": ["Svetlana  Chumachenko", "Wajeb  Gharibi", "Anna  Hahanova", "Aleksey  Sushanov"], "year": 2008, "n_citations": 0}
{"id": 3598105, "s2_id": "183aa23e0f2f54a25b27324c90160158edb7c759", "title": "Robust and Attack Resilient Logic Locking with a High Application-Level Impact", "abstract": "\n Logic locking is a hardware security technique aimed at protecting intellectual property against security threats in the IC supply chain, especially those posed by untrusted fabrication facilities. Such techniques incorporate additional locking circuitry within an integrated circuit (IC) that induces incorrect digital functionality when an incorrect verification key is provided by a user. The amount of error induced by an incorrect key is known as the\n effectiveness\n of the locking technique. A family of attacks known as \u201cSAT attacks\u201d provide a strong mathematical formulation to find the correct key of locked circuits. To achieve high\n SAT resilience\n (i.e., complexity of SAT attacks), many conventional logic locking schemes fail to inject sufficient error into the circuit when the key is incorrect. For example, in the case of SARLock and Anti-SAT, there are usually very few (or only one) input minterms that cause any error at the circuit output. The state-of-the-art\n s\n tripped functionality logic locking (SFLL) technique provides a wide spectrum of configurations that introduced a tradeoff between SAT resilience and effectiveness. In this work, we prove that such a tradeoff is universal among all logic locking techniques. To attain high effectiveness of locking without compromising SAT resilience, we propose a novel logic locking scheme, called Strong Anti-SAT (SAS). In addition to SAT attacks, removal-based attacks are another popular kind of attack formulation against logic locking where the attacker tries to identify and remove the locking structure. Based on SAS, we also propose Robust SAS (RSAS) that is resilient to removal attacks and maintains the same\n SAT resilience\n and\n effectiveness\n as SAS. SAS and RSAS have the following significant improvements over existing techniques. (1) We prove that the\n SAT resilience\n of SAS and RSAS against SAT attack is not compromised by increase in\n effectiveness\n . (2) In contrast to prior work that focused solely on the circuit-level locking impact, we integrate SAS-locked modules into an 80386 processor and show that SAS has a high application-level impact. (3) Our experiments show that SAS and RSAS exhibit better SAT resilience than SFLL and their effectiveness is similar to SFLL.\n", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Yuntao  Liu", "Michael  Zuzak", "Yang  Xie", "Abhishek  Chakraborty", "Ankur  Srivastava"], "year": 2021, "n_citations": 1}
{"id": 3600563, "s2_id": "501df50abe8e2200b4693bb5872fb5b022d2e4fa", "title": "Analytical models of Energy and Throughput for Caches in MPSoCs", "abstract": "General trends in computer architecture are shifting more towards parallelism. Multicore architectures have proven to be a major step in processor evolution. With the advancement in multicore architecture, researchers are focusing on finding different solutions to fully utilize the power of multiple cores. With an ever-increasing number of cores on a chip, the role of cache memory has become pivotal. An ideal memory configuration should be both large and fast, however, in fact, system architects have to strike a balance between the size and access time of the memory hierarchy. It is important to know the impact of a particular cache configuration on the throughput and energy consumption of the system at design time. This paper presents an enhanced version of previously proposed cache energy and throughput models for multicore systems. These models use significantly a smaller number of input parameters as compared to other models. This paper also validates the proposed models through cycle accurate simulator and a renowned processor power estimator. The results show that the proposed energy models provide accuracy within a maximum error range of 10% for single-core processors and around 5% for MPSoCs, and the throughput models result in a maximum error of up to 11.5% for both single and multicore architectures.", "venue": "ArXiv", "authors": ["Arsalan  Shahid", "Muhammad  Tayyab", "Muhammad Yasir Qadri", "Nadia N. Qadri", "Jameel  Ahmed"], "year": 2019, "n_citations": 0}
{"id": 3601022, "s2_id": "82f6fcafc2cd1d2e876023f9be2c7006724c45a8", "title": "Monarch: A Durable Polymorphic Memory For Data Intensive Applications", "abstract": "3D die stacking has often been proposed to build large-scale DRAM-based caches. Unfortunately, the power and performance overheads of DRAM limit the efficiency of high-bandwidth memories. Also, DRAM is facing serious scalability challenges that make alternative technologies more appealing. This paper examines Monarch, a resistive 3D stacked memory based on a novel reconfigurable crosspoint array called XAM. The XAM array is capable of switching between random access and content-addressable modes, which enables Monarch (i) to better utilize the in-package bandwidth and (ii) to satisfy both the random access memory and associative search requirements of various applications. Moreover, the Monarch controller ensures a given target lifetime for the resistive stack. Our simulation results on a set of parallel memory-intensive applications indicate that Monarch outperforms an ideal DRAM caching by 1.21\u00d7 on average. For in-memory hash table and string matching workloads, Monarch improves performance up to 12\u00d7 over the conventional high bandwidth memories.", "venue": "ArXiv", "authors": ["Ananth Krishna Prasad", "Mahdi Nazm Bojnordi"], "year": 2021, "n_citations": 0}
{"id": 3602815, "s2_id": "d79366509f04b9188837657d586fbf50b032dd22", "title": "Techniques for Shared Resource Management in Systems with Throughput Processors", "abstract": "The continued growth of the computational capability of throughput processors has made throughput processors the platform of choice for a wide variety of high performance computing applications. Graphics Processing Units (GPUs) are a prime example of throughput processors that can deliver high performance for applications ranging from typical graphics applications to general-purpose data parallel (GPGPU) applications. However, this success has been accompanied by new performance bottlenecks throughout the memory hierarchy of GPU-based systems. We identify and eliminate performance bottlenecks caused by major sources of interference throughout the memory hierarchy. \nWe introduce changes to the memory hierarchy for systems with GPUs that allow the memory hierarchy to be aware of both CPU and GPU applications' characteristics. We introduce mechanisms to dynamically analyze different applications' characteristics and propose four major changes throughout the memory hierarchy. We propose changes to the cache management and memory scheduling mechanisms to mitigate intra-application interference in GPGPU applications. We propose changes to the memory controller design and its scheduling policy to mitigate inter-application interference in heterogeneous CPU-GPU systems. We redesign the MMU and the memory hierarchy in GPUs to be aware of ddress-translation data in order to mitigate the inter-address-space interference. We introduce a hardware-software cooperative technique that modifies the memory allocation policy to enable large page support in order to further reduce the inter-address-space interference at the shared Translation Lookaside Buffer (TLB). Our evaluations show that the GPU-aware cache and memory management techniques proposed in this dissertation are effective at mitigating the interference caused by GPUs on current and future GPU-based systems.", "venue": "ArXiv", "authors": ["Rachata  Ausavarungnirun"], "year": 2018, "n_citations": 9}
{"id": 3604695, "s2_id": "400891424d0fb0e7165c23054ccd771ed7249d64", "title": "AdderNet and its Minimalist Hardware Design for Energy-Efficient Artificial Intelligence", "abstract": "Convolutional neural networks (CNN) have been widely used for boosting the performance of many machine intelligence tasks. However, the CNN models are usually computationally intensive and energy consuming, since they are often designed with numerous multiply-operations and considerable parameters for the accuracy reason. Thus, it is difficult to directly apply them in the resource-constrained environments such as \u2019Internet of Things\u2019 (IoT) devices and smart phones. To reduce the computational complexity and energy burden, here we present a novel minimalist hardware architecture using adder convolutional neural network (AdderNet), in which the original convolution is replaced by adder kernel using only additions. To maximally excavate the potential energy consumption, we explore the low-bit quantization algorithm for AdderNet with shared-scalingfactor method, and we design both specific and general-purpose hardware accelerators for AdderNet. Experimental results Chunjing Xu and Dacheng Tao supervised the project. Yunhe Wang and Mingqiang Huang proposed and designed the experiment, Kai Han, Hanting Chen and Yunhe Wang performed the AdderNet algorithm verification. Mingqiang Huang and Wei Zhang performed the hardware design. Mingqiang Huang, Yunhe Wang, and Kai Han analyzed the data and co-wrote the manuscript. All the authors discussed the results and commented on the manuscript. \u2021These authors contributed equally: Yunhe Wang, Mingqiang Huang. *E-mail: dacheng.tao@sydney.edu.au; yunhe.wang@huawei.com; mq.huang2@siat.ac.cn Yunhe Wang, Kai Han, Hanting Chen, Chunjing Xu Noah\u2019s Ark Lab, Huawei Technologies. Mingqiang Huang, Wei Zhang Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen 518055, China. Dacheng Tao School of Computer Science, Faculty of Engineering, the University of Sydney. show that the adder kernel with int8/int16 quantization also exhibits high performance, meanwhile consuming much less resources (theoretically \u223c81% off). In addition, we deploy the quantized AdderNet on FPGA (Field Programmable Gate Array) platform. The whole AdderNet can practically achieve 16% enhancement in speed, 67.6%-71.4% decrease in logic resource utilization and 47.85%-77.9% decrease in power consumption compared to CNN under the same circuit architecture. With a comprehensive comparison on the performance, power consumption, hardware resource consumption and network generalization capability, we conclude the AdderNet is able to surpass all the other competitors including the classical CNN, novel memristor-network, XNORNet and the shift-kernel based network, indicating its great potential in future high performance and energy-efficient artificial intelligence applications.", "venue": "ArXiv", "authors": ["Yunhe  Wang", "Mingqiang  Huang", "Kai  Han", "Hanting  Chen", "Wei  Zhang", "Chunjing  Xu", "Dacheng  Tao"], "year": 2021, "n_citations": 4}
{"id": 3606242, "s2_id": "06bcea7d11ea3456b534523a5bc3616f715e0d30", "title": "Evaluation of the Performance/Energy Overhead in DSP Video Decoding and its Implications", "abstract": "Video decoding is considered as one of the most compute and energy intensive application in energy constrained mobile devices. Some specific processing units, such as DSPs, are added to those devices in order to optimize the performance and the energy consumption. However, in DSP video decoding, the inter-processor communication overhead may have a considerable impact on the performance and the energy consumption. In this paper, we propose to evaluate this overhead and analyse its impact on the performance and the energy consumption as compared to the GPP decoding. Our work revealed that the GPP can be the best choice in many cases due to the a significant overhead in DSP decoding which may represents 30% of the total decoding energy.", "venue": "ArXiv", "authors": ["Yahia  Benmoussa", "Jalil  Boukhobza", "Eric  Senn", "Djamel  Benazzouz"], "year": 2013, "n_citations": 2}
{"id": 3609992, "s2_id": "7cbca99c85627b48b5c283bff56d83e28917c309", "title": "Direct CMOS Implementation of Neuromorphic Temporal Neural Networks for Sensory Processing", "abstract": "Temporal Neural Networks (TNNs) use time as a resource to represent and process information, mimicking the behavior of the mammalian neocortex. This work focuses on implementing TNNs using off-the-shelf digital CMOS technology. A microarchitecture framework is introduced with a hierarchy of building blocks including: multi-neuron columns, multi-column layers, and multi-layer TNNs. We present the direct CMOS gate-level implementation of the multi-neuron column model as the key building block for TNNs. Post-synthesis results are obtained using Synopsys tools and the 45 nm CMOS standard cell library. The TNN microarchitecture framework is embodied in a set of characteristic equations for assessing the total gate count, die area, compute time, and power consumption for any TNN design. We develop a multi-layer TNN prototype of 32M gates. In 7 nm CMOS process, it consumes only 1.54 mm^2 die area and 7.26 mW power and can process 28x28 images at 107M FPS (9.34 ns per image). We evaluate the prototype's performance and complexity relative to a recent state-of-the-art TNN model.", "venue": "ArXiv", "authors": ["Harideep  Nair", "John Paul Shen", "James E. Smith"], "year": 2020, "n_citations": 4}
{"id": 3615573, "s2_id": "c2e4ebcba82be474b7f9201cb534e5b1b76d4123", "title": "Spatially Coupled PLDPC-Hadamard Convolutional Codes", "abstract": "In this paper, we propose a new type of ultimate-Shannon-limit-approaching codes called spatially coupled protograph-based low-density parity-check Hadamard convolutional codes (SC-PLDPCH-CCs), which are constructed by spatially coupling PLDPC-Hadamard block codes. We also develop an efficient decoding algorithm that combines pipeline decoding and layered scheduling for the decoding of SCPLDPCH-CCs. To estimate the decoding thresholds of SC-PLDPCH-CCs, we first propose a layered protograph extrinsic information transfer (PEXIT) algorithm to evaluate the thresholds of spatially coupled PLDPC-Hadamard terminated codes (SC-PLDPCH-TDCs) with a moderate coupling length. With the use of the proposed layered PEXIT method, we develop a genetic algorithm to look for good SC-PLDPCH-TDCs in a systematic way. Subsequently, we extend the coupling length of these SCPLDPCH-TDCs with good thresholds to form good SC-PLDPCH-CCs. Based on the same set of split protomatrices, we regard the threshold of SC-PLDPCH-TDC as the proxy of SC-PLDPCH-CC when the SC-PLDPCH-TDC with long coupling length has almost the same code rate as the SC-PLDPCH-CC. Results show that our optimized SC-PLDPCH-CCs can achieve comparable thresholds to the block code counterparts. Simulations also illustrate the superiority of the SC-PLDPCH-CCs over the block code counterparts in terms of error performance. Moreover, for the rate-0.00295 SC-PLDPCH-CC, a BER of 10 is achieved at Eb/N0 = \u22121.45 dB, which is only 0.14 dB from the ultimate Shannon limit.", "venue": "ArXiv", "authors": ["Peng W. Zhang", "Francis C.M. Lau", "Chiu-W.  Sham"], "year": 2021, "n_citations": 0}
{"id": 3616025, "s2_id": "7ea0a83213b47d03c19702821785fd08373193b4", "title": "FPnew: An Open-Source Multiformat Floating-Point Unit Architecture for Energy-Proportional Transprecision Computing", "abstract": "The slowdown of Moore\u2019s law and the power wall necessitates a shift toward finely tunable precision (a.k.a. transprecision) computing to reduce energy footprint. Hence, we need circuits capable of performing floating-point operations on a wide range of precisions with high energy proportionality. We present FPnew, a highly configurable open-source transprecision floating-point unit (TP-FPU), capable of supporting a wide range of standard and custom FP formats. To demonstrate the flexibility and efficiency of FPnew in general-purpose processor architectures, we extend the RISC-V ISA with operations on half-precision, bfloat16, and an 8-bit FP format, as well as SIMD vectors and multiformat operations. Integrated into a 32-bit RISC-V core, our TP-FPU can speedup the execution of mixed-precision applications by $1.67\\times $ with respect to an FP32 baseline, while maintaining end-to-end precision and reducing system energy by 37%. We also integrate FPnew into a 64-bit RISC-V core, supporting five FP formats on scalars or 2, 4, or 8-way SIMD vectors. For this core, we measured the silicon manufactured in Globalfoundries 22FDX technology across a wide voltage range from 0.45 to 1.2 V. The unit achieves leading-edge measured energy efficiencies between 178 Gflop/sW (on FP64) and 2.95 Tflop/sW (on 8-bit mini-floats), and a performance between 3.2 and 25.3 Gflop/s.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Stefan  Mach", "Fabian  Schuiki", "Florian  Zaruba", "Luca  Benini"], "year": 2021, "n_citations": 9}
{"id": 3617438, "s2_id": "53d5cf17dfe45b02a98635151974c19ad4fa0a07", "title": "Memory efficient Multi-Scale Line Detector architecture for retinal blood vessel segmentation", "abstract": "This paper presents a memory efficient architecture that implements the Multi-Scale Line Detector (MSLD) algorithm for real-time retinal blood vessel detection in fundus images on a Zynq FPGA. This implementation benefits from the FPGA parallelism to drastically reduce the memory requirements of the MSLD from two images to a few values. The architecture is optimized in terms of resource utilization by reusing the computations and optimizing the bit-width. The throughput is increased by designing fully pipelined functional units. The architecture is capable of achieving a comparable accuracy to its software implementation but 70\u00d7 faster for low resolution images. For high resolution images, it achieves an acceleration by a factor of 323\u00d7.", "venue": "2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)", "authors": ["Hamza  Bendaoudi", "Farida  Cheriet", "J. M. Pierre Langlois"], "year": 2016, "n_citations": 2}
{"id": 3618009, "s2_id": "253b700b3535e95ab37e86c3ebe763eabe1b898f", "title": "Combining Emulation and Simulation to Evaluate a Near Memory Key/Value Lookup Accelerator", "abstract": "Processing large numbers of key/value lookups is an integral part of modern server databases and other\"Big Data\"applications. Prior work has shown that hash table based key/value lookups can benefit significantly from using a dedicated hardware lookup accelerator placed near memory. However, previous evaluations of this design on the Logic in Memory Emulator (LiME) were limited by the capabilities of the hardware on which it was emulated, which only supports a single CPU core and a single near-memory lookup engine. We extend the emulation results by incorporating simulation to evaluate this design in additional scenarios. By incorporating an HMC simulation model, we design optimizations that better mitigate the effects of the HMC closed page policy and that better utilize the HMC's parallelism, improving predicted performance by an order of magnitude. Additionally, we use simulation to evaluate the scaling performance of multiple near-memory lookup accelerators. Our work employs an open source emulator LiME, open source simulatation infrastructure SST, and the open source HMC-Sim simulator.", "venue": "ArXiv", "authors": ["Joshua  Landgraf", "Scott  Lloyd", "Maya  Gokhale"], "year": 2021, "n_citations": 1}
{"id": 3619064, "s2_id": "9f6fb0a87212e2a7bd45935b11337c7b447fb1e2", "title": "Synthesizing Power and Area Efficient Image Processing Pipelines on FPGAs using Customized Bit-widths", "abstract": "High-level synthesis (HLS) has received significant attention in recent years, improving programmability for FPGAs. PolyMage is a domain-specific language (DSL) for image processing pipelines that also has a HLS backend to translate the input DSL into an equivalent circuit that can be synthesized on FPGAs, while leveraging an HLS suite. The data at each stage of a pipeline is stored using a fixed-point data type (alpha,beta) where alpha and beta denote the number of integral and fractional bits. The power and area savings while performing arithmetic operations on fixed-point data type is known to be significant over using floating point. In this paper, we first propose an interval-arithmetic based range analysis (alpha-analysis) algorithm to estimate the number of bits required to store the integral part of the data at each stage of an image processing pipeline. The analysis algorithm uses the homogeneity of pixel signals at each stage to cluster them and perform a combined range analysis. Secondly, we propose a software architecture for easily deploying any kind of interval/affine arithmetic based range analyses in the DSL compiler. Thirdly, we propose a new range analysis technique using Satisfiability Modulo Theory (SMT) solvers, and show that the range estimates obtained through it are very close to the lower bounds obtained through profile-driven analysis.We evaluated our bitwidth analysis algorithms on four image processing benchmarks listed in the order of increasing complexity: Unsharp Mask, Down-Up Sampling, Harris Corner Detection and Horn-Schunck Optical Flow. For example, on Optical Flow, the interval analysis based approach showed an 1.4x and 1.14x improvement on area and power metrics over floating-point representation respectively; whereas the SMT solver based approach showed 2.49x and 1.58x improvement on area and power metrics when compared to interval analysis.", "venue": "ArXiv", "authors": ["Vinamra  Benara", "Sahithi  Rampalli", "Ziaul  Choudhury", "Suresh  Purini", "Uday  Bondhugula"], "year": 2018, "n_citations": 0}
{"id": 3620673, "s2_id": "c3efddfc37001b8ccc80c5feda97e6265e0f0d47", "title": "Multi-standard programmable baseband modulator for next generation wireless communication", "abstract": "Considerable research has taken place in recent times in the area of parameterization of software defined radio (SDR) architecture. Parameterization decreases the size of the software to be downloaded and also limits the hardware reconfiguration time. The present paper is based on the design and development of a programmable baseband modulator that perform the QPSK modulation schemes and as well as its other three commonly used variants to satisfy the requirement of several established 2G and 3G wireless communication standards. The proposed design has been shown to be capable of operating at a maximum data rate of 77 Mbps on Xilinx Virtex 2-Pro University field programmable gate array (FPGA) board. The pulse shaping root raised cosine (RRC) filter has been implemented using distributed arithmetic (DA) technique in the present work in order to reduce the computational complexity, and to achieve appropriate power reduction and enhanced throughput. The designed multiplier-less programmable 32tap FIR-based RRC filter has been found to withstand a peak inter-symbol interference (ISI) distortion of -41 dBs.", "venue": "ArXiv", "authors": ["Indranil  Hatai", "Indrajit  Chakrabarti"], "year": 2010, "n_citations": 2}
{"id": 3627296, "s2_id": "6537e4420cdc7d3ee11c7a305e8694f2999a9d62", "title": "A Novel Methodology for Thermal Aware Silicon Area Estimation for 2D & 3D MPSoCs", "abstract": "In a multiprocessor system on chip (MPSoC) IC the processor is one of the highest heat dissipating devices. The temperature generated in an IC may vary with floor plan of the chip. This paper proposes an integration and thermal analysis methodology to extract the peak temperature and temperature distribution of 2-dimensional and 3-dimensional multiprocessor system-on-chip. As we know the peak temperature of chip increases in 3-dimensional structures compared to 2-dimensional ones due to the reduced space in intra-layer and inter-layer components. In sub-nanometre scale technologies, it is inevitable to analysis the heat developed in individual chip to extract the temperature distribution of the entire chip. With the technology scaling in new generation ICs more and more components are integrated to a smaller area. Along with the other parameters threshold voltage is also scaled down which results in exponential increase in leakage current. This has resulted in rise in hotspot temperature value due to increase in leakage power. In this paper, we have analysed the temperature developed in an IC with four identical processors at 2.4 GHz in different floorplans. The analysis has been done for both 2D and 3D arrangements. In the 3D arrangement, a three layered structure has been considered with two Silicon layers and a thermal interface material (TIM) in between them. Based on experimental results the paper proposes a methodology to reduce the peak temperature developed in 2D and 3D integrated circuits .", "venue": "VLSIC 2011", "authors": ["C.  RamyaMenon", "Vinod  Pangracious"], "year": 2011, "n_citations": 5}
{"id": 3628123, "s2_id": "d5abc12475a8287fb496939e27cf88a5ebd69902", "title": "EXMA: A Genomics Accelerator for Exact-Matching", "abstract": "Genomics is the foundation of precision medicine, global food security and virus surveillance. Exact-match is one of the most essential operations widely used in almost every step of genomics such as alignment, assembly, annotation, and compression. Modern genomics adopts Ferragina-Manzini Index (FMIndex) augmenting space-efficient Burrows-Wheeler transform (BWT) with additional data structures to permit ultra-fast exact-match operations. However, FM-Index is notorious for its poor spatial locality and random memory access pattern. Prior works create GPU-, FPGA-, ASIC- and even process-in-memory (PIM)based accelerators to boost FM-Index search throughput. Though they achieve the state-of-the-art FM-Index search throughput, the same as all prior conventional accelerators, FM-Index PIMs process only one DNA symbol after each DRAM row activation, thereby suffering from poor memory bandwidth utilization. In this paper, we propose a hardware accelerator, EXMA, to enhance FM-Index search throughput. We first create a novel EXMA table with a multi-task-learning (MTL)-based index to process multiple DNA symbols with each DRAM row activation. We then build an accelerator to search over an EXMA table. We propose 2-stage scheduling to increase the cache hit rate of our accelerator. We introduce dynamic page policy to improve the row buffer hit rate of DRAM main memory. We also present CHAIN compression to reduce the data structure size of EXMA tables. Compared to state-of-the-art FM-Index PIMs, EXMA improves search throughput by $4.9 \\times$, and enhances search throughput per Watt by $4.8 \\times$.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Lei  Jiang", "Farzaneh  Zokaee"], "year": 2021, "n_citations": 0}
{"id": 3628742, "s2_id": "2ee6bdf2c1a7a3fd7cf8b820067ddce36d334cc9", "title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular Loads", "abstract": "The growing memory footprints of cloud and big data applications mean that data center CPUs can spend significant time waiting for memory. An attractive approach to improving performance in such centralized compute settings is to employ prefetchers that are customized per application, where gains can be easily scaled across thousands of machines. Helper thread prefetching is such a technique but has yet to achieve wide adoption since it requires spare thread contexts or special hardware/firmware support. In this paper, we propose an inline software prefetching technique that overcomes these restrictions by inserting the helper code into the main thread itself. Our approach is complementary to and does not interfere with existing hardware prefetchers since we target only delinquent irregular load instructions (those with no constant or striding address patterns). For each chosen load instruction, we generate and insert a customized software prefetcher extracted from and mimicking the application's dataflow, all without access to the application source code. For a set of irregular workloads that are memory-bound, we demonstrate up to 2X single-thread performance improvement on recent high-end hardware (Intel Skylake) and up to 83% speedup over a helper thread implementation on the same hardware, due to the absence of thread spawning overhead.", "venue": "ArXiv", "authors": ["Karthik  Sankaranarayanan", "Chit-Kwan  Lin", "Gautham  Chinya"], "year": 2020, "n_citations": 0}
{"id": 3634492, "s2_id": "9817bf0f78047452761e950c02a1a56f59a1e593", "title": "SynCron: Efficient Synchronization Support for Near-Data-Processing Architectures", "abstract": "Near-Data-Processing (NDP) architectures present a promising way to alleviate data movement costs and can provide significant performance and energy benefits to parallel applications. Typically, NDP architectures support several NDP units, each including multiple simple cores placed close to memory. To fully leverage the benefits of NDP and achieve high performance for parallel workloads, efficient synchronization among the NDP cores of a system is necessary. However, supporting synchronization in many NDP systems is challenging because they lack shared caches and hardware cache coherence support, which are commonly used for synchronization in multicore systems, and communication across different NDP units can be expensive.This paper comprehensively examines the synchronization problem in NDP systems, and proposes SynCron, an end-to-end synchronization solution for NDP systems. SynCron adds low-cost hardware support near memory for synchronization acceleration, and avoids the need for hardware cache coherence support. SynCron has three components: 1) a specialized cache memory structure to avoid memory accesses for synchronization and minimize latency overheads, 2) a hierarchical message-passing communication protocol to minimize expensive communication across NDP units of the system, and 3) a hardware-only overflow management scheme to avoid performance degradation when hardware resources for synchronization tracking are exceeded.We evaluate SynCron using a variety of parallel workloads, covering various contention scenarios. SynCron improves performance by $1.27\\times$ on average (up to $1.78\\times$) under high-contention scenarios, and by $ 1.35\\times$ on average (up to $2.29\\times)$ under low-contention real applications, compared to state-of-the-art approaches. SynCron reduces system energy consumption by $ 2.08\\times$ on average (up to $4.25\\times$).", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Christina  Giannoula", "Nandita  Vijaykumar", "Nikela  Papadopoulou", "Vasileios  Karakostas", "Ivan  Fernandez", "Juan  G'omez-Luna", "Lois  Orosa", "Nectarios  Koziris", "Georgios  Goumas", "Onur  Mutlu"], "year": 2021, "n_citations": 11}
{"id": 3638540, "s2_id": "bbee268185bd591f55a6e1c043ab2c460d6164a7", "title": "Optimizing Temporal Convolutional Network Inference on FPGA-Based Accelerators", "abstract": "Convolutional Neural Networks (CNNs) are extensively used in a wide range of applications, commonly including computer vision tasks like image and video classification, recognition and segmentation. Recent research results demonstrate that multi-layer (deep) network involving mono-dimensional convolutions and dilation can be effectively used in time series and sequences classification and segmentation, as well as in tasks involving sequence modeling. These structures, commonly referred to as Temporal Convolutional Networks (TCNs), represent an extremely promising alternative to recurrent architectures, commonly used across a broad range of sequence modeling tasks. While FPGA based inference accelerators for classic CNNs are widespread, literature is lacking in a quantitative evaluation of their usability on inference for TCN models. In this paper we present such an evaluation, considering a CNN accelerator with specific features supporting TCN kernels as a reference and a set of state-of-the-art TCNs as a benchmark. Experimental results show that, during TCN execution, operational intensity can be critical for the overall performance. We propose a convolution scheduling based on batch processing that can boost efficiency up to 96% of theoretical peak performance. Overall we can achieve up to 111,8 GOPS/s and a power efficiency of 33,8 GOPS/s/W on an Ultrascale+ ZU3EG (up to $10\\times$ speedup and $3\\times$ power efficiency improvement with respect to pure software implementation).", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Marco  Carreras", "Gianfranco  Deriu", "Luigi  Raffo", "Luca  Benini", "Paolo  Meloni"], "year": 2020, "n_citations": 10}
{"id": 3639339, "s2_id": "3a086a6d41507665b6a52ef2903af6024e9a7cfe", "title": "The microarchitecture of a multi-threaded RISC-V compliant processing core family for IoT end-nodes", "abstract": "Internet-of-Things end-nodes demand low power processing platforms characterized by heterogeneous dedicated units, controlled by a processor core running concurrent control threads. Such architecture scheme fits one of the main target application domain of the RISC-V instruction set. We present an open-source processing core compliant with RISC-V on the software side and with the popular Pulpino processor platform on the hardware side, while supporting interleaved multi-threading for IoT applications. The latter feature is a novel contribution in this application domain. We report details about the microarchitecture design along with performance data.", "venue": "ApplePies", "authors": ["Abdallah  Cheikh", "Gianmarco  Cerutti", "Antonio  Mastrandrea", "Francesco  Menichelli", "Mauro  Olivieri"], "year": 2017, "n_citations": 8}
{"id": 3642919, "s2_id": "2065cdf9f4019564a9004d893b24dcbbc113d7e3", "title": "SCARE: Side Channel Attack on In-Memory Computing for Reverse Engineering", "abstract": "In-memory computing (IMC) architectures provide a much needed solution to energy-efficiency barriers posed by Von-Neumann computing. The functions implemented in such in-memory architectures are often proprietary and constitute confidential intellectual property (IP). Our studies indicate that IMC architectures implemented using resistive RAM (RRAM) are susceptible to side channel attack (SCA). Unlike the conventional SCAs that are aimed to leak private keys from cryptographic implementations, SCA on IMC for reverse engineering (SCARE) can reveal the sensitive IP implemented within the memory through power/timing side channels. Therefore, the adversary does not need to perform invasive reverse engineering (RE) to unlock the functionality. We demonstrate SCARE by taking recent IMC architectures, such as dynamic computing in memory (DCIM) and memristor-aided logic (MAGIC) as test cases. Simulation results indicate that AND, OR, and NOR gates (which are the building blocks of complex functions) yield distinct power and timing signatures based on the number of inputs, making them vulnerable to SCA. We show that adversary can use templates (using foundry-calibrated simulations or fabricating known functions in test chips) and analysis to identify the structure of the implemented function by testing a limited number of patterns. We also propose countermeasures, such as redundant inputs and expansion of literals. Redundant inputs can mask the IP with 25% area and 20% power overhead. However, functions can be found at higher RE effort. Expansion of literals incurs 36% power overhead. However, it imposes a brute force search increasing the adversarial RE effort by $3.04\\times $ .", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Sina  Sayyah Ensan", "Karthikeyan  Nagarajan", "Mohammad Nasim Imtiaz Khan", "Swaroop  Ghosh"], "year": 2021, "n_citations": 0}
{"id": 3647996, "s2_id": "040ddac788b8ed4057b11ac4bb14170039c1e7ff", "title": "R2F: A Remote Retraining Framework for AIoT Processors With Computing Errors", "abstract": "Artificial Intelligence of Things (AIoT) processors fabricated with newer technology nodes suffer rising soft errors due to the shrinking transistor sizes and lower power supply. Soft errors on the AIoT processors particularly the deep learning accelerators (DLAs) with massive computing may cause substantial computing errors. These computing errors are difficult to be captured by the conventional training on general-purposed processors such as CPUs and GPUs in a server. Applying the offline trained neural network models to the edge accelerators with errors directly may lead to considerable prediction accuracy loss. To address the problem, we propose a remote retraining framework (R2F) for remote AIoT processors with computing errors. It takes the remote AIoT processor with soft errors in the training loop such that the on-site computing errors can be learned with the application data on the server and the retrained models can be resilient to the soft errors. Meanwhile, we propose an optimized partial triple modular redundancy (TMR) strategy to enhance the retraining. According to our experiments, R2F enables elastic design tradeoffs between the model accuracy and the performance penalty. The top-5 model accuracy can be improved by 1.93%\u201313.73% with 0%\u2013200% performance penalty at high fault error rate. In addition, we notice that the retraining requires massive data transmission and even dominates the training time and propose a sparse increment compression approach for the data transmission optimization, which reduces the retraining time by 38%\u201388% on average with negligible accuracy loss over straightforward remote retraining.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Dawen  Xu", "Meng  He", "Cheng  Liu", "Ying  Wang", "Long  Cheng", "Huawei  Li", "Xiaowei  Li", "Kwang-Ting  Cheng"], "year": 2021, "n_citations": 0}
{"id": 3649180, "s2_id": "576d4befc3ffb1407697fbb26669d1c3b1d473d3", "title": "Electromigration-Aware Architecture for Modern Microprocessors", "abstract": "Reliability is a fundamental requirement in any microprocessor to guarantee correct execution over its lifetime. The design rules related to reliability depend on the process technology being used and the expected operating conditions of the device. To meet reliability requirements, advanced process technologies (28 nm and below) impose highly challenging design rules. Such design-for-reliability rules have become a major burden on the flow of VLSI implementation because of the severe physical constraints they impose. \nThis paper focuses on electromigration (EM), which is one of the major critical factors affecting semiconductor reliability. EM is the aging process of on-die wires and vias and is induced by excessive current flow that can damage wires and may also significantly impact the integrated-circuit clock frequency. EM exerts a comprehensive global effect on devices because it impacts wires that may reside inside the standard or custom logical cells, between logical cells, inside memory elements, and within wires that interconnect functional blocks. \nThe design-implementation flow (synthesis and place-and-route) currently detects violations of EM-reliability rules and attempts to solve them. In contrast, this paper proposes a new approach to enhance these flows by using EM-aware architecture. Our results show that the proposed solution can relax EM design efforts in microprocessors and more than double microprocessor lifetime. This work demonstrates this proposed approach for modern microprocessors, although the principals and ideas can be adapted to other cases as well.", "venue": "ArXiv", "authors": ["Freddy  Gabbay", "Avi  Mendelson", "Yinnon  Stav"], "year": 2020, "n_citations": 0}
{"id": 3656053, "s2_id": "67f15628bd8e347b0ebd25341349bdced0cd1850", "title": "Relational Memory: Native In-Memory Accesses on Rows and Columns", "abstract": "Analytical database systems are typically designed to use a columnfirst data layout that maps better to analytical queries of access patterns. This choice is premised on the assumption that storing data in a row-first format leads to accessing unwanted fields; moreover, transforming rows to columns at runtime is expensive. On the other hand, new data items are constantly ingested in row-first form and transformed in the background to columns to facilitate future analytical queries. How will this design change if we can always access only the desired set of columns? In this paper, to address this question, we present a radically new approach to data transformation from rows to columns. We build upon recent advancements in commercial embedded platforms with tightly-coupled re-programmable logic to design native in-memory access on rows and columns. We propose a new database management system (DBMS) architecture that is the first hardware/software co-design. It relies on an FPGA-based accelerator to transparently transform base data to any group of columns with minimal overhead at runtime. This design allows the DBMS to access any group of columns as if it already exists in memory. Our method, termed relational memory, currently implements projection, and offers the groundwork for implementing selection, group by, aggregation, and supporting joins in hardware, thus, vastly simplifying the software logic and accelerating the query execution. We present a detailed analysis of relational memory using both synthetic benchmarks and realistic workloads. Our relational memory implementation can convert on the fly rows to arbitrary groups of columns without any latency penalty. Essentially, relational memory can load in cache the desired columns from a row-oriented base data layout as fast as reading from column-oriented base data layout by outsourcing data transformation to the hardware.", "venue": "ArXiv", "authors": ["Shahin  Roozkhosh", "Denis  Hoornaert", "Ju Hyoung Mun", "Tarikul Islam Papon", "Ulrich  Drepper", "Renato  Mancuso", "Manos  Athanassoulis"], "year": 2021, "n_citations": 0}
{"id": 3659260, "s2_id": "1e4f078dc296872d8eb2b429ac81714461ef3094", "title": "A Hardware Platform for Efficient Multi-Modal Sensing with Adaptive Approximation", "abstract": "We present Warp, a hardware platform to support research in approximate computing, sensor energy optimization, and energy-scavenged systems. Warp incorporates 11 state-of-the-art sensor integrated circuits, computation, and an energy-scavenged power supply, all within a miniature system that is just 3.6 cm x 3.3 cm x 0.5 cm. Warp's sensor integrated circuits together contain a total of 21 sensors with a range of precisions and accuracies for measuring eight sensing modalities of acceleration, angular rate, magnetic flux density (compass heading), humidity, atmospheric pressure (elevation), infrared radiation, ambient temperature, and color. Warp uses a combination of analog circuits and digital control to facilitate further tradeoffs between sensor and communication accuracy, energy efficiency, and performance. This article presents the design of Warp and presents an evaluation of our hardware implementation. The results show how Warp's design enables performance and energy efficiency versus ac- curacy tradeoffs.", "venue": "ArXiv", "authors": ["Phillip  Stanley-Marbell", "Martin C. Rinard"], "year": 2018, "n_citations": 5}
{"id": 3662309, "s2_id": "385b125f46ebfe219b8c107c5d35919e597b235e", "title": "The Tribes of Machine Learning and the Realm of Computer Architecture", "abstract": "Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.", "venue": "ArXiv", "authors": ["Ayaz  Akram", "Jason  Lowe-Power"], "year": 2020, "n_citations": 0}
{"id": 3666955, "s2_id": "ed5c21aa280062067ea8b620f3139fcd1176b6cd", "title": "Deep-PowerX: a deep learning-based framework for low-power approximate logic synthesis", "abstract": "This paper aims at integrating three powerful techniques namely Deep Learning, Approximate Computing, and Low Power Design into a strategy to optimize logic at the synthesis level. We utilize advances in deep learning to guide an approximate logic synthesis engine to minimize the dynamic power consumption of a given digital CMOS circuit, subject to a predetermined error rate at the primary outputs. Our framework, Deep-PowerX1, focuses on replacing or removing gates on a technology-mapped network and uses a Deep Neural Network (DNN) to predict error rates at primary outputs of the circuit when a specific part of the netlist is approximated. The primary goal of Deep-PowerX is to reduce the dynamic power whereas area reduction serves as a secondary objective. Using the said DNN, Deep-PowerX is able to reduce the exponential time complexity of standard approximate logic synthesis to linear time. Experiments are done on numerous open source benchmark circuits. Results show significant reduction in power and area by up to 1.47\u00d7 and 1.43\u00d7 compared to exact solutions and by up to 22% and 27% compared to state-of-the-art approximate logic synthesis tools while having orders of magnitudes lower run-time.", "venue": "ISLPED", "authors": ["Ghasem  Pasandi", "Mackenzie  Peterson", "Moises  Herrera", "Shahin  Nazarian", "Massoud  Pedram"], "year": 2020, "n_citations": 3}
{"id": 3676685, "s2_id": "c6887affb174fb04b766507f7813dc3e56634fa8", "title": "Umgebungserfassungssystem fuer mobile Roboter (environment logging system for mobile autonomous robots)", "abstract": "This diploma thesis describes the theoretical bases, the conception of the module and the final result of the development process in application. for the environment logging with a small mobile robot for interiors should be sketched an economical alternative to the expensive laser scanners. the structure, color or the material of the objects in the radius of action, as well as the environment brightness and illuminating are to have thereby no influence on the results of measurement.", "venue": "ArXiv", "authors": ["Dirk  Hesselbach"], "year": 2012, "n_citations": 0}
{"id": 3676714, "s2_id": "a187c0947af9d7f581444c9891d221ac681719db", "title": "Fast and reconfigurable packet classification engine in FPGA-based firewall", "abstract": "In data communication via internet, security is becoming one of the most influential aspects. One way to support it is by classifying and filtering ethernet packets within network devices. Packet classification is a fundamental task for network devices such as routers, firewalls, and intrusion detection systems. In this paper we present architecture of fast and reconfigurable Packet Classification Engine (PCE). This engine is used in FPGA-based firewall. Our PCE inspects multi-dimensional field of packet header sequentially based on tree-based algorithm. This algorithm simplifies overall system to a lower scale and leads to a more secure system. The PCE works with an adaptation of single cycle processor architecture in the system. Ethernet packet is examined with PCE based on Source IP Address, Destination IP Address, Source Port, Destination Port, and Protocol fields of the packet header. These are basic fields to know whether it is a dangerous or normal packet before inspecting the content. Using implementation of tree-based algorithm in the architecture, firewall rules are rebuilt into 24-bit sub-rules which are read as processor instruction in the inspection process. The inspection process is comparing one sub-rule with input field of header every clock cycle.", "venue": "Proceedings of the 2011 International Conference on Electrical Engineering and Informatics", "authors": ["Arief  Wicaksana", "Arif  Sasongko"], "year": 2011, "n_citations": 17}
{"id": 3681172, "s2_id": "818b097a7c593bda6cc28f4e53fe89bef49ca3b6", "title": "Low-cost multi-gigahertz test systems using CMOS FPGAs and PECL", "abstract": "The paper describes two projects researching the development of new low-cost techniques for testing devices with multiple high-speed (2 to 5 Gbps) signals. Each project uses commercially available components to keep costs low, yet achieves performance characteristics comparable to (and in some ways exceeding) more expensive ATE. A common CMOS FPGA-based logic core provides flexibility, adaptability, and communication with controlling computers while customized positive emitter-coupled logic (PECL) achieves multi-gigahertz data rates with about /spl plusmn/25 ps timing accuracy.", "venue": "Design, Automation and Test in Europe", "authors": ["David C. Keezer", "Carl Edward Gray", "Ashraf M. Majid", "Nafeez  Taher"], "year": 2005, "n_citations": 13}
{"id": 3682245, "s2_id": "e314038a7f88e16d9b37895d4571c09682739e06", "title": "Programming the Adapteva Epiphany 64-Core Network-on-Chip Coprocessor", "abstract": "With energy efficiency and power consumption being the primary impediment in the path to exascale systems, low-power high performance embedded systems are of increasing interest. The Parallella System-on-module (SoM) created by Adapteva combines the Epiphany-IV 64-core coprocessor with a host ARM processor housed in a Zynq System-on-chip. The Epiphany integrates low-power RISC cores on a 2D mesh network and promises up to 70 GFLOPS/Watt of processing efficiency. However, with just 32 KB of memory per eCore for storing both data and code, and only low level inter-core communication support, programming the Epiphany system presents several challenges. In this paper we evaluate the performance of the Epiphany system for a variety of basic compute and communication operations. Guided by this data we explore various strategies for implementing stencil based application codes on the Epiphany system. With future systems expected to house 4096 eCores, the merits of the Epiphany architecture as a path to exascale is compared to other competing power efficient systems.", "venue": "IPDPS Workshops", "authors": ["Anish  Varghese", "Bob  Edwards", "Gaurav  Mitra", "Alistair P. Rendell"], "year": 2014, "n_citations": 25}
{"id": 3683376, "s2_id": "0c3ebdc5904c135f6e39f4d9368f67f835d96442", "title": "Breaking Barriers: Maximizing Array Utilization for Compute in-Memory Fabrics", "abstract": "Compute in-memory (CIM) is a promising technique that minimizes data transport, the primary performance bottleneck and energy cost of most data intensive applications. This has found wide-spread adoption in accelerating neural networks for machine learning applications. Utilizing a crossbar architecture with emerging non-volatile memories (eNVM) such as dense resistive random access memory (RRAM) or phase change random access memory (PCRAM), various forms of neural networks can be implemented to greatly reduce power and increase on chip memory capacity. However, compute in-memory faces its own limitations at both the circuit and the device levels. Although compute in-memory using the crossbar architecture can greatly reduce data transport, the rigid nature of these large fixed weight matrices forfeits the flexibility of traditional CMOS and SRAM based designs. In this work, we explore the different synchronization barriers that occur from the CIM constraints. Furthermore, we propose a new allocation algorithm and data flow based on input data distributions to maximize utilization and performance for compute-in memory based designs. We demonstrate a $\\boldsymbol{7.47}\\times$ performance improvement over a naive allocation method for CIM accelerators on ResNet18.", "venue": "2020 IFIP/IEEE 28th International Conference on Very Large Scale Integration (VLSI-SOC)", "authors": ["Brian  Crafton", "Samuel  Spetalnick", "Gauthaman  Murali", "Tushar  Krishna", "Sung-Kyu  Lim", "Arijit  Raychowdhury"], "year": 2020, "n_citations": 4}
{"id": 3689063, "s2_id": "2a99e5047980ee8f1c0d376d89743d4b98e9796f", "title": "Benchmarking Inference Performance of Deep Learning Models on Analog Devices", "abstract": "Analog hardware implemented deep learning models are promising for computation and energy constrained systems such as edge computing devices. However, the analog nature of the device and the many associated noise sources will cause changes to the value of the weights in the trained deep learning models deployed on such devices. In this study, systematic evaluation of the inference performance of trained popular deep learning models for image classification deployed on analog devices has been carried out, where additive white Gaussian noise has been added to the weights of the trained models during inference. It is observed that deeper models and models with more redundancy in design, such as VGG, are more robust to the noise in general. Also, it is observed that the performance is affected by the design philosophy of the model, the detailed structure of the model, the exact machine learning task, as well as the datasets.", "venue": "2021 International Joint Conference on Neural Networks (IJCNN)", "authors": ["Omobayode  Fagbohungbe", "Lijun  Qian"], "year": 2021, "n_citations": 3}
{"id": 3693266, "s2_id": "899e43fc8f4869c3372e16da857f7bd4e026da03", "title": "Design paradigms of intelligent control systems on a chip", "abstract": "This paper focuses on the Field Programmable Gate Array (FPGA) design and implementation of intelligent control system applications on a chip, specifically fuzzy logic and genetic algorithm processing units. Initially, an overview of the FPGA technology is presented, followed by design methodologies, development tools and the use of hardware description languages (HDL). Two FPGA design examples with the use of Hardware Description Languages (HDLs) of parameterized fuzzy logic controller cores are discussed. Thereinafter, a System-on-a-Chip (SoC) designed by the authors in previous work and realized on FPGA featuring a Digital Fuzzy Logic Controller (DFLC) and a soft processor core for the path tracking problem of mobile robots is discussed. Finally a Genetic Algorithm implementation (previously published by the authors) in FPGA chip for the Traveling Salesman Problem (TSP) is also discussed.", "venue": "ArXiv", "authors": ["K. M. Deliparaschos", "Spyros G. Tzafestas"], "year": 2018, "n_citations": 2}
{"id": 3693729, "s2_id": "0578fad93d33d08487e71cda6d0670c273d25d9d", "title": "Towards an Area-Efficient Implementation of a High ILP EDGE Soft Processor", "abstract": "In-order scalar RISC architectures have been the dominant paradigm in FPGA soft processor design for twenty years. Prior out-of-order superscalar implementations have not exhibited competitive area or absolute performance. This paper describes a new way to build fast and area-efficient out-of-order superscalar soft processors by utilizing an Explicit Data Graph Execution (EDGE) instruction set architecture. By carefully mapping the EDGE microarchitecture, and in particular, its dataflow instruction scheduler, we demonstrate the feasibility of an out-of-order FPGA architecture. Two scheduler design alternatives are compared.", "venue": "ArXiv", "authors": ["Jan  Gray", "Aaron  Smith"], "year": 2018, "n_citations": 4}
{"id": 3703010, "s2_id": "375d39ff6303d966f59ecd61029f2cba782f15f4", "title": "Automated Floorplanning for Partially Reconfigurable Designs on Heterogenrous FPGAs", "abstract": "Floorplanning problem has been extensively explored for homogeneous FPGAs. Most modern FPGAs consist of heterogeneous resources in the form of configurable logic blocks, DSP blocks, BRAMs and more. Very little work has been done for heterogeneous FPGAs. In addition, features like partial reconfigurability allow on-the-fly changes to the executable design that can result in enhanced performance and very efficient utilization of resources. In this paper, we have designed a floorplanner for Partially Reconfigurable (PR) designs in FPGA that smartly decides one of the three proposed resource allocation schemes to floorplan a particular type of reconfigurable region. We also propose a White Space Detection algorithm for efficient management of white space inside an FPGA in order to reduce the area and the wire length. The floorplanner is demonstrated on Xilinx Virtex 5 and Artix 7 FPGA architectures and can be easily integrated with existing vendor-supplied Place and Route tools. The main objective of the floorplanner is to reduce the wire length, minimize wasted resources and the area. The performance of our floorplanner is evaluated using MCNC benchmarks. We have compared our proposed floorplanner with other previously published results reported in the literature. We observe a substantial improvement in the overall wire length as well as the execution time. Also, the floorplanner was integrated with vendor supplied place and route tools (Xilinx Vivado) to automate the floorplanning flow. The automation process was tested on a partially reconfigurable median filter used in image processing applications.", "venue": "ArXiv", "authors": ["Pingakshya  Goswami", "Dinesh  Bhatia"], "year": 2020, "n_citations": 0}
{"id": 3712879, "s2_id": "3c7aea559f545068ff59e11df3ebe6ae3afb1382", "title": "MF-Net: Compute-In-Memory SRAM for Multibit Precision Inference Using Memory-Immersed Data Conversion and Multiplication-Free Operators", "abstract": "We propose a co-design approach for <italic>compute-in-memory</italic> inference for deep neural networks (DNN). We use multiplication-free function approximators based on <inline-formula> <tex-math notation=\"LaTeX\">$\\ell _{1}$ </tex-math></inline-formula> norm along with a co-adapted processing array and compute flow. Using the approach, we overcame many deficiencies in the current <italic>art</italic> of in-SRAM DNN processing such as the need for digital-to-analog converters (DACs) at each operating SRAM row/column, the need for high precision analog-to-digital converters (ADCs), limited support for multi-bit precision weights, and limited vector-scale parallelism. Our co-adapted implementation seamlessly extends to multi-bit precision weights, it doesn\u2019t require DACs, and it easily extends to higher vector-scale parallelism. We also propose an SRAM-immersed successive approximation ADC (SA-ADC), where we exploit the parasitic capacitance of bit lines of SRAM array as a capacitive DAC. Since the dominant area overhead in SA-ADC comes due to its capacitive DAC, by exploiting the intrinsic parasitic of SRAM array, our approach allows low area implementation of within-SRAM SA-ADC. Our <inline-formula> <tex-math notation=\"LaTeX\">$8\\times 62$ </tex-math></inline-formula> SRAM macro, which requires a 5-bit ADC, achieves ~105 tera operations per second per Watt (TOPS/W) with 8-bit input/weight processing at 45 nm CMOS. Our <inline-formula> <tex-math notation=\"LaTeX\">$8\\times 30$ </tex-math></inline-formula> SRAM macro, which requires a 4-bit ADC, achieves ~84 TOPS/W. SRAM macros that require lower ADC precision are more tolerant of process variability, however, have lower TOPS/W as well. We evaluated the accuracy and performance of our proposed network for MNIST, CIFAR10, and CIFAR100 datasets. We chose a network configuration which adaptively mixes multiplication-free and regular operators. The network configurations utilize the multiplication-free operator for more than 85% operations from the total. The selected configurations are 98.6% accurate for MNIST, 90.2% for CIFAR10, and 66.9% for CIFAR100. Since most of the operations in the considered configurations are based on proposed SRAM macros, our compute-in-memory\u2019s efficiency benefits broadly translate to the system-level.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Shamma  Nasrin", "Diaa  Badawi", "Ahmet Enis Cetin", "Wilfred  Gomes", "Amit Ranjan Trivedi"], "year": 2021, "n_citations": 3}
{"id": 3714143, "s2_id": "eac4c05019566126842d59c8592b6345ed772a3e", "title": "How Parallel Circuit Execution Can Be Useful for NISQ Computing?", "abstract": "Quantum computing is performed on Noisy Intermediate-Scale Quantum (NISQ) hardware in the short term. Only small circuits can be executed reliably on a quantum machine due to the unavoidable noisy quantum operations on NISQ devices, leading to the under-utilization of hardware resources. With the growing demand to access quantum hardware, how to utilize it more efficiently while maintaining output fidelity is becoming a timely issue. A parallel circuit execution technique has been proposed to address this problem by executing multiple programs on hardware simultaneously. It can improve the hardware throughput and reduce the overall runtime. However, accumulative noises such as crosstalk can decrease the output fidelity in parallel workload execution. In this paper, we first give an in-depth overview of stateof-the-art parallel circuit execution methods. Second, we propose a Quantum Crosstalk-aware Parallel workload execution method (QuCP) without the overhead of crosstalk characterization. Third, we investigate the trade-off between hardware throughput and fidelity loss to explore the hardware limitation with parallel circuit execution. Finally, we apply parallel circuit execution to VQE and zero-noise extrapolation error mitigation method to showcase its various applications on advancing NISQ computing.", "venue": "ArXiv", "authors": ["Siyuan  Niu", "Aida  Todri"], "year": 2021, "n_citations": 0}
{"id": 3716288, "s2_id": "4c8c9018f78af339a9dfa5c38dec66d9a69cb6e7", "title": "Exploiting Process Variations to Secure Photonic NoC Architectures From Snooping Attacks", "abstract": "The compact size and high wavelength-selectivity of microring resonators (MRs) enable photonic networks-on-chip (PNoCs) to utilize dense-wavelength-division-multiplexing (DWDM) in their photonic waveguides, and as a result, attain high bandwidth on-chip data transfers. Unfortunately, a hardware Trojan (HT) in a PNoC can manipulate the electrical driving circuit of its MRs to cause the MRs to snoop data from the neighboring wavelength channels in a shared photonic waveguide, which introduces a serious security threat. This article presents a framework that utilizes process variation-based authentication signatures along with architecture-level enhancements to protect against data-snooping HT during unicast as well as multicast transfers in PNoCs. The evaluation results indicate that our framework can improve hardware security across various PNoC architectures with minimal overheads of up to 14.2% in average latency and of up to 14.6% in energy-delay-product (EDP).", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Sai Vineel Reddy Chittamuru", "Ishan G. Thakkar", "Sudeep  Pasricha", "Sairam  Sri Vatsavai", "Varun  Bhat"], "year": 2021, "n_citations": 5}
{"id": 3723796, "s2_id": "30ba3ef6d9fac257a7efcb8f6dd1201f4b2681c1", "title": "Modeling and Energy Optimization of LDPC Decoder Circuits With Timing Violations", "abstract": "This paper proposes a \"quasi-synchronous\" design approach for signal processing circuits, in which timing violations are permitted, but without the need for a hardware compensation mechanism. The case of a low-density parity-check (LDPC) decoder is studied, and a method for accurately modeling the effect of timing violations at a high level of abstraction is presented. The error-correction performance of code ensembles is then evaluated using density evolution while taking into account the effect of timing faults. Following this, several quasi-synchronous LDPC decoder circuits based on the offset min-sum algorithm are optimized, providing a 23%-40% reduction in energy consumption or energy-delay product, while achieving the same performance and occupying the same area as conventional synchronous circuits.", "venue": "IEEE Trans. Commun.", "authors": ["Fran\u00e7ois  Leduc-Primeau", "Frank R. Kschischang", "Warren J. Gross"], "year": 2018, "n_citations": 15}
{"id": 3725175, "s2_id": "d944719359b318c094e0af1b1e8994041e4644fa", "title": "Criticality Aware Multiprocessors", "abstract": "Typically, a memory request from a processor may need to go through many intermediate interconnect routers, directory node, owner node, etc before it is finally serviced. Current multiprocessors do not give preference to any particular memory request. But certain memory requests are more critical to multiprocessor's performance than other requests. Example: memory requests from critical sections, load request feeding into multiple dependent instructions, etc. This knowledge can be used to improve the performance of current multiprocessors by letting the ordering point and the interconnect routers prioritize critical requests over non-critical ones. In this paper, we evaluate using SIMICS/GEMS infrastructure. For lock-intensive microbenchmarks, criticality-aware multiprocessors showed 5-15% performance improvement over baseline multiprocessor. Criticality aware multiprocessor provides a new direction for tapping performance in a shared memory multiprocessor and can provide substantial speedup in lock intensive benchmarks.", "venue": "ArXiv", "authors": ["Sandeep  Navada", "Anil  Krishna"], "year": 2016, "n_citations": 0}
{"id": 3728920, "s2_id": "794dd6c708fd2212681e9e8ab354ed5b8d432b64", "title": "Can Broken Multicore Hardware be Mended?", "abstract": "A suggestion is made for mending multicore hardware, which has been diagnosed as broken. 1. THE MULTICORE ERA IS A CONSEQUENCE OF THE STALLING OF THE SINGLE-THREAD PERFORMANCE The multiand many-core (MC) era we have reached was triggered after the beginning of the century by the stalling of single-processor performance. Technology allowed more transistors to be placed on a die, but they could not reasonably be utilized to increase single-processor performance. Predictions about the number of cores has only partly been fulfilled: today\u2019s processors have dozens rather than the predicted hundreds of cores (although the Chinese supercomputer [3] announced in the middle of 2016 comprises 260 cores on a die). Despite this, the big players are optimistic. They expect that Moore-law persists, though based on presently unknown technologies. The effect of the stalled clock frequency is mitigated, and it is even predicted [6] that \u201dNow that there are multicore processors, there is no reason why computers shouldn\u2019t begin to work faster, whether due to higher frequency or because of parallel task execution. And with parallel task execution it provides even greater functionality and flexibility!.\u201d Parallelism is usually considered in many forums [4] to be the future, usually as the only hope, rather than as a panacea. People dealing with parallelism are less optimistic. In general, the technical development tends to reduce the human effort, but \u201dparallel programs ... are notoriously difficult to write, test, analyze, debug, and verify, much more so than the sequential versions\u201d [9]. The problems have led researchers to the ViewPoint [8], that multicore hardware for general-purpose parallel processing is broken. 2. MANYCORE ARCHITECTURES COULD BE FRESH MEAT ON THE MARKET OF PROCESSORS, BUT THEY ARE NOT Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 2015 ACM 0001-0782/08/0X00 ...$5.00. The essence of the present Viewpoint is that multicore hardware can perhaps be mended. Although one can profoundly agree with the arguments [8] that using manycore chips cannot contribute much to using parallelism in general, and especially not in executing irregular programs, one has to realize also that this is not the optimal battlefield for the manycore chips, at least not in their present architecture. Present manycore systems comprise many segregated processors, which make no distinction between two processing units that are neighbours within the same chip or are located in the next rack. The close physical proximity of the processing units offers additional possibilities, and provides a chance to implement Amdahl\u2019s dream [1] of cooperating processors. Paradigms used presently, however, assume a private processor and a private address space for a running process, and no external world. In many-core systems, it is relatively simple to introduce signals, storages, communication, etc., and deploy them in reasonable times. They cannot, however, be utilized in a reasonable way, if one cannot provide compatibiliy facades providing the illusion of the private world. Cooperation must be implemented in a way which provides complete (upward) compatibility with the presently exclusively used Single-Processor Approach (SPA) [1]. It means that on the one hand that new functionality must be formulated using the terms of conventional computing, while on the other, it provides considerably enhanced computing throughput and other advantages. It is well known, that general purpose processors have a huge handicap in performance when compared to special purpose chips, and that the presently used computing stack is the source of further serious inefficiencies. Proper utilization of available manycore processors can eliminate a lot of these performance losses, and in this way (keeping the same electronic and programming technology) can considerably enhance (apparently) the performance of the processor. Of course, there is no free lunch. Making these changes requires a simultanous change in nearly all elements of the present computing stack. Before making these changes, one should scrutinize the promised gain, and whether the required efforts will pay off. Below, some easy-to follow case studies are presented, all of which lead to the same conclusion: we need a cooperative and flexible rather than rigid architecture comprising segregated MCs, and the 70-years-old von Neumann computing paradigms should be extended. At the end, the feasibility of implementing such an architecture is discussed. The recently introduced Explicitly Many-Processor Approach [7] seems to ar X iv :1 61 1. 07 51 1v 1 [ cs .D C ] 1 2 N ov 2 01 6 L1 B = (C \u2217D) \u2212 (E \u2217 F ) A = (C \u2217D) + (E \u2217 F )", "venue": "ArXiv", "authors": ["J\u00e1nos  V\u00e9gh"], "year": 2016, "n_citations": 0}
{"id": 3730904, "s2_id": "ccbc33c3e700d1593754715ba696582599c4fede", "title": "An efficient transparent test scheme for embedded word-oriented memories", "abstract": "Memory cores are usually the densest portion with the smallest feature size in system-on-chip (SOC) designs. The reliability of memory cores thus has a heavy impact on the reliability of SOCs. The transparent test is a useful technique for improving the reliability of memories during their life time. The paper presents a systematic algorithm used for transforming a bit-oriented march test into a transparent word-oriented march test. The transformed transparent march test has shorter test complexity compared with those proposed previously (Nicolaidis, M., IEEE Trans. Computers, vol.45, no.10, p.1141-56, 1996; Thaller, K. and Steininger, A., IEEE Trans. Reliability, vol.52, no.4, p.413-22, 2003). For example, if a memory with 32-bit words is tested with March C-, the time complexity of the transparent word-oriented test transformed by the proposed scheme is only about 56% and 19% of the time complexity of the transparent word-oriented test converted by the schemes reported by Nicolaidis and by Thaller and Steininger, respectively.", "venue": "Design, Automation and Test in Europe", "authors": ["Jin-Fu  Li", "Tsu-Wei  Tseng", "Chin-Long  Wey"], "year": 2005, "n_citations": 5}
{"id": 3732987, "s2_id": "4d5ea3f0d2528d0f42e71d2c57ccec34a9c10765", "title": "State dependent statistical timing model for voltage scaled circuits", "abstract": "This paper presents a novel statistical state-dependent timing model for voltage over scaled (VoS) logic circuits that accurately and rapidly finds the timing distribution of output bits. Using this model erroneous VoS circuits can be represented as error-free circuits combined with an error-injector. A case study of a two point DFT unit employing the proposed model is presented and compared to HSPICE circuit simulation. Results show an accurate match, with significant speedup gains.", "venue": "2014 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Aras  Pirbadian", "Muhammad S. Khairy", "Ahmed M. Eltawil", "Fadi J. Kurdahi"], "year": 2014, "n_citations": 4}
{"id": 3734904, "s2_id": "0dd9da77571cb965fe9e5038be7c437f24e47aa5", "title": "Efficient Neural Network Deployment for Microcontroller", "abstract": "Edge computing for neural networks is getting important especially for low power applications and offline devices. TensorFlow Lite and PyTorch Mobile were released for this purpose. But they mainly support mobile devices instead of microcontroller level yet. Microcontroller support is an emerging area now. There are many approaches to reduce network size and compute load like pruning, binarization and layer manipulation i.e. operator reordering. This paper is going to explore and generalize convolution neural network deployment for microcontrollers with two novel optimization proposals offering memory saving and compute efficiency in 2D convolutions as well as fully connected layers. The first one is in-place max-pooling, if the stride is greater than or equal to pooling kernel size. The second optimization is to use ping-pong buffers between layers to reduce memory consumption significantly. The memory savings and performance will be compared with CMSIS-NN framework developed for ARM Cortex-M CPUs. The final purpose is to develop a tool consuming PyTorch model with trained network weights, and it turns into an optimized inference engine(forward pass) in C/C++ for low memory(kilobyte level) and limited computing capable microcontrollers.", "venue": "ArXiv", "authors": ["Hasan  Unlu"], "year": 2020, "n_citations": 1}
{"id": 3735953, "s2_id": "98fcab0ba13156b8f13871ba5eacfbb4d8d3fdea", "title": "Implementation of Goldschmidt's Algorithm with hardware reduction", "abstract": "Division algorithms have been developed to reduce latency and to improve the efficiency of the processors. Floating point division is considered as a high latency operation. This papers looks into one such division algorithm, examines the hardware block diagram and suggests an alternative path which may be cost effective.", "venue": "ArXiv", "authors": ["Taposh Dutta Roy"], "year": 2019, "n_citations": 1}
{"id": 3736489, "s2_id": "9c14f026098d1fc3e8aeb60c549109651343d814", "title": "PPAC: A Versatile In-Memory Accelerator for Matrix-Vector-Product-Like Operations", "abstract": "Processing in memory (PIM) moves computation into memories with the goal of improving throughput and energy-efficiency compared to traditional von Neumann-based architectures. Most existing PIM architectures are either general-purpose but only support atomistic operations, or are specialized to accelerate a single task. We propose the Parallel Processor in Associative Content-addressable memory (PPAC), a novel in-memory accelerator that supports a range of matrix-vector-product (MVP)-like operations that find use in traditional and emerging applications. PPAC is, for example, able to accelerate low-precision neural networks, exact/approximate hash lookups, cryptography, and forward error correction. The fully-digital nature of PPAC enables its implementation with standard-cell-based CMOS, which facilitates automated design and portability among technology nodes. To demonstrate the efficacy of PPAC, we provide post-layout implementation results in 28nm CMOS for different array sizes. A comparison with recent digital and mixed-signal PIM accelerators reveals that PPAC is competitive in terms of throughput and energy-efficiency, while accelerating a wide range of applications and simplifying development.", "venue": "2019 IEEE 30th International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Oscar  Casta\u00f1eda", "Maria  Bobbett", "Alexandra  Gallyas-Sanhueza", "Christoph  Studer"], "year": 2019, "n_citations": 9}
{"id": 3738103, "s2_id": "4640f536deb71984aabfc74ec5c3f2f27192d250", "title": "Designing Parity Preserving Reversible Circuits", "abstract": "Making a reversible circuit fault-tolerant is much more difficult than classical circuit and there have been only a few works in the area of parity-preserving reversible logic design. Moreover, all of these designs are ad hoc, based on some pre-defined parity preserving reversible gates as building blocks. In this paper, we for the first time propose a novel and systematic approach towards parity preserving reversible circuits design. We provide some related theoretical results and give two algorithms, one from reversible specification to parity preserving reversible specification and another from irreversible specification to parity preserving reversible specification. We also evaluate the effectiveness of our approach by extensive experimental results.", "venue": "RC", "authors": ["Goutam  Paul", "Anupam  Chattopadhyay", "Chander  Chandak"], "year": 2017, "n_citations": 4}
{"id": 3743544, "s2_id": "87511f0d2312e60b40a1fea72bc244b2f6af216e", "title": "A Compiler Infrastructure for FPGA and ASIC Development", "abstract": "This whitepaper proposes a unified framework for hardware design tools to ease the development and inter-operability of said tools. By creating a large ecosystem of hardware development tools across vendors, academia, and the open source community, we hope to significantly increase much need productivity in hardware design.", "venue": "ArXiv", "authors": ["John  Demme"], "year": 2020, "n_citations": 0}
{"id": 3744574, "s2_id": "2efa0879afdce1521453be8fdaa8bc5cd23cdd80", "title": "A Unified Hardware Architecture for Convolutions and Deconvolutions in CNN", "abstract": "Deconvolution plays an important role in the state-of-the-art convolutional neural networks (CNNs) for the tasks like semantic segmentation, image super resolution, etc. In this paper, a scalable neural network hardware architecture for image segmentation is proposed. By sharing the same computing resources, both convolution and deconvolution operations are handled by the same process element array. In addition, access to on-chip and off-chip memories is optimized to alleviate the burden introduced by partial sum. As an example, SegNet-Basic has been implemented using the proposed unified architecture by targeting on Xilinx ZC706 FPGA, which achieves the performance of 151.5 GOPS and 94.3 GOPS for convolution and deconvolution respectively. This unified convolution/deconvolution design is applicable to other CNNs with deconvolution.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Lin  Bai", "Yecheng  Lyu", "Xinming  Huang"], "year": 2020, "n_citations": 4}
{"id": 3744767, "s2_id": "345c62659a36ed5f7ac5b1ecff01c8d2c2f7d695", "title": "Runtime Task Scheduling Using Imitation Learning for Heterogeneous Many-Core Systems", "abstract": "Domain-specific systems-on-chip, a class of heterogeneous many-core systems, is recognized as a key approach to narrow down the performance and energy-efficiency gap between custom hardware accelerators and programmable processors. Reaching the full potential of these architectures depends critically on optimally scheduling the applications to available resources at runtime. Existing optimization-based techniques cannot achieve this objective at runtime due to the combinatorial nature of the task scheduling problem. As the main theoretical contribution, this article poses scheduling as a classification problem and proposes a hierarchical imitation learning (IL)-based scheduler that learns from an Oracle to maximize the performance of multiple domain-specific applications. Extensive evaluations with six streaming applications from wireless communications and radar domains show that the proposed IL-based scheduler approximates an offline Oracle policy with more than 99% accuracy for performance- and energy-based optimization objectives. Furthermore, it achieves almost identical performance to the Oracle with a low runtime overhead and successfully adapts to new applications, many-core system configurations, and runtime variations in application characteristics.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Anish  Krishnakumar", "Samet E. Arda", "A. Alper Goksoy", "Sumit K. Mandal", "Umit Y. Ogras", "Anderson L. Sartor", "Radu  Marculescu"], "year": 2020, "n_citations": 6}
{"id": 3747302, "s2_id": "8db7feba5f0220f899a8cbd32581403072053063", "title": "AWRP: Adaptive Weight Ranking Policy for Improving Cache Performance", "abstract": "Due to the huge difference in performance between the computer memory and processor , the virtual memory management plays a vital role in system performance .A Cache memory is the fast memory which is used to compensate the speed difference between the memory and processor. This paper gives an adaptive replacement policy over the traditional policy which has low overhead, better performance and is easy to implement. Simulations show that our algorithm performs better than Least-Recently-Used (LRU), First-In-First-Out (FIFO) and Clock with Adaptive Replacement (CAR).", "venue": "ArXiv", "authors": ["Debabala  Swain", "Bijay  Paikaray", "Debabrata  Swain"], "year": 2011, "n_citations": 17}
{"id": 3753759, "s2_id": "b47821265bb8af164fd811ea7e773960719ce1fa", "title": "A distributed memory, local configuration technique for re-configurable logic designs", "abstract": "The use and location of memory in integrated circuits plays a key factor in their performance. Memory requires large physical area, access times limit overall system performance and connectivity can result in large fan-out. Modern FPGA systems and ASICs contain an area of memory used to set the operation of the device from a series of commands set by a host. Implementing these settings registers requires a level of care otherwise the resulting implementation can result in a number of large fan-out nets that consume valuable resources complicating the placement of timing critical pathways. This paper presents an architecture for implementing and programming these settings registers in a distributed method across an FPGA and how the presented architecture works in both clock-domain crossing and dynamic partial re-configuration applications. The design is compared to that of a `global' settings register architecture. We implement the architectures using Intel FPGAs Quartus Prime software targeting an Intel FPGA Cyclone V. It is shown that the distributed memory architecture has a smaller resource cost (as small as 25% of the ALMs and 20% of the registers) compared to the global memory architectures.", "venue": "ArXiv", "authors": ["Alexander E. Beasley"], "year": 2020, "n_citations": 0}
{"id": 3755335, "s2_id": "628800f35db4c58d83a48aa0acb2970483873460", "title": "FATAL+: A Self-Stabilizing Byzantine Fault-tolerant Clocking Scheme for SoCs", "abstract": "We present concept and implementation of a self-stabilizing Byzantine fault-tolerant distributed clock generation scheme for multi-synchronous GALS architectures in critical applications. It combines a variant of a recently introduced self-stabilizing algorithm for generating low-frequency, low-accuracy synchronized pulses with a simple non-stabilizing high-frequency, high-accuracy clock synchronization algorithm. We provide thorough correctness proofs and a performance analysis, which use methods from fault-tolerant distributed computing research but also addresses hardware-related issues like metastability. The algorithm, which consists of several concurrent communicating asynchronous state machines, has been implemented in VHDL using Petrify in conjunction with some extensions, and synthetisized for an Altera Cyclone FPGA. An experimental validation of this prototype has been carried out to confirm the skew and clock frequency bounds predicted by the theoretical analysis, as well as the very short stabilization times (required for recovering after excessively many transient failures) achievable in practice.", "venue": "ArXiv", "authors": ["Danny  Dolev", "Matthias  F\u00fcgger", "Christoph  Lenzen", "Markus  Posch", "Ulrich  Schmid", "Andreas  Steininger"], "year": 2012, "n_citations": 0}
{"id": 3762329, "s2_id": "1862069c83eec56e49c277bb134bf7aef6a3c435", "title": "Mppsocgen: A framework for automatic generation of mppsoc architecture", "abstract": "Automatic code generation is a standard method in software engineering since it improves the code consistency and reduces the overall development time. In this context, this paper presents a design flow for automatic VHDL code generation of mppSoC (massively parallel processing System-on-Chip) configuration. Indeed, depending on the application requirements, a framework of Netbeans Platform Software Tool named MppSoCGEN was developed in order to accelerate the design process of complex mppSoC. Starting from an architecture parameters design, VHDL code will be automatically generated using parsing method. Configuration rules are proposed to have a correct and valid VHDL syntax configuration. Finally, an automatic generation of Processor Elements and network topologies models of mppSoC architecture will be done for Stratix II device family. Our framework improves its flexibility on Netbeans 5.5 version and centrino duo Core 2GHz with 22 Kbytes and 3 seconds average runtime. Experimental results for reduction algorithm validate our MppSoCGEN design flow and demonstrate the efficiency of generated architectures.", "venue": "ArXiv", "authors": ["Emna  Kallel", "Yassine  Aoudni", "Mouna  Baklouti", "Mohamed  Abid"], "year": 2012, "n_citations": 0}
{"id": 3764975, "s2_id": "d95171d3ea7139846d8b4842f56baa3084c0e7fe", "title": "Adaptive FPGA NoC-based Architecture for Multispectral Image Correlation", "abstract": "An adaptive FPGA architecture based on the NoC (Network-on-Chip) approach is used for the multispectral image correlation. This architecture must contain several distance algorithms depending on the characteristics of spectral images and the precision of the authentication. The analysis of distance algorithms is required which bases on the algorithmic complexity, result precision, execution time and the adaptability of the implementation. This paper presents the comparison of these distance computation algorithms on one spectral database. The result of a RGB algorithm implementation was discussed.", "venue": "CGIV/MCS", "authors": ["Linlin  Zhang", "Anne-Claire  Legrand", "Virginie  Fresse", "Viktor  Fischer"], "year": 2008, "n_citations": 2}
{"id": 3767249, "s2_id": "990a129be5f86e07c774604a1996bf32270f9019", "title": "HyCA: A Hybrid Computing Architecture for Fault Tolerant Deep Learning", "abstract": "Hardware faults on the regular 2-D computing array of a typical deep learning accelerator (DLA) can lead to dramatic prediction accuracy loss. Prior redundancy design approaches typically have each homogeneous redundant processing element (PE) to mitigate faulty PEs for a limited region of the 2-D computing array rather than the entire computing array to avoid the excessive hardware overhead. However, they fail to recover the computing array when the number of faulty PEs in any region exceeds the number of redundant PEs in the same region. The mismatch problem deteriorates when the fault injection rate rises and the faults are unevenly distributed. To address the problem, we propose a hybrid computing architecture (HyCA) for faulttolerant DLAs. It has a set of dot-production processing units (DPPUs) to recompute all the operations that are mapped to the faulty PEs despite the faulty PE locations. According to our experiments, HyCA shows significantly higher reliability, scalability, and performance with less chip area penalty when compared to the conventional redundancy approaches. Moreover, by taking advantage of the flexible recomputing, HyCA can also be utilized to scan the entire 2-D computing array and detect the faulty PEs effectively at runtime.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Dawen  Xu", "Qianlong  Wang", "Cheng  Liu", "Cheng  Chu", "Ying  Wang", "Huawei  Li", "Xiaowei  Li", "Kwang-Ting  Cheng"], "year": 2021, "n_citations": 1}
{"id": 3768254, "s2_id": "c09d5af2c2d2024f619373488ad99aed58207b0e", "title": "Designing high-speed, low-power full adder cells based on carbon nanotube technology", "abstract": "This article presents novel high speed and low power full adder cells based on carbon nanotube field effect transistor (CNFET). Four full adder cells are proposed in this article. First one (named CN9P4G) and second one (CN9P8GBUFF) utilizes 13 and 17 CNFETs respectively. Third design that we named CN10PFS uses only 10 transistors and is full swing. Finally, CN8P10G uses 18 transistors and divided into two modules, causing Sum and Cout signals are produced in a parallel manner. All inputs have been used straight, without inverting. These designs also used the special feature of CNFET that is controlling the threshold voltage by adjusting the diameters of CNFETs to achieve the best performance and right voltage levels. All simulation performed using Synopsys HSPICE software and the proposed designs are compared to other classical and modern CMOS and CNFET-based full adder cells in terms of delay, power consumption and power delay product.", "venue": "ArXiv", "authors": ["Mehdi  Masoudi", "Milad  Mazaheri", "Aliakbar  Rezaei", "Keivan  Navi"], "year": 2014, "n_citations": 6}
{"id": 3771639, "s2_id": "7e18a5ec16ec82650bd9a2e2feca2546b09ad6d6", "title": "Performance Comparison of Quasi-Delay-Insensitive Asynchronous Adders", "abstract": "In this technical note, we provide a comparison of the design metrics of various quasi-delay-insensitive (QDI) asynchronous adders, where the adders correspond to diverse architectures. QDI adders are robust, and the objective of this technical note is to point to those QDI adders which are suitable for low power/energy and less area. This information could be valuable for a resource-constrained low power VLSI design scenario. Non-QDI adders are excluded from the comparison since they are not robust although they may have optimized design metrics. All the QDI adders were realized using a 32/28nm CMOS process.", "venue": "ArXiv", "authors": ["P  Balasubramanian"], "year": 2019, "n_citations": 0}
{"id": 3773974, "s2_id": "a5807e24d7f90f398e7ad8afe221f8edaf4e74e3", "title": "Understanding and Improving the Latency of DRAM-Based Memory Systems", "abstract": "Over the past two decades, the storage capacity and access bandwidth of main memory have improved tremendously, by 128x and 20x, respectively. These improvements are mainly due to the continuous technology scaling of DRAM (dynamic random-access memory), which has been used as the physical substrate for main memory. In stark contrast with capacity and bandwidth, DRAM latency has remained almost constant, reducing by only 1.3x in the same time frame. Therefore, long DRAM latency continues to be a critical performance bottleneck in modern systems. Increasing core counts, and the emergence of increasingly more data-intensive and latency-critical applications further stress the importance of providing low-latency memory access. \nIn this dissertation, we identify three main problems that contribute significantly to long latency of DRAM accesses. To address these problems, we present a series of new techniques. Our new techniques significantly improve both system performance and energy efficiency. We also examine the critical relationship between supply voltage and latency in modern DRAM chips and develop new mechanisms that exploit this voltage-latency trade-off to improve energy efficiency. \nThe key conclusion of this dissertation is that augmenting DRAM architecture with simple and low-cost features, and developing a better understanding of manufactured DRAM chips together lead to significant memory latency reduction as well as energy efficiency improvement. We hope and believe that the proposed architectural techniques and the detailed experimental data and observations on real commodity DRAM chips presented in this dissertation will enable development of other new mechanisms to improve the performance, energy efficiency, or reliability of future memory systems.", "venue": "ArXiv", "authors": ["Kevin K. Chang"], "year": 2017, "n_citations": 56}
{"id": 3775884, "s2_id": "34cb70d3ac942199cd9f444969967df5d50e94cc", "title": "Evaluating Asymmetric Multicore Systems-on-Chip using Iso-Metrics", "abstract": "The end of Dennard scaling has pushed power consumption into a first order concern for current systems, on par with performance. As a result, near-threshold voltage computing (NTVC) has been proposed as a potential means to tackle the limited cooling capacity of CMOS technology. Hardware operating in NTV consumes significantly less power, at the cost of lower frequency, and thus reduced performance, as well as increased error rates. In this paper, we investigate if a low-power systems-on-chip, consisting of ARM's asymmetric big.LITTLE technology, can be an alternative to conventional high performance multicore processors in terms of power/energy in an unreliable scenario. For our study, we use the Conjugate Gradient solver, an algorithm representative of the computations performed by a large range of scientific and engineering codes.", "venue": "HiPEAC 2015", "authors": ["Charalampos  Chalios", "Dimitrios S. Nikolopoulos", "Enrique S. Quintana-Ort\u00ed"], "year": 2015, "n_citations": 3}
{"id": 3781214, "s2_id": "80f1049da1e67d0970a93e4450213e19271b1b28", "title": "Energy efficient video fusion with heterogeneous CPU-FPGA devices", "abstract": "This paper presents a complete video fusion system with hardware acceleration and investigates the energy trade-offs between computing in the CPU or the FPGA device. The video fusion application is based on the Dual-Tree Complex Wavelet Transforms (DT-CWT). Video fusion combines information from different spectral bands into a single representation and advanced algorithms based on wavelet transforms are compute and energy intensive. In this work the transforms are mapped to a hardware accelerator using high-level synthesis tools for the FPGA and also vectorized code for the single instruction multiple data (SIMD) engine available in the CPU. The accelerated system reduces computation time and energy by a factor of 2. Moreover, the results show a key finding that the FPGA is not always the best choice for acceleration, and the SIMD engine should be selected when the wavelet decomposition reduces the frame size below a certain threshold. This dependency on workload size means that an adaptive system that intelligently selects between the SIMD engine and the FPGA achieves the most energy and performance efficiency point.", "venue": "2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Peng  Sun", "Alin  Achim", "Ian  Hasler", "Paul  Hill", "Jose  Nunez-Yanez"], "year": 2016, "n_citations": 2}
{"id": 3781376, "s2_id": "b66f89897e44555aa990418bb7e9e378165df24f", "title": "Static Timing Model Extraction for Combinational Circuits", "abstract": "For large circuits, static timing analysis (STA) needs to be performed in a hierarchical manner to achieve higher performance in arrival time propagation. In hierarchical STA, efficient and accurate timing models of sub-modules need to be created. We propose a timing model extraction method that significantly reduces the size of timing models without losing any accuracy by removing redundant timing information. Circuit components which do not contribute to the delay of any input to output pair are removed. The proposed method is deterministic. Compared to the original models, the numbers of edges and vertices of the resulting timing models are reduced by 84% and 85% on average, respectively, which are significantly more than the results achieved by other methods.", "venue": "PATMOS", "authors": ["Bing  Li", "Christoph  Knoth", "Walter  Schneider", "Manuel  Schmidt", "Ulf  Schlichtmann"], "year": 2008, "n_citations": 3}
{"id": 3782505, "s2_id": "282d051959b482de8e9302167a42d334576dfb69", "title": "Active Access: A Mechanism for High-Performance Distributed Data-Centric Computations", "abstract": "Remote memory access (RMA) is an emerging high-performance programming model that uses RDMA hardware directly. Yet, accessing remote memories cannot invoke activities at the target which complicates implementation and limits performance of data-centric algorithms. We propose Active Access (AA), a mechanism that integrates well-known active messaging (AM) semantics with RMA to enable high-performance distributed data-centric computations. AA supports a new programming model where the user specifies handlers that are triggered when incoming puts and gets reference designated addresses. AA is based on a set of extensions to the Input/Output Memory Management Unit (IOMMU), a unit that provides high-performance hardware support for remapping I/O accesses to memory. We illustrate that AA outperforms existing AM and RMA designs, accelerates various codes such as distributed hashtables or logging schemes, and enables new protocols such as incremental checkpointing for RMA. We also discuss how extended IOMMUs can support a virtualized global address space in a distributed system that offers features known from on-node memory virtualization. We expect that AA and other IOMMU features can enhance the design of HPC operating and runtime systems in large computing centers.", "venue": "ICS", "authors": ["Maciej  Besta", "Torsten  Hoefler"], "year": 2015, "n_citations": 31}
{"id": 3783272, "s2_id": "c0e69090b44d8a25e9829376071082cf7931f289", "title": "CoDR: Computation and Data Reuse Aware CNN Accelerator", "abstract": "Computation and Data Reuse is critical for the resource-limited Convolutional Neural Network (CNN) accelerators. This paper presents Universal Computation Reuse to exploit weight sparsity, repetition, and similarity simultaneously in a convolutional layer. Moreover, CoDR decreases the cost of weight memory access by proposing a customized Run-Length Encoding scheme and the number of memory accesses to the intermediate results by introducing an input and output stationary dataflow. Compared to two recent compressed CNN accelerators [5] [1] with the same area of 2.85 mm, CoDR decreases SRAM access by 5.08\u00d7 and 7.99\u00d7, and consumes 3.76\u00d7 and 6.84\u00d7 less energy.", "venue": "ArXiv", "authors": ["Alireza  Khadem", "Haojie  Ye", "Trevor  Mudge"], "year": 2021, "n_citations": 0}
{"id": 3785507, "s2_id": "1f1a3f97b43d7fa4c8ea23db36d4148c4285248f", "title": "Holistic Management of the GPGPU Memory Hierarchy to Manage Warp-level Latency Tolerance", "abstract": "In a modern GPU architecture, all threads within a warp execute the same instruction in lockstep. For a memory instruction, this can lead to memory divergence: the memory requests for some threads are serviced early, while the remaining requests incur long latencies. This divergence stalls the warp, as it cannot execute the next instruction until all requests from the current instruction complete. In this work, we make three new observations. First, GPGPU warps exhibit heterogeneous memory divergence behavior at the shared cache: some warps have most of their requests hit in the cache, while other warps see most of their request miss. Second, a warp retains the same divergence behavior for long periods of execution. Third, requests going to the shared cache can incur queuing delays as large as hundreds of cycles, exacerbating the effects of memory divergence. We propose a set of techniques, collectively called Memory Divergence Correction (MeDiC), that reduce the negative performance impact of memory divergence and cache queuing. MeDiC delivers an average speedup of 21.8%, and 20.1% higher energy efficiency, over a state-of-the-art GPU cache management mechanism across 15 different GPGPU applications.", "venue": "ArXiv", "authors": ["Rachata  Ausavarungnirun", "Saugata  Ghose", "Onur  Kayiran", "Gabriel H. Loh", "Chita R. Das", "Mahmut T. Kandemir", "Onur  Mutlu"], "year": 2018, "n_citations": 5}
{"id": 3787218, "s2_id": "835be493a3babf3356d3f86fae34761cf58429a5", "title": "A Reconfigurable Streaming Deep Convolutional Neural Network Accelerator for Internet of Things", "abstract": "Convolutional neural network (CNN) offers significant accuracy in image detection. To implement image detection using CNN in the Internet of Things (IoT) devices, a streaming hardware accelerator is proposed. The proposed accelerator optimizes the energy efficiency by avoiding unnecessary data movement. With unique filter decomposition technique, the accelerator can support arbitrary convolution window size. In addition, max-pooling function can be computed in parallel with convolution by using separate pooling unit, thus achieving throughput improvement. A prototype accelerator was implemented in TSMC 65-nm technology with a core size of 5 mm2. The accelerator can support major CNNs and achieve 152GOPS peak throughput and 434GOPS/W energy efficiency at 350 mW, making it a promising hardware accelerator for intelligent IoT devices.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Li  Du", "Yuan  Du", "Yilei  Li", "Junjie  Su", "Yen-Cheng  Kuan", "Chun-Chen  Liu", "Mau-Chung Frank  Chang"], "year": 2018, "n_citations": 96}
{"id": 3789046, "s2_id": "e47f1071d159a3b2e41cc93a3cb44f66f25e4d39", "title": "Relative Performance of a Multi-level Cache with Last-Level Cache Replacement: An Analytic Review", "abstract": "Current day processors employ multi-level cache hierarchy with one or two levels of private caches and a shared last-level cache (LLC). An efficient cache replacement policy at LLC is essential for reducing the off-chip memory transfer as well as conflict for memory bandwidth. Cache replacement techniques for inclusive LLCs may not be efficient for multilevel cache as it can be shared by enormous applications with varying access behavior, running simultaneously. One application may dominate another by flooding of cache requests and evicting the useful data of the other application. From the performance point of view, an exclusive LLC make the replacement policies more demanding, as compared to an inclusive LLC. This paper analyzes some of the existing replacement techniques on the LLC with their performance assessment.", "venue": "ArXiv", "authors": ["Bijay  Paikaray"], "year": 2013, "n_citations": 0}
{"id": 3790877, "s2_id": "92a6a0cad69347c2aab720642c115e9f13426320", "title": "Hardware Implementation of four byte per clock RC4 algorithm", "abstract": "In the field of cryptography till date the 2-byte in 1-clock is the best known RC4 hardware design [1], while 1-byte in 1-clock [2], and the 1-byte in 3 clocks [3][4] are the best known implementation. The design algorithm in[2] considers two consecutive bytes together and processes them in 2 clocks. The design [1] is a pipelining architecture of [2]. The design of 1-byte in 3-clocks is too much modular and clock hungry. In this paper considering the RC4 algorithm, as it is, a simpler RC4 hardware design providing higher throughput is proposed in which 6 different architecture has been proposed. In design 1, 1-byte is processed in 1-clock, design 2 is a dynamic KSA-PRGA architecture of Design 1. Design 3 can process 2 byte in a single clock, where as Design 4 is Dynamic KSA-PRGA architecture of Design 3. Design 5 and Design 6 are parallelization architecture design 2 and design 4 which can compute 4 byte in a single clock. The maturity in terms of throughput, power consumption and resource usage, has been achieved from design 1 to design 6. The RC4 encryption and decryption designs are respectively embedded on two FPGA boards as co-processor hardware, the communication between the two boards performed using Ethernet.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Amlan  Chakrabarti", "Ranjan  Ghosh"], "year": 2014, "n_citations": 4}
{"id": 3790910, "s2_id": "d320a5f344d0f4f339934ad742c9d388b6c47140", "title": "A Systematic Study of Lattice-based NIST PQC Algorithms: from Reference Implementations to Hardware Accelerators", "abstract": "Security of currently deployed public key cryptography algorithms is foreseen to be vulnerable against quantum computer attacks. Hence, a community effort exists to develop post-quantum cryptography (PQC) algorithms, i.e., algorithms that are resistant to quantum attacks. In this work, we have investigated how lattice-based candidate algorithms from the NIST PQC standardization competition fare when conceived as hardware accelerators. To achieve this, we have assessed the reference implementations of selected algorithms with the goal of identifying what are their basic building blocks. We assume the hardware accelerators will be implemented in application specific integrated circuit (ASIC) and the targeted technology in our experiments is a commercial 65nm node. In order to estimate the characteristics of each algorithm, we have assessed their memory requirements, use of multipliers, and how each algorithm employs hashing functions. Furthermore, for these building blocks, we have collected area and power figures for 12 candidate algorithms. For memories, we make use of a commercial memory compiler. For logic, we make use of a standard cell library. In order to compare the candidate algorithms fairly, we select a reference frequency of operation of 500MHz. Our results reveal that our area and power numbers are comparable to the state of the art, despite targeting a higher frequency of operation and a higher security level in our experiments. The comprehensive investigation of lattice-based NIST PQC algorithms performed in this paper can be used for guiding ASIC designers when selecting an appropriate algorithm while respecting requirements and design constraints.", "venue": "ArXiv", "authors": ["Malik  Imran", "Zain Ul Abideen", "Samuel  Pagliarini"], "year": 2020, "n_citations": 2}
{"id": 3794953, "s2_id": "fa834678c2f59df95a8d0168d58e72f98e8bfc7c", "title": "Combined Integer and Variable Precision (CIVP) Floating Point Multiplication Architecture for FPGAs", "abstract": "In this paper, we propose an architecture/methodology for making FPGAs suitable for integer as well as variable precision floating point multiplication. The proposed work will of great importance in applications which requires variable precision floating point multiplication such as multi-media processing applications. In the proposed architecture/methodology, we propose the replacement of existing 18x18 bit and 25x18 bit dedicated multipliers in FPGAs with dedicated 24x24 bit and 24x9 bit multipliers, respectively. We have proved that our approach of providing the dedicated 24x24 bit and 24x9 bit multipliers in FPGAs will make them efficient for performing integer as well as single precision, double precision, and Quadruple precision floating point multiplications.", "venue": "PDPTA", "authors": ["Himanshu  Thapliyal", "Hamid R. Arabnia", "Rajnish  Bajpai", "Kamal K. Sharma"], "year": 2007, "n_citations": 15}
{"id": 3797347, "s2_id": "9cd7d167897f686e9099cf9ed0a09ad959b2a945", "title": "4-Bit High-Speed Binary Ling Adder", "abstract": "Binary addition is one of the most primitive and most commonly used applications in computer arithmetic. A large variety of algorithms and implementations have been proposed for binary addition. Huey Ling proposed a simpler form of CLA equations which rely on adjacent pair bits. Along with bit generate and bit propagate, we introduce another prefix bit, the half sum bit. Ling adder increases the speed of n-bit binary addition, which is an upgrade from the existing Carry-Look-Ahead adder. Several variants of the carry look-ahead equations, like Ling carries, have been presented that simplify carry computation and can lead to faster structures. Ling adders, make use of Ling carry and propagate bits, in order to calculate the sum bit. As a result, dependency on the previous bit addition is reduced; that is, ripple effect is lowered. This paper provides a comparative study on the implementation of the above mentioned high-speed adders.", "venue": "ArXiv", "authors": ["Projjal  Gupta"], "year": 2019, "n_citations": 0}
{"id": 3803466, "s2_id": "24714752eb2a606e6097994bf09c0f3a5cde6031", "title": "Union: A Unified HW-SW Co-Design Ecosystem in MLIR for Evaluating Tensor Operations on Spatial Accelerators", "abstract": "To meet the extreme compute demands for deep learning across commercial and scientific applications, dataflow accelerators are becoming increasingly popular. While these \u201cdomain-specific\u201d accelerators are not fully programmable like CPUs and GPUs, they retain varying levels of flexibility with respect to data orchestration, i.e., dataflow and tiling optimizations to enhance efficiency. There are several challenges when designing new algorithms and mapping approaches to execute the algorithms for a target problem on new hardware. Previous works have addressed these challenges individually. To address this challenge as a whole, in this work, we present a HW-SW codesign ecosystem for spatial accelerators called Union11https://github.com/union-codesign/union within the popular MLIR compiler infrastructure. Our framework allows exploring different algorithms and their mappings on several accelerator cost models. Union also includes a plug-and-play library of accelerator cost models and mappers which can easily be extended. The algorithms and accelerator cost models are connected via a novel mapping abstraction that captures the map space of spatial accelerators which can be systematically pruned based on constraints from the hardware, workload, and mapper. We demonstrate the value of Union for the community with several case studies which examine offloading different tensor operations (CONV/GEMM/Tensor Contraction) on diverse accelerator architectures using different mapping schemes.", "venue": "2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)", "authors": ["Geonhwa  Jeong", "Gokcen  Kestor", "Prasanth  Chatarasi", "Angshuman  Parashar", "Po-An  Tsai", "Sivasankaran  Rajamanickam", "Roberto  Gioiosa", "Tushar  Krishna"], "year": 2021, "n_citations": 1}
{"id": 3808366, "s2_id": "0b8d91fce0bc565bf9cf9daef4f75fe88c3cb3d2", "title": "A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks", "abstract": "FPGA-based hardware accelerators for convolutional neural networks (CNNs) have received attention due to their higher energy efficiency than GPUs. However, it is challenging for FPGA-based solutions to achieve a higher throughput than GPU counterparts. In this article, we demonstrate that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is trained with binary constraints on weights and activations. Specifically, we propose an optimized fully mapped FPGA accelerator architecture tailored for bitwise convolution and normalization that features massive spatial parallelism with deep pipelines stages. A key advantage of the FPGA accelerator is that its performance is insensitive to data batch size, while the performance of GPU acceleration varies largely depending on the batch size of the data. Experiment results show that the proposed accelerator architecture for binary CNNs running on a Virtex-7 FPGA is 8.3\u00d7 faster and 75\u00d7 more energy-efficient than a Titan X GPU for processing online individual requests in small batch sizes. For processing static data in large batch sizes, the proposed solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5\u00d7 higher energy efficiency.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Yixing  Li", "Zichuan  Liu", "Kai  Xu", "Hao  Yu", "Fengbo  Ren"], "year": 2018, "n_citations": 31}
{"id": 3810590, "s2_id": "6754c28691167c8140f0d0c8ee183473ca064d85", "title": "Application Specific Hardware Design Simulation for High Performance Embedded System", "abstract": "specific simulation is challenging task in various real time high performance embedded devices. In this study specific application is implemented with the help of Xilinx. Xilinx provides SDK and XPS tools, XPS tools used for develop complete hardware platform and SDK provides software platform for application creation and verification. Xilinx XUP-5 board have been used and implemented various specific Applications with hardware platform. In this study the base instruction set with customized instructions, supported with specific hardware resources are analyzed. Keywordsvirtex-5 FPGA board, simulation, hardware and software design, Xilinx Platform Studio.", "venue": "ArXiv", "authors": ["Ravi  Khatwal", "Manoj Kumar Jain"], "year": 2014, "n_citations": 0}
{"id": 3812095, "s2_id": "2f11636b0bffbaa625676f339f7448dbff00cab6", "title": "A New Reversible TSG Gate and Its Application For Designing Efficient Adder Circuits", "abstract": "In the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. This paper proposes a new 4 * 4 reversible gate called TSG gate. The proposed gate is used to design efficient adder units. The most significant aspect of the proposed gate is that it can work singly as a reversible full adder i.e reversible full adder can now be implemented with a single gate only. The proposed gate is then used to design reversible ripple carry and carry skip adders. It is demonstrated that the adder architectures designed using the proposed gate are much better and optimized, compared to their existing counterparts in literature; in terms of number of reversible gates and garbage outputs. Thus, this paper provides the initial threshold to building of more complex system which can execute more complicated operations using reversible logic.", "venue": "ArXiv", "authors": ["Himanshu  Thapliyal", "M. B. Srinivas"], "year": 2006, "n_citations": 47}
{"id": 3816366, "s2_id": "d7495d6967c4bc2f8d8958180f2da4b8afa1ecb7", "title": "Comparative Analysis of Polynomial and Rational Approximations of Hyperbolic Tangent Function for VLSI Implementation", "abstract": "Deep neural networks yield the state-of-the-art results in many computer vision and human machine interface applications such as object detection, speech recognition etc. Since, these networks are computationally expensive, customized accelerators are designed for achieving the required performance at lower cost and power. One of the key building blocks of these neural networks is non-linear activation function such as sigmoid, hyperbolic tangent (tanh), and ReLU. A low complexity accurate hardware implementation of the activation function is required to meet the performance and area targets of the neural network accelerators. Even though, various methods and implementations of tanh activation function have been published, a comparative study is missing. This paper presents comparative analysis of polynomial and rational methods and their hardware implementation.", "venue": "ArXiv", "authors": ["Mahesh  Chandra"], "year": 2020, "n_citations": 0}
{"id": 3817140, "s2_id": "653340e7e17b9ddddf11bc9dd24dc41a96752e68", "title": "Strawberry Detection Using a Heterogeneous Multi-Processor Platform", "abstract": "Over the last few years, the number of precision farming projects has increased specifically in harvesting robots and many of which have made continued progress from identifying crops to grasping the desired fruit or vegetable. One of the most common issues found in precision farming projects is that successful application is heavily dependent not just on identifying the fruit but also on ensuring that localisation allows for accurate navigation. These issues become significant factors when the robot is not operating in a prearranged environment, or when vegetation becomes too thick, thus covering crop. Moreover, running a state-of-the-art deep learning algorithm on an embedded platform is also very challenging, resulting most of the times in low frame rates. This paper proposes using the You Only Look Once version 3 (YOLOv3) Convolutional Neural Network (CNN) in combination with utilising image processing techniques for the application of precision farming robots targeting strawberry detection, accelerated on a heterogeneous multiprocessor platform. The results show a performance acceleration by five times when implemented on a Field-Programmable Gate Array (FPGA) when compared with the same algorithm running on the processor side with an accuracy of 78.3\\% over the test set comprised of 146 images.", "venue": "ArXiv", "authors": ["Samuel  Brandenburg", "Pedro  Machado", "Nikesh  Lama", "T. M. McGinnity"], "year": 2020, "n_citations": 0}
{"id": 3827892, "s2_id": "dbda34036ab4116f44a7e8600208db9d27afedcb", "title": "DATE: Defense Against TEmperature Side-Channel Attacks in DVFS Enabled MPSoCs", "abstract": "Given the constant rise in utilizing embedded devices in daily life, side channels remain a challenge to information flow control and security in such systems. One such important security flaw could be exploited through temperature side-channel attacks, where heat dissipation and propagation from the processing elements are observed over time in order to deduce security flaws. In our proposed methodology, DATE: Defense Against TEmperature side-channel attacks, we propose a novel approach of reducing spatial and temporal thermal gradient, which makes the system more secure against temperature side-channel attacks, and at the same time increases the reliability of the device in terms of lifespan. In this paper, we have also introduced a new metric, Thermal-Security-in-Multi-Processors (TSMP), which is capable of quantifying the security against temperature side-channel attacks on computing systems, and DATE is evaluated to be 139.24% more secure at the most for certain applications than the state-of-the-art, while reducing thermal cycle by 67.42% at the most.", "venue": "ArXiv", "authors": ["Somdip  Dey", "Amit Kumar Singh", "Xiaohang  Wang", "Klaus Dieter McDonald-Maier"], "year": 2020, "n_citations": 0}
{"id": 3830565, "s2_id": "7645bc0375a0a59fa4aace1f37061776942d0bab", "title": "Hardware Memory Management for Future Mobile Hybrid Memory Systems", "abstract": "The current mobile applications have rapidly growing memory footprints, posing a great challenge for memory system design. Insufficient DRAM main memory will incur frequent data swaps between memory and storage, a process that hurts performance, consumes energy, and deteriorates the write endurance of typical flash storage devices. Alternately, a larger DRAM has higher leakage power and drains the battery faster. Furthermore, DRAM scaling trends make further growth of DRAM in the mobile space prohibitive due to cost. Emerging nonvolatile memory (NVM) has the potential to alleviate these issues due to its higher capacity per cost than DRAM and minimal static power. Recently, a wide spectrum of NVM technologies, including phase-change memories (PCMs), memristor, and 3-D XPoint has emerged. Despite the mentioned advantages, NVM has longer access latency compared to DRAM and NVM writes can incur higher latencies and wear costs. Therefore, the integration of these new memory technologies in the memory hierarchy requires a fundamental rearchitecting of traditional system designs. In this work, we propose a hardware-accelerated memory manager (HMMU) that addresses in a flat address space, with a small partition of the DRAM reserved for subpage block-level management. We design a set of data placement and data migration policies within this memory manager such that we may exploit the advantages of each memory technology. By augmenting the system with this HMMU, we reduce the overall memory latency while also reducing writes to the NVM. The experimental results show that our design achieves a 39% reduction in energy consumption with only a 12% performance degradation versus an all-DRAM baseline that is likely untenable in the future.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Fei  Wen", "Mian  Qin", "Paul  Gratz", "Narasimha  Reddy"], "year": 2020, "n_citations": 2}
{"id": 3831827, "s2_id": "f03b41a842d095edc681cd6fb1bc904828fc1276", "title": "A Non-anchored Unified Naming System for Ad Hoc Computing Environments", "abstract": "A ubiquitous computing environment consists of many resources that need to be identified by users and applications. Users and developers require some way to identify resources by human readable names. In addition, ubiquitous computing environments impose additional requirements such as the ability to work well with ad hoc situations and the provision of names that depend on context. \nThe Non-anchored Unified Naming (NUN) system was designed to satisfy these requirements. It is based on relative naming among resources and provides the ability to name arbitrary types of resources. By having resources themselves take part in naming, resources are able to able contribute their specialized knowledge into the name resolution process, making context-dependent mapping of names to resources possible. The ease of which new resource types can be added makes it simple to incorporate new types of contextual information within names. \nIn this paper, we describe the naming system and evaluate its use.", "venue": "ArXiv", "authors": ["Yoo Chul Chung", "Dongman  Lee"], "year": 2006, "n_citations": 1}
{"id": 3833847, "s2_id": "7931a3b7ab95831d9f2e9dca8f148a215698e121", "title": "MELOPPR: Software/Hardware Co-design for Memory-efficient Low-latency Personalized PageRank", "abstract": "Personalized PageRank (PPR) is a graph algorithm that evaluates the importance of the surrounding nodes from a source node. Widely used in social network related applications such as recommender systems, PPR requires real-time responses (latency) for a better user experience. Existing works either focus on algorithmic optimization for improving precision while neglecting hardware implementations or focus on distributed global graph processing on large-scale systems for improving throughput rather than response time. Optimizing low-latency local PPR algorithm with a tight memory budget on edge devices remains unexplored. In this work, we propose a memory-efficient, low-latency PPR solution, namely MeLoPPR, with largely reduced memory requirement and a flexible trade-off between latency and precision. MeLoPPR is composed of stage decomposition and linear decomposition and exploits the node score sparsity: Through stage and linear decomposition, MeLoPPR breaks the computation on a large graph into a set of smaller sub-graphs, that significantly saves the computation memory; Through sparsity exploitation, MeLoPPR selectively chooses the sub-graphs that contribute the most to the precision to reduce the required computation. In addition, through software/hardware co-design, we propose a hardware implementation on a hybrid CPU and FPGA accelerating platform, that further speeds up the sub-graph computation. We evaluate the proposed MeLoPPR on memory-constrained devices including a personal laptop and Xilinx Kintex-7 KC705 FPGA using six real-world graphs. First, MeLoPPR demonstrates significant memory saving by $1. 5 \\times \\sim 13. 4 \\times$ on CPU and $73 \\times \\sim 8699 \\times$ on FPGA. Second, MeLoPPR allows flexible trade-offs between precision and execution time: when the precision is 80%, the speedup on CPU is up to $15\\times$ and up to $707\\times$ on FPGA; when the precision is around 90%, the speedup is up to $70\\times$ on FPGA.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Lixiang  Li", "Yao  Chen", "Zacharie  Zirnheld", "Pan  Li", "Cong  Hao"], "year": 2021, "n_citations": 0}
{"id": 3834034, "s2_id": "9e758c8898880c573be0824a964e8630db8fa6c5", "title": "Evaluation of the Sensitivity of RRAM Cells to Optical Fault Injection Attacks", "abstract": "Resistive Random Access Memory (RRAM) is a type of Non-Volatile Memory (NVM). In this paper we investigate the sensitivity of the TiN/Ti/Al:HfO2/TiN-based 1T-1R RRAM cells implemented in a 250 nm CMOS IHP technology to the laser irradiation in detail. Experimental results show the feasibility to influence the state of the cells under laser irradiation, i.e. successful optical Fault Injection. We focus on the selection of the parameters of the laser station and their influence on the success of optical Fault Injections.", "venue": "2020 23rd Euromicro Conference on Digital System Design (DSD)", "authors": ["Dmytro  Petryk", "Zoya  Dyka", "Eduardo  P\u00e9rez", "Mamathamba Kalishettyhalli Mahadevaiaha", "Ievgen  Kabin", "Christian  Wenger", "Peter  Langend\u00f6rfer"], "year": 2020, "n_citations": 3}
{"id": 3834104, "s2_id": "dc43b15ffc6f78d3a5aefd94b9a48f18c40259e5", "title": "FPRaker: A Processing Element For Accelerating Neural Network Training", "abstract": "We present FPRaker, a processing element for composing training accelerators. FPRaker processes several floating-point multiply-accumulation operations concurrently and accumulates their result into a higher precision accumulator. FPRaker boosts performance and energy efficiency during training by taking advantage of the values that naturally appear during training. It processes the significand of the operands of each multiply-accumulate as a series of signed powers of two. The conversion to this form is done on-the-fly. This exposes ineffectual work that can be skipped: values when encoded have few terms and some of them can be discarded as they would fall outside the range of the accumulator given the limited precision of floating-point. FPRaker also takes advantage of spatial correlation in values across channels and uses delta-encoding off-chip to reduce memory footprint and bandwidth. We demonstrate that FPRaker can be used to compose an accelerator for training and that it can improve performance and energy efficiency compared to using optimized bit-parallel floating-point units under iso-compute area constraints. We also demonstrate that FPRaker delivers additional benefits when training incorporates pruning and quantization. Finally, we show that FPRaker naturally amplifies performance with training methods that use a different precision per layer.", "venue": "MICRO", "authors": ["Omar Mohamed Awad", "Mostafa  Mahmoud", "Isak Edo Vivancos", "Ali Hadi Zadeh", "Ciaran  Bannon", "Anand  Jayarajan", "Gennady  Pekhimenko", "Andreas  Moshovos"], "year": 2021, "n_citations": 1}
{"id": 3838150, "s2_id": "7ef606c6d421491142eae7701143ee01c0fe49e2", "title": "Implementing CNN Layers on the Manticore Cluster-Based Many-Core Architecture", "abstract": "This document presents implementations of fundamental convolutional neural network (CNN) layers on the Manticore cluster-based many-core architecture and discusses their characteristics and trade-offs.", "venue": "ArXiv", "authors": ["Andreas  Kurth", "Fabian  Schuiki", "Luca  Benini"], "year": 2021, "n_citations": 0}
{"id": 3845470, "s2_id": "7da60005d6830c4bef3041891f01fdd035a39347", "title": "Hardware realization of residue number system algorithms by Boolean functions minimization", "abstract": "Residue number systems (RNS) represent numbers by their remainders modulo a set of relatively prime numbers. This paper pro- poses an efficient hardware implementation of modular multiplication and of the modulo function (X(mod P)), based on Boolean minimiza- tion. We report experiments showing a performance advantage up to 30 times for our approach vs. the results obtained by state-of-art industrial tools.", "venue": "ArXiv", "authors": ["Danila A. Gorodecky", "Tiziano  Villa"], "year": 2018, "n_citations": 0}
{"id": 3847729, "s2_id": "d7dfd76503d498c47cb6223ad3c4b7b3298bac17", "title": "LUXOR: An FPGA Logic Cell Architecture for Efficient Compressor Tree Implementations", "abstract": "We propose two tiers of modifications to FPGA logic cell architecture to deliver a variety of performance and utilization benefits with only minor area overheads. In the first tier, we augment existing commercial logic cell datapaths with a 6-input XOR gate in order to improve the expressiveness of each element, while maintaining backward compatibility. This new architecture is vendor-agnostic, and we refer to it as LUXOR. We also consider a secondary tier of vendor-specific modifications to both Xilinx and Intel FPGAs, which we refer to as X-LUXOR+ and I-LUXOR+ respectively. We demonstrate that compressor tree synthesis using generalized parallel counters (GPCs) is further improved with the proposed modifications. Using both the Intel adaptive logic module and the Xilinx slice at the 65nm technology node for a comparative study, it is shown that the silicon area overhead is less than 0.5% for LUXOR and 5-6% for LUXOR+, while the delay increments are 1-6% and 3-9% respectively. We demonstrate that LUXOR can deliver an average reduction of 13-19% in logic utilization on micro-benchmarks from a variety of domains. BNN benchmarks benefit the most with an average reduction of 37-47% in logic utilization, which is due to the highly-efficient mapping of the XnorPopcount operation on our proposed LUXOR+ logic cells.", "venue": "FPGA", "authors": ["Seyedramin  Rasoulinezhad", "Siddhartha", "Hao  Zhou", "Lingli  Wang", "David  Boland", "Philip H. W. Leong"], "year": 2020, "n_citations": 4}
{"id": 3849836, "s2_id": "4533aea0bf9227ef8f566d69e246f3447d427576", "title": "Memory-Efficient Dataflow Inference for Deep CNNs on FPGA", "abstract": "Custom dataflow Convolutional Neural Network (CNN) inference accelerators on FPGA are tailored to a specific CNN topology and store parameters in On-Chip Memory (OCM), creating the potential for high energy efficiency and low inference latency. However, in these accelerators the shapes of parameter memories are dictated by throughput constraints and do not map well to the underlying OCM, which becomes an implementation bottleneck. In this work, we propose an accelerator design methodology - Frequency Compensated Memory Packing (FCMP) - which improves the OCM utilization efficiency of dataflow accelerators with minimal reduction in throughput and no modifications to the physical structure of FPGA OCM. To validate our methodology, we apply it to several realizations of medium-sized CIFAR-10 inference accelerators and demonstrate up to 30% reduction in OCM utilization without loss of inference throughput, allowing us to port the accelerators from Xilinx Zynq 7020 to 7012S, reducing application cost. We also implement a custom dataflow FPGA inference accelerator for a quantized ResNet-50 CNN, utilizing on-chip weights, the largest topology ever implemented with this accelerator architecture. We demonstrate that by applying FCMP to the ResNet accelerator, the OCM bottleneck is alleviated which enables the accelerator to be ported from Alveo U250 to the smaller Alveo U280 board with less throughput loss compared to alternative techniques. By providing a finer-grained trade off between throughput and OCM requirements, FCMP increases the flexibility of custom dataflow CNN inference designs on FPGA.", "venue": "2020 International Conference on Field-Programmable Technology (ICFPT)", "authors": ["Lucian  Petrica", "Tobias  Alonso", "Mairin  Kroes", "Nicholas  Fraser", "Sorin  Cotofana", "Michaela  Blott"], "year": 2020, "n_citations": 6}
{"id": 3850331, "s2_id": "bb8dd697719abbd35b843801fa5d3b252a991023", "title": "Closed-Loop Neural Interfaces with Embedded Machine Learning", "abstract": "Neural interfaces capable of multi-site electrical recording, on-site signal classification, and closed-loop therapy are critical for the diagnosis and treatment of neurological disorders. However, deploying machine learning algorithms on low-power neural devices is challenging, given the tight constraints on computational and memory resources for such devices. In this paper, we review the recent developments in embedding machine learning in neural interfaces, with a focus on design trade-offs and hardware efficiency. We also present our optimized tree-based model for low-power and memory-efficient classification of neural signal in brain implants. Using energy-aware learning and model compression, we show that the proposed oblique trees can outperform conventional machine learning models in applications such as seizure or tremor detection and motor decoding.", "venue": "2020 27th IEEE International Conference on Electronics, Circuits and Systems (ICECS)", "authors": ["Bingzhao  Zhu", "Uisub  Shin", "Mahsa  Shoaran"], "year": 2020, "n_citations": 6}
{"id": 3850710, "s2_id": "c68daddb818d46d95711e27e182ef293c24e29ed", "title": "An Approximate Carry Estimating Simultaneous Adder with Rectification", "abstract": "Approximate computing has in recent times found significant applications towards lowering power, area, and time requirements for arithmetic operations. Several works done in recent years have furthered approximate computing along these directions. In this work, we propose a new approximate adder that employs a carry prediction method. This allows parallel propagation of the carry allowing faster calculations. In addition to the basic adder design, we also propose a rectification logic which would enable higher accuracy for larger computations. Experimental results show that our adder produces results 91.2% faster than the conventional ripple-carry adder. In terms of accuracy, the addition of rectification logic to the basic design produces results that are more accurate than state-of-the-art adders like SARA[13] and BCSA[5] by 74%.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Rajat  Bhattacharjya", "Vishesh  Mishra", "Saurabh  Singh", "Kaustav  Goswami", "Dip Sankar Banerjee"], "year": 2020, "n_citations": 3}
{"id": 3856119, "s2_id": "b248c9a1a421702a61cab3042f97114ea9ca6d90", "title": "BRDS: An FPGA-based LSTM Accelerator with Row-Balanced Dual-Ratio Sparsification", "abstract": "In this paper, first, a hardware-friendly pruning algorithm for reducing energy consumption and improving the speed of Long Short-Term Memory (LSTM) neural network accelerators is presented. Next, an FPGA-based platform for efficient execution of the pruned networks based on the proposed algorithm is introduced. By considering the sensitivity of two weight matrices of the LSTM models in pruning, different sparsity ratios (i.e., dual-ratio sparsity) are applied to these weight matrices. To reduce memory accesses, a row-wise sparsity pattern is adopted. The proposed hardware architecture makes use of computation overlapping and pipelining to achieve low-power and high-speed. The effectiveness of the proposed pruning algorithm and accelerator is assessed under some benchmarks for natural language processing, binary sentiment classification, and speech recognition. Results show that, e.g., compared to a recently published work in this field, the proposed accelerator could provide up to 272% higher effective GOPS/W and the perplexity error is reduced by up to 1.4% for the PTB dataset.", "venue": "ArXiv", "authors": ["Seyed Abolfazl Ghasemzadeh", "Erfan  Bank-Tavakoli", "Mehdi  Kamal", "Ali  Afzali-Kusha", "Massoud  Pedram"], "year": 2021, "n_citations": 3}
{"id": 3858911, "s2_id": "2939edbcf0c36f071197e9f62142f70a987bce3e", "title": "Enabling Cross-Layer Reliability and Functional Safety Assessment Through ML-Based Compact Models", "abstract": "Typical design flows are hierarchical and rely on assembling many individual technology elements from standard cells to complete boards. Providers use compact models to provide simplified views of their products to their users. Designers group simpler elements in more complex structures and have to manage the corresponding propagation of reliability and functional safety information through the hierarchy of the system, accompanied by the obvious problems of IP confidentiality, possibility of reverse engineering and so on. This paper proposes a machine-learningbased approach to integrate the many individual models of a subsystem\u2019s elements in a single compact model that can be reused and assembled further up in the hierarchy. The compact models provide consistency, accuracy and confidentiality, allowing technology, IP, component, sub-system or system providers to accompany their offering with high-quality reliability and functional safety compact models that can be safely and accurately consumed by their users.", "venue": "2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS)", "authors": ["Dan  Alexandrescu", "Aneesh  Balakrishnan", "Thomas  Lange", "Maximilien  Glorieux"], "year": 2020, "n_citations": 0}
{"id": 3859298, "s2_id": "836ae3345da0134a7670362d5bc8b277d28cc476", "title": "Acceleration of probabilistic reasoning through custom processor architecture", "abstract": "Probabilistic reasoning is an essential tool for robust decision-making systems because of its ability to explicitly handle real-world uncertainty, constraints and causal relations. Consequently, researchers are developing hybrid models by combining Deep Learning with probabilistic reasoning for safety-critical applications like self-driving vehicles, autonomous drones, etc. However, probabilistic reasoning kernels do not execute efficiently on CPUs or GPUs. This paper, therefore, proposes a custom programmable processor to accelerate sum-product networks, an important probabilistic reasoning execution kernel. The processor has an optimized datapath architecture and memory hierarchy optimized for sum-product networks execution. Experimental results show that the processor, while requiring fewer computational and memory units, achieves a 12x throughput benefit over the Nvidia Jetson TX2 embedded GPU platform.", "venue": "2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Nimish  Shah", "Laura Isabel Galindez Olascoaga", "Wannes  Meert", "Marian  Verhelst"], "year": 2020, "n_citations": 4}
{"id": 3859694, "s2_id": "c657cd55feb8b80bb8a71c5e4b193407f1a895f1", "title": "Reversible Arithmetic Logic Unit", "abstract": "A function is reversible if each input vector produces a unique output vector. Reversible logic is of growing importance to many future computer technologies. In this paper, the design of a reversible Arithmetic Logic Unit (ALU) is presented making use of multiplexer unit as well as control signals. ALU is one of the most important components of CPU that can be part of a programmable reversible computing device such as a quantum computer. In multiplexer based ALU the operations are performed depending on the selection line. The control unit based ALU is developed with 9\u00ab elementary reversible gates for four basic arithmetic logical operations on two w-bit operands. The series of operations are performed on the same line depending on control signals, instead of selecting the desired result by a multiplexer. The later design is found to be advantageous over the former in terms of number of garbage outputs and constant inputs produced.", "venue": "2011 3rd International Conference on Electronics Computer Technology", "authors": ["Y.  Syamala", "A. V. N. Tilak"], "year": 2011, "n_citations": 64}
{"id": 3860409, "s2_id": "1833875a29dbef9703c33414160fbc575ec7bfa6", "title": "Intelligent bees for QoS routing in Networks-on-Chip", "abstract": "Networks-on-Chip (NoCs) for future many-core processor platforms integrate more and more heterogeneous components of different types and many real-time and latency-sensitive applications can run on a single chip concurrently. The reconfigurable FPGA and reconfigurable NoCs have emerged for the purpose of reusability. Those types' traffics within NoCs exhibit diverse, burst, and unpredictable communication patterns. QoS guaranteed mechanisms are necessary to provide guaranteed throughput (GT) or guaranteed bandwidth (GB) performance for NoCs. In this paper, we propose a QoS routing algorithm inspired by bees' foraging behaviors to provide guaranteed bandwidth performance. Virtual circuits and Spatial Division Multiplexing are employed to maintain available paths for different type's traffics.", "venue": "2010 Second Pacific-Asia Conference on Circuits, Communications and System", "authors": ["Peibo  Xie", "Huaxi  Gu"], "year": 2010, "n_citations": 1}
{"id": 3861116, "s2_id": "8f611e16757c8cf6b1cc554f7316687f52974b96", "title": "Indicating Asynchronous Array Multipliers", "abstract": "Multiplication is an important arithmetic operation that is frequently encountered in microprocessing and digital signal processing applications, and multiplication is physically realized using a multiplier. This paper discusses the physical implementation of many indicating asynchronous array multipliers, which are inherently elastic and modular and are robust to timing, process and parametric variations. We consider the physical realization of many indicating asynchronous array multipliers using a 32/28nm CMOS technology. The weak-indication array multipliers comprise strong-indication or weak-indication full adders, and strong-indication 2-input AND functions to realize the partial products. The multipliers were synthesized in a semi-custom ASIC design style using standard library cells including a custom-designed 2-input C-element. 4x4 and 8x8 multiplication operations were considered for the physical implementations. The 4-phase return-to-zero (RTZ) and the 4-phase return-to-one (RTO) handshake protocols were utilized for data communication, and the delay-insensitive dual-rail code was used for data encoding. Among several weak-indication array multipliers, a weak-indication array multiplier utilizing a biased weak-indication full adder and the strong-indication 2-input AND function is found to have reduced cycle time and power-cycle time product with respect to RTZ and RTO handshaking for 4x4 and 8x8 multiplications. Further, the 4-phase RTO handshaking is found to be preferable to the 4-phase RTZ handshaking for achieving enhanced optimizations of the design metrics.", "venue": "ArXiv", "authors": ["P  Balasubramanian", "D L Maskell"], "year": 2019, "n_citations": 0}
{"id": 3862530, "s2_id": "ec682eadc448f4a1665391fbeb4ce47c166301b2", "title": "IRC Cross-Layer Design Exploration of Intermittent Robust Computation Units for IoTs", "abstract": "Energy-harvesting-powered computing offers intriguing and vast opportunities to dramatically transform the landscape of the Internet of Things (IoT) devices by utilizing ambient sources of energy to achieve battery-free computing. In order to operate within the restricted energy capacity and intermittency profile, it is proposed to innovate Intermittent Robust Computation (IRC) Unit as a new duty-cycle-variable computing approach leveraging the non-volatility inherent in spin-based switching devices. The foundations of IRC will be advanced from the device-level upwards, by extending a Spin Hall Effect Magnetic Tunnel Junction (SHE-MTJ) device. The device will then be used to realize SHE-MTJ Majority/Polymorphic Gate (MG/PG) logic approaches and libraries. Then a Logic-Embedded Flip-Flop (LE-FF) is developed to realize rudimentary Boolean logic functions along with an inherent state-holding capability within a compact footprint. Finally, the NV-Clustering synthesis procedure and corresponding tool module are proposed to instantiate the LE-FF library cells within conventional Register Transfer Language (RTL) specifications. This selectively clusters together logic and NV state-holding functionality, based on energy and area minimization criteria. It also realizes middleware-coherent, intermittent computation without checkpointing, micro-tasking, or software bloat and energy overheads vital to IoT. Simulation results for various benchmark circuits including ISCAS-89 validate functionality and power dissipation, area, and delay benefits.", "venue": "2019 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Arman  Roohi", "Ronald F. DeMara"], "year": 2019, "n_citations": 0}
{"id": 3863684, "s2_id": "5535fec7822abc2edc8e4a45df5432426b45e138", "title": "D-RaNGe: Using Commodity DRAM Devices to Generate True Random Numbers with Low Latency and High Throughput", "abstract": "We propose a new DRAM-based true random number generator (TRNG) that leverages DRAM cells as an entropy source. The key idea is to intentionally violate the DRAM access timing parameters and use the resulting errors as the source of randomness. Our technique specifically decreases the DRAM row activation latency (timing parameter tRCD) below manufacturer-recommended specifications, to induce read errors, or activation failures, that exhibit true random behavior. We then aggregate the resulting data from multiple cells to obtain a TRNG capable of providing a high throughput of random numbers at low latency. \nTo demonstrate that our TRNG design is viable using commodity DRAM chips, we rigorously characterize the behavior of activation failures in 282 state-of-the-art LPDDR4 devices from three major DRAM manufacturers. We verify our observations using four additional DDR3 DRAM devices from the same manufacturers. Our results show that many cells in each device produce random data that remains robust over both time and temperature variation. We use our observations to develop D-RanGe, a methodology for extracting true random numbers from commodity DRAM devices with high throughput and low latency by deliberately violating the read access timing parameters. We evaluate the quality of our TRNG using the commonly-used NIST statistical test suite for randomness and find that D-RaNGe: 1) successfully passes each test, and 2) generates true random numbers with over two orders of magnitude higher throughput than the previous highest-throughput DRAM-based TRNG.", "venue": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Jeremie S. Kim", "Minesh  Patel", "Hasan  Hassan", "Lois  Orosa", "Onur  Mutlu"], "year": 2019, "n_citations": 46}
{"id": 3864652, "s2_id": "5c7a33033911bf54d25be6bb22ea214ad91be92f", "title": "Policies of System Level Pipeline Modeling", "abstract": "Pipelining is a well understood and often used implementation technique for increasing the performance of a hardware system. We develop several SystemC/C++ modeling techniques that allow us to quickly model, simulate, and evaluate pipelines. We employ a small domain specific language (DSL) based on resource usage patterns that automates the drudgery of boilerplate code needed to configure connectivity in simulation models. The DSL is embedded directly in the host modeling language SystemC/C++. Additionally we develop several techniques for parameterizing a pipeline's behavior based on policies of function, communication, and timing (performance modeling).", "venue": "Electron. Notes Theor. Comput. Sci.", "authors": ["Edwin A. Harcourt"], "year": 2009, "n_citations": 1}
{"id": 3865724, "s2_id": "d3ea1141b7e8abf1f97cf0a0b6016f9e20004527", "title": "DVS for on-chip bus designs based on timing error correction", "abstract": "On-chip buses are typically designed to meet performance constraints for worst-case conditions, including process corner, temperature, IR-drop, and neighboring net switching pattern. This can result in significant performance slack at more typical operating conditions. We propose a dynamic voltage scaling (DVS) technique for buses, based on a double sampling latch which can detect and correct for delay errors without the need for retransmission. The proposed approach recovers the available slack at non-worst-case operating points through more aggressive voltage scaling and track changing conditions by monitoring the error recovery rate. Voltage margins needed in traditional designs to accommodate worst-case performance conditions are therefore eliminated, resulting in a significant improvement in energy efficiency. The approach was implemented for a 6 mm memory read bus operating at 1.5 GHz (0.13 /spl mu/m technology node) and was simulated for a number of benchmark programs. Even at the worst-case process and environment conditions, energy gains of up to 17% are achieved, with error recovery rates under 2.3%. At more typical process and environment conditions, energy gains range from 35% to 45%, with a performance degradation under 2%. An analysis of optimum interconnect architectures for maximizing energy gains with this approach shows that the proposed approach performs well with technology scaling.", "venue": "Design, Automation and Test in Europe", "authors": ["Himanshu  Kaul", "Dennis  Sylvester", "David  Blaauw", "Trevor N. Mudge", "Todd M. Austin"], "year": 2005, "n_citations": 15}
{"id": 3867293, "s2_id": "7ff71248cfd3343e3e22566ab63ff6baa52c01e4", "title": "Design Space Exploration of Algorithmic Multi-Port Memories in High-Performance Application-Specific Accelerators", "abstract": "Memory load/store instructions consume an important part in execution time and energy consumption in domain-specific accelerators. For designing highly parallel systems, available parallelism at each granularity is extracted from the workloads. The maximal use of parallelism at each granularity in these high-performance designs requires the utilization of multi-port memories. Currently, true multiport designs are less popular because there is no inherent EDA support for multiport memory beyond 2-ports, utilizing more ports requires circuit-level implementation and hence a high design time. In this work, we present a framework for Design Space Exploration of Algorithmic Multi-Port Memories (AMM) in ASICs. We study different AMM designs in the literature, discuss how we incorporate them in the Pre-RTL Aladdin Framework with different memory depth, port configurations and banking structures. From our analysis on selected applications from the MachSuite (accelerator benchmark suite), we understand and quantify the potential use of AMMs (as true multiport memories) for high performance in applications with low spatial locality in memory access patterns.", "venue": "ArXiv", "authors": ["Khushal  Sethi"], "year": 2020, "n_citations": 0}
{"id": 3867424, "s2_id": "bf4a9f32bcce640511fb4fd538c97ca86616a9c5", "title": "Energy-efficient hybrid stochastic-binary neural networks for near-sensor computing", "abstract": "Recent advances in neural networks (NNs) exhibit unprecedented success at transforming large, unstructured data streams into compact higher-level semantic information for tasks such as handwriting recognition, image classification, and speech recognition. Ideally, systems would employ near-sensor computation to execute these tasks at sensor endpoints to maximize data reduction and minimize data movement. However, near-sensor computing presents its own set of challenges such as operating power constraints, energy budgets, and communication bandwidth capacities. In this paper, we propose a stochastic-binary hybrid design which splits the computation between the stochastic and binary domains for near-sensor NN applications. In addition, our design uses a new stochastic adder and multiplier that are significantly more accurate than existing adders and multipliers. We also show that retraining the binary portion of the NN computation can compensate for precision losses introduced by shorter stochastic bit-streams, allowing faster run times at minimal accuracy losses. Our evaluation shows that our hybrid stochastic-binary design can achieve 9.8x energy efficiency savings, and application-level accuracies within 0.05% compared to conventional all-binary designs.", "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017", "authors": ["Vincent T. Lee", "Armin  Alaghi", "John P. Hayes", "Visvesh  Sathe", "Luis  Ceze"], "year": 2017, "n_citations": 63}
{"id": 3869191, "s2_id": "7fcaf86ef7c72a83d65a65d01cb0b1bfbc5b868e", "title": "Systolic arrays for lattice-reduction-aided mimo detection", "abstract": "Multiple-input multiple-output (MIMO) technology provides high data rate and enhanced quality of service for wireless communications. Since the benefits from MIMO result in a heavy computational load in detectors, the design of low-complexity sub- optimum receivers is currently an active area of research. Lattice- reduction-aided detection (LRAD) has been shown to be an effective low-complexity method with near-maximum-likelihood performance. In this paper, we advocate the use of systolic array architectures for MIMO receivers, and in particular we exhibit one of them based on LRAD. The \"Lenstra-Lenstra-Lovasz (LLL) lattice reduction algorithm\" and the ensuing linear detections or successive spatial-interference cancellations can be located in the same array, which is considerably hardware-efficient. Since the conventional form of the LLL algorithm is not immediately suitable for parallel processing, two modified LLL algorithms are considered here for the systolic array. LLL algorithm with full-size reduction- LLL is one of the versions more suitable for parallel processing. Another variant is the all-swap lattice-reduction (ASLR) algorithm for complex-valued lattices, which processes all lattice basis vectors simultaneously within one iteration. Our novel systolic array can operate both algorithms with different external logic controls. In order to simplify the systolic array design, we replace the Lovasz condition in the definition of LLL-reduced lattice with the looser Siegel condition. Simulation results show that for LR-aided linear detections, the bit-error-rate performance is still maintained with this relaxation. Comparisons between the two algorithms in terms of bit-error-rate performance, and average field-programmable gate array processing time in the systolic array are made, which shows that ASLR is a better choice for a systolic architecture, especially for systems with a large number of antennas.", "venue": "Journal of Communications and Networks", "authors": ["Ni-Chun  Wang", "Ezio  Biglieri", "Kung  Yao"], "year": 2011, "n_citations": 6}
{"id": 3871623, "s2_id": "970c06be6c27da311b6e129c5a9d3141d3ff75ad", "title": "Enabling multi-programming mechanism for quantum computing in the NISQ era", "abstract": "As NISQ devices have several physical limitations and unavoidable noisy quantum operations, only small circuits can be executed on a quantum machine to get reliable results. This leads to the quantum hardware under-utilization issue. Here, we address this problem and improve the quantum hardware throughput by proposing a multiprogramming approach to execute multiple quantum circuits on quantum hardware simultaneously. We first introduce a parallelism manager to select an appropriate number of circuits to be executed at the same time. Second, we present two different qubit partitioning algorithms to allocate reliable partitions to multiple circuits \u2013 a greedy and a heuristic. Third, we use the Simultaneous Randomized Benchmarking protocol to characterize the crosstalk properties and consider them in the qubit partition process to avoid crosstalk effect during simultaneous executions. Finally, we enhance the mapping transition algorithm to make circuits executable on hardware using a decreased number of inserted gates. We demonstrate the performance of our multi-programming approach by executing circuits of different sizes on IBM quantum hardware simultaneously. We also investigate this method on VQE algorithm to reduce its overhead.", "venue": "ArXiv", "authors": ["Siyuan  Niu", "Aida  Todri"], "year": 2021, "n_citations": 25}
{"id": 3875039, "s2_id": "78c5a75a53767900b09af6daea22db6026de7364", "title": "Machine Learning and Manycore Systems Design: A Serendipitous Symbiosis", "abstract": "Tight collaboration between manycore system designers and machine-learning experts is necessary to create a data-driven manycore design framework that integrates both learning and expert knowledge. Such a framework will be necessary to address the rising complexity of designing large-scale manycore systems and machine-learning techniques.", "venue": "Computer", "authors": ["Ryan Gary Kim", "Janardhan Rao Doppa", "Partha Pratim Pande", "Diana  Marculescu", "Radu  Marculescu"], "year": 2018, "n_citations": 17}
{"id": 3876570, "s2_id": "e9cdefb3963126aa6c0cb64364071a9d72169be7", "title": "Combinational Logic Circuit Design with the Buchberger Algorithm", "abstract": "We detail a procedure for the computation of the polynomial form of an electronic combinational circuit from the design equations in a truth table. The method uses the Buchberger algorithm rather than current traditional methods based on search algorithms. We restrict the analysis to a single output, but the procedure can be generalized to multiple outputs. The procedure is illustrated with the design of a simple arithmetic and logic unit with two 3-bit operands and two control bits.", "venue": "ArXiv", "authors": ["Germain  Drolet"], "year": 2006, "n_citations": 0}
{"id": 3877683, "s2_id": "f116a9525b1d241c8b7a85b5ea46e483f623028b", "title": "DANCE: Differentiable Accelerator/Network Co-Exploration", "abstract": "This work presents DANCE, a differentiable approach towards the co-exploration of hardware accelerator and network architecture design. At the heart of DANCE is a differentiable evaluator network. By modeling the hardware evaluation software with a neural network, the relation between the accelerator design and the hardware metrics becomes differentiable, allowing the search to be performed with backpropagation. Compared to the naive existing approaches, our method performs co-exploration in a significantly shorter time, while achieving superior accuracy and hardware cost metrics.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Kanghyun  Choi", "Deokki  Hong", "Hojae  Yoon", "Joonsang  Yu", "Youngsok  Kim", "Jinho  Lee"], "year": 2021, "n_citations": 10}
{"id": 3879460, "s2_id": "5250b846669a25df15de774071166f9137bf0d20", "title": "A Survey on Domain-Specific Memory Architectures", "abstract": "The never-ending demand for high performance and energy efficiency is pushing designers towards an increasing level of heterogeneity and specialization in modern computing systems. In such systems, creating efficient memory architectures is one of the major opportunities for optimizing modern workloads (e.g., computer vision, machine learning, graph analytics, etc.) that are extremely data-driven. However, designers demand proper design methods to tackle the increasing design complexity and address several new challenges, like the security and privacy of the data to be elaborated.This paper overviews the current trend for the design of domain-specific memory architectures. Domain-specific architectures are tailored for the given application domain, with the introduction of hardware accelerators and custom memory modules while maintaining a certain level of flexibility. We describe the major components, the common challenges, and the state-of-the-art design methodologies for building domain-specific memory architectures. We also discuss the most relevant research projects, providing a classification based on our main topics.", "venue": "Journal of Integrated Circuits and Systems", "authors": ["Stephanie  Soldavini", "Christian  Pilato"], "year": 2021, "n_citations": 1}
{"id": 3882942, "s2_id": "7da7803084e60a31504e4b1ec6539d8cac4351a0", "title": "Speculative interference attacks: breaking invisible speculation schemes", "abstract": "Recent security vulnerabilities that target speculative execution (e.g., Spectre) present a significant challenge for processor design. These highly publicized vulnerabilities use speculative execution to learn victim secrets by changing the cache state. As a result, recent computer architecture research has focused on invisible speculation mechanisms that attempt to block changes in cache state due to speculative execution. Prior work has shown significant success in preventing Spectre and other attacks at modest performance costs. In this paper, we introduce speculative interference attacks, which show that prior invisible speculation mechanisms do not fully block speculation-based attacks that use cache state. We make two key observations. First, mis-speculated younger instructions can change the timing of older, bound-to-retire instructions, including memory operations. Second, changing the timing of a memory operation can change the order of that memory operation relative to other memory operations, resulting in persistent changes to the cache state. Using both of these observations, we demonstrate (among other attack variants) that secret information accessed by mis-speculated instructions can change the order of bound-to-retire loads. Load timing changes can therefore leave secret-dependent changes in the cache, even in the presence of invisible speculation mechanisms. We show that this problem is not easy to fix. Speculative interference converts timing changes to persistent cache-state changes, and timing is typically ignored by many cache-based defenses. We develop a framework to understand the attack and demonstrate concrete proof-of-concept attacks against invisible speculation mechanisms. We conclude with a discussion of security definitions that are sufficient to block the attacks, along with preliminary defense ideas based on those definitions.", "venue": "ASPLOS", "authors": ["Mohammad  Behnia", "Prateek  Sahu", "Riccardo  Paccagnella", "Jiyong  Yu", "Zirui  Zhao", "Xiang  Zou", "Thomas  Unterluggauer", "Josep  Torrellas", "Carlos  Rozas", "Adam  Morrison", "Frank  Mckeen", "Fangfei  Liu", "Ron  Gabor", "Christopher W. Fletcher", "Abhishek  Basak", "Alaa  Alameldeen"], "year": 2021, "n_citations": 14}
{"id": 3883155, "s2_id": "855c7fdf9214cf7a76d709f307626285e58b006b", "title": "High speed fault tolerant secure communication for muon chamber using fpga based gbt emulator", "abstract": "The Compressed Baryonic Matter (CBM) experiment is a part of the Facility for Antiproton and Ion Research (FAIR) in Darmstadt at the GSI. The CBM experiment will investigate the highly compressed nuclear matter using nucleus-nucleus collisions. This experiment will examine lieavy-ion collisions in fixed target geometry and will be able to measure hadrons, electrons and muons. CBM requires precise time synchronization, compact hardware, radiation tolerance, self-triggered front-end electronics, efficient data aggregation schemes and capability to handle high data rate (up to several TB/s). As a part of the implementation of read out chain of Muon Cliamber(MUCH) [1] in India, we have tried to implement FPGA based emulator of GBTx in India. GBTx is a radiation tolerant ASIC that can be used to implement multipurpose high speed bidirectional optical links for high-energy physics (HEP) experiments and is developed by CERN. GBTx will be used in highly irradiated area and more prone to be affected by multi bit error. To mitigate this effect instead of single bit error correcting RS code we have used two bit error correcting (15, 7) BCH code. It will increase the redundancy which in turn increases the reliability of the coded data. So the coded data will be less prone to be affected by noise due to radiation. The data will go from detector to PC through multiple nodes through the communication channel. The computing resources are connected to a network which can be accessed by authorized person to prevent unauthorized data access which might happen by compromising the network security. Thus data encryption is essential. In order to make the data communication secure, advanced encryption standard [2] (AES - a symmetric key cryptography) and RSA [3], [4] (asymmetric key cryptography) are used after the channel coding. We have implemented GBTx emulator on two Xilinx Kintex-7 boards (KC705). One will act as transmitter and other will act as receiver and they are connected through optical fiber through small form-factor pluggable (SFP) port. We have tested the setup in the runtime environment using Xilinx Cliipscope Pro Analyzer. We also measure the resource utilization, throughput., power optimization of implemented design.", "venue": "ArXiv", "authors": ["Suman  Sau", "Swagata  Mandal", "Jogender  Saini", "Amlan  Chakrabarti", "Subhasis  Chattopadhyay"], "year": 2015, "n_citations": 0}
{"id": 3883327, "s2_id": "a2626da887fc0e504378cbaca2960f03723a1f22", "title": "MPU: Towards Bandwidth-abundant SIMT Processor via Near-bank Computing", "abstract": "With the growing number of data-intensive workloads, GPU, which is the state-of-the-art single-instructionmultiple-thread (SIMT) processor, is hindered by the memory bandwidth wall. To alleviate this bottleneck, previously proposed 3D-stacking near-bank computing accelerators benefit from abundant bank-internal bandwidth by bringing computations closer to the DRAM banks. However, these accelerators are specialized for certain application domains with simple architecture data paths and customized software mapping schemes. For general purpose scenarios, lightweight hardware designs for diverse data paths, architectural supports for the SIMT programming model, and end-to-end software optimizations remain challenging. To address these issues, we propose MPU (Memory-centric Processing Unit), the first SIMT processor based on 3D-stacking near-bank computing architecture. First, to realize diverse data paths with small overheads while leveraging bank-level bandwidth, MPU adopts a hybrid pipeline with the capability of offloading instructions to near-bank compute-logic. Second, we explore two architectural supports for the SIMT programming model, including a near-bank shared memory design and a multiple activated row-buffers enhancement. Third, we present an end-to-end compilation flow for MPU to support CUDA programs. To fully utilize MPU\u2019s hybrid pipeline, we develop a backend optimization for the instruction offloading decision. The evaluation results of MPU demonstrate 3.46\u00d7 speedup and 2.57\u00d7 energy reduction compared with an NVIDIA Tesla V100 GPU on a set of representative data-intensive workloads.", "venue": "ArXiv", "authors": ["Xinfeng  Xie", "Peng  Gu", "Yufei  Ding", "Dimin  Niu", "Hongzhong  Zheng", "Yuan  Xie"], "year": 2021, "n_citations": 0}
{"id": 3883419, "s2_id": "27c6be13044ebdb683880a2cc04f71ec027c3e58", "title": "Privacy and Integrity Preserving Training Using Trusted Hardware", "abstract": "Privacy and security-related concerns are growing as machine learning reaches diverse application domains. The data holders want to train with private data while exploiting accelerators, such as GPUs, that are hosted in the cloud. However, Cloud systems are vulnerable to the attackers that compromise privacy of data and integrity of computations. This work presents DarKnight, a framework for large DNN training while protecting input privacy and computation integrity. DarKnight relies on cooperative execution between trusted execution environments (TEE) and accelerators, where the TEE provides privacy and integrity verification, while accelerators perform the computation heavy linear algebraic operations.", "venue": "ArXiv", "authors": ["Hanieh  Hashemi", "Yongqin  Wang", "Murali  Annavaram"], "year": 2021, "n_citations": 0}
{"id": 3884151, "s2_id": "e7468364fea4e5bf51c562e48197a1e01893a62c", "title": "Slim NoC: A Low-Diameter On-Chip Network Topology for High Energy Efficiency and Scalability", "abstract": "Emerging chips with hundreds and thousands of cores require networks with unprecedented energy/area efficiency and scalability. To address this, we propose Slim NoC (SN): a new on-chip network design that delivers significant improvements in efficiency and scalability compared to the state-of-the-art. The key idea is to use two concepts from graph and number theory, degree-diameter graphs combined with non-prime finite fields, to enable the smallest number of ports for a given core count. SN is inspired by state-of-the-art off-chip topologies; it identifies and distills their advantages for NoC settings while solving several key issues that lead to significant overheads on-chip. SN provides NoC-specific layouts, which further enhance area/energy efficiency. We show how to augment SN with state-of-the-art router microarchitecture schemes such as Elastic Links, to make the network even more scalable and efficient. Our extensive experimental evaluations show that SN outperforms both traditional low-radix topologies (e.g., meshes and tori) and modern high-radix networks (e.g., various Flattened Butterflies) in area, latency, throughput, and static/dynamic power consumption for both synthetic and real workloads. SN provides a promising direction in scalable and energy-efficient NoC topologies.", "venue": "ASPLOS 2018", "authors": ["Maciej  Besta", "Syed Minhaj Hassan", "Sudhakar  Yalamanchili", "Rachata  Ausavarungnirun", "Onur  Mutlu", "Torsten  Hoefler"], "year": 2018, "n_citations": 40}
{"id": 3889376, "s2_id": "ba7145999b6861f27282f5cf424e63fb5799f395", "title": "Meta-level issues in Offloading: Scoping, Composition, Development, and their Automation", "abstract": "This paper argues for an accelerator development toolchain that takes into account the whole system containing the accelerator. With whole-system visibility, the toolchain can better assist accelerator scoping and composition in the context of the expected workloads and intended performance objectives. Despite being focused on the 'meta-level' of accelerators, this would build on existing and ongoing DSLs and toolchains for accelerator design. Basing this on our experience in programmable networking and reconfigurable-hardware programming, we propose an integrative approach that relies on three activities: (i) generalizing the focus of acceleration to offloading to accommodate a broader variety of non-functional needs -- such as security and power use -- while using similar implementation approaches, (ii) discovering what to offload, and to what hardware, through semi-automated analysis of a whole system that might compose different offload choices that changeover time, (iii) connecting with research and state-of-the-art approaches for using domain-specific languages (DSLs) and high-level synthesis (HLS) systems for custom offload development. We outline how this integration can drive new development tooling that accepts models of programs and resources to assist system designers through design-space exploration for the accelerated system.", "venue": "ArXiv", "authors": ["Andr'e  DeHon", "Hans  Giesen", "Nik  Sultana", "Yuanlong  Xiao"], "year": 2021, "n_citations": 0}
{"id": 3893360, "s2_id": "7d0011a5e613ae29d1a001ae7799cb8443fc31da", "title": "SoK: Opportunities for Software-Hardware-Security Codesign for Next Generation Secure Computing", "abstract": "Users are demanding increased data security. As a result, security is rapidly becoming a first-order design constraint in next generation computing systems. Researchers and practitioners are exploring various security technologies to meet user demand such as trusted execution environments (e.g., Intel SGX, ARM TrustZone), homomorphic encryption, and differential privacy. Each technique provides some degree of security, but differs with respect to threat coverage, performance overheads, as well as implementation and deployment challenges. In this paper, we present a systemization of knowledge (SoK) on these design considerations and trade-offs using several prominent security technologies. Our study exposes the need for \\textit{software-hardware-security} codesign to realize efficient and effective solutions of securing user data. In particular, we explore how design considerations across applications, hardware, and security mechanisms must be combined to overcome fundamental limitations in current technologies so that we can minimize performance overhead while achieving sufficient threat model coverage. Finally, we propose a set of guidelines to facilitate putting these secure computing technologies into practice.", "venue": "ArXiv", "authors": ["Deeksha  Dangwal", "Meghan  Cowan", "Armin  Alaghi", "Vincent T. Lee", "Brandon  Reagen", "Caroline  Trippel"], "year": 2021, "n_citations": 0}
{"id": 3893519, "s2_id": "6e08a3696461c7ea0df2adcd4716930d6739f2fd", "title": "IOCA: High-Speed I/O-Aware LLC Management for Network-Centric Multi-Tenant Platform", "abstract": "In modern server CPUs, last-level cache (LLC) is a critical hardware resource that exerts significant influence on the performance of the workloads, and how to manage LLC is a key to the performance isolation and QoS in the cloud with multi-tenancy. In this paper, we argue that besides CPU cores, high-speed network I/O is also important for LLC management. This is because of an Intel architectural innovation -- Data Direct I/O (DDIO) -- that directly injects the inbound I/O traffic to (part of) the LLC instead of the main memory. We summarize two problems caused by DDIO and show that (1) the default DDIO configuration may not always achieve optimal performance, (2) DDIO can decrease the performance of non-I/O workloads which share LLC with it by as high as 32%. \nWe then present IOCA, the first LLC management mechanism for network-centric platforms that treats the I/O as the first-class citizen. IOCA monitors and analyzes the performance of the cores, LLC, and DDIO using CPU's hardware performance counters, and adaptively adjusts the number of LLC ways for DDIO or the tenants that demand more LLC capacity. In addition, IOCA dynamically chooses the tenants that share its LLC resource with DDIO, to minimize the performance interference by both the tenants and the I/O. Our experiments with multiple microbenchmarks and real-world applications in two major end-host network models demonstrate that IOCA can effectively reduce the performance degradation caused by DDIO, with minimal overhead.", "venue": "ArXiv", "authors": ["Yifan  Yuan", "Mohammad  Alian", "Yipeng  Wang", "Ilia  Kurakin", "Ren  Wang", "Tsung-Yuan Charlie Tai", "Nam Sung Kim"], "year": 2020, "n_citations": 0}
{"id": 3898246, "s2_id": "99094773fb36c06a7376c2727fb93620b30ce6a0", "title": "ENBB Processor: Towards the ExaScale Numerical Brain Box [Position Paper]", "abstract": "ExaScale systems will be a key driver for simulations that are essential for advance of science and economic growth. We aim to present a new concept of microprocessor for floating-point computations useful for being a basic building block of ExaScale systems and beyond. The proposed microprocessor architecture has a frontend for programming interface based on the concept of event-driven simulation. The user program is executed as an event-driven simulation using a hardware/software co-designed simulator. This is the flexible part of the system. The back-end exploits the concept of uniform topology as in a brain: a massive packet switched interconnection network with flit credit-based flow control with virtual channels that incorporates seamlessly communication, arithmetic and storage. Floating-point computations are incorporated as on-line arithmetic operators in the output ports of the switches as virtual arithmetic output channels, and storage as virtual input channels. The front-end carries out the event-driven simulation of the user program, and uses the arithmetic network for the hard floating-point work by means of virtual dataflows. We expect to reduce significantly the needs of main memory due to the execution model proposed, where variables are just virtual interconnections in the network or signals stored in the virtual channels. Moreover, we have the hypothesis that the problem size assigned to a microprocessor should allow maximum concurrency and it should not be oversized. This may lead to systems composed of microprocessors with main memory incorporated in 3D chips. We identified several challenges that a research to develop this microprocessor should address, and several hypothesis that should be demonstrated by means of scientific evidence.", "venue": "ArXiv", "authors": ["Elisardo  Antelo"], "year": 2019, "n_citations": 0}
{"id": 3899190, "s2_id": "498bd63852b5b81072c5c807b170c30f454676e4", "title": "Toward an End-to-End Auto-tuning Framework in HPC PowerStack", "abstract": "Efficiently utilizing procured power and optimizing performance of scientific applications under power and energy constraints are challenging. The HPC PowerStack defines a software stack to manage power and energy of high-performance computing systems and standardizes the interfaces between different components of the stack. This survey paper presents the findings of a working group focused on the end-to-end tuning of the PowerStack. First, we provide a background on the PowerStack layer-specific tuning efforts in terms of their high-level objectives, the constraints and optimization goals, layer-specific telemetry, and control parameters, and we list the existing software solutions that address those challenges. Second, we propose the PowerStack end-to-end auto-tuning framework, identify the opportunities in co-tuning different layers in the PowerStack, and present specific use cases and solutions. Third, we discuss the research opportunities and challenges for collective auto-tuning of two or more management layers (or domains) in the PowerStack. This paper takes the first steps in identifying and aggregating the important R&D challenges in streamlining the optimization efforts across the layers of the PowerStack.", "venue": "2020 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Xingfu  Wu", "Aniruddha  Marathe", "Siddhartha  Jana", "Ondrej  Vysocky", "Jophin  John", "Andrea  Bartolini", "Lubomir  Riha", "Michael  Gerndt", "Valerie  Taylor", "Sridutt  Bhalachandra"], "year": 2020, "n_citations": 2}
{"id": 3900499, "s2_id": "267283c9a55c5a7176b9e594269c7d13adf04272", "title": "Transactional WaveCache: Towards Speculative and Out-of-Order DataFlow Execution of Memory Operations", "abstract": "The WaveScalar is the first dataflow architecture that can efficiently provide the sequential memory semantics required by imperative languages. This work presents a speculative memory disambiguation mechanism for this architecture, the transaction WaveCache. Our mechanism maintains the execution order of memory operations within blocks of code, called waves, but adds the ability to speculatively execute, out-of-order, operations from different waves. This mechanism is inspired by progress in supporting transactional memories. Waves are considered as atomic regions and executed as nested transactions. Wave that have finished the execution of all their memory operations are committed, as soon as the previous waves are also committed. If a hazard is detected in a speculative wave, all the following waves (children) are aborted and re-executed. We evaluated the transactional WaveCache on a set of benchmarks from Spec 2000, Mediabench and Mibench (telecomm). Speedups ranging from 1.31 to 2.24 (related to the original WaveScalar) where observed when the benchmark doesn't perform lots of emulated function calls or access memory very often. Low speedups of 1.1 to slowdowns of 0.96 were observed when the opposite happens or when the memory concurrency was high.", "venue": "2008 20th International Symposium on Computer Architecture and High Performance Computing", "authors": ["Leandro A. J. Marzulo", "Felipe Maia Galv\u00e3o Fran\u00e7a", "V\u00edtor Santos Costa"], "year": 2008, "n_citations": 11}
{"id": 3900634, "s2_id": "78bcc915fb8af2548892ad25cb895f168d997b07", "title": "SME: A High Productivity FPGA Tool for Software Programmers", "abstract": "For several decades, the CPU has been the standard model to use in the majority of computing. While the CPU does excel in some areas, heterogeneous computing, such as reconfigurable hardware, is showing increasing potential in areas like parallelization, performance, and power usage. This is especially prominent in problems favoring deep pipelining or tight latency requirements. However, due to the nature of these problems, they can be hard to program, at least for software developers. Synchronous Message Exchange (SME) is a runtime environment that allows development, testing and verification of hardware designs for FPGA devices in C#, with access to modern debugging and code features. The goal is to create a framework for software developers to easily implement systems for FPGA devices without having to obtain heavy hardware programming knowledge. This article presents a short introduction to the SME model as well as new updates to SME. Lastly, a selection of student projects and examples will be presented in order to show how it is possible to create quite complex structures in SME, even by students with no hardware experience.", "venue": "ArXiv", "authors": ["Carl-Johannes  Johnsen", "Alberte  Thegler", "Kenneth  Skovhede", "Brian  Vinter"], "year": 2021, "n_citations": 0}
{"id": 3911132, "s2_id": "3c89921d72f740430ad2c5cddeb723898ed3d669", "title": "Approximate Logic Synthesis: A Reinforcement Learning-Based Technology Mapping Approach", "abstract": "Approximate Logic Synthesis (ALS) is the process of synthesizing and mapping a given Boolean network to a library of logic cells so that the magnitude/rate of error between outputs of the approximate and initial (exact) Boolean netlists is bounded from above by a predetermined total error threshold. In this paper, we present Q-ALS, a novel framework for ALS with focus on the technology mapping phase. Q-ALS incorporates reinforcement learning and utilizes Boolean difference calculus to estimate the maximum error rate that each node of the given network can tolerate such that the total error rate at non of the outputs of the mapped netlist exceeds a predetermined maximum error rate, and the worst case delay and the total area are minimized. Maximum Hamming Distance (MHD) between exact and approximate truth tables of cuts of each node is used as the error metric. In Q-ALS, a Q-Learning agent is trained with a sufficient number of iterations aiming to select the fittest values of MHD for each node, and in a cut-based technology mapping approach, the best supergates (in terms of delay and area, bounded further by the fittest MHD) are selected towards implementing each node. Experimental results show that having set the required accuracy of 95% at the primary outputs, Q-ALS reduces the total cost in terms of area and delay by up to 70 % and 36%, respectively, and also reduces the run-time by 2.21 x on average, when compared to the best state-of-the-art academic ALS tools.", "venue": "20th International Symposium on Quality Electronic Design (ISQED)", "authors": ["Ghasem  Pasandi", "Shahin  Nazarian", "Massoud  Pedram"], "year": 2019, "n_citations": 8}
{"id": 3913014, "s2_id": "073f15f25656ce76f12560b9b13ec913fe7bdd59", "title": "On the Effect of Quantum Interaction Distance on Quantum Addition Circuits", "abstract": "We investigate the theoretical limits of the effect of the quantum interaction distance on the speed of exact quantum addition circuits. For this study, we exploit graph embedding for quantum circuit analysis. We study a logical mapping of qubits and gates of any \u03a9(log n)-depth quantum adder circuit for two n-qubit registers onto a practical architecture, which limits interaction distance to the nearest neighbors only and supports only one- and two-qubit logical gates. Unfortunately, on the chosen k-dimensional practical architecture, we prove that the depth lower bound of any exact quantum addition circuits is no longer \u03a9(log n), but \u03a9(k\u221an). This result, the first application of graph embedding to quantum circuits and devices, provides a new tool for compiler development, emphasizes the impact of quantum computer architecture on performance, and acts as a cautionary note when evaluating the time performance of quantum algorithms.", "venue": "JETC", "authors": ["Byung-Soo  Choi", "Rodney Van Meter"], "year": 2011, "n_citations": 31}
{"id": 3918260, "s2_id": "1d5a2ced6de660d0d3ca6f037610676d1bfddb58", "title": "Google Neural Network Models for Edge Devices: Analyzing and Mitigating Machine Learning Inference Bottlenecks", "abstract": "Emerging edge computing platforms often contain machine learning (ML) accelerators that can accelerate inference for a wide range of neural network (NN) models. These models are designed to fit within the limited area and energy constraints of the edge computing platforms, each targeting various applications (e.g., face detection, speech recognition, translation, image captioning, video analytics). To understand how edge ML accelerators perform, we characterize the performance of a commercial Google Edge TPU, using 24 Google edge NN models (which span a wide range of NN model types) and analyzing each NN layer within each model. We find that the Edge TPU suffers from three major shortcomings: (1) it operates significantly below peak computational throughput, (2) it operates significantly below its theoretical energy efficiency, and (3) its memory system is a large energy and performance bottleneck. Our characterization reveals that the one-size-fits-all, monolithic design of the Edge TPU ignores the high degree of heterogeneity both across different NN models and across different NN layers within the same NN model, leading to the shortcomings we observe. We propose a new acceleration framework called Mensa. Mensa incorporates multiple heterogeneous edge ML accelerators (including both on-chip and near-data accelerators), each of which caters to the characteristics of a particular subset of NN models and layers. During NN inference, for each NN layer, Mensa decides which accelerator to schedule the layer on, taking into account both the optimality of each accelerator for the layer and layer-to-layer communication costs. Our comprehensive analysis of the Google edge NN models shows that all of the layers naturally group into a small number of clusters, which allows us to design an efficient implementation of Mensa for these models with only three specialized accelerators. Averaged across all 24 Google edge NN models, Mensa improves energy efficiency and throughput by 3.0x and 3.1x over the Edge TPU, and by 2.4x and 4.3x over Eyeriss v2, a state-of-the-art accelerator.", "venue": "2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)", "authors": ["Amirali  Boroumand", "Saugata  Ghose", "Berkin  Akin", "Ravi  Narayanaswami", "Geraldo F. Oliveira", "Xiaoyu  Ma", "Eric  Shiu", "Onur  Mutlu"], "year": 2021, "n_citations": 3}
{"id": 3920871, "s2_id": "c59bfc4e2244d4edefae513606ed6be8e30c24af", "title": "Towards Accurate Performance Modeling of RISC-V Designs", "abstract": "Microprocessor design, debug, and validation research and development are increasingly based on modeling and simulation at different abstraction layers. Microarchitecture-level simulators have become the most commonly used tools for performance evaluation, due to their high simulation throughput, compared to lower levels of abstraction, but usually come at the cost of loss of hardware accuracy. As a result, the implementation, speed, and accuracy of microarchitectural simulators are becoming more and more crucial for researchers and microprocessor architects. One of the most critical aspects of a microarchitectural simulator is its ability to accurately express design standards as various aspects of the microarchitecture change during design refinement. On the other hand, modern microprocessor models rely on dedicated hardware implementations, making the design space exploration a time-consuming process that can be performed using a variety of methods, ranging from high-level models to hardware prototyping. Therefore, the tradeoff between simulation speed and accuracy, can be significantly varied, and an application\u2019s performance measurements uncertain. In this paper, we present a microarchitecture-level simulation modeling study, which enables as accurate as possible performance modeling of a RISC-V out-of-order superscalar microprocessor core. By diligently adjusting several important microarchitectural parameters of the widely used gem5 simulator, we investigate the challenges of accurate performance modeling on microarchitecture-level simulation compared to accuracy and low simulation throughput of RTL simulation of the target design. Further, we demonstrate the main sources of errors that prevent high accuracy levels of the microarchitecture-level modeling.", "venue": "ArXiv", "authors": ["Odysseas  Chatzopoulos", "George-Marios  Fragkoulis", "George  Papadimitriou", "Dimitris  Gizopoulos"], "year": 2021, "n_citations": 0}
{"id": 3921264, "s2_id": "93e1a7f8f4a86f7aa8a787711384005bdc94a21f", "title": "CLARINET: A RISC-V Based Framework for Posit Arithmetic Empiricism", "abstract": "Many engineering and scientific applications require high precision arithmetic. IEEE 754-2008 compliant (floating-point) arithmetic is the de facto standard for performing these computations. Recently, posit arithmetic has been proposed as a drop-in replacement for floating-point arithmetic. The posit data representation and arithmetic offer several absolute advantages over the floating-point format and arithmetic including higher dynamic range, better accuracy, and superior performance-area trade-offs. \nIn this paper, we present a consolidated general-purpose processor-based framework to support posit arithmetic empiricism. The end-users of the framework have the liberty to seamlessly experiment with their applications using posit and floating-point arithmetic since the framework is designed for the two number systems to coexist. The framework consists of Melodica and Clarinet. Melodica is a posit arithmetic core that implements parametric fused-multiply-accumulate and, more importantly, supports the quire data type. Clarinet is a Melodica-enabled processor based on the RISC-V ISA. To the best of our knowledge, this is the first-ever integration of quire to a RISC-V core. To show the effectiveness of the Clarinet platform, we perform an extensive application study and benchmarking on some of the common linear algebra and computer vision kernels. We perform ASIC synthesis of Clarinet and Melodica on a 90 nm-CMOS Faraday process. Finally, based on our analysis and synthesis results, we define a quality metric for the different instances of Clarinet that gives us initial recommendations on the goodness of the instances. Clarinet-Melodica is an easy-to-experiment platform that will be made available in open-source for posit arithmetic empiricism.", "venue": "ArXiv", "authors": ["Riya  Jain", "Niraj  Sharma", "Farhad  Merchant", "Sachin  Patkar", "Rainer  Leupers"], "year": 2020, "n_citations": 6}
{"id": 3923032, "s2_id": "ed08428c0e2e3820af1cffdaca3b40432bef9461", "title": "Novel graph processor architecture, prototype system, and results", "abstract": "Graph algorithms are increasingly used in applications that exploit large databases. However, conventional processor architectures are inadequate for handling the throughput and memory requirements of graph computation. Lincoln Laboratory's graph-processor architecture represents a rethinking of parallel architectures for graph problems. Our processor utilizes innovations that include a sparse matrix-based graph instruction set, a cacheless memory system, accelerator-based architecture, a systolic sorter, high-bandwidth multidimensional toroidal communication network, and randomized communications. A field-programmable gate array (FPGA) prototype of the new graph processor has been developed with significant performance enhancement over conventional processors in graph computational throughput.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["William S. Song", "Vitaliy  Gleyzer", "Alexei  Lomakin", "Jeremy  Kepner"], "year": 2016, "n_citations": 18}
{"id": 3924461, "s2_id": "5b4610a5ec15dab972d05b82a1deae1d5bd17fb5", "title": "Binary Adder Circuits of Asymptotically Minimum Depth, Linear Size, and Fan-Out Two", "abstract": "We consider the problem of constructing fast and small binary adder circuits. Among widely used adders, the Kogge-Stone adder is often considered the fastest, because it computes the carry bits for two n-bit numbers (where n is a power of two) with a depth of 2 log2n logic gates, size 4 nlog2n, and all fan-outs bounded by two. Fan-outs of more than two are disadvantageous in practice, because they lead to the insertion of repeaters for repowering the signal and additional depth in the physical implementation. However, the depth bound of the Kogge-Stone adder is off by a factor of two from the lower bound of log2n. Two separate constructions by Brent and Krapchenko achieve this lower bound asymptotically. Brent\u2019s construction gives neither a bound on the fan-out nor the size, while Krapchenko\u2019s adder has linear size, but can have up to linear fan-out. With a fan-out bound of two, neither construction achieves a depth of less than 2 log2n. In a further approach, Brent and Kung proposed an adder with linear size and fan-out two but twice the depth of the Kogge-Stone adder. These results are 33\u201343 years old and no substantial theoretical improvement for has been made since then. In this article, we integrate the individual advantages of all previous adder circuits into a new family of full adders, the first to improve on the depth bound of 2 log2n while maintaining a fan-out bound of two. Our adders achieve an asymptotically optimum logic gate depth of log2n + o(log 2n) and linear size O(n).", "venue": "ACM Trans. Algorithms", "authors": ["Stephan  Held", "Sophie Theresa Spirkl"], "year": 2018, "n_citations": 1}
{"id": 3926145, "s2_id": "c10ce3415f808ae23fc1d98648719fb0c186da4b", "title": "Hydra: An Accelerator for Real-Time Edge-Aware Permeability Filtering in 65nm CMOS", "abstract": "Many modern video processing pipelines rely on edge-aware (EA) filtering methods. However, recent high-quality methods are challenging to run in real-time on embedded hardware due to their computational load. To this end, we propose an area-efficient and real-time capable hardware implementation of a high quality EA method. In particular, we focus on the recently proposed permeability filter (PF) that delivers promising quality and performance in the domains of high dynamic range (HDR) tone mapping, disparity and optical flow estimation. We present an efficient hardware accelerator that implements a tiled variant of the PF with low on-chip memory requirements and a significantly reduced external memory bandwidth (6.4\u00d7 w.r.t. the non-tiled PF). The design has been taped out in 65 nm CMOS technology, is able to filter 720p grayscale video at 24.8 Hz and achieves a high compute density of 6.7GFLOPS/mm2 (12\u00d7 higher than embedded GPUs when scaled to the same technology node). The low area and bandwidth requirements make the accelerator highly suitable for integration into systems-on-chip (SoCs) where silicon area budget is constrained and external memory is typically a heavily contended resource.", "venue": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Manuel  Eggimann", "Christelle  Gloor", "Florian  Scheidegger", "Lukas  Cavigelli", "Michael  Schaffner", "Aljoscha  Smolic", "Luca  Benini"], "year": 2018, "n_citations": 0}
{"id": 3928250, "s2_id": "a44d3a270070e2464130dc2884798f5b9c328d27", "title": "The Normalized Singular Value Decomposition of Non-Symmetric Matrices Using Givens fast Rotations", "abstract": "In this paper we introduce the algorithm and the fixed point hardware to calculate the normalized singular value decomposition of a non-symmetric matrices using Givens fast (approximate) rotations. This algorithm only uses the basic combinational logic modules such as adders, multiplexers, encoders, Barrel shifters (B-shifters), and comparators and does not use any lookup table. This method in fact combines the iterative properties of singular value decomposition method and CORDIC method in one single iteration. The introduced architecture is a systolic architecture that uses two different types of processors, diagonal and non-diagonal processors. The diagonal processor calculates, transmits and applies the horizontal and vertical rotations, while the non-diagonal processor uses a fully combinational architecture to receive, and apply the rotations. The diagonal processor uses priority encoders, Barrel shifters, and comparators to calculate the rotation angles. Both processors use a series of adders to apply the rotation angles. The design presented in this work provides $2.83\\sim649$ times better energy per matrix performance compared to the state of the art designs. This performance achieved without the employment of pipelining; a better performance advantage is expected to be achieved employing pipelining.", "venue": "ArXiv", "authors": ["Ehsan  Rohani", "Gwan S. Choi", "Mi  Lu"], "year": 2017, "n_citations": 0}
{"id": 3934574, "s2_id": "7065052a685b06c3e13e77ffccef83393eb1bed2", "title": "On-FPGA Training with Ultra Memory Reduction: A Low-Precision Tensor Method", "abstract": "Various hardware accelerators have been developed for energy-efficient and realtime inference of neural networks on edge devices. However, most training is done on high-performance GPUs or servers, and the huge memory and computing costs prevent training neural networks on edge devices. This paper proposes a novel tensor-based training framework, which offers orders-of-magnitude memory reduction in the training process. We propose a novel rank-adaptive tensorized neural network model, and design a hardware-friendly low-precision algorithm to train this model. We present an FPGA accelerator to demonstrate the benefits of this training method on edge devices. Our preliminary FPGA implementation achieves 59\u00d7 speedup and 123\u00d7 energy reduction compared to embedded CPU, and 292\u00d7 memory reduction over a standard full-size training.", "venue": "ArXiv", "authors": ["Kaiqi  Zhang", "Cole  Hawkins", "Xiyuan  Zhang", "Cong  Hao", "Zheng  Zhang"], "year": 2021, "n_citations": 4}
{"id": 3937980, "s2_id": "0cae347f34716708d706716e9c3acb0de3fafb99", "title": "Block convolution: Towards memory-efficient inference of large-scale CNNs on FPGA", "abstract": "FPGA-based CNN accelerators are gaining popularity due to high energy efficiency and great flexibility in recent years. However, as the networks grow in depth and width, the great volume of intermediate data is too large to store on chip, data transfers between on-chip memory and off-chip memory should be frequently executed, which leads to unexpected offchip memory access latency and energy consumption. In this paper, we propose a block convolution approach, which is a memory-efficient, simple yet effective block-based convolution to completely avoid intermediate data from streaming out to off-chip memory during network inference. Experiments on the very large VGG-16 network show that the improved top-1/top-5 accuracy of 72.60%/91.10% can be achieved on the ImageNet classification task with the proposed approach. As a case study, we implement the VGG-16 network with block convolution on Xilinx Zynq ZC706 board, achieving a frame rate of 12.19fps under 150MHz working frequency, with all intermediate data staying on chip.", "venue": "2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Gang  Li", "Fanrong  Li", "Tianli  Zhao", "Jian  Cheng"], "year": 2018, "n_citations": 9}
{"id": 3940768, "s2_id": "52f3ff33d32706c433abfee2f3a5f69a510696d6", "title": "A Light-Based Device for Solving the Hamiltonian Path Problem", "abstract": "In this paper we suggest the use of light for performing useful computations. Namely, we propose a special device which uses light rays for solving the Hamiltonian path problem on a directed graph. The device has a graph-like representation and the light is traversing it following the routes given by the connections between nodes. In each node the rays are uniquely marked so that they can be easily identified. At the destination node we will search only for particular rays that have passed only once through each node. We show that the proposed device can solve small and medium instances of the problem in reasonable time.", "venue": "UC", "authors": ["Mihai  Oltean"], "year": 2006, "n_citations": 28}
{"id": 3946695, "s2_id": "956c7d2b6137097ce16853fae0c5a550c40b6a15", "title": "Memristive Stochastic Computing for Deep Learning Parameter Optimization", "abstract": "Stochastic Computing (SC) is a computing paradigm that allows for the low-cost and low-power computation of various arithmetic operations using stochastic bit streams and digital logic. In contrast to conventional representation schemes used within the binary domain, the sequence of bit streams in the stochastic domain is inconsequential, and computation is usually non-deterministic. In this brief, we exploit the stochasticity during switching of probabilistic Conductive Bridging RAM (CBRAM) devices to efficiently generate stochastic bit streams in order to perform Deep Learning (DL) parameter optimization, reducing the size of Multiply and Accumulate (MAC) units by 5 orders of magnitude. We demonstrate that in using a 40-nm Complementary Metal Oxide Semiconductor (CMOS) process our scalable architecture occupies 1.55mm2 and consumes approximately $167~\\mu \\text{W}$ when optimizing parameters of a Convolutional Neural Network (CNN) while it is being trained for a character recognition task, observing no notable reduction in accuracy post-training.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Corey  Lammie", "Jason K. Eshraghian", "Wei D. Lu", "Mostafa Rahimi Azghadi"], "year": 2021, "n_citations": 2}
{"id": 3951644, "s2_id": "4be8936d32bc595d8a695b5e1b328aee78ebfe7f", "title": "Feature Engineering for Scalable Application-Level Post-Silicon Debugging", "abstract": "We present systematic and efficient solutions for both observability enhancement and root-cause diagnosis of postsilicon System-on-Chips (SoCs) validation with diverse usage scenarios. We model specification of interacting flows in typical applications for message selection. Our method for message selection optimizes flow specification coverage and trace buffer utilization. We define the diagnosis problem as identifying buggy traces as outliers and bug-free traces as inliers/normal behaviors, for which we use unsupervised learning algorithms for outlier detection. Instead of direct application of machine learning algorithms over trace data using the signals as raw features, we use feature engineering to transform raw features into more sophisticated features using domain specific operations. The engineered features are highly relevant to the diagnosis task and are generic to be applied across any hardware designs. We present debugging and root cause analysis of subtle post-silicon bugs in industry-scale OpenSPARC T2 SoC. We achieve a trace buffer utilization of 98.96% with a flow specification coverage of 94.3% (average). Our diagnosis method was able to diagnose up to 66.7% more bugs and took up to 847\u00d7 less diagnosis time as compared to the manual debugging with a diagnosis precision of 0.769.", "venue": "ArXiv", "authors": ["Debjit  Pal", "Shobha  Vasudevan"], "year": 2021, "n_citations": 0}
{"id": 3953552, "s2_id": "7297a3da4ce6adc21bed6bc258c00c17d250cbb6", "title": "Profile-Driven Automated Mixed Precision", "abstract": "We present a scheme to automatically set the precision of floating point variables in an application. We design a framework that profiles applications to measure undesirable numerical behavior at the floating point operation level. We use this framework to perform mixed precision analysis to heuristically set the precision of all variables in an application based on their numerical profiles. We experimentally evaluate the mixed precision analysis to show that it can generate a range of results with different accuracy and performance characteristics.", "venue": "ArXiv", "authors": ["Ralph  Nathan", "Helia  Naeimi", "Daniel J. Sorin", "Xiaobai  Sun"], "year": 2016, "n_citations": 5}
{"id": 3955689, "s2_id": "e80f4b04b5a273987501298b7e8d153396edde06", "title": "RISC micrprocessor verification", "abstract": "Today's microprocessors have grown significantly in complexity and functionality. Most of today's processors provide at least three levels of memory hierarchy, are heavily pipelined, and support some sort of cache coherency protocol. These features are extremely complex and sophisticated, and present their own set of unique verification challenges. Verification is clearly not a point tool, but is part of a process that starts from initial product conception and is to some degrees complete when the product goes to market. Functional verification is necessary to verify the functionality at RTL level. Complex micro-processors like ARM are high performance, low cost and low power 32-bit RISC processors. In our paper complex microprocessor is ARM cortex M3, developed for the embedded applications having low interrupt latency, low gate count, 3- stage pipelining, branch prediction, THUMB and THUMB-2 instruction set. Functional verification is used to verify that the circuit full fills each abstract assertion under the implementation mapping. we explore several aspects of processor design, including caches, pipeline depth, ALUs, and bypass logic.The verification was done concurrently with the design implementation of the processor.", "venue": "ArXiv", "authors": ["Mitul S Nagar", "Haresh A Suthar", "Chintan  Panchal"], "year": 2020, "n_citations": 0}
{"id": 3955865, "s2_id": "f98b9b50d85ceb9bc522274d381385c0f7841933", "title": "A quality-of-service mechanism for interconnection networks in system-on-chips", "abstract": "As Moore's Law continues to fuel the ability to build ever increasing complex systems-on-chips (SoCs), achieving performance goals is rising as a critical challenge to completing designs. In particular, the system interconnect must efficiently service a diverse set of data flows with widely ranging quality-of-service (QoS) requirements. However the known solutions for off-chip interconnects, such as large-scale networks, are not necessarily applicable to the on-chip environment. Latency and memory constraints for on-chip interconnects are quite different from larger-scale interconnects. The paper introduces a novel on-chip interconnect arbitration scheme. We show how this scheme can be distributed across a chip for high-speed implementation. We compare the performance of the arbitration scheme with other known interconnect arbitration schemes. Existing schemes typically focus heavily on either low latency of service for some initiators or on guaranteed bandwidth delivery for other initiators. Our scheme allows service latency on some initiators to be traded off smoothly against jitter bounds on other initiators, while still delivering bandwidth guarantees. This scheme is a subset of the QoS controls that are available in the SonicsMX/spl trade/ (SMX) product.", "venue": "Design, Automation and Test in Europe", "authors": ["Wolf-Dietrich  Weber", "Joe  Chou", "Ian  Swarbrick", "Drew  Wingard"], "year": 2005, "n_citations": 50}
{"id": 3956292, "s2_id": "3fcb6943eaca1f075d5090c20347ab562df9dd8d", "title": "Quantum Algorithm Processors to Reveal Hamiltonian Cycles", "abstract": "Quantum computer versus quantum algorithm processor in CMOS are compared to find (in parallel) all Hamiltonian cycles in a graph with m edges and n vertices, each represented by k bits. A quantum computer uses quantum states analogous to CMOS registers. With efficient initialization, number of CMOS registers is proportional to (n-1)! Number of qubits in a quantum computer is approximately proportional to kn+2mn in the approach below. Using CMOS, the bits per register is about proportional to kn, which is less since bits can be irreversibly reset. In either concept, number of gates, or operations to identify Hamiltonian cycles is proportional to kmn. However, a quantum computer needs an additional exponentially large number of operations to accomplish a probabilistic readout. In contrast, CMOS is deterministic and readout is comparable to ordinary memory.", "venue": "ArXiv", "authors": ["John Robert Burger"], "year": 2005, "n_citations": 1}
{"id": 3956466, "s2_id": "ed43aed792ce705ee7911d94dcabf72d7ce073f1", "title": "Exploiting Row-Level Temporal Locality in DRAM to Reduce the Memory Access Latency", "abstract": "This paper summarizes the idea of ChargeCache, which was published in HPCA 2016 [51], and examines the work's significance and future potential. DRAM latency continues to be a critical bottleneck for system performance. In this work, we develop a low-cost mechanism, called ChargeCache, that enables faster access to recently-accessed rows in DRAM, with no modifications to DRAM chips. Our mechanism is based on the key observation that a recently-accessed row has more charge and thus the following access to the same row can be performed faster. To exploit this observation, we propose to track the addresses of recently-accessed rows in a table in the memory controller. If a later DRAM request hits in that table, the memory controller uses lower timing parameters, leading to reduced DRAM latency. Row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency. We evaluate ChargeCache on a wide variety of workloads and show that it provides significant performance and energy benefits for both single-core and multi-core systems.", "venue": "ArXiv", "authors": ["Hasan  Hassan", "Gennady  Pekhimenko", "Nandita  Vijaykumar", "Vivek  Seshadri", "Donghyuk  Lee", "Oguz  Ergin", "Onur  Mutlu"], "year": 2018, "n_citations": 1}
{"id": 3959216, "s2_id": "5beda4c8ff1e78b284070b29afd3411d134df0ba", "title": "ECI-Cache: A High-Endurance and Cost-Efficient I/O Caching Scheme for Virtualized Platforms", "abstract": "In recent years, high interest in using Virtual Machines (VMs) in data centers and cloud computing has significantly increased the demand for high-performance data storage systems. A straightforward approach to providing a high-performance storage system is using Solid-State Drives (SSDs). Inclusion of SSDs in storage systems, however, imposes significantly higher cost compared to Hard Disk Drives (HDDs). Recent studies suggest using SSDs as a caching layer for HDD-based storage subsystems in virtualized platforms. Such studies neglect to address the endurance and cost of SSDs, which can significantly affect the efficiency of I/O caching. Moreover, previous studies only configure the cache size to provide the required performance level for each VM, while neglecting other important parameters such as cache write policy and request type, which can adversely affect both performance-per-cost and endurance.\n In this paper, we propose a new high-Endurance and Cost-efficient I/O caching (ECI-Cache) scheme for virtualized platforms in large-scale data centers, which improves both performance-per-cost and endurance of the SSD cache. ECI-Cache dynamically assigns 1) an efficient cache size for each VM, to maximize the overall performance of the running VMs and 2) an effective write policy for each VM, to enhance the endurance and performance-per-cost of the storage subsystem.", "venue": "SIGMETRICS", "authors": ["Saba  Ahmadian", "Onur  Mutlu", "Hossein  Asadi"], "year": 2018, "n_citations": 0}
{"id": 3961191, "s2_id": "d1fb58c07788ab8e2db198d357c5f708a94baadf", "title": "Disaggregated and optically interconnected memory: when will it be cost effective?", "abstract": "The \"Disaggregated Server\" concept has been proposed for datacenters where the same type server resources are aggregated in their respective pools, for example a compute pool, memory pool, network pool, and a storage pool. Each server is constructed dynamically by allocating the right amount of resources from these pools according to the workload's requirements. Modularity, higher packaging and cooling efficiencies, and higher resource utilization are among the suggested benefits. With the emergence of very large datacenters, \"clouds\" containing tens of thousands of servers, datacenter efficiency has become an important topic. Few computer chip and systems vendors are working on and making frequent announcements on silicon photonics and disaggregated memory systems. \nIn this paper we study the trade-off between cost and performance of building a disaggregated memory system where DRAM modules in the datacenter are pooled, for example in memory-only chassis and racks. The compute pool and the memory pool are interconnected by an optical interconnect to overcome the distance and bandwidth issues of electrical fabrics. We construct a simple cost model that includes the cost of latency, cost of bandwidth and the savings expected from a disaggregated memory system. We then identify the level at which a disaggregated memory system becomes cost competitive with a traditional direct attached memory system. \nOur analysis shows that a rack-scale disaggregated memory system will have a non-trivial performance penalty, and at the datacenter scale the penalty is impractically high, and the optical interconnect costs are at least a factor of 10 more expensive than where they should be when compared to the traditional direct attached memory systems.", "venue": "ArXiv", "authors": ["B\u00fclent  Abali", "Richard J. Eickemeyer", "Hubertus  Franke", "Chung-Sheng  Li", "Marc  Taubenblatt"], "year": 2015, "n_citations": 29}
{"id": 3962212, "s2_id": "4b1ecd6fdad080fedc26aceace7ee4e77667948c", "title": "Going Deep: Using deep learning techniques with simplified mathematical models against XOR BR and TBR PUFs (Attacks and Countermeasures)", "abstract": "This paper contributes to the study of PUFs vulnerability against modeling attacks by evaluating the security of XOR BR PUFs, XOR TBR PUFs, and obfuscated architectures of XOR BR PUF using a simplified mathematical model and deep learning (DL) techniques. DL modeling attacks were invoked against PUFs with different stage sizes (e.g. 64, 128, 256) and all are implemented on FPGA chips. Obtained results show that DL modeling attacks could easily break the security of 4-input XOR BR PUFs and 4-input XOR TBR PUFs with modeling accuracy $\\sim 99$%. Similar attacks were executed using single-layer neural networks (NN) and support vector machines (SVM) with polynomial kernel and the obtained results showed that single NNs failed to break the PUF security. Furthermore, SVM results confirmed the same modeling accuracy reported in previous research $(\\sim 50\\%)$. For the first time, this research empirically shows that DL networks can be used as powerful modeling techniques against these complex PUF architectures for which previous conventional machine learning techniques had failed. Furthermore, a detailed scalability analysis is conducted on the DL networks with respect to PUFs\u2019 stage size and complexity. The analysis shows that the number of layers and hidden neurons inside every layer has a linear relationship with PUFs\u2019 stage size, which agrees with the theoretical findings in deep learning. Consequently, A new obfuscated architecture is introduced as a first step to counter DL modeling attacks and it showed significant resistance against such attacks (16% - 40% less accuracy). This research provides an important step towards prioritizing the efforts to introduce new PUF architectures that are more secure and invulnerable to modeling attacks. Moreover, it triggers future discussions on the removal of influential bits and the level of obfuscation needed to confirm that a specific PUF architecture is resistant against powerful DL modeling attacks.", "venue": "2020 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)", "authors": ["Mahmoud  Khalafalla", "Mahmoud A. Elmohr", "Catherine  Gebotys"], "year": 2020, "n_citations": 2}
{"id": 3963553, "s2_id": "a4d35181e401e4d12d5f4cb5956bc87e596c283f", "title": "Modeling the non-linear behavior of library cells for an accurate static noise analysis", "abstract": "In signal integrity analysis, the joint effect of propagated noise through library cells, and of the noise injected on a quiet net by neighboring switching nets through coupling capacitances, must be considered in order to accurately estimate the overall noise impact on design functionality and performances. In this work the impact of the cell non-linearity on the noise glitch waveform is analyzed in detail, and a new macromodel that allows to accurately and efficiently model the non-linear effects of the victim driver in noise analysis is presented. Experimental results demonstrate the effectiveness of our method, and confirm that existing noise analysis approaches based on linear superposition of the propagated and crosstalk-injected noise can be highly inaccurate, thus impairing the sign-off functional verification phase.", "venue": "Design, Automation and Test in Europe", "authors": ["Cristiano  Forzan", "Davide  Pandini"], "year": 2005, "n_citations": 0}
{"id": 3966004, "s2_id": "a96488b94038debfbfebb546316b51f5be7eb3e9", "title": "Hybrid Cell Assignment and Sizing for Power, Area, Delay-Product Optimization of SRAM Arrays", "abstract": "Memory accounts for a considerable portion of the total power budget and area of digital systems. Furthermore, it is typically the performance bottleneck of the processing units. Therefore, it is critical to optimize the memory with respect to the product of power, area, and delay (PAD). We propose a hybrid cell assignment method based on multi-sized and dual- ${V} _{\\mathbf {th}}$ SRAM cells which improves the PAD cost function by 34% compared to the conventional cell assignment. We also utilize the sizing of SRAM cells for minimizing the data retention voltage, and voltages for the read and write operations in the SRAM array. Experimental results in a 32-nm technology show that combining the proposed hybrid cell assignment and the cell sizing methods can lower PAD by up to 41% when compared to the conventional cell design and assignment.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Ghasem  Pasandi", "Raghav  Mehta", "Massoud  Pedram", "Shahin  Nazarian"], "year": 2019, "n_citations": 1}
{"id": 3967678, "s2_id": "96cf16e39e778fe9fca7e8b79bfb8ee60fce21b0", "title": "Computer Arithmetic Preserving Hamming Distance of Operands in Operation Result", "abstract": "The traditional approach to fault tolerant computing involves replicating computation units and applying a majority vote operation on individual result bits. This approach, however, has several limitations; the most severe is the resource requirement. This paper presents a new method for fault tolerant computing where for a given error rate, the hamming distance between correct inputs and faulty inputs as well as the hamming distance between a correct result and a faulty result is preserved throughout processing thereby enabling correction of up to transient faults per computation cycle. The new method is compared and contrasted with current protection methods and its cost / performance is analyzed.", "venue": "ArXiv", "authors": ["Shlomi  Dolev", "Sergey  Frenkel", "Dan E. Tamir"], "year": 2011, "n_citations": 0}
{"id": 3968317, "s2_id": "509a98006abe3378c644ef9d2273df293c43bc5e", "title": "Sapphire: A Configurable Crypto-Processor for Post-Quantum Lattice-based Protocols", "abstract": "Public key cryptography protocols, such as RSA and elliptic curve cryptography, will be rendered insecure by Shor\u2019s algorithm when large-scale quantum computers are built. Cryptographers are working on quantum-resistant algorithms, and lattice-based cryptography has emerged as a prime candidate. However, high computational complexity of these algorithms makes it challenging to implement lattice-based protocols on low-power embedded devices. To address this challenge, we present Sapphire \u2013 a lattice cryptography processor with configurable parameters. Efficient sampling, with a SHA-3-based PRNG, provides two orders of magnitude energy savings; a single-port RAM-based number theoretic transform memory architecture is proposed, which provides 124k-gate area savings; while a low-power modular arithmetic unit accelerates polynomial computations. Our test chip was fabricated in TSMC 40nm low-power CMOS process, with the Sapphire cryptographic core occupying 0.28 mm2 area consisting of 106k logic gates and 40.25 KB SRAM. Sapphire can be programmed with custom instructions for polynomial arithmetic and sampling, and it is coupled with a low-power RISC-V micro-processor to demonstrate NIST Round 2 lattice-based CCA-secure key encapsulation and signature protocols Frodo, NewHope, qTESLA, CRYSTALS-Kyber and CRYSTALS-Dilithium, achieving up to an order of magnitude improvement in performance and energy-efficiency compared to state-of-the-art hardware implementations. All key building blocks of Sapphire are constant-time and secure against timing and simple power analysis side-channel attacks. We also discuss how masking-based DPA countermeasures can be implemented on the Sapphire core without any changes to the hardware.", "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.", "authors": ["Utsav  Banerjee", "Tenzin S. Ukyab", "Anantha P. Chandrakasan"], "year": 2019, "n_citations": 59}
{"id": 3977373, "s2_id": "9f923aad56b2b20b07f0b9f62b9e9316234900ad", "title": "SAMO: Optimised Mapping of Convolutional Neural Networks to Streaming Architectures", "abstract": "Toolflows that map Convolutional Neural Network (CNN) models to Field Programmable Gate Arrays (FPGAs) have been an important tool in accelerating a range of applications across different deployment settings. However, the significance of the problem of finding an optimal mapping is often overlooked, with the expectation that the end user will tune their generated hardware to their desired platform. This is particularly prominent within Streaming Architectures toolflows, where there is a large design space to explore. There have been many Streaming Architectures proposed [1]\u2013[3], however apart from fpgaConvNet [1], there is limited support for optimisation methods that explore both performance objectives and platform constraints. In this work, we establish a framework, SAMO: a Streaming Architecture Mapping Optimiser, which generalises the optimisation problem of mapping Streaming Architectures to FPGA platforms. We also implement both Brute Force and Simulated Annealing optimisation methods in order to generate valid, high performance designs for a range of target platforms and CNN models. We are able to observe a 4x increase in performance compared to example designs for the popular Streaming Architecture framework FINN [3].", "venue": "ArXiv", "authors": ["Alexander  Montgomerie-Corcoran", "Zhewen  Yu", "Christos-Savvas  Bouganis"], "year": 2021, "n_citations": 0}
{"id": 3983152, "s2_id": "028ed4c17c2d0d04d3317dac9ea2a5b1d554374b", "title": "DORY: Automatic End-to-End Deployment of Real-World DNNs on Low-Cost IoT MCUs", "abstract": "The deployment of Deep Neural Networks (DNNs) on end-nodes at the extreme edge of the Internet-of-Things is a critical enabler to support pervasive Deep Learning-enhanced applications. Low-Cost MCU-based end-nodes have limited on-chip memory and often replace caches with scratchpads, to reduce area overheads and increase energy efficiency \u2013 requiring explicit DMA-based memory transfers between different levels of the memory hierarchy. Mapping modern DNNs on these systems requires aggressive topology-dependent tiling and double-buffering. In this work, we propose DORY (Deployment Oriented to memoRY) \u2013 an automatic tool to deploy DNNs on low cost MCUs with typically less than 1MB of on-chip SRAM memory. DORY abstracts tiling as a Constraint Programming (CP) problem: it maximizes L1 memory utilization under the topological constraints imposed by each DNN layer. Then, it generates ANSI C code to orchestrate off- and on-chip transfers and computation phases. Furthermore, to maximize speed, DORY augments the CP formulation with heuristics promoting performance-effective tile sizes. As a case study for DORY, we target GreenWaves Technologies GAP8, one of the most advanced parallel ultra-low power MCU-class devices on the market. On this device, DORY achieves up to 2.5\u00d7 better MAC/cycle than the GreenWaves proprietary software solution and 18.1\u00d7 better than the state-of-the-art result on an STM32-H743 MCU on single layers. Using our tool, GAP-8 can perform end-to-end inference of a 1.0-MobileNet-128 network consuming just 63 pJ/MAC on average @ 4.3 fps \u2013 15.4\u00d7 better than an STM32-H743. We release all our developments \u2013 the DORY framework, the optimized backend kernels, and the related heuristics \u2013 as open-source software.", "venue": "IEEE Transactions on Computers", "authors": ["Alessio  Burrello", "Angelo  Garofalo", "Nazareno  Bruschi", "Giuseppe  Tagliavini", "Davide  Rossi", "Francesco  Conti"], "year": 2021, "n_citations": 15}
{"id": 3987253, "s2_id": "8053c8b13550a1360bfe6e76029a28dc7b7cd75c", "title": "Serpens: A High Bandwidth Memory Based Accelerator for General-Purpose Sparse Matrix-Vector Multiplication", "abstract": "Sparse matrix-vector multiplication (SpMV) multiplies a sparse matrix with a dense vector. SpMV plays a crucial role in many applications, from graph analytics to deep learning. The random memory accesses of the sparse matrix make accelerator design challenging. However, high bandwidth memory (HBM) based FPGAs are a good fit for designing accelerators for SpMV. In this paper, we present Serpens, an HBM based accelerator for general-purpose SpMV. Serpens features (1) a general-purpose design, (2) memory-centric processing engines, and (3) index coalescing to support the efficient processing of arbitrary SpMVs. From the evaluation of twelve large-size matrices, Serpens is 1.91\u00d7 and 1.76\u00d7 better in terms of geomean throughput than the latest accelerators GraphLiLy and Sextans, respectively. We also evaluate 2,519 SuiteSparse matrices, and Serpens achieves 2.10\u00d7 higher throughput than a K80 GPU. For the energy efficiency, Serpens is 1.71\u00d7, 1.90\u00d7, and 42.7\u00d7 better compared with GraphLily, Sextans, and K80, respectively. After scaling up to 24 HBM channels, Serpens achieves up to 30,204 MTEPS and up to 3.79\u00d7 over GraphLily.", "venue": "ArXiv", "authors": ["Linghao  Song", "Yuze  Chi", "Licheng  Guo", "Jason  Cong"], "year": 2021, "n_citations": 0}
{"id": 3987277, "s2_id": "f15cbfb0a560f2d93ad30a425c4649bd76545409", "title": "Asymmetric Aging Effect on Modern Microprocessors", "abstract": "Reliability is a crucial requirement in any modern microprocessor to assure correct execution over its lifetime. As mission critical components are becoming common in commodity systems; e.g., control of autonomous cars, the demand for reliable processing has even further heightened. Latest process technologies even worsened the situation; thus, microprocessors design has become highly susceptible to reliability concerns. This paper examines asymmetric aging phenomenon, which is a major reliability concern in advanced process nodes. In this phenomenon, logical elements and memory cells suffer from unequal timing degradation over time and consequently introduce reliability concerns. So far, most studies approached asymmetric aging from circuit or physical design viewpoint, but these solutions were quite limited and suboptimal. In this paper we introduce an asymmetric aging aware micro-architecture that aims at reducing its impact. The study is mainly focused on the following subsystems: execution units, register files and the memory hierarchy. Our experiments indicate that the proposed solutions incur minimal overhead while significantly mitigating the asymmetric aging stress.", "venue": "ArXiv", "authors": ["Freddy  Gabbay", "Avi  Mendelson"], "year": 2020, "n_citations": 0}
{"id": 3995471, "s2_id": "ecdeb0091b63bba525a1abfb630969ea76fb059a", "title": "BlockHammer: Preventing RowHammer at Low Cost by Blacklisting Rapidly-Accessed DRAM Rows", "abstract": "Aggressive memory density scaling causes modern DRAM devices to suffer from RowHammer, a phenomenon where rapidly activating (i.e., hammering) a DRAM row can cause bit-flips in physically-nearby rows. Recent studies demonstrate that modern DDR4/LPDDR4 DRAM chips, including chips previously marketed as RowHammer-safe, are even more vulnerable to RowHammer than older DDR3 DRAM chips. Many works show that attackers can exploit RowHammer bit-flips to reliably mount system-level attacks to escalate privilege and leak private data. Therefore, it is critical to ensure RowHammersafe operation on all DRAM-based systems as they become increasingly more vulnerable to RowHammer. Unfortunately, state-of-the-art RowHammer mitigation mechanisms face two major challenges. First, they incur increasingly higher performance and/or area overheads when applied to more vulnerable DRAM chips. Second, they require either closely-guarded proprietary information about the DRAM chips\u2019 physical circuit layouts or modifications to the DRAM chip design.In this paper, we show that it is possible to efficiently and scalably prevent RowHammer bit-flips without knowledge of or modification to DRAM internals. To this end, we introduce BlockHammer, a low-cost, effective, and easy-to-adopt RowHammer mitigation mechanism that prevents all RowHammer bit-flips while overcoming the two key challenges. BlockHammer selectively throttles memory accesses that could otherwise potentially cause RowHammer bit-flips. The key idea of BlockHammer is to (1) track row activation rates using area-efficient Bloom filters, and (2) use the tracking data to ensure that no row is ever activated rapidly enough to induce RowHammer bit-flips. By guaranteeing that no DRAM row ever experiences a RowHammer-unsafe activation rate, BlockHammer (1) makes it impossible for a RowHammer bit-flip to occur and (2) greatly reduces a RowHammer attack\u2019s impact on the performance of co-running benign applications. Our evaluations across a comprehensive range of 280 workloads show that, compared to the best of six state-of-the-art RowHammer mitigation mechanisms (all of which require knowledge of or modification to DRAM internals), BlockHammer provides (1) competitive performance and energy when the system is not under a RowHammer attack and (2) significantly better performance and energy when the system is under a RowHammer attack.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Abdullah Giray Yaglik\u00e7i", "Minesh  Patel", "Jeremie S. Kim", "Roknoddin  Azizi", "Ataberk  Olgun", "Lois  Orosa", "Hasan  Hassan", "Jisung  Park", "Konstantinos  Kanellopoulos", "Taha  Shahroodi", "Saugata  Ghose", "Onur  Mutlu"], "year": 2021, "n_citations": 14}
{"id": 3996237, "s2_id": "97d34e938d58d2485b933a7b2c0f6c574a835813", "title": "A Design Space Exploration Methodology for Parameter Optimization in Multicore Processors", "abstract": "The need for application-specific design of multicore/manycore processing platforms is evident with computing systems finding use in diverse application domains. In order to tailor multicore/manycore processors for application specific requirements, a multitude of processor design parameters have to be tuned accordingly which involves rigorous and extensive design space exploration over large search spaces. In this paper, we propose an efficient methodology for design space exploration. We evaluate our methodology over two search spaces small and large, using a cycle-accurate simulator (ESESC) and a standard set of PARSEC and SPLASH-2 benchmarks. For the smaller design space, we compare results obtained from our design space exploration methodology with results obtained from fully exhaustive search. The results show that solution quality obtained from our methodology are within 1.35 - 3.69 percent of the results obtained from fully exhaustive search while only exploring 2.74 - 3 percent of the design space. For larger design space, we compare solution quality of different results obtained by varying the number of tunable processor design parameters included in the exhaustive search phase of our methodology. The results show that including more number of tunable parameters in the exhaustive search phase of our methodology greatly improves solution quality.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Prasanna  Kansakar", "Arslan  Munir"], "year": 2016, "n_citations": 6}
{"id": 3997083, "s2_id": "568f3d2ed5a2c2990054b7d89ff9842006f7f401", "title": "Performance Analysis and Optimization Opportunities for NVIDIA Automotive GPUs", "abstract": "Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD) bring unprecedented performance requirements for automotive systems. Graphic Processing Unit (GPU) based platforms have been deployed with the aim of meeting these requirements, being NVIDIA Jetson TX2 and its high-performance successor, NVIDIA AGX Xavier, relevant representatives. However, to what extent high-performance GPU configurations are appropriate for ADAS and AD workloads remains as an open question. This paper analyzes this concern and provides valuable insights on this question by modeling two recent automotive NVIDIA GPU-based platforms, namely TX2 and AGX Xavier. In particular, our work assesses their microarchitectural parameters against relevant benchmarks, identifying GPU setups delivering increased performance within a similar cost envelope, or decreasing hardware costs while preserving original performance levels. Overall, our analysis identifies opportunities for the optimization of automotive GPUs to further increase system efficiency.", "venue": "J. Parallel Distributed Comput.", "authors": ["Hamid  Tabani", "Fabio  Mazzocchetti", "Pedro  Benedicte", "Jaume  Abella", "Francisco J. Cazorla"], "year": 2021, "n_citations": 2}
{"id": 4003627, "s2_id": "41263469ebaf3683c0a002b7b931fcd01ff41911", "title": "Rapid Cycle-Accurate Simulator for High-Level Synthesis", "abstract": "A large semantic gap between the high-level synthesis (HLS) design and the low-level (on-board or RTL) simulation environment often creates a barrier for those who are not FPGA experts. Moreover, such low-level simulation takes a long time to complete. Software-based HLS simulators can help bridge this gap and accelerate the simulation process; however, we found that the current FPGA HLS commercial software simulators sometimes produce incorrect results. In order to solve this correctness issue while maintaining the high speed of a software-based simulator, this paper proposes a new HLS simulation flow named FLASH. The main idea behind the proposed flow is to extract the scheduling information from the HLS tool and automatically construct an equivalent cycle-accurate simulation model while preserving C semantics. Experimental results show that FLASH runs three orders of magnitude faster than the RTL simulation.", "venue": "FPGA", "authors": ["Yuze  Chi", "Young-kyu  Choi", "Jason  Cong", "Jie  Wang"], "year": 2019, "n_citations": 7}
{"id": 4007826, "s2_id": "63603b6324c28e07210d00d1d83bd8ef762ee64c", "title": "Multivalued circuits and Interconnect issues", "abstract": "Many papers have presented multi-valued circuits in various technologies as a solution to reduce or solve interconnection issues in binary circuits. This assumption is discussed. While 4-valued signaling could divide by two the number of interconnects between building blocks, it turns out that circuit designers use interconnect standards based on differential pairs such as PCIe, Infiniband, RapidIO, NVLink, etc. Doubling the number of binary signals is a better solution than using single-ended quaternary signals. The design of quaternary basic gates, adders and multipliers are compared with the corresponding binary ones. At each level, the transistor count ratio between quaternary and binary circuits is greater than the x2 information ratio between base 4 and base 2. Quaternary signaling is not a solution, either between or within circuit blocks .", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2020, "n_citations": 0}
{"id": 4010776, "s2_id": "db1de45e985876e01fbcbd497e04f74213781af2", "title": "DeACT: Architecture-Aware Virtual Memory Support for Fabric Attached Memory Systems", "abstract": "1 The exponential growth of data has driven technology providers to develop new protocols, such as cache coherent interconnects and memory semantic fabrics, to help users and facilities leverage advances in memory technologies to satisfy these growing memory and storage demands. Using these new protocols, fabric-attached memories (FAM) can be directly attached to a system interconnect and be easily integrated with a variety of processing elements (PEs). Moreover, systems that support FAM can be smoothly upgraded and allow multiple PEs to share the FAM memory pools using well-defined protocols. The sharing of FAM between PEs allows efficient data sharing, improves memory utilization, reduces cost by allowing flexible integration of different PEs and memory modules from several vendors, and makes it easier to upgrade the system. One promising use-case for FAMs is in High-Performance Compute (HPC) systems, where the underutilization of memory is a major challenge. However, adopting FAMs in HPC systems brings new challenges. In addition to cost, flexibility, and efficiency, one particular problem that requires rethinking is virtual memory support for security and performance. To address these challenges, this paper presents decoupled access control and address translation (DeACT), a novel virtual memory implementation that supports HPC systems equipped with FAM. Compared to the state-of-the-art two-level translation approach, DeACT achieves speedup of up to 4.59x (1.8x on average) without compromising security.1Part of this work was done when Vamsee was working under the supervision of Amro Awad at UCF. Amro Awad is now with the ECE Department at NC State.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Vamsee Reddy Kommareddy", "Clayton  Hughes", "Simon David Hammond", "Amro  Awad"], "year": 2021, "n_citations": 2}
{"id": 4014563, "s2_id": "df53d5ae8bc5f0fe27d8b319cb9801ff28ed6b50", "title": "Simulating spin systems on IANUS, an FPGA-based computer", "abstract": "We describe the hardwired implementation of algorithms for Monte Carlo simulations of a large class of spin models. We have implemented these algorithms as VHDL codes and we have mapped them onto a dedicated processor based on a large FPGA device. The measured performance on one such processor is comparable to O(100) carefully programmed high-end PCs: it turns out to be even better for some selected spin models. We describe here codes that we are currently executing on the IANUS massively parallel FPGA-based system.", "venue": "Comput. Phys. Commun.", "authors": ["Francesco  Belletti", "Maria  Cotallo", "Andr\u00e9s Cruz Flor", "Luis Antonio Fern\u00e1ndez", "Antonio  Gordillo", "Andrea  Maiorano", "Filippo  Mantovani", "Enzo  Marinari", "Victor  Martin-Mayor", "Antonio Mu\u00f1oz Sudupe", "Denis  Navarro", "Sergio Perez Gaviro", "Juan Jesus Ruiz-Lorenzo", "Sebastiano Fabio Schifano", "Daniele  Sciretti", "Alfonso  Taranc\u00f3n", "Raffaele  Tripiccione", "Jose Luis Velasco"], "year": 2008, "n_citations": 48}
{"id": 4017021, "s2_id": "3e5fa9de9c0dacfdc9488b61e4e4534f0d087504", "title": "A Complexity Reduction Method for Successive Cancellation List Decoding", "abstract": "This brief introduces a hardware complexity reduction method for successive cancellation list (SCL) decoders. Specifically, we propose to use a sorting scheme so that <inline-formula> <tex-math notation=\"LaTeX\">$L$ </tex-math></inline-formula> paths with smallest path metrics are also sorted according to their path indexes for path pruning. We prove that such sorting scheme reduces the input number of multiplexers in any hardware implementation of SCL decoding from <inline-formula> <tex-math notation=\"LaTeX\">$L$ </tex-math></inline-formula> to <inline-formula> <tex-math notation=\"LaTeX\">$(L/2+1)$ </tex-math></inline-formula> without any changes in the decoding latency. Field programmable gate array (FPGA) implementations show that the proposed method achieves significant gain in hardware consumptions, especially for large list sizes and block lengths.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Onur  Dizdar"], "year": 2020, "n_citations": 0}
{"id": 4018477, "s2_id": "57d896ff1559aabc12e821da598c82e0ed78da02", "title": "FPGA Implementation of Minimum Mean Brightness Error Bi-Histogram Equalization", "abstract": "Histogram Equalization (HE) is a popular method for contrast enhancement. Generally, mean brightness is not conserved in Histogram Equalization. Initially, Bi-Histogram Equalization (BBHE) was proposed to enhance contrast while maintaining a the mean brightness. However, when mean brightness is primary concern, Minimum Mean Brightness Error Bi-Histogram Equalization (MMBEBHE) is the best technique. There are several implementations of Histogram Equalization on FPGA, however to our knowledge MMBEBHE has not been implemented on FPGAs before. Therefore, we present an implementation of MMBEBHE on FPGA.", "venue": "ArXiv", "authors": ["Abhishek  Saroha", "Avichal  Rakesh", "Rajiv Kumar Tripathi"], "year": 2020, "n_citations": 0}
{"id": 4018637, "s2_id": "fc367371bd3c2034b3a3d26ea44a59f4bdfc5e18", "title": "Brain-inspired Cognition in Next Generation Racetrack Memories", "abstract": "Hyperdimensional computing (HDC) is an emerging computational framework inspired by the brain that operates on vectors with thousands of dimensions to emulate cognition. Unlike conventional computational frameworks that operate on numbers, HDC, like the brain, uses high dimensional random vectors and is capable of one-shot learning. HDC is based on a well-defined set of arithmetic operations and is highly error-resilient. The core operations of HDC manipulate HD vectors in bulk bit-wise fashion, offering many opportunities to leverage parallelism. Unfortunately, on conventional Von-Neuman architectures, the continuous movement of HD vectors among the processor and the memory can make the cognition task prohibitively slow and energy-intensive. Hardware accelerators only marginally improve related metrics. On the contrary, only partial implementation of an HDC framework inside memory, using emerging memristive devices, has reported considerable performance/energy gains. This paper presents an architecture based on racetrack memory (RTM) to conduct and accelerate the entire HDC framework within the memory. The proposed solution requires minimal additional CMOS circuitry and uses a read operation across multiple domains in RTMs called transverse read (TR) to realize exclusive-or (XOR) and addition operations. To minimize the overhead the CMOS circuitry, we propose an RTM nanowires-based counting mechanism that leverages the TR operation and the standard RTM operations. Using language recognition as the use case demonstrates 7.8\u00d7 and 5.3\u00d7 reduction in the overall runtime and energy consumption compared to the FPGA design, respectively. Compared to the state-of-the-art in-memory implementation, the proposed HDC system reduces the energy consumption by 8.6\u00d7.", "venue": "ArXiv", "authors": ["Asif Ali Khan", "Sebastien  Ollivier", "Stephen  Longofono", "Gerald  Hempel", "Jeronimo  Castrillon", "Alex K. Jones"], "year": 2021, "n_citations": 0}
{"id": 4019178, "s2_id": "15afcc24992290ccb6eb69ebc27aa5a35b8afb93", "title": "An Energy-Efficient Heterogeneous Memory Architecture for Future Dark Silicon Embedded Chip-Multiprocessors", "abstract": "Main memories play an important role in overall energy consumption of embedded systems. Using conventional memory technologies in future designs in nanoscale era causes a drastic increase in leakage power consumption and temperature-related problems. Emerging non-volatile memory (NVM) technologies offer many desirable characteristics such as near-zero leakage power, high density and non-volatility. They can significantly mitigate the issue of memory leakage power in future embedded chip-multiprocessor (eCMP) systems. However, they suffer from challenges such as limited write endurance and high write energy consumption which restrict them for adoption in modern memory systems. In this article, we present a convex optimization model to design a 3D stacked hybrid memory architecture in order to minimize the future embedded systems energy consumption in the dark silicon era. This proposed approach satisfies endurance constraint in order to design a reliable memory system. Our convex model optimizes numbers and placement of eDRAM and STT-RAM memory banks on the memory layer to exploit the advantages of both technologies in future eCMPs. Energy consumption, the main challenge in the dark silicon era, is represented as a major target in this work and it is minimized by the detailed optimization model in order to design a dark silicon aware 3D Chip-Multiprocessor. Experimental results show that in comparison with the Baseline memory design, the proposed architecture improves the energy consumption and performance of the 3D CMP on average about 61.33 and 9 percent respectively.", "venue": "IEEE Transactions on Emerging Topics in Computing", "authors": ["Salman  Onsori", "Arghavan  Asad", "Kaamran  Raahemifar", "Mahmood  Fathy"], "year": 2019, "n_citations": 6}
{"id": 4021266, "s2_id": "7288f8c8f99c42c0a39ec7309bee97e3ab78b4ef", "title": "ZCSD: a Computational Storage Device over Zoned Namespaces (ZNS) SSDs", "abstract": "The Big Data trend is putting strain on modern storage systems, which have to support high-performance I/O accesses for the large quantities of data. With the prevalent Von Neumann computing architecture, this data is constantly moved back and forth between the computing (i.e., CPU) and storage entities (DRAM, Non-Volatile Memory NVM storage). Hence, as the data volume grows, this constant data movement between the CPU and storage devices has emerged as a key performance bottleneck. To improve the situation, researchers have advocated to leverage computational storage devices (CSDs), which offer a programmable interface to run userdefined data processing operations close to the storage without excessive data movement, thus offering performance improvements. However, despite its potential, building CSD-aware applications remains a challenging task due to the lack of exploration and experimentations with the right API and abstraction. This is due to the limited accessibility to latest CSD/NVM devices, emerging device interfaces, and closed-source software internals of the devices. To remedy the situation, in this work we present an open-source CSD prototype over emerging NVMe Zoned Namespaces (ZNS) SSDs and an interface that can be used to explore application designs for CSD/NVM storage devices. In this paper we summarize the current state of the practice with CSD devices, make a case for designing a CSD prototype with the ZNS interface and eBPF (ZCSD), and present our initial findings. The prototype is available at https://github.com/Dantali0n/qemu-csd.", "venue": "ArXiv", "authors": ["Corne  Lukken", "Giulia  Frascaria", "Animesh  Trivedi"], "year": 2021, "n_citations": 0}
{"id": 4025361, "s2_id": "41a42bf05836a949b1e73067ce82056f5029648f", "title": "Advances in computer architecture", "abstract": "In the past, efforts were taken to improve the performance of a processor via frequency scaling. However, industry has reached the limits of increasing the frequency and therefore concurrent execution of instructions on multiple cores seems the only possible option. It is not enough to provide concurrent execution by the hardware, software also have to introduce concurrency in order to exploit the parallelism.", "venue": "ArXiv", "authors": ["M. Irfan Uddin"], "year": 2013, "n_citations": 4}
{"id": 4027322, "s2_id": "a07efc0915da8cd2aa4b1935eed7e41b7da60c5e", "title": "Substream-Centric Maximum Matchings on FPGA", "abstract": "Developing high-performance and energy-efficient algorithms for maximum matchings is becoming increasingly important in social network analysis, computational sciences, scheduling, and others. In this work, we propose the first maximum matching algorithm designed for FPGAs; it is energy-efficient and has provable guarantees on accuracy, performance, and storage utilization. To achieve this, we forego popular graph processing paradigms, such as vertex-centric programming, that often entail large communication costs. Instead, we propose a substream-centric approach, in which the input stream of data is divided into substreams processed independently to enable more parallelism while lowering communication costs. We base our work on the theory of streaming graph algorithms and analyze 14 models and 28 algorithms. We use this analysis to provide theoretical underpinning that matches the physical constraints of FPGA platforms. Our algorithm delivers high performance (more than 4\u00d7 speedup over tuned parallel CPU variants), low memory, high accuracy, and effective usage of FPGA resources. The substream-centric approach could easily be extended to other algorithms to offer low-power and high-performance graph processing on FPGAs.", "venue": "ACM Trans. Reconfigurable Technol. Syst.", "authors": ["Maciej  Besta", "Marc  Fischer", "Tal  Ben-Nun", "Dimitri  Stanojevic", "Johannes de Fine Licht", "Torsten  Hoefler"], "year": 2020, "n_citations": 0}
{"id": 4027841, "s2_id": "170ca1d446d2b3a77bd26c7b69dc9bdebef7d237", "title": "Software Variants for Hardware Trojan Detection and Resilience in COTS Processors", "abstract": "The commercial off-the-shelf (COTS) component based ecosystem provides an attractive system design paradigm due to the drastic reduction in development time and cost compared to custom solutions. However, it brings in a growing concern of trustworthiness arising from the possibility of embedded malicious logic, or hardware Trojans in COTS components. Existing trust-verification approaches, are typically not applicable to COTS hardware due to the absence of golden models and the lack of observability of internal signals. In this work, we propose a novel approach for runtime Trojan detection and resilience in untrusted COTS processors through judicious modifications in software. The proposed approach does not rely on any hardware redundancy or architectural modification and hence seamlessly integrates with the COTS-based system design process. Trojan resilience is achieved through the execution of multiple functionally equivalent software variants. We have developed and implemented a solution for compiler-based automatic generation of program variants, metric-guided selection of variants, and their integration in a single executable. To evaluate the proposed approach, we first analyzed the effectiveness of program variants in avoiding the activation of a random pool of Trojans. By implementing several Trojans in an OpenRISC 1000 processor, we analyzed the detectability and resilience during Trojan activation in both single and multiple variants. We also present delay and code size overhead for the automatically generated variants for several programs and discuss future research directions to reduce the overhead.", "venue": "ArXiv", "authors": ["Mahmudul  Hasan", "Jonathan  Cruz", "Prabuddha  Chakraborty", "Swarup  Bhunia", "Tamzidul  Hoque"], "year": 2021, "n_citations": 0}
{"id": 4028826, "s2_id": "a730eb4f3b5b0f5d913246f845c5a440ec7668d5", "title": "A Low Power In-Memory Multiplication andAccumulation Array with Modified Radix-4 Inputand Canonical Signed Digit Weights", "abstract": "A mass of data transfer between the processing and storage units has been the leading bottleneck in modern VonNeuman computing systems, especially when used for Artificial Intelligence (AI) tasks. Computing-in-Memory (CIM) has shown great potential to reduce both latency and power consumption. However, the conventional analog CIM schemes are suffering from reliability issues, which may significantly degenerate the accuracy of the computation. Recently, CIM schemes with digitized input data and weights have been proposed for high reliable computing. However, the properties of the digital memory and input data are not fully utilized. This paper presents a novel low power CIM scheme to further reduce the power consumption by using a Modified Radix-4 (M-RD4) booth algorithm at the input and a Modified Canonical Signed Digit (M-CSD) for the network weights. The simulation results show that M-Rd4 and M-CSD reduce the ratio of 1\u00d7 1 by 78.5% on LeNet and 80.2% on AlexNet, and improve the computing efficiency by 41.6% in average. The computing-power rate at the fixed-point 8-bit is 60.68 TOPS/s/W.", "venue": "ArXiv", "authors": ["Rui  Xiao", "Kejie  Huang", "Yewei  Zhang", "Haibin  Shen"], "year": 2021, "n_citations": 0}
{"id": 4031499, "s2_id": "1d1782862634263bb4b6ab6c42a9fbfb49f392e8", "title": "Prototyping RISC Based, Reconfigurable Networking Applications in Open Source", "abstract": "In the last decade we have witnessed a rapid growth in data center systems, requiring new and highly complex networking devices. The need to refresh networking infrastructure whenever new protocols or functions are introduced, and the increasing costs that this entails, are of a concern to all data center providers. New generations of Systems on Chip (SoC), integrating microprocessors and higher bandwidth interfaces, are an emerging solution to this problem. These devices permit entirely new systems and architectures that can obviate the replacement of existing networking devices while enabling seamless functionality change. In this work, we explore open source, RISC based, SoC architectures with high performance networking capabilities. The prototype architectures are implemented on the NetFPGA-SUME platform. Beyond details of the architecture, we also describe the hardware implementation and the porting of operating systems to the platform. The platform can be exploited for the development of practical networking appliances, and we provide use case examples.", "venue": "ArXiv", "authors": ["Jong Hun Han", "Noa  Zilberman", "Bjoern A. Zeeb", "Andreas  Fiessler", "Andrew W. Moore"], "year": 2016, "n_citations": 1}
{"id": 4045939, "s2_id": "e8331b11a22a5ad27fd494558e08b008b0fcccb2", "title": "Memory controller design under cloud workloads", "abstract": "This work studies the behavior of state-of-the-art memory controller designs when executing scale-out workloads. It considers memory scheduling techniques, memory page management policies, the number of memory channels, and the address mapping scheme used. Experimental measurements demonstrate: 1) Several recently proposed memory scheduling policies are not a good match for these scale-out workloads. 2) The relatively simple First-Ready-First-Come- First-Served (FR-FCFS) policy performs consistently better, and 3) for most of the studied workloads, the even simpler First-Come-First-Served scheduling policy is within 1% of FRFCFS. 4) Increasing the number of memory channels offers negligible performance benefits, e.g., performance improves by 1.7% on average for 4-channels vs. 1-channel. 5) 77%- 90% of DRAM rows activations are accessed only once before closure. These observation can guide future development and optimization of memory controllers for scale-out workloads.", "venue": "2016 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Mostafa  Mahmoud", "Andreas  Moshovos"], "year": 2016, "n_citations": 1}
{"id": 4052007, "s2_id": "4747473883a7d6fcb6c4409a8297f3b551eec257", "title": "AIRCHITECT: Learning Custom Architecture Design and Mapping Space", "abstract": "Design space exploration is an important but costly step involved in the design/deployment of custom architectures to squeeze out maximum possible performance and energy efficiency. Conventionally, optimizations require iterative sampling of the design space using simulation or heuristic tools. In this paper we investigate the possibility of learning the optimization task using machine learning (ML) and hence using the learnt model to predict optimal parameters for the design and mapping space of custom architectures, bypassing any exploration step. We use three case studies involving the optimal array design, SRAM buffer sizing, mapping, and schedule determination for systolicarray-based custom architecture design and mapping space. We perform systematic design-aware and statistical analysis of the optimization space for our case studies and highlight the patterns in the design space. We formulate the architecture design and mapping as a ML problem that allows us to leverage existing ML models for training and inference. We design and train a custom network architecture called AIRCHITECT, which is capable of learning the architecture design space with as high as 94.3% test accuracy and predicting optimal configurations which achieve on average (GeoMean) of 99.9% the best possible performance on a test dataset with 105 GEMM workloads.", "venue": "ArXiv", "authors": ["Ananda  Samajdar", "Jan Moritz Joseph", "Matthew  Denton", "Tushar  Krishna"], "year": 2021, "n_citations": 0}
{"id": 4055524, "s2_id": "90fd6c214ab9fe4d6c5f469b870e1f3206344f7c", "title": "Third ArchEdge Workshop: Exploring the Design Space of Efficient Deep Neural Networks", "abstract": "This paper gives an overview of our ongoing work on the design space exploration of efficient deep neural networks (DNNs). Specifically, we cover two aspects: (1) static architecture design efficiency and (2) dynamic model execution efficiency. For static architecture design, different from existing end-to-end hardware modeling assumptions, we conduct full-stack profiling at the GPU core level to identify better accuracy-latency trade-offs for DNN designs. For dynamic model execution, different from prior work that tackles model redundancy at the DNN-channels level, we explore a new dimension of DNN feature map redundancy to be dynamically traversed at runtime. Last, we highlight several open questions that are poised to draw research attention in the next few years.", "venue": "ArXiv", "authors": ["Fuxun  Yu", "Dimitrios  Stamoulis", "Di  Wang", "Dimitrios  Lymberopoulos", "Xiang  Chen"], "year": 2020, "n_citations": 0}
{"id": 4056102, "s2_id": "72264715cedaf3162b0c60b4d313f3bb28d7ed20", "title": "Enabling mixed-precision quantized neural networks in extreme-edge devices", "abstract": "The deployment of Quantized Neural Networks (QNN) on advanced microcontrollers requires optimized software to exploit digital signal processing (DSP) extensions of modern instruction set architectures (ISA). As such, recent research proposed optimized libraries for QNNs (from 8-bit to 2-bit) such as CMSIS-NN and PULP-NN. This work presents an extension to the PULP-NN library targeting the acceleration of mixed-precision Deep Neural Networks, an emerging paradigm able to significantly shrink the memory footprint of deep neural networks with negligible accuracy loss. The library, composed of 27 kernels, one for each permutation of input feature maps, weights, and output feature maps precision (considering 8-bit, 4-bit and 2-bit), enables efficient inference of QNN on parallel ultra-low-power (PULP) clusters of RISC-V based processors, featuring the RV32IMCXpulpV2 ISA. The proposed solution, benchmarked on an 8-cores GAP-8 PULP cluster, reaches peak performance of 16 MACs/cycle on 8 cores, performing 21\u00d7 to 25\u00d7 faster than an STM32H7 (powered by an ARM Cortex M7 processor) with 15\u00d7 to 21\u00d7 better energy efficiency.", "venue": "CF", "authors": ["Nazareno  Bruschi", "Angelo  Garofalo", "Francesco  Conti", "Giuseppe  Tagliavini", "Davide  Rossi"], "year": 2020, "n_citations": 4}
{"id": 4057050, "s2_id": "cc95830a57010a960fda72531e1f811cafc14b77", "title": "GraphR: Accelerating Graph Processing Using ReRAM", "abstract": "Graph processing recently received intensive interests in light of a wide range of needs to understand relationships. It is well-known for the poor locality and high memory bandwidth requirement. In conventional architectures, they incur a significant amount of data movements and energy consumption which motivates several hardware graph processing accelerators. The current graph processing accelerators rely on memory access optimizations or placing computation logics close to memory. Distinct from all existing approaches, we leverage an emerging memory technology to accelerate graph processing with analog computation. This paper presents GRAPHR, the first ReRAM-based graph processing accelerator. GRAPHR follows the principle of near-data processing and explores the opportunity of performing massive parallel analog operations with low hardware and energy cost. The analog computation is suitable for graph processing because: 1) The algorithms are iterative and could inherently tolerate the imprecision; 2) Both probability calculation (e.g., PageRank and Collaborative Filtering) and typical graph algorithms involving integers (e.g., BFS/SSSP) are resilient to errors. The key insight of GRAPHR is that if a vertex program of a graph algorithm can be expressed in sparse matrix vector multiplication (SpMV), it can be efficiently performed by ReRAM crossbar. We show that this assumption is generally true for a large set of graph algorithms. GRAPHR is a novel accelerator architecture consisting of two components: memory ReRAM and graph engine (GE). The core graph computations are performed in sparse matrix format in GEs (ReRAM crossbars). The vector/matrix-based graph computation is not new, but ReRAM offers the unique opportunity to realize the massive parallelism with unprecedented energy efficiency and low hardware cost. With small subgraphs processed by GEs, the gain of performing parallel operations overshadows the wastes due to sparsity. The experiment results show that GRAPHR achieves a 16.01\u00d7 (up to 132.67\u00d7) speedup and a 33.82\u00d7 energy saving on geometric mean compared to a CPU baseline system. Compared to GPU, GRAPHR achieves 1.69\u00d7 to 2.19\u00d7 speedup and consumes 4.77\u00d7 to 8.91\u00d7 less energy. GRAPHR gains a speedup of 1.16\u00d7 to 4.12\u00d7, and is 3.67\u00d7 to 10.96\u00d7 more energy efficiency compared to PIM-based architecture.", "venue": "2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Linghao  Song", "Youwei  Zhuo", "Xuehai  Qian", "Hai  Li", "Yiran  Chen"], "year": 2018, "n_citations": 116}
{"id": 4059828, "s2_id": "547bdba25603b088e67cd805dc450e6da3496f40", "title": "NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems", "abstract": "This paper introduces NVCACHE, an approach that uses a non-volatile main memory (NVMM) as a write cache to improve the write performance of legacy applications. We compare NVCACHE against file systems tailored for NVMM (Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite, RocksDB). Our evaluation shows that NVCACHE reaches the performance level of the existing state-of-the-art systems for NVMM, but without their limitations: NVCACHE does not limit the size of the stored data to the size of the NVMM, and works transparently with unmodified legacy applications, providing additional persistence guarantees even when their source code is not available.", "venue": "2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)", "authors": ["R'emi  Dulong", "Rafael  Pires", "Andreia  Correia", "Valerio  Schiavoni", "Pedro  Ramalhete", "Pascal  Felber", "Gael  Thomas"], "year": 2021, "n_citations": 0}
{"id": 4065018, "s2_id": "fc2df8187d3580b2e2f2b4fa693c27883e27bb17", "title": "Report on power, thermal and reliability prediction for 3D Networks-on-Chip", "abstract": "By combining Three Dimensional Integrated Circuits with the Network-on-Chip infrastructure to obtain 3D Networks-on-Chip (3D-NoCs), the new on-chip communication paradigm brings several advantages on lower power, smaller footprint and lower latency. However, thermal dissipation is one of the most critical challenges for 3D-ICs where the heat cannot easily transfer through several layers of silicon. Consequently, the high-temperature area also confronts the reliability threat as the Mean Time to Failure (MTTF) decreases exponentially with the operating temperature. Apparently, 3D-NoCs must tackle this fundamental problem in order to be widely used. Therefore, in this work, we investigate the thermal distribution and reliability prediction of 3D-NoCs. We first present a new method to help simulate the temperature (both steady and transient) using traffics value from realistic and synthetic benchmarks and the power consumption from standard VLSI design flow. Then, based on the proposed method, we further predict the relative reliability between different parts of the network. Experimental results show that the method has an extremely fast execution time in comparison to the acceleration lifetime test. Furthermore, we compare the thermal behavior and reliability between Monolithic design and TSV-based TSV. We also explorer the ability to implement the thermal via a mechanism to help reduce the operating temperature.", "venue": "ArXiv", "authors": ["Khanh N. Dang", "Akram Ben Ahmed", "Ben A. Abderazek", "Xuan-Tu  Tran"], "year": 2020, "n_citations": 0}
{"id": 4066337, "s2_id": "98708d23c2326b0d588381f695bc280a28207536", "title": "Terabit-per-Second Multicore Polar Code Successive Cancellation Decoders", "abstract": "This work presents a high throughput and energy efficient multicore (MC) successive cancellation (SC) decoder architecture for polar codes. SC is a low-complexity decoding algorithm with a set of sequential operations. The sequential processing nature of SC limits parallelism but promotes not only pipelining but also multiple copies of SC decoder with an optimized pipeline depth to achieve Tb/s throughput. The MCSC decoder architecture consists of multiple SC decoders with lower frequency and pipeline depth to process multiple codewords in parallel to achieve lower power consumption. The pipeline depth of MCSC is optimized separately for each multicore configuration using register reduction/balancing (R-RB) method. This enables an efficient implementation for the 1-core, 2-core 4-core and 8-core candidate MCSC decoders. To reduce the complexity of the implementation, an adaptive log-likelihood ratio (LLR) quantization scheme is used for internal LLRs within the range of 1-5 bits. The post-placement-routing results at 28nm High-k Metal Gate (HKMG) ASIC technology show that 4-core MCSC decoder achieves 1 Tb/s throughput on 3.92 mm area with 1.55 pJ/bit energy efficiency.", "venue": "ArXiv", "authors": ["Altug  S\u00fcral", "Ertugrul  Kolagasioglu"], "year": 2021, "n_citations": 0}
{"id": 4069484, "s2_id": "7f2858f93cc9e1a84113dc9f7b2cdf934e51b549", "title": "Predictable accelerator design with time-sensitive affine types", "abstract": "Field-programmable gate arrays (FPGAs) provide an opportunity to co-design applications with hardware accelerators, yet they remain difficult to program. High-level synthesis (HLS) tools promise to raise the level of abstraction by compiling C or C++ to accelerator designs. Repurposing legacy software languages, however, requires complex heuristics to map imperative code onto hardware structures. We find that the black-box heuristics in HLS can be unpredictable: changing parameters in the program that should improve performance can counterintuitively yield slower and larger designs. This paper proposes a type system that restricts HLS to programs that can predictably compile to hardware accelerators. The key idea is to model consumable hardware resources with a time-sensitive affine type system that prevents simultaneous uses of the same hardware structure. We implement the type system in Dahlia, a language that compiles to HLS C++, and show that it can reduce the size of HLS parameter spaces while accepting Pareto-optimal designs.", "venue": "PLDI", "authors": ["Rachit  Nigam", "Sachille  Atapattu", "Samuel  Thomas", "Zhijing  Li", "Theodore  Bauer", "Yuwei  Ye", "Apurva  Koti", "Adrian  Sampson", "Zhiru  Zhang"], "year": 2020, "n_citations": 19}
{"id": 4069837, "s2_id": "5ae10124536a8e6f053a6418eb1aca190d9d6e28", "title": "Understanding the Impact of On-chip Communication on DNN Accelerator Performance", "abstract": "Deep Neural Networks have flourished at an unprecedented pace in recent years. They have achieved outstanding accuracy in fields such as computer vision, natural language processing, medicine or economics. Specifically, Convolutional Neural Networks (CNN) are particularly suited to object recognition or identification tasks. This, however, comes at a high computational cost, prompting the use of specialized GPU architectures or even ASICs to achieve high speeds and energy efficiency. ASIC accelerators streamline the execution of certain dataflows amenable to CNN computation that imply the constant movement of large amounts of data, thereby turning on-chip communication into a critical function within the accelerator. This paper studies the communication flows within CNN inference accelerators of edge devices, with the aim to justify current and future decisions in the design of the on-chip networks that interconnect their processing elements. Leveraging this analysis, we then qualitatively discuss the potential impact of introducing the novel paradigm of wireless on-chip network in this context.", "venue": "2019 26th IEEE International Conference on Electronics, Circuits and Systems (ICECS)", "authors": ["Robert  Guirado", "Hyoukjun  Kwon", "Eduard  Alarc'on", "Sergi  Abadal", "Tushar  Krishna"], "year": 2019, "n_citations": 5}
{"id": 4073374, "s2_id": "c7be1cd830fb1dcee0cb0e3ab848f15808509a53", "title": "Introducing a Performance Model for Bandwidth-Limited Loop Kernels", "abstract": "We present a diagnostic performance model for bandwidth-limited loop kernels which is founded on the analysis of modern cache based microarchitectures. This model allows an accurate performance prediction and evaluation for existing instruction codes. It provides an in-depth understanding of how performance for different memory hierarchy levels is made up. The performance of raw memory load, store and copy operations and a stream vector triad are analyzed and benchmarked on three modern x86-type quad-core architectures in order to demonstrate the capabilities of the model.", "venue": "PPAM", "authors": ["Jan  Treibig", "Georg  Hager"], "year": 2009, "n_citations": 70}
{"id": 4073625, "s2_id": "ab39ddb0c2599f0c50f279aa7fb2350ff18dcd54", "title": "Cache Hierarchy Optimization", "abstract": "Power consumption, off-chip memory bandwidth, chip area and Network on Chip (NoC) capacity are among main chip resources limiting the scalability of Chip Multiprocessors (CMP). A closed form analytical solution for optimizing the CMP cache hierarchy and optimally allocating area among hierarchy levels under such constrained resources is developed. The optimization framework is extended by incorporating the impact of data sharing on cache miss rate. An analytical model for cache access time as a function of cache size is proposed and verified using CACTI simulation.", "venue": "IEEE Computer Architecture Letters", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar"], "year": 2014, "n_citations": 16}
{"id": 4075082, "s2_id": "fd337d7f89bda15fc701927c220ecd1d9149e775", "title": "On metric sorting for successive cancellation list decoding of polar codes", "abstract": "We focus on the metric sorter unit of successive cancellation list decoders for polar codes, which lies on the critical path in all current hardware implementations of the decoder. We review existing metric sorter architectures and we propose two new architectures that exploit the structure of the path metrics in a log-likelihood ratio based formulation of successive cancellation list decoding. Our synthesis results show that, for the list size of L = 32, our first proposed sorter is 14% faster and 45% smaller than existing sorters, while for smaller list sizes, our second sorter has a higher delay in return for up to 36% reduction in the area.", "venue": "2015 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Alexios  Balatsoukas-Stimming", "Mani Bastani Parizi", "Andreas Peter Burg"], "year": 2015, "n_citations": 48}
{"id": 4076269, "s2_id": "4f0b7a42c5a2e7062c74179d5a7628a486c08947", "title": "Shift-BNN: Highly-Efficient Probabilistic Bayesian Neural Network Training via Memory-Friendly Pattern Retrieving", "abstract": "Bayesian Neural Networks (BNNs) that possess a property of uncertainty estimation have been increasingly adopted in a wide range of safety-critical AI applications which demand reliable and robust decision making, e.g., self-driving, rescue robots, medical image diagnosis. The training procedure of a probabilistic BNN model involves training an ensemble of sampled DNN models, which induces orders of magnitude larger volume of data movement than training a single DNN model. In this paper, we reveal that the root cause for BNN training inefficiency originates from the massive off-chip data transfer by Gaussian Random Variables (GRVs). To tackle this challenge, we propose a novel design that eliminates all the off-chip data transfer by GRVs through the reversed shifting of Linear Feedback Shift Registers (LFSRs) without incurring any training accuracy loss. To efficiently support our LFSR reversion strategy at the hardware level, we explore the design space of the current DNN accelerators and identify the optimal computation mapping scheme to best accommodate our strategy. By leveraging this finding, we design and prototype the first highly efficient BNN training accelerator, named Shift-BNN, that is low-cost and scalable. Extensive evaluation on five representative BNN models demonstrates that Shift-BNN achieves an average of 4.9 \u00d7 (up to 10.8 \u00d7) boost in energy efficiency and 1.6 \u00d7 (up to 2.8 \u00d7) speedup over the baseline DNN training accelerator.", "venue": "MICRO", "authors": ["Qiyu  Wan", "Haojun  Xia", "Xingyao  Zhang", "Lening  Wang", "Shuaiwen  Song", "Xin  Fu"], "year": 2021, "n_citations": 0}
{"id": 4078518, "s2_id": "10983e512d54126e4e1eeb70964a81f8970abb1f", "title": "ARAPrototyper: Enabling Rapid Prototyping and Evaluation for Accelerator-Rich Architectures", "abstract": "Compared to conventional general-purpose processors, accelerator-rich architectures (ARAs) can provide orders-of-magnitude performance and energy gains and are emerging as one of the most promising solutions in the age of dark silicon. However, many design issues related to the complex interaction between general-purpose cores, accelerators, customized on-chip interconnects, and memory systems remain unclear and difficult to evaluate. \nIn this paper we design and implement the ARAPrototyper to enable rapid design space explorations for ARAs in real silicons and reduce the tedious prototyping efforts far down to manageable efforts. First, ARAPrototyper provides a reusable baseline prototype with a highly customizable memory system, including interconnect between accelerators and buffers, interconnect between buffers and last-level cache (LLC) or DRAM, coherency choice at LLC or DRAM, and address translation support. Second, ARAPrototyper provides a clean interface to quickly integrate users' own accelerators written in high-level synthesis (HLS) code. The whole design flow is highly automated to generate a prototype of ARA on an FPGA system-on-chip (SoC). Third, to quickly develop applications that run seamlessly on the ARA prototype, ARAPrototyper provides a system software stack, abstracts the accelerators as software libraries, and provides APIs for software developers. Our experimental results demonstrate that ARAPrototyper enables a wide range of design space explorations for ARAs at manageable prototyping efforts, which has 4,000X to 10,000X faster evaluation time than full-system simulations. We believe that ARAPrototyper can be an attractive alternative for ARA design and evaluation.", "venue": "ArXiv", "authors": ["Yu-Ting  Chen", "Jason  Cong", "Zhenman  Fang", "Bingjun  Xiao", "Peipei  Zhou"], "year": 2016, "n_citations": 4}
{"id": 4078912, "s2_id": "0e97ba7e4b88ae943bdcf43a9827bc5658ad0b9a", "title": "Epiphany-V: A 1024 processor 64-bit RISC System-On-Chip", "abstract": "This paper describes the design of a 1024-core processor chip in 16nm FinFet technology. The chip (\"Epiphany-V\") contains an array of 1024 64-bit RISC processors, 64MB of on-chip SRAM, three 136-bit wide mesh Networks-On-Chip, and 1024 programmable IO pins. The chip has taped out and is being manufactured by TSMC. \nThis research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.", "venue": "ArXiv", "authors": ["Andreas  Olofsson"], "year": 2016, "n_citations": 61}
{"id": 4080037, "s2_id": "94f3a6773417a0196d629a7b3442ae69ff969525", "title": "Prefetcher-based DRAM Architecture", "abstract": "Advancement in Processor technology has made it easy to handle data-intensive workloads, but limiting main memory advances has created performance bottlenecks. In DRAM, there have been improvements in DRAM access latency as well as reduction in cost-per-bit with the increase in cell density. But still DRAM data transfer rate lags behind the processing speed of the current generation processors. As Memory advancements based on hardware have been progressing at a slower pace, to cope up with High-end Processors, Architectural level advancements such as Prediction techniques, Replacement policies, etc are the major subject. In the recent field of research, Data prediction is a sought out topic as correct prediction can boost performance by decreasing the amount of excess memory access by predicting data beforehand using data access trends and behaviors. Though prediction techniques have been implemented at most of the Computer Architecture, We propose implementing data prediction in DRAM level architectures like TL-DRAM [1] and CROW [2]. Both of these method distributes the DRAM into different parts which contain a smaller section which is faster and larger section which contains the bulk of data but is comparatively slower. We wish to use data prediction in between these sections of memory to have predicted data transferred to the faster sections to improve the overall performance by reducing the memory access time.", "venue": "ArXiv", "authors": ["Saurabh  Jaiswal", "Shailendra Kumar Gupta", "Soumya Soubhagya Dandapat"], "year": 2021, "n_citations": 0}
{"id": 4083282, "s2_id": "d2dfdbb7a5d64176968f51342a2eff0ff45db4b5", "title": "Bit-Pragmatic Deep Neural Network Computing", "abstract": "Deep Neural Networks expose a high degree of parallelism, making them amenable to highly data parallel architectures. However, data-parallel architectures often accept inefficiency in individual computations for the sake of overall efficiency. We show that on average, activation values of convolutional layers during inference in modern Deep Convolutional Neural Networks (CNNs) contain 92% zero bits. Processing these zero bits entails ineffectual computations that could be skipped. We propose Pragmatic (PRA), a massively data-parallel architecture that eliminates most of the ineffectual computations on-the-fly, improving performance and energy efficiency compared to state-of-the-art high-performance accelerators [5]. The idea behind PRA is deceptively simple: use serial-parallel shift-and-add multiplication while skipping the zero bits of the serial input. However, a straightforward implementation based on shift-and-add multiplication yields unacceptable area, power and memory access overheads compared to a conventional bit-parallel design. PRA incorporates a set of design decisions to yield a practical, area and energy efficient design. Measurements demonstrate that for convolutional layers, PRA is 4.31$\\times$ faster than DaDianNao [5] (DaDN) using a 16-bit fixed-point representation. While PRA requires 1.68$\\times$ more area than DaDN, the performance gains yield a 1.70$\\times$ increase in energy efficiency in a 65nm technology. With 8-bit quantized activations, PRA is 2.25$\\times$ faster and 1.31$\\times$ more energy efficient than an 8-bit version of DaDN. CCS CONCEPTS \u2022 Computing methodologies $\\rightarrow$ Machine learning; Neural networks; \u2022 Computer systems organization $\\rightarrow$ Single instruction, multiple data; \u2022 Hardware $\\rightarrow$ Arithmetic and datapath circuits;", "venue": "2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Jorge  Albericio", "Alberto  Delmas", "Patrick  Judd", "Sayeh  Sharify", "Gerard  O'Leary", "Roman  Genov", "Andreas  Moshovos"], "year": 2017, "n_citations": 155}
{"id": 4084990, "s2_id": "7c6aa2df00e42cf12d7e6428048d6d68b50433cb", "title": "Locality-based Graph Reordering for Processing Speed-Ups and Impact of Diameter", "abstract": "Graph analysis involves a high number of random memory access patterns. Earlier research has shown that the cache miss latency is responsible for more than half of the graph processing time, with the CPU execution having the smaller share. There has been significant study on decreasing the CPU computing time for example, by employing better cache prefetching and replacement policies. In this paper, we study the various methods that do so by attempting to decrease the CPU cache miss ratio. Graph Reordering attempts to exploit the power-law distribution of graphsfew sparsely-populated vertices in the graph have high number of connectionsto keep the frequently accessed vertices together locally and hence decrease the cache misses. However, reordering the graph by keeping the hot vertices together may affect the spatial locality of the graph, and thus add to the total CPU compute time. Also, we also need to have a control over the total reordering time and its inverse relation with the final CPU execution time In order to exploit this trade-off between reordering as per vertex hotness and spatial locality, we introduce the light-weight Community-based Reordering. We attempt to maintain the community-structure of the graph by storing the hot-members in the community locally together. The implementation also takes into consideration the impact of graph diameter on the execution time. We compare our implementation with other reordering implementations and find a significantly better result on five graph processing algorithmsBFS, CC, CCSV, PR and BC. Lorder achieved speed-up of upto 7\u00d7 and an average speed-up of 1.2\u00d7 as compared to other reordering algorithms", "venue": "ArXiv", "authors": ["Vedant  Satav", "Virendra  Singh"], "year": 2021, "n_citations": 0}
{"id": 4085698, "s2_id": "c71074a913504bbf25c5d6231440b11c0413acf1", "title": "Simple DRAM and Virtual Memory Abstractions to Enable Highly Efficient Memory Systems", "abstract": "In most modern systems, the memory subsystem is managed and accessed at multiple different granularities at various resources. We observe that such multi-granularity management results in significant inefficiency in the memory subsystem. Specifically, we observe that 1) page-granularity virtual memory unnecessarily triggers large memory operations, and 2) existing cache-line granularity memory interface is inefficient for performing bulk data operations and operations that exhibit poor spatial locality. To address these problems, we present a series of techniques in this thesis. \nFirst, we propose page overlays, a framework augments the existing virtual memory framework with the ability to track a new version of a subset of cache lines within each virtual page. We show that this extension is powerful by demonstrating its benefits on a number of applications. \nSecond, we show that DRAM can be used to perform more complex operations than just store data. We propose RowClone, a mechanism to perform bulk data copy and initialization completely inside DRAM, and Buddy RAM, a mechanism to perform bulk bitwise operations using DRAM. Both these techniques achieve an order-of-magnitude improvement in the efficiency of the respective operations. \nThird, we propose Gather-Scatter DRAM, a technique that exploits DRAM organization to effectively gather/scatter values with a power-of-2 strided access patterns. For these access patterns, GS-DRAM achieves near-ideal bandwidth and cache utilization, without increasing the latency of fetching data from memory. \nFinally, we propose the Dirty-Block Index, a new way of tracking dirty blocks. In addition to improving the efficiency of bulk data coherence, DBI has several applications including high-performance memory scheduling, efficient cache lookup bypassing, and enabling heterogeneous ECC.", "venue": "ArXiv", "authors": ["Vivek  Seshadri"], "year": 2016, "n_citations": 29}
{"id": 4087404, "s2_id": "954c8b925b88c3b5e937c50c0a2ba6b20c315d62", "title": "Advancing Computing's Foundation of US Industry & Society", "abstract": "While past information technology (IT) advances have transformed society, future advances hold even greater promise. For example, we have only just begun to reap the changes from artificial intelligence (AI), especially machine learning (ML). Underlying IT\u2019s impact are the dramatic improvements in computer hardware, which deliver performance that unlock new capabilities. For example, recent successes in AI/ML required the synergy of improved algorithms and hardware architectures (e.g., general-purpose graphics processing units). However, unlike in the 20th Century and early 2000s, tomorrow\u2019s performance aspirations must be achieved without continued semiconductor scaling formerly provided by Moore\u2019s Law and Dennard Scaling. How will one deliver the next 100x improvement in capability at similar or less cost to enable great value? Can we make the next AI leap without 100x better hardware?", "venue": "ArXiv", "authors": ["Thomas M. Conte", "Ian T. Foster", "William  Gropp", "Mark D. Hill"], "year": 2021, "n_citations": 1}
{"id": 4087410, "s2_id": "9b5ec720e85c30f5f86b32b1d72f41f36702396e", "title": "CRC: Fully General Model of Confidential Remote Computing", "abstract": "Digital services have been offered through remote systems for decades. The questions of how these systems can be built in a trustworthy manner and how their security properties can be understood are given fresh impetus by recent hardware developments, allowing a fuller, more general, exploration of the possibilities than has previously been seen in the literature. Drawing on and consolidating the disparate strains of research, technologies and methods employed throughout the adaptation of confidential computing, we present a novel, dedicated Confidential Remote Computing (CRC) model. CRC proposes a compact solution for next-generation applications to be built on strong hardware-based security primitives, control of secure software products\u2019 trusted computing base, and a way to make correct use of proofs and evidence reports generated by the attestation mechanisms. The CRC model illustrates the trade-offs between decentralisation, task size and transparency overhead. We conclude the paper with six lessons learned from our approach, and suggest two future research directions.", "venue": "ArXiv", "authors": ["Kubilay Ahmet K\u00fc\u00e7\u00fck", "Andrew C. Martin"], "year": 2021, "n_citations": 0}
{"id": 4089112, "s2_id": "1db027d936b7e74948dfcd4c6876bdef396f6f05", "title": "Best CNTFET Ternary Adders?", "abstract": "The MUX implementation of ternary half adders and full adders using predecessor and successor functions lead to the most efficient efficient implementation using the smallest transistor count. These designs are compared with the binary implementation of the corresponding half adders and full adders using the MUX technique or the typical complementary CMOS circuit style. The transistor count ratio between ternary and binary implementations is always greater than the information ratio (log2(3)/log2(2) = 1.585) between ternary and binary wires.", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2021, "n_citations": 0}
{"id": 4089806, "s2_id": "039f59c5bea8e0439788c7520fa9805c43e6afb4", "title": "hXDP: Efficient Software Packet Processing on FPGA NICs", "abstract": "FPGA accelerators on the NIC enable the offloading of expensive packet processing tasks from the CPU. However, FPGAs have limited resources that may need to be shared among diverse applications, and programming them is difficult. \nWe present a solution to run Linux's eXpress Data Path programs written in eBPF on FPGAs, using only a fraction of the available hardware resources while matching the performance of high-end CPUs. The iterative execution model of eBPF is not a good fit for FPGA accelerators. Nonetheless, we show that many of the instructions of an eBPF program can be compressed, parallelized or completely removed, when targeting a purpose-built FPGA executor, thereby significantly improving performance. We leverage that to design hXDP, which includes (i) an optimizing-compiler that parallelizes and translates eBPF bytecode to an extended eBPF Instruction-set Architecture defined by us; a (ii) soft-CPU to execute such instructions on FPGA; and (iii) an FPGA-based infrastructure to provide XDP's maps and helper functions as defined within the Linux kernel. \nWe implement hXDP on an FPGA NIC and evaluate it running real-world unmodified eBPF programs. Our implementation is clocked at 156.25MHz, uses about 15% of the FPGA resources, and can run dynamically loaded programs. Despite these modest requirements, it achieves the packet processing throughput of a high-end CPU core and provides a 10x lower packet forwarding latency.", "venue": "OSDI", "authors": ["Marco Spaziani Brunella", "Giacomo  Belocchi", "Marco  Bonola", "Salvatore  Pontarelli", "Giuseppe  Siracusano", "Giuseppe  Bianchi", "Aniello  Cammarano", "Alessandro  Palumbo", "Luca  Petrucci", "Roberto  Bifulco"], "year": 2020, "n_citations": 13}
{"id": 4091034, "s2_id": "370a2a0620bc0e65f9283795d86af2843aa53f3b", "title": "Graphene-based Wireless Agile Interconnects for Massive Heterogeneous Multi-chip Processors", "abstract": "The main design principles in computer architecture have recently shifted from a monolithic scaling-driven approach to the development of heterogeneous architectures that tightly co-integrate multiple specialized processor and memory chiplets. In such data-hungry multi-chip architectures, current Networks-in-Package (NiPs) may not be enough to cater to their heterogeneous and fast-changing communication demands. This position paper makes the case for wireless in-package nanonetworking as the enabler of efficient and versatile wired-wireless interconnect fabrics for massive heterogeneous processors. To that end, the use of graphene-based antennas and transceivers with unique frequency-beam reconfigurability in the terahertz band is proposed. The feasibility of such a nanonetworking vision and the main research challenges towards its realization are analyzed from the technological, communications, and computer architecture perspectives.", "venue": "ArXiv", "authors": ["Sergi  Abadal", "Robert  Guirado", "Hamidreza  Taghvaee", "Akshay  Jain", "Elana Pereira de Santana", "Peter Haring Bol\u00edvar", "Mohamed Saeed Elsayed", "Renato  Negra", "Zhenxing  Wang", "Kun-Ta  Wang", "Max C. Lemme", "Joshua  Klein", "Marina  Zapater", "Alexandre  Levisse", "David  Atienza", "Davide  Rossi", "Francesco  Conti", "Martino  Dazzi", "Geethan  Karunaratne", "Irem  Boybat", "Abu  Sebastian"], "year": 2020, "n_citations": 3}
{"id": 4091131, "s2_id": "ac8689072ff36a0bcc04385221d2544d6c043e5e", "title": "Thread Batching for High-performance Energy-efficient GPU Memory Design", "abstract": "Massive multi-threading in GPU imposes tremendous pressure on memory subsystems. Due to rapid growth in thread-level parallelism of GPU and slowly improved peak memory bandwidth, memory becomes a bottleneck of GPU\u2019s performance and energy efficiency. In this article, we propose an integrated architectural scheme to optimize the memory accesses and therefore boost the performance and energy efficiency of GPU. First, we propose a thread batch enabled memory partitioning (TEMP) to improve GPU memory access parallelism. In particular, TEMP groups multiple thread blocks that share the same set of pages into a thread batch and applies a page coloring mechanism to bound each stream multiprocessor (SM) to the dedicated memory banks. After that, TEMP dispatches the thread batch to an SM to ensure high-parallel memory-access streaming from the different thread blocks. Second, a thread batch-aware scheduling (TBAS) scheme is introduced to improve the GPU memory access locality and to reduce the contention on memory controllers and interconnection networks. Experimental results show that the integration of TEMP and TBAS can achieve up to 10.3% performance improvement and 11.3% DRAM energy reduction across diverse GPU applications. We also evaluate the performance interference of the mixed CPU+GPU workloads when they are run on a heterogeneous system that employs our proposed schemes. Our results show that a simple solution can effectively ensure the efficient execution of both GPU and CPU applications.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Bing  Li", "Mengjie  Mao", "Xiaoxiao  Liu", "Tao  Liu", "Zihao  Liu", "Wujie  Wen", "Yiran  Chen", "Hai  Li"], "year": 2019, "n_citations": 1}
{"id": 4100270, "s2_id": "0aa3b60e1f500f886e401d036435b1a9f0dbb377", "title": "Hypervector Design for Efficient Hyperdimensional Computing on Edge Devices", "abstract": "Hyperdimensional computing (HDC) has emerged as a new lightweight learning algorithm with smaller computation and energy requirements compared to conventional techniques. In HDC, data points are represented by high dimensional vectors (hypervectors), which are mapped to high dimensional space (hyperspace). Typically, a large hypervector dimension (\u2265 1000) is required to achieve accuracies comparable to conventional alternatives. However, unnecessarily large hypervectors increase hardware and energy costs, which can undermine their benefits. This paper presents a technique to minimize the hypervector dimension while maintaining the accuracy and improving the robustness of the classifier. To this end, we formulate hypervector design as a multi-objective optimization problem for the first time in the literature. The proposed approach decreases the hypervector dimension by more than 128\u00d7 while maintaining or increasing the accuracy achieved by conventional HDC. Experiments on a commercial hardware platform show that the proposed approach achieves more than two orders of magnitude reduction in model size, inference time, and energy consumption. We also demonstrate the trade-off between accuracy and robustness to noise and provide Pareto front solutions as a design parameter in our hypervector design. CCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learning; \u2022 Hardware\u2192 Power and energy.", "venue": "ArXiv", "authors": ["Toygun  Basaklar", "Yigit  Tuncel", "Shruti Yadav Narayana", "Suat  Gumussoy", "Umit Y. Ogras"], "year": 2021, "n_citations": 0}
{"id": 4101959, "s2_id": "912ced9b4412af4817fbbf0f5ed3c711ebce0df2", "title": "Optimal Mapping for Near-Term Quantum Architectures based on Rydberg Atoms", "abstract": "Quantum algorithms promise quadratic or exponential speedups for applications in cryptography, chemistry and material sciences. The topologies of today's quantum computers offer limited connectivity, leading to significant overheads for implementing such quantum algorithms. One-dimensional topology displacements that remedy these limits have been recently demonstrated for architectures based on Rydberg atoms, and they are possible in principle in photonic and ion trap architectures. We present the first optimal quantum circuit-to-architecture mapping algorithm that exploits such one-dimensional topology displacements. We benchmark our method on quantum circuits with up to 15 qubits and investigate the improvements compared with conventional mapping based on inserting swap gates into the quantum circuits. Depending on underlying technology parameters, our approach can decrease the quantum circuit depth by up to 58% and increase the fidelity by up to 29%. We also study runtime and fidelity requirements on one-dimensional displacements and swap gates to derive conditions under which one-dimensional topology displacements provide benefits.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Sebastian  Brandhofer", "Hans Peter B\u00fcchler", "Ilia  Polian"], "year": 2021, "n_citations": 0}
{"id": 4106783, "s2_id": "6bbee9a09906c53acdc6b6c35d32c924cc4e17f5", "title": "Practical Encrypted Computing for IoT Clients", "abstract": "Privacy and energy are primary concerns for sensor devices that offload compute to a potentially untrusted edge server or cloud. Homomorphic Encryption (HE) enables offload processing of encrypted data. HE offload processing retains data privacy, but is limited by the need for frequent communication between the client device and the offload server. Existing client-aided encrypted computing systems are optimized for performance on the offload server, failing to sufficiently address client costs, and precluding HE offload for low-resource (e.g., IoT) devices. We introduce Client-aided HE for Opaque Compute Offloading (CHOCO), a client-optimized system for encrypted offload processing. CHOCO introduces rotational redundancy, an algorithmic optimization to minimize computing and communication costs. We design Client-aided HE for Opaque Compute Offloading Through Accelerated Cryptographic Operations (CHOCO-TACO), a comprehensive architectural accelerator for client-side cryptographic operations that eliminates most of their time and energy costs. Our evaluation shows that CHOCO makes client-aided HE offloading feasible for resourceconstrained clients. Compared to existing encrypted computing solutions, CHOCO reduces communication cost by up to 2948\u00d7. With hardware support, client-side encryption/decryption is faster by 1094\u00d7 and uses 648\u00d7 less energy. In our end-to-end implementation of a large-scale DNN (VGG16), CHOCO uses 37% less energy than local (unencrypted) computation.", "venue": "ArXiv", "authors": ["McKenzie van der Hagen", "Brandon  Lucia"], "year": 2021, "n_citations": 2}
{"id": 4108446, "s2_id": "7e654c21978d214018f8abd189b25167ee9310df", "title": "High-Level FPGA Accelerator Design for Structured-Mesh-Based Explicit Numerical Solvers", "abstract": "This paper presents a workflow for synthesizing near-optimal FPGA implementations of structured-mesh based stencil applications for explicit solvers. It leverages key characteristics of the application class and its computation-communication pattern and the architectural capabilities of the FPGA to accelerate solvers for high-performance computing applications. Key new features of the workflow are (1) the unification of standard state-of-the-art techniques with a number of high-gain optimizations such as batching and spatial blocking/tiling, motivated by increasing throughput for real-world workloads and (2) the development and use of a predictive analytical model to explore the design space, and obtain resource and performance estimates. Three representative applications are implemented using the design workflow on a Xilinx Alveo U280 FPGA, demonstrating near-optimal performance and over 85% predictive model accuracy. These are compared with equivalent highly-optimized implementations of the same applications on modern HPC-grade GPUs (Nvidia V100), analyzing time to solution, bandwidth, and energy consumption. Performance results indicate comparable runtimes with the V100 GPU, with over 2\u00d7 energy savings for the largest non-trivial application on the FPGA. Our investigation shows the challenges of achieving high performance on current generation FPGAs compared to traditional architectures. We discuss determinants for a given stencil code to be amenable to FPGA implementation, providing insights into the feasibility and profitability of a design and its resulting performance.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Kamalakkannan  Kamalavasan", "Gihan R. Mudalige", "I. Z. Reguly", "Suhaib A. Fahmy"], "year": 2021, "n_citations": 1}
{"id": 4109489, "s2_id": "b6299d975dfcf6ebbf8943168aa922816effad1c", "title": "Partial Reversible Gates(PRG) for Reversible BCD Arithmetic", "abstract": "IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and a major enhancement to the standard is the addition of decimal format. Furthermore, in the recent years reversible logic has emerged as a promising computing paradigm having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. The major goal in reversible logic is to minimize the number of reversible gates and garbage outputs. Thus, this paper proposes the novel concept of partial reversible gates that will satisfy the reversibility criteria for specific cases in BCD arithmetic. The partial reversible gate is proposed to minimize the number of reversible gates and garbage outputs, while designing the reversible BCD arithmetic circuits.", "venue": "CDES", "authors": ["Himanshu  Thapliyal", "Hamid R. Arabnia", "Rajnish  Bajpai", "Kamal K. Sharma"], "year": 2007, "n_citations": 10}
{"id": 4110242, "s2_id": "acdfe34aab867d1a0101c88dba299a56274388d8", "title": "CLR-DRAM: A Low-Cost DRAM Architecture Enabling Dynamic Capacity-Latency Trade-Off", "abstract": "DRAM is the prevalent main memory technology, but its long access latency can limit the performance of many workloads. Although prior works provide DRAM designs that reduce DRAM access latency, their reduced storage capacities hinder the performance of workloads that need large memory capacity. Because the capacity-latency trade-off is fixed at design time, previous works cannot achieve maximum performance under very different and dynamic workload demands.This paper proposes Capacity-Latency-Reconfigurable DRAM (CLR-DRAM), a new DRAM architecture that enables dynamic capacity-latency trade-off at low cost. CLR-DRAM allows dynamic reconfiguration of any DRAM row to switch between two operating modes: 1) max-capacity mode, where every DRAM cell operates individually to achieve approximately the same storage density as a density-optimized commodity DRAM chip and 2) high-performance mode, where two adjacent DRAM cells in a DRAM row and their sense amplifiers are coupled to operate as a single low-latency logical cell driven by a single logical sense amplifier.We implement CLR-DRAM by adding isolation transistors in each DRAM subarray. Our evaluations show that CLR-DRAM can improve system performance and DRAM energy consumption by 18.6% and 29.7% on average with four-core multiprogrammed workloads. We believe that CLR-DRAM opens new research directions for a system to adapt to the diverse and dynamically changing memory capacity and access latency demands of workloads.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Haocong  Luo", "Taha  Shahroodi", "Hasan  Hassan", "Minesh  Patel", "Abdullah Giray Yaglikci", "Lois  Orosa", "Jisung  Park", "Onur  Mutlu"], "year": 2020, "n_citations": 18}
{"id": 4113101, "s2_id": "6e2c86860f6d743bf851f16e60a80153e6b9c83b", "title": "A probabilistic collocation method based statistical gate delay model considering process variations and multiple input switching", "abstract": "Since the advent of new nanotechnologies, the variability of gate delay due to process variations has become a major concern. This paper proposes a new gate delay model that includes impact from both process variations and multiple input switching. The proposed model uses an orthogonal polynomial based probabilistic collocation method to construct a delay analytical equation from circuit timing performance. From the experimental results, our approach has less that 0.2% error on the mean delay of gates and less than 3% error on the standard deviation.", "venue": "Design, Automation and Test in Europe", "authors": ["Y. Satish Kumar", "Jun  Li", "Claudio  Talarico", "Janet  Roveda"], "year": 2005, "n_citations": 47}
{"id": 4114202, "s2_id": "ee7b99235a895c39115b5b643ab1a6e94c7ac1fb", "title": "Exceeding Conservative Limits: A Consolidated Analysis on Modern Hardware Margins", "abstract": "Modern large-scale computing systems (data centers, supercomputers, cloud and edge setups and high-end cyber-physical systems) employ heterogeneous architectures that consist of multicore CPUs, general-purpose many-core GPUs, and programmable FPGAs. The effective utilization of these architectures poses several challenges, among which a primary one is power consumption. Voltage reduction is one of the most efficient methods to reduce power consumption of a chip. With the galloping adoption of hardware accelerators (i.e., GPUs and FPGAs) in large datacenters and other large-scale computing infrastructures, a comprehensive evaluation of the safe voltage reduction levels for each different chip can be employed for efficient reduction of the total power. We present a survey of recent studies in voltage margins reduction at the system level for modern CPUs, GPUs and FPGAs. The pessimistic voltage guardbands inserted by the silicon vendors can be exploited in all devices for significant power savings. On average, voltage reduction can reach 12% in multicore CPUs, 20% in manycore GPUs and 39% in FPGAs.", "venue": "IEEE Transactions on Device and Materials Reliability", "authors": ["George  Papadimitriou", "Athanasios  Chatzidimitriou", "Dimitris  Gizopoulos", "Vijay Janapa Reddi", "Jingwen  Leng", "Behzad  Salami", "Osman Sabri Unsal", "Adrian Cristal Kestelman"], "year": 2020, "n_citations": 9}
{"id": 4118430, "s2_id": "d7b636c08550385d24d6e3c7fd1e36f97bf45907", "title": "Reconfigurable Hardware Implementation of the Successive Overrelaxation Method", "abstract": "A Comprehensive Movement Compatibility Study for Hong Kong Chinese.- A Study of Comparative Design Satisfaction Between Culture and Modern Bamboo Chair.- Factors Influencing Symbol-Training Effectiveness.- Multiple-Colony Ant Algorithm with Forward-Backward Scheduling Approach for Job-Shop Scheduling Problem.- Proposal of New Paradigm for Hand and Foot Controls in the Context of Spatial Compatibility Effect.- Development of a Mathematical Model for Process with S-Type Quality Characteristics to a Quality Selection Problem.- Temporal Aggregation and the Production Smoothing Model: Evidence from Electronic Parts and Components Manufacturing in Taiwan.- Simulations of Gear Shaving and the Tooth Contact Analysis.- On Aggregative Methods of Supplier Assessment.- Human Factors and Ergonomics for Nondestructive Testing.- A Novel Matrix Approach to Determine Makespan for Zero-Wait Batch Processes.- Interactive Meta-Goal Programming: A Decision Analysis Approach for Collaborative Manufacturing.- Nonlinear Programming Based on Particle Swarm Optimization.- A Heuristic for the Capacitated Single Allocation Hub Location Problem.- Multimodal Transport: A Framework for Analysis.- Fractional Matchings of Graphs.- Correlation Functions for Dynamic Load Balancing of Cycle Shops.- Neural Network-Based Integral Sliding Mode Control for Nonlinear Uncertain Systems.- Decentralized Neuro-Fuzzy Control of a Class of Nonlinear Systems.- A New Training Algorithm of Adaptive Fuzzy Control for Chaotic Dynamic Systems.- General-Purpose Simulation Management for Satellite Navigation Signal Simulation.- Multilayered Quality-of-Service Architecture with Cross-layer Coordination for Teleoperation System.- Improvement of State Estimation for Systems with Chaotic Noise.- Combined Sensitivity-Complementary Sensitivity Min-Max Approach for Load Disturbance-Setpoint Trade-off Design.- Nonlinear Adaptive Sliding Mode Control for a Rotary Inverted Pendulum.- Robust Load Frequency Sliding Mode Control Based on Uncertainty and Disturbance Estimator.- Robust Intelligent Motion Control for Linear Piezoelectric Ceramic Motor System Using Self-constructing Neural Network.- Development of Hybrid Magnetic Bearings System for Axial-Flow Blood Pump.- Critical Angle for Optimal Correlation Assignment to Control Memory and Computational Load Requirements in a Densely Populated Target Environment.- High-Precision Finite Difference Method Calculations of Electrostatic Potential.- Newton-Tau Method.- Reconfigurable Hardware Implementation of the Successive Overrelaxation Method.- Tabu Search Algorithm Based on Strategic Oscillation for Nonlinear Minimum Spanning Tree Problems.- Customization of Visual Lobe Measurement System for Testing the Effects of Foveal Load.", "venue": "ArXiv", "authors": ["Safaa J. Kasbah", "Ramzi A. Haraty", "Issam W. Damaj"], "year": 2019, "n_citations": 1}
{"id": 4119298, "s2_id": "b8e1bee4f7785b2b2c5ec66040bf372d3e2968c6", "title": "Hybrid In-Memory Computing Architecture for the Training of Deep Neural Networks", "abstract": "The cost involved in training deep neural networks (DNNs) on von-Neumann architectures has motivated the development of novel solutions for efficient DNN training accelerators. We propose a hybrid in-memory computing (HIC) architecture for the training of DNNs on hardware accelerators that results in memory-efficient inference and outperforms baseline software accuracy in benchmark tasks. We introduce a weight representation technique that exploits both binary and multi-level phase-change memory (PCM) devices, and this leads to a memory-efficient inference accelerator. Unlike previous in-memory computing- based implementations, we use a low precision weight update accumulator that results in more memory savings. We trained the ResNet-32 network to classify CIFAR-10 images using HIC. For a comparable model size, HIC-based training outperforms baseline network, trained in floating-point 32-bit (FP32) precision, by leveraging appropriate network width multiplier. Furthermore, we observe that HIC-based training results in about 50 % less inference model size to achieve baseline comparable accuracy. We also show that the temporal drift in PCM devices has a negligible effect on post-training inference accuracy for extended periods (year). Finally, our simulations indicate HIC-based training naturally ensures that the number of write-erase cycles seen by the devices is a small fraction of the endurance limit of PCM, demonstrating the feasibility of this architecture for achieving hardware platforms that can learn in the field.", "venue": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Vinay  Joshi", "Wangxin  He", "Jae-sun  Seo", "Bipin  Rajendran"], "year": 2021, "n_citations": 0}
{"id": 4121880, "s2_id": "213192a6b246a349ceed4420383f4c01f0df3951", "title": "Generic Connectivity-Based CGRA Mapping via Integer Linear Programming", "abstract": "Coarse-grained reconfigurable architectures (CGRAs) are programmable logic devices with large coarsegrained ALU-like logic blocks, and multi-bit datapath-style routing. CGRAs often have relatively restricted data routing networks, so they attract CAD mapping tools that use exact methods, such as Integer Linear Programming (ILP). However, tools that target general architectures must use large constraint systems to fully describe an architecture's flexibility, resulting in lengthy run-times. In this paper, we propose to derive connectivity information from an otherwise generic device model, and use this to create simpler ILPs, which we combine in an iterative schedule and retain most of the exactness of a fully-generic ILP approach. This new approach has a speed-up geometric mean of 5.88x when considering benchmarks that do not hita time-limit of 7.5 hours on the fully-generic ILP, and 37.6x otherwise. This was measured using the set of benchmarks used to originally evaluate the fully-generic approach and several more benchmarks representing computation tasks, over three different CGRA architectures. All run-times of the new approach are less than 20 minutes, with 90th percentile time of 410 seconds. The proposed mapping techniques are integrated into, and evaluated using the open-source CGRA-ME architecture modelling and exploration framework.", "venue": "2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Matthew J. P. Walker", "Jason Helge Anderson"], "year": 2019, "n_citations": 8}
{"id": 4122367, "s2_id": "f179fe3c70b7dc9c5f88bca78cf62faf6e8b4838", "title": "SafeSpec: Banishing the Spectre of a Meltdown with Leakage-Free Speculation", "abstract": "Speculative attacks, such as Spectre and Meltdown, target speculative execution to access privileged data and leak it through a side-channel. In this paper, we introduce (SafeSpec), a new model for supporting speculation in a way that is immune to the side-channel leakage by storing side effects of speculative instructions in separate structures until they commit. Additionally, we address the possibility of a covert channel from speculative instructions to committed instructions before these instructions are committed. We develop a cycle accurate model of modified design of an x86-64 processor and show that the performance impact is negligible.", "venue": "2019 56th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Khaled N. Khasawneh", "Esmaeil Mohammadian Koruyeh", "Chengyu  Song", "Dmitry  Evtyushkin", "Dmitry V. Ponomarev", "Nael B. Abu-Ghazaleh"], "year": 2019, "n_citations": 98}
{"id": 4122546, "s2_id": "49edb22f56390330b8461ce332b4f46148707b29", "title": "SPINBIS: Spintronics-Based Bayesian Inference System With Stochastic Computing", "abstract": "Bayesian inference is an effective approach for solving statistical learning problems, especially with uncertainty and incompleteness. However, Bayesian inference is a computing-intensive task whose efficiency is physically limited by the bottlenecks of conventional computing platforms. In this paper, a spintronics-based stochastic computing (SC) approach is proposed for efficient Bayesian inference. The inherent stochastic switching behaviors of spintronic devices are exploited to build a stochastic bitstream generator (SBG) for SC with hybrid CMOS/magnetic tunnel junction (MTJ) circuits design. Aiming to improve the inference efficiency, an SBG sharing strategy is leveraged to reduce the required SBG array scale by integrating a switch network between SBG array and SC logic. A device-to-architecture level framework is proposed to evaluate the performance of spintronics-based Bayesian inference system (SPINBIS). Experimental results on data fusion applications have shown that SPINBIS could improve the energy efficiency about <inline-formula> <tex-math notation=\"LaTeX\">$12 {\\times }$ </tex-math></inline-formula> than MTJ-based approach with 45% design area overhead and about <inline-formula> <tex-math notation=\"LaTeX\">$26 {\\times }$ </tex-math></inline-formula> than FPGA-based approach.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Xiaotao  Jia", "Jianlei  Yang", "Pengcheng  Dai", "Runze  Liu", "Yiran  Chen", "Weisheng  Zhao"], "year": 2020, "n_citations": 6}
{"id": 4125031, "s2_id": "9fb5d3bf807e9d0931148741cbeca435e4cf929e", "title": "DIALED: Data Integrity Attestation for Low-end Embedded Devices", "abstract": "Verifying integrity of software execution in low-end microcontroller units (MCUs) is a well-known open problem. The central challenge is how to securely detect software exploits with minimal overhead, since these MCUs are designed for low cost, low energy and small size. Some recent work yielded inexpensive hardware/software co-designs for remotely verifying code and execution integrity. In particular, a means of detecting unauthorized code modifications and control-flow attacks were proposed, referred to as Remote Attestation (\u211bA) and Control-Flow Attestation (CFA), respectively. Despite this progress, detection of data-only attacks remains elusive. Such attacks exploit software vulnerabilities to corrupt intermediate computation results stored in data memory, changing neither the program code nor its control flow. Motivated by lack of any current techniques (for low-end MCUs) that detect these attacks, in this paper we propose, implement and evaluate DIALED, the first Data-Flow Attestation (CFA) technique applicable to the most resource-constrained embedded devices (e.g., TI MSP430). DIALED works in tandem with a companion CFA scheme to detect all (currently known) types of runtime software exploits at fairly low cost.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Ivan De Oliveira Nunes", "Sashidhar  Jakkamsetti", "Gene  Tsudik"], "year": 2021, "n_citations": 2}
{"id": 4125755, "s2_id": "d1e00ff6e29284800c4de993e36c04be8f36192b", "title": "On-Chip Mechanisms to Reduce Effective Memory Access Latency", "abstract": "This dissertation develops hardware that automatically reduces the effective latency of accessing memory in both single-core and multi-core systems. To accomplish this, the dissertation shows that all last level cache misses can be separated into two categories: dependent cache misses and independent cache misses. Independent cache misses have all of the source data that is required to generate the address of the memory access available on-chip, while dependent cache misses depend on data that is located off-chip. This dissertation proposes that dependent cache misses are accelerated by migrating the dependence chain that generates the address of the memory access to the memory controller for execution. Independent cache misses are accelerated using a new mode for runahead execution that only executes filtered dependence chains. With these mechanisms, this dissertation demonstrates a 62% increase in performance and a 19% decrease in effective memory access latency for a quad-core processor on a set of high memory intensity workloads.", "venue": "ArXiv", "authors": ["Milad  Hashemi"], "year": 2016, "n_citations": 1}
{"id": 4131139, "s2_id": "3dc822204575cc5a50a96996c607b73457aaa016", "title": "ReCA: An Efficient Reconfigurable Cache Architecture for Storage Systems with Online Workload Characterization", "abstract": "In recent years, Solid-State Drives (SSDs) have gained tremendous attention in computing and storage systems due to significant performance improvement over Hard Disk Drives (HDDs). The cost per capacity of SSDs, however, prevents them from entirely replacing HDDs in such systems. One approach to effectively take advantage of SSDs is to use them as a caching layer to store performance critical data blocks in order to reduce the number of accesses to HDD-based disk subsystem. Due to characteristics of Flash-based SSDs such as limited write endurance and long latency on write operations, employing caching algorithms at the Operating System (OS) level necessitates to take such characteristics into consideration. Previous OS-level caching techniques are optimized towards only one type of application, which affects both generality and applicability. In addition, they are not adaptive when the workload pattern changes over time. This paper presents an efficient Reconfigurable Cache Architecture (ReCA) for storage systems using a comprehensive workload characterization to find an optimal cache configuration for I/O intensive applications. For this purpose, we first investigate various types of I/O workloads and classify them into five major classes. Based on this characterization, an optimal cache configuration is presented for each class of workloads. Then, using the main features of each class, we continuously monitor the characteristics of an application during system runtime and the cache organization is reconfigured if the application changes from one class to another class of workloads. The cache reconfiguration is done online and workload classes can be extended to emerging I/O workloads in order to maintain its efficiency with the characteristics of I/O requests. Experimental results obtained by implementing ReCA in a 4U rackmount server with SATA 6Gb/s disk interfaces running Linux 3.17.0 show that the proposed architecture improves performance and lifetime up to 24 and 33 percent, respectively.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Reza  Salkhordeh", "Shahriar  Ebrahimi", "Hossein  Asadi"], "year": 2018, "n_citations": 21}
{"id": 4139738, "s2_id": "ccf9d38d5fedb48daf920030eaf3a65bda8ddebf", "title": "Verifying Sequential Consistency on Shared-Memory Multiprocessors by Model Checking", "abstract": "The memory model of a shared-memory multiprocessor is a contract between the designer and the programmer of the multiprocessor. A memory model is typically implemented by means of a cache-coherence protocol. The design of this protocol is one of the most complex aspects of multiprocessor design and is consequently quite error-prone. However, it is imperative to ensure that the cache-coherence protocol satisfies the shared-memory model. We present a novel technique based on model checking to tackle this difficult problem for the important and well-known shared-memory model of sequential consistency. Surprisingly, verifying sequential consistency is undecidable in general, even for finite-state cache-coherence protocols. In practice, cache-coherence protocols satisfy the properties of causality and data independence. Causality is the property that values of read events flow from values of write events. Data independence is the property that all traces can be generated by renaming data values from traces where the written values are pairwise distinct. We show that, if a causal and data independent system also has the property that the logical order of write events to each location is identical to their temporal order, then sequential consistency is decidable. We present a novel model checking algorithm to verify sequential consistency on such systems for a finite number of processors and memory locations and an arbitrary number of data values.", "venue": "IEEE Trans. Parallel Distributed Syst.", "authors": ["Shaz  Qadeer"], "year": 2003, "n_citations": 59}
{"id": 4140755, "s2_id": "c2b3b460372deafede4a2e45ee88ad0e9fdbdc97", "title": "Device to Remotely Track and Locate the Position of a Child for Safety", "abstract": "Parents are always worried about the wellbeing of their children. As per the Statistics Report 2017 by Missing Children Europe Organization, a child is reported missing every 2 minutes. Due to the imminent threat, parents are prone to buy their children mobile phones to keep in touch with them. However, giving a Mobile phone to a child can cause issues including cyber bullying, improper use of social networks, access to mature age and illicit content on the internet and possibly, phone theft. As an effort to tackle some of those issues, this paper proposes a solution which enables parents to call, locate and track their children using a child-friendly mobile device. The common scenario the device would come to play is in enhancing the safety of a child who would travel alone on a typical route; for instance a child who walks from home to school and back. The device can be calibrated to keep track of a typical route of travel. Then, if the device detects some deviation from the usual route, it would trigger a notification to parents. A probability matrix based novel algorithm is introduced to detect route deviation. Design details of the mobile device, along with the details of the route deviation detection algorithm are presented in this paper.", "venue": "2020 IEEE Integrated STEM Education Conference (ISEC)", "authors": ["S.M.K.C.S.B.  Egodawela", "H.M.D.M.B.  Herath", "R. D. Ranaweera", "J. V. Wijayakulasooriya"], "year": 2020, "n_citations": 0}
{"id": 4140780, "s2_id": "f3f2e272176c60793b22701eb8e8918435d0766e", "title": "Proximu: Efficiently Scaling DNN Inference in Multi-core CPUs through Near-Cache Compute", "abstract": "Deep Neural Network (DNN) inference is emerging as the fundamental bedrock for a multitude of utilities and services. CPUs continue to scale up their raw compute capabilities for DNN inference along with mature high performance libraries to extract optimal performance. While general purpose CPUs offer unique attractive advantages for DNN inference at both datacenter and edge, they have primarily evolved to optimize single thread performance. For highly parallel, throughput-oriented DNN inference, this results in inefficiencies in both power and performance, impacting both raw performance scaling and overall performance/watt. \nWe present Proximu$\\$$, where we systematically tackle the root inefficiencies in power and performance scaling for CPU DNN inference. Performance scales efficiently by distributing light-weight tensor compute near all caches in a multi-level cache hierarchy. This maximizes the cumulative utilization of the existing bandwidth resources in the system and minimizes movement of data. Power is drastically reduced through simple ISA extensions that encode the structured, loop-y workload behavior. This enables a bulk offload of pre-decoded work, with loop unrolling in the light-weight near-cache units, effectively bypassing the power-hungry stages of the wide Out-of-Order (OOO) CPU pipeline. \nAcross a number of DNN models, Proximu$\\$$ achieves a 2.3x increase in convolution performance/watt with a 2x to 3.94x scaling in raw performance. Similarly, Proximu$\\$$ achieves a 1.8x increase in inner-product performance/watt with 2.8x scaling in performance. With no changes to the programming model, no increase in cache capacity or bandwidth and minimal additional hardware, Proximu$\\$$ enables unprecedented CPU efficiency gains while achieving similar performance to state-of-the-art Domain Specific Accelerators (DSA) for DNN inference in this AI era.", "venue": "ArXiv", "authors": ["Anant V. Nori", "Rahul  Bera", "Shankar  Balachandran", "Joydeep  Rakshit", "Om Ji Omer", "Avishaii  Abuhatzera", "Belliappa  Kuttanna", "Sreenivas  Subramoney"], "year": 2020, "n_citations": 2}
{"id": 4141202, "s2_id": "b29e370161e3ab4ac2a6750e25c63b7de854ffe4", "title": "A Unified Learning Platform for Dynamic Frequency Scaling in Pipelined Processors", "abstract": "A machine learning (ML) design framework is proposed for dynamically adjusting clock frequency based on propagation delay of individual instructions. A Random Forest model is trained to classify propagation delays in real-time, utilizing current operation type, current operands, and computation history as ML features. The trained model is implemented in Verilog as an additional pipeline stage within a baseline processor. The modified system is simulated at the gate-level in 45 nm CMOS technology, exhibiting a speed-up of 68% and energy reduction of 37% with coarse-grained ML classification. A speed-up of 95% is demonstrated with finer granularities at additional energy costs.", "venue": "ArXiv", "authors": ["Arash Fouman Ajirlou", "Inna  Partin-Vaisband"], "year": 2020, "n_citations": 0}
{"id": 4146707, "s2_id": "4766c8e08db2be42a64b902b085c47a2dd0c4ecb", "title": "SIFO: Secure Computational Infrastructure using FPGA Overlays", "abstract": "Secure Function Evaluation (SFE) has received recent attention due to the massive collection and mining of personal data, but remains impractical due to its large computational cost. Garbled Circuits (GC) is a protocol for implementing SFE which can evaluate any function that can be expressed as a Boolean circuit and obtain the result while keeping each party's input private. Recent advances have led to a surge of garbled circuit implementations in software for a variety of different tasks. However, these implementations are inefficient and therefore GC is not widely used, especially for large problems. This research investigates, implements and evaluates secure computation generation using a heterogeneous computing platform featuring FPGAs. We have designed and implemented SIFO: Secure computational Infrastructure using FPGA Overlays. Unlike traditional FPGA design, a coarse grained overlay architecture is adopted which supports mapping SFE problems that are too large to map to a single FPGA. Host tools provided include SFE problem generator, parser and automatic host code generation. Our design allows re-purposing an FPGA to evaluate different SFE tasks without the need for reprogramming, and fully explores the parallelism for any GC problem. Our system demonstrates an order of magnitude speedup compared with an existing software platform.", "venue": "Int. J. Reconfigurable Comput.", "authors": ["Xin  Fang", "Stratis  Ioannidis", "Miriam  Leeser"], "year": 2019, "n_citations": 4}
{"id": 4153301, "s2_id": "f22fe82f805ee70c214c7acfb6723b3590e342a5", "title": "TRIAD: A triple patterning lithography aware detailed router", "abstract": "TPL-friendly detailed routers require a systematic approach to detect TPL conflicts. However, the complexity of conflict graph (CG) impedes directly detecting TPL conflicts in CG. This work proposes a token graph-embedded conflict graph (TECG) to facilitate the TPL conflict detection while maintaining high coloring-flexibility. We then develop a TPL aware detailed router (TRIAD) by applying TECG to a gridless router with the TPL stitch generation. Compared to a greedy coloring approach, experimental results indicate that TRIAD generates no conflicts and few stitches with shorter wirelength at the cost of 2.41\u00d7 of runtime.", "venue": "2012 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Yen-Hung  Lin", "Bei  Yu", "David Z. Pan", "Yih-Lang  Li"], "year": 2012, "n_citations": 45}
{"id": 4156817, "s2_id": "47405b520dde30f879d78b3ba3116e99f459d7ce", "title": "Proceedings of the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015)", "abstract": "This volume contains the papers accepted at the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with the Conference on Design, Automation and Test in Europe (DATE).", "venue": "ArXiv", "authors": ["Frank  Hannig", "Dietmar  Fey", "Anton  Lokhmotov"], "year": 2015, "n_citations": 1}
{"id": 4157756, "s2_id": "0a76e83cb56ffd933ef40bda4639461b8f2cd82e", "title": "Agile SoC development with open ESP", "abstract": "ESP is an open-source research platform for heterogeneous SoC design. The platform combines a modular tile-based architecture with a variety of application-oriented flows for the design and optimization of accelerators. The ESP architecture is highly scalable and strikes a balance between regularity and specialization. The companion methodology raises the level of abstraction to system-level design and enables an automated flow from software and hardware development to full-system prototyping on FPGA. For application developers, ESP offers domain-specific automated solutions to synthesize new accelerators for their software and to map complex workloads onto the SoC architecture. For hardware engineers, ESP offers automated solutions to integrate their accelerator designs into the complete SoC. Conceived as a heterogeneous integration platform and tested through years of teaching at Columbia University, ESP supports the open-source hardware community by providing a flexible platform for agile SoC development.", "venue": "Proceedings of the 39th International Conference on Computer-Aided Design", "authors": ["Paolo  Mantovani", "Davide  Giri", "Giuseppe Di Guglielmo", "Luca  Piccolboni", "Joseph  Zuckerman", "Emilio G. Cota", "Michele  Petracca", "Christian  Pilato", "Luca P. Carloni"], "year": 2020, "n_citations": 17}
{"id": 4159760, "s2_id": "dfd5224dc386778f2f57d1461c41c41d93210b7e", "title": "IRONMAN: GNN-assisted Design Space Exploration in High-Level Synthesis via Reinforcement Learning", "abstract": "Despite the great success of High-Level Synthesis (HLS) tools, we observe several unresolved challenges: 1) the high-level abstraction of programming styles in HLS conceals optimization opportunities; 2) existing HLS tools do not provide flexible trade-offs among different objectives and constraints; 3) the actual quality of the resulting RTL designs is hard to predict. To this end, we propose an end-to-end framework, IRONMAN. The primary goal is to enable a flexible and automated design space exploration (DSE), which can provide either optimized solutions under user-specified constraints, or Pareto trade-offs among different objectives (e.g., resource types, area, and latency). IronMan consists of three components: GPP (a highly accurate graph-neural-network-based performance predictor), RLMD (a reinforcement-learning-based DSE engine that explores the optimized resource allocation strategy), and CT (a code transformer that assists RLMD and GPP by extracting data flow graphs from original HLS C/C++). Experimental results show that, 1) GPP achieves high prediction accuracy, reducing prediction errors of HLS tools by 10.9X in resource usage and 5.7X in timing; 2) RLMD obtains optimized or Pareto solutions outperforming genetic algorithm and simulated annealing by 12.7% and 12.9%, respectively; 3) IronMan can find optimized solutions perfectly matching various DSP constraints, with 2.54X fewer DSPs and up to 6X shorter latency than those of HLS tools. IronMan is also up to 400X faster than meta-heuristic techniques and HLS tools.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Nan  Wu", "Yuan  Xie", "Cong  Hao"], "year": 2021, "n_citations": 7}
{"id": 4160164, "s2_id": "89cc6e1df11b374c3de49cb953d7d4c0b9ecb7b3", "title": "Leveraging Architectural Support of Three Page Sizes with Trident", "abstract": "Large pages are commonly deployed to reduce address translation overheads for big-memory workloads. Modern x86-64 processors from Intel and AMD support two large page sizes -- 1GB and 2MB. However, previous works on large pages have primarily focused on 2MB pages, partly due to lack of substantial evidence on the profitability of 1GB pages to real-world applications. We argue that in fact, inadequate system software support is responsible for a decade of underutilized hardware support for 1GB pages. \nThrough extensive experimentation on a real system, we demonstrate that 1GB pages can improve performance over 2MB pages, and when used in tandem with 2MB pages for an important set of applications; the support for the latter is crucial but missing in current systems. Our design and implementation of \\trident{} in Linux fully exploit hardware supported large pages by dynamically and transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable. \\trident{} speeds up eight memory-intensive applications by {$18\\%$}, on average, over Linux's use of 2MB pages. We also propose \\tridentpv{}, an extension to \\trident{} that effectively virtualizes 1GB pages via copy-less promotion and compaction in the guest OS. Overall, this paper shows that even GB-sized pages have considerable practical significance with adequate software enablement, in turn motivating architects to continue investing/innovating in large pages.", "venue": "ArXiv", "authors": ["Venkat Sri Sai Ram", "Ashish  Panwar", "Arkaprava  Basu"], "year": 2020, "n_citations": 0}
{"id": 4162538, "s2_id": "9839f8d6b0ce4af071d04b6e0ea2e93a018af479", "title": "Reduced-latency SC polar decoder architectures", "abstract": "Polar codes have become one of the most favorable capacity achieving error correction codes (ECC) along with their simple encoding method. However, among the very few prior successive cancellation (SC) polar decoder designs, the required long code length makes the decoding latency high. In this paper, conventional decoding algorithm is transformed with look-ahead techniques. This reduces the decoding latency by 50%. With pipelining and parallel processing schemes, a parallel SC polar decoder is proposed. Sub-structure sharing approach is employed to design the merged processing element (PE). Moreover, inspired by the real FFT architecture, this paper presents a novel input generating circuit (ICG) block that can generate additional input signals for merged PEs on-the-fly. Gate-level analysis has demonstrated that the proposed design shows advantages of 50% decoding latency and twice throughput over the conventional one with similar hardware cost.", "venue": "2012 IEEE International Conference on Communications (ICC)", "authors": ["Chuan  Zhang", "Bo  Yuan", "Keshab K. Parhi"], "year": 2012, "n_citations": 89}
{"id": 4162630, "s2_id": "d7060b87e606d3f7bf3e4d0489e55addf1f69511", "title": "Learning Pareto-Frontier Resource Management Policies for Heterogeneous SoCs: An Information-Theoretic Approach", "abstract": "Mobile system-on-chips (SoCs) are growing in their complexity and heterogeneity (e.g., Arm\u2019s Big-Little architecture) to meet the needs of emerging applications, including games and artificial intelligence. This makes it very challenging to optimally manage the resources (e.g., controlling the number and frequency of different types of cores) at runtime to meet the desired trade-offs among multiple objectives such as performance and energy. This paper proposes a novel information-theoretic framework referred to as PaRMIS to create Pareto-optimal resource management policies for given target applications and design objectives. PaRMIS specifies parametric policies to manage resources and learns statistical models from candidate policy evaluation data in the form of target design objective values. The key idea is to select a candidate policy for evaluation in each iteration guided by statistical models that maximize the information gain about the true Pareto front. Experiments on a commercial heterogeneous SoC show that PaRMIS achieves better Pareto fronts and is easily usable to optimize complex objectives (e.g., performance per Watt) when compared to prior methods", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Aryan  Deshwal", "Syrine  Belakaria", "Ganapati  Bhat", "Janardhan Rao Doppa", "Partha Pratim Pande"], "year": 2021, "n_citations": 1}
{"id": 4162645, "s2_id": "8b4e4df8e3334046ae69feb2827b3d279cc809dd", "title": "Introducing \"Neuromorphic Computing and Engineering\"", "abstract": "The standard nature of computing is currently being challenged by a range of problems that start to hinder technological progress. One of the strategies being proposed to address some of these problems is to develop novel brain-inspired processing methods and technologies, and apply them to a wide range of application scenarios. This is an extremely challenging endeavor that requires researchers in multiple disciplines to combine their efforts and co-design at the same time the processing methods, the supporting computing architectures, and their underlying technologies. The journal ``Neuromorphic Computing and Engineering'' (NCE) has been launched to support this new community in this effort and provide a forum and repository for presenting and discussing its latest advances. Through close collaboration with our colleagues on the editorial team, the scope and characteristics of NCE have been designed to ensure it serves a growing transdisciplinary and dynamic community across academia and industry.", "venue": "Neuromorph. Comput. Eng.", "authors": ["Giacomo  Indiveri"], "year": 2021, "n_citations": 1}
{"id": 4164147, "s2_id": "94b49b2604fabda19fc9c4d8c8c78867ee2ba755", "title": "An Automated Framework for Board-level Trojan Benchmarking", "abstract": "Economic and operational advantages have led the supply chain of printed circuit boards (PCBs) to incorporate various untrusted entities. Any of the untrusted entities are capable of introducing malicious alterations to facilitate a functional failure or leakage of secret information during field operation. While researchers have been investigating the threat of malicious modification within the scale of individual microelectronic components, the possibility of a board-level malicious manipulation has essentially been unexplored. In the absence of standard benchmarking solutions, prospective countermeasures for PCB trust assurance are likely to utilize homegrown representation of the attacks that undermines their evaluation and does not provide scope for comparison with other techniques. In this paper, we have developed the first-ever benchmarking solution to facilitate an unbiased and comparable evaluation of countermeasures applicable to PCB trust assurance. Based on a taxonomy tailored for PCB-level alterations, we have developed high-level Trojan models. From these models, we have generated a custom pool of board-level Trojan designs of varied complexity and functionality. We have also developed a tool-flow for automatically inserting these Trojans into various PCB designs and generate the Trojan benchmarks (i.e., PCB designs with Trojan). The tool-based Trojan insertion facilitate a comprehensive evaluation against large number of diverse Trojan implementations and application of data mining for trust verification. Finally, with experimental measurements from a fabricated PCB, we analyze the stealthiness of the Trojan designs.", "venue": "ArXiv", "authors": ["Tamzidul  Hoque", "Shuo  Yang", "Aritra  Bhattacharyay", "Jonathan  Cruz", "Swarup  Bhunia"], "year": 2020, "n_citations": 2}
{"id": 4165258, "s2_id": "814019a2151146e25d4900419f78181b1a20ff13", "title": "Efficient Fault Injection based on Dynamic HDL Slicing Technique", "abstract": "This work proposes a fault injection methodology where Hardware Description Language (HDL) code slicing is exploited to prune fault injection locations, thus enabling more efficient campaigns for safety mechanisms evaluation. In particular, the dynamic HDL slicing technique provides for a highly collapsed critical fault list and allows avoiding injections at redundant locations or time-steps. Experimental results show that the proposed methodology integrated into commercial tool flow doubles the simulation speed when comparing to the state-of-the-art industrial-grade EDA tool flows.", "venue": "2019 IEEE 25th International Symposium on On-Line Testing and Robust System Design (IOLTS)", "authors": ["Ahmet Cagri Bagbaba", "Maksim  Jenihhin", "Jaan  Raik", "Christian  Sauer"], "year": 2019, "n_citations": 2}
{"id": 4168215, "s2_id": "486b8ceab0c647312500dc2b21d1eebe0ab65f21", "title": "Difficulties in the Implementation of Quantum Computers", "abstract": "This paper reviews various engineering hurdles facing the field of quantum computing. Specifically, problems related to decoherence, state preparation, error correction, and implementability of gates are considered.", "venue": "ArXiv", "authors": ["Abhilash  Ponnath"], "year": 2006, "n_citations": 8}
{"id": 4172724, "s2_id": "6d0322c37a9ef23c7bf29ae35fddabda7ba4fce1", "title": "Silicon Photonic Microring Based Chip-Scale Accelerator for Delayed Feedback Reservoir Computing", "abstract": "To perform temporal and sequential machine learning tasks, the use of conventional Recurrent Neural Networks (RNNs) has been dwindling due to the training complexities of RNNs. To this end, accelerators for delayed feedback reservoir computing (DFRC) have attracted attention in lieu of RNNs, due to their simple hardware implementations. A typical implementation of a DFRC accelerator consists of a delay loop and a single nonlinear neuron, together acting as multiple virtual nodes for computing. In prior work, photonic DFRC accelerators have shown an undisputed advantage of fast computation over their electronic counterparts. In this paper, we propose a more energy-efficient chip-scale DFRC accelerator that employs a silicon photonic microring (MR) based nonlinear neuron along with on-chip photonic waveguides-based delayed feedback loop. Our evaluations show that, compared to a well-known photonic DFRC accelerator from prior work, our proposed MR-based DFRC accelerator achieves 35% and 98.7% lower normalized root mean square error (NRMSE), respectively, for the prediction tasks of NARMA10 and Santa Fe time series. In addition, our MR-based DFRC accelerator achieves 58.8% lower symbol error rate (SER) for the Non-Linear Channel Equalization task. Moreover, our MR-based DFRC accelerator has 98\u00d7 and 93\u00d7 faster training time, respectively, compared to an electronic and a photonic DFRC accelerators from prior work.", "venue": "2021 34th International Conference on VLSI Design and 2021 20th International Conference on Embedded Systems (VLSID)", "authors": ["Sairam Sri Vatsavai", "Ishan  Thakkar"], "year": 2021, "n_citations": 0}
{"id": 4173777, "s2_id": "a170a2e07d02b1ee0281f8f657aef820cb303d4c", "title": "Dissecting GPU Memory Hierarchy Through Microbenchmarking", "abstract": "Memory access efficiency is a key factor in fully utilizing the computational power of graphics processing units (GPUs). However, many details of the GPU memory hierarchy are not released by GPU vendors. In this paper, we propose a novel fine-grained microbenchmarking approach and apply it to three generations of NVIDIA GPUs, namely Fermi, Kepler, and Maxwell, to expose the previously unknown characteristics of their memory hierarchies. Specifically, we investigate the structures of different GPU cache systems, such as the data cache, the texture cache and the translation look-aside buffer (TLB). We also investigate the throughput and access latency of GPU global memory and shared memory. Our microbenchmark results offer a better understanding of the mysterious GPU memory hierarchy, which will facilitate the software optimization and modelling of GPU architectures. To the best of our knowledge, this is the first study to reveal the cache properties of Kepler and Maxwell GPUs, and the superiority of Maxwell in shared memory performance under bank conflict.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Xinxin  Mei", "Xiaowen  Chu"], "year": 2017, "n_citations": 152}
{"id": 4175098, "s2_id": "11a4294cacdd05fa27ba58e35662659ea57608d8", "title": "Running Identical Threads in C-Slow Retiming based Designs for Functional Failure Detection", "abstract": "This paper shows the usage of C-Slow Retiming (CSR) in safety critical and low power applications. CSR generates C copies of a design by reusing the given logic resources in a time sliced fashion. When all C design copies are stimulated with the same input values, then all C design copies should behave the same way and will therefore create a redundant system. The paper shows that this special method of using CSR offers great benefits when used in safety critical and low power applications. Additional optimization techniques towards reducing register count are shown and an on-the-fly recovery mechanism is discussed.", "venue": "ArXiv", "authors": ["Tobias  Strauch"], "year": 2015, "n_citations": 0}
{"id": 4178100, "s2_id": "d441e66baa101273fc5323a1fb05f5141f5a8a51", "title": "QUAC-TRNG: High-Throughput True Random Number Generation Using Quadruple Row Activation in Commodity DRAM Chips", "abstract": "True random number generators (TRNG) sample random physical processes to create large amounts of random numbers for various use cases, including security-critical cryptographic primitives, scientific simulations, machine learning applications, and even recreational entertainment. Unfortunately, not every computing system is equipped with dedicated TRNG hardware, limiting the application space and security guarantees for such systems. To open the application space and enable security guarantees for the overwhelming majority of computing systems that do not necessarily have dedicated TRNG hardware (e.g., processing-in-memory systems), we develop QUAC-TRNG, a new high-throughput TRNG that can be fully implemented in commodity DRAM chips, which are key components in most modern systems.QUAC-TRNG exploits the new observation that a carefully-engineered sequence of DRAM commands activates four consecutive DRAM rows in rapid succession. This QUadruple ACtivation (QUAC) causes the bitline sense amplifiers to non-deterministically converge to random values when we activate four rows that store conflicting data because the net deviation in bitline voltage fails to meet reliable sensing margins.We experimentally demonstrate that QUAC reliably generates random values across 136 commodity DDR4 DRAM chips from one major DRAM manufacturer. We describe how to develop an effective TRNG (QUAC-TRNG) based on QUAC. We evaluate the quality of our TRNG using the commonly-used NIST statistical test suite for randomness and find that QUAC-TRNG successfully passes each test. Our experimental evaluations show that QUAC-TRNG reliably generates true random numbers with a throughput of 3.44 Gb/s (per DRAM channel), outperforming the state-of-the-art DRAM-based TRNG by 15.08\u00d7 and 1.41\u00d7 for basic and throughput-optimized versions, respectively. We show that QUAC-TRNG utilizes DRAM bandwidth better than the state-of-the-art, achieving up to 2.03\u00d7 the throughput of a throughput-optimized baseline when scaling bus frequencies to 12 GT/s.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Ataberk  Olgun", "Minesh  Patel", "Abdullah Giray Yaglik\u00e7i", "Haocong  Luo", "Jeremie S. Kim", "Nisa  Bostanci", "Nandita  Vijaykumar", "Oguz  Ergin", "Onur  Mutlu"], "year": 2021, "n_citations": 7}
{"id": 4182965, "s2_id": "f7dfc144d904adf7263fe6a9746f207b46634650", "title": "Object Detection and Ranging for Autonomous Navigation of Mobile Robots", "abstract": "In the recent decade, electronic technology gets advanced day by day the methodologies too should update. For the purpose of ranging various methods such Radio Detection and Ranging (RADAR), Light Detection and Ranging (LIDAR) and Sonic Navigation and Ranging (SONAR) etc. are used. Later, by adapting the earlier technologies and further modifying the purposes of detection and ranging in navigation, the technology of Sonic Detection and Ranging (SODAR) is used in modern robotics. The SODAR can be defined as a child of SONAR and also a twin of Echo sounder. The echo-sounder is used only for ranging. But the SODAR use the low-frequency wave of 33 kHz to measure the underwater depth and also to detect the objects below the water medium. So, this work comprises the designing of a system to evaluate the Object Detection and Ranging for Autonomous Navigation of Mobile Robots. \u00a9 2021 The Authors, Published in ResearchGate. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)", "venue": "ArXiv", "authors": ["Md Ziaul Haque Zim", "Nimai Chandra Das"], "year": 2021, "n_citations": 0}
{"id": 4185301, "s2_id": "2d0caa6c7a18a74bad842fd6d7cb7a41b3de5883", "title": "Persistence and Synchronization: Friends or Foes?", "abstract": "Emerging non-volatile memory (NVM) technologies promise memory speed byte-addressable persistent storage with a load/store interface. However, programming applications to directly manipulate NVM data is complex and error-prone. Applications generally employ libraries that hide the low-level details of the hardware and provide a transactional programming model to achieve crash-consistency. Furthermore, applications continue to expect correctness during concurrent executions, achieved through the use of synchronization. To achieve this, applications seek well-known ACID guarantees. However, realizing this presents designers of transactional systems with a range of choices in how to combine several low-level techniques, given target hardware features and workload characteristics. In this paper, we provide a comprehensive evaluation of the impact of combining existing crash-consistency and synchronization methods for achieving performant and correct NVM transactional systems. We consider different hardware characteristics, in terms of support for hardware transactional memory (HTM) and the boundaries of the persistence domain (transient or persistent caches). By characterizing persistent transactional systems in terms of their properties, we make it possible to better understand the tradeoffs of different implementations and to arrive at better design choices for providing ACID guarantees. We use both real hardware with Intel Optane DC persistent memory and simulation to evaluate a persistent version of hardware transactional memory, a persistent version of software transactional memory, and undo/redo logging. Through our empirical study, we show two major factors that impact the cost of supporting persistence in transactional systems: the persistence domain (transient or persistent caches) and application characteristics, such as transaction size and parallelism.", "venue": "ArXiv", "authors": ["Pradeep  Fernando", "Irina  Calciu", "Jayneel  Gandhi", "Aasheesh  Kolli", "Ada  Gavrilovska"], "year": 2020, "n_citations": 0}
{"id": 4186460, "s2_id": "db8ed23aaf65f4234ca08d19c5ae53238384e713", "title": "3DCAM: A Low Overhead Crosstalk Avoidance Mechanism for TSV-Based 3D ICs", "abstract": "Three Dimensional Integrated Circuits (3D IC) offer lower power consumption, higher performance, higher bandwidth, and scalability over the conventional two dimensional ICs. Through-Silicon Via (TSV) is one of the fabrication mechanisms that connects stacked dies to each other. The large size of TSVs and the proximity between them lead to undesirable coupling capacitance. This interference causes mutual influences between adjacent TSVs and produces crosstalk noise. Furthermore, this effect threats the reliability of data during traversal between layers. This paper proposes a mechanism that efficiently reduces crosstalk noise between TSVs with lower area overhead as compared to previous works. This mechanism revolves around the fact that retaining TSV value in current state can reduce coupling in some cases. To evaluate the mechanism, gem5 simulator is used for data extraction and several benchmarks are taken from the SPEC2006 suite. The simulation results show that the proposed mechanism reduces crosstalk noise with only 30% imposed TSV overhead while delay decreased up to 25.7% as compared to a recent related work.", "venue": "ArXiv", "authors": ["Reza  Mirosanlou", "Mohammadkazem  Taram", "Zahra  Shirmohammadi", "Seyed Ghassem Miremadi"], "year": 2019, "n_citations": 0}
{"id": 4189759, "s2_id": "7084b6fa51906037a1da2a8a9bee33ed606b69ca", "title": "Versatile and concurrent FPGA-based architecture for practical quantum communication systems", "abstract": "Andrea Stanco1,\u2217, Francesco B. L. Santagiustina1,2, Luca Calderaro1 Marco Avesani1, Tommaso Bertapelle1, Daniele Dequal3, Giuseppe Vallone1,4 and Paolo Villoresi1 Dipartimento di Ingegneria dell\u2019Informazione, Universit\u00e0 degli Studi di Padova, via Gradenigo 6B, 35131 Padova, Italy Dipartimento di Matematica \u201dTullio Leci-Civita\u201d, Universit\u00e0 degli Studi di Padova, Via Trieste 63, 35121 Padova, Italy Unit\u00e0 Telecomunicazioni e Navigazione, Agenzia Spaziale Italiana, contrada Terlecchia s.n.c., Matera, Italy Dipartimento di Fisica e Astronomia, Universit\u00e0 degli Studi di Padova, via Marzolo 8, 35131 Padova, Italy", "venue": "ArXiv", "authors": ["Andrea  Stanco", "Francesco B. L. Santagiustina", "Luca  Calderaro", "Marco  Avesani", "Tommaso  Bertapelle", "Daniele  Dequal", "Giuseppe  Vallone", "Paolo  Villoresi"], "year": 2021, "n_citations": 1}
{"id": 4192991, "s2_id": "48015c154f8243e519ba72e7c92ea6c77b6106a9", "title": "FPGA Based Implementation of Distributed Minority and Majority Voting Based Redundancy for Mission and Safety-Critical Applications", "abstract": "Electronic circuits and systems used in mission and safety-critical applications usually employ redundancy in the design to overcome arbitrary fault(s) or failure(s) and guarantee the correct operation. In this context, the distributed minority and majority voting based redundancy (DMMR) scheme forms an efficient alternative to the conventional N-modular redundancy (NMR) scheme for implementing mission and safety-critical circuits and systems by significantly minimizing their weight and design cost and also their design metrics whilst providing a similar degree of fault tolerance. This article presents the first FPGAs based implementation of example DMMR circuits and compares it with counterpart NMR circuits on the basis of area occupancy and critical path delay viz. area-delay product (ADP). The example DMMR circuits and counterpart NMR circuits are able to accommodate the faulty or failure states of 2, 3 and 4 function modules. For physical synthesis, two commercial Xilinx FPGAs viz. Spartan 3E and Virtex 5 corresponding to 90nm and 65nm CMOS processes, and two radiation-tolerant and military grade Xilinx FPGAs viz. QPro Virtex 2 and QPro Virtex E corresponding to 150nm and 180nm CMOS processes were considered for the NMR and DMMR circuit realizations which employ the 4-by-4 array multiplier as a representative function module. To achieve a fault tolerance of 2 function modules, both the DMMR and the NMR schemes provide near similar mean ADPs across all the four FPGAs. But while achieving a fault tolerance of 3 function modules the DMMR features reduced ADP by 44.5% on average compared to the NMR, and in achieving a fault tolerance of 4 function modules the DMMR reports reduced ADP by 56.5% on average compared to the NMR with respect to all the four FPGAs considered.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 2}
{"id": 4195256, "s2_id": "ac4308eecc0079d37275b94cc74db160791dc417", "title": "8 Steps to 3.7 TFLOP/s on NVIDIA V100 GPU: Roofline Analysis and Other Tricks", "abstract": "Performance optimization can be a daunting task especially as the hardware architecture becomes more and more complex. This paper takes a kernel from the Materials Science code BerkeleyGW, and demonstrates a few performance analysis and optimization techniques. Despite challenges such as high register usage, low occupancy, complex data access patterns, and the existence of several long-latency instructions, we have achieved 3.7 TFLOP/s of double-precision performance on an NVIDIA V100 GPU, with 8 optimization steps. This is 55% of the theoretical peak, 6.7 TFLOP/s, at nominal frequency 1312 MHz, and 70% of the more customized peak based on our 58% FMA ratio, 5.3 TFLOP/s. An array of techniques used to analyze this OpenACC kernel and optimize its performance are shown, including the use of hierarchical Roofline performance model and the performance tool Nsight Compute. This kernel exhibits computational characteristics that are commonly seen in many high-performance computing (HPC) applications, and are expected to be very helpful to a general audience of HPC developers and computational scientists, as they pursue more performance on NVIDIA GPUs.", "venue": "ArXiv", "authors": ["Charlene  Yang"], "year": 2020, "n_citations": 4}
{"id": 4197590, "s2_id": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery", "abstract": "After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.", "venue": "Commun. ACM", "authors": ["Sara  Hooker"], "year": 2021, "n_citations": 34}
{"id": 4200321, "s2_id": "641a44c8caa908ec8f268e582d7ce88d3c90c6ad", "title": "Extended Bit-Plane Compression for Convolutional Neural Network Accelerators", "abstract": "After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I/O bandwidth and the energy consumption is dominated by I/O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4\u00d7 relative to uncompressed data and a gain of 60% over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.", "venue": "2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)", "authors": ["Lukas  Cavigelli", "Luca  Benini"], "year": 2019, "n_citations": 15}
{"id": 4200427, "s2_id": "0c201c52260963665e0d30b3fe0fa31291af210b", "title": "VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference", "abstract": "Quantization enables efficient acceleration of deep neural networks by reducing model memory footprint and exploiting low-cost integer math hardware units. Quantization maps floating-point weights and activations in a trained model to low-bitwidth integer values using scale factors. Excessive quantization, reducing precision too aggressively, results in accuracy degradation. When scale factors are shared at a coarse granularity across many dimensions of each tensor, effective precision of individual elements within the tensor are limited. To reduce quantization-related accuracy loss, we propose using a separate scale factor for each small vector of (\u224816-64) elements within a single dimension of a tensor. To achieve an efficient hardware implementation, the per-vector scale factors can be implemented with low-bitwidth integers when calibrated using a two-level quantization scheme. We find that per-vector scaling consistently achieves better inference accuracy at low precision compared to conventional scaling techniques for popular neural networks without requiring retraining. We also modify a deep learning accelerator hardware design to study the area and energy overheads of per-vector scaling support. Our evaluation demonstrates that per-vector scaled quantization with 4-bit weights and activations achieves 37% area saving and 24% energy saving while maintaining over 75% accuracy for ResNet50 on ImageNet. 4-bit weights and 8-bit activations achieve near-full-precision accuracy for both BERT-base and BERT-large on SQuAD while reducing area by 26% compared to an 8-bit baseline.", "venue": "ArXiv", "authors": ["Steve  Dai", "Rangharajan  Venkatesan", "Haoxing  Ren", "Brian  Zimmer", "William J. Dally", "Brucek  Khailany"], "year": 2021, "n_citations": 6}
{"id": 4200635, "s2_id": "4dac604d02c6422fd84c7afbab7a044d50777aed", "title": "SHEARer: highly-efficient hyperdimensional computing by software-hardware enabled multifold approximation", "abstract": "Hyperdimensional computing (HD) is an emerging paradigm for machine learning based on the evidence that the brain computes on high-dimensional, distributed, representations of data. The main operation of HD is encoding, which transfers the input data to hyperspace by mapping each input feature to a hypervector, followed by a bundling procedure that adds up the hypervectors to realize the encoding hypervector. The operations of HD are simple and highly parallelizable, but the large number of operations hampers the efficiency of HD in embedded domain. In this paper, we propose SHEARer, an algorithmhardware co-optimization to improve the performance and energy consumption of HD computing. We gain insight from a prudent scheme of approximating the hypervectors that, thanks to error resiliency of HD, has minimal impact on accuracy while provides high prospect for hardware optimization. Unlike previous works that generate the encoding hypervectors in full precision and then and then perform ex-post quantization, we compute the encoding hypervectors in an approximate manner that saves resources yet affords high accuracy. We also propose a novel FPGA architecture that achieves striking performance through massive parallelism with low power consumption. Moreover, we develop a software framework that enables training HD models by emulating the proposed approximate encodings. The FPGA implementation of SHEARer achieves an average throughput boost of 104,904\u00d7 (15.7\u00d7) and energy savings of up to 56,044\u00d7 (301\u00d7) compared to state-of-the-art encoding methods implemented on Raspberry Pi 3 (GeForce GTX 1080 Ti) using practical machine learning datasets.", "venue": "ISLPED", "authors": ["Behnam  Khaleghi", "Sahand  Salamat", "Anthony  Thomas", "Fatemeh  Asgarinejad", "Yeseong  Kim", "Tajana  Simunic"], "year": 2020, "n_citations": 0}
{"id": 4202261, "s2_id": "8d30044d89a57babde1274ceced253946620409d", "title": "ILP-Based Alleviation of Dense Meander Segments With Prioritized Shifting and Progressive Fixing in PCB Routing", "abstract": "Length-matching is an important technique to balance delays of bus signals in high-performance printed circuit board (PCB) routing. Existing routers, however, may generate very dense meander segments. Signals propagating along these meander segments exhibit a speedup effect due to crosstalk between the segments of the same wire, thus leading to mismatch of arrival times even under the same physical wire length. In this paper, we present a post-processing method to enlarge the width and the distance of meander segments and hence distribute them more evenly on the board so that crosstalk can be reduced. In the proposed framework, we model the sharing of available routing areas after removing dense meander segments from the initial routing, as well as the generation of relaxed meander segments and their groups for wire length compensation. This model is transformed into an ILP problem and solved for a balanced distribution of wire patterns. In addition, we adjust the locations of long wire segments according to wire priorities to swap free spaces toward critical wires that need much length compensation. To reduce the problem space of the ILP model, we also introduce a progressive fixing technique so that wire patterns are grown gradually from the edge of the routing toward the center area. Experimental results show that the proposed method can expand meander segments significantly even under very tight area constraints, so that the speedup effect can be alleviated effectively in high-performance PCB designs.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Tsun-Ming  Tseng", "Bing  Li", "Tsung-Yi  Ho", "Ulf  Schlichtmann"], "year": 2015, "n_citations": 7}
{"id": 4202315, "s2_id": "40f34a2d32e908603c853f8ac5fc7c8aad32660f", "title": "Design guidelines for high-performance SCM hierarchies", "abstract": "With emerging storage-class memory (SCM) nearing commercialization, there is evidence that it will deliver the much-anticipated high density and access latencies within only a few factors of DRAM. Nevertheless, the latency-sensitive nature of memory-resident services makes seamless integration of SCM in servers questionable. In this paper, we ask the question of how best to introduce SCM for such servers to improve overall performance/cost over existing DRAM-only architectures. We first show that even with the most optimistic latency projections for SCM, the higher memory access latency results in prohibitive performance degradation. However, we find that deployment of a modestly sized high-bandwidth 3D stacked DRAM cache makes the performance of an SCM-mostly memory system competitive. The high degree of spatial locality that memory-resident services exhibit not only simplifies the DRAM cache's design as page-based, but also enables the amortization of increased SCM access latencies and the mitigation of SCM's read/write latency disparity. We identify the set of memory hierarchy design parameters that plays a key role in the performance and cost of a memory system combining an SCM technology and a 3D stacked DRAM cache. We then introduce a methodology to drive provisioning for each of these design parameters under a target performance/cost goal. Finally, we use our methodology to derive concrete results for specific SCM technologies. With PCM as a case study, we show that a two bits/cell technology hits the performance/cost sweet spot, reducing the memory subsystem cost by 40% while keeping performance within 3% of the best performing DRAM-only system, whereas single-level and triple-level cell organizations are impractical for use as memory replacements.", "venue": "MEMSYS", "authors": ["Dmitrii  Ustiugov", "Alexandros  Daglis", "Javier  Picorel", "Mark  Sutherland", "Edouard  Bugnion", "Babak  Falsafi", "Dionisios N. Pnevmatikatos"], "year": 2018, "n_citations": 6}
{"id": 4202363, "s2_id": "a34ff3daa44075a42aa51f45ea203668280f7e92", "title": "DS3: A System-Level Domain-Specific System-on-Chip Simulation Framework", "abstract": "Heterogeneous systems-on-chip (SoCs) are highly favorable computing platforms due to their superior performance and energy efficiency potential compared to homogeneous architectures. They can be further tailored to a specific domain of applications by incorporating processing elements (PEs) that accelerate frequently used kernels in these applications. However, this potential is contingent upon optimizing the SoC for the target domain and utilizing its resources effectively at runtime. To this end, system-level design - including scheduling, power-thermal management algorithms and design space exploration studies - plays a crucial role. This article presents a system-level domain-specific SoC simulation (DS3) framework to address this need. DS3 enables both design space exploration and dynamic resource management for power-performance optimization of domain applications. We showcase DS3 using six real-world applications from wireless communications and radar processing domain. DS3, as well as the reference applications, is shared as open-source software to stimulate research in this area.", "venue": "IEEE Transactions on Computers", "authors": ["Samet E. Arda", "Anish  Krishnakumar", "A. Alper Goksoy", "Nirmal  Kumbhare", "Joshua  Mack", "Anderson L. Sartor", "Ali  Akoglu", "Radu  Marculescu", "Umit Y. Ogras"], "year": 2020, "n_citations": 11}
{"id": 4203173, "s2_id": "a4653472cb603f03068adfbc8489dec96bb9476b", "title": "Analog Signal Processing Solution for Image Alignment", "abstract": "Imaging and Image sensors is a field that is continuously evolving. There are new products coming into the market every day. Some of these have very severe Size, Weight and Power constraints whereas other devices have to handle very high computational loads. Some require both these conditions to be met simultaneously. Current imaging architectures and digital image processing solutions will not be able to meet these ever increasing demands. There is a need to develop novel imaging architectures and image processing solutions to address these requirements. In this work we propose analog signal processing as a solution to this problem. The analog processor is not suggested as a replacement to a digital processor but it will be used as an augmentation device which works in parallel with the digital processor, making the system faster and more efficient. In order to show the merits of analog processing the highly computational Normalized Cross Correlation algorithm is implemented. We propose two novel modifications to the algorithm and a new imaging architecture which, significantly reduces the computation time.", "venue": "ArXiv", "authors": ["Nihar  Athreyas", "Zhiguo  Lai", "Jai  Gupta", "Dev  Gupta"], "year": 2014, "n_citations": 0}
{"id": 4206789, "s2_id": "0e44b7d3f7b72c88bf9234b281c559d0f4eeb37c", "title": "EQUAL: Improving the Fidelity of Quantum Annealers by Injecting Controlled Perturbations", "abstract": "Quantum computing is an information processing paradigm that uses quantum-mechanical properties to speedup computationally hard problems. Gate-based quantum computers and Quantum Annealers (QAs) are two commercially available hardware platforms that are accessible to users today. Although promising, existing gate-based quantum computers consist of only a few dozen qubits and are not large enough for most applications. On the other hand, existing QAs with few thousand of qubits have the potential to solve some domainspecific optimization problems. QAs are single instruction machines and to execute a program, the problem is cast to a Hamiltonian, embedded on the hardware, and a single quantum machine instruction (QMI) is run. Unfortunately, noise and imperfections in hardware result in sub-optimal solutions on QAs even if the QMI is run for thousands of trials. The limited programmability of QAs mean that the user executes the same QMI for all trials. This subjects all trials to a similar noise profile throughout the execution, resulting in a systematic bias. We observe that systematic bias leads to sub-optimal solutions and cannot be alleviated by executing more trials or using existing error-mitigation schemes. To address this challenge, we propose EQUAL (Ensemble QUantum AnneaLing). EQUAL generates an ensemble of QMIs by adding controlled perturbations to the program QMI. When executed on the QA, the ensemble of QMIs steers the program away from encountering the same bias during all trials and thus, improves the quality of solutions. Our evaluations using the 2041-qubit D-Wave QA show that EQUAL bridges the difference between the baseline and the ideal by an average of 14% (and up to 26%), without requiring any additional trials. EQUAL can be combined with existing error mitigation schemes to further bridge the difference between the baseline and ideal by an average of 55% (and up to 68%).", "venue": "ArXiv", "authors": ["Ramin  Ayanzadeh", "Poulami  Das", "Swamit S. Tannu", "Moinuddin  Qureshi"], "year": 2021, "n_citations": 1}
{"id": 4207330, "s2_id": "ff5121530f66131ac9e9772feef93b8de2d399a9", "title": "Architectural Impact on Performance of In-memory Data Analytics: Apache Spark Case Study", "abstract": "While cluster computing frameworks are contin-uously evolving to provide real-time data analysis capabilities,Apache Spark has managed to be at the forefront of big data an-alytics for being a unif ...", "venue": "ArXiv", "authors": ["Ahsan Javed Awan", "Mats  Brorsson", "Vladimir  Vlassov", "Eduard  Ayguad\u00e9"], "year": 2016, "n_citations": 6}
{"id": 4209008, "s2_id": "ae76ae8ee555d3e6364899273bd375a6f6030fac", "title": "Platform based design for automotive sensor conditioning", "abstract": "A general architecture suitable for interfacing several kinds of sensors for automotive applications is presented. A platform based design approach is pursued to improve system performance while minimizing time-to-market. The platform is composed of an analog front-end and a digital section. The latter is based on a microcontroller core (8051 IP by Oregano) plus a set of hardware dedicated to the complex signal processing required for sensor conditioning. The microcontroller also handles the communication with external devices (such as a PC) for data output and fast prototyping. A case study is presented concerning the conditioning of a gyro yaw rate sensor for automotive applications. Measured performance results outperform current state-of-the-art commercial devices.", "venue": "Design, Automation and Test in Europe", "authors": ["Luca  Fanucci", "Adolfo  Giambastiani", "Francesco  Iozzi", "Corrado  Marino", "Alessandro  Rocchi"], "year": 2005, "n_citations": 14}
{"id": 4210584, "s2_id": "722e82bcf4e91b28500663958988e92024bddb27", "title": "Data Cache Prefetching with Perceptron Learning", "abstract": "Cache prefetcher greatly eliminates compulsory cache misses, by fetching data from slower memory to faster cache before it is actually required by processors. Sophisticated prefetchers predict next use cache line by repeating program's historical spatial and temporal memory access pattern. However, they are error prone and the mis-predictions lead to cache pollution and exert extra pressure on memory subsystem. In this paper, a novel scheme of data cache prefetching with perceptron learning is proposed. The key idea is a two-level prefetching mechanism. A primary decision is made by utilizing previous table-based prefetching mechanism, e.g. stride prefetching or Markov prefetching, and then, a neural network, perceptron is taken to detect and trace program memory access patterns, to help reject those unnecessary prefetching decisions. The perceptron can learn from both local and global history in time and space, and can be easily implemented by hardware. This mechanism boost execution performance by ideally mitigating cache pollution and eliminating redundant memory request issued by prefetcher. Detailed evaluation and analysis were conducted based on SPEC CPU 2006 benchmarks. The simulation results show that generally the proposed scheme yields a geometric mean of 60.64%-83.84% decrease in prefetching memory requests without loss in instruction per cycle(IPC)(floating between -2.22% and 2.55%) and cache hit rate(floating between -1.67% and 2.46%). Though it is possible that perceptron may refuse useful blocks and thus cause minor raise in cache miss rate, lower memory request count can decrease average memory access latency, which compensate for the loss, and in the meantime, enhance overall performance in multi-programmed workloads.", "venue": "ArXiv", "authors": ["Haoyuan  Wang", "Zhiwei  Luo"], "year": 2017, "n_citations": 7}
{"id": 4210797, "s2_id": "20941b4ca2fb99512bdceeec3f038921b2dff293", "title": "High Level Synthesis with a Dataflow Architectural Template", "abstract": "In this work, we present a new approach to high level synthesis (HLS), where high level functions are first mapped to an architectural template, before hardware synthesis is performed. As FPGA platforms are especially suitable for implementing streaming processing pipelines, we perform transformations on conventional high level programs where they are turned into multi-stage dataflow engines [1]. This target template naturally overlaps slow memory data accesses with computations and therefore has much better tolerance towards memory subsystem latency. Using a state-of-the-art HLS tool for the actual circuit generation, we observe up to 9x improvement in overall performance when the dataflow architectural template is used as an intermediate compilation target.", "venue": "ArXiv", "authors": ["Shaoyi  Cheng", "John  Wawrzynek"], "year": 2016, "n_citations": 8}
{"id": 4210817, "s2_id": "58b16b87d121a0c9f65d9752d3ce1fae4e40ef3b", "title": "A Secure and Persistent Memory System for Non-volatile Memory", "abstract": "In the non-volatile memory, ensuring the security and correctness of persistent data is fundamental. However, the security and persistence issues are usually studied independently in existing work. To achieve both data security and persistence, simply combining existing persistence schemes with memory encryption is inefficient due to crash inconsistency and significant performance degradation. To bridge the gap between security and persistence, this paper proposes SecPM, a Secure and Persistent Memory system, which consists of a counter cache write-through (CWT) scheme and a locality-aware counter write reduction (CWR) scheme. Specifically, SecPM leverages the CWT scheme to guarantee the crash consistency via ensuring both the data and its counter are durable before the data flush completes, and leverages the CWR scheme to improve the system performance via exploiting the spatial locality of counter storage, log and data writes. We have implemented SecPM in gem5 with NVMain and evaluated it using five widely-used workloads. Extensive experimental results demonstrate that SecPM reduces up to half of write requests and speeds up the transaction execution by 1.3-2.0 times via using the CWR scheme, and achieves the performance close to an un-encrypted persistent memory system for large transactions.", "venue": "ArXiv", "authors": ["Pengfei  Zuo", "Yu  Hua", "Yuan  Xie"], "year": 2019, "n_citations": 1}
{"id": 4212527, "s2_id": "7854ceaa5a93ed7f0962da03b852a81f688fb98c", "title": "NTX: An Energy-efficient Streaming Accelerator for Floating-point Generalized Reduction Workloads in 22 nm FD-SOI", "abstract": "Specialized coprocessors for Multiply-Accumulate (MAC) intensive workloads such as Deep Learning are becoming widespread in SoC platforms, from GPUs to mobile SoCs. In this paper we revisit NTX (an efficient accelerator developed for training Deep Neural Networks at scale) as a generalized MAC and reduction streaming engine. The architecture consists of a set of 32 bit floating-point streaming co-processors that are loosely coupled to a RISC-V core in charge of orchestrating data movement and computation. Post-layout results of a recent silicon implementation in 22 nm FD-SOI technology show the accelerator\u2019s capability to deliver up to 20 Gflop/s at 1.25 GHz and 168 mW. Based on these results we show that a version of NTX scaled down to 14 nm can achieve a 3\u00d7 energy efficiency improvement over contemporary GPUs at 10.4\u00d7 less silicon area, and a compute performance of 1.4 Tflop/s for training large state-of-the-art networks with full floating-point precision. An extended evaluation of MAC-intensive kernels shows that NTX can consistently achieve up to 87% of its peak performance across general reduction workloads beyond machine learning. Its modular architecture enables deployment at different scales ranging from high-performance GPU-class to low-power embedded scenarios.", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Fabian  Schuiki", "Michael  Schaffner", "Luca  Benini"], "year": 2019, "n_citations": 3}
{"id": 4215802, "s2_id": "28e9961a7221ce75addcfedb3981b0044611ba70", "title": "De-specializing an HLS library for Deep Neural Networks: improvements upon hls4ml", "abstract": "Custom hardware accelerators for Deep Neural Networks are increasingly popular: in fact, the flexibility and performance offered by FPGAs are well-suited to the computational effort and low latency constraints required by many image recognition and natural language processing tasks. The gap between high-level Machine Learning frameworks (e.g., Tensorflow, Pytorch) and low-level hardware design in Verilog/VHDL creates a barrier to widespread adoption of FPGAs, which can be overcome with the help of High-Level Synthesis. hls4ml is a framework that translates Deep Neural Networks into annotated C++ code for High-Level Synthesis, offering a complete and userfriendly design process that has been enthusiastically adopted in physics research. We analyze the strengths and weaknesses of hls4ml, drafting a plan to enhance its core library of components in order to allow more advanced optimizations, target a wider selection of FPGAs, and support larger Neural Network models.", "venue": "ArXiv", "authors": ["Serena  Curzel", "Nicolo  Ghielmetti", "Michele  Fiorito", "Fabrizio  Ferrandi"], "year": 2021, "n_citations": 0}
{"id": 4216038, "s2_id": "4854de2126b4ec6d897c21b90a36cd11a9962e7a", "title": "Enabling Cross-Domain Communication: How to Bridge the Gap between AI and HW Engineers", "abstract": "A key issue in system design is the lack of communication between hardware, software and domain expert. Recent research work shows progress in automatic HW/SW co-design flows of neural accelerators that seems to make this kind of communication obsolete. Most real-world systems, however, are a composition of multiple processing units, communication networks and memories. A HW/SW co-design process of (reconfigurable) neural accelerators, therefore, is an important sub-problem towards a common co-design methodology. The ultimate challenge is to define the constraints for the design space exploration on system level a task which requires deep knowledge and understanding of hardware architectures, mapping of workloads onto hardware and the application domain, e.g. artificial intelligence. For most projects, these skills are distributed among several people or even different teams which is one of the major reasons why there is no established end-to-end development methodology for digital systems. This position paper discusses possibilities how to establish such a methodology for systems that include (reconfigurable) dedicated accelerators and outlines the central role that languages and tools play in the process.", "venue": "ArXiv", "authors": ["Michael J. Klaiber", "Axel J. Acosta", "Ingo  Feldner", "Falk  Rehm"], "year": 2021, "n_citations": 1}
{"id": 4225665, "s2_id": "e9e9db94967f93b93065c1769d8f5befa4e3feea", "title": "Substream-Centric Maximum Matchings on FPGA", "abstract": "Developing high-performance and energy-efficient algorithms for maximum matchings is becoming increasingly important in social network analysis, computational sciences, scheduling, and others. In this work, we propose the first maximum matching algorithm designed for FPGAs; it is energy-efficient and has provable guarantees on accuracy, performance, and storage utilization. To achieve this, we forego popular graph processing paradigms, such as vertex-centric programming, that often entail large communication costs. Instead, we propose a substream-centric approach, in which the input stream of data is divided into substreams processed independently to enable more parallelism while lowering communication costs. We base our work on the theory of streaming graph algorithms and analyze 14 models and 28 algorithms. We use this analysis to provide theoretical underpinning that matches the physical constraints of FPGA platforms. Our algorithm delivers high performance (more than 4x speedup over tuned parallel CPU variants), low memory, high accuracy, and effective usage of FPGA resources. The substream-centric approach could easily be extended to other algorithms to offer low-power and high-performance graph processing on FPGAs.", "venue": "FPGA", "authors": ["Maciej  Besta", "Marc  Fischer", "Tal  Ben-Nun", "Johannes de Fine Licht", "Torsten  Hoefler"], "year": 2019, "n_citations": 23}
{"id": 4226979, "s2_id": "5d63de226f46d144c21faf2960ac802a0f10e4ac", "title": "ROMANet: Fine-Grained Reuse-Driven Off-Chip Memory Access Management and Data Organization for Deep Neural Network Accelerators", "abstract": "Enabling high energy efficiency is crucial for embedded implementations of deep learning. Several studies have shown that the DRAM-based off-chip memory accesses are one of the most energy-consuming operations in deep neural network (DNN) accelerators and, thereby, limit the designs from achieving efficiency gains at the full potential. DRAM access energy varies depending upon the number of accesses required and the energy consumed per-access. Therefore, searching for a solution toward the minimum DRAM access energy is an important optimization problem. Toward this, we propose the ROMANet methodology that aims at reducing the number of memory accesses, by searching for the appropriate data partitioning and scheduling for each layer of a network using a design space exploration, based on the knowledge of the available on-chip memory and the data reuse factors. Moreover, ROMANet also targets decreasing the number of DRAM row buffer conflicts and misses by exploiting the DRAM multibank burst feature to improve the energy-per-access. Besides providing the energy benefits, our proposed DRAM data mapping also results in an increased effective DRAM throughput, which is useful for latency-constraint scenarios. Our experimental results show that the ROMANet saves DRAM access energy by 12% for the AlexNet, 36% for the VGG-16, 46% for the MobileNet, and 45% for the SqueezeNet while improving the DRAM throughput by 10% on average across different networks compared to the state of the art, i.e., bus-width aware (BWA) technique.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad Abdullah Hanif", "Muhammad  Shafique"], "year": 2021, "n_citations": 3}
{"id": 4229276, "s2_id": "153bb2df7a70662c1435de721dee660bf78c0498", "title": "Efficient On-Chip Communication for Parallel Graph-Analytics on Spatial Architectures", "abstract": "Large-scale graph processing has drawn great attention in recent years. Most of the modern-day datacenter workloads can be represented in the form of Graph Processing such as MapReduce etc. Consequently, a lot of designs for Domain-Specific Accelerators have been proposed for Graph Processing. Spatial Architectures have been promising in the execution of Graph Processing, where the graph is partitioned on several nodes and each node works in parallel. We conduct experiments to analyze the on-chip movement of data in graph processing on a Spatial Architecture. Based on the observations, we identify a data movement bottleneck, in the execution of such highly-parallel processing accelerators. To mitigate the bottleneck we propose a novel power-law aware Graph Partitioning and Data Mapping scheme to reduce the communication latency by minimizing the hop counts on a scalable network-on-chip. The experimental results on popular graph algorithms show that our implementation makes the execution 2 \u2212 5\u00d7 faster and 2.7 \u2212 4\u00d7 energy-efficient by reducing the data movement time in comparison to a baseline implementation.", "venue": "ArXiv", "authors": ["Khushal  Sethi"], "year": 2021, "n_citations": 0}
{"id": 4229462, "s2_id": "ceab6044c120555dacb8900e8c2d88b399a74378", "title": "Algorithmic Obfuscation for LDPC Decoders", "abstract": "In order to protect intellectual property against untrusted foundry, many logic-locking schemes have been developed. The main idea of logic locking is to insert a key-controlled block into a circuit to make the circuit function incorrectly without right keys. However, in the case that the algorithm implemented by the circuit is naturally fault-tolerant or selfcorrecting, existing logic-locking schemes do not affect the system performance much even if wrong keys are used. One example is low-density parity-check (LDPC) error-correcting decoder, which has broad applications in digital communications and storage. This paper proposes two algorithmic-level obfuscation methods for LDPC decoders. By modifying the decoding process and locking the stopping criterion, our new designs substantially degrade the decoder throughput and/or error-correcting performance when the wrong key is used. Besides, our designs are also resistant to the SAT, AppSAT and removal attacks. For an example LDPC decoder, our proposed methods reduce the throughput to less than 1/3 and/or increase the decoder error rate by at least two orders of magnitude with only 0.33% area overhead.", "venue": "ArXiv", "authors": ["Jingbo  Zhou", "Xinmiao  Zhang"], "year": 2021, "n_citations": 0}
{"id": 4232706, "s2_id": "fcb1ec8c3d205793dd5118b95b67c57afc339cdf", "title": "Reuse Cache for Heterogeneous CPU-GPU Systems", "abstract": "It is generally observed that the fraction of live lines in shared last-level caches (SLLC) is very small for chip multiprocessors (CMPs). This can be tackled using promotion-based replacement policies like re-reference interval prediction (RRIP) instead of LRU [3], dead-block predictors [4], or reuse-based cache allocation schemes [5]. In GPU systems, similar LLC issues are alleviated using various cache bypassing techniques. These issues are worsened in heterogeneous CPU-GPU systems because the two processors have different data access patterns and frequencies. GPUs generally work on streaming data, but have many more threads accessing memory as compared to CPUs. As such, most traditional cache replacement and allocation policies prove ineffective due to the higher number of cache accesses in GPU applications, resulting in higher allocation for GPU cache lines, despite their minimal reuse. In this work, we implement \u201cThe Reuse Cache\u201d approach [5] for heterogeneous CPU-GPU systems. The reuse cache is a decoupled tag/data SLLC which is designed to only store the data that is being accessed more than once. This design is based on the observation that most of the cache lines in the LLC are stored but do not get reused before being replaced. We find that the reuse cache achieves within 0.8% of the IPC gains of a statically partitioned LLC, while decreasing the area cost of the LLC by an average of 40%.", "venue": "ArXiv", "authors": ["Tejas  Shah", "Bobbi  Yogatama", "Kyle  Roarty", "Rami  Dahman"], "year": 2021, "n_citations": 0}
{"id": 4236753, "s2_id": "27aabf475994254dd14a641ce789a6fb990af5fd", "title": "Improving 3D NAND Flash Memory Lifetime by Tolerating Early Retention Loss and Process Variation", "abstract": "Compared to planar NAND flash memory, 3D NAND flash memory uses a new flash cell design, and vertically stacks dozens of silicon layers in a single chip. This allows 3D NAND flash memory to increase storage density using a much less aggressive manufacturing process technology than planar NAND. The circuit-level and structural changes in 3D NAND flash memory significantly alter how different error sources affect the reliability of the memory. Our goal is to (1)~identify and understand these new error characteristics of 3D NAND flash memory, and (2)~develop new techniques to mitigate prevailing 3D NAND flash errors. \\chIIIn this paper, we perform a rigorous experimental characterization of real, state-of-the-art 3D NAND flash memory chips, and identify three new error characteristics that were not previously observed in planar NAND flash memory, but are fundamental to the new architecture of 3D NAND flash memory. \\beginenumerate [leftmargin=13pt] \u0131tem 3D NAND flash memory exhibits layer-to-layer process variation, a new phenomenon specific to the 3D nature of the device, where the average error rate of each 3D-stacked layer in a chip is significantly different. We are the first to provide detailed experimental characterization results of layer-to-layer process variation in real flash devices in open literature. Our results show that the raw bit error rate in the middle layer can be 6\u00d7 the error rate in the top layer. \u0131tem 3D NAND flash memory experiences \\emphearly retention loss, a new phenomenon where the number of errors due to charge leakage increases quickly within several hours after programming, but then increases at a much slower rate. We are the first to perform an extended-duration observation of early retention loss over the course of 24~days. Our results show that the retention error rate in a 3D NAND flash memory block quickly increases by an order of magnitude within $\\sim$3 hours after programming. \u0131tem 3D NAND flash memory experiences retention interference, a new phenomenon where the rate at which charge leaks from a flash cell is dependent on the amount of charge stored in neighboring flash cells. Our results show that charge leaks at a lower rate (i.e., the retention loss speed is slower) when the neighboring cell is in a state that holds more charge (i.e., a higher-voltage state). \\endenumerate Our experimental observations indicate that we must revisit the error models and error mitigation mechanisms devised for planar NAND flash, as they are no longer accurate for 3D NAND flash behavior. To this end, we develop \\emphnew analytical model\\chIs of (1)~the layer-to-layer process variation in 3D NAND flash memory, and (2)~retention loss in 3D NAND flash memory. Our models estimate the raw bit error rate (RBER), threshold voltage distribution, and the \\emphoptimal read reference voltage (i.e., the voltage at which RBER is minimized when applied during a read operation) for each flash page. Both models are useful for developing techniques to mitigate raw bit errors in 3D NAND flash memory. Motivated by our new findings and models, we develop four new techniques to mitigate process variation and early retention loss in 3D NAND flash memory. Our first technique, LaVAR, reduces process variation by fine-tuning the read reference voltage independently for each layer. Our second technique, LI-RAID, improves reliability by changing how pages are grouped under the RAID (Redundant Array of Independent Disks) error recovery technique, using information about layer-to-layer process variation to reduce the likelihood that the RAID recovery of a group could fail significantly earlier during the flash lifetime than recovery of other groups. Our third technique, ReMAR, reduces retention errors in 3D NAND flash memory by tracking the retention age of the data using our retention model and adapting the read reference voltage to data age. Our fourth technique, ReNAC, adapts the read reference voltage to the amount of retention interference to re-read the data after a read operation fails. These four techniques are complementary, and can be combined together to significantly improve flash memory reliability. Compared to a state-of-the-art baseline, our techniques, when combined, improve flash memory lifetime by 1.85\u00d7. Alternatively, if a NAND flash manufacturer wants to keep the lifetime of the 3D NAND flash memory device constant, our techniques reduce the storage overhead required to hold error correction information by 78.9%. For more information on our new experimental characterization of modern 3D NAND flash memory chips and our proposed models and techniques, please refer to the full version of our paper~\\citeluo.pomacs18.", "venue": "SIGMETRICS", "authors": ["Yixin  Luo", "Saugata  Ghose", "Yu  Cai", "Erich F. Haratsch", "Onur  Mutlu"], "year": 2018, "n_citations": 0}
{"id": 4237617, "s2_id": "5be5286ccaf8bb02172ba8b27dacf345aa08cd7d", "title": "Correlation Differential Power Analysis Attack to Midori64", "abstract": "Article History: Received Revised Accepted Background and Objectives: Today, Internet communication security has become more complex as technology becomes faster and more efficient, especially for resource-limited devices such as embedded devices, wireless sensors, and radio frequency identification (RFID) tags, and Internet of Things (IoT). Lightweight encryption algorithms provide security for these devices to protect data against intruders. A complete understanding Lightweight encryption helps people find better ways to protect valuable information as technology advances faster. But the limitation of using energy in lightweight block ciphers (LBCs) is one of the major challenges for ever-expanding IoT technologies. Also, these LBC are subject to numerous attacks. Side-channel attacks are among the most cited threats to these ciphers. In these attacks, the attacker benefits from a physical leak of the cryptographic chip to reach the cryptographic key. A type of side-channel attack is power analysis in which the attacker reaches the key using the relationship between the power consumption of the chip during the algorithm running, data processing, and operations. Methods: A popular LBC that has been recently introduced in the IoT is Midori. In this paper, a differential power attack (DPA) to the Midori64 block cipher is designed. According to the proposed method, an attack on the Sboxes of the first round is done to obtain half of the master key bits. Then, the S-boxes of the second round were attacked to obtain remaining the master key bits. Results: The results confirmed that the key is ultimately obtained in this cipher by performing an attack, so the block cipher is not resistant to a DPA. With the low volume of computational complexity, we obtained the Midori block cipher key, which was considered secure, just by using 300 samples of the plaintext. Conclusion: Following the running of Midori64 on the AVR microcontroller of the Atmega32 model, the master key of Midori block cipher is discovered with 300 known texts. Furthermore, we obtained the master key with a smaller number of samples than the electromagnetic analysis attack.", "venue": "ArXiv", "authors": ["Behrooz  Khadem", "Hamid  Ghanbari", "Amin Masoumi souteh"], "year": 2021, "n_citations": 0}
{"id": 4239411, "s2_id": "a31775de9902ee19ae424b94faccc05c447e60e7", "title": "High-Performance FPGA-based Accelerator for Bayesian Neural Networks", "abstract": "Neural networks (NNs) have demonstrated their potential in a wide range of applications such as image recognition, decision making or recommendation systems. However, standard NNs are unable to capture their model uncertainty which is crucial for many safety-critical applications including healthcare and autonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to express uncertainty in their prediction via a mathematical grounding. Nevertheless, BNNs have not been as widely used in industrial practice, mainly because of their expensive computational cost and limited hardware performance. This work proposes a novel FPGA based hardware architecture to accelerate BNNs inferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN accelerators, the proposed accelerator can achieve up to 4 times higher energy efficiency and 9 times better compute efficiency. Considering partial Bayesian inference, an automatic framework is proposed, which explores the trade-off between hardware and algorithmic performance. Extensive experiments are conducted to demonstrate that our proposed framework can effectively find the optimal points in the design space.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Hongxiang  Fan", "Martin  Ferianc", "Miguel  Rodrigues", "Hongyu  Zhou", "Xinyu  Niu", "Wayne  Luk"], "year": 2021, "n_citations": 1}
{"id": 4243181, "s2_id": "942f95ba3f514469942418cc7c712c3d4df6e9a1", "title": "The Promise of Dataflow Architectures in the Design of Processing Systems for Autonomous Machines", "abstract": "The commercialization of autonomous machines is a thriving sector, and likely to be the next major computing demand driver, after PC, cloud computing, and mobile computing. Nevertheless, a suitable computer architecture for autonomous machines is missing, and many companies are forced to develop ad hoc computing solutions that are neither scalable nor extensible. In this article, we analyze the demands of autonomous machine computing, and argue for the promise of dataflow architectures in autonomous machines. 1 Rise of the Autonomous Machines The commercialization of autonomous machines is a thriving sector, with projected average compound annual growth rate (CAGR) of 26%, and by 2030 this sector will have a market size of $1 trillion [1]. Hence, this sector is likely to be the next major computing demand driver, after personal computers, cloud computing, and mobile computing. Autonomous machines exist in multiple forms, e.g., cars, aerial drones, service robots, industrial robots. Different kinds of autonomous machines are quite diverse in size, shape, mission, goals, location, propulsion, etc. [2] Generally, completely independent teams have approached the design, resulting in a bevy of solutions, some replications but certainly no standardization. A better understanding of the underlying issues, a formalization of the common problems, and a certain unification of the solutions would all yield a more efficient approach to the design process. The core of autonomous machines obviously resides in their computing systems, and encompasses both the hardware and the software level, entailing algorithms, systems software, compilers, as well as computer architectures. Despite the recent advancements in autonomous machine systems design of such major industrial organizations as Google [3], Tesla [4], Mobileye [5], Nvidia [6], the architecture of autonomous machine systems still largely remains an open research question since existing solutions are often made on an ad hoc basis, and not only the development process takes a long time, but the design itself is neither scalable nor extensible [7]. In this article, we first review the advantages of dataflow architectures and their implementation difficulties. Then we summarize the observations of autonomous machine computing, and delve into the details of why dataflow architectures may be extremely well adapted for autonomous machine computing. 2 Dataflow Architectures Before delving into the details of autonomous machines computing, let us first review the benefits of dataflow architectures and their implementation roadblocks. 1 ar X iv :2 10 9. 07 04 7v 1 [ cs .A R ] 1 5 Se p 20 21 Dataflow concepts originated in the 1970s and 1980s, with pioneering work by Jack Dennis and Arvind, and several others [8, 9]. The central idea of dataflow architectures was to replace the classic control flow, or von Neumann, architectures. In a von Neumann architecture, the processor follows an explicit control flow, executing instructions one after another. In a dataflow architecture, execution is event-driven such that an instruction is ready to execute as soon as all its inputs, or \u201ctokens,\u201d are available, rather than when the control flow gets to it. To bridge the gap between traditional architectures and the dataflow computing model, Gao et al. have developed the codelet execution model that incorporates the advantages of macro-dataflow and von Neumann model [10, 11]. The codelet execution model can be used to describe programs in massive parallel systems. Specifically, the observations of autonomous machine computing, as we summarize in section 3, reveal that when the programming model renders the appropriate level of abstraction, a hybrid dataflow and domain-specific accelerator (DSA) architecture is extremely well adapted for autonomous machine applications. Classic dataflow architecture, by representing a program as a dataflow graph, can naturally explore the instruction-level parallelism in a program by firing instructions as soon as their operands are ready, hence minimizing the control overheads. Despite the implementation difficulties of the pure dataflow concepts, such as the cost/benefit problem [12], the merit of dataflow graph representation and data driven execution has led to the emergence of superscalar architectures (using restricted dataflow graph to exploit instruction level parallelism), and multiple grid architectures (hybrids of control-flow and dataflow) [13, 14], which map dataflow graphs onto grid processors and execute operations in a dataflow fashion to enable concurrent execution. In addition to the general purpose processors, the dataflow architecture\u2019s properties of decentralized control and data-driven execution have been successfully used to synthesize power efficient application specific hardware [15]. In more detail, the original dataflow architecture proposals specify that dataflow machines are fine-grain parallel computers, where the processes are about the size of a single instruction in a conventional computer. Instructions are known as nodes, and the data passed between different nodes are called tokens. A producing node is connected to a consuming node by an arc, and the point where an arc enters a node is called an input port. The execution of an instruction is called the firing of a node. Execution of a node only occurs if the node is enabled when each input port contains a token. Under the dataflow execution model, there is no such thing as control flow and the problem of synchronizing data and control flow has disappeared, making dataflow programs well suited for parallel processing. In a dataflow graph, the arcs between the instructions directly reflect the partial ordering imposed by their data dependencies [16]. The dataflow execution model is remarkably powerful and, in recent years, has been widely used in cloud and distributed computing [17]. Hardware architectures based on the dataflow execution model, however, have not had equal amount of success, primarily for four reasons: \u2022 Insufficient Amount of Parallelism: At the instruction level, many conventional programs do not have enough intrinsic parallelisms to utilize a realistic dataflow hardware except when processing large arrays. The lack of ILP in conventional programs begs the question: is instruction the right level of abstraction for dataflow architectures? \u2022 Explosion of Parallelism: Conversely, the very lack of central control, so central to allowing parallelism may lead to an uncontrolled (and uncontrollable) demand for parallel resources, leading to deadlocks. \u2022 Producer-Consumer Speed Mismatch: When the speed between the producer and consumer nodes are mismatched, either the producer (or consumer) will have to stall, leading to", "venue": "ArXiv", "authors": ["Shaoshan  Liu", "Yuhao  Zhu", "Bo  Yu", "Jean-Luc  Gaudiot", "Guang R. Gao"], "year": 2021, "n_citations": 0}
{"id": 4252214, "s2_id": "ff535cc8d9735f65fffaac0c0e78325a91102a7f", "title": "Demystifying memory access patterns of FPGA-based graph processing accelerators", "abstract": "Recent advances in reprogrammable hardware (e. g., FPGAs) and memory technology (e. g., DDR4, HBM) promise to solve performance problems inherent to graph processing like irregular memory access patterns on traditional hardware (e. g., CPU). While several of these graph accelerators were proposed in recent years, it remains difficult to assess their performance and compare them on common graph workloads and accelerator platforms, due to few open source implementations and excessive implementation effort. In this work, we build on a simulation environment for graph processing accelerators, to make several existing accelerator approaches comparable. This allows us to study relevant performance dimensions such as partitioning schemes and memory technology, among others. The evaluation yields insights into the strengths and weaknesses of current graph processing accelerators along these dimensions, and features a novel in-depth comparison.", "venue": "GRADES-NDA@SIGMOD", "authors": ["Jonas  Dann", "Daniel  Ritter", "Holger  Fr\u00f6ning"], "year": 2021, "n_citations": 0}
{"id": 4262500, "s2_id": "aea370f725635c48a8997deb3788db351d907a58", "title": "NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration with Spiking Neural Networks", "abstract": "Recently, both industry and academia have proposed many different neuromorphic architectures to execute applications that are designed with Spiking Neural Network (SNN). Consequently, there is a growing need for an extensible simulation framework that can perform architectural explorations with SNNs, including both platform-based design of today\u2019s hardware, and hardware-software co-design and design-technology co-optimization of the future. We present NeuroXplorer, a fast and extensible framework that is based on a generalized template for modeling a neuromorphic architecture that can be infused with the specific details of a given hardware and/or technology. NeuroXplorer can perform both low-level cycle-accurate architectural simulations and high-level analysis with data-flow abstractions. NeuroXplorer\u2019s optimization engine can incorporate hardware-oriented metrics such as energy, throughput, and latency, as well as SNN-oriented metrics such as inter-spike interval distortion and spike disorder, which directly impact SNN performance. We demonstrate the architectural exploration capabilities of NeuroXplorer through case studies with many state-of-the-art machine learning models.", "venue": "ICONS", "authors": ["Adarsha  Balaji", "Shihao  Song", "Twisha  Titirsha", "Anup  Das", "Jeffrey  Krichmar", "Nikil  Dutt", "James  Shackleford", "Nagarajan  Kandasamy", "Francky  Catthoor"], "year": 2021, "n_citations": 7}
{"id": 4264571, "s2_id": "6d34e5a5e109392954566c679249059477b22538", "title": "Hardware Implementation of an OPC UA Server for Industrial Field Devices", "abstract": "Industrial plants suffer from a high degree of complexity and incompatibility in their communication infrastructure, caused by a wild mix of proprietary technologies. This prevents transformation toward Industry 4.0 and the Industrial Internet of Things. Open platform communications unified architecture (OPC UA) is a standardized protocol that addresses these problems with uniform and semantic communication across all levels of the hierarchy. However, its adoption in embedded field devices, such as sensors and actuators, is still lacking due to prohibitive memory and power requirements of software implementations. We have developed a dedicated hardware engine that offloads processing of the OPC UA protocol and enables the realization of compact and low-power field devices with OPC UA support. As part of a proof-of-concept embedded system, we have implemented this engine in a 22-nm FDSOI technology, representing the first ASIC implementation of an OPC UA server. We measured performance, power consumption, and memory footprint of our test chip and compared it with a software implementation based on open62541 and a Raspberry Pi 2B. Our OPC UA hardware engine is 50 times more energy efficient and only requires 36 KiB of memory. The complete system consumes only 24 mW under full load, making it suitable for low-power embedded applications.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Heiner  Bauer", "Sebastian  H\u00f6ppner", "Chris  Iatrou", "Zohra  Charania", "Stephan  Hartmann", "Saif Ur Rehman", "Andreas  Dixius", "Georg  Ellguth", "Dennis  Walter", "Johannes  Uhlig", "Felix  Neum\u00e4rker", "Marc  Berthel", "Marco  Stolba", "Florian  Kelber", "Leon  Urbas", "Christian  Mayr"], "year": 2021, "n_citations": 0}
{"id": 4266952, "s2_id": "9ea71b7723fa636b1ada1f14322fa9a8e355be18", "title": "Coherence Traffic in Manycore Processors with Opaque Distributed Directories", "abstract": "Manycore processors feature a high number of general-purpose cores designed to work in a multithreaded fashion. Recent manycore processors are kept coherent using scalable distributed directories. A paramount example is the Intel Mesh interconnect, which consists of a network-on-chip interconnecting \"tiles\", each of which contains computation cores, local caches, and coherence masters. The distributed coherence subsystem must be queried for every out-of-tile access, imposing an overhead on memory latency. This paper studies the physical layout of an Intel Knights Landing processor, with a particular focus on the coherence subsystem, and uncovers the pseudo-random mapping function of physical memory blocks across the pieces of the distributed directory. Leveraging this knowledge, candidate optimizations to improve memory latency through the minimization of coherence traffic are studied. Although these optimizations do improve memory throughput, ultimately this does not translate into performance gains due to inherent overheads stemming from the computational complexity of the mapping functions.", "venue": "ArXiv", "authors": ["Steve  Kommrusch", "Marcos  Horro", "Louis-Noel  Pouchet", "Gabriel  Rodr'iguez", "Juan  Tourino"], "year": 2020, "n_citations": 0}
{"id": 4267248, "s2_id": "46c462c389080d0eec5df105af5817e856bea89c", "title": "WoLFRaM: Enhancing Wear-Leveling and Fault Tolerance in Resistive Memories using Programmable Address Decoders", "abstract": "Resistive memories have limited lifetime caused by limited write endurance and highly non-uniform write access patterns. Two main techniques to mitigate endurance-related memory failures are 1) wear-leveling, to evenly distribute the writes across the entire memory, and 2) fault tolerance, to correct memory cell failures. However, one of the main open challenges in extending the lifetime of existing resistive memories is to make both techniques work together seamlessly and efficiently. To address this challenge, we propose WoLFRaM, a new mechanism that combines both wear-leveling and fault tolerance techniques at low cost by using a programmable resistive address decoder (PRAD). The key idea of WoLFRaM is to use PRAD for implementing 1) a new efficient wear-leveling mechanism that remaps write accesses to random physical locations on the fly, and 2) a new effiCient fault tolerance mechanism that recovers from faults by remapping failed memory blocks to available physical locations. Our evaluations show that, for a Phase Change Memory (PCM) based system with cell endurance of 108 writes, WoLFRaM increases the memory lifetime by 68% compared to a baseline that implements the best state-of-the-art wear-leveling and fault correction mechanisms. WoLFRaM's average / worst-case performance and energy overheads are 0.51% /3.8% and 0.47% /2.1% respectively.", "venue": "2020 IEEE 38th International Conference on Computer Design (ICCD)", "authors": ["Leonid  Yavits", "Lois  Orosa", "Suyash  Mahar", "Jo\u00e3o Dinis Ferreira", "Mattan  Erez", "Ran  Ginosar", "Onur  Mutlu"], "year": 2020, "n_citations": 3}
{"id": 4268072, "s2_id": "0efe25fc0f52affdb4601613ef0237d8a26f6852", "title": "Design of a Dynamic Parameter-Controlled Chaotic-PRNG in a 65 nm CMOS process", "abstract": "In this paper, we present the design of a new chaotic map circuit with a 65 nm CMOS process. This chaotic map circuit uses a dynamic parameter-control topology and generates a wide chaotic range. We propose two designs of dynamic parameter-controlled chaotic map (DPCCM)-based pseudo-random number generators (PRNG). The randomness of the generated sequence is verified using three different statistical tests, namely, NIST SP 800\u201322 test, FIPS PUB 140\u20132 test, and Diehard test. Our first design offers a throughput of 200 MS/s with an on-chip area of 0.024 mm2 and a power consumption of 2.33 mW. The throughput of our second design is 300 MS/s with an area consumption of 0.132 mm2 and power consumption of 2.14 mW. The wider chaotic range and lower-overhead, offered by our designs, can be highly suitable for various applications such as, logic obfuscation, chaos-based cryptography, re-configurable random number generation, and hard-ware security for resource-constrained edge devices like IoT.", "venue": "2020 IEEE 14th Dallas Circuits and Systems Conference (DCAS)", "authors": ["Partha Sarathi Paul", "Maisha  Sadia", "Md Sakib Hasan"], "year": 2020, "n_citations": 1}
{"id": 4269716, "s2_id": "c2be7e8f73e8cc7b2116f86cac52d304781aba0b", "title": "Parallel Scheduling Self-attention Mechanism: Generalization and Optimization", "abstract": "Over the past few years, self-attention is shining in the field of deep learning, especially in the domain of natural language processing(NLP). Its impressive effectiveness, along with ubiquitous implementations, have aroused our interest in efficiently scheduling the data-flow of corresponding computations onto architectures with many computing units to realize parallel computing. In this paper, based on the theory of self-attention mechanism and state-of-the-art realization of self-attention in language models, we propose a general scheduling algorithm, which is derived from the optimum scheduling for small instances solved by a satisfiability checking(SAT) solver, to parallelize typical computations of self-attention. Strategies for further optimization on skipping redundant computations are put forward as well, with which reductions of almost 25% and 50% of the original computations are respectively achieved for two widely-adopted application schemes of self-attention. With the proposed optimization adopted, we have correspondingly come up with another two scheduling algorithms. The proposed algorithms are applicable regardless of problem sizes, as long as the number of input vectors is divisible to the number of computing units available in the architecture. Due to the complexity of proving the correctness of the algorithms mathematically for general cases, we have conducted experiments to reveal their validity, together with the superior quality of the solutions provided by which, by solving SAT problems for particular instances.", "venue": "ArXiv", "authors": ["Mingfei  Yu", "Masahiro  Fujita"], "year": 2020, "n_citations": 0}
{"id": 4269872, "s2_id": "1a6e68264a1726b867a0dcdafae536a7e1d2af78", "title": "Compiler-driven FPGA virtualization with SYNERGY", "abstract": "FPGAs are increasingly common in modern applications, and cloud providers now support on-demand FPGA acceleration in data centers. Applications in data centers run on virtual infrastructure, where consolidation, multi-tenancy, and workload migration enable economies of scale that are fundamental to the provider\u2019s business. However, a general strategy for virtualizing FPGAs has yet to emerge. While manufacturers struggle with hardware-based approaches, we propose a compiler/runtime-based solution called Synergy. We show a compiler transformation for Verilog programs that produces code able to yield control to software at sub-clock-tick granularity according to the semantics of the original program. Synergy uses this property to efficiently support core virtualization primitives: suspend and resume, program migration, and spatial/temporal multiplexing, on hardware which is available today. We use Synergy to virtualize FPGA workloads across a cluster of Altera SoCs and Xilinx FPGAs on Amazon F1. The workloads require no modification, run within 3\u22124\u00d7 of unvirtualized performance, and incur a modest increase in FPGA fabric utilization.", "venue": "ASPLOS", "authors": ["Joshua  Landgraf", "Tiffany  Yang", "Will  Lin", "Christopher J. Rossbach", "Eric  Schkufza"], "year": 2021, "n_citations": 0}
{"id": 4271241, "s2_id": "2be7b8fe0dae155a1b17e0574d7ce3bd87203399", "title": "SecureD: A Secure Dual Core Embedded Processor", "abstract": "Security of embedded computing systems is becoming of paramount concern as these devices become more ubiquitous, contain personal information and are increasingly used for financial transactions. Security attacks targeting embedded systems illegally gain access to the information in these devices or destroy information. The two most common types of attacks embedded systems encounter are code-injection and power analysis attacks. In the past, a number of countermeasures, both hardware- and software-based, were proposed individually against these two types of attacks. However, no single system exists to counter both of these two prominent attacks in a processor based embedded system. Therefore, this paper, for the first time, proposes a hardware/software based countermeasure against both code-injection attacks and power analysis based side-channel attacks in a dual core embedded system. The proposed processor, named SecureD, has an area overhead of just 3.80% and an average runtime increase of 20.0% when compared to a standard dual processing system. The overhead were measured using a set of industry standard application benchmarks, with two encryption and five other programs.", "venue": "ArXiv", "authors": ["Roshan G. Ragel", "Jude Angelo Ambrose", "Sri  Parameswaran"], "year": 2015, "n_citations": 1}
{"id": 4272040, "s2_id": "36be42bdfc15a124c4d36950381f03d05f19051c", "title": "Associative control processor with a rigid structure", "abstract": "The approach of applying associative processor for decision making problem was proposed. It focuses on hardware implementations of fuzzy processing systems, associativity as effective management basis of fuzzy processor. The structural approach is being developed resulting in a quite simple and compact parallel associative memory unit (PAMU). The memory cost and speed comparison of processors with rigid and soft-variable structure is given. Also the example PAMU flashing is considered.", "venue": "ArXiv", "authors": ["Isa  Magomedov", "Omar  Khazamov"], "year": 2010, "n_citations": 0}
{"id": 4273522, "s2_id": "c4f35cd948f3f3154d80ee64b37538c8a1928970", "title": "A Framework for Accelerating Bottlenecks in GPU Execution with Assist Warps", "abstract": "Modern graphics processing units (GPUs) are well provisioned to support the concurrent execution of thousands of threads. Unfortunately, different bottlenecks during execution and heterogeneous application requirements create imbalances in utilization of resources in the cores. For example, when a GPU is bottlenecked by the available off-chip memory bandwidth, its computational resources are often overwhelmingly idle, waiting for data from memory to arrive.", "venue": "ArXiv", "authors": ["Nandita  Vijaykumar", "Gennady  Pekhimenko", "Adwait  Jog", "Saugata  Ghose", "Abhishek  Bhowmick", "Rachata  Ausavarungnirun", "Chita R. Das", "Mahmut T. Kandemir", "Todd C. Mowry", "Onur  Mutlu"], "year": 2016, "n_citations": 10}
{"id": 4274610, "s2_id": "8d61ad312eed5847c6375a66b16488d19cb58306", "title": "GuardNN: Secure DNN Accelerator for Privacy-Preserving Deep Learning", "abstract": "This paper proposes GuardNN, a secure deep neural network (DNN) accelerator, which provides strong hardware-based protection for user data and model parameters even in an untrusted environment. GuardNN shows that the architecture and protection can be customized for a specific application to provide strong confidentiality and integrity protection with negligible overhead. The design of the GuardNN instruction set reduces the TCB to just the accelerator and enables confidentiality protection without the overhead of integrity protection. GuardNN also introduces a new application-specific memory protection scheme to minimize the overhead of memory encryption and integrity verification. The scheme shows that most of the off-chip meta-data in today's state-of-the-art memory protection can be removed by exploiting the known memory access patterns of a DNN accelerator. GuardNN is implemented as an FPGA prototype, which demonstrates effective protection with less than 2% performance overhead for inference over a variety of modern DNN models.", "venue": "ArXiv", "authors": ["Weizhe  Hua", "Muhammad  Umar", "Zhiru  Zhang", "G. Edward Suh"], "year": 2020, "n_citations": 5}
{"id": 4276559, "s2_id": "cb2fb27edfdf783fb21b348ea5ff37d5542c15ae", "title": "The Distributed Network Processor: a novel off-chip and on-chip interconnection network architecture", "abstract": "One of the most demanding challenges for the designers of parallel computing architectures is to deliver an efficient network infrastructure providing low latency, high bandwidth communications while preserving scalability. Besides off-chip communications between processors, recent multi-tile (i.e. multi-core) architectures face the challenge for an efficient on-chip interconnection network between processor's tiles. In this paper, we present a configurable and scalable architecture, based on our Distributed Network Processor (DNP) IP Library, targeting systems ranging from single MPSoCs to massive HPC platforms. The DNP provides inter-tile services for both on-chip and off-chip communications with a uniform RDMA style API, over a multi-dimensional direct network with a (possibly) hybrid topology.", "venue": "ArXiv", "authors": ["Andrea  Biagioni", "Francesca Lo Cicero", "Alessandro  Lonardo", "Pier Stanislao Paolucci", "Mersia  Perra", "Davide  Rossetti", "Carlo  Sidore", "Francesco  Simula", "Laura  Tosoratto", "Piero  Vicini"], "year": 2012, "n_citations": 8}
{"id": 4283251, "s2_id": "0d13f83f0fcb71a5e6c6f9dc812711efaaa5ff7f", "title": "PowerPlanningDL: Reliability-Aware Framework for On-Chip Power Grid Design using Deep Learning", "abstract": "With the increase in the complexity of chip designs, VLSI physical design has become a time-consuming task, which is an iterative design process. Power planning is that part of the floorplanning in VLSI physical design where power grid networks are designed in order to provide adequate power to all the underlying functional blocks. Power planning also requires multiple iterative steps to create the power grid network while satisfying the allowed worst-case IR drop and Electromigration (EM) margin. For the first time, this paper introduces Deep learning (DL)-based framework to approximately predict the initial design of the power grid network, considering different reliability constraints. The proposed framework reduces many iterative design steps and speeds up the total design cycle. Neural Network-based multi-target regression technique is used to create the DL model. Feature extraction is done, and training dataset is generated from the floorplans of some of the power grid designs extracted from IBM processor. The DL model is trained using the generated dataset. The proposed DL-based framework is validated using a new set of power grid specifications (obtained by perturbing the designs used in the training phase). The results show that the predicted power grid design is closer to the original design with minimal prediction error (~2%). The proposed DL- based approach also improves the design cycle time with a speedup of ~6x for standard power grid benchmarks.", "venue": "2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Sukanta  Dey", "Sukumar  Nandi", "Gaurav  Trivedi"], "year": 2020, "n_citations": 4}
{"id": 4284623, "s2_id": "e799f8ddffa0a645e5a80e9a681d7f2b38603a33", "title": "A 1.2-V 162.9-pJ/cycle Bitmap Index Creation Core with 0.31-pW/bit Standby Power on 65-nm SOTB", "abstract": "Abstract Maximizing the performance during peak workload hours and minimizing the power consumption during off-peak time plays a significant role in the energy-efficient systems. Our previous work has proposed an efficient architecture of a bitmap index creator (BIC) that produced higher indexing throughput than the central processing units and graphics processing units. This paper extends the previous study by focusing on the ASIC implementation of BIC in a 65-nm silicon-on-thin-buried-oxide (SOTB) CMOS process. The fabricated chip could operate at different supply voltages, from 0.4\u202fV to 1.2\u202fV. In the active mode with the supply voltage of 1.2\u202fV, it was fully operational at 41\u202fMHz and consumed 6.68\u202fmW, or 162.9\u00a0pJ/cycle. In the standby mode with the supply voltage of 0.4\u202fV and clock gated, the power consumption lowered to 10.6\u00a0\u03bcW. More significantly, when the reverse back-gate bias voltages are supplied, the standby power deeply reduced to 2.64\u00a0nW. This achievement is of considerable importance to the energy-efficient systems.", "venue": "Microprocess. Microsystems", "authors": ["Xuan-Thuan  Nguyen", "Trong-Thuc  Hoang", "Hong-Thu  Nguyen", "Katsumi  Inoue", "Cong-Kha  Pham"], "year": 2019, "n_citations": 1}
{"id": 4285999, "s2_id": "fd05626fb93efbe9d5a8fb5effae1ae7e858d2c1", "title": "Mobile Machine Learning Hardware at ARM: A Systems-on-Chip (SoC) Perspective", "abstract": "Machine learning is playing an increasingly significant role in emerging mobile application domains such as AR/VR, ADAS, etc. Accordingly, hardware architects have designed customized hardware for machine learning algorithms, especially neural networks, to improve compute efficiency. However, machine learning is typically just one processing stage in complex end-to-end applications, which involve multiple components in a mobile Systems-on-a-chip (SoC). Focusing on just ML accelerators loses bigger optimization opportunity at the system (SoC) level. This paper argues that hardware architects should expand the optimization scope to the entire SoC. We demonstrate one particular case-study in the domain of continuous computer vision where camera sensor, image signal processor (ISP), memory, and NN accelerator are synergistically co-designed to achieve optimal system-level efficiency.", "venue": "ArXiv", "authors": ["Yuhao  Zhu", "Matthew  Mattina", "Paul N. Whatmough"], "year": 2018, "n_citations": 17}
{"id": 4286201, "s2_id": "01b4a073b769992ae395a7aec9402f80acc74daa", "title": "ForASec: Formal Analysis of Security Vulnerabilities in Sequential Circuits", "abstract": "Security vulnerability analysis of Integrated Circuits using conventional design-time validation and verification techniques (like simulations, emulations, etc.) is generally a computationally intensive task and incomplete by nature, especially under limited resources and time constraints. To overcome this limitation, we propose a novel methodology based on model checking to formally analyze security vulnerabilities in sequential circuits while considering side-channel parameters like propagation delay, switching power, and leakage power. In particular, we present a novel algorithm to efficiently partition the state-space into corresponding smaller state-spaces to enable distributed security analysis of complex sequential circuits and thereby mitigating the associated state-space explosion due to their feedback loops. We analyze multiple ISCAS89 and trust-hub benchmarks to demonstrate the efficacy of our framework in identifying security vulnerabilities. The experimental results show that ForASec successfully performs the complete analysis of the given complex and large sequential circuits, and provides approximately 11x to 16x speedup in analysis time compared to state-of-the-art model checking-based techniques. Moreover, it also identifies the number of gates required by an HT that can go undetected for a given design and variability conditions.", "venue": "ArXiv", "authors": ["Faiq  Khalid", "Imran Hafeez Abbassi", "Semeen  Rehman", "Osman  Hasan", "Muhammad  Shafique"], "year": 2018, "n_citations": 2}
{"id": 4287242, "s2_id": "17f1134976bf433bb5ea5b8dc595d16779cb7fed", "title": "Design of an Encryption-Decryption Module Oriented for Internet Information Security SOC Design", "abstract": "In order to protect the security of network data, a high speed chip module for encrypting and decrypting of network data packet is designed. The chip module is oriented for internet information security SOC (System on Chip) design. During the design process, AES (Advanced Encryption Standard) and 3DES (Data Encryption Standard) encryption algorithm are adopted to protect the security of network data. The following points are focused: (1) The SOC (System on Chip) design methodology based on IP (Intellectual Property) core is used. AES (Advanced Encryption Standard) and 3DES (Data Encryption Standard) IP (Intellectual Property) cores are embedded in the chip module, peripheral control sub-modules are designed to control the encryption-decryption module, which is capable of shortening the design period of the chip module. (2) The implementation of encryption-decryption with hardware was presented, which improves the safety of data through the encryption-decryption chip and reduce the load of CPU. (3) In our hardware solution, two AES (Advanced Encryption Standard) cores are used to work in parallel, which improves the speed of the encryption module. Moreover, the key length of AES (Advanced Encryption Standard) encryption algorithm is designed with three optional configurations at 128 bits, 256 bits and 192 bits respectively and six optional encryption algorithm modes: CBC (Cipher Block Chaining) mode, ECB (Electronic Code Book) mode, GCM (Galois/Counter Mode) mode, XTS(cipherteXT Stealing) mode, CTR (CounTeR) mode and 3DES respectively, which adds the flexibility to its applications.", "venue": "SOCO 2012", "authors": ["Yixin  Liu", "Haipeng  Zhang", "Tao  Feng"], "year": 2012, "n_citations": 4}
{"id": 4290635, "s2_id": "7bcc51a7ae54372208740e6497649f4f75b5bee6", "title": "Wideband Spectrum Sensing at Sub-Nyquist Rates", "abstract": "We present a mixed analog-digital spectrum sensing method that is especially suited to the typical wideband setting of cognitive radio (CR). The advantages of our system with respect to current architectures are threefold. First, our analog front-end is fixed and does not involve scanning hardware. Second, both the analog-to-digital conversion (ADC) and the digital signal processing (DSP) rates are substantially below Nyquist. Finally, the sensing resources are shared with the reception path of the CR, so that the lowrate streaming samples can be used for communication purposes of the device, besides the sensing functionality they provide. Combining these advantages leads to a real time map of the spectrum with minimal use of mobile resources. Our approach is based on the modulated wideband converter (MWC) system, which samples sparse wideband inputs at sub-Nyquist rates. We report on results of hardware experiments, conducted on an MWC prototype circuit, which affirm fast and accurate spectrum sensing in parallel to CR communication.", "venue": "ArXiv", "authors": ["Moshe  Mishali", "Yonina C. Eldar"], "year": 2010, "n_citations": 71}
{"id": 4291981, "s2_id": "38bad6f8cce35b580703560085eb7fe2876e81b5", "title": "Reversible arithmetic logic unit", "abstract": "Quantum computer requires quantum arithmetic. The sophisticated design of a reversible arithmetic logic unit (reversible ALU) for quantum arithmetic has been investigated in this letter. We provide explicit construction of reversible ALU effecting basic arithmetic operations. By provided the corresponding control unit, the proposed reversible ALU can combine the classical arithmetic and logic operation in a reversible integrated system. This letter provides actual evidence to prove the possibility of the realization of reversible Programmable Logic Device (RPLD) using reversible ALU.", "venue": "ArXiv", "authors": ["Rigui  Zhou", "Yang  Shi", "Manqun  Zhang"], "year": 2011, "n_citations": 0}
{"id": 4306761, "s2_id": "1ade18528759508b222a69310b9e8f71e0a7b50e", "title": "Modular Acquisition and Stimulation System for Timestamp-Driven Neuroscience Experiments", "abstract": "Dedicated systems are fundamental for neuroscience experimental protocols that require timing determinism and synchronous stimuli generation. We developed a data acquisition and stimuli generator system for neuroscience research, optimized for recording timestamps from up to 6 spiking neurons and entirely specified in a high-level Hardware Description Language (HDL). Despite the logic complexity penalty of synthesizing from such a language, it was possible to implement our design in a low-cost small reconfigurable device. Under a modular framework, we explored two different memory arbitration schemes for our system, evaluating both their logic element usage and resilience to input activity bursts. One of them was designed with a decoupled and latency insensitive approach, allowing for easier code reuse, while the other adopted a centralized scheme, constructed specifically for our application. The usage of a high-level HDL allowed straightforward and stepwise code modifications to transform one architecture into the other. The achieved modularity is very useful for rapidly prototyping novel electronic instrumentation systems tailored to scientific research.", "venue": "ARC", "authors": ["Paulo  Matias", "Rafael Tuma Guariento", "L\u00edrio Onofre Baptista de Almeida", "Jan Frans Willem Slaets"], "year": 2015, "n_citations": 0}
{"id": 4312266, "s2_id": "5143923744f27bf688ed5daa181ef5b0392f40ba", "title": "A Very Compact Embedded CNN Processor Design Based on Logarithmic Computing", "abstract": "In this paper, we propose a very compact embedded CNN processor design based on a modified logarithmic computing method using very low bit-width representation. Our high-quality CNN processor can easily fit into edge devices. For Yolov2, our processing circuit takes only 0.15 mm2 using TSMC 40 nm cell library. The key idea is to constrain the activation and weight values of all layers uniformly to be within the range [-1, 1] and produce low bit-width logarithmic representation. With the uniform representations, we devise a unified, reusable CNN computing kernel and significantly reduce computing resources. The proposed approach has been extensively evaluated on many popular image classification CNN models (AlexNet, VGG16, and ResNet-18/34) and object detection models (Yolov2). The hardware-implemented results show that our design consumes only minimal computing and storage resources, yet attains very high accuracy. The design is thoroughly verified on FPGAs, and the SoC integration is underway with promising results. With extremely efficient resource and energy usage, our design is excellent for edge computing purposes.", "venue": "ArXiv", "authors": ["Tsung-Ying  Lu", "Hsu-Hsun  Chin", "Hsin-I  Wu", "Ren-Song  Tsay"], "year": 2020, "n_citations": 0}
{"id": 4312336, "s2_id": "062130f345b07d83ef041d4d78abcf3f651cad5e", "title": "A high-level model of embedded flash energy consumption", "abstract": "The alignment of code in the flash memory of deeply embedded SoCs can have a large impact on the total energy consumption of a computation. We investigate the effect of code alignment in six SoCs and find that a large proportion of this energy (up to 15% of total SoC energy consumption) can be saved by changes to the alignment. A flexible model is created to predict the read-access energy consumption of flash memory on deeply embedded SoCs, where code is executed in place. This model uses the instruction level memory accesses performed by the processor to calculate the flash energy consumption of a sequence of instructions. We derive the model parameters for five SoCs and validate them. The error is as low as 5%, with a 11% average normalized RMS deviation overall. The scope for using this model to optimize code alignment is explored across a range of benchmarks and SoCs. Analysis shows that over 30% of loops can be better aligned. This can significantly reduce energy while increasing code size by less than 4%. We conclude that this effect has potential as an effective optimization, saving significant energy in deeply embedded SoCs.", "venue": "2014 International Conference on Compilers, Architecture and Synthesis for Embedded Systems (CASES)", "authors": ["James  Pallister", "Kerstin  Eder", "Simon J. Hollis", "Jeremy  Bennett"], "year": 2014, "n_citations": 15}
{"id": 4312818, "s2_id": "8c23e70ceb3e04aab3bcd429c8196f70e038f522", "title": "Stockade: Hardware Hardening for Distributed Trusted Sandboxes", "abstract": "Recent studies showed that a cloud application consists of multiple distributed modules provided by mutually distrustful parties. For trusted services, such applications can use trusted execution environments (TEEs) communicating through software-encrypted memory channels. Such an emerging TEE execution model requires a new type of bi-directional protection: protecting the rest of the system from the enclave module with sandboxing and protecting the enclave module from third-party modules and the operating system. However, the current TEE model cannot efficiently represent such distributed sandbox applications. To overcome the lack of hardware supports, this paper proposes an extended TEE model called STOCKADE, which supports distributed sandboxes hardened by hardware. STOCKADE proposes new three key techniques. First, it extends the hardware-based memory isolation in SGX to confine a user software module only within its TEE (enclave). Second, it proposes a trusted monitor enclave that filters and validates systems calls from enclaves. Finally, it allows hardware-protected memory sharing between a pair of enclaves for efficient protected communication without software-based encryption. Using an emulated SGX platform with the proposed extensions, this paper shows that distributed sandbox applications can be effectively supported with small changes of SGX hardware.", "venue": "ArXiv", "authors": ["Joongun  Park", "Seunghyo  Kang", "Sanghyeon  Lee", "Taehoon  Kim", "Jongse  Park", "Youngjin  Kwon", "Jaehyuk  Huh"], "year": 2021, "n_citations": 0}
{"id": 4316438, "s2_id": "883854c9c9478bff99f2c17e685ce105ad16c392", "title": "Reconfigurable multiplier architecture based on memristor-cmos with higher flexibility", "abstract": "Multiplication is an indispensable operation in most of digital signal processing systems. Recently, many systems need to execute different types of algorithms on a multiplier. Therefore, it needs complicated computation and large area occupation. In this regard a fixed multiplier is inefficient and the development of a reconfigurable multiplier becomes increasingly important. The advent of memristor-CMOS hybrid circuits provides an opportunity for reducing area occupation. This paper introduces memristor-CMOS based reconfigurable multiplier which provides flexible multiplication according to various bit-width. Performance of the proposed multiplier is estimated with some applications and comparison with conventional multipliers, using memristor SPICE model and proprietary 180-nm CMOS process.", "venue": "ArXiv", "authors": ["Seungbum  Baek"], "year": 2019, "n_citations": 0}
{"id": 4320940, "s2_id": "5ca1ac3999ad64f477833fcd164bcbde1bd0a63a", "title": "Approximate FPGA-based LSTMs under Computation Time Constraints", "abstract": "Recurrent Neural Networks and in particular Long Short-Term Memory (LSTM) networks have demonstrated state-of-the-art accuracy in several emerging Artificial Intelligence tasks. However, the models are becoming increasingly demanding in terms of computational and memory load. Emerging latency-sensitive applications including mobile robots and autonomous vehicles often operate under stringent computation time constraints. In this paper, we address the challenge of deploying computationally demanding LSTMs at a constrained time budget by introducing an approximate computing scheme that combines iterative low-rank compression and pruning, along with a novel FPGA-based LSTM architecture. Combined in an end-to-end framework, the approximation method's parameters are optimised and the architecture is configured to address the problem of high-performance LSTM execution in time-constrained applications. Quantitative evaluation on a real-life image captioning application indicates that the proposed methods required up to 6.5x less time to achieve the same application-level accuracy compared to a baseline method, while achieving an average of 25x higher accuracy under the same computation time constraints.", "venue": "ARC", "authors": ["Michalis  Rizakis", "Stylianos I. Venieris", "Alexandros  Kouris", "Christos-Savvas  Bouganis"], "year": 2018, "n_citations": 24}
{"id": 4321896, "s2_id": "6e518240148085020d19ba999313adb4ba62747d", "title": "Design of a Ternary Edge-Triggered D Flip-Flap-Flop for Multiple-Valued Sequential Logic", "abstract": "Development of large computerized systems requires both combinational and sequential circuits. Registers and counters are two important examples of sequential circuits, which are widely used in practical applications like CPUs. The basic element of sequential logic is Flip-Flop, which stores an input value and returns two outputs (Q and Q_bar). This paper presents an innovative ternary D Flip-Flap-Flop, which offers circuit designers to customize their design by eliminating one of the outputs if it is not required. This unique feature of the new design leads to considerable power reduction in comparison with the previously presented structures. The proposed design is simulated and tested by HSPICE and 45 nm CMOS technology.", "venue": "ArXiv", "authors": ["Reza Faghih Mirzaee", "Niloofar  Farahani"], "year": 2016, "n_citations": 0}
{"id": 4328962, "s2_id": "371caf8d7f0af85befe129c1a51b4f013fa378e6", "title": "A Survey of Methods for Analyzing and Improving GPU Energy Efficiency", "abstract": "Recent years have witnessed phenomenal growth in the computational capabilities and applications of GPUs. However, this trend has also led to a dramatic increase in their power consumption. This article surveys research works on analyzing and improving energy efficiency of GPUs. It also provides a classification of these techniques on the basis of their main research idea. Further, it attempts to synthesize research works that compare the energy efficiency of GPUs with other computing systems (e.g., FPGAs and CPUs). The aim of this survey is to provide researchers with knowledge of the state of the art in GPU power management and motivate them to architect highly energy-efficient GPUs of tomorrow.", "venue": "ACM Comput. Surv.", "authors": ["Sparsh  Mittal", "Jeffrey S. Vetter"], "year": 2014, "n_citations": 152}
{"id": 4329074, "s2_id": "772a9eaee2ec06656ddf784060f28510e515e365", "title": "Influence of memory hierarchies on predictability for time constrained embedded software", "abstract": "Safety-critical embedded systems having to meet real-time constraints are expected to be highly predictable in order to guarantee at design time that certain timing deadlines will always be met. This requirement usually prevents designers from utilizing caches due to their highly dynamic, thus hardly predictable, behavior. The integration of scratchpad memories represents an alternative approach which allows the system to benefit from a performance gain comparable to that of caches, while at the same time maintaining predictability. We compare the impact of scratchpad memories and caches on worst case execution time (WCET) analysis results. We show that caches, despite requiring complex techniques, can have a negative impact on the predicted WCET while the estimated WCET for scratchpad memories scales with the achieved performance gain at no extra analysis cost.", "venue": "Design, Automation and Test in Europe", "authors": ["Lars  Wehmeyer", "Peter  Marwedel"], "year": 2005, "n_citations": 66}
{"id": 4329660, "s2_id": "f0f818b453d71e55b8fee6649df9af1bd225b1d5", "title": "IRS-Assisted Wireless Powered NOMA: Do We Really Need Different Phase Shifts in DL and UL?", "abstract": "Intelligent reflecting surface (IRS) is a promising technology to improve the performance of wireless powered communication networks (WPCNs) due to its capability to reconfigure signal propagation environments via smart reflection. In particular, the high passive beamforming gain promised by IRS can significantly enhance the efficiency of both downlink wireless power transfer (DL WPT) and uplink wireless information transmission (UL WIT) in WPCNs. Although adopting different IRS phase shifts for DL WPT and UL WIT, i.e., dynamic IRS beamforming, is in principle possible but incurs additional signaling overhead and computational complexity, it is an open problem whether it is actually beneficial. To answer this question, we consider an IRS-assisted WPCN where multiple devices employ a hybrid access point (HAP) to first harvest energy and then transmit information using non-orthogonal multiple access (NOMA). Specifically, we aim to maximize the sum throughput of all devices by jointly optimizing the IRS phase shifts and the resource allocation. To this end, we first prove that dynamic IRS beamforming is not needed for the considered system, which helps reduce the number of IRS phase shifts to be optimized. Then, we propose both joint and alternating optimization based algorithms to solve the resulting problem. Simulation results demonstrate the effectiveness of our proposed designs over benchmark schemes and also provide useful insights into the importance of IRS for realizing spectrum and energy efficient WPCNs.", "venue": "IEEE Wireless Communications Letters", "authors": ["Qingqing  Wu", "Xiaobo  Zhou", "Robert  Schober"], "year": 2021, "n_citations": 10}
{"id": 4335105, "s2_id": "accba3080a6ece93c174d8e54cd06a1ba645c7fb", "title": "Machine Learning Clustering Techniques for Selective Mitigation of Critical Design Features", "abstract": "Selective mitigation or selective hardening is an effective technique to obtain a good trade-off between the improvements in the overall reliability of a circuit and the hardware overhead induced by the hardening techniques. Selective mitigation relies on preferentially protecting circuit instances according to their susceptibility and criticality. However, ranking circuit parts in terms of vulnerability usually requires computationally intensive fault-injection simulation campaigns. This paper presents a new methodology which uses machine learning clustering techniques to group flip-flops with similar expected contributions to the overall functional failure rate, based on the analysis of a compact set of features combining attributes from static elements and dynamic elements. Fault simulation campaigns can then be executed on a per-group basis, significantly reducing the time and cost of the evaluation. The effectiveness of grouping similar sensitive flip-flops by machine learning clustering algorithms is evaluated on a practical example.Different clustering algorithms are applied and the results are compared to an ideal selective mitigation obtained by exhaustive fault-injection simulation.", "venue": "2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS)", "authors": ["Thomas  Lange", "Aneesh  Balakrishnan", "Maximilien  Glorieux", "Dan  Alexandrescu", "Luca  Sterpone"], "year": 2020, "n_citations": 1}
{"id": 4337621, "s2_id": "cc4aae37b4366919eef3f2c5365cc899b1258bef", "title": "On Complexity, Energy- and Implementation-Efficiency of Channel Decoders", "abstract": "Future wireless communication systems require efficient and flexible baseband receivers. Meaningful efficiency metrics are key for design space exploration to quantify the algorithmic and the implementation complexity of a receiver. Most of the current established efficiency metrics are based on counting operations, thus neglecting important issues like data and storage complexity. In this paper we introduce suitable energy and area efficiency metrics which resolve the afore-mentioned disadvantages. These are decoded information bit per energy and throughput per area unit. Efficiency metrics are assessed by various implementations of turbo decoders, LDPC decoders and convolutional decoders. An exploration approach is presented, which permit an appropriate benchmarking of implementation efficiency, communications performance, and flexibility trade-offs. Two case studies demonstrate this approach and show that design space exploration should result in various efficiency evaluations rather than a single snapshot metric as done often in state-of-the-art approaches.", "venue": "IEEE Transactions on Communications", "authors": ["Frank  Kienle", "Norbert  Wehn", "Heinrich  Meyr"], "year": 2011, "n_citations": 48}
{"id": 4337651, "s2_id": "b12181536182e62036177cd055131e36b9fe5a16", "title": "An Overflow/Underflow-Free Fixed-Point Bit-Width Optimization Method for OS-ELM Digital Circuit", "abstract": "Currently there has been increasing demand for real-time training on resource-limited IoT devices such as smart sensors, which realizes standalone online adaptation for streaming data without data transfers to remote servers. OS-ELM (Online Sequential Extreme Learning Machine) has been one of promising neural-network-based online algorithms for on-chip learning because it can perform online training at low computational cost and is easy to implement as a digital circuit. Existing OSELM digital circuits employ fixed-point data format and the bit-widths are often manually tuned, however, this may cause overflow or underflow which can lead to unexpected behavior of the circuit. For on-chip learning systems, an overflow/underflow-free design has a great impact since online training is continuously performed and the intervals of intermediate variables will dynamically change as time goes by. In this paper, we propose an overflow/underflow-free bit-width optimization method for fixed-point digital circuit of OS-ELM. Experimental results show that our method realizes overflow/underflow-free OS-ELM digital circuits with 1.0x 1.5x more area cost compared to an ordinary simulation-based optimization method where overflow or underflow can happen.", "venue": "IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences", "authors": ["Mineto  Tsukada", "Hiroki  Matsutani"], "year": 2021, "n_citations": 0}
{"id": 4343303, "s2_id": "23edddd075d769305680374fe43c10d8c3adeacc", "title": "Mitigating Read-disturbance Errors in STT-RAM Caches by Using Data Compression", "abstract": "Abstract Due to its high density and close to SRAM read latency, spin transfer torque RAM (STT-RAM) is considered one of the most promising emerging memory technologies for designing large last level caches (LLCs). However, in a deep submicron region, STT-RAM shows read disturbance error (RDE) whereby a read operation may modify the stored data value, and this presents a severe threat to performance and reliability of STT-RAM caches. In this paper, we present a technique, named SHIELD, to mitigate RDE in STT-RAM LLCs. SHIELD uses data compression to reduce the number of read operations from STT-RAM blocks to avoid RDE, and also to reduce the number of bits written to cache during both write and restore operations. Experimental results have shown that SHIELD provides significant improvement in performance and energy efficiency. SHIELD consumes smaller energy than the two previous RDE-mitigation techniques, namely high current restore required read (HCRR, also called restore-after-read) and low current long latency read (LCLL) and even an ideal RDE-free STT-RAM cache.", "venue": "Nanoelectronics", "authors": ["Sparsh  Mittal"], "year": 2019, "n_citations": 3}
{"id": 4344844, "s2_id": "77913ed730cffe0e4b1512f53b90182e778fb12b", "title": "Amorphous Dynamic Partial Reconfiguration with Flexible Boundaries to Remove Fragmentation", "abstract": "Dynamic partial reconfiguration (DPR) allows one region of an field-programmable gate array (FPGA) fabric to be reconfigured without affecting the operations on the rest of the fabric. To use an FPGA as a dynamically shared compute resource, one could partition and manage an FPGA fabric as multiple DPR partitions that can be independently reconfigured at runtime with different application function units (AFUs). Unfortunately, dividing a fabric into DPR partitions with fixed boundaries causes the available fabric resources to become fragmented. An AFU of a given size cannot be loaded unless a sufficiently large DPR partition was floorplanned at build time. To overcome this inefficiency, we devised an \"amorphous\" DPR technique that is compatible with current device and tool support but does not require the DPR partition boundaries to be a priori fixed. A collection of AFU bitstreams can be simultaneously loaded on the fabric if their footprints (the actual area used by an AFU) in the fabric do not overlap. We verified the feasibility of amorphous DPR on Xilinx Zynq System-on-Chip (SoC) FPGAs using Vivado. We evaluated the benefits of amorphous DPR in the context of a dynamically reconfigurable vision processing pipeline framework.", "venue": "ArXiv", "authors": ["Marie  Nguyen", "James C. Hoe"], "year": 2017, "n_citations": 2}
{"id": 4345610, "s2_id": "ca1363bc8053d803725268526bc7756213112117", "title": "Transaction-level Model Simulator for Communication-Limited Accelerators", "abstract": "Rapid design space exploration in early design stage is critical to algorithm-architecture co-design for accelerators. In this work, a pre-RTL cycle-accurate accelerator simulator based on SystemC transaction-level modeling (TLM), AccTLMSim, is proposed for convolutional neural network (CNN) accelerators. The accelerator simulator keeps track of each bus transaction between accelerator and DRAM, taking into account the communication bandwidth. The simulation results are validated against the implementation results on the Xilinx Zynq. Using the proposed simulator, it is shown that the communication bandwidth is severely affected by DRAM latency and bus protocol overhead. In addition, the loop tiling is optimized to maximize the performance under the constraint of on-chip SRAM size. Furthermore, a new performance estimation model is proposed to speed up the design space exploration. Thanks to the proposed simulator and performance estimation model, it is possible to explore a design space of millions of architectural options within a few tens of minutes.", "venue": "ArXiv", "authors": ["Sunwoo  Kim", "Jooho  Wang", "Youngho  Seo", "Sanghun  Lee", "Yeji  Park", "Sungkyung  Park", "Chester Sungchung Park"], "year": 2020, "n_citations": 1}
{"id": 4348300, "s2_id": "1c3472f9ed951582503174cf6a1ad3185a3329ac", "title": "The challenge of multi-operand adders in CNNs on FPGAs: how not to solve it!", "abstract": "Convolutional Neural Networks (CNNs) are computationally intensive algorithms that currently require dedicated hardware to be executed. In the case of FPGA-Based accelerators, we point-out in this work the challenge of Multi-Operand Adders (MOAs) and their high resource utilization in an FPGA implementation of a CNN. To address this challenge, two optimization strategies, that rely on time-multiplexing and approximate computing, are investigated. At first glance, the two strategies looked promising to reduce the footprint of a given architectural mapping, but when synthesized on the device, none of them gave the expected results. Experimental sections analyze the reasons of these unexpected results.", "venue": "SAMOS", "authors": ["Kamel  Abdelouahab", "Franccois  Berry", "Maxime  Pelcat"], "year": 2018, "n_citations": 9}
{"id": 4351928, "s2_id": "1c319244f241393591ffbd588beeb72e762a85d7", "title": "A Range Matching CAM for Hierarchical Defect Tolerance Technique in NRAM Structures", "abstract": "Due to the small size of nanoscale devices, they are highly prone to process disturbances which results in manufacturing defects. Some of the defects are randomly distributed throughout the nanodevice layer. Other disturbances tend to be local and lead to cluster defects caused by factors such as layer misintegration and line width variations. In this paper, we propose a method for identifying cluster defects from random ones. The motivation is to repair the cluster defects using rectangular ranges in a range matching content-addressable memory (RM-CAM) and random defects using triple-modular redundancy (TMR). It is believed a combination of these two approaches is more effective for repairing defects at high error rate with less resource. With the proposed fault repairing technique, defect recovery results are examined for different fault distribution scenarios. Also the mapping circuit structure required for two conceptual 32*32 and 64*64 bit RAMs are presented and their speed, power and transistor count are reported.", "venue": "ArXiv", "authors": ["Hossein  Pourmeidani", "Mehdi  Habibi"], "year": 2019, "n_citations": 0}
{"id": 4355254, "s2_id": "3ab69e432d2ed279bbd3a2837aaeeff0f596f594", "title": "An 8-bit In Resistive Memory Computing Core with Regulated Passive Neuron and Bit Line Weight Mapping", "abstract": "The rapid development of Artificial Intelligence (AI) and Internet of Things (IoT) increases the requirement for edge computing with low power and relatively high processing speed devices. The Computing-In-Memory(CIM) schemes based on emerging resistive Non-Volatile Memory(NVM) show great potential in reducing the power consumption for AI computing. However, the device inconsistency of the non-volatile memory may significantly degenerate the performance of the neural network. In this paper, we propose a low power Resistive RAM (RRAM) based CIM core to not only achieve high computing efficiency but also greatly enhance the robustness by bit line regulator and bit line weight mapping algorithm. The simulation results show that the power consumption of our proposed 8-bit CIM core is only 3.61mW (256*256). The SFDR and SNDR of the CIM core achieve 59.13 dB and 46.13 dB, respectively. The proposed bit line weight mapping scheme improves the top-1 accuracy by 2.46% and 3.47% for AlexNet and VGG16 on ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC 2012) in 8-bit mode, respectively.", "venue": "ArXiv", "authors": ["Yewei  Zhang", "Kejie  Huang", "Rui  Xiao", "Haibin  Shen"], "year": 2020, "n_citations": 2}
{"id": 4356154, "s2_id": "ac7aac9474f2c6327af429411543c53f740bbc4a", "title": "A Secure Asynchronous FPGA Architecture, Experimental Results and Some Debug Feedback", "abstract": "This article presents an asynchronous FPGA architecture for implementing cryptographic algorithms secured against physical cryptanalysis. We discuss the suitability of asynchronous reconfigurable architectures for such applications before proceeding to model the side channel and defining our objectives. The logic block architecture is presented in detail. We discuss several solutions for the interconnect architecture, and how these solutions can be ported to other flavours of interconnect (i.e. single driver). Next We discuss in detail a high speed asynchronous configuration chain architecture used to configure our asynchronous FPGA with simulation results, and we present a 3 X 3 prototype FPGA fabricated in 65 nm CMOS. Lastly we present experiments to test the high speed asynchronous configuration chain and evaluate how far our objectives have been achieved with proposed solutions, and we conclude with emphasis on complementary FPGA CAD algorithms, and the effect of CMOS variation on Side-Channel Vulnerability.", "venue": "ArXiv", "authors": ["Sumanta  Chaudhuri", "Sylvain  Guilley", "Philippe  Hoogvorst", "Jean-Luc  Danger", "Taha  Beyrouthy", "Alin  Razafindraibe", "Laurent  Fesquet", "Marc  Renaudin"], "year": 2011, "n_citations": 1}
{"id": 4358318, "s2_id": "af03063f7adb756e00de5f66a2d59b0116312ffb", "title": "RNNFast: An Accelerator for Recurrent Neural Networks Using Domain Wall Memory", "abstract": "Recurrent Neural Networks (RNNs) are an important class of neural networks designed to retain and incorporate context into current decisions. RNNs are particularly well suited for machine learning problems in which context is important, such as speech recognition and language translation. This work presents RNNFast, a hardware accelerator for RNNs that leverages an emerging class of non-volatile memory called domain-wall memory (DWM). We show that DWM is very well suited for RNN acceleration due to its very high density and low read/write energy. At the same time, the sequential nature of input/weight processing of RNNs mitigates one of the downsides of DWM, which is the linear (rather than constant) data access time.RNNFast is very efficient and highly scalable, with flexible mapping of logical neurons to RNN hardware blocks. The basic hardware primitive, the RNN processing element (PE) includes custom DWM-based multiplication, sigmoid and tanh units for high density and low-energy. The accelerator is designed to minimize data movement by closely interleaving DWM storage and computation. We compare our design with a state-of-the-art GPGPU and find21.8x higher performance with70x lower energy", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Mohammad Hossein Samavatian", "Anys  Bacha", "Li  Zhou", "Radu  Teodorescu"], "year": 2020, "n_citations": 4}
{"id": 4358523, "s2_id": "cd3e92dd2be95f2f29dbc08a9627b87a51b92959", "title": "An Investigation on Inherent Robustness of Posit Data Representation", "abstract": "As the dimensions and operating voltages of computer electronics shrink to cope with consumers' demand for higher performance and lower power consumption, circuit sensitivity to soft errors increases dramatically. Recently, a new data-type is proposed in the literature called posit data type. Posit arithmetic has absolute advantages such as higher numerical accuracy, speed, and simpler hardware design than IEEE 754\u20132008 technical standard-compliant arithmetic. In this paper, we propose a comparative robustness study between 32-bit posit and 32-bit IEEE 754\u20132008 compliant representations. At first, we propose a theoretical analysis for IEEE 754 compliant numbers and posit numbers for single bit flip and double bit flips. Then, we conduct exhaustive fault injection experiments that show a considerable inherent resilience in posit format compared to classical IEEE 754 compliant representation. To show a relevant use-case of fault-tolerant applications, we perform experiments on a set of machine-learning applications. In more than 95% of the exhaustive fault injection exploration, posit representation is less impacted by faults than the IEEE 754 compliant floating-point representation. Moreover, in 100% of the tested machine-learning applications, the accuracy of posit-implemented systems is higher than the classical floating-point-based ones.", "venue": "2021 34th International Conference on VLSI Design and 2021 20th International Conference on Embedded Systems (VLSID)", "authors": ["Ihsen  Alouani", "Anouar Ben Khalifa", "Farhad  Merchant", "Rainer  Leupers"], "year": 2021, "n_citations": 3}
{"id": 4362826, "s2_id": "fb8beefb38be0999c5db9988614f2422fd746581", "title": "Higher-Level Hardware Synthesis of the KASUMI Algorithm", "abstract": "Programmable Logic Devices (PLDs) continue to grow in size and currently contain several millions of gates. At the same time, research effort is going into higher-level hardware synthesis methodologies for reconfigurable computing that can exploit PLD technology. In this paper, we explore the effectiveness and extend one such formal methodology in the design of massively parallel algorithms. We take a step-wise refinement approach to the development of correct reconfigurable hardware circuits from formal specifications. A functional programming notation is used for specifying algorithms and for reasoning about them. The specifications are realised through the use of a combination of function decomposition strategies, data refinement techniques, and off-the-shelf refinements based upon higher-order functions. The off-the-shelf refinements are inspired by the operators of Communicating Sequential Processes (CSP) and map easily to programs in Handel-C (a hardware description language). The Handel-C descriptions are directly compiled into reconfigurable hardware. The practical realisation of this methodology is evidenced by a case studying the third generation mobile communication security algorithms. The investigated algorithm is the KASUMI block cipher. In this paper, we obtain several hardware implementations with different performance characteristics by applying different refinements to the algorithm. The developed designs are compiled and tested under Celoxica\u2019s RC-1000 reconfigurable computer with its 2 million gates Virtex-E FPGA. Performance analysis and evaluation of these implementations are included.", "venue": "Journal of Computer Science and Technology", "authors": ["Issam W. Damaj"], "year": 2007, "n_citations": 5}
{"id": 4364947, "s2_id": "3a2ac9bb1fdebd7b1e03c97d54ae4d1812674d5b", "title": "Efficient bypass in mesh and torus NoCs", "abstract": "Abstract Minimizing latency and power are key goals in the design of NoC routers. Different proposals combine lookahead routing and router bypass to skip the arbitration and buffering, reducing router delay. However, the conditions to use them requires completely empty buffers in the intermediate routers. This restricts the amount of flits that use the bypass pipeline especially at medium and high loads, increasing latency and power. This paper presents NEBB, Non-Empty Buffer Bypass, a mechanism that allows to bypass flits even if the buffers to bypass are not empty. The mechanism applies to wormhole and virtual-cut-through, each of them with different advantages. NEBB-Hybrid is proposed to employ the best flow control in each situation. The mechanism is extended to torus topologies, using FBFC and shared buffers. The proposals have been evaluated using Booksim, showing up to 75% reduction of the buffered flits for single-flit packets, which translates into latency and dynamic power reductions of up to 30% and 23% respectively. For bimodal traffic, these improvements are 20 and 21% respectively. Additionally, the bypass utilization is largely independent of the number of VCs when using shared buffers and very competitive with few private ones, allowing to simplify the allocation mechanisms.", "venue": "J. Syst. Archit.", "authors": ["Ivan  Perez", "Enrique  Vallejo", "Ram\u00f3n  Beivide"], "year": 2020, "n_citations": 0}
{"id": 4368450, "s2_id": "3d6b4cfee501dc977f0eb67e579d42101d6c7e35", "title": "FIXAR: A Fixed-Point Deep Reinforcement Learning Platform with Quantization-Aware Training and Adaptive Parallelism", "abstract": "Deep reinforcement learning (DRL) is a powerful technology to deal with decision-making problem in various application domains such as robotics and gaming, by allowing an agent to learn its action policy in an environment to maximize a cumulative reward. Unlike supervised models which actively use data quantization, DRL still uses the single-precision floating-point for training accuracy while it suffers from computationally intensive deep neural network (DNN) computations. In this paper, we present a deep reinforcement learning acceleration platform named FIXAR, which employs fixed-point data types and arithmetic units for the first time using a SW/HW co-design approach. We propose a quantization-aware training algorithm in fixed-point, which enables to reduce the data precision by half after a certain amount of training time without losing accuracy. We also design a FPGA accelerator that employs adaptive dataflow and parallelism to handle both inference and training operations. Its processing element has configurable datapath to efficiently support the proposed quantized-aware training. We validate our FIXAR platform, where the host CPU emulates the DRL environment and the FPGA accelerates the agent\u2019s DNN operations, by running multiple benchmarks in continuous action spaces based on a latest DRL algorithm called DDPG. Finally, the FIXAR platform achieves 25293.3 inferences per second (IPS) training throughput, which is 2.7 times higher than the CPU-GPU platform. In addition, its FPGA accelerator shows 53826.8 IPS and 2638.0 IPS/W energy efficiency, which are 5.5 times higher and 15.4 times more energy efficient than those of GPU, respectively. FIXAR also shows the best IPS throughput and energy efficiency among other state-of-the-art acceleration platforms using FPGA, even it targets one of the most complex DNN models.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Je  Yang", "Seongmin  Hong", "Joo-Young  Kim"], "year": 2021, "n_citations": 0}
{"id": 4374178, "s2_id": "feb6457906b27af90d5b63fdafbcdb178a917e5f", "title": "A transprecision floating-point platform for ultra-low power computing", "abstract": "In modern low-power embedded platforms, the execution of floating-point (FP) operations emerges as a major contributor to the energy consumption of compute-intensive applications with large dynamic range. Experimental evidence shows that 50% of the energy consumed by a core and its data memory is related to FP computations. The adoption of FP formats requiring a lower number of bits is an interesting opportunity to reduce energy consumption, since it allows to simplify the arithmetic circuitry and to reduce the memory bandwidth required to transfer data between memory and registers by enabling vectorization. From a theoretical point of view, the adoption of multiple FP types perfectly fits with the principle of transprecision computing, allowing fine-grained control of approximation while meeting specified constraints on the precision of final results. In this paper we propose an extended FP type system with complete hardware support to enable transprecision computing on low-power embedded processors, including two standard formats (binary32 and binary16) and two new formats (binary8 and binary16alt). First, we introduce a software library that enables exploration of FP types by tuning both precision and dynamic range of program variables. Then, we present a methodology to integrate our library with an external tool for precision tuning, and experimental results that highlight the clear benefits of introducing the new formats. Finally, we present the design of a transprecision FP unit capable of handling 8-bit and 16-bit operations in addition to standard 32-bit operations. Experimental results on FP-intensive benchmarks show that up to 90% of FP operations can be safely scaled down to 8-bit or 16-bit formats. Thanks to precision tuning and vectorization, execution time is decreased by 12% and memory accesses are reduced by 27% on average, leading to a reduction of energy consumption up to 30%.", "venue": "2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Giuseppe  Tagliavini", "Stefan  Mach", "Davide  Rossi", "Andrea  Marongiu", "Luca  Benini"], "year": 2018, "n_citations": 60}
{"id": 4376972, "s2_id": "ba22e36b3d7eff80e4185bef0087ae0bd3fe2e18", "title": "High throughput neural network based embedded streaming multicore processors", "abstract": "With power consumption becoming a critical processor design issue, specialized architectures for low power processing are becoming popular. Several studies have shown that neural networks can be used for signal processing and pattern recognition applications. This study examines the design of memristor based multicore neural processors that would be used primarily to process data directly from sensors. Additionally, we have examined the design of SRAM based neural processors for the same task. Full system evaluation of the multicore processors based on these specialized cores were performed taking I/O and routing circuits into consideration. The area and power benefits were compared with traditional multicore RISC processors. Our results show that the memristor based architectures can provide an energy efficiency between three and five orders of magnitude greater than that of RISC processors for the benchmarks examined.", "venue": "2016 IEEE International Conference on Rebooting Computing (ICRC)", "authors": ["Raqibul  Hasan", "Tarek M. Taha", "Chris  Yakopcic", "David J. Mountain"], "year": 2016, "n_citations": 11}
{"id": 4380460, "s2_id": "e0e9c1f36666bb9cc9c68bc61b82ba96c4a9f8ad", "title": "Compiling Spiking Neural Networks to Neuromorphic Hardware", "abstract": "Machine learning applications that are implemented with spike-based computation model, e.g., Spiking Neural Network (SNN), have a great potential to lower the energy consumption when executed on a neuromorphic hardware. How- ever, compiling and mapping an SNN to the hardware is challenging, especially when compute and storage resources of the hardware (viz. crossbars) need to be shared among the neurons and synapses of the SNN. We propose an approach to analyze and compile SNNs on resource-constrained neuromorphic hardware, providing guarantees on key performance metrics such as execution time and throughput. Our approach makes the following three key contributions. First, we propose a greedy technique to partition an SNN into clusters of neurons and synapses such that each cluster can fit on to the resources of a crossbar. Second, we exploit the rich semantics and expressiveness of Synchronous Dataflow Graphs (SDFGs) to represent a clustered SNN and analyze its performance using Max-Plus Algebra, considering the available compute and storage capacities, buffer sizes, and communication bandwidth. Third, we propose a self-timed execution-based fast technique to compile and admit SNN-based applications to a neuromorphic hardware at run-time, adapting dynamically to the available resources on the hard- ware. We evaluate our approach with standard SNN-based applications and demonstrate a significant performance improvement compared to current practices.", "venue": "LCTES", "authors": ["Shihao  Song", "Adarsha  Balaji", "Anup  Das", "Nagarajan  Kandasamy", "James  Shackleford"], "year": 2020, "n_citations": 22}
{"id": 4383503, "s2_id": "873ad3825bda8c1e2f11de781f9564b2060c2798", "title": "Reversible CAM Processor Modeled After Quantum Computer Behavior", "abstract": "Proposed below is a reversible digital computer modeled after the natural behavior of a quantum system. Using approaches usually reserved for idealized quantum computers, the Reversible CAM, or State Vector Parallel (RSVP) processor can easily find keywords in an unstructured database (that is, it can solve a needle in a haystack problem). The RSVP processor efficiently solves a SAT (Satisfiability of Boolean Formulae) problem; also it can aid in the solution of a GP (Global Properties of Truth Table) problem. The power delay product of the RSVP processor is exponentially lower than that of a standard CAM programmed to perform similar operations.", "venue": "ArXiv", "authors": ["John Robert Burger"], "year": 2005, "n_citations": 1}
{"id": 4385499, "s2_id": "1c414e4d70429faa90ac3c1524656dc8e6150d1d", "title": "A Survey of Network-On-Chip Tools", "abstract": "Nowadays System-On-Chips (SoCs) have evolved considerably in term of performances, reliability and integration capacity. The last advantage has induced the growth of the number of cores or Intellectual Properties (IPs) in a same chip. Unfortunately, this important number of IPs has caused a new issue which is the intra-communication between the elements of a same chip. To resolve this problem, a new paradigm has been introduced which is the Network-On-Chip (NoC). Since the introduction of the NoC paradigm in the last decade, new methodologies and approaches have been presented by research community and many of them have been adopted by industrials. The literature contains many relevant studies and surveys discussing NoC proposals and contributions. However, few of them have discussed or proposed a comparative study of NoC tools. The objective of this work is to establish a reliable survey about available design, simulation or implementation NoC tools. We collected an important amount of information and characteristics about NoC dedicated tools that we will present throughout this survey. This study is built around a respectable amount of references and we hope it will help scientists. Keywords\u2014Embedded Systems; Network-On-Chip; CAD Tools; Performance Analysis; Verification and Measurement", "venue": "ArXiv", "authors": ["Ahmed Ben Achballah", "Slim Ben Saoud"], "year": 2013, "n_citations": 49}
{"id": 4389930, "s2_id": "211c1ca901260f50aa13a0982626c12ec91aac66", "title": "Architectural exploration of heterogeneous memory systems", "abstract": "Heterogeneous systems appear as a viable design alternative for the dark silicon era. In this paradigm, a processor chip includes several different technological alternatives for implementing a certain logical block (e.g., core, on-chip memories) which cannot be used at the same time due to power constraints. The programmer and compiler are then responsible for selecting which of the alternatives should be used for maximizing performance and/or energy efficiency for a given application. This paper presents an initial approach for the exploration of different technological alternatives for the implementation of on-chip memories. It hinges on a linear programming-based model for theoretically comparing the performance offered by the available alternatives, namely SRAM and STT-RAM scratchpads or caches. Experimental results using a cycle-accurate simulation tool confirm that this is a viable model for implementation into production compilers.", "venue": "ArXiv", "authors": ["Marcos  Horro", "Gabriel  Rodr\u00edguez", "Juan  Touri\u00f1o", "Mahmut T. Kandemir"], "year": 2018, "n_citations": 0}
{"id": 4389989, "s2_id": "85ad97ce0e7f520f6b90ace1cf4ad978b83abed1", "title": "Fast statistical timing analysis for circuits with post-silicon tunable clock buffers", "abstract": "Post-Silicon Tunable (PST) clock buffers are widely used in high performance designs to counter process variations. By allowing delay compensation between consecutive register stages, PST buffers can effectively improve the yield of digital circuits. To date, the evaluation of manufacturing yield in the presence of PST buffers is only possible using Monte Carlo simulation. In this paper, we propose an alternative method based on graph transformations, which is much faster, more than 1000 times, and computes a parametric minimum clock period. It also identifies the gates which are most critical to the circuit performance, therefore enabling a fast analysis-optimization flow.", "venue": "ICCAD 2011", "authors": ["Bing  Li", "Ning  Chen", "Ulf  Schlichtmann"], "year": 2011, "n_citations": 0}
{"id": 4391811, "s2_id": "d067e067dca00bea14c4b492f548ca31df524fea", "title": "Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-In-Memory Hardware", "abstract": "Many modern workloads such as neural network inference and graph processing are fundamentally memory-bound. For such workloads, data movement between memory and CPU cores imposes a significant overhead in terms of both latency and energy. A major reason is that this communication happens through a narrow bus with high latency and limited bandwidth, and the low data reuse in memory-bound workloads is insufficient to amortize the cost of memory access. Fundamentally addressing this data movement bottleneck requires a paradigm where the memory system assumes an active role in computing by integrating processing capabilities. This paradigm is known as processing-in-memory (PIM). Recent research explores different forms of PIM architectures, motivated by the emergence of new technologies that integrate memory with a logic layer, where processing elements can be easily placed. Past works evaluate these architectures in simulation or, at best, with simplified hardware prototypes. In contrast, the UPMEM company has designed and manufactured the first publicly-available real-world PIM architecture. The UPMEM PIM architecture combines traditional DRAM memory arrays with general-purpose in-order cores, called DRAM Processing Units (DPUs), integrated in the same chip. This paper presents key takeaways from the first comprehensive analysis [1] of the first publicly-available real-world PIM architecture. First, we introduce our experimental characterization of the UPMEM PIM architecture using microbenchmarks, and present PrIM (Processing-In-Memory benchmarks), a benchmark suite of 16 workloads from different application domains (e.g., dense/sparse linear algebra, databases, data analytics, graph processing, neural networks, bioinformatics, image processing), which we identify as memory-bound. Second, we provide four key takeaways about the UPMEM PIM architecture, which stem from our study of the performance and scaling characteristics of PrIM benchmarks on the UPMEM PIM architecture, and their performance and energy consumption comparison to their state-of-the-art CPU and GPU counterparts. More insights about suitability of different workloads to the PIM system, programming recommendations for software designers, and suggestions and hints for hardware and architecture designers of future PIM systems are available in [1].", "venue": "2021 12th International Green and Sustainable Computing Conference (IGSC)", "authors": ["Juan  G'omez-Luna", "Izzat El Hajj", "Ivan  Fernandez", "Christina  Giannoula", "Geraldo F. Oliveira", "Onur  Mutlu"], "year": 2021, "n_citations": 1}
{"id": 4397781, "s2_id": "774900ec9d15bed7217d43ce15619ebe72ceb938", "title": "Practice Problems for Hardware Engineers", "abstract": "This book is to help undergraduate and graduate students of electrical and computer engineering disciplines with their job interviews. It may also be used as a practice resource while taking courses in VLSI, logic and computer architecture design. The first edition consists of more than 150 problems and their solutions which the author has used in his VLSI, logic, and architectures courses while teaching at USC. The author wishes this book to be available free of charge, subject to the copyright policy on page 3.", "venue": "ArXiv", "authors": ["Shahin  Nazarian"], "year": 2021, "n_citations": 0}
{"id": 4400180, "s2_id": "7509eca57d48e0763587f093b3d12f5a7a9b2c7c", "title": "FMMU: A Hardware-Automated Flash Map Management Unit for Scalable Performance of NAND Flash-Based SSDs", "abstract": "NAND flash-based Solid State Drives (SSDs), which are widely used from embedded systems to enterprise servers, are enhancing performance by exploiting the parallelism of NAND flash memories. To cope with the performance improvement of SSDs, storage systems have rapidly adopted the host interface for SSDs from Serial-ATA, which is used for existing hard disk drives, to high-speed PCI express. Since NAND flash memory does not allow in-place updates, it requires special software called Flash Translation Layer (FTL), and SSDs are equipped with embedded processors to run FTL. Existing SSDs increase the clock frequency of embedded processors or increase the number of embedded processors in order to prevent FTL from acting as bottleneck of SSD performance, but these approaches are not scalable. This paper proposes a hardware-automated Flash Map Management Unit, called FMMU, that handles the address translation process dominating the execution time of the FTL by hardware automation. FMMU provides methods for exploiting the parallelism of flash memory by processing outstanding requests in a non-blocking manner while reducing the number of flash operations. The experimental results show that the FMMU reduces the FTL execution time in the map cache hit case and the miss case by 44% and 37%, respectively, compared with the existing software-based approach operating in 4-core. FMMU also prevents FTL from acting as a performance bottleneck for up to 32-channel, 8-way SSD using PCIe 3.0 x32 host interface.", "venue": "ArXiv", "authors": ["Yeong-Jae  Woo", "Sang Lyul Min"], "year": 2017, "n_citations": 0}
{"id": 4400812, "s2_id": "c60a77fa0a9f7d8b52afad1b63d4e56b0f7e34a4", "title": "Implementing High-Order FIR Filters in FPGAs", "abstract": "Contemporary field-programmable gate arrays (FPGAs) are predestined for the application of finite impulse response (FIR) filters. Their embedded digital signal processing~(DSP) blocks for multiply-accumulate operations enable efficient fixed-point computations, in cases where the filter structure is accurately mapped to the dedicated hardware architecture. This brief presents a generic systolic structure for high-order FIR filters, efficiently exploiting the hardware resources of an FPGA in terms of routability and timing. Although this seems to be an easily implementable task, the synthesizing tools require an adaptation of the straightforward digital filter implementation for an optimal mapping. Using the example of a symmetric FIR filter with 90 taps, we demonstrate the performance of the proposed structure with FPGAs from Xilinx and Altera. The implementation utilizes less than 1% of slice logic and runs at clock frequencies up to 526 MHz. Moreover, an enhancement of the structure ultimately provides an extended dynamic range for the quantized coefficients without the costs of additional slice logic.", "venue": "ArXiv", "authors": ["Philipp  F\u00f6disch", "Artsiom  Bryksa", "Bert  Lange", "Wolfgang  Enghardt", "Peter  Kaever"], "year": 2016, "n_citations": 4}
{"id": 4401358, "s2_id": "a36c0d35ba592ef24ad8e73a258796666599e171", "title": "Automated Space/Time Scaling of Streaming Task Graph", "abstract": "In this paper, we describe a high-level synthesis (HLS) tool that automatically allows area/throughput trade-offs for implementing streaming task graphs (STG). Our tool targets a massively parallel processor array (MPPA) architecture, very similar to the Ambric MPPA chip architecture, which is to be implemented as an FPGA overlay. Similar to Ambric tools, our HLS tool accepts a STG as input written in a subset of Java and a structural language in the style of a Kahn Processing Network (KPN). Unlike the Ambric tools, our HLS tool analyzes the parallelism internal to each Java \"node\" and evaluates the throughput and area of several possible implementations. It then analyzes the full graph for bottlenecks or excess compute capacity, selects an implementation for each node, and even considers replicating or splitting nodes while either minimizing area (for a fixed throughput target), or maximizing throughput (for a fixed area target). In addition to traditional node selection and replication methods used in prior work, we have uniquely implemented node combining and splitting to find a better area/throughput trade-off. We present two optimization approaches, a formal ILP formulation and a heuristic solution. Results show that the heuristic is more flexible and can find design points not available to the ILP, thereby achieving superior results.", "venue": "ArXiv", "authors": ["Hossein  Omidian", "Guy  Lemieux"], "year": 2016, "n_citations": 4}
{"id": 4409101, "s2_id": "b77460221b52d6c324a7c36ca80514aca348edae", "title": "Transparent FPGA Acceleration with TensorFlow", "abstract": "Today, artificial neural networks are one of the major innovators pushing the progress of machine learning. This has particularly affected the development of neural network accelerating hardware. However, since most of these architectures require specialized toolchains, there is a certain amount of additional effort for developers each time they want to make use of a new deep learning accelerator. Furthermore the flexibility of the device is bound to the architecture itself, as well as to the functionality of the runtime environment. In this paper we propose a toolflow using TensorFlow as frontend, thus offering developers the opportunity of using a familiar environment. On the backend we use an FPGA, which is addressable via an HSA runtime environment. In this way we are able to hide the complexity of controlling new hardware from the user, while at the same time maintaining a high amount of flexibility. This can be achieved by our HSA toolflow, since the hardware is not statically configured with the structure of the network. Instead, it can be dynamically reconfigured during runtime with the respective kernels executed by the network and simultaneously from other sources e.g. OpenCL/OpenMP.", "venue": "ArXiv", "authors": ["Simon  Pfenning", "Philipp  Holzinger", "Marc  Reichenbach"], "year": 2021, "n_citations": 1}
{"id": 4413618, "s2_id": "56cd1a8024e9a8211d5f0a59d5f11dff135bf70d", "title": "An Approach to Data Prefetching Using 2-Dimensional Selection Criteria", "abstract": "We propose an approach to data memory prefetching which augments the standard prefetch buffer with selection criteria based on performance and usage pattern of a given instruction. This approach is built on top of a pattern matching based prefetcher, specifically one which can choose between a stream, a stride, or a stream followed by a stride. We track the most recently called instructions to make a decision on the quantity of data to prefetch next. The decision is based on the frequency with which these instructions are called and the hit/miss rate of the prefetcher. In our approach, we separate the amount of data to prefetch into three categories: a high degree, a standard degree and a low degree. We ran tests on different values for the high prefetch degree, standard prefetch degree and low prefetch degree to determine that the most optimal combination was 1, 4, 8 lines respectively. The 2 dimensional selection criteria improved the performance of the prefetcher by up to 9.5% over the first data prefetching championship winner. Unfortunately performance also fell by as much as 14%, but remained similar on average across all of the benchmarks we tested.", "venue": "ArXiv", "authors": ["Jean  Sung", "Sebastian  Krupa", "Andrew  Fishberg", "Josef B. Spjut"], "year": 2015, "n_citations": 0}
{"id": 4415468, "s2_id": "8ec84a40b7c6944ddec1a9eee510a5fa95509af4", "title": "OpTorch: Optimized deep learning architectures for resource limited environments", "abstract": "Deep learning algorithms have made many breakthroughs and have various applications in real life. Computational resources become a bottleneck as the data and complexity of the deep learning pipeline increases. In this paper, we propose optimized deep learning pipelines in multiple aspects of training including time and memory. OpTorch is a machine learning library designed to overcome weaknesses in existing implementations of neural network training. OpTorch provides features to train complex neural networks with limited computational resources. OpTorch achieved the same accuracy as existing libraries on Cifar-10 and Cifar-100 datasets while reducing memory usage to approximately 50%. We also explore the effect of weights on total memory usage in deep learning pipelines. In our experiments, parallel encoding-decoding along with sequential checkpoints results in much improved memory and time usage while keeping the accuracy similar to existing pipelines. OpTorch python package is available at available at https://github.com/cbrl-nuces/optorch.", "venue": "ArXiv", "authors": ["Salman  Ahmed", "Hammad  Naveed"], "year": 2021, "n_citations": 0}
{"id": 4416586, "s2_id": "7f56ae86bb0db3a20cfa3ca16764e3a298ad89d5", "title": "Studying the Potential of Automatic Optimizations in the Intel FPGA SDK for OpenCL", "abstract": "High Level Synthesis (HLS) tools, like the Intel FPGA SDK for OpenCL, improve hardware design productivity and enable efficient design space exploration, by providing simple program directives (pragmas) and/or API calls that allow hardware programmers to use higher-level languages (like HLS-C or OpenCL). However, modern HLS tools sometimes miss important optimizations that are necessary for high performance. In this poster, we present a study of the tradeoffs in HLS optimizations, and the potential of a modern HLS tool in automatically optimizing an application. We perform the study on a generic, 5-stage camera ISP pipeline using the Intel FPGA SDK for OpenCL and an Arria 10 FPGA Dev Kit. We show that automatic optimizations in the HLS tool are valuable, achieving up to 2.7x speedup over equivalent CPU execution. With further hand tuning, however, we can achieve up to 36.5x speedup over CPU. We draw several specific lessons about the effectiveness of automatic optimizations guided by simple directives and about the nature of manual rewriting required for high performance. Finally, we conclude that there is a gap in the current potential of HLS tools which needs to be filled by next-gen research.", "venue": "FPGA", "authors": ["Adel  Ejjeh", "Vikram  Adve", "Rob A. Rutenbar"], "year": 2020, "n_citations": 0}
{"id": 4417823, "s2_id": "514847268a8c06e8d611ceda7c7d8436cccbd172", "title": "Dynamic Reliability Management in Neuromorphic Computing", "abstract": "Neuromorphic computing systems execute machine learning tasks designed with spiking neural networks. These systems are embracing non-volatile memory to implement high-density and low-energy synaptic storage. Elevated voltages and currents needed to operate non-volatile memories cause aging of CMOS-based transistors in each neuron and synapse circuit in the hardware, drifting the transistor\u2019s parameters from their nominal values. If these circuits are used continuously for too long, the parameter drifts cannot be reversed, resulting in permanent degradation of circuit performance over time, eventually leading to hardware faults. Aggressive device scaling increases power density and temperature, which further accelerates the aging, challenging the reliable operation of neuromorphic systems. Existing reliability-oriented techniques periodically de-stress all neuron and synapse circuits in the hardware at fixed intervals, assuming worst-case operating conditions, without actually tracking their aging at run-time. To de-stress these circuits, normal operation must be interrupted, which introduces latency in spike generation and propagation, impacting the inter-spike interval and hence, performance (e.g., accuracy). We observe that in contrast to long-term aging, which permanently damages the hardware, short-term aging in scaled CMOS transistors is mostly due to bias temperature instability. The latter is heavily workload-dependent and, more importantly, partially reversible. We propose a new architectural technique to mitigate the aging-related reliability problems in neuromorphic systems by designing an intelligent run-time manager (NCRTM), which dynamically de-stresses neuron and synapse circuits in response to the short-term aging in their CMOS transistors during the execution of machine learning workloads, with the objective of meeting a reliability target. NCRTM de-stresses these circuits only when it is absolutely necessary to do so, otherwise reducing the performance impact by scheduling de-stress operations off the critical path. We evaluate NCRTM with state-of-the-art machine learning workloads on a neuromorphic hardware. Our results demonstrate that NCRTM significantly improves the reliability of neuromorphic hardware, with marginal impact on performance.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Shihao  Song", "Jui  Hanamshet", "Adarsha  Balaji", "Anup  Das", "Jeffrey L. Krichmar", "Nikil D. Dutt", "Nagarajan  Kandasamy", "Francky  Catthoor"], "year": 2021, "n_citations": 7}
{"id": 4418551, "s2_id": "edbbead2219f0474862d3f21efca893c39f3b1b3", "title": "FPGA Implementations of Layered MinSum LDPC Decoders Using RCQ Message Passing", "abstract": "Non-uniform message quantization techniques such as reconstruction-computation-quantization (RCQ) improve error-correction performance and decrease hardware complexity of low-density parity-check (LDPC) decoders that use a flooding schedule. Layered MinSum RCQ (L-msRCQ) enables message quantization to be utilized for layered decoders and irregular LDPC codes. We investigate field-programmable gate array (FPGA) implementations of L-msRCQ decoders. Three design methods for message quantization are presented, which we name the Lookup, Broadcast, and Dribble methods. The decoding performance and hardware complexity of these schemes are compared to a layered offset MinSum (OMS) decoder. Simulation results on a (16384, 8192) protograph-based raptor-like (PBRL) LDPC code show that a 4-bit L-msRCQ decoder using the Broadcast method can achieve a 0.03 dB improvement in errorcorrection performance while using 12% fewer registers than the OMS decoder. A Broadcast-based 3-bit L-msRCQ decoder uses 15% fewer lookup tables, 18% fewer registers, and 13% fewer routed nets than the OMS decoder, but results in a 0.09 dB loss in performance.", "venue": "ArXiv", "authors": ["Caleb  Terrill", "Linfang  Wang", "Sean  Chen", "Chester  Hulse", "Calvin  Kuo", "Richard  Wesel", "Dariush  Divsalar"], "year": 2021, "n_citations": 1}
{"id": 4431930, "s2_id": "3ed9c03c7773e6b395cfa4d249731902ba2eb9d2", "title": "Five Modular Redundancy with Mitigation Technique to Recover the Error Module", "abstract": "Hazard radiation can lead the system fault therefore Fault Tolerance is required. Fault Tolerant is a system, which is designed to keep operations running, despite the degradation in the specific module is happening. Many fault tolerances have been developed to handle the problem, to find the most robust and efficient in the possible technology. This paper will present the Five Modular Redundancy (FMR) with Mitigation Technique to Recover the Error Module. With Dynamic Partial Reconfiguration technology that have already available today, such fault tolerance technique can be implemented successfully. The project showed the robustness of the system is increased and module which is error can be recovered immediately.", "venue": "ArXiv", "authors": ["Haryono", "Jazi Eko Istiyanto", "Agus  Harjoko", "Agfianto Eko Putra"], "year": 2014, "n_citations": 5}
{"id": 4434147, "s2_id": "59e41c629714d616579aa2a7e9cd15088cb48b03", "title": "DAMO: Deep Agile Mask Optimization for Full Chip Scale", "abstract": "Continuous scaling of the VLSI system leaves a great challenge on manufacturing, thus optical proximity correction (OPC) is widely applied in conventional design flow for manufacturability optimization. Traditional techniques conduct OPC by leveraging a lithography model but may suffer from prohibitive computational overhead. In addition, most of them focus on optimizing a single and local clip instead of addressing how to tackle the full-chip scale. In this paper, we present DAMO, a high performance and scalable deep learning-enabled OPC system for full-chip scale. It is an end-to-end mask optimization paradigm that contains a deep lithography simulator (DLS) for lithography modeling and a deep mask generator (DMG) for mask pattern generation. Moreover, a novel layout splitting algorithm customized for DAMO is proposed to handle full-chip OPC problem. Extensive experiments show that DAMO outperforms state-of-the-art OPC solutions in both academia and industrial commercial toolkit.", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Guojin  Chen", "Wanli  Chen", "Yuzhe  Ma", "Haoyu  Yang", "Bei  Yu"], "year": 2020, "n_citations": 5}
{"id": 4435446, "s2_id": "3d1bd14765b5a842b97d3cb4049775ae9e5b638b", "title": "RecPipe: Co-designing Models and Hardware to Jointly Optimize Recommendation Quality and Performance", "abstract": "Deep learning recommendation systems must provide high quality, personalized content under strict tail-latency targets and high system loads. This paper presents RecPipe, a system to jointly optimize recommendation quality and inference performance. Central to RecPipe is decomposing recommendation models into multi-stage pipelines to maintain quality while reducing compute complexity and exposing distinct parallelism opportunities. RecPipe implements an inference scheduler to map multi-stage recommendation engines onto commodity, heterogeneous platforms (e.g., CPUs, GPUs). While the hardware-aware scheduling improves ranking efficiency, the commodity platforms suffer from many limitations requiring specialized hardware. Thus, we design RecPipeAccel (RPAccel), a custom accelerator that jointly optimizes quality, tail-latency, and system throughput. RPAccel is designed specifically to exploit the distinct design space opened via RecPipe. In particular, RPAccel processes queries in sub-batches to pipeline recommendation stages, implements dual static and dynamic embedding caches, a set of top-k filtering units, and a reconfigurable systolic array. Compared to previously proposed specialized recommendation accelerators and at iso-quality, we demonstrate that RPAccel improves latency and throughput by 3 \u00d7 and 6 \u00d7.", "venue": "MICRO", "authors": ["Udit  Gupta", "Samuel  Hsia", "Jeff  Zhang", "Mark  Wilkening", "Javin  Pombra", "Hsien-Hsin S. Lee", "Gu-Yeon  Wei", "Carole-Jean  Wu", "David  Brooks"], "year": 2021, "n_citations": 1}
{"id": 4437754, "s2_id": "9bde9379d81cc29d59be9dd86eed0dfbe956f1ef", "title": "High performance and energy efficient inference for deep learning on ARM processors", "abstract": "We evolve PyDTNN, a framework for distributed parallel training of Deep Neural Networks (DNNs), into an efficient inference tool for convolutional neural networks. Our optimization process on multicore ARM processors involves several high-level transformations of the original framework, such as the development and integration of Cython routines to exploit thread-level parallelism; the design and development of micro-kernels for the matrix multiplication, vectorized with ARM\u2019s NEON intrinsics, that can accommodate layer fusion; and the appropriate selection of several cache configuration parameters tailored to the memory hierarchy of the target ARM processors. Our experiments evaluate both inference throughput (measured in processed images/s) and inference latency (i.e., time-to-response) as well as energy consumption per image when varying the level of thread parallelism and the processor power modes. The experiments with the new inference engine are reported for the ResNet50 v1.5 model on the ImageNet dataset from the MLPerf suite using the ARM v8.2 cores in the NVIDIA Jetson AGX Xavier board. These results show superior performance compared with the well-spread TFLite from Google and slightly inferior results when compared with ArmNN, the native library from ARM for DNN inference.", "venue": "ArXiv", "authors": ["Adri'an  Castell'o", "Sergio  Barrachina", "Manuel F. Dolz", "Enrique S. Quintana-Ort'i", "Pau San Juan"], "year": 2021, "n_citations": 0}
{"id": 4440944, "s2_id": "01444f5683dfc2be5e559ac0d4d20b905fc05b89", "title": "Hard-ODT: Hardware-Friendly Online Decision Tree Learning Algorithm and System", "abstract": "Decision trees are machine learning models commonly used in various application scenarios. In the era of big data, traditional decision tree induction algorithms are not suitable for learning large-scale datasets due to their stringent data storage requirement. Online decision tree learning algorithms have been devised to tackle this problem by concurrently training with incoming samples and providing inference results. However, even the most up-to-date online tree learning algorithms still suffer from either high memory usage or high computational intensity with dependency and long latency, making them challenging to implement in hardware. To overcome these difficulties, we introduce a new quantile-based algorithm to improve the induction of the Hoeffding tree, one of the state-of-the-art online learning models. The proposed algorithm is lightweight in terms of both memory and computational demand, while still maintaining high generalization ability. A series of optimization techniques dedicated to the proposed algorithm have been investigated from the hardware perspective, including coarse-grained and fine-grained parallelism, dynamic and memory-based resource sharing, pipelining with data forwarding. Following this, we present Hard-ODT, a high-performance, hardware-efficient and scalable online decision tree learning system on a field-programmable gate array (FPGA) with system-level optimization techniques. Performance and resource utilization are modeled for the complete learning system for early and fast analysis of the tradeoff between various design metrics. Finally, we propose a design flow in which the proposed learning system is applied to FPGA run-time power monitoring as a case study. Experimental results show that our proposed algorithm outperforms the state-of-the-art Hoeffding tree learning method, leading to 0.05% to 12.3% improvement in inference accuracy. Real implementation of the complete learning system on the FPGA demonstrates a $384\\times $ to $1581\\times $ speedup in execution time over the state-of-the-art design. The power modeling strategy with Hard-ODT achieves an average power prediction error within 4.93% of a commercial gate-level power estimation tool.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Zhe  Lin", "Sharad  Sinha", "Wei  Zhang"], "year": 2021, "n_citations": 2}
{"id": 4443803, "s2_id": "b2080028d6a57ac127970ec1f7ef3758cf8018b6", "title": "Power, Delay and Area Comparisons of Majority Voters relevant to TMR Architectures", "abstract": "N-modular redundancy (NMR) is commonly used to enhance the fault tolerance of a circuit/system, when subject to a fault-inducing environment such as in space or military systems, where upsets due to radiation phenomena, temperature and/or other environmental conditions are anticipated. Triple Modular Redundancy (TMR), which is a 3-tuple version of NMR, is widely preferred for mission-control space, military, and aerospace, and safety-critical nuclear, power, medical, and industrial control and automation systems. The TMR scheme involves the two-times duplication of a simplex system hardware, with a majority voter ensuring correctness provided at least two out of three copies of the hardware remain operational. Thus the majority voter plays a pivotal role in ensuring the correct operation of the TMR scheme. In this paper, a number of standard-cell based majority voter designs relevant to TMR architectures are presented, and their power, delay and area parameters are estimated based on physical realization using a 32/28nm CMOS process.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 17}
{"id": 4446732, "s2_id": "a2c4647ce555fa71a7e39f94c08a1329bbd0da85", "title": "Understanding Training Efficiency of Deep Learning Recommendation Models at Scale", "abstract": "The use of GPUs has proliferated for machine learning workflows and is now considered mainstream for many deep learning models. Meanwhile, when training state-of-the-art personal recommendation models, which consume the highest number of compute cycles at our large-scale datacenters, the use of GPUs came with various challenges due to having both compute-intensive and memory-intensive components. GPU performance and efficiency of these recommendation models are largely affected by model architecture configurations such as dense and sparse features, MLP dimensions. Furthermore, these models often contain large embedding tables that do not fit into limited GPU memory. The goal of this paper is to explain the intricacies of using GPUs for training recommendation models, factors affecting hardware efficiency at scale, and learnings from a new scale-up GPU server design, Zion.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Bilge  Acun", "Matthew  Murphy", "Xiaodong  Wang", "Jade  Nie", "Carole-Jean  Wu", "Kim  Hazelwood"], "year": 2021, "n_citations": 17}
{"id": 4447769, "s2_id": "972f03a6ea16e2ba7a3df13d077887f0e823d9e5", "title": "Power efficient carry propagate adder", "abstract": "Here we describe the design details and performance of proposed Carry Propagate Adder based on GDI technique. GDI technique is power efficient technique for designing digital circuit that consumes less power as compare to most commonly used CMOS technique. GDI also has an advantage of minimum propagation delay, minimum area required and less complexity for designing any digital circuit. We designed Carry Propagate Adder using GDI technique and compared its performance with CMOS technique in terms of area, delay and power dissipation. Circuit designed using CADENCE EDA tool and simulated using SPECTRE VIRTUOSO tool at 0.18m technology. Comparative performance result shows that Carry Propagate Adder using GDI technique dissipated 55.6% less power as compare to Carry Propagate Adder using CMOS technique.", "venue": "VLSIC 2013", "authors": ["Laxmi  Kumre", "Ajay  Somkuwar", "Ganga  Agnihotri"], "year": 2013, "n_citations": 7}
{"id": 4451357, "s2_id": "df6604103779a0d3cb4b98cfaf0bc0ce5cd7cade", "title": "A Construction Kit for Efficient Low Power Neural Network Accelerator Designs", "abstract": "Implementing embedded neural network processing at the edge requires efficient hardware acceleration that couples high computational performance with low power consumption. Driven by the rapid evolution of network architectures and their algorithmic features, accelerator designs are constantly updated and improved. To evaluate and compare hardware design choices, designers can refer to a myriad of accelerator implementations in the literature. Surveys provide an overview of these works but are often limited to system-level and benchmarkspecific performance metrics, making it difficult to quantitatively compare the individual effect of each utilized optimization technique. This complicates the evaluation of optimizations for new accelerator designs, slowing-down the research progress. This work provides a survey of neural network accelerator optimization approaches that have been used in recent works and reports their individual effects on edge processing performance. It presents the list of optimizations and their quantitative effects as a construction kit, allowing to assess the design choices for each building block separately. Reported optimizations range from up to 10\u2019000x memory savings to 33x energy reductions, providing chip designers an overview of design choices for implementing efficient low power neural network accelerators.", "venue": "ArXiv", "authors": ["Petar  Jokic", "Erfan  Azarkhish", "Andrea  Bonetti", "Marc  Pons", "Stephane  Emery", "Luca  Benini"], "year": 2021, "n_citations": 0}
{"id": 4458997, "s2_id": "2849a916b6ab2f6794a8fcbcdbc5aaeb4eecf64b", "title": "Compositional memory systems for multimedia communicating tasks", "abstract": "Conventional cache models are not suited for real-time parallel processing because tasks may flush each other's data out of the cache in an unpredictable manner In this way the system is not compositional so the overall performance is difficult to predict and the integration of new tasks expensive. This paper proposes a new method that imposes compositionality to the system's performance and makes different memory hierarchy optimizations possible for multimedia communicating tasks when running on embedded multiprocessor architectures. The method is based on a cache allocation strategy that assigns sets of the unified cache exclusively to tasks and to the communication buffers. We also analytically formulate the problem and describe a method to compute the cache partitioning ratio for optimizing the throughput and the consumed power. When applied to a multiprocessor with memory hierarchy our technique delivers also performance gain. Compared to the shared cache case, for an application consisting of two jpeg decoders and one edge detection algorithm 5 times less misses are experienced and for an mpeg2 decoder 6.5 times less misses are experienced.", "venue": "Design, Automation and Test in Europe", "authors": ["Anca Mariana Molnos", "Marc J. M. Heijligers", "Sorin  Cotofana", "Jos T. J. van Eijndhoven"], "year": 2005, "n_citations": 18}
{"id": 4459855, "s2_id": "9eb72be72a2d48cca5427fee54c52275bf006179", "title": "Scalable Light-Weight Integration of FPGA Based Accelerators with Chip Multi-Processors", "abstract": "Modern multicore systems are migrating from homogeneous systems to heterogeneous systems with accelerator-based computing in order to overcome the barriers of performance and power walls. In this trend, FPGA-based accelerators are becoming increasingly attractive, due to their excellent flexibility and low design cost. In this paper, we propose the architectural support for efficient interfacing between FPGA-based multi-accelerators and chip-multiprocessors (CMPs) connected through the network-on-chip (NoC). Distributed packet receivers and hierarchical packet senders are designed to maintain scalability and reduce the critical path delay under a heavy task load. A dedicated accelerator chaining mechanism is also proposed to facilitate intra-FPGA data reuse among accelerators to circumvent prohibitive communication overhead between the FPGA and processors. In order to evaluate the proposed architecture, a complete system emulation with programmability support is performed using FPGA prototyping. Experimental results demonstrate that the proposed architecture has high-performance, and is light-weight and scalable in characteristics.", "venue": "IEEE Transactions on Multi-Scale Computing Systems", "authors": ["Zhe  Lin", "Sharad  Sinha", "Hao  Liang", "Liang  Feng", "Wei  Zhang"], "year": 2018, "n_citations": 3}
{"id": 4460162, "s2_id": "966a2d26b5d80042e8ba5998f74ba360d06430c8", "title": "Scratchpad Sharing in GPUs", "abstract": "General-Purpose Graphics Processing Unit (GPGPU) applications exploit on-chip scratchpad memory available in the Graphics Processing Units (GPUs) to improve performance. The amount of thread level parallelism (TLP) present in the GPU is limited by the number of resident threads, which in turn depends on the availability of scratchpad memory in its streaming multiprocessor (SM). Since the scratchpad memory is allocated at thread block granularity, part of the memory may remain unutilized. In this article, we propose architectural and compiler optimizations to improve the scratchpad memory utilization. Our approach, called Scratchpad Sharing, addresses scratchpad under-utilization by launching additional thread blocks in each SM. These thread blocks use unutilized scratchpad memory and also share scratchpad memory with other resident blocks. To improve the performance of scratchpad sharing, we propose Owner Warp First (OWF) scheduling that schedules warps from the additional thread blocks effectively. The performance of this approach, however, is limited by the availability of the part of scratchpad memory that is shared among thread blocks. We propose compiler optimizations to improve the availability of shared scratchpad memory. We describe an allocation scheme that helps in allocating scratchpad variables such that shared scratchpad is accessed for short duration. We introduce a new hardware instruction, relssp, that when executed releases the shared scratchpad memory. Finally, we describe an analysis for optimal placement of relssp instructions, such that shared scratchpad memory is released as early as possible, but only after its last use, along every execution path. We implemented the hardware changes required for scratchpad sharing and the relssp instruction using the GPGPU-Sim simulator and implemented the compiler optimizations in Ocelot framework. We evaluated the effectiveness of our approach on 19 kernels from 3 benchmarks suites: CUDA-SDK, GPGPU-Sim, and Rodinia. The kernels that under-utilize scratchpad memory show an average improvement of 19% and maximum improvement of 92.17% in terms of the number of instruction executed per cycle when compared to the baseline approach, without affecting the performance of the kernels that are not limited by scratchpad memory.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Vishwesh  Jatala", "Jayvant  Anantpur", "Amey  Karkare"], "year": 2017, "n_citations": 1}
{"id": 4466874, "s2_id": "bb8ea0daa398dac906063df86e67358ecae064da", "title": "On the operating unit size of load/store architectures\u2020", "abstract": "We introduce a strict version of the concept of a load/store instruction set architecture in the setting of Maurer machines. We take the view that transformations on the states of a Maurer machine are achieved by applying threads as considered in thread algebra to the Maurer machine. We study how the transformations on the states of the main memory of a strict load/store instruction set architecture that can be achieved by applying threads depend on the operating unit size, the cardinality of the instruction set and the maximal number of states of the threads.", "venue": "Mathematical Structures in Computer Science", "authors": ["Jan A. Bergstra", "Kees  Middelburg"], "year": 2010, "n_citations": 6}
{"id": 4468763, "s2_id": "f94642df8c03b80cb8ff8c287fbc439f24c28461", "title": "Inter-Thread Communication in Multithreaded, Reconfigurable Coarse-Grain Arrays", "abstract": "Traditional von Neumann GPGPUs only allow threads to communicate through memory on a group-to-group basis. In this model, a group of producer threads writes intermediate values to memory, which are read by a group of consumer threads after a barrier synchronization. To alleviate the memory bandwidth imposed by this method of communication, GPGPUs provide a small scratchpad memory that prevents intermediate values from overloading DRAM bandwidth. In this paper we introduce direct inter-thread communications for massively multithreaded CGRAs, where intermediate values are communicated directly through the compute fabric on a point-to-point basis. This method avoids the need to write values to memory, eliminates the need for a dedicated scratchpad, and avoids workgroup global barriers. We introduce our proposed extensions to the programming model (CUDA) and execution model, as well as the hardware primitives that facilitate the communication. Our simulations of Rodinia benchmarks running on the new system show that direct inter-thread communication provides an average speedup of 2.8x (10.3x max) and reduces system power by an average of 5x (22x max), when compared to an equivalent Nvidia GPGPU.", "venue": "2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Dani  Voitsechov", "Yoav  Etsion"], "year": 2018, "n_citations": 12}
{"id": 4470076, "s2_id": "37f8ec08b7296f47d6ea6a1c788c88903b3f1409", "title": "Snowflake: A Model Agnostic Accelerator for Deep Convolutional Neural Networks", "abstract": "Deep convolutional neural networks (CNNs) are the deep learning model of choice for performing object detection, classification, semantic segmentation and natural language processing tasks. CNNs require billions of operations to process a frame. This computational complexity, combined with the inherent parallelism of the convolution operation make CNNs an excellent target for custom accelerators. However, when optimizing for different CNN hierarchies and data access patterns, it is difficult for custom accelerators to achieve close to 100% computational efficiency. In this work, we present Snowflake, a scalable and efficient accelerator that is agnostic to CNN workloads, and was designed to always perform at near-peak hardware utilization. Snowflake is able to achieve a computational efficiency of over 91% on modern CNN models. Snowflake, implemented on a Xilinx Zynq XC7Z045 SoC is capable of achieving a peak throughput of 128 G-ops/s and a measured throughput of 100 frames per second and 120 G-ops/s on the AlexNet CNN model, 36 frames per second and 116 Gops/s on the GoogLeNet CNN model and 17 frames per second and 122 G-ops/s on the ResNet-50 CNN model. To the best of our knowledge, Snowflake is the only implemented system capable of achieving over 91% efficiency on modern CNNs and the only implemented system with GoogLeNet and ResNet as part of the benchmark suite.", "venue": "ArXiv", "authors": ["Vinayak  Gokhale", "Aliasger  Zaidy", "Andre Xian Ming Chang", "Eugenio  Culurciello"], "year": 2017, "n_citations": 10}
{"id": 4471810, "s2_id": "6489bb5d7716bb9ef6c61a64609385fdd231dced", "title": "A Dwarf-based Scalable Big Data Benchmarking Methodology", "abstract": "Different from the traditional benchmarking methodology that creates a new benchmark or proxy for every possible workload, this paper presents a scalable big data benchmarking methodology. Among a wide variety of big data analytics workloads, we identify eight big data dwarfs, each of which captures the common requirements of each class of unit of computation while being reasonably divorced from individual implementations. We implement the eight dwarfs on different software stacks, e.g., OpenMP, MPI, Hadoop as the dwarf components. For the purpose of architecture simulation, we construct and tune big data proxy benchmarks using the directed acyclic graph (DAG)-like combinations of the dwarf components with different weights to mimic the benchmarks in BigDataBench. Our proxy benchmarks preserve the micro-architecture, memory, and I/O characteristics, and they shorten the simulation time by 100s times while maintain the average micro-architectural data accuracy above 90 percentage on both X86 64 and ARMv8 processors. We will open-source the big data dwarf components and proxy benchmarks soon.", "venue": "ArXiv", "authors": ["Wanling  Gao", "Lei  Wang", "Jianfeng  Zhan", "Chunjie  Luo", "Daoyi  Zheng", "Zhen  Jia", "Biwei  Xie", "Chen  Zheng", "Qiang  Yang", "Haibin  Wang"], "year": 2017, "n_citations": 7}
{"id": 4471937, "s2_id": "b080ba53a471348e7e76234decdf14e730fea7db", "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers", "abstract": "Transformers have transformed the field of natural language processing. Their superior performance is largely attributed to the use of stacked \u201cself-attention\u201d layers, each of which consists of matrix multiplies as well as softmax operations. As a result, unlike other neural networks, the softmax operation accounts for a significant fraction of the total run-time of Transformers. To address this, we propose Softermax, a hardware-friendly softmax design. Softermax consists of base replacement, low-precision softmax computations, and an online normalization calculation. We show Softermax results in 2.35x the energy efficiency at 0.90x the size of a comparable baseline, with negligible impact on network accuracy.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Jacob R. Stevens", "Rangharajan  Venkatesan", "Steve  Dai", "Brucek  Khailany", "Anand  Raghunathan"], "year": 2021, "n_citations": 2}
{"id": 4473468, "s2_id": "0fdc1ff4f17b32f85d2ed7b4aa732565fb077059", "title": "Performance Evaluation of Low Power MIPS Crypto Processor based on Cryptography Algorithms", "abstract": "This paper presents the design and implementation of low power 32-bit encrypted and decrypted MIPS processor for Data Encryption Standard (DES), Triple DES, Advanced Encryption Standard (AES) based on MIPS pipeline architecture. The organization of pipeline stages has been done in such a way that pipeline can be clocked at high frequency. Encryption and Decryption blocks of three standard cryptography algorithms on MIPS processor and dependency among themselves are explained in detail with the help of a block diagram. Clock gating technique is used to reduce the power consumption in MIPS crypto processor. This approach results in processor that meets power consumption and performance specification for security applications. Proposed Implementation approach concludes higher system performance while reducing operating power consumption. Testing results shows that the MIPS crypto processor operates successfully at a working frequency of 218MHz and a bandwidth of 664Mbits/s.", "venue": "ArXiv", "authors": ["Kirat Pal Singh", "Dilip  Kumar"], "year": 2013, "n_citations": 9}
{"id": 4478963, "s2_id": "c0fb4db6351276d24a574aa455eedf45b7b1cdb6", "title": "The Bitlet Model: Defining a Litmus Test for the Bitwise Processing-in-Memory Paradigm", "abstract": "This paper describes an analytical modeling tool called Bitlet that can be used, in a parameterized fashion, to understand the affinity of workloads to processing-in-memory (PIM) as opposed to traditional computing. The tool uncovers interesting trade-offs between operation complexity (cycles required to perform an operation through PIM) and other key parameters, such as system memory bandwidth, data transfer size, the extent of data alignment, and effective memory capacity involved in PIM computations. Despite its simplicity, the model has already proven useful. In the future, we intend to extend and refine Bitlet to further increase its utility.", "venue": "ArXiv", "authors": ["Kunal  Korgaonkar", "Ronny  Ronen", "Anupam  Chattopadhyay", "Shahar  Kvatinsky"], "year": 2019, "n_citations": 3}
{"id": 4481225, "s2_id": "e6f4702b28118c387766b7abd2bb69be86306def", "title": "AdEle: An Adaptive Congestion-and-Energy-Aware Elevator Selection for Partially Connected 3D NoCs", "abstract": "By lowering the number of vertical connections in fully connected 3D networks-on-chip (NoCs), partially connected 3D NoCs (PC-3DNoCs) help alleviate reliability and fabrication issues. This paper proposes a novel, adaptive congestion- and energy-aware elevator-selection scheme called AdEle to improve the traffic distribution in PC-3DNoCs. AdEle employs an offline multi-objective simulated-annealing-based algorithm to find good elevator subsets and an online elevator selection policy to enhance elevator selection during routing. Compared to the state-of-the-art techniques under different real-application traffics and configuration scenarios, AdEle improves the network latency by 10.9% on average (up to 14.6%) with less than 6.9% energy consumption overhead.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Ebadollah  Taheri", "Ryan G. Kim", "Mahdi  Nikdast"], "year": 2021, "n_citations": 2}
{"id": 4481304, "s2_id": "7ec1dc50cccbf848598728043ee668790f1f448c", "title": "Dynamic Merge Point Prediction", "abstract": "Despite decades of research, conditional branch mispredictions still pose a significant problem for performance. Moreover, limit studies on infinite size predictors show that many of the remaining branches are impossible to predict by current strategies. Our work focuses on mitigating performance loss in the face of impossible to predict branches. This paper presents a dynamic merge point predictor, which uses instructions fetched on the wrong path of the branch to dynamically detect the merge point. Our predictor locates the merge point with an accuracy of 95%, even when faced with branches whose direction is impossible to predict. Furthermore, we introduce a novel confidence-cost system, which identifies costly hard-to-predict branches. Our complete system replaces 58% of all branch mispredictions with a correct merge point prediction, effectively reducing MPKI by 43%. This result demonstrates the potential for dynamic merge point prediction to significantly improve performance.", "venue": "ArXiv", "authors": ["Stephen  Pruett", "Yale  Patt"], "year": 2020, "n_citations": 1}
{"id": 4484805, "s2_id": "91ef5b5cdac223384f65dbe303d2d8df13629646", "title": "New perspectives and opportunities from the wild west of microelectronic biochips", "abstract": "The application of microelectronics to bioanalysis is an emerging field which holds great promise. From the standpoint of electronic and system design, biochips imply a radical change of perspective, since new, completely different constraints emerge, while other usual constraints can be relaxed. While electronic parts of the system can rely on the usual established design-flow, fluidic and packaging design calls for a new approach which relies significantly on experiments. We make some general considerations based on our experience in the development of biochips for cell analysis.", "venue": "Design, Automation and Test in Europe", "authors": ["Nicol\u00f2  Manaresi", "Gianni  Medoro", "Melanie  Abonnenc", "Vincent  Auger", "Paul  Vulto", "Aldo  Romani", "Luigi  Altomare", "Marco  Tartagni", "Roberto  Guerrieri"], "year": 2005, "n_citations": 3}
{"id": 4486832, "s2_id": "0773a7a163054962a4d1a39e8988cc3b93e18fbf", "title": "Thermal-aware task allocation and scheduling for embedded systems", "abstract": "Temperature affects not only the reliability but also the performance, power, and cost of the embedded system. This paper proposes a thermal-aware task allocation and scheduling algorithm for embedded systems. The algorithm is used as a subroutine for hardware/software co-synthesis to reduce the peak temperature and achieve a thermally even distribution while meeting real time constraints. The paper investigates both power-aware and thermal-aware approaches to task allocation and scheduling. The experimental results show that the thermal-aware approach outperforms the power-aware schemes in terms of maximal and average temperature reductions. To the best of our knowledge, this is the first task allocation and scheduling algorithm that takes temperature into consideration.", "venue": "Design, Automation and Test in Europe", "authors": ["Wei-Lun  Hung", "Yuan  Xie", "Narayanan  Vijaykrishnan", "Mahmut T. Kandemir", "Mary Jane Irwin"], "year": 2005, "n_citations": 99}
{"id": 4486879, "s2_id": "b399669557e1b6ea904c8cbf3c48582aa61a459a", "title": "A Framework for Fast Scalable BNN Inference using Googlenet and Transfer Learning", "abstract": "Efficient and accurate object detection in video and image analysis is one of the major beneficiaries of the advancement in computer vision systems with the help of deep learning. With the aid of deep learning, more powerful tools evolved, which are capable to learn highlevel and deeper features and thus can overcome the existing problems in traditional architectures of object detection algorithms. The huge amount of visual information and demand of high throughput requirements are the major the bottle neck in the existing object detection architectures. The work in this project aims to achieve high accuracy in object detection with good real time performance. In the area of computer vision, lot of research are going in the area of detection and processing of visual information, by improving the existing algorithms. Binarized neural network have shown high performance in various vision tasks such as image classification, object detection and semantic segmentation. In this thesis, object detection is done using binarized neural network with tensor flow to improve the accuracy and to reduce the execution time. The Modified National Institute of Standards and Technology database (MNIST), Canadian Institute For Advanced Research (CIFAR) and Street View House Numbers (SVHN) datasets are used which is implemented using pretrained convolutional neural network (CNN) that is 22 layers deep. Supervised learning is used in the work, which classifies the particular dataset with the proper structure of the model. In still images, to improve the accuracy, Googlenet is used. The final layer of the Googlenet are replaced with the transfer learning to improve the accuracy of the Googlenet. At the same time, the accuracy in moving images can be maintained by transfer learning techniques. Hardware is the main backbone for any model to obtain faster results with large number of datasets. Here, Nvidia Jetson Nano is used which is a graphics processing unit (GPU), that can handle large number of computations in the process of object detection. Results show that the accuracy of object detected by transfer learning method is more when compared to the existing methods.", "venue": "ArXiv", "authors": ["E  Karthik"], "year": 2021, "n_citations": 0}
{"id": 4488518, "s2_id": "eb987e5fd68d68169484335a3a5a941c6675afe9", "title": "A Survey on Coarse-Grained Reconfigurable Architectures From a Performance Perspective", "abstract": "With the end of both Dennard\u2019s scaling and Moore\u2019s law, computer users and researchers are aggressively exploring alternative forms of computing in order to continue the performance scaling that we have come to enjoy. Among the more salient and practical of the post-Moore alternatives are reconfigurable systems, with Coarse-Grained Reconfigurable Architectures (CGRAs) seemingly capable of striking a balance between performance and programmability. In this paper, we survey the landscape of CGRAs. We summarize nearly three decades of literature on the subject, with a particular focus on the premise behind the different CGRAs and how they have evolved. Next, we compile metrics of available CGRAs and analyze their performance properties in order to understand and discover knowledge gaps and opportunities for future CGRA research specialized towards High-Performance Computing (HPC). We find that there are ample opportunities for future research on CGRAs, in particular with respect to size, functionality, support for parallel programming models, and to evaluate more complex applications.", "venue": "IEEE Access", "authors": ["Artur  Podobas", "Kentaro  Sano", "Satoshi  Matsuoka"], "year": 2020, "n_citations": 18}
{"id": 4490307, "s2_id": "ae94e04f02379e13fecb4584e3b0c523e7860b3b", "title": "LP-Based Power Grid Enhancement Methodology", "abstract": "In this paper, we explored the opportunity to enhance power grid robustness after routing stage, and propose a linear programming based algorithm that maximizes the improvement of power grid strengthening with given available routing resource. We further discussed some techniques to leverage tradeoffs between runtime and optimality of the solutions. Experimental results show substantial power integrity improvement with \"zero cost\".", "venue": "ArXiv", "authors": ["Tapio  Bohn", "Paul  Salmi", "Albert  Milner"], "year": 2017, "n_citations": 0}
{"id": 4491165, "s2_id": "9325a94b0f0f84efadd9f7e3802f2e6f86b9b67c", "title": "HPIPE: Heterogeneous Layer-Pipelined and Sparse-Aware CNN Inference for FPGAs", "abstract": "This poster presents a novel cross-layer-pipelined Convolutional Neural Network accelerator architecture, and network compiler, that make use of precision minimization and parameter pruning to fit ResNet-50 entirely into on-chip memory on a Stratix 10 2800 FPGA. By statically partitioning the hardware across each of the layers in the network, our architecture enables full DSP utilization and reduces the soft logic per DSP ratio by roughly 4x over prior work on sparse CNN accelerators for FPGAs. This high DSP utilization, a frequency of 420MHz, and skipping zero weights enable our architecture to execute a sparse ResNet-50 model at a batch size of 1 at 3300 images/s, which is nearly 3x higher throughput than NVIDIA's fastest machine learning targeted GPU, the V100. We also present a network compiler and a flexible hardware interface that make it easy to add support for new types of neural networks, and to optimize these networks for FPGAs with different on-chip resources.", "venue": "FPGA", "authors": ["Mathew  Hall", "Vaughn  Betz"], "year": 2020, "n_citations": 4}
{"id": 4496335, "s2_id": "4f5e40261a77a0f237b49e80c6d351cdc34681be", "title": "Probabilistic Value-Deviation-Bounded Integer Codes for Approximate Communication", "abstract": "When computing systems can tolerate the effects of errors or erasures in their communicated data values, they can trade this tolerance for improved resource efficiency. One method for enabling this tradeoff in the I/O subsystems of computing systems, is to use channel codes that reduce the power needed to send bits on a channel in exchange for bounded errors and erasures on numeric program values---value-deviation-bounded (VDB) codes. Unlike rate distortion codes, which guarantee a bound on the expected value of channel distortion, the probabilistic VDB codes we present guarantee any desired tail distribution on integer distances of words transmitted over a channel. We extend prior work to present tighter upper bounds on the efficiency for VDB codes. We present a new probabilistic VDB encoder that lowers power dissipation in exchange for bounded channel integer distortions. The code we present takes the peculiar approach of changing the channel bit error rate across the ordinal bit positions in a word to reduce power dissipation. We implement the code table generator in a software tool built on the dReal SMT solver and we validate the generated codes using Monte Carlo simulation. We present one realization of hardware to implement the technique, requiring 2 mm$^2$ of circuit board area and dissipating less than 0.5 $\\mu$W.", "venue": "ArXiv", "authors": ["Phillip  Stanley-Marbell", "Paul  Hurley"], "year": 2018, "n_citations": 5}
{"id": 4500372, "s2_id": "26a429c9f25df95c8c46c4b7a6a493b892c33a37", "title": "Mesorasi: Architecture Support for Point Cloud Analytics via Delayed-Aggregation", "abstract": "Point cloud analytics is poised to become a key workload on battery-powered embedded and mobile platforms in a wide range of emerging application domains, such as autonomous driving, robotics, and augmented reality, where efficiency is paramount. This paper proposes Mesorasi, an algorithm-architecture co-designed system that simultaneously improves the performance and energy efficiency of point cloud analytics while retaining its accuracy.Our extensive characterizations of state-of-the-art point cloud algorithms show that, while structurally reminiscent of convolutional neural networks (CNNs), point cloud algorithms exhibit inherent compute and memory inefficiencies due to the unique characteristics of point cloud data. We propose delayed-aggregation, a new algorithmic primitive for building efficient point cloud algorithms. Delayed-aggregation hides the performance bottlenecks and reduces the compute and memory redundancies by exploiting the approximately distributive property of key operations in point cloud algorithms. Delayed-aggregation let point cloud algorithms achieve 1.6\u00d7 speedup and 51.1% energy reduction on a mobile GPU while retaining the accuracy (-0.9% loss to 1.2% gains). To maximize the algorithmic benefits, we propose minor extensions to contemporary CNN accelerators, which can be integrated into a mobile Systems-on-a-Chip (SoC) without modifying other SoC components. With additional hardware support, Mesorasi achieves up to 3.6\u00d7 speedup.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Yu  Feng", "Boyuan  Tian", "Tiancheng  Xu", "Paul  Whatmough", "Yuhao  Zhu"], "year": 2020, "n_citations": 6}
{"id": 4500460, "s2_id": "2ceaffd14e2c926926ea8f1b9a1436a50fabbde9", "title": "Domino: A Tailored Network-on-Chip Architecture to Enable Highly Localized Inter- and Intra-Memory DNN Computing", "abstract": "The ever-increasing computation complexity of fast-growing Deep Neural Networks (DNNs) has requested new computing paradigms to overcome the memory wall in conventional Von Neumann computing architectures. The emerging Computing-In-Memory (CIM) architecture has been a promising candidate to accelerate neural network computing. However, the data movement between CIM arrays may still dominate the total power consumption in conventional designs. This paper proposes a flexible CIM processor architecture named Domino to enable stream computing and local data access to significantly reduce the data movement energy. Meanwhile, Domino employs tailored distributed instruction scheduling within Network-on-Chip (NoC) to implement inter-memory-computing and attain mapping flexibility. The evaluation with prevailing CNN models shows that Domino achieves 1.15-to-9.49\u00d7 power efficiency over several stateof-the-art CIM accelerators and improves the throughput by 1.57-to-12.96\u00d7.", "venue": "ArXiv", "authors": ["Kaining  Zhou", "Yangshuo  He", "Rui  Xiao", "Kejie  Huang"], "year": 2021, "n_citations": 0}
{"id": 4510052, "s2_id": "140992bce67d0b7cdf1a8b945b3b055a10d07d50", "title": "Combined Spatial and Temporal Blocking for High-Performance Stencil Computation on FPGAs Using OpenCL", "abstract": "Recent developments in High Level Synthesis tools have attracted software programmers to accelerate their high-performance computing applications on FPGAs. Even though it has been shown that FPGAs can compete with GPUs in terms of performance for stencil computation, most previous work achieve this by avoiding spatial blocking and restricting input dimensions relative to FPGA on-chip memory. In this work we create a stencil accelerator using Intel FPGA SDK for OpenCL that achieves high performance without having such restrictions. We combine spatial and temporal blocking to avoid input size restrictions, and employ multiple FPGA-specific optimizations to tackle issues arisen from the added design complexity. Accelerator parameter tuning is guided by our performance model, which we also use to project performance for the upcoming Intel Stratix 10 devices. On an Arria 10 GX 1150 device, our accelerator can reach up to 760 and 375 GFLOP/s of compute performance, for 2D and 3D stencils, respectively, which rivals the performance of a highly-optimized GPU implementation. Furthermore, we estimate that the upcoming Stratix 10 devices can achieve a performance of up to 3.5 TFLOP/s and 1.6 TFLOP/s for 2D and 3D stencil computation, respectively.", "venue": "FPGA", "authors": ["Hamid Reza Zohouri", "Artur  Podobas", "Satoshi  Matsuoka"], "year": 2018, "n_citations": 70}
{"id": 4513016, "s2_id": "599c71d941ed2337e0cf55f07257db382fa1d53f", "title": "Applying the Residue Number System to Network Inference", "abstract": "This work explores the lesser studied objective of optimizing the multiply-and-accumulates executed during evaluation of the network. In particular, we propose using the Residue Number System (RNS) as the internal number representation across all layer evaluations, allowing us to explore usage of the more power-efficient RNS multipliers and adders. Using results from simulation of our RNS arithmetic block implementations, we show theoretical power advantages of using RNS for an end-to-end evaluator.", "venue": "ArXiv", "authors": ["Mohamed  Abdelhamid", "Skanda  Koppula"], "year": 2017, "n_citations": 2}
{"id": 4513025, "s2_id": "899a6603361c6155694907a7c0f5c7c2b09a42dc", "title": "Privacy leakages in approximate adders", "abstract": "Approximate computing has recently emerged as a promising method to meet the low power requirements of digital designs. The erroneous outputs produced in approximate computing can be partially a function of each chip's process variation. We show that, in such schemes, the erroneous outputs produced on each chip instance can reveal the identity of the chip that performed the computation, possibly jeopardizing user privacy. In this work, we perform simulation experiments on 32-bit Ripple Carry Adders, Carry Lookahead Adders, and Han-Carlson Adders running at over-scaled operating points. Our results show that identification is possible, we contrast the identifiability of each type of adder, and we quantify how success of identification varies with the extent of over-scaling and noise. Our results are the first to show that approximate digital computations may compromise privacy. Designers of future approximate computing systems should be aware of the possible privacy leakages and decide whether mitigation is warranted in their application.", "venue": "2017 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Shahrzad  Keshavarz", "Daniel  Holcomb"], "year": 2017, "n_citations": 7}
{"id": 4516976, "s2_id": "3a470c855ac56cfd3824c8090de8847b99f7bfb0", "title": "Kickstarting high-performance energy-efficient manycore architectures with Epiphany", "abstract": "In this paper we introduce Epiphany as a highperformance energy-efficient manycore architecture suitable for real-time embedded systems. This scalable architecture supports floating point operations in hardware and achieves 50 GFLOPS/W in 28 nm technology, making it suitable for high performance streaming applications like radio base stations and radar signal processing. Through an efficient 2D mesh Network-on-Chip and a distributed shared memory model, the architecture is scalable to thousands of cores on a single chip. An Epiphany-based open source computer named Parallella was launched in 2012 through Kickstarter crowd funding and has now shipped to thousands of customers around the world.", "venue": "2014 48th Asilomar Conference on Signals, Systems and Computers", "authors": ["Andreas  Olofsson", "Tomas  Nordstr\u00f6m", "Zain-ul-Abdin"], "year": 2014, "n_citations": 97}
{"id": 4517907, "s2_id": "de7703cdeca39e4cb3e308be99df29983e8a3537", "title": "Design Automation for Binarized Neural Networks: A Quantum Leap Opportunity?", "abstract": "Design automation in general, and in particular logic synthesis, can play a key role in enabling the design of application-specific Binarized Neural Networks (BNN). This paper presents the hardware design and synthesis of a purely combinational BNN for ultra-low power near-sensor processing. We leverage the major opportunities raised by BNN models, which consist mostly of logical bit-wise operations and integer counting and comparisons, for pushing ultra-low power deep learning circuits close to the sensor and coupling them with binarized mixed-signal image sensor data. We analyze area, power and energy metrics of BNNs synthesized as combinational networks. Our synthesis results in GlobalFoundries 22 nm SOI technology shows a silicon area of 2.61 mm2 for implementing a combinational BNN with 32\u00d732 binary input sensor receptive field and weight parameters fixed at design time. This is 2.2\u00d7 smaller than a synthesized network with re-configurable parameters. With respect to other comparable techniques for deep learning near-sensor processing, our approach features a 10\u00d7 higher energy efficiency.", "venue": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Manuele  Rusci", "Lukas  Cavigelli", "Luca  Benini"], "year": 2018, "n_citations": 12}
{"id": 4518581, "s2_id": "814a50b1e2b3af63d9a10f17d22983208705c1ef", "title": "A New, Computationally Efficient \u201cBlech Criterion\u201d for Immortality in General Interconnects", "abstract": "Traditional methodologies for analyzing electromigration (EM) in VLSI circuits first filter immortal wires using Blech\u2019s criterion, and then perform detailed EM analysis on the remaining wires. However, Blech\u2019s criterion was designed for two-terminal wires and does not extend to general structures. This paper demonstrates a first-principles-based solution technique for determining the steady-state stress at all the nodes of a general interconnect structure, and develops an immortality test whose complexity is linear in the number of edges of an interconnect structure. The proposed model is applied to a variety of structures. The method is shown to match well with results from numerical solvers, to be scalable to large structures.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Mohammad Abdullah Al Shohel", "Vidya A. Chhabria", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 3}
{"id": 4521121, "s2_id": "cac149b4955d659db48ebe001257f93119a8c88d", "title": "Soft-Error and Hard-fault Tolerant Architecture and Routing Algorithm for Reliable 3D-NoC Systems", "abstract": "Network-on-Chip (NoC) paradigm has been proposed as an auspicious solution to handle the strict communication requirements between the increasingly large number of cores on a single multi and many-core chips. However, NoC systems are exposed to a variety of manufacturing, design and energetic particles factors making them vulnerable to permanent (hard) faults and transient (soft) errors. In this paper, we present a comprehensive soft error and hard fault tolerant 3D-NoC architecture, named 3D-Hard-Fault-Soft-Error-Tolerant-OASIS-NoC (3D-FETO). With the aid of adaptive algorithms, 3D-FETO is capable of detecting and recovering from soft errors occurring in the routing pipeline stages and is leveraging on reconfigurable components to handle permanent faults occurrence in links, input buffers, and crossbar. In-depth evaluation results show that the 3D-FETO system is able to work around different kinds of hard faults and soft errors while ensuring graceful performance degradation, minimizing the additional hardware complexity and remaining power-efficient.", "venue": "ArXiv", "authors": ["Khanh N. Dang", "Yuichi  Okuyama", "Ben A. Abderazek"], "year": 2020, "n_citations": 0}
{"id": 4521494, "s2_id": "cf66efc963280c9883a0596422b4998c9a93f65b", "title": "Custom Tailored Suite of Random Forests for Prefetcher Adaptation", "abstract": "To close the gap between memory and processors, and in turn improve performance, there has been an abundance of work in the area of data/instruction prefetcher designs. Prefetchers are deployed in each level of the memory hierarchy, but typically, each prefetcher gets designed without comprehensively accounting for other prefetchers in the system. As a result, these individual prefetcher designs do not always complement each other, and that leads to low average performance gains and/or many negative outliers. In this work, we propose SuitAP (Suite of random forests for Adaptation of Prefetcher system configuration), which is a hardware prefetcher adapter that uses a suite of random forests to determine at runtime which prefetcher should be ON at each memory level, such that they complement each other. Compared to a design with no prefetchers, using SuitAP we improve IPC by 46% on average across traces generated from SPEC2017 suite with 12KB overhead. Moreover, we also reduce negative outliers using SuitAP.", "venue": "ArXiv", "authors": ["Furkan  Eris", "Sadullah  Canakci", "Cansu  Demirkiran", "Ajay  Joshi"], "year": 2020, "n_citations": 0}
{"id": 4522995, "s2_id": "ae5e7794aaff65590e265bdf4d65c44f6fa05064", "title": "Dual-side Sparse Tensor Core", "abstract": "Leveraging sparsity in deep neural network (DNN) models is promising for accelerating model inference. Yet existing GPUs can only leverage the sparsity from weights but not activations, which are dynamic, unpredictable, and hence challenging to exploit. In this work, we propose a novel architecture to efficiently harness the dual-side sparsity (i.e., weight and activation sparsity). We take a systematic approach to understand the (dis)advantages of previous sparsity-related architectures and propose a novel, unexplored paradigm that combines outer-product computation primitive and bitmap-based encoding format. We demonstrate the feasibility of our design with minimal changes to the existing production-scale inner-product-based Tensor Core. We propose a set of novel ISA extensions and co-design the matrix-matrix multiplication and convolution algorithms, which are the two dominant computation patterns in today\u2019s DNN models, to exploit our new dual-side sparse Tensor Core. Our evaluation shows that our design can fully unleash the dual-side DNN sparsity and improve the performance by up to one order of magnitude with small hardware overhead.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Yang  Wang", "Chen  Zhang", "Zhiqiang  Xie", "Cong  Guo", "Yunxin  Liu", "Jingwen  Leng"], "year": 2021, "n_citations": 2}
{"id": 4525936, "s2_id": "1b2b54595f72a4a32258c8368196e7d1942e6d0e", "title": "The short-term memory (d.c. response) of the memristor demonstrates the causes of the memristor frequency effect", "abstract": "A memristor is often identified by showing its distinctive pinched hysteresis curve and testing for the effect of frequency. The hysteresis size should relate to frequency and shrink to zero as the frequency approaches infinity. Although mathematically understood, the material causes for this are not well known. The d.c. response of the memristor is a decaying curve with its own timescale. We show via mathematical reasoning that this decaying curve when transformed to a.c. leads to the frequency effect by considering a descretized curve. We then demonstrate the validity of this approach with experimental data from two different types of memristors.", "venue": "2014 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Ella  Gale", "Ben de Lacy Costello", "Victor  Erokhin", "Andrew  Adamatzky"], "year": 2014, "n_citations": 8}
{"id": 4526507, "s2_id": "54c5ebda60fbe795a2d54bd435fd7af0aaea4c04", "title": "Optimal Metastability-Containing Sorting via Parallel Prefix Computation", "abstract": "Friedrichs <italic>et al.</italic> (TC 2018) showed that metastability can be <italic>contained</italic> when sorting inputs arising from time-to-digital converters, i.e., measurement values can be correctly sorted <italic>without</italic> resolving metastability using synchronizers first. However, this work left open whether this can be done by small circuits. We show that this is indeed possible, by providing a circuit that sorts Gray code inputs (possibly containing a metastable bit) and has asymptotically optimal depth and size. Our solution utilizes the parallel prefix computation (PPC) framework (JACM 1980). We improve this construction by bounding its fan-out by an arbitrary <inline-formula><tex-math notation=\"LaTeX\">$f\\geq 3$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>f</mml:mi><mml:mo>\u2265</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"bund-ieq1-2939818.gif\"/></alternatives></inline-formula>, without affecting depth and increasing circuit size by a small constant factor only. Thus, we obtain the first PPC circuits with asymptotically optimal size, constant fan-out, and optimal depth. To show that applying the PPC framework to the sorting task is feasible, we prove that the latter can, despite potential metastability, be decomposed such that the core operation is associative. We obtain asymptotically optimal metastability-containing sorting networks. We complement these results with simulations, independently verifying the correctness as well as small size and delay of our circuits. Proofs are omitted in this version; the article with full proofs is provided online at <uri>http://arxiv.org/abs/1911.00267</uri>.", "venue": "IEEE Transactions on Computers", "authors": ["Johannes  Bund", "Christoph  Lenzen", "Moti  Medina"], "year": 2020, "n_citations": 2}
{"id": 4529016, "s2_id": "a9a472a687b2d292b3ceb25f639cc9022dff7d39", "title": "Hardware Implementation of Algorithm for Cryptanalysis", "abstract": "Cryptanalysis of block ciphers involves massive computations which are independent of each other and can be instantiated simultaneously so that the solution space is explored at a faster rate. With the advent of low cost Field Programmable Gate Arrays, building special purpose hardware for computationally intensive applications has now become possible. For this the Data Encryption Standard is used as a proof of concept. This paper presents the design for Hardware implementation of DES cryptanalysis on FPGA using exhaustive key search. Two architectures viz. Rolled and Unrolled DES architecture are compared and based on experimental result the Rolled architecture is implemented on FPGA. The aim of this work is to make cryptanalysis faster and better.", "venue": "ArXiv", "authors": ["Harshali D. Zodpe", "Prakash W. Wani", "Rakesh R. Mehta"], "year": 2013, "n_citations": 0}
{"id": 4529858, "s2_id": "ee76cb175393f19188aa12e5bf94b11f289f6470", "title": "Enabling Efficient Dynamic Resizing of Large DRAM Caches via A Hardware Consistent Hashing Mechanism", "abstract": "Die-stacked DRAM has been proposed for use as a large, high-bandwidth, last-level cache with hundreds or thousands of megabytes of capacity. Not all workloads (or phases) can productively utilize this much cache space, however. Unfortunately, the unused (or under-used) cache continues to consume power due to leakage in the peripheral circuitry and periodic DRAM refresh. Dynamically adjusting the available DRAM cache capacity could largely eliminate this energy overhead. However, the current proposed DRAM cache organization introduces new challenges for dynamic cache resizing. The organization diers from a conventional SRAM cache organization because it places entire cache sets and their tags within a single bank to reduce on-chip area and power overhead. Hence, resizing a DRAM cache requires remapping sets from the powered-down banks to active banks. In this paper, we propose CRUNCH (Cache Resizing Using Native Consistent Hashing), a hardware data remapping scheme inspired by consistent hashing, an algorithm originally proposed to uniformly and dynamically distribute Internet trac across a changing population of web servers. CRUNCH provides a load-balanced remapping of data from the powered-down banks alone to the active banks, without requiring sets from all banks to be remapped, unlike naive schemes to achieve load balancing. CRUNCH remaps only sets from the powered-down banks, so it achieves this load balancing with low bank power-up/down transition latencies. CRUNCH\u2019s combination of good load balancing and low transition latencies provides a substrate to enable ecient DRAM cache resizing.", "venue": "ArXiv", "authors": ["Kevin Kai-Wei Chang", "Gabriel H. Loh", "Mithuna  Thottethodi", "Yasuko  Eckert", "Mike  O'Connor", "Srilatha  Manne", "Lisa  Hsu", "Lavanya  Subramanian", "Onur  Mutlu"], "year": 2016, "n_citations": 4}
{"id": 4532158, "s2_id": "cbfa97ef6407401f5509a55d1c77d29db00691e5", "title": "Positive/Negative Approximate Multipliers for DNN Accelerators", "abstract": "Recent Deep Neural Networks (DNNs) manage to deliver superhuman accuracy levels on many AI tasks. DNN accelerators are becoming integral components of modern systems-on-chips. DNNs perform millions of arithmetic operations per inference and DNN accelerators integrate thousands of multiply-accumulate units leading to increased energy requirements. To lower the energy consumption of DNN accelerators, approximate computing principles are employed. However, complex DNNs can be increasingly sensitive to approximation. In this work, we present a dynamically configurable approximate multiplier that supports three operation modes, i.e., exact, positive error, and negative error. In addition, we propose a filter-oriented approximation method to map the weights to the appropriate modes of the approximate multiplier. Our mapping algorithm balances the positive with the negative errors due to the approximate multiplications, aiming at maximizing the energy reduction while minimizing the overall convolution error. We evaluate our approach on multiple DNNs and datasets against state-of-the-art approaches, where our method achieves 18.33% energy gains on average across 7 NNs on 4 different datasets for a maximum accuracy drop of only 1%.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Ourania  Spantidi", "Georgios  Zervakis", "Iraklis  Anagnostopoulos", "Hussam  Amrouch", "Jorg  Henkel"], "year": 2021, "n_citations": 1}
{"id": 4532478, "s2_id": "80c6090a734f3f34e4f63434bee4ecfb9221bf65", "title": "A Novel Approach for Designing Online Testable Reversible Circuits", "abstract": "Reversible logic is gaining interest of many researchers due to its low power dissipating characteristic. In this paper we proposed a new approach for designing online testable reversible circuits. The resultant testable reversible circuit can detect any single bit error whiles it is operating. Appropriate theorems and lemmas are presented to clarify the proposed design. The experimental results show that our design approach is superior in terms of number of number of gates, garbage outputs and quantum cost.", "venue": "ArXiv", "authors": ["Md. Selim Al Mamun", "Pronab Kumar Mondal", "Uzzal Kumar Prodhan"], "year": 2013, "n_citations": 1}
{"id": 4533740, "s2_id": "ddc5b25638c6b4471fc884de1c7991e27088b225", "title": "Emulated ASIC Power and Temperature Monitor System for FPGA Prototyping of an Invasive MPSoC Computing Architecture", "abstract": "In this contribution the emulation of an ASIC temperature and power monitoring system (TPMon) for FPGA prototyping is presented and tested to control processor temperatures under different control targets and operating strategies. The approach for emulating the power monitor is based on an instruction-level energy model. For emulating the temperature monitor, a thermal RC model is used. The monitoring system supplies an invasive MPSoC computing architecture with hardware status information (power and temperature data of the processors within the system). These data are required for resource-aware load distribution. As a proof of concept different operating strategies and control targets were evaluated for a 2-tile invasive MPSoC computing system.", "venue": "ArXiv", "authors": ["Elisabeth  Glocker", "Qingqing  Chen", "Asheque M. Zaidi", "Ulf  Schlichtmann", "Doris  Schmitt-Landsiedel"], "year": 2014, "n_citations": 0}
{"id": 4536725, "s2_id": "905e2fdee5e30a8aee32f2008854bf543f7293a7", "title": "Fast and Scalable Sparse Triangular Solver for Multi-GPU Based HPC Architectures", "abstract": "Designing efficient and scalable sparse linear algebra kernels on modern multi-GPU based HPC systems is a challenging task due to significant irregular memory references and workload imbalance across GPUs. These challenges are particularly compounded in the case of Sparse Triangular Solver (SpTRSV), which introduces additional complexity of two-dimensional computation dependencies among subsequent computation steps. Dependency information may need to be exchanged and shared among GPUs, thus warranting for efficient memory allocation, data partitioning, and workload distribution as well as fine-grained communication and synchronization support. In this work, we focus on designing algorithm for SpTRSV in a single-node, multi-GPU setting. We demonstrate that directly adopting unified memory can adversely affect the performance of SpTRSV on multi-GPU architectures, despite linking via fast interconnect like NVLinks and NVSwitches. Alternatively, we employ the latest NVSHMEM technology based on Partitioned Global Address Space programming model to enable efficient fine-grained communication and drastic synchronization overhead reduction. Furthermore, to handle workload imbalance, we propose a malleable task-pool execution model which can further enhance the utilization of GPUs. By applying these techniques, our experiments on the NVIDIA multi-GPU supernode V100-DGX-1 and DGX-2 systems demonstrate that our design can achieve an average of 3.53 \u00d7 (up to 9.86 \u00d7) speedup on a DGX-1 system and 3.66 \u00d7 (up to 9.64 \u00d7) speedup on a DGX-2 system with four GPUs over the Unified-Memory design. The comprehensive sensitivity and scalability studies also show that the proposed zero-copy SpTRSV is able to fully utilize the computing and communication resources of the multi-GPU systems.", "venue": "ICPP", "authors": ["Chenhao  Xie", "Jieyang  Chen", "Jesun S Firoz", "Jiajia  Li", "Shuaiwen Leon Song", "Kevin  Barker", "Mark  Raugas", "Ang  Li"], "year": 2021, "n_citations": 1}
{"id": 4538152, "s2_id": "93fd8ceb99dca4d2529adcee1a241bfb4396c61e", "title": "ReaLPrune: ReRAM Crossbar-aware Lottery Ticket Pruned CNNs", "abstract": "ReRAM-based architectures offer highperformance yet energy efficient computing platforms for CNN training/inferencing. However, ReRAM-based architectures are not scalable with the size of the CNN. Larger CNNs have more weights, which requires more ReRAM cells that cannot be integrated in a single chip. Pruning is an effective way to solve this problem. However, existing pruning techniques are either targeted for inferencing only, or they are not crossbar-aware. This leads to sub-optimal hardware savings and performance benefits for CNN training on ReRAM-based architectures. In this paper, we address this problem by proposing a novel crossbar-aware pruning strategy, referred as ReaLPrune, which can prune more than 90% of CNN weights. The pruned model can be trained from scratch without any accuracy loss. Experimental results indicate that ReaLPrune reduces hardware requirements by 77.2% and accelerates CNN training by ~ 20 \u00d7 compared to unpruned CNNs. ReaLPrune also outperforms other state-of-the-art crossbar-aware pruning techniques in terms of both performance and hardware savings. Keywords\u2014CNN training, ReRAM, Pruning, Accelerators", "venue": "ArXiv", "authors": ["Biresh Kumar Joardar", "Janardhan Rao Doppa", "Hai  Li", "Krishnendu  Chakrabarty", "Partha Pratim Pande"], "year": 2021, "n_citations": 0}
{"id": 4543863, "s2_id": "897c693869fb691dc9a72c83c71e0d2ae53fec19", "title": "Page Tables: Keeping them Flat and Hot (Cached)", "abstract": "As memory capacity has outstripped TLB coverage, applications that use large data sets suffer from frequent page table walks. The underlying problem is that traditional tree-based page tables are designed to save memory capacity through the use of fine-grained indirection, which is a poor match for today's systems which have significant memory capacity but high latencies for indirections. \nIn this work we reduce the penalty of page table walks by both reducing the number of indirections required for page walks and by reducing the number of main memory accesses required for the indirections. We achieve this by first flattening the page table by allocating page table nodes in large pages. This results in fewer levels in the page table, which reduces the number of indirections needed to walk it. While the use of large pages in the page table does waste memory capacity for unused entries, this cost is small compared to using large pages for data as the page table itself is far smaller. Second we prioritize caching page table entries during phases of high TLB misses. This slightly increases data misses, but does so during phases of low-locality, and the resulting decrease in page walk latency outweighs this loss. \nBy flattening the page table we are able to reduce the number of indirections needed for a page walk from 4 to 2 in a non-virtualized system and from 24 to 8 under virtualization. However, the actual effect is much smaller as the Page Walker/Paging Structure Cache already captures most locality in the first two levels of the page table. Overall we are able to reduce the number of main memory accesses per page walk from 1.6 to 1.0 for non-virtualized systems and from 4.6 to 3.0 for virtualized systems.", "venue": "ArXiv", "authors": ["Chang Hyun Park", "Ilias  Vougioukas", "Andreas  Sandberg", "David  Black-Schaffer"], "year": 2020, "n_citations": 0}
{"id": 4545710, "s2_id": "f0fdd7c52549d9e71bb45b9535005653ea59ac44", "title": "Barrier-Free Large-Scale Sparse Tensor Accelerator (BARISTA) For Convolutional Neural Networks", "abstract": "Convolutional neural networks (CNNs) are emerging as powerful tools for visual recognition. Recent architecture proposals for sparse CNNs exploit natural and transformed zeros in the feature maps and filters for performance and energy without losing accuracy. Sparse architectures that exploit two-sided sparsity in both feature maps and filters have been studied only at small scales (e.g., 1K multiply-accumulate units (MACs)). However, to realize their advantages in full, the sparse architectures have to be scaled up to levels of the dense architectures (e.g., 32K MACs in the TPU). Such scaling is challenging because achieving reuse through broadcasts incurs implicit barrier cost which raises the inter-related issues of load imbalance, buffering, and on-chip bandwidth demand. SparTen, a previous scheme, addresses one aspect of load balancing but not other aspects, nor the other issues of buffering and bandwidth. To that end, we propose the barrier-free large-scale sparse tensor accelerator (BARISTA). BARISTA (1) is the first architecture for scaling up sparse CNN accelerators; (2) reduces on-chip bandwidth demand by telescoping request-combining the input map requests and snarfing the filter requests; (3) reduces buffering via basic buffer sharing and avoids the ensuing barriers between consecutive input maps by coloring the output buffers; (4) load balances intra-filter work via dynamic round-robin work assignment; and (5) employs hierarchical buffering which achieves high cache bandwidth via a few, wide, shared buffers and low buffering via narrower, private buffers at the compute. Our simulations show that, on average, BARISTA performs 5.4x, 2.2x, 1.7x, 2.5x better than a dense, a one-sided, a naively-scaled two-sided, and an isoarea two-sided architecture, respectively. Using 45-nm technology, ASIC synthesis of our RTL implementation for four clusters of 8K MACs each reports 1 GHz clock speed, 213 mm2 area and 170 W power.", "venue": "ArXiv", "authors": ["Ashish  Gondimalla", "Sree Charan Gundabolu", "T. N. Vijaykumar", "Mithuna  Thottethodi"], "year": 2021, "n_citations": 0}
{"id": 4546391, "s2_id": "af23974dd4e1880a2d795384921714da8dcbca0a", "title": "Neighbors From Hell: Voltage Attacks Against Deep Learning Accelerators on Multi-Tenant FPGAs", "abstract": "Field-programmable gate arrays (FPGAs) are becoming widely used accelerators for a myriad of datacenter applications due to their flexibility and energy efficiency. Among these applications, FPGAs have shown promising results in accelerating low-latency real-time deep learning (DL) inference, which is becoming an indispensable component of many end-user applications. With the emerging research direction towards virtualized cloud FPGAs that can be shared by multiple users, the security aspect of FPGA-based DL accelerators requires careful consideration. In this work, we evaluate the security of DL accelerators against voltage-based integrity attacks in a multi-tenant FPGA scenario. We first demonstrate the feasibility of such attacks on a state-of-the-art Stratix 10 card using different attacker circuits that are logically and physically isolated in a separate attacker role, and cannot be flagged as malicious circuits by conventional bitstream checkers. We show that aggressive clock gating, an effective power-saving technique, can also be a potential security threat in modern FPGAs. Then, we carry out the attack on a DL accelerator running ImageNet classification in the victim role to evaluate the inherent resilience of DL models against timing faults induced by the adversary. We find that, even when using the strongest attacker circuit, the prediction accuracy of the DL accelerator is not compromised when running at its safe operating frequency. Furthermore, we can achieve 1.18-1.31\u00d7 higher inference performance by over-clocking the DL accelerator without affecting its prediction accuracy.", "venue": "2020 International Conference on Field-Programmable Technology (ICFPT)", "authors": ["Andrew  Boutros", "Mathew  Hall", "Nicolas  Papernot", "Vaughn  Betz"], "year": 2020, "n_citations": 3}
{"id": 4548276, "s2_id": "e4f9bfb525b8e76c28995c3fd9cd451f86822a5b", "title": "On-chip test infrastructure design for optimal multi-site testing of system chips", "abstract": "Multi-site testing is a popular and effective way to increase test throughput and reduce test costs. We present a test throughput model, in which we focus on wafer testing, and consider parameters like test time, index time, abort-on-fail, and contact yield. Conventional multi-site testing requires sufficient ATE resources, such as ATE channels, to allow the test of multiple SOCs in parallel. In this paper, we design and optimize on-chip DfT in order to maximize the test throughput for a given SOC and ATE. The onchip DfT consists of an E-RPCT wrapper, and, for modular SOCs, module wrappers and TAMs. We present experimental results for a Philips SOC and several ITC'02 SOC test benchmarks.", "venue": "Design, Automation and Test in Europe", "authors": ["Sandeep Kumar Goel", "Erik Jan Marinissen"], "year": 2005, "n_citations": 16}
{"id": 4550086, "s2_id": "db078206a2f763fc85ceb65d12ccdb4593caa380", "title": "CDTP Chain Distributed Transfer Protocol", "abstract": "The rapid growth of the internet in general and of bandwidth capacity at internet clients in particular poses increasing computation and bandwidth demands on internet servers. Internet access technologies like ADSL [DSL], Cable Modem and Wireless modem allow internet clients to access the internet with orders of magnitude more bandwidth than using traditional modems. We present CDTP a distributed transfer protocol that allows clients to cooperate and therefore remove the strain from the internet server thus achieving much better performance than traditional transfer protocols (e.g. FTP [FTP]). The CDTP server and client tools are presented also as well as results of experiments. Finally a bandwidth measurement technique is presented. CDTP tools use this technique to differentiate between slow and fast clients.", "venue": "ArXiv", "authors": ["Shmuel  Vagner"], "year": 2004, "n_citations": 0}
{"id": 4554237, "s2_id": "a9a5fb382080eb141735f7b3b099a31ce358e416", "title": "Peer to Peer Networks for Defense Against Internet Worms", "abstract": "Internet worms, which spread in computer networks without human mediation, pose a severe threat to computer systems today. The rate of propagation of worms has been measured to be extremely high and they can infect a large fraction of their potential hosts in a short time. We study two different methods of patch dissemination to combat the spread of worms. We first show that using a fixed number of patch servers performs inadequately against Internet worms. We then show that by exploiting the exponential data dissemination capability of P2P systems, the spread of worms can be halted effectively. We compare the two methods by using fluid models to compute two quantities of interest: the time taken to effectively combat the progress of the worm, and the maximum number of infected hosts. We validate our models using simulations.", "venue": "IEEE Journal on Selected Areas in Communications", "authors": ["Srinivas  Shakkottai", "R.  Srikant"], "year": 2007, "n_citations": 26}
{"id": 4559146, "s2_id": "67b11e6c40d36760e197e58b9944ceba8858b838", "title": "Using Propagation for Solving Complex Arithmetic Constraints", "abstract": "Solving a system of nonlinear inequalities is an important problem for which conventional numerical analysis has no satisfactory method. With a box-consistency algorithm one can compute a cover for the solution set to arbitrarily close approximation. Because of difficulties in the use of propagation for complex arithmetic expressions, box consistency is computed with interval arithmetic. In this paper we present theorems that support a simple modification of propagation that allows complex arithmetic expressions to be handled efficiently. The version of box consistency that is obtained in this way is stronger than when interval arithmetic is used.", "venue": "ArXiv", "authors": ["M. H. van Emden", "Belaid  Moa"], "year": 2003, "n_citations": 1}
{"id": 4562541, "s2_id": "b40032881c6dbcca58dc0922fb20fbcbea186776", "title": "Zero Aware Configurable Data Encoding by Skipping Transfer for Error Resilient Applications", "abstract": "Data transfer across DRAM channels accounts for nearly a quarter of the total energy consumption of DDR4 DRAMs. Modern applications with high bandwidth requirements further increase channel energy consumption. However, channel energy consumption is dependent on data being transferred. Pseudo Open Drain (POD) asymmetric termination, used in current DDR4 systems, consumes energy only when 1\u2019s are being transmitted over the channels. Many modern applications, including AI/ML ones are resilient to errors in data, and can work well with approximate data. This resilience can vary widely across and within applications, which provides a number of ways for exploiting these characteristics to save data transfer energy across the DRAM channel. However, all DRAM data encoding schemes have been targeted towards applications that require exact data and are not approximation resilient. In this paper, we propose Zero Aware Configurable Data Encoding by Skipping Transfer (ZAC-DEST), a data encoding scheme to reduce the energy consumption of DRAM channels, specifically targeted towards approximate computing and error resilient applications. ZAC-DEST exploits the similarity between recent data transfers across channels and information about error resilience behaviour of applications to reduce on-die termination and switching energy by reducing the number of 1\u2019s transmitted over the channels. ZAC-DEST also provides a number of knobs for trading off application\u2019s accuracy for energy savings, and vice versa, and can be applied to both training and inference. We apply ZAC-DEST to five machine learning applications. On average, across all applications and configurations, we observed a reduction of 40% in termination energy and 37% in switching energy as compared to the state of the art data encoding technique BD-Coder with an average output quality loss of 10%. We show that if both training and testing are done assuming the presence of ZAC-DEST, the output quality of the applications can be improved upto $9\\times $ as compared to when ZAC-DEST is only applied during testing leading to energy savings during training and inference with increased output quality.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Chandan Kumar Jha", "Shreyas  Singh", "Riddhi  Thakker", "Manu  Awasthi", "Joycee  Mekie"], "year": 2021, "n_citations": 0}
{"id": 4564471, "s2_id": "a955f00285db7b730c6354cacde092a3326eee1f", "title": "Adaptive-Latency DRAM (AL-DRAM)", "abstract": "This paper summarizes the idea of Adaptive-Latency DRAM (AL-DRAM), which was published in HPCA 2015. The key goal of AL-DRAM is to exploit the extra margin that is built into the DRAM timing parameters to reduce DRAM latency. The key observation is that the timing parameters are dictated by the worst-case temperatures and worst-case DRAM cells, both of which lead to small amount of charge storage and hence high access latency. One can therefore reduce latency by adapting the timing parameters to the current operating temperature and the current DIMM that is being accessed. Using an FPGA-based testing platform, our work first characterizes the extra margin for 115 DRAM modules from three major manufacturers. The experimental results demonstrate that it is possible to reduce four of the most critical timing parameters by a minimum/maximum of 17.3%/54.8% at 55C while maintaining reliable operation. AL-DRAM adaptively selects between multiple different timing parameters for each DRAM module based on its current operating condition. AL-DRAM does not require any changes to the DRAM chip or its interface; it only requires multiple different timing parameters to be specified and supported by the memory controller. Real system evaluations show that AL-DRAM improves the performance of memory-intensive workloads by an average of 14% without introducing any errors.", "venue": "ArXiv", "authors": ["Donghyuk  Lee", "Yoongu  Kim", "Gennady  Pekhimenko", "Samira Manabi Khan", "Vivek  Seshadri", "Kevin Kai-Wei Chang", "Onur  Mutlu"], "year": 2016, "n_citations": 0}
{"id": 4574393, "s2_id": "158fd0ff5701b15ddea63f06f4e10592f069a947", "title": "FPGA-based Multi-Chip Module for High-Performance Computing", "abstract": "Current integration, architectural design and manufacturing technologies are not suited for the computing density and power efficiency requested by Exascale computing. New approaches in hardware architecture are thus needed to overcome the technological barriers preventing the transition to the Exascale era. In that scope, we report successful fabrication of first ExaNoDe's MCM prototypes dedicated to Exascale computing applications. Each MCM was composed of 2 Xilinx Zynq Ultrascale+ MPSoC, assembled on advanced 68.5 mm x 55 mm laminate substrates specifically designed and fabricated for the project. Acoustic microscopy, x-ray, cross-section and Thermo-Moire investigations revealed no voids, shorts, delamination, cracks or warpage issues. Two MCMs were mounted on a daughter board by FORTH for testing purposes. The DDR memories on the 4 SODIMMs of the daughter board were successfully tested by running extensive Xilinx memory tests with clock frequencies of 1866 MHz and 2133 MHz. All 4 FPGAs were programmed with the Xilinx integrated bit error ratio test (IBERT) tailored for this board for links testing. All intra-board high-speed links between all FPGAs were stable at 10 Gbps, even under the more demanding 31-bit PRBS (Pseudorandom Binary Sequence) tests.", "venue": "ArXiv", "authors": ["Yann  Beilliard", "Maxime  Godard", "Aggelos  Ioannou", "Astrinos  Damianakis", "Michael  Ligerakis", "Iakovos  Mavroidis", "Pierre-Yves  Martinez", "David  Danovitch", "Julien  Sylvestre", "Dominique  Drouin"], "year": 2019, "n_citations": 0}
{"id": 4578295, "s2_id": "1243f1b6d3df83961fde788c9c2da90ac646ed68", "title": "APNN-TC: accelerating arbitrary precision neural networks on ampere GPU tensor cores", "abstract": "Over the years, accelerating neural networks with quantization has been widely studied. Unfortunately, prior efforts with diverse precisions (e.g., 1-bit weights and 2-bit activations) are usually restricted by limited precision support on GPUs (e.g., int1 and int4). To break such restrictions, we introduce the first Arbitrary Precision Neural Network framework (APNN-TC)1 to fully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically, APNN-TC first incorporates a novel emulation algorithm to support arbitrary short bit-width computation with int1 compute primitives and XOR/AND Boolean operations. Second, APNN-TC integrates arbitrary precision layer designs to efficiently map our emulation algorithm to Tensor Cores with novel batching strategies and specialized memory organization. Third, APNN-TC embodies a novel arbitrary precision NN design to minimize memory access across layers and further improve performance. Extensive evaluations show that APNN-TC can achieve significant speedup over CUTLASS kernels and various NN models, such as ResNet and VGG.", "venue": "SC", "authors": ["Boyuan  Feng", "Yuke  Wang", "Tong  Geng", "Ang  Li", "Yufei  Ding"], "year": 2021, "n_citations": 1}
{"id": 4584188, "s2_id": "0e1362b4b73f7081501077f225aae84a790d1e7e", "title": "Efficient FPGA Floorplanning for Partial Reconfiguration-Based Applications", "abstract": "This paper introduces an efficient automatic floorplanning algorithm, which takes into account the heterogeneous architectures of modern FPGA families, as well as partial reconfiguration (PR) constraints, introducing the aspect ratio (AR) constraint to optimize routing. The algorithm generates possible placements of the partial modules, and then applies a recursive pseudo-bipartitioning heuristic search to find the best floorplan. The experiments show that its performance is significantly better than the one of other algorithms in this field.", "venue": "2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Norbert  Deak", "Octavian  Cret", "Horia  Hedesiu"], "year": 2019, "n_citations": 2}
{"id": 4590800, "s2_id": "c841fcd728e7fe12af5f02c898e99859432f42ff", "title": "Beyond the Memory Wall: A Case for Memory-Centric HPC System for Deep Learning", "abstract": "As the models and the datasets to train deep learning (DL) models scale, system architects are faced with new challenges, one of which is the memory capacity bottleneck, where the limited physical memory inside the accelerator device constrains the algorithm that can be studied. We propose a memory-centric deep learning system that can transparently expand the memory capacity available to the accelerators while also providing fast inter-device communication for parallel training. Our proposal aggregates a pool of memory modules locally within the device-side interconnect, which are decoupled from the host interface and function as a vehicle for transparent memory capacity expansion. Compared to conventional systems, our proposal achieves an average 2.8x speedup on eight DL applications and increases the system-wide memory capacity to tens of TBs.", "venue": "2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Youngeun  Kwon", "Minsoo  Rhu"], "year": 2018, "n_citations": 30}
{"id": 4592499, "s2_id": "309e2adeff6358a67e59c89a3b04f49243e94daf", "title": "End-to-end 100-TOPS/W Inference With Analog In-Memory Computing: Are We There Yet?", "abstract": "In-Memory Acceleration (IMA) promises major efficiency improvements in deep neural network (DNN) inference, but challenges remain in the integration of IMA within a digital system. We propose a heterogeneous architecture coupling 8 RISC-V cores with an IMA in a shared-memory cluster, analyzing the benefits and trade-offs of in-memory computing on the realistic use case of a MobileNetV2 bottleneck layer. We explore several IMA integration strategies, analyzing performance, area, and energy efficiency. We show that while pointwise layers achieve significant speed-ups over software implementation, on depthwise layer the inability to efficiently map parameters on the accelerator leads to a significant trade-off between throughput and area. We propose a hybrid solution where pointwise convolutions are executed on IMA while depthwise on the cluster cores, achieving a speed-up of 3x over SW execution while saving 50% of area when compared to an all-in IMA solution with similar performance.", "venue": "2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)", "authors": ["Gianmarco  Ottavi", "Geethan  Karunaratne", "Francesco  Conti", "Irem  Boybat", "Luca  Benini", "Davide  Rossi"], "year": 2021, "n_citations": 0}
{"id": 4593073, "s2_id": "8dbd80d4719ebd5cc15c64932524c7e04d75f9a0", "title": "Input-Output Logic based Fault-Tolerant Design Technique for SRAM-based FPGAs", "abstract": "Effects of radiation on electronic circuits used in extra-terrestrial applications and radiation prone environments need to be corrected. Since FPGAs offer flexibility, the effects of radiation on them need to be studied and robust methods of fault tolerance need to be devised. In this paper a new fault-tolerant design strategy has been presented. This strategy exploits the relation between changes in inputs and the expected change in output. Essentially, it predicts whether or not a change in the output is expected and thereby calculates the error. As a result this strategy reduces hardware and time redundancy required by existing strategies like Duplication with Comparison (DWC) and Triple Modular Redundancy (TMR). The design arising from this strategy has been simulated and its robustness to fault-injection has been verified. Simulations for a 16 bit multiplier show that the new design strategy performs better than the state-of-the-art on critical factors such as hardware redundancy, time redundancy and power consumption.", "venue": "ArXiv", "authors": ["Aditya Srinivas Timmaraju", "Aniket Anand Deshmukh", "Mohammed Amir Khan", "Zafar Ali Khan"], "year": 2013, "n_citations": 0}
{"id": 4593484, "s2_id": "dff4f9c2cab4b29dc0b219f64b358e3841598292", "title": "Intelligent Management of Mobile Systems through Computational Self-Awareness", "abstract": "Runtime resource management for many-core systems is increasingly complex. The complexity can be due to diverse workload characteristics with conflicting demands, or limited shared resources such as memory bandwidth and power. Resource management strategies for many-core systems must distribute shared resource(s) appropriately across workloads, while coordinating the high-level system goals at runtime in a scalable and robust manner. \nTo address the complexity of dynamic resource management in many-core systems, state-of-the-art techniques that use heuristics have been proposed. These methods lack the formalism in providing robustness against unexpected runtime behavior. One of the common solutions for this problem is to deploy classical control approaches with bounds and formal guarantees. Traditional control theoretic methods lack the ability to adapt to (1) changing goals at runtime (i.e., self-adaptivity), and (2) changing dynamics of the modeled system (i.e., self-optimization). \nIn this chapter, we explore adaptive resource management techniques that provide self-optimization and self-adaptivity by employing principles of computational self-awareness, specifically reflection. By supporting these self-awareness properties, the system can reason about the actions it takes by considering the significance of competing objectives, user requirements, and operating conditions while executing unpredictable workloads.", "venue": "ArXiv", "authors": ["Bryan  Donyanavard", "Amir M. Rahmani", "Axel  Jantsch", "Onur  Mutlu", "Nikil  Dutt"], "year": 2020, "n_citations": 2}
{"id": 4602753, "s2_id": "3aef285d724db47d1218c09a0cb7c25573828c4b", "title": "CoSA: Scheduling by Constrained Optimization for Spatial Accelerators", "abstract": "Recent advances in Deep Neural Networks (DNNs) have led to active development of specialized DNN accelerators, many of which feature a large number of processing elements laid out spatially, together with a multi-level memory hierarchy and flexible interconnect. While DNN accelerators can take advantage of data reuse and achieve high peak throughput, they also expose a large number of runtime parameters to the programmers who need to explicitly manage how computation is scheduled both spatially and temporally. In fact, different scheduling choices can lead to wide variations in performance and efficiency, motivating the need for a fast and efficient search strategy to navigate the vast scheduling space.To address this challenge, we present CoSA, a constrained-optimization-based approach for scheduling DNN accelerators. As opposed to existing approaches that either rely on designers\u2019 heuristics or iterative methods to navigate the search space, CoSA expresses scheduling decisions as a constrained-optimization problem that can be deterministically solved using mathematical optimization techniques. Specifically, CoSA leverages the regularities in DNN operators and hardware to formulate the DNN scheduling space into a mixed-integer programming (MIP) problem with algorithmic and architectural constraints, which can be solved to automatically generate a highly efficient schedule in one shot. We demonstrate that CoSA-generated schedules significantly outperform state-of-the-art approaches by a geometric mean of up to 2.5\u00d7 across a wide range of DNN networks while improving the time-to-solution by 90\u00d7.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Qijing  Huang", "Minwoo  Kang", "Grace  Dinh", "Thomas  Norell", "Aravind  Kalaiah", "James  Demmel", "John  Wawrzynek", "Yakun Sophia Shao"], "year": 2021, "n_citations": 3}
{"id": 4602811, "s2_id": "fcde5d2ca6742d407bd0ce5921180f08803d47e4", "title": "E-BATCH: Energy-Efficient and High-Throughput RNN Batching", "abstract": "Recurrent Neural Network (RNN) inference exhibits low hardware utilization due to the strict data dependencies across time-steps. Batching multiple requests can increase throughput. However, RNN batching requires a large amount of padding since the batched input sequences may largely differ in length. Schemes that dynamically update the batch every few time-steps avoid padding. However, they require executing different RNN layers in a short timespan, decreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and energy-efficient batching scheme tailored to RNN accelerators. It consists of a runtime system and effective hardware support. The runtime concatenates multiple sequences to create large batches, resulting in substantial energy savings. Furthermore, the accelerator notifies it when the evaluation of a sequence is done, so that a new sequence can be immediately added to a batch, thus largely reducing the amount of padding. E-BATCH dynamically controls the number of time-steps evaluated per batch to achieve the best trade-off between latency and energy efficiency for the given hardware platform. We evaluate E-BATCH on top of E-PUR and TPU. In E-PUR, E-BATCH improves throughput by 1.8x and energy-efficiency by 3.6x, whereas in TPU, it improves throughput by 2.1x and energy-efficiency by 1.6x, over the state-of-the-art.", "venue": "ArXiv", "authors": ["Franyell  Silfa", "Jose Maria Arnau", "Antonio  Gonzalez"], "year": 2020, "n_citations": 1}
{"id": 4602923, "s2_id": "b55035046bd1c5b9f0fae7796b79866e90e4f4d7", "title": "Q-SpiNN: A Framework for Quantizing Spiking Neural Networks", "abstract": "A prominent technique for reducing the memory footprint of Spiking Neural Networks (SNNs) without decreasing the accuracy significantly is quantization. However, the state-of-the-art only focus on employing the weight quantization directly from a specific quantization scheme, i.e., either the post-training quantization (PTQ) or the in-training quantization (ITQ), and do not consider (1) quantizing other SNN parameters (e.g., neurons' membrane potential), (2) exploring different combinations of quantization approaches (i.e., quantization schemes, precision levels, and rounding schemes), and (3) selecting the SNN model with a good memory-accuracy trade-off at the end. Therefore, the memory saving offered by these state-of-the-art to meet the targeted accuracy is limited, thereby hindering processing SNNs on the resource-constrained systems (e.g., the IoT-Edge devices). Towards this, we propose Q-SpiNN, a novel quantization framework for memory-efficient SNNs. The key mechanisms of the Q-SpiNN are: (1) employing quantization for different SNN parameters based on their significance to the accuracy, (2) exploring different combinations of quantization schemes, precision levels, and rounding schemes to find efficient SNN model candidates, and (3) developing an algorithm that quantifies the benefit of the memory-accuracy trade-off obtained by the candidates, and selects the Pareto-optimal one. The experimental results show that, for the unsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while maintaining the accuracy within 1% from the baseline on the MNIST dataset. For the supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping the accuracy within 2% from the baseline on the DVS-Gesture dataset.", "venue": "2021 International Joint Conference on Neural Networks (IJCNN)", "authors": ["Rachmad Vidya Wicaksana Putra", "Muhammad  Shafique"], "year": 2021, "n_citations": 4}
{"id": 4603349, "s2_id": "47735c7f210dee764be89d8aea04e787b8c35310", "title": "AISC: Approximate Instruction Set Computer", "abstract": "This paper makes the case for a single-ISA heterogeneous computing platform, AISC, where each compute engine (be it a core or an accelerator) supports a different subset of the very same ISA. An ISA subset may not be functionally complete, but the union of the (per compute engine) subsets renders a functionally complete, platform-wide single ISA. Tailoring the microarchitecture of each compute engine to the subset of the ISA that it supports can easily reduce hardware complexity. At the same time, the energy efficiency of computing can improve by exploiting algorithmic noise tolerance: by mapping code sequences that can tolerate (any potential inaccuracy induced by) the incomplete ISA-subsets to the corresponding compute engines.", "venue": "ArXiv", "authors": ["Alexandra  Ferreron", "Jes\u00fas  Alastruey-Bened\u00e9", "Dar\u00edo Su\u00e1rez Gracia", "Ulya R. Karpuzcu"], "year": 2018, "n_citations": 1}
{"id": 4606359, "s2_id": "e3cf24d8cfdd41700f80ea2827dbe42fda5953f3", "title": "Towards Latency-aware DNN Optimization with GPU Runtime Analysis and Tail Effect Elimination", "abstract": "Despite the superb performance of State-Of-The-Art (SOTA) DNNs, the increasing computational cost makes them very challenging to meet real-time latency and accuracy requirements. Although DNN runtime latency is dictated by model property (e.g., architecture, operations), hardware property (e.g., utilization, throughput), and more importantly, the effective mapping between these two, many existing approaches focus only on optimizing model property such as FLOPS reduction and overlook the mismatch between DNN model and hardware properties. In this work, we show that the mismatch between the varied DNN computation workloads and GPU capacity can cause the idle GPU tail effect, leading to GPU under-utilization and low throughput. As a result, the FLOPs reduction cannot bring effective latency reduction, which causes sub-optimal accuracy versus latency trade-offs. Motivated by this, we propose a GPU runtime-aware DNN optimization methodology to eliminate such GPU tail effect adaptively on GPU platforms. Our methodology can be applied on top of existing SOTA DNN optimization approaches to achieve better latency and accuracy trade-offs. Experiments show 11%-27% latency reduction and 2.5%-4.0% accuracy improvement over several SOTA DNN pruning and NAS methods, respectively", "venue": "ArXiv", "authors": ["Fuxun  Yu", "Zirui  Xu", "Tong  Shen", "Dimitrios  Stamoulis", "Longfei  Shangguan", "Di  Wang", "Rishi  Madhok", "Chunshui  Zhao", "Xin  Li", "Nikolaos  Karianakis", "Dimitrios  Lymberopoulos", "Ang  Li", "ChenChen  Liu", "Yiran  Chen", "Xiang  Chen"], "year": 2020, "n_citations": 2}
{"id": 4608331, "s2_id": "f12def1e00023a727ea2cbd80929148d7f961dd5", "title": "Virtual-Link: A Scalable Multi-Producer Multi-Consumer Message Queue Architecture for Cross-Core Communication", "abstract": "Cross-core communication is increasingly a bottleneck as the number of processing elements increase per systemon-chip. Typical hardware solutions to cross-core communication are often inflexible; while software solutions are flexible, they have performance scaling limitations. A key problem, as we will show, is that of shared state in software-based message queue mechanisms. This paper proposes Virtual-Link (VL), a novel light-weight communication mechanism with hardware support to facilitate M:N lock-free data movement. VL reduces the amount of coherent shared state, which is a bottleneck for many approaches, to zero. VL provides further latency benefit by keeping data on the fast path (i.e., within the onchip interconnect). VL enables directed cache-injection (stashing) between PEs on the coherence bus, reducing the latency for coreto-core communication. VL is particularly effective for fine-grain tasks on streaming data. Evaluation on a full system simulator with 7 benchmarks shows that VL achieves a $2.09\\times$ speedup over state-of-the-art software-based communication mechanisms, while reducing memory traffic by 61%.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Qinzhe  Wu", "Jonathan  Beard", "Ashen  Ekanayake", "Andreas  Gerstlauer", "Lizy K. John"], "year": 2021, "n_citations": 0}
{"id": 4609004, "s2_id": "8622aa63e3e12d53f84b4c3c6f067da5c945a965", "title": "NAX: Co-Designing Neural Network and Hardware Architecture for Memristive Xbar based Computing Systems", "abstract": "In-Memory Computing (IMC) hardware using Memristive Crossbar Arrays (MCAs) are gaining popularity to accelerate Deep Neural Networks (DNNs) since it alleviates the \u201dmemory wall\u201d problem associated with von-Neumann architecture. The hardware efficiency (energy, latency and area) as well as application accuracy (considering device and circuit non-idealities) of DNNs mapped to such hardware are codependent on network parameters, such as kernel size, depth etc. and hardware architecture parameters such as crossbar size. However, co-optimization of both network and hardware parameters presents a challenging search space comprising of different kernel sizes mapped to varying crossbar sizes. To that effect, we propose NAX \u2013 an efficient neural architecture search engine that co-designs neural network and IMC based hardware architecture. NAX explores the aforementioned search space to determine kernel and corresponding crossbar sizes for each DNN layer to achieve optimal tradeoffs between hardware efficiency and application accuracy. Our results from NAX show that the networks have heterogeneous crossbar sizes across different network layers, and achieves optimal hardware efficiency and accuracy considering the non-idealities in crossbars. On CIFAR-10 and Tiny ImageNet, our models achieve 0.8%, 0.2% higher accuracy, and 17%, 4% lower EDAP (energy-delayarea product) compared to a baseline ResNet-20 and ResNet-18 models, respectively.", "venue": "ArXiv", "authors": ["Shubham  Negi", "Indranil  Chakraborty", "Aayush  Ankit", "Kaushik  Roy"], "year": 2021, "n_citations": 0}
{"id": 4615870, "s2_id": "164297dfd8cf58d35e639bd46281cf4832ca9f0f", "title": "FPGA Implementation of Stair Matrix based Massive MIMO Detection", "abstract": "Approximate matrix inversion based methods is widely used for linear massive multiple-input multiple-output (MIMO) received symbol vector detection. Such detectors typically utilize the diagonally dominant channel matrix of a massive MIMO system. Instead of diagonal matrix, a stair matrix can be utilized to improve the error-rate performance of a massive MIMO detector. In this paper, we present very large-scale integration (VLSI) architecture and field programmable gate array (FPGA) implementation of a stair matrix based iterative detection algorithm. The architecture supports a base station with 128 antennas, 8 users with single antenna, and 256 quadrature amplitude modulation (QAM). The stair matrix based detector can deliver a 142.34 Mbps data rate and reach a clock frequency of 258 MHz in a Xilinx Virtex -7FPGA. The detector provides superior error-rate performance and higher scaled throughput than most contemporary massive MIMO detectors.", "venue": "2021 IEEE 12th Latin America Symposium on Circuits and System (LASCAS)", "authors": ["Shahriar  Shahabuddin", "Mahmoud A. M. Albreem", "Mohammad Shahanewaz Shahabuddin", "Zaheer  Khan", "Markku J. Juntti"], "year": 2021, "n_citations": 1}
{"id": 4616676, "s2_id": "389c8060a80377ae10fbd6691f0db290008753d9", "title": "Apollo: Transferable Architecture Exploration", "abstract": "The looming end of Moore\u2019s Law and ascending use of deep learning drives the design of custom accelerators that are optimized for specific neural architectures. Architecture exploration for such accelerators forms a challenging constrained optimization problem over a complex, high-dimensional, and structured input space with a costly to evaluate objective function. Existing approaches for accelerator design are sample-inefficient and do not transfer knowledge between related optimizations tasks with different design constraints, such as area and/or latency budget, or neural architecture configurations. In this work, we propose a transferable architecture exploration framework, dubbed APOLLO, that leverages recent advances in black-box function optimization for sample-efficient accelerator design. We use this framework to optimize accelerator configurations of a diverse set of neural architectures with alternative design constraints. We show that our framework finds high reward design configurations (up to 24.6% speedup) more sample-efficiently than a baseline black-box optimization approach. We further show that by transferring knowledge between target architectures with different design constraints, APOLLO is able to find optimal configurations faster and often with better objective value (up to 25% improvements). This encouraging outcome portrays a promising path forward to facilitate generating higher quality accelerators.", "venue": "ArXiv", "authors": ["Amir  Yazdanbakhsh", "Christof  Angermueller", "Berkin  Akin", "Yanqi  Zhou", "Albin  Jones", "Milad  Hashemi", "Kevin  Swersky", "Satrajit  Chatterjee", "Ravi  Narayanaswami", "James  Laudon"], "year": 2021, "n_citations": 5}
{"id": 4617262, "s2_id": "71b676fdf3ee3eec9c11089a957e55749a8334d5", "title": "A Survey of Machine Learning for Computer Architecture and Systems", "abstract": "It has been a long time that computer architecture and systems are optimized to enable efficient execution of machine learning (ML) algorithms or models. Now, it is time to reconsider the relationship between ML and systems, and let ML transform the way that computer architecture and systems are designed. This embraces a twofold meaning: the improvement of designers\u2019 productivity, and the completion of the virtuous cycle. In this paper, we present a comprehensive review of work that applies ML for system design, which can be grouped into two major categories, ML-based modelling that involves predictions of performance metrics or some other criteria of interest, and ML-based design methodology that directly leverages ML as the design tool. For ML-based modelling, we discuss existing studies based on their target level of system, ranging from the circuit level to the architecture/system level. For ML-based design methodology, we follow a bottom-up path to review current work, with a scope of (micro-)architecture design (memory, branch prediction, NoC), coordination between architecture/system and workload (resource allocation and management, data center management, and security), compiler, and design automation. We further provide a future vision of opportunities and potential directions, and envision that applying ML for computer architecture and systems would thrive in the community.", "venue": "ArXiv", "authors": ["Nan  Wu", "Yuan  Xie"], "year": 2021, "n_citations": 2}
{"id": 4617543, "s2_id": "542249c8f61e9ddc55b389f6a08cf3f828396a63", "title": "Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks", "abstract": "This article presents Neural Cache architecture, which repurposes cache structures to transform them into massively parallel compute units capable of running inferences for deep neural networks. Techniques to do in situ arithmetic in SRAM arrays create efficient data mapping, and reducing data movement is proposed. Neural Cache architecture is capable of fully executing convolutional, fully connected, and pooling layers in cache. Our experimental results show that the proposed architecture can improve efficiency over a GPU by $\\mathbf {128\\times}$128\u00d7 while requiring a minimal area overhead of 2%.", "venue": "IEEE Micro", "authors": ["Charles  Eckert", "Xiaowei  Wang", "Jingcheng  Wang", "Arun  Subramaniyan", "Dennis  Sylvester", "David  Blaauw", "Reetuparna  Das", "Ravi R. Iyer"], "year": 2019, "n_citations": 3}
{"id": 4619630, "s2_id": "674e25671aefeb960e000c717c539ee01f23c47c", "title": "A Parallel Bitstream Generator for Stochastic Computing", "abstract": "Stochastic computing (SC) presents high error tolerance and low hardware cost, and has great potential in applications such as neural networks and image processing. However, the bitstream generator, which converts a binary number to bitstreams, occupies a large area and energy consumption, thus weakening the superiority of SC. In this paper, we propose a novel technique for generating bitstreams in parallel, which needs only one clock for conversion and significantly reduces the hardware cost. Synthesis results demonstrate that the proposed parallel bitstream generator improves 2.5\u00d7 area and 712\u00d7 energy consumption.", "venue": "2019 Silicon Nanoelectronics Workshop (SNW)", "authors": ["Yawen  Zhang", "Runsheng  Wang", "Xinyue  Zhang", "Zherui  Zhang", "Jiahao  Song", "Zuodong  Zhang", "Yuan  Wang", "Ru  Huang"], "year": 2019, "n_citations": 5}
{"id": 4620551, "s2_id": "75c4c37d47f5bb01cec4f69e06fa28e69dc2ee63", "title": "A 75kb SRAM in 65nm CMOS for In-Memory Computing Based Neuromorphic Image Denoising", "abstract": "This paper presents an in-memory computing (IMC) architecture for image denoising. The proposed SRAM based inmemory processing framework works in tandem with approximate computing on a binary image generated from neuromorphic vision sensors. Implemented in TSMC 65nm process, the proposed architecture enables \ue306 2000X energy savings (\ue306 222X from IMC) compared to a digital implementation when tested with the video recordings from a DAVIS sensor and achieves a peak throughput of 1.25 \u2013 1.66 frames/\u03bcs.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Sumon Kumar Bose", "Vivek  Mohan", "Arindam  Basu"], "year": 2020, "n_citations": 2}
{"id": 4621842, "s2_id": "ce267f251cf6cafe4e402b7a508316225f1a841c", "title": "Is Leakage Power a Linear Function of Temperature?", "abstract": "In this work, we present a study of the leakage power modeling techniques commonly used in the architecture community. We further provide an analysis of the error in leakage power estimation using the various modeling techniques. We strongly believe that this study will help researchers determine an appropriate leakage model to use in their work, based on the desired modeling accuracy and speed.", "venue": "ArXiv", "authors": ["Hameedah  Sultan", "Shashank  Varshney", "Smruti R. Sarangi"], "year": 2018, "n_citations": 2}
{"id": 4624908, "s2_id": "df0653c66997f7dcce84aac9fb89fc3e1f89f5ec", "title": "XSP: Across-Stack Profiling and Analysis of Machine Learning Models on GPUs", "abstract": "There has been a rapid proliferation of machine learning/deep learning (ML) models and wide adoption of them in many application domains. This has made profiling and characterization of ML model performance an increasingly pressing task for both hardware designers and system providers, as they would like to offer the best possible system to serve ML models with the target latency, throughput, cost, and energy requirements while maximizing resource utilization. Such an endeavor is challenging as the characteristics of an ML model depend on the interplay between the model, framework, system libraries, and the hardware (or the HW/SW stack). Existing profiling tools are disjoint, however, and only focus on profiling within a particular level of the stack, which limits the thoroughness and usefulness of the profiling results.This paper proposes XSP \u2014 an across-stack profiling design that gives a holistic and hierarchical view of ML model execution. XSP leverages distributed tracing to aggregate and correlate profile data from different sources. XSP introduces a leveled and iterative measurement approach that accurately captures the latencies at all levels of the HW/SW stack in spite of the profiling overhead. We couple the profiling design with an automated analysis pipeline to systematically analyze 65 state-of-the-art ML models. We demonstrate that XSP provides insights which would be difficult to discern otherwise.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Cheng  Li", "Abdul  Dakkak", "Jinjun  Xiong", "Wei  Wei", "Lingjie  Xu", "Wen-mei  Hwu"], "year": 2020, "n_citations": 13}
{"id": 4626875, "s2_id": "2df127d38c607f6dc01ca53907d538fc0f10861a", "title": "PMEvo: portable inference of port mappings for out-of-order processors by evolutionary optimization", "abstract": "Achieving peak performance in a computer system requires optimizations in every layer of the system, be it hardware or software. A detailed understanding of the underlying hardware, and especially the processor, is crucial to optimize software. One key criterion for the performance of a processor is its ability to exploit instruction-level parallelism. This ability is determined by the port mapping of the processor, which describes the execution units of the processor for each instruction. Processor manufacturers usually do not share the port mappings of their microarchitectures. While approaches to automatically infer port mappings from experiments exist, they are based on processor-specific hardware performance counters that are not available on every platform. We present PMEvo, a framework to automatically infer port mappings solely based on the measurement of the execution time of short instruction sequences. PMEvo uses an evolutionary algorithm that evaluates the fitness of candidate mappings with an analytical throughput model formulated as a linear program. Our prototype implementation infers a port mapping for Intel's Skylake architecture that predicts measured instruction throughput with an accuracy that is competitive to existing work. Furthermore, it finds port mappings for AMD's Zen+ architecture and the ARM Cortex-A72 architecture, which are out of scope of existing techniques.", "venue": "PLDI", "authors": ["Fabian  Ritter", "Sebastian  Hack"], "year": 2020, "n_citations": 5}
{"id": 4629072, "s2_id": "14149d8414c6c351ef918a6153bdb9c7d355594b", "title": "A Multicore Processor based Real-Time System for Automobile management application", "abstract": "In this paper we propose an Intelligent Management System which is capable of managing the automobile functions using the rigorous real-time principles and a multicore processor in order to realize higher efficiency and safety for the vehicle. It depicts how various automobile functionalities can be fine grained and treated to fit in real time concepts. It also shows how the modern multicore processors can be of good use in organizing vast amounts of correlated functions to be executed in real-time with excellent time commitments. The modeling of the automobile tasks with real time commitments, organizing appropriate scheduling for various real time tasks and the usage of a multicore processor enables the system to realize higher efficiency and offer better safety levels to the vehicle. The industry available real time operating system is used for scheduling various tasks and jobs on the multicore processor.", "venue": "ArXiv", "authors": ["M.  Vaidehi", "T. R. Gopalakrishnan Nair"], "year": 2010, "n_citations": 0}
{"id": 4629177, "s2_id": "14672f7ed5f1aa7eb54d1e64c6347eb2a35345f4", "title": "Probabilistic Localization of Insect-Scale Drones on Floating-Gate Inverter Arrays", "abstract": "We propose a novel compute-in-memory (CIM)based ultra-low-power framework for probabilistic localization of insect-scale drones. Localization is a critical subroutine for pathplanning and rotor control in drones, where a drone is required to continuously estimate its pose (position and orientation) in flying space. The conventional probabilistic localization approaches rely on the three-dimensional (3D) Gaussian Mixture Model (GMM)based representation of a 3D map. The GMM map model is synthesized by scanning the 3D space and by modeling the density of matter. A GMM model with hundreds of mixture functions is typically needed to adequately learn and represent the intricacies of the map. Meanwhile, localization using complex GMM map models is computationally intensive. Since insect-scale drones operate under extremely limited area/power budget, continuous localization using GMM models entails much higher operating energy \u2013 thereby, limiting flying duration and/or size of the drone due to a larger battery. Addressing the computational challenges of localization in an insect-scale drone using a CIM approach, we propose a novel framework of 3D map representation using a harmonic mean of \u201cGaussian-like\u201d mixture (HMGM) model. We show that short-circuit current of a multi-input floating-gate CMOS-based inverter follows the harmonic mean of a Gaussianlike function. Therefore, the likelihood function useful for drone localization can be efficiently implemented by connecting many multi-input inverters in parallel, each programmed with the parameters of the 3D map model represented as HMGM. When the depth measurements are projected to the input of the implementation, the summed current of the inverters emulates the likelihood of the measurement. We have characterized our approach on an RGB-D indoor localization dataset. The average localization error in our approach is \u223c0.1125 m which is only slightly degraded than software-based evaluation where the average localization error is \u223c0.08 m. Meanwhile, our localization framework is ultra-low-power, consuming as little as \u223c17 \u03bcW power while processing a depth frame in 1.33 ms over hundred pose hypotheses in the particle-filtering (PF) algorithm used to localize the drone. Comparatively, a custom-designed 8-bit digital processor requires \u223c7.6 mW power for the same workload. Therefore, the proposed approach consumes \u223c450\u00d7 less energy than the traditional, paving the way for tiny autonomous drones.", "venue": "ArXiv", "authors": ["Priyesh  Shukla", "Ankith  Muralidhar", "Nick  Iliev", "Theja  Tulabandhula", "Sawyer B. Fuller", "Amit Ranjan Trivedi"], "year": 2021, "n_citations": 0}
{"id": 4637931, "s2_id": "689be261392865db35acb904e24e99bd605b33c0", "title": "TCIM: Triangle Counting Acceleration With Processing-In-MRAM Architecture", "abstract": "Triangle counting (TC) is a fundamental problem in graph analysis and has found numerous applications, which motivates many TC acceleration solutions in the traditional computing platforms like GPU and FPGA. However, these approaches suffer from the bandwidth bottleneck because TC calculation involves a large amount of data transfers. In this paper, we propose to overcome this challenge by designing a TC accelerator utilizing the emerging processing-in-MRAM (PIM) architecture. The true innovation behind our approach is a novel method to perform TC with bitwise logic operations (such as AND), instead of the traditional approaches such as matrix computations. This enables the efficient in-memory implementations of TC computation, which we demonstrate in this paper with computational Spin-Transfer Torque Magnetic RAM (STT-MRAM) arrays. Furthermore, we develop customized graph slicing and mapping techniques to speed up the computation and reduce the energy consumption. We use a device-to-architecture co-simulation framework to validate our proposed TC accelerator. The results show that our data mapping strategy could reduce 99.99% of the computation and 72% of the memory WRITE operations. Compared with the existing GPU or FPGA accelerators, our in-memory accelerator achieves speedups of 9\u00d7 and 23.4\u00d7, respectively, and a 20.6\u00d7 energy efficiency improvement over the FPGA accelerator.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Xueyan  Wang", "Jianlei  Yang", "Yinglin  Zhao", "Yingjie  Qi", "Meichen  Liu", "Xingzhou  Cheng", "Xiaotao  Jia", "Xiaoming  Chen", "Gang  Qu", "Weisheng  Zhao"], "year": 2020, "n_citations": 1}
{"id": 4640013, "s2_id": "b59a2931c57fa0b48b783e8cea64cc9f9fba653f", "title": "SeaPlace: Process Variation Aware Placement for Reliable Combinational Circuits against SETs and METs", "abstract": "Nowadays nanoscale combinational circuits are facing significant reliability challenges including soft errors and process variations. This paper presents novel process variation-aware placement strategies that include two algorithms to increase the reliability of combinational circuits against both Single Event Transients (SETs) andMultiple Event Transients (METs). The first proposed algorithm is a global placement method (called SeaPlace-G) that places the cells for hardening the circuit against SETs by solving a quadratic formulation. Afterwards, a detailed placement algorithm (named SeaPlace-D) is proposed to increase the circuit reliability against METs by solving a linear programming optimization problem. Experimental results show that SeaPlace-G and SeaPlace-D averagely achieve 41.78% and 32.04% soft error reliability improvement against SET and MET, respectively. Moreover, when SeaPlace-D is followed by SeaPlace-G, MET reduction can be improved by up to 53.3%.", "venue": "ArXiv", "authors": ["Kiarash  Saremi", "Hossein  Pedram", "Behnam  Ghavami", "Mohsen  Raji", "Zhenman  Fang", "Lesley  Shannon"], "year": 2021, "n_citations": 0}
{"id": 4644305, "s2_id": "ae464dc54a594de682fddd59479736b1e65bbf52", "title": "TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation", "abstract": "Accelerating tensor applications on spatial architectures provides high performance and energy-efficiency, but requires accurate performance models for evaluating various dataflow alternatives. Such modeling relies on the notation of tensor dataflow and the formulation of performance metrics. Recent proposed compute-centric and data-centric notations describe the dataflow using imperative directives. However, these two notations are less expressive and thus lead to limited optimization opportunities and inaccurate performance models.In this paper, we propose a framework TENET that models hardware dataflow of tensor applications. We start by introducing a relation-centric notation, which formally describes the hardware dataflow for tensor computation. The relation-centric notation specifies the hardware dataflow, PE interconnection, and data assignment in a uniform manner using relations. The relation-centric notation is more expressive than the compute-centric and data-centric notations by using more sophisticated affine transformations. Another advantage of relation-centric notation is that it inherently supports accurate metrics estimation, including data reuse, bandwidth, latency, and energy. TENET computes each performance metric by counting the relations using integer set structures and operators. Overall, TENET achieves 37.4% and 51.4% latency reduction for CONV and GEMM kernels compared with the state-of-the-art data-centric notation by identifying more sophisticated hardware dataflows.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Liqiang  Lu", "Naiqing  Guan", "Yuyue  Wang", "Liancheng  Jia", "Zizhang  Luo", "Jieming  Yin", "Jason  Cong", "Yun  Liang"], "year": 2021, "n_citations": 5}
{"id": 4646016, "s2_id": "efc7191382abedfab87836b546f52e6ed1ae6de8", "title": "Hardware support for arbitrarily complex loop structures in embedded applications", "abstract": "The program control unit of an embedded RISC processor is enhanced with a novel zero-overhead loop controller (ZOLC) supporting arbitrary loop structures with multiple-entry/exit nodes. The ZOLC has been incorporated to an open RISC processor core to evaluate the performance of the proposed unit for alternative configurations of the selected processor. It is proven that speed improvements of 8.4% to 48.2% are feasible for the used benchmarks.", "venue": "Design, Automation and Test in Europe", "authors": ["Nikolaos  Kavvadias", "Spiridon  Nikolaidis"], "year": 2005, "n_citations": 2}
{"id": 4648432, "s2_id": "77d3aef1b08db777bcea073f80733bc6a14491ca", "title": "Vis-TOP: Visual Transformer Overlay Processor", "abstract": "In recent years, Transformer [23] has achieved good results in Natural Language Processing (NLP) and has also started to expand into Computer Vision (CV). Excellent models such as the Vision Transformer [5] and Swin Transformer [17] have emerged. At the same time, the platform for Transformer models was extended to embedded devices to meet some resource-sensitive application scenarios. However, due to the large number of parameters, the complex computational flow and the many different structural variants of Transformer models, there are a number of issues that need to be addressed in its hardware design. This is both an opportunity and a challenge. We propose Vis-TOP (Visual Transformer Overlay Processor), an overlay processor for various visual Transformer models. It differs from coarse-grained overlay processors such as CPU, GPU, NPE, and from fine-grained customized designs for a specific model. Vis-TOP summarizes the characteristics of all visual Transformer models and implements a three-layer and two-level transformation structure that allows the model to be switched or changed freely without changing the hardware architecture. At the same time, the corresponding instruction bundle and hardware architecture are designed in three-layer and two-level transformation structure. After quantization of Swin Transformer tiny model using 8-bit fixed points (fix_8), we implemented an overlay processor on the ZCU102. Compared to GPU, the TOP throughput is 1.5x higher. Compared to the existing Transformer accelerators, our throughput per DSP is between 2.2x and 11.7x higher than others. In a word, the approach in this paper meets the requirements of real-time AI in terms of both resource consumption and inference speed. Vis-TOP provides a cost-effective and power-effective solution based on reconfigurable devices for computer vision at the edge.", "venue": "ArXiv", "authors": ["Wei  Hu", "Dian  Xu", "Zimeng  Fan", "Fang  Liu", "Yanxiang  He"], "year": 2021, "n_citations": 0}
{"id": 4650724, "s2_id": "57fb8c6dce376ddfe988a3510f1e110d3d099307", "title": "RVCoreP-32IM: An effective architecture to implement mul/div instructions for five stage RISC-V soft processors", "abstract": "RISC-V, an open instruction set architecture, is getting the attention of soft processor developers. Implementing only a basic 32-bit integer instruction set of RISC-V, which is defined as RV32I, might be satisfactory for embedded systems. However, multiplication and division instructions are not present in RV32I, rather than defined as M-extension. Several research projects have proposed both RV32I and RV32IM processor. However, there is no indication of how much performance can be improved by adding M-extension to RV32I. In other words, when we should consider adding M-extension into the soft processor and how much hardware resource requirements will increase. \nIn this paper, we propose an extension of the RVCoreP soft processor (which implements RV32I instruction set only) to support RISC-V M-extension instructions. A simple fork-join method is used to expand the execution capability to support M-extension instructions as well as a possible future enhancement. We then perform the benchmark using Dhrystone, Coremark, and Embench programs. We found that RV32IM is 1.87 and 3.13 times better in performance for radix-4 and DSP multiplier, respectively. In addition to that, our RV32IM implementation is 13\\% better than the equivalent RISC-V processor.", "venue": "ArXiv", "authors": ["Md. Ashraful Islam", "Hiromu  Miyazaki", "Kenji  Kise"], "year": 2020, "n_citations": 1}
{"id": 4651016, "s2_id": "f0936a8fb405abb4fb3c5fc8410a26fcaf3f38fd", "title": "Janus II: A new generation application-driven computer for spin-system simulations", "abstract": "This paper describes the architecture, the development and the implementation of Janus II, a new generation application-driven number cruncher optimized for Monte Carlo simulations of spin systems (mainly spin glasses). This domain of computational physics is a recognized grand challenge of high-performance computing: the resources necessary to study in detail theoretical models that can make contact with experimental data are by far beyond those available using commodity computer systems. On the other hand, several specific features of the associated algorithms suggest that unconventional computer architectures \u2013 that can be implemented with available electronics technologies \u2013 may lead to order of magnitude increases in performance, reducing to acceptable values on human scales the time needed to carry out simulation campaigns that would take centuries on commercially available machines. Janus II is one such machine, recently developed and commissioned, that builds upon and improves on the successful JANUS machine, which has been used for physics since 2008 and is still in operation today. This paper describes in detail the motivations behind the project, the computational requirements, the architecture and the implementation of this new machine and compares its expected performances with those of currently available commercial systems.", "venue": "Comput. Phys. Commun.", "authors": ["Marco  Baity-Jesi", "Rachel A. Ba\u00f1os", "Andr\u00e9s Cruz Flor", "Luis Antonio Fern\u00e1ndez", "Jos\u00e9 Miguel Gil-Narvi\u00f3n", "Antonio  Gordillo", "David  I\u00f1iguez", "Andrea  Maiorano", "Filippo  Mantovani", "Enzo  Marinari", "Victor  Martin-Mayor", "Jorge  Monforte-Garcia", "Antonio Mu\u00f1oz Sudupe", "Denis  Navarro", "Giorgio  Parisi", "Sergio Perez Gaviro", "Marcello  Pivanti", "Federico  Ricci-Tersenghi", "Juan Jesus Ruiz-Lorenzo", "Sebastiano Fabio Schifano", "Beatriz  Seoane", "Alfonso  Taranc\u00f3n", "Raffaele  Tripiccione", "David  Yllanes"], "year": 2014, "n_citations": 32}
{"id": 4651591, "s2_id": "b2dec507a6293930113360d1dd2c3f28dd074c44", "title": "Reinforcement Learning based Interconnection Routing for Adaptive Traffic Optimization", "abstract": "Applying Machine Learning (ML) techniques to design and optimize computer architectures is a promising research direction. Optimizing the runtime performance of a Network-on-Chip (NoC) necessitates a continuous learning framework. In this work, we demonstrate the promise of applying reinforcement learning (RL) to optimize NoC runtime performance. We present three RL-based methods for learning optimal routing algorithms. The experimental results show the algorithms can successfully learn a near-optimal solution across different environment states. Reproducible Code: github.com/huckiyang/interconnect-routing-gym", "venue": "ArXiv", "authors": ["Sheng-Chun  Kao", "Chao-Han Huck Yang", "Pin-Yu  Chen", "Xiaoli  Ma", "Tushar  Krishna"], "year": 2019, "n_citations": 2}
{"id": 4656616, "s2_id": "c8f3cd127acf76f29d6c625bcb2993f367d89ab3", "title": "ReGraphX: NoC-enabled 3D Heterogeneous ReRAM Architecture for Training Graph Neural Networks", "abstract": "Graph Neural Network (GNN) is a variant of Deep Neural Networks (DNNs) operating on graphs. However, GNNs are more complex compared to traditional DNNs as they simultaneously exhibit features of both DNN and graph applications. As a result, architectures specifically optimized for either DNNs or graph applications are not suited for GNN training. In this work, we propose a 3D heterogeneous manycore architecture for on-chip GNN training to address this problem. The proposed architecture, ReGraphX, involves heterogeneous ReRAM crossbars to fulfill the disparate requirements of both DNN and graph computations simultaneously. The ReRAM-based architecture is complemented with a multicast-enabled 3D NoC to improve the overall achievable performance. We demonstrate that ReGraphX outperforms conventional GPUs by up to 3.5X (on an average 3X) in terms of execution time, while reducing energy consumption by as much as 11X.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Aqeeb Iqbal Arka", "Biresh Kumar Joardar", "Janardhan Rao Doppa", "Partha Pratim Pande", "Krishnendu  Chakrabarty"], "year": 2021, "n_citations": 2}
{"id": 4656834, "s2_id": "02bb48b5dab6c7f62b6674da56322e4f57fbdb9e", "title": "Spintronics based stochastic computing for efficient Bayesian inference system", "abstract": "Bayesian inference is an effective approach for solving statistical learning problems especially with uncertainty and incompleteness. However, inference efficiencies are physically limited by the bottlenecks of conventional computing platforms. In this paper, an emerging Bayesian inference system is proposed by exploiting spintronics based stochastic computing. A stochastic bitstream generator is realized as the kernel components by leveraging the inherent randomness of spintronics devices. The proposed system is evaluated by typical applications of data fusion and Bayesian belief networks. Simulation results indicate that the proposed approach could achieve significant improvement on inference efficiencies in terms of power consumption and inference speed.", "venue": "2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Xiaotao  Jia", "Jianlei  Yang", "Zhaohao  Wang", "Yiran  Chen", "Hai  Li", "Weisheng  Zhao"], "year": 2018, "n_citations": 10}
{"id": 4657760, "s2_id": "7d38f733a22101bdc53fb33717a5fc5818a6d57d", "title": "On Delay Faults Affecting I/O Blocks of an SRAM-Based FPGA Due to Ionizing Radiations", "abstract": "Experimental means to characterize delay faults induced by bit flips and SEUs in I/O blocks of SRAM-based FPGAs are proposed. A delay fault up to 6.2ns sensitized by an events chain is reported.", "venue": "ArXiv", "authors": ["Fatima Zahra Tazi", "Claude  Thibeault", "Yvon  Savaria", "Simon  Pichette", "Yves  Audet"], "year": 2014, "n_citations": 1}
{"id": 4659283, "s2_id": "46bb5162da1ad9b3591a9e056550135e7a50bc2b", "title": "Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect", "abstract": "High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Ang  Li", "Shuaiwen  Song", "Jieyang  Chen", "Jiajia  Li", "Xu  Liu", "Nathan R. Tallent", "Kevin J. Barker"], "year": 2020, "n_citations": 69}
{"id": 4660998, "s2_id": "e4cef250bc66c6e94dabeeed7a8af8d18f098d00", "title": "ESP4ML: Platform-Based Design of Systems-on-Chip for Embedded Machine Learning", "abstract": "We present ESP4ML, an open-source system-level design flow to build and program SoC architectures for embedded applications that require the hardware acceleration of machine learning and signal processing algorithms. We realized ESP4ML by combining two established open-source projects (ESP and HLS4ML) into a new, fully-automated design flow. For the SoC integration of accelerators generated by HLS4ML, we designed a set of new parameterized interface circuits synthesizable with high-level synthesis. For accelerator configuration and management, we developed an embedded software runtime system on top of Linux. With this HW/SW layer, we addressed the challenge of dynamically shaping the data traffic on a network-on-chip to activate and support the reconfigurable pipelines of accelerators that are needed by the application workloads currently running on the SoC. We demonstrate our vertically-integrated contributions with the FPGA-based implementations of complete SoC instances booting Linux and executing computer-vision applications that process images taken from the Google Street View database", "venue": "2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Davide  Giri", "Kuan-Lin  Chiu", "Giuseppe Di Guglielmo", "Paolo  Mantovani", "Luca P. Carloni"], "year": 2020, "n_citations": 17}
{"id": 4663473, "s2_id": "3ccddb9316e82ce76cb70383c6c18a4a01596ad1", "title": "Lazy Batching: An SLA-aware Batching System for Cloud Machine Learning Inference", "abstract": "In cloud ML inference systems, batching is an essential technique to increase throughput which helps optimize total-cost-of-ownership. Prior graph batching combines the individual DNN graphs into a single one, allowing multiple inputs to be concurrently executed in parallel. We observe that the coarse-grained graph batching becomes suboptimal in effectively handling the dynamic inference request traffic, leaving significant performance left on the table. This paper proposes LazyBatching, an SLA-aware batching system that considers both scheduling and batching in the granularity of individual graph nodes, rather than the entire graph for flexible batching. We show that LazyBatching can intelligently determine the set of nodes that can be efficiently batched together, achieving an average $15\\times, 1.5\\times$, and $5.5\\times$ improvement than graph batching in terms of average response time, throughput, and SLA satisfaction, respectively.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Yujeong  Choi", "Yunseong  Kim", "Minsoo  Rhu"], "year": 2021, "n_citations": 3}
{"id": 4664395, "s2_id": "c55e0eb3fbd5ad3b8f0286332bdc799d20045244", "title": "LightOn Optical Processing Unit : Scaling-up AI and HPC with a Non von Neumann co-processor", "abstract": "Beyond pure Von Neumann processing Scalability of AI / HPC models is limited by the Von Neumann bottleneck for accessing massive amounts of memory, driving up power consumption.", "venue": "2021 IEEE Hot Chips 33 Symposium (HCS)", "authors": ["Charles  Brossollet", "Alessandro  Cappelli", "Igor  Carron", "Charidimos  Chaintoutis", "Am'elie  Chatelain", "Laurent  Daudet", "Sylvain  Gigan", "Daniel  Hesslow", "Florent  Krzakala", "Julien  Launay", "Safa  Mokaadi", "Fabien  Moreau", "Kilian  Muller", "Ruben  Ohana", "Gustave  Pariente", "Iacopo  Poli", "Giuseppe L. Tommasone"], "year": 2021, "n_citations": 2}
{"id": 4666623, "s2_id": "f9d1d6b8bb56e9036a0a787cdc003ab3ea907f0f", "title": "ReveR: Software Simulator of Reversible Processor with Stack", "abstract": "A software model of a reversible processor ReveR with the stack is discussed in this paper. An architecture, the minimal set of elementary reversible operations together with an implementation of the basic control flow structures and procedures calls using simple assembler language are described.", "venue": "ArXiv", "authors": ["Alexander Yu. Vlasov"], "year": 2011, "n_citations": 0}
{"id": 4667019, "s2_id": "b82d1bbdd040c1ce1d52f9ff3f34c7761e83c982", "title": "Compute RAMs: Adaptable Compute and Storage Blocks for DL-Optimized FPGAs", "abstract": "The configurable building blocks of current FPGAs \u2014 Logic blocks (LBs), Digital Signal Processing (DSP) slices, and Block RAMs (BRAMs) \u2014 make them efficient hardware accelerators for the rapid-changing world of Deep Learning (DL). Communication between these blocks happens through an interconnect fabric consisting of switching elements spread throughout the FPGA. In this paper, a new block, Compute RAM, is proposed. Compute RAMs provide highly-parallel processing-inmemory (PIM) by combining computation and storage capabilities in one block. Compute RAMs can be integrated in the FPGA fabric just like the existing FPGA blocks and provide two modes of operation (storage or compute) that can be dynamically chosen. They reduce power consumption by reducing data movement, provide adaptable precision support, and increase the effective on-chip memory bandwidth. Compute RAMs also help increase the compute density of FPGAs. In our evaluation of addition, multiplication and dot-product operations across multiple data precisions (int4, int8 and bfloat16), we observe an average savings of 80% in energy consumption, and an improvement in execution time ranging from 20% to 80%. Adding Compute RAMs can benefit non-DL applications as well, and make FPGAs more efficient, flexible, and performant accelerators.", "venue": "ArXiv", "authors": ["Aman  Arora", "Bagus  Hanindhito", "Lizy K. John"], "year": 2021, "n_citations": 0}
{"id": 4669762, "s2_id": "ccaace1c44e8bb5f6a2529e0c8bf4939c12a4063", "title": "Learning on Hardware: A Tutorial on Neural Network Accelerators and Co-Processors", "abstract": "Deep neural networks (DNNs) have the advantage that they can take into account a large number of parameters, which enables them to solve complex tasks. In computer vision and speech recognition, they have a better accuracy than common algorithms, and in some tasks, they boast an even higher accuracy than human experts. With the progress of DNNs in recent years, many other fields of application such as diagnosis of diseases and autonomous driving are taking advantage of them. The trend at DNNs is clear: The network size is growing exponentially, which leads to an exponential increase in computational effort and required memory size. For this reason, optimized hardware accelerators are used to increase the performance of the inference of neuronal networks. However, there are various neural network hardware accelerator platforms, such as graphics processing units (GPUs), application specific integrated circuits (ASICs) and field programmable gate arrays (FPGAs). Each of these platforms offer certain advantages and disadvantages. Also, there are various methods for reducing the computational effort of DNNs, which are differently suitable for each hardware accelerator. In this article an overview of existing neural network hardware accelerators and acceleration methods is given. Their strengths and weaknesses are shown and a recommendation of suitable applications is given. In particular, we focus on acceleration of the inference of convolutional neural networks (CNNs) used for image recognition tasks. Given that there exist many different hardware architectures. FPGA-based implementations are well-suited to show the effect of DNN optimization methods on accuracy and throughput. For this reason, the focus of this work is more on FPGA-based implementations.", "venue": "ArXiv", "authors": ["Lukas  Baischer", "Matthias  Wess", "Nima  TaheriNejad"], "year": 2021, "n_citations": 0}
{"id": 4670048, "s2_id": "6afba276d03f3c2c28cdd05e803556ae31884ac7", "title": "How Flexible is Your Computing System", "abstract": "In literature computer architectures are frequently claimed to be highly flexible, typically implying there exist trade-offs between flexibility and performance or energy efficiency. Processor flexibility, however, is not very sharply defined, and as such these claims can not be validated, nor can such hypothetical relations be fully understood and exploited in the design of computing systems. This paper is an attempt to introduce scientific rigour to the notion of flexibility in computing systems. A survey is conducted to compile an overview of references to flexibility in literature both in the computer architecture domain as well as related fields. A classification is introduced to categorize different views on flexibility, which ultimately form the foundation for a qualitative definition of flexibility. Departing from the qualitative definition of flexibility, a generic quantifiable metric is proposed, enabling valid quantitative comparison of the flexibility of various architectures. To validate the proposed method, and evaluate the relation between the proposed metric and the general notion of flexibility, the flexibility metric is measured for 25 computing systems, including CPUs, GPUs, DSPs, and FPGAs, and 40 ASIPs taken from literature. The obtained results provide insights into some of the speculative trade-offs between flexibility and properties such as energy efficiency and area efficiency. Overall the proposed quantitative flexibility metric shows to be commensurate with some generally accepted qualitative notions of flexibility collected in the survey, although some surprising discrepancies can also be observed. The proposed metric and the obtained results are placed into context of the state of the art on compute flexibility, and extensive reflection provides not only a complete overview of the field, but also discusses possible alternative approaches and open issues. Note that this work does not aim to provide a final answer to the definition of flexibility, but rather provides a framework to initiate a broader discussion in the computer architecture society on defining, understanding, and ultimately taking advantage of flexibility.", "venue": "ArXiv", "authors": ["Shihua  Huang", "Luc  Waeijen", "Henk  Corporaal"], "year": 2021, "n_citations": 0}
{"id": 4670322, "s2_id": "f1dc40ccb1aa8416f215e499e9c93753bc4b1798", "title": "Spiking memristor logic gates are a type of time-variant perceptron", "abstract": "Memristors are low-power memory-holding resistors thought to be useful for neuromophic computing, which can compute via spike-interactions mediated through the device's short-term memory. Using interacting spikes, it is possible to build an AND gate that computes OR at the same time, similarly a full adder can be built that computes the arithmetical sum of its inputs. Here we show how these gates can be understood by modelling the memristors as a novel type of perceptron: one which is sensitive to input order. The memristor's memory can change the input weights for later inputs, and thus the memristor gates cannot be accurately described by a single perceptron, requiring either a network of time-invarient perceptrons or a complex time-varying self-reprogrammable perceptron. This work demonstrates the high functionality of memristor logic gates, and also that the addition of theasholding could enable the creation of a standard perceptron in hardware, which may have use in building neural net chips.", "venue": "ArXiv", "authors": ["Ella M. Gale"], "year": 2018, "n_citations": 0}
{"id": 4671431, "s2_id": "15e938abd89f781fa7cf7c704d264fa42b44ebce", "title": "Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training", "abstract": "Personalized recommendations are one of the most widely deployed machine learning (ML) workload serviced from cloud datacenters. As such, architectural solutions for high-performance recommendation inference have recently been the target of several prior literatures. Unfortunately, little have been explored and understood regarding the training side of this emerging ML workload. In this paper, we first perform a detailed workload characterization study on training recommendations, root-causing sparse embedding layer training as one of the most significant performance bottlenecks. We then propose our algorithm-architecture co-design called Tensor Casting, which enables the development of a generic accelerator architecture for tensor gather-scatter that encompasses all the key primitives of training embedding layers. When prototyped on a real CPUGPU system, Tensor Casting provides $1.9 - 21 \\times$ improvements in training throughput compared to state-of-the-art approaches.", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "authors": ["Youngeun  Kwon", "Yunjae  Lee", "Minsoo  Rhu"], "year": 2021, "n_citations": 7}
{"id": 4672256, "s2_id": "d1a082fb5e84420c68ecb4bcdfa029d7771f2d02", "title": "A Transferable Approach for Partitioning Machine Learning Models on Multi-Chip-Modules", "abstract": "Multi-Chip-Modules (MCMs) reduce the design and fabrication cost of machine learning (ML) accelerators while delivering performance and energy efficiency on par with a monolithic large chip. However, ML compilers targeting MCMs need to solve complex optimization problems optimally and efficiently to achieve this high performance. One such problem is the multi-chip partitioning problem where compilers determine the optimal partitioning and placement of operations in tensor computation graphs on chiplets in MCMs. Partitioning ML graphs for MCMs is particularly hard as the search space grows exponentially with the number of chiplets available and the number of nodes in the neural network. Furthermore, the constraints imposed by the underlying hardware produce a search space where valid solutions are extremely sparse. In this paper, we present a strategy using a deep reinforcement learning (RL) framework to emit a possibly invalid candidate partition that is then corrected by a constraint solver. Using the constraint solver ensures that RL encounters valid solutions in the sparse space frequently enough to converge with fewer samples as compared to non-learned strategies. The architectural choices we make for the policy network allow us to generalize across different ML graphs. Our evaluation of a production-scale model, BERT, on real hardware reveals that the partitioning generated using RL policy achieves 6.11% and 5.85% higher throughput than random search and simulated annealing. In addition, fine-tuning the pre-trained RL policy reduces the search time from 3 hours to only 9 minutes, while achieving the same throughput as training RL policy from scratch.", "venue": "ArXiv", "authors": ["Xinfeng  Xie", "Prakash  Prabhu", "Ulysse  Beaugnon", "Phitchaya Mangpo Phothilimthana", "Sudip  Roy", "Azalia  Mirhoseini", "Eugene  Brevdo", "James  Laudon", "Yanqi  Zhou"], "year": 2021, "n_citations": 0}
{"id": 4682674, "s2_id": "2d92fe4323f1fac4f580fdce28b5ab06e9c79ceb", "title": "EHAP-ORAM: Efficient Hardware-Assisted Persistent ORAM System for Non-volatile Memory", "abstract": "Oblivious RAM (ORAM) protected access pattern is essential for secure NVM. In the ORAM system, data and PosMap metadata are maps in pairs to perform secure access. Therefore, we focus on the problem of crash consistency in the ORAM system. Unfortunately, using traditional software-based support for ORAM system crash consistency is not only expensive, it can also lead to information leaks. At present, there is no relevant research on the specific crash consistency mechanism supporting the ORAM system. To support crash consistency without damaging ORAM system security and compromising the performance, we propose EHAP-ORAM. Firstly, we analyze the access steps of basic ORAM to obtain the basic requirements to support the ORAM system crash consistency. Secondly, improve the ORAM controller. Thirdly, for the improved hardware system, we propose several persistence protocols supporting the ORAM system crash consistency. Finally, we compared our persistent ORAM with the system without crash consistency support, non-recursive and recursive EHAP-ORAM only incurs 3.36% and 3.65% performance overhead. The results show that EHAP-ORAM not only supports effective crash consistency with minimal performance and hardware overhead but also is friendly to NVM lifetime.", "venue": "ArXiv", "authors": ["Gang  Liu", "Kenli  Li", "Zheng  Xiao", "Rujia  Wang"], "year": 2020, "n_citations": 0}
{"id": 4684334, "s2_id": "f56588330a8468ea39fa7660598d653814a9b24c", "title": "Performance Evaluation of ECC in Single and Multi Processor Architectures on FPGA Based Embedded System", "abstract": "Cryptographic algorithms are computationally costly and the challenge is more if we need to execute them in resource constrained embedded systems. Field Programmable Gate Arrays (FPGAs) having programmable logic devices and processing cores, have proven to be highly feasible implementation platforms for embedded systems providing lesser design time and reconfigurability. Design parameters like throughput, resource utilization and power requirements are the key issues. The popular Elliptic Curve Cryptography (ECC), which is superior over other public-key crypto-systems like RSA in many ways, such as providing greater security for a smaller key size, is chosen in this work and the possibilities of its implementation in FPGA based embedded systems for both single and dual processor core architectures involving task parallelization have been explored. This exploration, which is first of its kind considering the other existing works, is a needed activity for evaluating the best possible architectural environment for ECC implementation on FPGA (Virtex4 XC4VFX12, FF668, -10) based embedded platform.", "venue": "ArXiv", "authors": ["Sruti  Agarwal", "Sangeet  Saha", "Rourab  Paul", "Amlan  Chakrabarti"], "year": 2014, "n_citations": 2}
{"id": 4686953, "s2_id": "85955c2f36ec71475373cf33636f071135657e74", "title": "Efficiently Reclaiming Space in a Log Structured Store", "abstract": "Modern storage devices do not support update-in-place. Rather, flash and shingled disks, are forms of log structured stores. Such a store writes a number of diverse and non-contiguous logical pages into a unit of contiguous storage we call a segment instead of using a write I/O to update each page in place. The result is that pages need to be relocated and remapped on every write. Log structuring was invented for and used initially to improve performance in file systems. Segments need to be garbage collected, but can be only when they no longer house any current pages. A process of \"cleaning\" produces an empty segment by, when necessary, moving (re-writing) still current pages of the segment to another location. Cleaning effectiveness has a major impact on the performance of modern storage devices, and for flash, impacts the rate of wear and hence the lifetime of the device. We analyze cleaning performance and introduce a cleaning strategy that uses a new way to prioritize the order in which segments are cleaned. Our cleaning strategy approximates an \"optimal cleaning strategy\". Simulation studies confirm the results of the analysis. This strategy is a significant improvement over previous cleaning strategies.", "venue": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "authors": ["David  Lomet", "Chen  Luo"], "year": 2021, "n_citations": 1}
{"id": 4693648, "s2_id": "367fced5991530c3269f903c5d29f0d2ca2f064a", "title": "Multi-Amdahl: Optimal Resource Sharing with Multiple Program Execution Segments", "abstract": "This paper presents Multi-Amdahl, a resource allocation analytical tool for heterogeneous systems. Our model includes multiple program execution segments, where each one is accelerated by a specific hardware unit. The acceleration speedup of the specific hardware unit is a function of a limited resource, such as the unit area, power, or energy. Using the Lagrange theorem we discover the optimal resource distribution between all specific units. We then illustrate this general Multi-Amdahl technique using several examples of area and power allocation among several cores and accelerators.", "venue": "ArXiv", "authors": ["Tsahee  Zidenberg", "Isaac  Keslassy", "Uri C. Weiser"], "year": 2011, "n_citations": 1}
{"id": 4697424, "s2_id": "b434af9d2702080136f2ed42457c7866abb0f280", "title": "Ascertaining Uncertainty for Efficient Exact Cache Analysis", "abstract": "Static cache analysis characterizes a program\u2019s cache behavior by determining in a sound but approximate manner which memory accesses result in cache hits and which result in cache misses. Such information is valuable in optimizing compilers, worst-case execution time analysis, and side-channel attack quantification and mitigation.", "venue": "CAV", "authors": ["Valentin  Touzeau", "Claire  Ma\u00efza", "David  Monniaux", "Jan  Reineke"], "year": 2017, "n_citations": 173}
{"id": 4701199, "s2_id": "67d1ef40666fec9f48546ee41d2dc861b4adbde8", "title": "A joint communication and application simulator for NoC-based SoCs", "abstract": "NoCs have become a widespread paradigm in the system-on-chip design world, not only for multi-purpose SoCs, but also for application-specific ICs. The common approach in the NoC design world is to separate the design of the interconnection from the design of the processing elements: this is well suited for a large number of developments, but the need for joint application and NoC design is not uncommon, especially in the application specific case. The correlation between processing and communication tasks can be strong, and separate or trace-based simulations fall often short of the desired precision. In this work, the OMNET++ based JANoCS simulator is presented: concurrent simulation of processing and communication allow cycle-accurate evaluation of the system. Two cases of study are presented, showing both the need for joint simulations and the effectiveness of JANoCS.", "venue": "ArXiv", "authors": ["Carlo  Condo", "Amer  Baghdadi", "Guido  Masera"], "year": 2013, "n_citations": 1}
{"id": 4703912, "s2_id": "715f56b3eaf9964a03ed4f74b83b0c548913cddb", "title": "HybridDNN: A Framework for High-Performance Hybrid DNN Accelerator Design and Implementation", "abstract": "To speedup Deep Neural Networks (DNN) accelerator design and enable effective implementation, we propose HybridDNN, a framework for building high-performance hybrid DNN accelerators and delivering FPGA-based hardware implementations. Novel techniques include a highly flexible and scalable architecture with a hybrid Spatial/Winograd convolution (CONV) Processing Engine (PE), a comprehensive design space exploration tool, and a complete design flow to fully support accelerator design and implementation. Experimental results show that the accelerators generated by HybridDNN can deliver 3375.7 and 83.3 GOPS on a high-end FPGA (VU9P) and an embedded FPGA (PYNQ-Z1), respectively, which achieve a 1.8x higher performance improvement compared to the state-of-art accelerator designs. This demonstrates that HybridDNN is flexible and scalable and can target both cloud and embedded hardware platforms with vastly different resource constraints.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Hanchen  Ye", "Xiaofan  Zhang", "Zhize  Huang", "Gengsheng  Chen", "Deming  Chen"], "year": 2020, "n_citations": 15}
{"id": 4705426, "s2_id": "32ec8b9fb7dc700e4d4c6806abbdb4ad2629ab5b", "title": "Multi-output, multi-level, multi-gate design using non-linear programming", "abstract": "Using logic gates is the traditional way of designing logic circuits. However, most of the minimization algorithms concern a limited set of gates (complete sets), like sum of products, exclusive-or sum of products, NAND gates, NOR gates e.t.c.. In this paper, a method is proposed for minimizing multi-output Boolean functions using any kind of two-input gates although it can easily be extended to multi-input gates. The method is based on non-linear mixed integer programming. The experimental results show that the method gives the same or better results compared to other methods available in the literature. However, other methods do not ensure that they produce the minimal solution, while the main advantages of the proposed method are that it does guarantee minimality and it can also handle Boolean functions for incompletely specified functions. The method is general enough and can easily be extended to more complicated design modules than just basic gates.", "venue": "ArXiv", "authors": ["A. C. Dimopoulos", "C.  Pavlatos", "G.  Papakonstantinou"], "year": 2021, "n_citations": 0}
{"id": 4712249, "s2_id": "63cbd3fa07584e62c568d0a5005438bbade63953", "title": "An overview about Networks-on-Chip with multicast suppor", "abstract": "Modern System-on-Chip (SoC) platforms typically consist of multiple processors and a communication interconnect between them. Network-on-Chip (NoC) arises as a solution to interconnect these systems, which provides a scalable, reusable, and an efficient interconnect. For these SoC platforms, multicast communication is significantly used for parallel applications. Cache coherency in distributed sharedmemory,clock synchronization, replication, or barrier synchronization are examples of these requests. This paper presents an overview of research on NoC with support for multicast communication and delineates the major issues addressed so far by the scientific community in this investigation area.", "venue": "ArXiv", "authors": ["Marcelo Daniel Berejuck"], "year": 2016, "n_citations": 1}
{"id": 4712792, "s2_id": "c7d7d9ac278f6c8e8a5c4adebd8b5765d1c2a84a", "title": "An FPGA-Based Hardware Accelerator for Energy-Efficient Bitmap Index Creation", "abstract": "Bitmap index is recognized as a promising candidate for online analytics processing systems, because it effectively supports not only parallel processing but also complex and multi-dimensional queries. However, bitmap index creation is a time-consuming task. In this paper, by taking full advantage of massive parallel computing of field-programmable gate array (FPGA), two hardware accelerators of bitmap index creation, namely BIC64K8 and BIC32K16, are originally proposed. Each of the accelerator contains two primary components, namely an enhanced content-addressable memory and a query logic array module, which allow BIC64K8 and BIC32K16 to index 65 536 8-bit words and 32 768 16-bit words in parallel, at every clock cycle. The experimental results on an Intel Arria V 5ASTFD5 FPGA prove that at 100 MHz, BIC64K8 and BIC32K16 achieve the approximate indexing throughput of 1.43 GB/s and 1.46 GB/s, respectively. The throughputs are also proven to be stable, regardless the size of the data sets. More significantly, BIC32K16 only consumes as low as 6.76% and 3.28% of energy compared to the central-processing-unit- and graphic-processing-unit-based designs, respectively.", "venue": "IEEE Access", "authors": ["Xuan-Thuan  Nguyen", "Trong-Thuc  Hoang", "Hong-Thu  Nguyen", "Katsumi  Inoue", "Cong-Kha  Pham"], "year": 2018, "n_citations": 13}
{"id": 4712820, "s2_id": "f5fa97f455278637c46f566f889ea77249bf78d8", "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical report)", "abstract": "To increase performance and efficiency, systems use FPGAs as reconfigurable accelerators. A key challenge in designing these systems is partitioning computation between processors and an FPGA. An appropriate division of labor may be difficult to predict in advance and require experiments and measurements. When an investigation requires rewriting part of the system in a new language or with a new programming model, its high cost can retard the study of different configurations. A single-language system with an appropriate programming model and compiler that targets both platforms simplifies this exploration to a simple recompile with new compiler directives. This work introduces StreamBlocks, an open-source compiler and runtime that uses the CAL dataflow programming language to partition computations across heterogeneous (CPU/accelerator) platforms. Because of the dataflow model\u2019s semantics and the CAL language, StreamBlocks can exploit both thread parallelism in multi-core CPUs and the inherent parallelism of FPGAs. StreamBlocks supports exploring the design space with a profile-guided tool that helps identify the best hardware-software partitions.", "venue": "ArXiv", "authors": ["Endri  Bezati", "Mahyar  Emami", "Jorn  Janneck", "James  Larus"], "year": 2021, "n_citations": 0}
{"id": 4713665, "s2_id": "f1e07e644efcf33c45657df07144d5059ed21bc3", "title": "DNN is not all you need: Parallelizing Non-Neural ML Algorithms on Ultra-Low-Power IoT Processors", "abstract": "Machine Learning (ML) functions are becoming ubiquitous in latencyand privacy-sensitive IoT applications, prompting for a shift toward near-sensor processing at the extreme edge and the consequent increasing adoption of Parallel Ultra-Low Power (PULP) IoT processors. These computeand memory-constrained parallel architectures need to run efficiently a wide range of algorithms, including key Non-Neural ML kernels that compete favorably with Deep Neural Networks (DNNs) in terms of accuracy under severe resource constraints. In this paper, we focus on enabling efficient parallel execution of Non-Neural ML algorithms on two RISCV-based PULP platforms, namely GAP8, a commercial chip, and PULP-OPEN, a research platform running on an FPGA emulator. We optimized the parallel algorithms through a fine-grained analysis and intensive optimization to maximize the speedup, considering two alternative Floating-Point (FP) emulation libraries on GAP8 and the native FPU support on PULP-OPEN. Experimental results show that a target-optimized emulation library can lead to an average 1.61\u00d7 runtime improvement compared to a standard emulation library, while the native FPU support reaches up to 32.09\u00d7. In terms of parallel speedup, our design improves the sequential execution by 7.04\u00d7 on average on the targeted octa-core platforms. Lastly, we present a comparison with the ARM Cortex-M4 microcontroller (MCU), a widely adopted commercial solution for edge deployments, which is 12.87\u00d7 slower than PULP-OPEN.", "venue": "ArXiv", "authors": ["Enrico  Tabanelli", "Giuseppe  Tagliavini", "Luca  Benini"], "year": 2021, "n_citations": 0}
{"id": 4717946, "s2_id": "7974bd7a5653ba2722f4c35df8fa5c0582c481a4", "title": "Network flow-based simultaneous retiming and slack budgeting for low power design", "abstract": "Low power design has become one of the most significant requirements when CMOS technology entered the nanometer era. Therefore, timing budget is often performed to slow down as many components as possible so that timing slacks can be applied to reduce the power consumption while maintaining the performance of the whole design. Retiming is a procedure that involves the relocation of flip-flops (FFs) across logic gates to achieve faster clocking speed. In this paper we show that the retiming and slack budgeting problem can be formulated to a convex cost dual network flow problem. Both the theoretical analysis and experimental results show the efficiency of our approach which can not only reduce power consumption by 8.9%, but also speedup previous work by 500 times.", "venue": "16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011)", "authors": ["Bei  Yu", "Sheqin  Dong", "Yuchun  Ma", "Tao  Lin", "Yu  Wang", "Song  Chen", "Satoshi  Goto"], "year": 2011, "n_citations": 2}
{"id": 4719153, "s2_id": "8aea737a9847c5045aa057cc2e1200a1abeaae28", "title": "Programming the Adapteva Epiphany 64-core network-on-chip coprocessor", "abstract": "Energy efficiency is the primary impediment in the path to exascale computing. Consequently, the high-performance computing community is increasingly interested in low-power high-performance embedded systems as building blocks for large-scale high-performance systems. The Adapteva Epiphany architecture integrates low-power RISC cores on a 2D mesh network and promises up to 70 GFLOPS/Watt of theoretical performance. However, with just 32\u2009KB of memory per eCore for storing both data and code, programming the Epiphany system presents significant challenges. In this paper we evaluate the performance of a 64-core Epiphany system with a variety of basic compute and communication micro-benchmarks. Further, we implemented two well known application kernels, 5-point star-shaped heat stencil with a peak performance of 65.2 GFLOPS and matrix multiplication with 65.3 GFLOPS in single precision across 64 Epiphany cores. We discuss strategies for implementing high-performance computing application kernels on such memory constrained low-power devices and compare the Epiphany with competing low-power systems. With future Epiphany revisions expected to house thousands of cores on a single chip, understanding the merits of such an architecture is of prime importance to the exascale initiative.", "venue": "2014 IEEE International Parallel & Distributed Processing Symposium Workshops", "authors": ["Anish  Varghese", "Bob  Edwards", "Gaurav  Mitra", "Alistair P. Rendell"], "year": 2014, "n_citations": 34}
{"id": 4719386, "s2_id": "5b13fae7218cf5e7f7b0aaf88ba366d35c9a39c6", "title": "Design and Scaffolded Training of an Efficient DNN Operator for Computer Vision on the Edge", "abstract": "Massively parallel systolic arrays and resource-efficient depthwise separable convolutions are two promising hardware and software techniques to accelerate DNN inference on the edge. Interestingly, their combination is inefficient: Computational patterns of depthwise separable convolutions do not exhibit a rhythmic systolic flow and lack sufficient data reuse to saturate systolic arrays. In this paper, we formally analyse this inefficiency and propose an efficient operator, an optimal hardware dataflow, and a superior training methodology towards alleviating this. The efficient operator, called Fully-Separable Convolutions (FuSeConv) 1, is a drop-in replacement for depthwise-separable convolutions. FuSeConv generalizes factorization of convolution fully along their spatial and depth dimensions. The resultant computation is systolic and efficiently maps to systolic arrays. The optimal hardware dataflow, called Spatial-Tiled Output Stationary (ST-OS), maximizes the efficiency of FuSeConv on systolic arrays. It maps independent convolutions to rows of the systolic array to maximize resource-utilization with negligible VLSI overheads. Neural Operator Scaffolding (NOS) scaffolds the training of FuSeConv operators by distilling knowledge from the more expensive depthwise separable convolution operation. This bridges the accuracy gap between FuSeConv networks and networks with depthwise-separable convolutions. Additionally, NOS can be combined with Neural Architecture Search (NAS) to trade-off latency and accuracy. The hardware-software co-design of FuSeConv with ST-OS achieves a significant speedup of 4.1 \u2212 9.25\u00d7 with state-of-the-art efficient networks for the ImageNet dataset. The parameter efficiency of FuSeConv and its significant out-performance over depthwiseseparable convolutions on systolic arrays illustrates their promise as a strong solution on the edge. Training FuSeConv networks with NOS achieves accuracy comparable to the depthwise-separable convolution baselines. Further, by combining NOS with NAS, we design networks that define state-of-the-art models improving on both accuracy and latency for computer vision on systolic arrays.", "venue": "ArXiv", "authors": ["Vinod  Ganesan", "Pratyush  Kumar"], "year": 2021, "n_citations": 0}
{"id": 4723298, "s2_id": "f23c73f32c4c4870388fb9aa2df5484bd042699f", "title": "MUSE: Multi-Use Error Correcting Codes", "abstract": "In this work we present a new set of error correcting codes \u2013 Multi-Use Error Correcting Codes (MUSE ECC) \u2013 that have the ability to match reliability guarantees of all commodity, conventional state-of-the-art ECC with fewer bits of storage. MUSE ECC derives its power by building on arithmetic coding methods (first used in an experimental system in 1960s). We show that our MUSE construction can be used as a \u201cdrop in\u201d replacement within error correction frameworks used widely today. Further, we show how MUSE is a promising fit for emerging technologies such as a DDR5 memories. Concretely, all instantiations of MUSE we show in this paper offer 100% Single Error Correction, and multi-bit error detection between 70% and 95% while using fewer check bits. MUSE ECC corrects failure of a single chip on a DIMM with check bit space savings of 12.5% compared to conventional techniques. The performance overheads, if any, are negligible. Our results open the possibility of reusing ECC storage for things beyond reliability without compromising reliability, thus solving a 40-year-old puzzle.", "venue": "ArXiv", "authors": ["Evgeny  Manzhosov", "Adam  Hastings", "Meghna  Pancholi", "Ryan  Piersma", "Mohamed Tarek Ibn Ziad", "Simha  Sethumadhavan"], "year": 2021, "n_citations": 0}
{"id": 4725450, "s2_id": "388c9cca20f2fdfa9d18f1a566abb8e04fcdcf80", "title": "Low Power Shift and Add Multiplier Design", "abstract": "Today every circuit has to face the power consumption issue for both portable device aiming at large battery life and high end circuits avoiding cooling packages and reliability issues that are too complex. It is generally accepted that during logic synthesis power tracks well with area. This means that a larger design will generally consume more power. The multiplier is an important kernel of digital signal processors. Because of the circuit complexity, the power consumption and area are the two important design considerations of the multiplier. In this paper a low power low area architecture for the shift and add multiplier is proposed. For getting the low power low area architecture, the modifications made to the conventional architecture consist of the reduction in switching activities of the major blocks of the multiplier, which includes the reduction in switching activity of the adder and counter. This architecture avoids the shifting of the multiplier register. The simulation result for 8 bit multipliers shows that the proposed low power architecture lowers the total power consumption by 35.25% and area by 52.72 % when compared to the conventional architecture. Also the reduction in power consumption increases with the increase in bit width.", "venue": "ArXiv", "authors": ["C. N. Marimuthu", "P.  Thangaraj", "Aswathy  Ramesan"], "year": 2010, "n_citations": 30}
{"id": 4725731, "s2_id": "7234d3755fd58fe4713e0b3f336e713270df2edb", "title": "An Empirical-cum-Statistical Approach to Power-Performance Characterization of Concurrent GPU Kernels", "abstract": "Growing deployment of power and energy efficient throughput accelerators (GPU) in data centers demands enhancement of power-performance co-optimization capabilities of GPUs. Realization of exascale computing using accelerators requires further improvements in power efficiency. With hardwired kernel concurrency enablement in accelerators, inter- and intra-workload simultaneous kernels computation predicts increased throughput at lower energy budget. To improve Performance-per-Watt metric of the architectures, a systematic empirical study of real-world throughput workloads (with concurrent kernel execution) is required. To this end, we propose a multi-kernel throughput workload generation framework that will facilitate aggressive energy and performance management of exascale data centers and will stimulate synergistic power-performance co-optimization of throughput architectures. Also, we demonstrate a multi-kernel throughput benchmark suite based on the framework that encapsulates symmetric, asymmetric and co-existing (often appears together) kernel based workloads. On average, our analysis reveals that spatial and temporal concurrency within kernel execution in throughput architectures saves energy consumption by 32%, 26% and 33% in GTX470, Tesla M2050 and Tesla K20 across 12 benchmarks. Concurrency and enhanced utilization are often correlated but do not imply significant deviation in power dissipation. Diversity analysis of proposed multi-kernels confirms characteristic variation and power-profile diversity within the suite. Besides, we explain several findings regarding power-performance co-optimization of concurrent throughput workloads.", "venue": "ArXiv", "authors": ["Nilanjan  Goswami", "Amer  Qouneh", "Chao  Li", "Tao  Li"], "year": 2020, "n_citations": 0}
{"id": 4728159, "s2_id": "7b82a61d65b3dff02d48676c61fd1db61f78ffea", "title": "EPIC: Efficient prediction of IC manufacturing hotspots with a unified meta-classification formulation", "abstract": "In this paper we present EPIC, an efficient and effective predictor for IC manufacturing hotspots in deep sub-wavelength lithography. EPIC proposes a unified framework to combine different hotspot detection methods together, such as machine learning and pattern matching, using mathematical programming/optimization. EPIC algorithm has been tested on a number of industry benchmarks under advanced manufacturing conditions. It demonstrates so far the best capability in selectively combining the desirable features of various hotspot detection methods (3.5-8.2% accuracy improvement) as well as significant suppression of the detection noise (e.g., 80% false-alarm reduction). These characteristics make EPIC very suitable for conducting high performance physical verification and guiding efficient manufacturability friendly physical design.", "venue": "17th Asia and South Pacific Design Automation Conference", "authors": ["Duo  Ding", "Bei  Yu", "Joydeep  Ghosh", "David Z. Pan"], "year": 2012, "n_citations": 78}
{"id": 4728200, "s2_id": "8177936dd0a7d75898acc45ca83c23e3755a13e6", "title": "A Geographic Directed Preferential Internet Topology Model", "abstract": "The goal of this work is to model the peering arrangements between autonomous systems (ASes). Most existing models of the AS-graph assume an undirected graph. However, peering arrangements are mostly asymmetric customer-provider arrangements, which are better modeled as directed edges. Furthermore, it is well known that the AS-graph, and in particular its clustering structure, is influenced by geography. We introduce a new model that describes the AS-graph as a directed graph, with an edge going from the customer to the provider, but also models symmetric peer-to-peer arrangements. In addition, our model takes geography into account. We are able to mathematically analyze its power-law exponent and number of leaves. Beyond the analysis, we have implemented our model as a synthetic network generator called GDNG. Experimentation with GDNG shows that the networks it produces are more realistic than those generated by other network generators, in terms of its power-law exponent, fractions of customer-provider and symmetric peering arrangements, and the size of its dense core. We believe that our model is the first to manifest realistic regional dense cores that have a clear geographic flavor. Our synthetic networks also exhibit path inflation effects that are similar to those observed in the real AS graph.", "venue": "MASCOTS", "authors": ["Sagy  Bar", "Mira  Gonen", "Avishai  Wool"], "year": 2005, "n_citations": 18}
{"id": 4729996, "s2_id": "64267b3cbc2e6804c90cd7a61e7f869f489f1d29", "title": "Sampling-based buffer insertion for post-silicon yield improvement under process variability", "abstract": "At submicron manufacturing technology nodes process variations affect circuit performance significantly. This trend leads to a large timing margin and thus overdesign to maintain yield. To combat this pessimism, post-silicon clock tuning buffers can be inserted into circuits to balance timing budgets of critical paths with their neighbors. After manufacturing, these clock buffers can be configured for each chip individually so that chips with timing failures may be rescued to improve yield. In this paper, we propose a sampling-based method to determine the proper locations of these buffers. The goal of this buffer insertion is to reduce the number of buffers and their ranges, while still maintaining a good yield improvement. Experimental results demonstrate that our algorithm can achieve a significant yield improvement (up to 35%) with only a small number of buffers.", "venue": "2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Grace Li Zhang", "Bing  Li", "Ulf  Schlichtmann"], "year": 2016, "n_citations": 17}
{"id": 4736817, "s2_id": "71528922112e581bc6fd2499d78159d393bc57b0", "title": "Pre-Quantized Deep Learning Models Codified in ONNX to Enable Hardware/Software Co-Design", "abstract": "This paper presents a methodology to separate the quantization process from the hardware-specific model compilation stage via a pre-quantized deep learning model description in standard ONNX format. Separating the quantization process from the model compilation stage enables independent development. The methodology is expressive to convey hardware-specific operations and to embed key quantization parameters into a ONNX model which enables hardware/software co-design. Detailed examples are given for both MLP and CNN based networks, which can be extended to other networks in a straightforward fashion.", "venue": "ArXiv", "authors": ["Ulf  Hanebutte", "Andrew  Baldwin", "Senad  Durakovic", "Igor  Filipovich", "Chien-Chun  Chou", "Damian  Adamowicz", "Derek  Chickles", "David  Hawkes"], "year": 2021, "n_citations": 0}
{"id": 4739147, "s2_id": "b2852fe1c67859872b8e9fc26e11db74134f750b", "title": "Some Schemes for Implementation of Arithmetic Operations with Complex Numbers Using Squaring Units", "abstract": "In this paper, new schemes for a squarer, multiplier and divider of complex numbers are proposed. Traditional structural solutions for each of these operations require the presence some number of general-purpose binary multipliers. The advantage of our solutions is a removing of multiplications through replacing them by less costly squarers. We use Logan's trick and quarter square technique, which propose to replace the calculation of the product of two real numbers by summing the squares. Replacing usual multipliers on digital squares implies reducing power consumption as well as decreases hardware circuit complexity. The squarer requiring less area and power as compared to general-purpose multiplier, it is interesting to assess the use of squarers to implementation of complex arithmetic.", "venue": "ArXiv", "authors": ["Aleksandr  Cariow", "Galina  Cariowa"], "year": 2017, "n_citations": 0}
{"id": 4739234, "s2_id": "f1b937f418c9024b38a3067d5486170e0e76864e", "title": "On the Optimization of Behavioral Logic Locking for High-Level Synthesis", "abstract": "The globalization of the electronics supply chain is requiring effective methods to thwart reverse engineering and IP theft. Logic locking is a promising solution but there are still several open concerns. Even when applied at high level of abstraction, logic locking leads to large overhead without guaranteeing that the obfuscation metric is actually maximized. We propose a framework to optimize the use of behavioral logic locking for a given security metric. We explore how to apply behavioral logic locking techniques during the HLS of IP cores. Operating on the chip behavior, our method is compatible with commercial HLS tools, complementing existing industrial design flows. We offer a framework where the designer can implement different meta-heuristics to explore the design space and select where to apply logic locking. Our methods optimizes a given security metric better than complete obfuscation, allows us to 1) obtain better protection, 2) reduce the obfuscation cost.", "venue": "ArXiv", "authors": ["Christian  Pilato", "Luca  Collini", "Luca  Cassano", "Donatella  Sciuto", "Siddharth  Garg", "Ramesh  Karri"], "year": 2021, "n_citations": 0}
{"id": 4739882, "s2_id": "dd74e9dddf2c089ed8db9a47a9934085e93bde24", "title": "Generalized SAT-Attack-Resistant Logic Locking", "abstract": "Logic locking is used to protect integrated circuits (ICs) from piracy and counterfeiting. An encrypted IC implements the correct function only when the right key is input. Many existing logic-locking methods are subject to the powerful satisfiability (SAT)-based attack. Recently, an Anti-SAT scheme has been developed. By adopting two complementary logic blocks that consist of AND/NAND trees, it makes the number of iterations needed by the SAT attack exponential to the number of input bits. Nevertheless, the Anti-SAT scheme is vulnerable to the later AppSAT and removal attacks. This article proposes a generalized (G-)Anti-SAT scheme. Different from the Anti-SAT scheme, a variety of complementary or non-complementary functions can be adopted for the two blocks in our G-Anti-SAT scheme. The Anti-SAT scheme is just a special case of our proposed design. Our design can achieve higher output corruptibility, which is also tunable, so that better resistance to the AppSAT and removal attacks is achieved. Meanwhile, unlike existing AppSAT-resilient designs, our design does not sacrifice the resistance to the SAT attack.", "venue": "IEEE Transactions on Information Forensics and Security", "authors": ["Jingbo  Zhou", "Xinmiao  Zhang"], "year": 2021, "n_citations": 1}
{"id": 4743528, "s2_id": "e1516e9e9e7ff30402da6dbbd30796d3144ebcf8", "title": "FPGA Energy Efficiency by Leveraging Thermal Margin", "abstract": "FPGA devices are continuously evolving to meet high computation and performance demand for emerging applications. As a result, cutting edge FPGAs are not energy efficient as conventionally presumed to be, and therefore, aggressive power-saving techniques have become imperative. The clock rate of an FPGA-mapped design is set based on worst-case conditions to ensure reliable operation under all circumstances. This usually leaves a considerable timing margin that can be exploited to reduce power consumption by scaling voltage without lowering clock frequency. There are hurdles for such opportunistic voltage scaling in FPGAs because (a) critical paths change with designs, making timing evaluation difficult as voltage changes, (b) each FPGA resource has particular power-delay trade-off with voltage, (c) data corruption of configuration cells and memory blocks further hampers voltage scaling. In this paper, we propose a systematical approach to leverage the available thermal headroom of FPGA-mapped designs for power and energy improvement. By comprehensively analyzing the timing and power consumption of FPGA building blocks under varying temperatures and voltages, we propose a thermal-aware voltage scaling flow that effectively utilizes the thermal margin to reduce power consumption without degrading performance. We show the proposed flow can be employed for energy optimization as well, whereby power consumption and delay are compromised to accomplish the tasks with minimum energy. Lastly, we propose a simulation framework to be able to examine the efficiency of the proposed method for other applications that are inherently tolerant to a certain amount of error, granting further power saving opportunity. Experimental results over a set of industrial benchmarks indicate up to 36% power reduction with the same performance, and 66% total energy saving when energy is the optimization target.", "venue": "2019 IEEE 37th International Conference on Computer Design (ICCD)", "authors": ["Behnam  Khaleghi", "Sahand  Salamat", "Mohsen  Imani", "Tajana  Simunic"], "year": 2019, "n_citations": 9}
{"id": 4746280, "s2_id": "b01ac779854804dc80d72f86b2484b88753b03cf", "title": "Triple patterning lithography (TPL) layout decomposition using end-cutting", "abstract": "Triple patterning lithography (TPL) is one of the most promising techniques in the 14nm logic node and beyond. However, traditional LELELE type TPL technology suffers from native conflict and overlapping problems. Recently LELEEC process was proposed to overcome the limitations, where the third mask is used to generate the end-cuts. In this paper we propose the first study for LELEEC layout decomposition. Conflict graphs and end- cut graphs are constructed to extract all the geometrical relationships of input layout and end-cut candidates. Based on these graphs, integer linear programming (ILP) is formulated to minimize the con ict number and the stitch number.", "venue": "Advanced Lithography", "authors": ["Bei  Yu", "Jhih-Rong  Gao", "David Z. Pan"], "year": 2013, "n_citations": 24}
{"id": 4749970, "s2_id": "febbe6482e4aacba81c2fbe9af501e7949bee575", "title": "Depth First Always On Routing Trace Algorithm", "abstract": "In this paper, we discussed current limitation in the electronic-design-automotation (EDA) tool on tracing the always on routing. We developed an algorithm to efficiently track the secondary power routing and accurately estimate the routing quality using approximate voltage drop as the criteria. The fast check can identify potential hotspot issues without going through sign-off checks. It helps designers to capture issues at early stages and fix the issues with less design effort. We also discussed some limitations to our algorithm.", "venue": "ArXiv", "authors": ["Anthony  Kim", "Sung Hyun Chen", "Chen  Zheng"], "year": 2017, "n_citations": 0}
{"id": 4751099, "s2_id": "7a6f63d69484a435c29b7c32b6a6bf28cd7b08df", "title": "A full-custom ASIC design of a 8-bit, 25 MHz, Pipeline ADC using 0.35 um CMOS technology", "abstract": "The purpose of this project was to design and implement a pipeline Analog-to-Digital Converter using 0.35um CMOS technology. Initial requirements of a 25-MHz conversion rate and 8-bits of resolution where the only given ones. Although additional secondary goals such as low power consumption and small area were stated. The architecture is based on a 1.5 bit per stage structure utilizing digital correction for each stage [12]. A differential switched capacitor circuit consisting of a cascade gm-C op-amp with 200MHz ft is used for sampling and amplification in each stage [12]. Differential dynamic comparators are used to implement the decision levels required for the 1.5-b per stage structure. Correction of the pipeline is accomplished by using digital correction circuit consist of D-latches and full-adders. Area and Power consumption of whole design was 0.24mm2 and 35mW respectively. The maximum sample rate at which the converter gave an adequate output was 33MHz.", "venue": "ArXiv", "authors": ["Moslem  Rashidi", "Mikael  Hogrud", "Donatas  Siaudinis", "Affaq  Qamar", "Imran  Khan"], "year": 2010, "n_citations": 0}
{"id": 4751244, "s2_id": "9c4295d1d2929c3cd89ed95944ea5232ca7234d2", "title": "Peer to Peer Networks for Defense Against Internet Worms", "abstract": "Internet worms, which spread in computer networks without human mediation, pose a severe threat to computer systems today. The rate of propagation of worms has been measured to be extremely high and they can infect a large fraction of their potential hosts in a short time. We study two different methods of patch dissemination to combat the spread of worms. We first show that using a fixed number of patch servers performs inadequately against Internet worms. We then show that by exploiting the exponential data dissemination capability of P2P systems, the spread of worms can be halted effectively. We compare the two methods by using fluid models to compute two quantities of interest: the time taken to effectively combat the progress of the worm, and the maximum number of infected hosts. We validate our models using simulations.", "venue": "IEEE J. Sel. Areas Commun.", "authors": ["Srinivas  Shakkottai", "R.  Srikant"], "year": 2007, "n_citations": 7}
{"id": 4755107, "s2_id": "0dd87409420caf4e045ce9e0f40dde95bf8d1e11", "title": "Computational ghost imaging using a field-programmable gate array", "abstract": "Computational ghost imaging is a promising technique for single-pixel imaging because it is robust to disturbance and can be operated over broad wavelength bands, unlike common cameras. However, one disadvantage of this method is that it has a long calculation time for image reconstruction. In this paper, we have designed a dedicated calculation circuit that accelerated the process of computational ghost imaging. We implemented this circuit by using a field-programmable gate array, which reduced the calculation time for the circuit compared to a CPU. The dedicated circuit reconstructs images at a frame rate of 300 Hz.", "venue": "OSA Continuum", "authors": ["Ikuo  Hoshi", "Tomoyoshi  Shimobaba", "Takashi  Kakue", "Tomoyoshi  Ito"], "year": 2019, "n_citations": 6}
{"id": 4756105, "s2_id": "8bd63a8c9e27e9f61744833360f04c0d11a62aaa", "title": "MIMHD: Accurate and Efficient Hyperdimensional Inference Using Multi-Bit In-Memory Computing", "abstract": "Hyperdimensional Computing (HDC) is an emerging computational framework that mimics important brain functions by operating over high-dimensional vectors, called hypervectors (HVs). In-memory computing implementations of HDC are desirable since they can significantly reduce data transfer overheads. All existing in-memory HDC platforms consider binary HVs where each dimension is represented with a single bit. However, utilizing multi-bit HVs allows HDC to achieve acceptable accuracies in lower dimensions which in turn leads to higher energy efficiencies. Thus, we propose a highly accurate and efficient multi-bit in-memory HDC inference platform called MIMHD. MIMHD supports multi-bit operations using ferroelectric field-effect transistor (FeFET) crossbar arrays for multiply-and-add and FeFET multi-bit content-addressable memories for associative search. We also introduce a novel hardware-aware retraining framework (HWART) that trains the HDC model to learn to work with MIMHD. For six popular datasets and 4000 dimension HVs, MIMHD using 3-bit (2-bit) precision HVs achieves (i) average accuracies of 92.6% (88.9%) which is 8.5% (4.8%) higher than binary implementations; (ii) 84.1\u00d7 (78.6\u00d7) energy improvement over a GPU, and (iii) 38.4\u00d7(34.3\u00d7) speedup over a GPU, respectively. The 3-bit MIMHD is 4.3\u00d7 and 13\u00d7 faster and more energy-efficient than binary HDC accelerators while achieving similar accuracies.", "venue": "2021 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)", "authors": ["Arman  Kazemi", "Mohammad Mehdi Sharifi", "Zhuowen  Zou", "Michael  Niemier", "X. Sharon Hu", "Mohsen  Imani"], "year": 2021, "n_citations": 1}
{"id": 4756144, "s2_id": "9913fdd4478d88cdbf3ba1f4fcf0f689a5721939", "title": "Static Address Generation Easing: a design methodology for parallel interleaver architectures", "abstract": "For high throughput applications, turbo-like iterative decoders are implemented with parallel architectures. However, to be efficient parallel architectures require to avoid collision accesses i.e. concurrent read/write accesses should not target the same memory block. This consideration applies to the two main classes of turbo-like codes which are Low Density Parity Check (LDPC) and Turbo-Codes. In this paper we propose a methodology which finds a collision-free mapping of the variables in the memory banks and which optimizes the resulting interleaving architecture. Finally, we show through a pedagogical example the interest of our approach compared to state-of-the-art techniques.", "venue": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing", "authors": ["C.  Chavet", "P.  Coussy", "P.  Urard", "E.  Martin"], "year": 2010, "n_citations": 25}
{"id": 4756526, "s2_id": "e5c4ac096a91d02a376cd614129b72a6119d00af", "title": "Latency Optimized Asynchronous Early Output Ripple Carry Adder based on Delay-Insensitive Dual-Rail Data Encoding", "abstract": "Asynchronous circuits employing delay-insensitive codes for data representation i.e. encoding and following a 4-phase return-to-zero protocol for handshaking are generally robust. Depending upon whether a single delay-insensitive code or multiple delay-insensitive code(s) are used for data encoding, the encoding scheme is called homogeneous or heterogeneous delay-insensitive data encoding. This article proposes a new latency optimized early output asynchronous ripple carry adder (RCA) that utilizes single-bit asynchronous full adders (SAFAs) and dual-bit asynchronous full adders (DAFAs) which incorporate redundant logic and are based on the delay-insensitive dual-rail code i.e. homogeneous data encoding, and follow a 4-phase return-to-zero handshaking. Amongst various RCA, carry lookahead adder (CLA), and carry select adder (CSLA) designs, which are based on homogeneous or heterogeneous delay-insensitive data encodings which correspond to the weak-indication or the early output timing model, the proposed early output asynchronous RCA that incorporates SAFAs and DAFAs with redundant logic is found to result in reduced latency for a dual-operand addition operation. In particular, for a 32-bit asynchronous RCA, utilizing 15 stages of DAFAs and 2 stages of SAFAs leads to reduced latency. The theoretical worst-case latencies of the different asynchronous adders were calculated by taking into account the typical gate delays of a 32/28nm CMOS digital cell library, and a comparison is made with their practical worst-case latencies estimated. The theoretical and practical worst-case latencies show a close correlation....", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "K.  Prasad"], "year": 2017, "n_citations": 7}
{"id": 4757932, "s2_id": "70d508df8567ea2f1fd70c8e0ef823a0ed7bdbd7", "title": "A Simple Self-calibration Method for The Internal Time Synchronization of MEMS LiDAR", "abstract": "This paper proposes a simple self-calibration method for the internal time synchronization of MEMS(Micro-electromechanical systems) LiDAR during research and development. Firstly, we introduced the problem of internal time misalignment in MEMS lidar. Then, a robust Minimum Vertical Gradient(MVG) prior is proposed to calibrate the time difference between the laser and MEMS mirror, which can be calculated automatically without any artificial participation or specially designed cooperation target. Finally, actual experiments on MEMS LiDARs are implemented to demonstrate the effectiveness of the proposed method. It should be noted that the calibration can be implemented in a simple laboratory environment without any ranging equipment and artificial participation, which greatly accelerate the progress of research and development in practical applications. \u00a9 2021 Optica Publishing Group under the terms of the Optica Publishing Group Publishing Agreement", "venue": "ArXiv", "authors": ["Yu  Zhang", "Xiaoguang  Di", "Shiyu  Yan", "Bin  Zhang", "Baoling  Qi", "Chunhui  Wang"], "year": 2021, "n_citations": 0}
{"id": 4758649, "s2_id": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb", "title": "Accelerating Sparse Deep Neural Networks", "abstract": "As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity \u2013 encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores.", "venue": "ArXiv", "authors": ["Asit  Mishra", "Jorge Albericio Latorre", "Jeff  Pool", "Darko  Stosic", "Dusan  Stosic", "Ganesh  Venkatesh", "Chong  Yu", "Paulius  Micikevicius"], "year": 2021, "n_citations": 13}
{"id": 4759279, "s2_id": "6d20d2bdc31230e4f7d30bb31abb5f5eab2e1386", "title": "Proceedings of the 3rd International Workshop on Overlay Architectures for FPGAs (OLAF 2017)", "abstract": "The 3rd International Workshop on Overlay Architectures for FPGAs (OLAF 2017) was held on 22 Feb, 2017 as a co-located workshop at the 25th ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA 2017). This year, the program committee selected 3 papers and 3 extended abstracts to be presented at the workshop, which are subsequently collected in this online volume.", "venue": "ArXiv", "authors": ["Hayden Kwok-Hay So", "John  Wawrzynek"], "year": 2017, "n_citations": 0}
{"id": 4760674, "s2_id": "d9a8450080387c17ee2b3c5af6d6159e7ec45fb3", "title": "PolarBear: A 28-nm FD-SOI ASIC for Decoding of Polar Codes", "abstract": "Polar codes are a recently proposed class of block codes that provably achieve the capacity of various communication channels. They received a lot of attention as they can do so with low-complexity encoding and decoding algorithms, and they have an explicit construction. Their recent inclusion in a 5G communication standard will only spur more research. However, only a couple of ASICs featuring decoders for polar codes were fabricated, and none of them implements a list-based decoding algorithm. In this paper, we present ASIC measurement results for a fabricated 28-nm CMOS chip that implements two different decoders: the first decoder is tailored toward error-correction performance and flexibility. It supports any code rate as well as three different decoding algorithms: successive cancellation (SC), SC flip, and SC list (SCL). The flexible decoder can also decode both non-systematic and systematic polar codes. The second decoder targets speed and energy efficiency. We present measurement results for the first silicon-proven SCL decoder, where its coded throughput is shown to be of 306.8 Mbps with a latency of 3.34 us and an energy per bit of 418.3 pJ/b at a clock frequency of 721 MHz for a supply of 1.3 V. The energy per bit drops down to 178.1 pJ/b with a more modest clock frequency of 308 MHz, lower throughput of 130.9 Mbps and a reduced supply voltage of 0.9 V. For the other two operating modes, the energy per bit is shown to be of approximately 95 pJ/b. The less flexible high-throughput unrolled decoder can achieve a coded throughput of 9.2 Gbps and a latency of 628 ns for a measured energy per bit of 1.15 pJ/b at 451 MHz.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Pascal  Giard", "Alexios  Balatsoukas-Stimming", "Thomas Christoph M\u00fcller", "Andrea  Bonetti", "Claude  Thibeault", "Warren J. Gross", "Philippe  Flatresse", "Andreas  Burg"], "year": 2017, "n_citations": 30}
{"id": 4763141, "s2_id": "202aff0523074ef9e1984309858f9176b517e318", "title": "Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct Feedback Alignment", "abstract": "The scaling hypothesis motivates the expansion of models past trillions of parameters as a path towards better performance. Recent significant developments, such as GPT-3, have been driven by this conjecture. However, as models scale-up, training them efficiently with backpropagation becomes difficult. Because model, pipeline, and data parallelism distribute parameters and gradients over compute nodes, communication is challenging to orchestrate: this is a bottleneck to further scaling. In this work, we argue that alternative training methods can mitigate these issues, and can inform the design of extreme-scale training hardware. Indeed, using a synaptically asymmetric method with a parallelizable backward pass, such as Direct Feedback Alignement, communication needs are drastically reduced. We present a photonic accelerator for Direct Feedback Alignment, able to compute random projections with trillions of parameters. We demonstrate our system on benchmark tasks, using both fully-connected and graph convolutional networks. Our hardware is the first architecture-agnostic photonic co-processor for training neural networks. This is a significant step towards building scalable hardware, able to go beyond backpropagation, and opening new avenues for deep learning.", "venue": "ArXiv", "authors": ["Julien  Launay", "Iacopo  Poli", "Kilian  Muller", "Gustave  Pariente", "Igor  Carron", "Laurent  Daudet", "Florent  Krzakala", "Sylvain  Gigan"], "year": 2020, "n_citations": 4}
{"id": 4763535, "s2_id": "65ca18cb9a02acefca12c30f06fff2f6de53065d", "title": "Feasibility Conditions for Interference Alignment", "abstract": "The degrees of freedom (DoF) of K-user MIMO interference networks with constant channel coefficients are not known in general. Determining the feasibility of a linear interference alignment is a key step toward solving this open problem. Our approach in this paper is to view the alignment problem for interference networks as a multivariate polynomial system and determine its solvability by comparing the number of equations and the number of variables. Consequently, we divide the interference networks into two classes - proper and improper, where interference alignment is and is not achievable, respectively. An interference network is called proper if the cardinality of every subset of equations in the corresponding polynomial system is less than or equal to the number of variables involved in that subset of equations. Otherwise, it is called improper. Our intuition in this paper is that for general channel matrices, proper systems are almost surely feasible and improper systems are almost surely infeasible. We prove the direct link between proper (improper) and feasible (infeasible) systems for some important cases, thus significantly strengthening our intuition. Numerical simulation results also support our intuition.", "venue": "GLOBECOM 2009 - 2009 IEEE Global Telecommunications Conference", "authors": ["Cenk M.  Yetis", "Tiangao  Gou", "Syed A.  Jafar", "Ahmet H.  Kayran"], "year": 2009, "n_citations": 149}
{"id": 4767426, "s2_id": "8c6d7745bffe8741d3f17d953f713d54c2817390", "title": "Fetch-Directed Instruction Prefetching Revisited", "abstract": "Prior work has observed that fetch-directed prefetching (FDIP) is highly effective at covering instruction cache misses. The key to FDIP's effectiveness is having a sufficiently large BTB to accommodate the application's branch working set. In this work, we introduce several optimizations that significantly extend the reach of the BTB within the available storage budget. Our optimizations target nearly every source of storage overhead in each BTB entry; namely, the tag, target address, and size fields. \nWe observe that while most dynamic branch instances have short offsets, a large number of branches has longer offsets or requires the use of full target addresses. Based on this insight, we break-up the BTB into multiple smaller BTBs, each storing offsets of different length. This enables a dramatic reduction in storage for target addresses. We further compress tags to 16 bits and avoid the use of the basic-block-oriented BTB advocated in prior FDIP variants. The latter optimization eliminates the need to store the basic block size in each BTB entry. Our final design, called FDIP-X, uses an ensemble of 4 BTBs and always outperforms conventional FDIP with a unified basic-block-oriented BTB for equal storage budgets.", "venue": "ArXiv", "authors": ["Truls  Asheim", "Rakesh  Kumar", "Boris  Grot"], "year": 2020, "n_citations": 1}
{"id": 4769353, "s2_id": "09b538eae6992a26e9dad314084d51bd5ba68917", "title": "Modeling and simulation of multiprocessor systems MPSoC by SystemC/TLM2", "abstract": "The current manufacturing technology allows the integration of a complex multiprocessor system on one piece of silicon (MPSoC for Multiprocessor System-on- Chip). One way to manage the growing complexity of these systems is to increase the level of abstraction and to address the system-level design. In this paper, we focus on the implementation in SystemC language with TLM (Transaction Level Model) to model an MPSOC platform. Our main contribution is to define a comprehensive, fast and accurate method for designing and evaluating performance for MPSoC systems. The studied MPSoC is composed of MicroBlaze microprocessors, memory, a timer, a VGA and an interrupt handler with two examples of software. This paper has two novel contributions: the first is to develop this MPSOC at CABA and TLM for ISS (Instruction Set Simulator), Native simulations and timed Programmer's View (PV+T); the second is to show that with PV+T simulations we can achieve timing fidelity with higher speeds than CABA simulations and have almost the same precision.", "venue": "ArXiv", "authors": ["Abdelhakim  Alali", "Ismail  Assayad", "Mohamed  Sadik"], "year": 2014, "n_citations": 7}
{"id": 4769743, "s2_id": "b2e6e6de3c4480e270d887a34c55d029c7e33cde", "title": "Development of FEB Configuration Test Board for ATLAS NSW Upgrade", "abstract": "A front-end board (FEB) configuration test board is developed for examining the configuration interfaces of various application-specific integrated circuits (ASICs) developed for the ATLAS New Small Wheel project. In this paper, we present the development of a FEB configuration test board, and some functions are developed in terms of configurations of the key chips on the FEB\u2013VMM3 and trigger data serializer (TDS2) using giga-bit transceiver-slow control adapter (GBT-SCA). Additionally, the protocols provided by GBT-SCA and the front-end board ASICs are used to verify the whole data link. The board provides technical reference for prototyping the FEB key chip configuration and data readout, as well as the final system configuration.", "venue": "IEEE Transactions on Nuclear Science", "authors": ["Houbing  Lu", "Feng  Li", "Peng  Miao", "Kun  Hu", "Zhilei  Zhang", "Rongqi  Sun", "Qingli  Ma", "Ge  Jin"], "year": 2019, "n_citations": 0}
{"id": 4771462, "s2_id": "1e3d4515f284a420e07e505b938b28be6bc41ec5", "title": "Ptolemy: Architecture Support for Robust Deep Learning", "abstract": "Deep learning is vulnerable to adversarial attacks, where carefully-crafted input perturbations could mislead a well-trained Deep Neural Network (DNN) to produce incorrect results. Adversarial attacks jeopardize the safety, security, and privacy of DNN-enabled systems. Today\u2019s countermeasures to adversarial attacks either do not have the capability to detect adversarial samples at inference-time, or introduce prohibitively high overhead to be practical at inference-time.We propose Ptolemy, an algorithm-architecture co-designed system that detects adversarial attacks at inference time with low overhead and high accuracy. We exploit the synergies between DNN inference and imperative program execution: an input to a DNN uniquely activates a set of neurons that contribute significantly to the inference output, analogous to the sequence of basic blocks exercised by an input in a conventional program. Critically, we observe that adversarial samples tend to activate distinctive paths from those of benign inputs. Leveraging this insight, we propose an adversarial sample detection framework, which uses canary paths generated from offline profiling to detect adversarial samples at runtime. The Ptolemy compiler along with the co-designed hardware enable efficient execution by exploiting the unique algorithmic characteristics. Extensive evaluations show that Ptolemy achieves higher or similar adversarial sample detection accuracy than today\u2019s mechanisms with a much lower (as low as 2%) runtime overhead.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Yiming  Gan", "Yuxian  Qiu", "Jingwen  Leng", "Minyi  Guo", "Yuhao  Zhu"], "year": 2020, "n_citations": 11}
{"id": 4771859, "s2_id": "a99af5fe1a54d28193b4c84477a165ab33cc7cdd", "title": "Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure DNN Accelerators", "abstract": "Deep neural network (DNN) accelerators received considerable attention in recent years due to the potential to save energy compared to mainstream hardware. Low-voltage operation of DNN accelerators allows to further reduce energy consumption significantly, however, causes bit-level failures in the memory storing the quantized DNN weights. Furthermore, DNN accelerators have been shown to be vulnerable to adversarial attacks on voltage controllers or individual bits. In this paper, we show that a combination of robust fixed-point quantization, weight clipping, as well as random bit error training (RANDBET) or adversarial bit error training (ADVBET) improves robustness against random or adversarial bit errors in quantized DNN weights significantly. This leads not only to high energy savings for low-voltage operation as well as low-precision quantization, but also improves security of DNN accelerators. Our approach generalizes across operating voltages and accelerators, as demonstrated on bit errors from profiled SRAM arrays, and achieves robustness against both targeted and untargeted bit-level attacks. Without losing more than 0.8%/2% in test accuracy, we can reduce energy consumption on CIFAR10 by 20%/30% for 8/4-bit quantization using RANDBET. Allowing up to 320 adversarial bit errors, ADVBET reduces test error from above 90% (chance level) to 26.22% on CIFAR10.", "venue": "ArXiv", "authors": ["David  Stutz", "Nandhini  Chandramoorthy", "Matthias  Hein", "Bernt  Schiele"], "year": 2021, "n_citations": 0}
{"id": 4778320, "s2_id": "6c19afe7af69589e7428887de1ce207bae362fd7", "title": "RNNAccel: A Fusion Recurrent Neural Network Accelerator for Edge Intelligence", "abstract": "Many edge devices employ Recurrent Neural Networks (RNN) to enhance their product intelligence. However, the increasing computation complexity poses challenges for performance, energy efficiency and product development time. In this paper, we present an RNN deep learning accelerator, called RNNAccel, which supports Long Short-Term Memory (LSTM) network, Gated Recurrent Unit (GRU) network, and Fully Connected Layer (FC)/ Multiple-Perceptron Layer (MLP) networks. This RNN accelerator addresses (1) computing unit utilization bottleneck caused by RNN data dependency, (2) inflexible design for specific applications, (3) energy consumption dominated by memory access, (4) accuracy loss due to coefficient compression, and (5) unpredictable performance resulting from processor-accelerator integration. Our proposed RNN accelerator consists of a configurable 32-MAC array and a coefficient decompression engine. The MAC array can be scaled-up to meet throughput requirement and power budget. Its sophisticated off-line compression and simple hardware-friendly on-line decompression, called NeuCompression, reduces memory footprint up to 16x and decreases memory access power. Furthermore, for easy SOC integration, we developed a tool set for bit-accurate simulation and integration result validation. Evaluated using a keyword spotting application, the 32-MAC RNN accelerator achieves 90% MAC utilization, 1.27 TOPs/W at 40nm process, 8x compression ratio, and 90% inference accuracy.", "venue": "ArXiv", "authors": ["Chao-Yang  Kao", "Huang-Chih  Kuo", "Jian-Wen  Chen", "Chiung-Liang  Lin", "Pin-Han  Chen", "Youn-Long  Lin"], "year": 2020, "n_citations": 2}
{"id": 4783605, "s2_id": "cf83cd77d34f76bacadcaeedd6aeb7789675b9f8", "title": "MANA: Microarchitecting an Instruction Prefetcher", "abstract": "L1 instruction (L1-I) cache misses are a source of performance bottleneck. Sequential prefetchers are simple solutions to mitigate this problem; however, prior work has shown that these prefetchers leave considerable potentials uncovered. This observation has motivated many researchers to come up with more advanced instruction prefetchers. In 2011, Proactive Instruction Fetch (PIF) showed that a hardware prefetcher could effectively eliminate all of the instruction-cache misses. However, its enormous storage cost makes it an impractical solution. Consequently, reducing the storage cost was the main research focus in the instruction prefetching in the past decade. Several instruction prefetchers, including RDIP and Shotgun, were proposed to offer PIF-level performance with significantly lower storage overhead. However, our findings show that there is a considerable performance gap between these proposals and PIF. While these proposals use different mechanisms for instruction prefetching, the performance gap is largely not because of the mechanism, and instead, is due to not having sufficient storage. Prior proposals suffer from one or both of the following shortcomings: (1) a large number of metadata records to cover the potential, and (2) a high storage cost of each record. The first problem causes metadata-miss, and the second problem prohibits the prefetcher from storing enough records within reasonably-sized storage. In this paper, we make the case that the key to designing a powerful and cost-effective instruction prefetcher is choosing the right metadata record and microarchitecting the prefetcher to minimize the storage. We find that high spatial correlation among instruction accesses leads to compact, accurate, and minimal metadata records. We also show that chaining these records is an effective way to enable robust and timely prefetching. Based on the findings, we propose MANA, which offers PIF-level performance with 15.7\u00d7 lower storage cost. MANA outperforms RDIP and Shotgun by 12.5 and 29%, respectively. We also evaluate a version of MANA with no storage overhead and show that it offers 98% of the peak performance benefits.", "venue": "ArXiv", "authors": ["Ali  Ansari", "Fatemeh  Golshan", "Pejman  Lotfi-Kamran", "Hamid  Sarbazi-Azad"], "year": 2021, "n_citations": 4}
{"id": 4789018, "s2_id": "34b18875d8fd7a6d269d5d00c06a2943b72c59fe", "title": "Towards Multidimensional Verification: Where Functional Meets Non-Functional", "abstract": "Trends in advanced electronic systems\u2019 design have a notable impact on design verification technologies. The recent paradigms of Internet-of-Things (IoT) and CyberPhysical Systems (CPS) assume devices immersed in physical environments, significantly constrained in resources and expected to provide levels of security, privacy, reliability, performance and low power features. In recent years, numerous extra-functional aspects of electronic systems were brought to the front and imply verification of hardware design models in multidimensional space along with the functional concerns of the target system. However, different from the software domain such a holistic approach remains underdeveloped. The contributions of this paper are a taxonomy for multidimensional hardware verification aspects, a state-of-the-art survey of related research works and trends towards the multidimensional verification concept. The concept is motivated by an example for the functional and power verification dimensions.", "venue": "2018 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC)", "authors": ["Maksim  Jenihhin", "Xinhui  Lai", "Tara  Ghasempouri", "Jaan  Raik"], "year": 2018, "n_citations": 5}
{"id": 4800572, "s2_id": "f249f2c92541ba7774924f8a49407640a671308c", "title": "Parallel Programming for FPGAs", "abstract": "This book focuses on the use of algorithmic high-level synthesis (HLS) to build application-specific FPGA systems. Our goal is to give the reader an appreciation of the process of creating an optimized hardware design using HLS. Although the details are, of necessity, different from parallel programming for multicore processors or GPUs, many of the fundamental concepts are similar. For example, designers must understand memory hierarchy and bandwidth, spatial and temporal locality of reference, parallelism, and tradeoffs between computation and storage. This book is a practical guide for anyone interested in building FPGA systems. In a university environment, it is appropriate for advanced undergraduate and graduate courses. At the same time, it is also useful for practicing system designers and embedded programmers. The book assumes the reader has a working knowledge of C/C++ and includes a significant amount of sample code. In addition, we assume familiarity with basic computer architecture concepts (pipelining, speedup, Amdahl's Law, etc.). A knowledge of the RTL-based FPGA design flow is helpful, although not required.", "venue": "ArXiv", "authors": ["Ryan  Kastner", "Janarbek  Matai", "Stephen  Neuendorffer"], "year": 2018, "n_citations": 30}
{"id": 4800871, "s2_id": "edd3bf6bec0029d45b02e9e34d59b8fdc0df2286", "title": "HERO: Heterogeneous Embedded Research Platform for Exploring RISC-V Manycore Accelerators on FPGA", "abstract": "Heterogeneous embedded systems on chip (HESoCs) co-integrate a standard host processor with programmable manycore accelerators (PMCAs) to combine general-purpose computing with domain-specific, efficient processing capabilities. While leading companies successfully advance their HESoC products, research lags behind due to the challenges of building a prototyping platform that unites an industry-standard host processor with an open research PMCA architecture. In this work we introduce HERO, an FPGA-based research platform that combines a PMCA composed of clusters of RISC-V cores, implemented as soft cores on an FPGA fabric, with a hard ARM Cortex-A multicore host processor. The PMCA architecture mapped on the FPGA is silicon-proven, scalable, configurable, and fully modifiable. HERO includes a complete software stack that consists of a heterogeneous cross-compilation toolchain with support for OpenMP accelerator programming, a Linux driver, and runtime libraries for both host and PMCA. HERO is designed to facilitate rapid exploration on all software and hardware layers: run-time behavior can be accurately analyzed by tracing events, and modifications can be validated through fully automated hard ware and software builds and executed tests. We demonstrate the usefulness of HERO by means of case studies from our research.", "venue": "ArXiv", "authors": ["Andreas  Kurth", "Pirmin  Vogel", "Alessandro  Capotondi", "Andrea  Marongiu", "Luca  Benini"], "year": 2017, "n_citations": 20}
{"id": 4805263, "s2_id": "2ac85423d20ae61bf987d7b2f90ac695dc7dfad9", "title": "When HLS Meets FPGA HBM: Benchmarking and Bandwidth Optimization", "abstract": "With the recent release of High Bandwidth Memory (HBM) based FPGA boards, developers can now exploit unprecedented external memory bandwidth. This allows more memory-bounded applications to benefit from FPGA acceleration. However, we found that it is not easy to fully utilize the available bandwidth when developing some applications with high-level synthesis (HLS) tools. This is due to the limitation of existing HLS tools when accessing HBM board's large number of independent external memory channels. In this paper, we measure the performance of three recent representative HBM FPGA boards (Intel's Stratix 10 MX and Xilinx's Alveo U50/U280 boards) with microbenchmarks and analyze the HLS overhead. Next, we propose HLS-based optimization techniques to improve the effective bandwidth when a PE accesses multiple HBM channels or multiple PEs access an HBM channel. Our experiment demonstrates that the effective bandwidth improves by 2.4X-3.8X. We also provide a list of insights for future improvement of the HBM FPGA HLS design flow.", "venue": "ArXiv", "authors": ["Young-kyu  Choi", "Yuze  Chi", "Jie  Wang", "Licheng  Guo", "Jason  Cong"], "year": 2020, "n_citations": 7}
{"id": 4805302, "s2_id": "0c3abd6adfad2cb47fdd1b1956889771f352d1ab", "title": "Accelerating BLAS and LAPACK via Efficient Floating Point Architecture Design", "abstract": "Basic Linear Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK) form basic building blocks for several High Performance Computing (HPC) applications and hence dictate performance of the...", "venue": "Parallel Process. Lett.", "authors": ["Farhad  Merchant", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan"], "year": 2017, "n_citations": 10}
{"id": 4811559, "s2_id": "dd881cb5223b09e61d7b96c226eac6da6e1c0ab8", "title": "Sorting Network for Reversible Logic Synthesis", "abstract": "In this paper, we have introduced an algorithm to implement a sorting network for reversible logic synthesis based on swapping bit strings. The algorithm first constructs a network in terms of n*n Toffoli gates read from left to right. The number of gates in the circuit produced by our algorithm is then reduced by template matching and removing useless gates from the network. We have also compared the efficiency of the proposed method with the existing ones.", "venue": "ArXiv", "authors": ["Md. Saiful Islam", "Md. Rafiqul Islam", "Abdullah Al Mahmud", "Muhammad Rezaul Karim"], "year": 2010, "n_citations": 4}
{"id": 4811820, "s2_id": "be6919e80eb050c7f242c0331015aebb138dcadb", "title": "Theoretical Modeling and Simulation of Phase-Locked Loop (PLL) for Clock Data Recovery (CDR)", "abstract": "Modern communication and computer systems require rapid (Gbps), efficient\u00a0 and large bandwidth data transfers. Agressive scaling of digital integrated systems\u00a0 allow buses and communication controller circuits to be integrated with the microprocessor on the same chip. The\u00a0 Peripheral Component Interconnect Express (PCIe) protocol handles all communcation between the central processing unit (CPU) and hardware devices. PCIe buses require efficient clock data recovery circuits (CDR) to recover clock signals embedded in data during transmission. This paper describes the theoretical modeling and simulation of a phase-locked loop (PLL) used in a CDR circuit. A simple PLL architecture for a 5 GHz CDR circuit is proposed\u00a0 and elaborated in this work. Simulations were carried out using a Hardware Description Language, Verilog-AMS. The effect of jitter on the proposed design is also simulated and evaluated in this work. It was found that the proposed design is robust against both input and VCO jitter. ABSTRAK: Sistem komunikasi dan komputer moden memerlukan pemindahan data yang cekap (Gbps), dan bandwidth yang besar. Pengecilan agresif menggunakan teknik sistem digital bersepadu membenarkan bas dan litar pengawal komunikasi disatukan dengan\u00a0 mikroprocessor dalam cip yang sama. Protokol persisian komponen sambung tara ekspres (PCIe) mengendalikan semua komunikasi antara unit pemprosesan pusat (CPU) dan peranti perkakasan. Bas PCIe memerlukan litar jam pemulihan data (CDR) yang cekap untuk mendapatkan kembali isyarat jam yang tertanam dalam data semasa transmisi. Karya ini menerangkan teori pemodelan dan simulasi gelung fasa terkunci (PLL) untuk CDR. Rekabentuk 5 GHz PLL yang mudah telah dicadangkan dalm kertas kerja ini. Simulasi telah dijalankan menggunakan perisian verilog-AMS. Simulasi mengunnakan kesan ketar dalam reka bentuk yang dicadangkan telah dinilai. Reka bentuk yang dicadangkan terbukti teguh mengatasi ganguan ketar di input dan VCO. KEY WORDS :\u00a0 phase-locked loop (PLL); jitter; phase detector; low-pass filter; voltage-controlled oscillator", "venue": "ArXiv", "authors": ["Zainab  Ashari", "Anis Nurashikin Nordin"], "year": 2012, "n_citations": 0}
{"id": 4831700, "s2_id": "fef0c0b997dd88c49c4ac5b0d4d4b55b64277af4", "title": "Layout decomposition for triple patterning lithography", "abstract": "As minimum feature size and pitch spacing further decrease, triple patterning lithography (TPL) is a possible 193nm extension along the paradigm of double patterning lithography (DPL). However, there is very little study on TPL layout decomposition. In this paper, we show that TPL layout decomposition is a more difficult problem than that for DPL. We then propose a general integer linear programming formulation for TPL layout decomposition which can simultaneously minimize conflict and stitch numbers. Since ILP has very poor scalability, we propose three acceleration techniques without sacrificing solution quality: independent component computation, layout graph simplification, and bridge computation. For very dense layouts, even with these speedup techniques, ILP formulation may still be too slow. Therefore, we propose a novel vector programming formulation for TPL decomposition, and solve it through effective semidefinite programming (SDP) approximation. Experimental results show that the ILP with acceleration techniques can reduce 82% runtime compared to the baseline ILP. Using SDP based algorithm, the runtime can be further reduced by 42% with some tradeoff in the stitch number (reduced by 7%) and the conflict (9% more). However, for very dense layouts, SDP based algorithm can achieve 140\u00d7 speed-up even compared with accelerated ILP.", "venue": "2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Bei  Yu", "Kun  Yuan", "Boyang  Zhang", "Duo  Ding", "David Z. Pan"], "year": 2011, "n_citations": 211}
{"id": 4833471, "s2_id": "de7eff32b91ab9c2a73077e5e7258a3877ffcb94", "title": "Fuzzing Hardware Like Software", "abstract": "Hardware flaws are permanent and potent: hardware cannot be patched once fabricated, and any flaws may undermine even formally verified software executing on top. Consequently, verification time dominates implementation time. The gold standard in hardware Design Verification (DV) is concentrated at two extremes: random dynamic verification and formal verification. Both techniques struggle to root out the subtle flaws in complex hardware that often manifest as security vulnerabilities. The root problem with random verification is its undirected nature, making it inefficient, while formal verification is constrained by the state-space explosion problem, making it infeasible to apply to complex designs. What is needed is a solution that is directed, yet under-constrained. Instead of making incremental improvements to existing hardware verification approaches, we leverage the observation that existing software fuzzers already provide such a solution; we adapt it for hardware verification, thus leveraging existing\u2014more advanced\u2014software verification tools. Specifically, we translate RTL hardware to a software model and fuzz that model. The central challenge we address is how best to mitigate the differences between the hardware execution model and software execution model. This includes: 1) how to represent test cases, 2) what is the hardware equivalent of a crash, 3) what is an appropriate coverage metric, and 4) how to create a generalpurpose fuzzing harness for hardware. To evaluate our approach, we design, implement, and opensource a Hardware Fuzzing Pipeline that enables fuzzing hardware at scale, using only open-source tools. Using our pipeline, we fuzz four IP blocks from Google\u2019s OpenTitan Root-of-Trust chip. Our experiments reveal a two orders-of-magnitude reduction in run time to achieve Finite State Machine (FSM) coverage over traditional dynamic verification schemes. Moreover, with our design-agnostic harness, we achieve over 88% HDL line coverage in three out of four of our designs\u2014even without any initial seeds.", "venue": "ArXiv", "authors": ["Timothy  Trippel", "Kang G. Shin", "Alex  Chernyakhovsky", "Garret  Kelly", "Dominic  Rizzo", "Matthew  Hicks"], "year": 2021, "n_citations": 5}
{"id": 4834219, "s2_id": "cf3c54d55b26776b9921875d42e6b878f1ca3949", "title": "Tvarak: Software-managed hardware offload for DAX NVM storage redundancy", "abstract": "Tvarak efficiently implements system-level redundancy for direct-access (DAX) NVM storage. Production storage systems complement device-level ECC (which covers media errors) with system-checksums and cross-device parity. This system-level redundancy enables detection of and recovery from data corruption due to device firmware bugs (e.g., reading data from the wrong physical location). Direct access to NVM penalizes software-only implementations of system-level redundancy, forcing a choice between lack of data protection or significant performance penalties. Offloading the update and verification of system-level redundancy to Tvarak, a hardware controller co-located with the last-level cache, enables efficient protection of data from such bugs in memory controller and NVM DIMM firmware. Simulation-based evaluation with seven data-intensive applications shows Tvarak's performance and energy efficiency. For example, Tvarak reduces Redis set-only performance by only 3%, compared to 50% reduction for a state-of-the-art software-only approach.", "venue": "ArXiv", "authors": ["Rajat  Kateja", "Nathan  Beckmann", "Gregory R. Ganger"], "year": 2019, "n_citations": 2}
{"id": 4837045, "s2_id": "6ae6f172b6dfcd33a0132896377350aaab060132", "title": "High-Throughput VLSI Architecture for Soft-Decision Decoding with ORBGRAND", "abstract": "Guessing Random Additive Noise Decoding (GRAND) is a recently proposed approximate Maximum Likelihood (ML) decoding technique that can decode any linear error-correcting block code. Ordered Reliability Bits GRAND (ORBGRAND) is a powerful variant of GRAND, which outperforms the original GRAND technique by generating error patterns in a specific order. Moreover, their simplicity at the algorithm level renders GRAND family a desirable candidate for applications that demand very high throughput. This work reports the first-ever hardware architecture for ORBGRAND, which achieves an average throughput of up to 42.5 Gbps for a code length of 128 at an SNR of 10 dB. Moreover, the proposed hardware can be used to decode any code provided the length and rate constraints. Compared to the state-of-the-art fast dynamic successive cancellation flip decoder (Fast-DSCF) using a 5G polar (128,105) code, the proposed VLSI implementation has 49\u00d7 more average throughput while maintaining similar decoding performance.", "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Syed Mohsin Abbas", "Thibaud  Tonnellier", "Furkan  Ercan", "Marwan  Jalaleddine", "Warren J. Gross"], "year": 2021, "n_citations": 4}
{"id": 4842251, "s2_id": "6776b474a2c7df5e9e829e775f529fc687dd6f2e", "title": "Energy-Efficient On-Chip Networks through Profiled Hybrid Switching", "abstract": "Virtual channel (VC) flow control is the de facto choice for modern networks-on-chip (NoCs) to allow better utilization of the link bandwidth through buffering and packet switching (PS), which are also the sources of large power footprint and long per-hop latency. However, bandwidth can be plentiful for parallel workloads under VC flow control. Thus, dated but simpler mechanisms, such as circuit switching (CS), can help improve the energy efficiency of modern NoCs. In this paper, we propose to apply CS to part of the link bandwidth so that a considerable amount of traffic can be transmitted bufferlessly without routing. Evaluations reveal that this proposal leads to a reduction of energy per flit by up to 32% while also provides very competitive latency when compared to networks under VC flow control.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Yuan  He", "Jinyu  Jiao", "Thang  Cao", "Masaaki  Kondo"], "year": 2020, "n_citations": 1}
{"id": 4842271, "s2_id": "342e7ec685e14884ef11d7a6b3e0ffc7d0ae9107", "title": "ARXON: A Framework for Approximate Communication Over Photonic Networks-on-Chip", "abstract": "The approximate computing paradigm advocates for relaxing accuracy goals in applications to improve energy-efficiency and performance. Recently, this paradigm has been explored to improve the energy-efficiency of silicon photonic networks-on-chip (PNoCs). Silicon photonic interconnects suffer from high power dissipation because of laser sources, which generate carrier wavelengths, and tuning power required for regulating photonic devices under different uncertainties. In this article, we propose a framework called AppRoXimation framework for On-chip photonic Networks (ARXON) to reduce such power dissipation overhead by enabling intelligent and aggressive approximation during communication over silicon photonic links in PNoCs. Our framework reduces laser and tuning-power overhead while intelligently approximating communication, such that application output quality is not distorted beyond an acceptable limit. Simulation results show that our framework can achieve up to 56.4% lower laser power consumption and up to 23.8% better energy-efficiency than the best-known prior work on approximate communication with silicon photonic interconnects and for the same application output quality.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Febin P. Sunny", "Asif  Mirza", "Ishan  Thakkar", "Mahdi  Nikdast", "Sudeep  Pasricha"], "year": 2021, "n_citations": 1}
{"id": 4843656, "s2_id": "ee4a67e48a6b2fccd79b1140d73a5e97e9fc0755", "title": "A 51.3-TOPS/W, 134.4-GOPS In-Memory Binary Image Filtering in 65-nm CMOS", "abstract": "Neuromorphic vision sensors (NVSs) can enable energy savings due to their event-driven that exploits the temporal redundancy in video streams from a stationary camera. However, noise-driven events lead to the false triggering of the object recognition processor. Image denoise operations require memory-intensive processing leading to a bottleneck in energy and latency. In this article, we present in-memory filtering (IMF), a 6T-SRAM in-memory computing (IMC)-based image denoising for event-based binary image (EBBI) frame from an NVS. We propose a non-overlap median filter (NOMF) for image denoising. An IMC framework enables hardware implementation of NOMF leveraging the inherent read disturb phenomenon of 6T-SRAM. To demonstrate the energy-saving and effectiveness of the algorithm, we fabricated the proposed architecture in a 65-nm CMOS process. Compared to fully digital implementation, IMF enables >70 $\\times $ energy savings and a >3 $\\times $ improvement of processing time when tested with the video recordings from a DAVIS sensor and achieves a peak throughput of 134.4 GOPS. Furthermore, the peak energy efficiencies of the NOMF are 51.3 TOPS/W, comparable with state-of-the-art in-memory processors. We also show that the accuracy of the images obtained by NOMF provides comparable accuracy in tracking and classification applications compared with images obtained by conventional median filtering.", "venue": "IEEE Journal of Solid-State Circuits", "authors": ["Sumon Kumar Bose", "Deepak  Singla", "Arindam  Basu"], "year": 2022, "n_citations": 0}
{"id": 4844823, "s2_id": "047f06dc5f83e49ac6019fd79f8ea868ab1e2826", "title": "Palimpsest Memories Stored in Memristive Synapses", "abstract": "Biological synapses store multiple memories on top of each other in a palimpsest fashion and at different timescales. Palimpsest consolidation is facilitated by the interaction of hidden biochemical processes that govern synaptic efficacy during varying lifetimes. This arrangement allows idle memories to be temporarily overwritten without being forgotten, in favour of new memories utilised in the short-term. While embedded artificial intelligence can greatly benefit from such functionality, a practical demonstration in hardware is still missing. Here, we show how the intrinsic properties of metal-oxide volatile memristors emulate the hidden processes that support biological palimpsest consolidation. Our memristive synapses exhibit an expanded doubled capacity which can protect a consolidated long-term memory while up to hundreds of uncorrelated short-term memories temporarily overwrite it. The synapses can also implement familiarity detection of previously forgotten memories. Crucially, palimpsest operation is achieved automatically and without the need for specialised instructions. We further demonstrate a practical adaptation of this technology in the context of image vision. This showcases the use of emerging memory technologies to efficiently expand the capacity of artificial intelligence hardware towards more generalised learning memories.", "venue": "ArXiv", "authors": ["Christos  Giotis", "Alexander  Serb", "Vasileios  Manouras", "Spyros  Stathopoulos", "Themis  Prodromakis"], "year": 2021, "n_citations": 0}
{"id": 4852795, "s2_id": "e11b318e3533a54580427e5bd620ae3db7899afc", "title": "Eternal-Thing 2.0: Analog-Trojan Resilient Ripple-Less Solar Energy Harvesting System for Sustainable IoT in Smart Cities and Smart Villages", "abstract": "Recently, harvesting natural energy is gaining more attention than other conventional approaches for sustainable Internet-of-Things (IoT). System on chip (SoC) power requirement for the IoT and generating higher voltages on-chip is a massive challenge for on-chip peripherals and systems. Many sensors are employed in smart cities and smart villages in decision-making, whose power requirement is an issue, and it must be uninterrupted. Previously, we presented Security-by-Design (SbD) principle to bring energy dissipation and cybersecurity together through our \u201dEternal-Thing\u201d. In this paper, an on-chip reliable energy harvesting system (EHS) is designed for IoT end node devices which is called \u201cEternal-Thing 2.0\u201d. The management section monitors the process load and also the recharging of the battery/super-capacitor. An efficient maximum power point tracking (MPPT) algorithm is used to avoid quiescent power consumption. The reliability of the proposed EHS is improved by using an aging tolerant ring oscillator. The proposed EHS is intended and simulated in CMOS 90nm technology. The output voltage is within the vary of 3-3.55V with an input of 1-1.5V. The EHS consumes 22\u03bcW of power, that satisfies the ultra-low-power necessities of IoT sensible nodes.", "venue": "ArXiv", "authors": ["Saswat K. Ram", "Sauvagya R. Sahoo", "Banee B. Das", "Kamalakanta  Mahapatra", "Saraju P. Mohanty"], "year": 2021, "n_citations": 0}
{"id": 4853564, "s2_id": "459a56f24e9025634e5f4ef3dc1b29c80cbffa69", "title": "Fast and accurate transaction level modeling of an extended AMBA2.0 bus architecture", "abstract": "A transaction level modeling (TLM) approach is used to meet the simulation speed as well as cycle accuracy for large scale SoC performance analysis. We implemented the transaction-level model of a proprietary bus called AHB+ which supports an extended AMBA2.0 protocol. The AHB+ transaction-level model is shown to be 353 times faster than the pin-accurate RTL model, while maintaining 97% accuracy on average. We also present the TLM development procedure of a bus architecture.", "venue": "Design, Automation and Test in Europe", "authors": ["Young-Taek  Kim", "Taehun  Kim", "Youngduk  Kim", "Chulho  Shin", "Eui-Young  Chung", "Kyu-Myung  Choi", "Jeong-Taek  Kong", "Soo-Kwan  Eo"], "year": 2005, "n_citations": 15}
{"id": 4855040, "s2_id": "4d11ad7b05162f753eb07720ff3b2f3f9d9ef751", "title": "CREW: Computation Reuse and Efficient Weight Storage for Hardware-accelerated MLPs and RNNs", "abstract": "Deep Neural Networks (DNNs) have achieved tremendous success for cognitive applications. The core operation in a DNN is the dot product between quantized inputs and weights. Prior works exploit the weight/input repetition that arises due to quantization to avoid redundant computations in Convolutional Neural Networks (CNNs). However, in this paper we show that their effectiveness is severely limited when applied to Fully-Connected (FC) layers, which are commonly used in state-of-the-art DNNs, as it is the case of modern Recurrent Neural Networks (RNNs) and Transformer models. To improve energy-efficiency of FC computation we present CREW, a hardware accelerator that implements Computation Reuse and an Efficient Weight Storage mechanism to exploit the large number of repeated weights in FC layers. CREW first performs the multiplications of the unique weights by their respective inputs and stores the results in an on-chip buffer. The storage requirements are modest due to the small number of unique weights and the relatively small size of the input compared to convolutional layers. Next, CREW computes each output by fetching and adding its required products. To this end, each weight is replaced offline by an index in the buffer of unique products. Indices are typically smaller than the quantized weights, since the number of unique weights for each input tends to be much lower than the range of quantized weights, which reduces storage and memory bandwidth requirements. Overall, CREW greatly reduces the number of multiplications and provides significant savings in model memory footprint and memory bandwidth usage. We evaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x speedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN, a state-of-art computation reuse technique, CREW achieves 2.10x speedup and 2.08x energy savings on average.", "venue": "ArXiv", "authors": ["Marc  Riera", "Jose-Maria  Arnau", "Antonio  Gonzalez"], "year": 2021, "n_citations": 0}
{"id": 4860896, "s2_id": "f11fc097ab068e61d0d31f8413b0bd5751b5ad72", "title": "Elastic Fidelity: Trading-off Computational Accuracy for Energy Reduction", "abstract": "Power dissipation and energy consumption have become one of the most important problems in the design of processors today. This is especially true in power-constrained environments, such as embedded and mobile computing. While lowering the operational voltage can reduce power consumption, there are limits imposed at design time, beyond which hardware components experience faulty operation. Moreover, the decrease in feature size has led to higher susceptibility to process variations, leading to reliability issues and lowering yield. However, not all computations and all data in a workload need to maintain 100% fidelity. In this paper, we explore the idea of employing functional or storage units that let go the conservative guardbands imposed on the design to guarantee reliable execution. Rather, these units exhibit Elastic Fidelity, by judiciously lowering the voltage to trade-off reliable execution for power consumption based on the error guarantees required by the executing code. By estimating the accuracy required by each computational segment of a workload, and steering each computation to different functional and storage units, Elastic Fidelity Computing obtains power and energy savings while reaching the reliability targets required by each computational segment. Our preliminary results indicate that even with conservative estimates, Elastic Fidelity can reduce the power and energy consumption of a processor by 11-13% when executing applications involving human perception that are typically included in modern mobile platforms, such as audio, image, and video decoding.", "venue": "ArXiv", "authors": ["Sourya  Roy", "Tyler  Clemons", "S. M. Faisal", "Ke  Liu", "Nikolaos  Hardavellas", "Srinivasan  Parthasarathy"], "year": 2011, "n_citations": 3}
{"id": 4863138, "s2_id": "35e5e5b3b8ebd55050306733086710e8b7722443", "title": "hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices", "abstract": "Accessible machine learning algorithms, software, and diagnostic tools for energy-efficient devices and systems are extremely valuable across a broad range of application domains. In scientific domains, real-time near-sensor processing can drastically improve experimental design and accelerate scientific discoveries. We have developed hls4ml, an open-source software-hardware co-design workflow to interpret and translate machine learning algorithms for implementation in FPGAs and ASICs specifically to support domain scientists. In this paper, we describe the essential features of the hls4ml workflow including network optimization techniques\u2014 such as pruning and quantization-aware training\u2014which can be incorporated naturally into the device implementations. We expand on previous hls4ml work by extending capabilities and techniques towards low-power implementations and increased usability: new PythonAPIs, quantization-aware pruning, end-to-end FPGAworkflows, long pipeline kernels for low power, and new device backends include an ASIC workflow. Taken together, these and continued efforts in hls4ml will arm a new generation of domain scientists with accessible, efficient, and powerful tools for machine-learningaccelerated discovery.", "venue": "ArXiv", "authors": ["Farah  Fahim", "Benjamin  Hawks", "Christian  Herwig", "James  Hirschauer", "Sergo  Jindariani", "Nhan  Tran", "Luca P. Carloni", "Giuseppe Di Guglielmo", "Philip C. Harris", "Jeffrey D. Krupa", "Dylan  Rankin", "Manuel Blanco Valentin", "Josiah  Hester", "Yingyi  Luo", "John  Mamish", "Seda  Orgrenci-Memik", "Thea  Aarrestad", "Hamza  Javed", "Vladimir  Loncar", "Maurizio  Pierini", "Adrian Alan Pol", "Sioni  Summers", "Javier M. Duarte", "Scott  Hauck", "Shih-Chieh  Hsu", "Jennifer  Ngadiuba", "Mia  Liu", "Duc  Hoang", "Edward  Kreinar", "Zhenbin  Wu"], "year": 2021, "n_citations": 8}
{"id": 4864886, "s2_id": "c530b5cd1f6998536048217c484c59c6dec98fc0", "title": "A Fast Method for Steady-State Memristor Crossbar Array Circuit Simulation", "abstract": "Abstract\u2014In this work we propose an effective preconditioning technique to accelerate the steady-state simulation of large-scale memristor crossbar arrays (MCAs). We exploit the structural regularity of MCAs to develop a specially-crafted preconditioner that can be efficiently evaluated utilizing tensor products and block matrix inversion. Numerical experiments demonstrate the efficacy of the proposed technique compared to mainstream preconditioners.", "venue": "ArXiv", "authors": ["Rui  Xie", "Mingyang  Song", "Junzhuo  Zhou", "Jie  Mei", "Quan  Chen"], "year": 2021, "n_citations": 0}
{"id": 4872402, "s2_id": "1a82a5b613aa76c9ad58ea95c01825bbaf6eced4", "title": "A Terabit Hybrid FPGA-ASIC Platform for Switch Virtualization", "abstract": "The roll-out of technologies like 5G and the need for multi-terabit bandwidth in backbone networks requires networking companies to make significant investments to keep up with growing service demands. For lower capital expenditure and faster time-to-market, companies can resort to anything-as-a-service providers to lease virtual resources. Nevertheless, existing virtualization technologies are still lagging behind next-generation networks\u2019 requirements. This paper breaks the terabit barrier by introducing a hybrid FPGA-ASIC architecture to virtualize programmable forwarding planes. In contrast to existing solutions, our architecture involves an ASIC that multiplexes network flows between programmable virtual switches running in an FPGA capable of full and partial reconfiguration, enabling virtual switch hot-swapping. Our evaluation shows the feasibility of a switch virtualization architecture capable of achieving a combined throughput of 3.2 Tbps by having up to 26 virtual switch instances in parallel with low resource occupation overhead.", "venue": "2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Mateus  Saquetti", "Raphael M. Brum", "Bruno  Zatt", "Samuel  Pagliarini", "Weverton  Cordeiro", "Jose R. Azambuja"], "year": 2021, "n_citations": 0}
{"id": 4873283, "s2_id": "b70e331f3fb97c9d035683fdda7bb5e3dd2a0851", "title": "A High Throughput List Decoder Architecture for Polar Codes", "abstract": "While long polar codes can achieve the capacity of arbitrary binary-input discrete memoryless channels when decoded by a low complexity successive-cancellation (SC) algorithm, the error performance of the SC algorithm is inferior for polar codes with finite block lengths. The cyclic redundancy check (CRC)-aided SC list (SCL) decoding algorithm has better error performance than the SC algorithm. However, current CRC-aided SCL decoders still suffer from long decoding latency and limited throughput. In this paper, a reduced latency list decoding (RLLD) algorithm for polar codes is proposed. Our RLLD algorithm performs the list decoding on a binary tree, whose leaves correspond to the bits of a polar code. In existing SCL decoding algorithms, all the nodes in the tree are traversed, and all possibilities of the information bits are considered. Instead, our RLLD algorithm visits much fewer nodes in the tree and considers fewer possibilities of the information bits. When configured properly, our RLLD algorithm significantly reduces the decoding latency and, hence, improves throughput, while introducing little performance degradation. Based on our RLLD algorithm, we also propose a high throughput list decoder architecture, which is suitable for larger block lengths due to its scalable partial sum computation unit. Our decoder architecture has been implemented for different block lengths and list sizes using the TSMC 90-nm CMOS technology. The implementation results demonstrate that our decoders achieve significant latency reduction and area efficiency improvement compared with the other list polar decoders in the literature.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Jun  Lin", "Chenrong  Xiong", "Zhiyuan  Yan"], "year": 2016, "n_citations": 50}
{"id": 4873881, "s2_id": "87d4bafeaff0d28f7317f2fe13354207d4df8588", "title": "Modeling a Cache Coherence Protocol with the Guarded Action Language", "abstract": "We present a formal model built for verification of the hardware Tera-Scale ARchitecture (TSAR), focusing on its Distributed Hybrid Cache Coherence Protocol (DHCCP). This protocol is by nature asynchronous, concurrent and distributed, which makes classical validation of the design (e.g. through testing) difficult. We therefore applied formal methods to prove essential properties of the protocol, such as absence of deadlocks, eventual consensus, and fairness.", "venue": "MARS/VPT@ETAPS", "authors": ["Quentin L. Meunier", "Yann  Thierry-Mieg", "Emmanuelle  Encrenaz-Tiph\u00e8ne"], "year": 2018, "n_citations": 0}
{"id": 4874634, "s2_id": "1f4278515156b564fd4b8873cff3a995bb8c4984", "title": "SimpleSSD: Modeling Solid State Drives for Holistic System Simulation", "abstract": "Existing solid state drive (SSD) simulators unfortunately lack hardware and/or software architecture models. Consequently, they are far from capturing the critical features of contemporary SSD devices. More importantly, while the performance of modern systems that adopt SSDs can vary based on their numerous internal design parameters and storage-level configurations, a full system simulation with traditional SSD models often requires unreasonably long runtimes and excessive computational resources. In this work, we propose SimpleSSD, a high-fidelity simulator that models all detailed characteristics of hardware and software, while simplifying the nondescript features of storage internals. In contrast to existing SSD simulators, SimpleSSD can easily be integrated into publicly-available full system simulators. In addition, it can accommodate a complete storage stack and evaluate the performance of SSDs along with diverse memory technologies and microarchitectures. Thus, it facilitates simulations that explore the full design space at different levels of system abstraction.", "venue": "IEEE Computer Architecture Letters", "authors": ["Myoungsoo  Jung", "Jie  Zhang", "Ahmed  Abulila", "Miryeong  Kwon", "Narges  Shahidi", "John  Shalf", "Nam Sung Kim", "Mahmut  Kandemir"], "year": 2018, "n_citations": 23}
{"id": 4877491, "s2_id": "134e33088ca7a0ca4dc76fd05e5f22c7f11b4728", "title": "CHIPKIT: An Agile, Reusable Open-Source Framework for Rapid Test Chip Development", "abstract": "The current trend for domain-specific architectures has led to renewed interest in research test chips to demonstrate new specialized hardware. Tapeouts also offer huge pedagogical value garnered from real hands-on exposure to the whole system stack. However, success with tapeouts requires hard-earned experience, and the design process is time consuming and fraught with challenges. Therefore, custom chips have remained the preserve of a small number of research groups, typically focused on circuit design research. This article describes the CHIPKIT framework: a reusable SoC subsystem which provides basic IO, an on-chip programmable host, off-chip hosting, memory, and peripherals. This subsystem can be readily extended with new IP blocks to generate custom test chips. Central to CHIPKIT is an agile RTL development flow, including a code generation tool called VGEN. Finally, we discuss best practices for full-chip validation across the entire design cycle.", "venue": "IEEE Micro", "authors": ["Paul N. Whatmough", "Marco  Donato", "Glenn G. Ko", "David  Brooks", "Gu-Yeon  Wei"], "year": 2020, "n_citations": 3}
{"id": 4878004, "s2_id": "f4cc9cd822de92298d91fcf9e43acead4d58a31a", "title": "Accelerating Genome Analysis: A Primer on an Ongoing Journey", "abstract": "Genome analysis fundamentally starts with a process known as read mapping, where sequenced fragments of an organism's genome are compared against a reference genome. Read mapping is currently a major bottleneck in the entire genome analysis pipeline, because state-of-the-art genome sequencing technologies are able to sequence a genome much faster than the computational techniques employed to analyze the genome. We describe the ongoing journey in significantly improving the performance of read mapping. We explain state-of-the-art algorithmic methods and hardware-based acceleration approaches. Algorithmic approaches exploit the structure of the genome as well as the structure of the underlying hardware. Hardware-based acceleration approaches exploit specialized microarchitectures or various execution paradigms (e.g., processing inside or near memory). We conclude with the challenges of adopting these hardware-accelerated read mappers.", "venue": "IEEE Micro", "authors": ["Mohammed  Alser", "Z\u00fclal  Bing\u00f6l", "Damla Senol Cali", "Jeremie S. Kim", "Saugata  Ghose", "Can  Alkan", "Onur  Mutlu"], "year": 2020, "n_citations": 18}
{"id": 4878801, "s2_id": "01ef752393c3f0c7fb07f2f6729d7a9277db2692", "title": "BasicBlocker: ISA Redesign to Make Spectre-Immune CPUs Faster", "abstract": "Recent research has revealed an ever-growing class of microarchitectural attacks that exploit speculative execution, a standard feature in modern processors. Proposed and deployed countermeasures involve a variety of compiler updates, firmware updates, and hardware updates. None of the deployed countermeasures have convincing security arguments, and many of them have already been broken. The obvious way to simplify the analysis of speculative-execution attacks is to eliminate speculative execution. This is normally dismissed as being unacceptably expensive, but the underlying cost analyses consider only software written for current instruction-set architectures, so they do not rule out the possibility of a new instruction-set architecture providing acceptable performance without speculative execution. A new ISA requires compiler and hardware updates, but these are happening in any case. This paper introduces BasicBlocker, a generic ISA modification that works for all common ISAs and that allows non-speculative CPUs to obtain most of the performance benefit that would have been provided by speculative execution. To demonstrate the feasibility of BasicBlocker, this paper defines a variant of the RISC-V ISA called BBRISC-V and provides a thorough evaluation on both a 5-stage in-order soft core and a superscalar out-of-order processor using an associated compiler and a variety of benchmarks.", "venue": "RAID", "authors": ["Jan Philipp Thoma", "Jakob  Feldtkeller", "Markus  Krausz", "Tim  G\u00fcneysu", "Daniel J. Bernstein"], "year": 2021, "n_citations": 0}
{"id": 4879549, "s2_id": "a97a38f0c25685b1b486968b2b457f2263af4999", "title": "SwitchAgg: A Further Step Towards In-Network Computation", "abstract": "Many distributed applications adopt a partition/aggregation pattern to achieve high performance and scalability. The aggregation process, which usually takes a large portion of the overall execution time, incurs large amount of network traffic and bottlenecks the system performance. To reduce network traffic, some researches take advantage of network devices to commit innetwork aggregation. However, these approaches use either special topology or middle-boxes, which cannot be easily deployed in current datacenters. The emerging programmable RMT switch brings us new opportunities to implement in-network computation task. However, we argue that the architecture of RMT switch is not suitable for in-network aggregation since it is designed primarily for implementing traditional network functions. In this paper, we first give a detailed analysis of in-network aggregation, and point out the key factor that affects the data reduction ratio. We then propose SwitchAgg, which is an innetwork aggregation system that is compatible with current datacenter infrastructures. We also evaluate the performance improvement we have gained from SwitchAgg. Our results show that, SwitchAgg can process data aggregation tasks at line rate and gives a high data reduction rate, which helps us to cut down network traffic and alleviate pressure on server CPU. In the system performance test, the job-completion-time can be reduced as much as 50%.", "venue": "FPGA", "authors": ["Fan  Yang", "Zhan  Wang", "Xiaoxiao  Ma", "Guojun  Yuan", "Xuejun  An"], "year": 2019, "n_citations": 4}
{"id": 4881352, "s2_id": "19fc146a738abed35c456d8bb809594eb365b5a2", "title": "Testable Designs of Toffoli Fredkin Reversible Circuits", "abstract": "Loss of every bit in traditional logic circuits involves dissipation of power in the form of heat that evolve to the environment. Reversible logic is one of the alternatives that have capabilities to mitigate this dissipation by preventing the loss of bits. It also have the potential to broaden the horizon of futuristic reckon with its applications to quantum computation. Application of testing strategies to the logic circuits is a necessity that guarantees their true functioning where the researchers are at par with solutions for the upcoming challenges and agreements for reversible logic circuits. Novel methods of designing Toffoli, Fredkin and mixed Toffoli-Fredkin gates based reversible circuits for testability are put fourth in this article. The proposed designs are independent of the implementation techniques and can be brought into real hardware devices after obtaining a stable fabrication environment. The experimentation for the proposed models are performed on RCViewer and RevKit tools to verify the functionality and computation of cost metrics. Fault simulations are carried out using C++ and Java to calculate fault coverage in respective methodologies. The results confirmed that all the presented work outperforms existing state-of-art approaches.", "venue": "ArXiv", "authors": ["Hari Mohan Gaur", "Ashutosh Kumar Singh", "Umesh  Ghanekar"], "year": 2021, "n_citations": 1}
{"id": 4885836, "s2_id": "364728e473f2a8fee56424e760b4369b82a44d83", "title": "Efficient Error-Correcting-Code Mechanism for High-Throughput Memristive Processing-in-Memory", "abstract": "Inefficient data transfer between computation and memory inspired emerging processing-in-memory (PIM) technologies. Many PIM solutions enable storage and processing using memristors in a crossbar-array structure, with techniques such as memristor-aided logic (MAGIC) used for computation. This approach provides highly-paralleled logic computation with minimal data movement. However, memristors are vulnerable to soft errors and standard error-correcting-code (ECC) techniques are difficult to implement without moving data outside the memory. We propose a novel technique for efficient ECC implementation along diagonals to support reliable computation inside the memory without explicitly reading the data. Our evaluation demonstrates an improvement of over eight orders of magnitude in reliability (mean time to failure) for an increase of about 26% in computation latency.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Orian  Leitersdorf", "Ben  Perach", "Ronny  Ronen", "Shahar  Kvatinsky"], "year": 2021, "n_citations": 1}
{"id": 4890608, "s2_id": "a00492e7638ad12b0801dea47dc280f9ef5cb97b", "title": "RTL2RTL Formal Equivalence: Boosting the Design Confidence", "abstract": "Increasing design complexity driven by feature and performance requirements and the Time to Market (TTM) constraints force a faster design and validation closure. This in turn enforces novel ways of identifying and debugging behavioral inconsistencies early in the design cycle. Addition of incremental features and timing fixes may alter the legacy design behavior and would inadvertently result in undesirable bugs. The most common method of verifying the correctness of the changed design is to run a dynamic regression test suite before and after the intended changes and compare the results, a method which is not exhaustive. Modern Formal Verification (FV) techniques involving new methods of proving Sequential Hardware Equivalence enabled a new set of solutions for the given problem, with complete coverage guarantee. Formal Equivalence can be applied for proving functional integrity after design changes resulting from a wide variety of reasons, ranging from simple pipeline optimizations to complex logic redistributions. We present here our experience of successfully applying the RTL to RTL (RTL2RTL) Formal Verification across a wide spectrum of problems on a Graphics design. The RTL2RTL FV enabled checking the design sanity in a very short time, thus enabling faster and safer design churn. The techniques presented in this paper are applicable to any complex hardware design.", "venue": "FSFMA", "authors": ["M. V. Achutha Kiran Kumar", "Aarti  Gupta", "S. S. Bindumadhava"], "year": 2014, "n_citations": 1}
{"id": 4891324, "s2_id": "25685586186a70da989910b5533d6d59f7eb7147", "title": "A fast diagnosis scheme for distributed small embedded SRAMs", "abstract": "The paper proposes a diagnosis scheme aimed at reducing the diagnosis time of distributed small embedded SRAMs (e-SRAMs). This scheme improves on one proposed previously (Huang, D.C. et al., Proc. Int. Conf. VLSI Design, p.397-402, 2001; Huang and Jone, W.B., IEEE Trans. Computer-Aided Design of Integrated Circuits and Systems, vol.21, no.4, p.449-65, 2002). The improvements are mainly twofold. On one hand, the diagnosis of time-consuming data retention faults (DRFs), which is neglected by the diagnosis architecture of Huang et al., is now considered and performed via a DFT technique referred to as the \"no write recovery test mode\" (NWRTM). On the other hand, a pair comprising a serial-to-parallel converter (SPC) and a parallel-to-serial converter (PSC) is utilized to replace the bidirectional serial interface, to avoid the problems of serial fault masking and defect rate dependent diagnosis. Results from our evaluations show that the proposed diagnosis scheme achieves an increased diagnosis coverage and reduces diagnosis time compared to those obtained by Huang et al., with negligible extra area cost.", "venue": "Design, Automation and Test in Europe", "authors": ["Baosheng  Wang", "Yuejian  Wu", "Andr\u00e9  Ivanov"], "year": 2005, "n_citations": 8}
{"id": 4894258, "s2_id": "9de4dda65ddb10fc032bcb9fa4e8d97734812504", "title": "Satisfiability modulo theory based methodology for floorplanning in VLSI circuits", "abstract": "This paper proposes a Satisfiability Modulo Theory based formulation for flooplanning in VLSI circuits. The proposed approach allows a number of fixed blocks to be placed within a layout region without overlapping of one block over the other and at the same time area of the layout region is minimized. The proposed approach also allows a number of fixed blocks with ability to rotate and flexible blocks (with variable width and height) to be placed within a layout without overlap. Our target in all cases is reduction in area occupied on a chip which is of vital importance in obtaining a good circuit design. Satisfiability Modulo Theory provides a richer modeling language than is possible with pure Boolean SAT formulas.", "venue": "2016 Sixth International Symposium on Embedded Computing and System Design (ISED)", "authors": ["Suchandra  Banerjee", "Anand  Ratna", "Suchismita  Roy"], "year": 2016, "n_citations": 3}
{"id": 4895377, "s2_id": "ef42b6373484e1afa5df9cb239ed95c83fa94da2", "title": "Hardware Acceleration of HPC Computational Flow Dynamics using HBM-enabled FPGAs", "abstract": "Scientific computing is at the core of many HighPerformance Computing (HPC) applications, including computational flow dynamics. Because of the uttermost importance to simulate increasingly larger computational models, hardware acceleration is receiving increased attention due to its potential to maximize the performance of scientific computing. A FieldProgrammable Gate Array (FPGA) is a reconfigurable hardware accelerator that is fully customizable in terms of computational resources and memory storage requirements of an application during its lifetime. Therefore, it is an ideal candidate to accelerate scientific computing applications because of the possibility to fully customize the memory hierarchy important in irregular applications such as iterative linear solvers found in scientific libraries. In this paper, we study the potential of using FPGA in HPC because of the rapid advances in reconfigurable hardware, such as the increase in on-chip memory size, increasing number of logic cells, and the integration of High-Bandwidth Memories on board. To perform this study, we first propose a novel ILU0 preconditioner tightly integrated with a BiCGStab solver kernel designed using a mixture of High-Level Synthesis (HLS) and Register-Transfer Level (RTL) hand-coded design. Second, we integrate the developed preconditioned iterative solver in Flow from the Open Porous Media (OPM) project, a state-of-the-art open-source reservoir simulator. Finally, we perform a thorough evaluation of the FPGA solver kernel in both standalone mode and integrated into the reservoir simulator that includes all the on-chip URAM and BRAM, on-board High-Bandwidth Memory (HBM), and off-chip CPU memory data transfers required in a complex simulator software such as OPM\u2019s Flow. We evaluate the performance on the Norne field, a real-world case reservoir model using a grid with more than 10 cells and using 3 unknowns per cell, and we find that the FPGA is on par with the CPU execution and 3 times slower than the GPU implementation when comparing only the kernel executions.", "venue": "ArXiv", "authors": ["Tom  Hogervorst", "Tong Dong Qiu", "Giacomo  Marchiori", "Alf Birger Rustad", "Markus  Blatt", "Razvan  Nane"], "year": 2021, "n_citations": 0}
{"id": 4897648, "s2_id": "7b1c8837c3eb47e72a924260594e512ee0376e53", "title": "An ASIC Implementation and Evaluation of a Profiled Low-Energy Instruction Set Architecture Extension", "abstract": "This paper presents an extension to an existing instruction set architecture, which gains considerable reduction in power consumption. The reduction in power consumption is achieved through coding of the most commonly executed instructions in a short format done by the compiler based on a profile of previous executions. This leads to fewer accesses to the instruction cache and that more instructions can fit in the cache. As a secondary effect, this turned out to be very beneficial in terms of power. Another major advantage, which is the main concern of this paper is the reduction in the number of instruction fetch cycles which will also contribute significantly towards reduction in power consumption. The work involves implementing the new processor architecture in ASIC and estimation of power-consumption compared to the normal architecture.", "venue": "ArXiv", "authors": ["Bobby  Sleeba", "Mikael  Collin", "Mats  Brorsson"], "year": 2021, "n_citations": 1}
{"id": 4900372, "s2_id": "7105fffe467643983667b252c6399d669449848b", "title": "Tiny but Accurate: A Pruned, Quantized and Optimized Memristor Crossbar Framework for Ultra Efficient DNN Implementation", "abstract": "The memristor crossbar array has emerged as an intrinsically suitable matrix computation and low-power acceleration framework for DNN applications. Many techniques such as memristor-based weight pruning and memristor-based quantization have been studied. However, the high accuracy solution for the above techniques is still waiting for unraveling. In this paper, we propose a memristor-based DNN framework which combines both structured weight pruning and quantization by incorporating ADMM algorithm for better pruning and quantization performance. We also discover the non-optimality of the ADMM solution in weight pruning and the unused data path in a structured pruned model. We design a software-hardware co-optimization framework which contains the first proposed Network Purification and Unused Path Removal algorithms targeting on post-processing a structured pruned model after ADMM steps. By taking memristor hardware constraints into our whole framework, we achieve extreme high compression rate with minimum accuracy loss. For quantizing structured pruned model, our framework achieves nearly no accuracy loss after quantizing weights to 8-bit memristor weight representation. We share our models at anonymous link https://bit.ly/2VnMUy0.", "venue": "2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Xiaolong  Ma", "Geng  Yuan", "Sheng  Lin", "Caiwen  Ding", "Fuxun  Yu", "Tao  Liu", "Wujie  Wen", "Xiang  Chen", "Yanzhi  Wang"], "year": 2020, "n_citations": 17}
{"id": 4903941, "s2_id": "e4341086af4fc13d5bf604ec5f414f4860b446d5", "title": "Pick the Right Edge Device: Towards Power and Performance Estimation of CUDA-based CNNs on GPGPUs", "abstract": "The emergence of Machine Learning (ML) as a powerful technique has been helping nearly all fields of business to increase operational efficiency or to develop new value propositions. Besides the challenges of deploying and maintaining ML models, picking the right edge device (e.g., GPGPUs) to run these models (e.g., CNN with the massive computational process) is one of the most pressing challenges faced by organizations today. As the cost of renting (on Cloud) or purchasing an edge device is directly connected to the cost of final products or services, choosing the most efficient device is essential. However, this decision making requires deep knowledge about performance and power consumption of the ML models running on edge devices that must be identified at the early stage of ML workflow. In this paper, we present a novel ML-based approach that provides ML engineers with the early estimation of both power consumption and performance of CUDA-based CNNs on GPGPUs. The proposed approach empowers ML engineers to pick the most efficient GPGPU for a given CNN model at the early stage of development.", "venue": "ArXiv", "authors": ["Christopher A. Metz", "Mehran  Goli", "Rolf  Drechsler"], "year": 2021, "n_citations": 1}
{"id": 4905807, "s2_id": "3cf6a8031e5bd9e9680f819b6f272b236715d9ff", "title": "DSPatch: Dual Spatial Pattern Prefetcher", "abstract": "High main memory latency continues to limit performance of modern high-performance out-of-order cores. While DRAM latency has remained nearly the same over many generations, DRAM bandwidth has grown significantly due to higher frequencies, newer architectures (DDR4, LPDDR4, GDDR5) and 3D-stacked memory packaging (HBM). Current state-of-the-art prefetchers do not do well in extracting higher performance when higher DRAM bandwidth is available. Prefetchers need the ability to dynamically adapt to available bandwidth, boosting prefetch count and prefetch coverage when headroom exists and throttling down to achieve high accuracy when the bandwidth utilization is close to peak. To this end, we present the Dual Spatial Pattern Prefetcher (DSPatch) that can be used as a standalone prefetcher or as a lightweight adjunct spatial prefetcher to the state-of-the-art delta-based Signature Pattern Prefetcher (SPP). DSPatch builds on a novel and intuitive use of modulated spatial bit-patterns. The key idea is to: (1) represent program accesses on a physical page as a bit-pattern anchored to the first \"trigger\" access, (2) learn two spatial access bit-patterns: one biased towards coverage and another biased towards accuracy, and (3) select one bit-pattern at run-time based on the DRAM bandwidth utilization to generate prefetches. Across a diverse set of workloads, using only 3.6KB of storage, DSPatch improves performance over an aggressive baseline with a PC-based stride prefetcher at the L1 cache and the SPP prefetcher at the L2 cache by 6% (9% in memory-intensive workloads and up to 26%). Moreover, the performance of DSPatch+SPP scales with increasing DRAM bandwidth, growing from 6% over SPP to 10% when DRAM bandwidth is doubled.", "venue": "MICRO", "authors": ["Rahul  Bera", "Anant V. Nori", "Onur  Mutlu", "Sreenivas  Subramoney"], "year": 2019, "n_citations": 12}
{"id": 4909609, "s2_id": "4aefb12ed0bb8adeaf538e26c07d951874d8a58b", "title": "Mapping Surface Code to Superconducting Quantum Processors", "abstract": "In this paper, we formally describe the three challenges of mapping surface code on superconducting devices, and present a comprehensive synthesis framework to overcome these challenges. The proposed framework consists of three optimizations. First, we adopt a geometrical method to allocate data qubits which ensures the existence of shallow syndrome extraction circuit. The proposed data qubit layout optimization reduces the overhead of syndrome extraction and serves as a good initial point for following optimizations. Second, we only use bridge qubits enclosed by data qubits and reduce the number of bridge qubits by merging short path between data qubits. The proposed bridge qubit optimization reduces the probability of bridge qubit conflicts and further minimizes the syndrome extraction overhead. Third, we propose an efficient heuristic to schedule syndrome extractions. Based on the proposed data qubit allocation, we devise a good initial schedule of syndrome extractions and further refine this schedule to minimize the total time needed by a complete surface code error detection cycle. Our experiments on mainsstream superconducting quantum architectures have demonstrated the efficiency of the proposed framework. 1 ar X iv :2 11 1. 13 72 9v 1 [ qu an tph ] 2 6 N ov 2 02 1", "venue": "ArXiv", "authors": ["Anbang  Wu", "Gushu  Li", "Hezi  Zhang", "Gian Giacomo Guerreschi", "Yufei  Ding", "Yuan  Xie"], "year": 2021, "n_citations": 0}
{"id": 4909674, "s2_id": "2c17267512f789403961c8c7c2a32758c1b715f3", "title": "A Computational Model for Tensor Core Units", "abstract": "To respond to the need for efficient training and inference of deep neural networks, a plethora of domain-specific architectures have been introduced, such as Google Tensor Processing Units and NVIDIA Tensor Cores. A common feature of these architectures is the design for efficiently computing a dense matrix product of a given small size. In order to broaden the class of algorithms that exploit these systems, we propose a computational model, named the TCU model, that captures the ability to natively multiply small matrices. We then use the TCU model for designing fast algorithms for several problems, including dense and sparse matrix multiplication and the Discrete Fourier Transform. We finally highlight a relation between the TCU model and the external memory model.", "venue": "SPAA", "authors": ["Francesco  Silvestri", "Flavio  Vella"], "year": 2020, "n_citations": 3}
{"id": 4911555, "s2_id": "88cdd95b64ad757022e3973a942ca891e8fe1b6b", "title": "Workload-Aware Opportunistic Energy Efficiency in Multi-FPGA Platforms", "abstract": "The continuous growth of big data applications with high computational and scalability demands has resulted in increasing popularity of cloud computing. Optimizing the performance and power consumption of cloud resources is therefore crucial to relieve the costs of data centers. In recent years, multi-FPGA platforms have gained traction in data centers as low-cost yet high-performance solutions particularly as acceleration engines, thanks to the high degree of parallelism they provide. Nonetheless, the size of data centers workloads varies during service time, leading to significant underutilization of computing resources while consuming a large amount of power, which turns out as a key factor of data center inefficiency, regardless of the underlying hardware structure. In this paper, we propose an efficient framework to throttle the power consumption of multi-FPGA platforms by dynamically scaling the voltage and hereby frequency during runtime according to prediction of, and adjustment to the workload level, while maintaining the desired Quality of Service (QoS). This is in contrast to, and more efficient than, conventional approaches that merely scale (i.e., power-gate) the computing nodes or frequency. The proposed framework carefully exploits a pre-characterized library of delay-voltage, and power-voltage information of FPGA resources, which we show is indispensable to obtain the efficient operating point due to the different sensitivity of resources w.r.t. voltage scaling, particularly considering multiple power rails residing in these devices. Our evaluations by implementing state-of-the-art deep neural network accelerators revealed that, providing an average power reduction of 4.0\u00d7, the proposed framework surpasses the previous works by 33.6% (up to 83%).", "venue": "2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Sahand  Salamat", "Behnam  Khaleghi", "Mohsen  Imani", "Tajana  Simunic"], "year": 2019, "n_citations": 14}
{"id": 4918934, "s2_id": "043e9f1b3d98386434055d0d93350a78f8eed624", "title": "Hardware Synthesis of State-Space Equations; Application to FPGA Implementation of Shallow and Deep Neural Networks", "abstract": "Nowadays, shallow and deep Neural Networks (NNs) have vast applications including biomedical engineering, image processing, computer vision, and speech recognition. Many researchers have developed hardware accelerators including field-programmable gate arrays (FPGAs) for implementing high-performance and energy efficient NNs. Apparently, the hardware architecture design process is specific and time-consuming for each NN. Therefore, a systematic way to design, implement and optimize NNs is highly demanded. The paper presents a systematic approach to implement state-space models in register transfer level (RTL), with special interest for NN implementation. The proposed design flow is based on the iterative nature of state-space models and the analogy between state-space formulations and finite-state machines. The method can be used in linear/nonlinear and time-varying/time-invariant systems. It can also be used to implement either intrinsically iterative systems (widely used in various domains such as signal processing, numerical analysis, computer arithmetic, and control engineering), or systems that could be rewritten in equivalent iterative forms. The implementation of recurrent NNs such as long short-term memory (LSTM) NNs, which have intrinsic state-space forms, are another major applications for this framework. As a case study, it is shown that state-space systems can be used for the systematic implementation and optimization of NNs (as nonlinear and time-varying dynamic systems). An RTL code generating software is also provided online, which simplifies the automatic generation of NNs of arbitrary size.", "venue": "ArXiv", "authors": ["Amir-Hossein  Kiamarzi", "Pezhman  Torabi", "Reza  Sameni"], "year": 2021, "n_citations": 0}
{"id": 4925316, "s2_id": "56e1a8b80a2d05f56262a47d8c053737fb20ea94", "title": "An FPGA-Based Fully Pipelined Bilateral Grid for Real-Time Image Denoising", "abstract": "The bilateral filter (BF) is widely used in image processing because it can perform denoising while preserving edges. It has disadvantages in that it is nonlinear, and its computational complexity and hardware resources are directly proportional to its window size. Thus far, several approximation methods and hardware implementations have been proposed to solve these problems. However, processing large-scale and high-resolution images in real time under severe hardware resource constraints remains a challenge. This paper proposes a real-time image denoising system that uses an FPGA based on the bilateral grid (BG). In the BG, a 2D image consisting of x- and y-axes is projected onto a 3D space called a \u201cgrid,\u201d which consists of axes that correlate to the x-component, y-component, and intensity value of the input image. This grid is then blurred using the Gaussian filter, and the output image is generated by interpolating the grid. Although it is possible to change the window size in the BF, it is impossible to change it on the input image in the BG. This makes it difficult to associate the BG with the BF and to obtain the property of suppressing the increase in hardware resources when the window radius is enlarged. This study demonstrates that a BG with a variable-sized window can be realized by introducing the window radius parameter wherein the window radius on the grid is always 1. We then implement this BG on an FPGA in a fully pipelined manner. Further, we verify that our design suppresses the increase in hardware resources even when the window size is enlarged and outperforms the existing designs in terms of computation speed and hardware resources.", "venue": "2021 31st International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Nobuho  Hashimoto", "Shinya  Takamaeda-Yamazaki"], "year": 2021, "n_citations": 0}
{"id": 4936421, "s2_id": "3623a39bfbbefff942a2f370d76dd18fbc1d9139", "title": "Demystifying BERT: Implications for Accelerator Design", "abstract": "Transfer learning in natural language processing (NLP), as realized using models like BERT (Bi-directional Encoder Representations from Transformer), has significantly improved language representation with models that can tackle challenging language problems. Consequently, these applications are driving the requirements of future systems. Thus, we focus on BERT, one of the most popular NLP transfer learning algorithms, to identify how BERT\u2019s algorithmic behavior can guide future accelerator design. To this end, we carefully profile BERT training and identify key algorithmic behaviors which are worthy of attention in accelerator design. We observe that while computations which manifest as matrix multiplication dominate BERT\u2019s overall runtime, as in many convolutional neural networks, memory-intensive computations also feature prominently. We characterize these computations, which have received little attention so far. Further, we also identify heterogeneity in compute-intensive BERT computations and discuss software and possible hardware mechanisms to further optimize these computations. Finally, we discuss implications of these behaviors as networks get larger and use distributed training environments, and how techniques such as micro-batching and mixed precision training scale. Overall, our analysis identifies holistic solutions to optimize systems for BERT-like models.", "venue": "ArXiv", "authors": ["Suchita  Pati", "Shaizeen  Aga", "Nuwan  Jayasena", "Matthew D. Sinclair"], "year": 2021, "n_citations": 0}
{"id": 4937696, "s2_id": "f6f4ddf84843fcbb6688807afbbfe9fc0d8377b9", "title": "Data-Dependent Clock Gating approach for Low Power Sequential System", "abstract": "Power dissipation in the sequential systems of modern CPU integrated chips (CPU-IC viz., Silicon Chip) is in discussion since the last decade. Researchers have been cultivating many low power design methods to choose the best potential candidate for reducing both static and dynamic power of a chip. Though, clock gating (CG) has been an accepted technique to control dynamic power dissipation, question still loiters on its credibility to handle the static power of the system. Therefore in this paper, we have revisited the popular CG schemes and found out some scope of improvisation to support the simultaneous reduction of static and dynamic power dissipation. Our proposed CG is simulated for 90nm CMOS using Cadence Virtuoso and has been tested on a conventional Master-Slave Flip-flop at 5GHz clock with a power supply of 1.1Volt. This assignment clearly depicts its supremacy in terms of power and timing metrics in comparison to the implementation of existing CG schemes.", "venue": "ArXiv", "authors": ["Dhiraj  Sarkar", "Pritam  Bhattacharjee", "Alak  Majumder"], "year": 2018, "n_citations": 0}
{"id": 4940496, "s2_id": "b610e75a7041a928bc263ffd49ba3fddeafbdd35", "title": "Reversible Logic Synthesis of Fault Tolerant Carry Skip BCD Adder", "abstract": "USER 11.9999 Normal 0 false false false MicrosoftInternetExplorer4 \nst1\\:*{behavior:url(#ieooui) } \n /* Style Definitions */ \n table.MsoNormalTable \n {mso-style-name:\"Table Normal\"; \n mso-tstyle-rowband-size:0; \n mso-tstyle-colband-size:0; \n mso-style-noshow:yes; \n mso-style-parent:\"\"; \n mso-padding-alt:0cm 5.4pt 0cm 5.4pt; \n mso-para-margin:0cm; \n mso-para-margin-bottom:.0001pt; \n mso-pagination:widow-orphan; \n font-size:10.0pt; \n font-family:\"Times New Roman\"; \n mso-ansi-language:#0400; \n mso-fareast-language:#0400; \n mso-bidi-language:#0400;} \n Reversible logic is emerging as an important research area having its application in diverse fields such as low power CMOS design, digital signal processing, cryptography, quantum computing and optical information processing. This paper presents a new 4*4 parity preserving reversible logic gate, IG. The proposed parity preserving reversible gate can be used to synthesize any arbitrary Boolean function. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. It is shown that a fault tolerant reversible full adder circuit can be realized using only two IGs. The proposed fault tolerant full adder (FTFA) is used to design other arithmetic logic circuits for which it is used as the fundamental building block. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts. Keywords: Reversible Logic, Parity Preserving Reversible Gate, IG Gate, FTFA and Carry Skip Logic. doi: 10.3329/jbas.v32i2.2431 Journal of Bangladesh Academy of Sciences Vol.32(2) 2008 234-250", "venue": "ArXiv", "authors": ["Md. Saiful Islam", "Zerina  Begum"], "year": 2010, "n_citations": 40}
{"id": 4944725, "s2_id": "573083dcd7c718115074662947fee3a393d14259", "title": "A reduced-precision streaming SpMV architecture for Personalized PageRank on FPGA", "abstract": "Sparse matrix-vector multiplication is often employed in many data-analytic workloads in which low latency and high throughput are more valuable than exact numerical convergence. FPGAs provide quick execution times while offering precise control over the accuracy of the results thanks to reduced-precision fixed-point arithmetic. In this work, we propose a novel streaming implementation of Coordinate Format (COO) sparse matrix-vector multiplication, and study its effectiveness when applied to the Personalized PageR-ank algorithm, a common building block of recommender systems in e-commerce websites and social networks. Our implementation achieves speedups up to 6x over a reference floating-point FPGA architecture and a state-of-the-art multi-threaded CPU implementation on 8 different data-sets, while preserving the numerical fidelity of the results and reaching up to 42x higher energy efficiency compared to the CPU implementation.", "venue": "2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Alberto  Parravicini", "Francesco  Sgherzi", "Marco D. Santambrogio"], "year": 2021, "n_citations": 2}
{"id": 4948217, "s2_id": "711e5309cebf6abda99e0b08b8916c66cfd5becd", "title": "The Integration of On-Line Monitoring and Reconfiguration Functions using EDAA - European design and Automation Association1149.4 Into a Safety Critical Automotive Electronic Control Unit", "abstract": "This paper presents an innovative application of EDAA - European design and Automation Association 1149.4 and the Integrated Diagnostic Reconfiguration (IDR) as tools for the implementation of an embedded test solution for an Automotive Electronic Control Unit implemented as a fully integrated mixed signal system. The paper described how the test architecture can be used for fault avoidance with results from a hardware prototype presented. The paper concludes that fault avoidance can be integrated into mixed signal electronic systems to handle key failure modes.", "venue": "ArXiv", "authors": ["Carl  Jeffrey", "Reuben  Cutajar", "Stephen  Prosser", "M.  Lickess", "Andrew  Richardson", "Stephen  Riches"], "year": 2007, "n_citations": 0}
{"id": 4952607, "s2_id": "24d6782a1b9d0e1d92b8b9195ba1c9d146f9a928", "title": "Application of a design space exploration tool to enhance interleaver generation", "abstract": "This paper presents a methodology to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall performance of the system is significantly affected by communication architectures, as a consequence the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named SpaceTime AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a Resource Constraints Graph (RCG). Design space exploration is then performed through associated tools, to synthesize a STAR component under time-to-market constraints. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.", "venue": "2007 15th European Signal Processing Conference", "authors": ["Cyrille  Chavet", "Philippe  Coussy", "Pascal  Urard", "Eric  Martin"], "year": 2007, "n_citations": 1}
{"id": 4952958, "s2_id": "145842c2bd5003ad8f5941d18a7310061bd00c48", "title": "C-slow Technique vs Multiprocessor in designing Low Area Customized Instruction set Processor for Embedded Applications", "abstract": "The demand for high performance embedded processors, for consumer electronics, is rapidly increasing for the past few years. Many of these embedded processors depend upon custom built Instruction Ser Architecture (ISA) such as game processor (GPU), multimedia processors, DSP processors etc. Primary requirement for consumer electronic industry is low cost with high performance and low power consumption. A lot of research has been evolved to enhance the performance of embedded processors through parallel computing. But some of them focus superscalar processors i.e. single processors with more resources like Instruction Level Parallelism (ILP) which includes Very Long Instruction Word (VLIW) architecture, custom instruction set extensible processor architecture and others require more number of processing units on a single chip like Thread Level Parallelism (TLP) that includes Simultaneous Multithreading (SMT), Chip Multithreading (CMT) and Chip Multiprocessing (CMP). In this paper, we present a new technique, named C-slow, to enhance performance for embedded processors for consumer electronics by exploiting multithreading technique in single core processors. Without resulting into the complexity of micro controlling with Real Time Operating system (RTOS), C-slowed processor can execute multiple threads in parallel using single datapath of Instruction Set processing element. This technique takes low area & approach complexity of general purpose processor running RTOS.", "venue": "ArXiv", "authors": ["Muhammad Adeel Akram", "Aamir  Khan", "Muhammad Masood Sarfaraz"], "year": 2012, "n_citations": 8}
{"id": 4955506, "s2_id": "4c6a654da71f294cd88a6568cada43eb5f9e0f0e", "title": "Real-Time Impulse Noise Removal from MR Images for Radiosurgery Applications", "abstract": "In the recent years image processing techniques are used as a tool to improve detection and diagnostic capabilities in the medical applications. Medical applications have been so much affected by these techniques which some of them are embedded in medical instruments such as MRI, CT and other medical devices. Among these techniques, medical image enhancement algorithms play an essential role in removal of the noise which can be produced by medical instruments and during image transfer. It has been proved that impulse noise is a major type of noise, which is produced during medical operations, such as MRI, CT, and angiography, by their image capturing devices. An embeddable hardware module which is able to denoise medical images before and during surgical operations could be very helpful. In this paper an accurate algorithm is proposed for real-time removal of impulse noise in medical images. All image blocks are divided into three categories of edge, smooth, and disordered areas. A different reconstruction method is applied to each category of blocks for the purpose of noise removal. The proposed method is tested on MR images. Simulation results show acceptable denoising accuracy for various levels of noise. Also an FPAG implementation of our denoising algorithm shows acceptable hardware resource utilization. Hence, the algorithm is suitable for embedding in medical hardware instruments such as radiosurgery devices.", "venue": "ArXiv", "authors": ["Zohreh  HosseinKhani", "Mohsen  Hajabdollahi", "Nader  Karimi", "S. Mohamad R. Soroushmehr", "Shahram  Shirani", "Shadrokh  Samavi", "Kayvan  Najarian"], "year": 2017, "n_citations": 1}
{"id": 4959462, "s2_id": "6b86992b88007e66f53099af5e442d26cc6e89d0", "title": "MPNA: A Massively-Parallel Neural Array Accelerator with Dataflow Optimization for Convolutional Neural Networks", "abstract": "The state-of-the-art accelerators for Convolutional Neural Networks (CNNs) typically focus on accelerating only the convolutional layers, but do not prioritize the fully-connected layers much. Hence, they lack a synergistic optimization of the hardware architecture and diverse dataflows for the complete CNN design, which can provide a higher potential for performance/energy efficiency. Towards this, we propose a novel Massively-Parallel Neural Array (MPNA) accelerator that integrates two heterogeneous systolic arrays and respective highly-optimized dataflow patterns to jointly accelerate both the convolutional (CONV) and the fully-connected (FC) layers. Besides fully-exploiting the available off-chip memory bandwidth, these optimized dataflows enable high data-reuse of all the data types (i.e., weights, input and output activations), and thereby enable our MPNA to achieve high energy savings. We synthesized our MPNA architecture using the ASIC design flow for a 28nm technology, and performed functional and timing validation using multiple real-world complex CNNs. MPNA achieves 149.7GOPS/W at 280MHz and consumes 239mW. Experimental results show that our MPNA architecture provides 1.7x overall performance improvement compared to state-of-the-art accelerator, and 51% energy saving compared to the baseline architecture.", "venue": "ArXiv", "authors": ["Muhammad Abdullah Hanif", "Rachmad Vidya Wicaksana Putra", "Muhammad  Tanvir", "Rehan  Hafiz", "Semeen  Rehman", "Muhammad  Shafique"], "year": 2018, "n_citations": 14}
{"id": 4959744, "s2_id": "10c04614372205e797783f7bd081a5949d9e1f4f", "title": "A Survey on Silicon Photonics for Deep Learning", "abstract": "Deep learning has led to unprecedented successes in solving some very difficult problems in domains such as computer vision, natural language processing, and general pattern recognition. These achievements are the culmination of decades-long research into better training techniques and deeper neural network models, as well as improvements in hardware platforms that are used to train and execute the deep neural network models. Many application-specific integrated circuit (ASIC) hardware accelerators for deep learning have garnered interest in recent years due to their improved performance and energy-efficiency over conventional CPU and GPU architectures. However, these accelerators are constrained by fundamental bottlenecks due to (1) the slowdown in CMOS scaling, which has limited computational and performance-per-watt capabilities of emerging electronic processors; and (2) the use of metallic interconnects for data movement, which do not scale well and are a major cause of bandwidth, latency, and energy inefficiencies in almost every contemporary processor. Silicon photonics has emerged as a promising CMOS-compatible alternative to realize a new generation of deep learning accelerators that can use light for both communication and computation. This article surveys the landscape of silicon photonics to accelerate deep learning, with a coverage of developments across design abstractions in a bottom-up manner, to convey both the capabilities and limitations of the silicon photonics paradigm in the context of deep learning acceleration.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Febin P Sunny", "Ebadollah  Taheri", "Mahdi  Nikdast", "Sudeep  Pasricha"], "year": 2021, "n_citations": 5}
{"id": 4962015, "s2_id": "0bfab34ed015072a9269ea63a21b328b22f9edb6", "title": "FPGA-based Hyrbid Memory Emulation System", "abstract": "Hybrid memory systems, comprised of emerging non-volatile memory (NVM) and DRAM, have been proposed to address the growing memory demand of applications. Emerging NVM technologies, such as phase-change memories (PCM), memristor, and 3D XPoint, have higher capacity density, minimal static power consumption and lower cost per GB. However, NVM has longer access latency and limited write endurance as opposed to DRAM. The different characteristics of two memory classes point towards the design of hybrid memory systems containing multiple classes of main memory. \nIn the iterative and incremental development of new architectures, the timeliness of simulation completion is critical to project progression. Hence, a highly efficient simulation method is needed to evaluate the performance of different hybrid memory system designs. Design exploration for hybrid memory systems is challenging, because it requires emulation of the full system stack, including the OS, memory controller, and interconnect. Moreover, benchmark applications for memory performance test typically have much larger working sets, thus taking even longer simulation warm-up period. \nIn this paper, we propose a FPGA-based hybrid memory system emulation platform. We target at the mobile computing system, which is sensitive to energy consumption and is likely to adopt NVM for its power efficiency. Here, because the focus of our platform is on the design of the hybrid memory system, we leverage the on-board hard IP ARM processors to both improve simulation performance while improving accuracy of the results. Thus, users can implement their data placement/migration policies with the FPGA logic elements and evaluate new designs quickly and effectively. Results show that our emulation platform provides a speedup of 9280x in simulation time compared to the software counterpart Gem5.", "venue": "ArXiv", "authors": ["Fei  Wen", "Mian  Qin", "Paul V. Gratz", "A.L.Narasimha  Reddy"], "year": 2020, "n_citations": 0}
{"id": 4962873, "s2_id": "29f91612171be53694f192d3df4bf5ec5b6a6b36", "title": "Integrating DRAM power-down modes in gem5 and quantifying their impact", "abstract": "Across applications, DRAM is a significant contributor to the overall system power, with the DRAM access energy per bit up to three orders of magnitude higher compared to on-chip memory accesses. To improve the power efficiency, DRAM technology incorporates multiplepower-down modes, each with different trade-offs between achievable power savings and performance impact due to entry and exit delay requirements. Accurate modeling of these low power modes and entry and exit control is crucial to analyze the trade-offs across controller configurations and workloads with varied memory access characteristics. To address this, we integrate the power-down modes into the DRAM controller model in the open-source simulator gem5. This is the first publicly available full-system simulator with DRAM power-down modes, providing the research community a tool for DRAM power analysis for a breadth of use cases. We validate the power-down functionality with sweep tests, which trigger defined memory access characteristics. We further evaluate the model with real HPC workloads, illustrating the value of integrating low power functionality into a full system simulator.", "venue": "MEMSYS", "authors": ["Radhika  Jagtap", "Matthias  Jung", "Wendy  Elsasser", "Christian  Weis", "Andreas  Hansson", "Norbert  Wehn"], "year": 2017, "n_citations": 6}
{"id": 4965150, "s2_id": "4db2b4ec0b9eeedcd0f60d321c5c2bebe3804b00", "title": "Uncertainty Modeling of Emerging Device based Computing-in-Memory Neural Accelerators with Application to Neural Architecture Search", "abstract": "Emerging device based Computing-in-memory (CiM) has been proved to be a promising candidate for high energy efficiency deep neural network (DNN) computations. However, most emerging devices suffer uncertainty issues, resulting in a difference between actual data stored and the weight value it is design to be. This leads to an accuracy drop from trained models to actually deployed platforms. In this work, we offer a thorough analysis on the effect of such uncertainties induced changes in DNN models. To reduce the impact of device uncertainties, we propose UAE, a uncertainty-aware Neural Architecture Search scheme to identify a DNN model that is both accurate and robust against device uncertainties.", "venue": "2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Zheyu  Yan", "Da-Cheng  Juan", "Xiaobo Sharon Hu", "Yiyu  Shi"], "year": 2021, "n_citations": 1}
{"id": 4974623, "s2_id": "c36b9dd7254073260994b0b1300be25ef9060ec4", "title": "Post-route refinement for high-frequency PCBs considering meander segment alleviation", "abstract": "In this paper, we propose a post-processing framework which iteratively refines the routing results from an existing PCB router by removing dense meander segments. By swapping and detouring dense meander segments the proposed method can effectively alleviate accumulating crosstalk noise, while respecting pre-defined area constraints. Experimental results show more than 85% reduction of the meander segments and hence the noise cost.", "venue": "ACM Great Lakes Symposium on VLSI", "authors": ["Tsun-Ming  Tseng", "Bing  Li", "Tsung-Yi  Ho", "Ulf  Schlichtmann"], "year": 2013, "n_citations": 2}
{"id": 4975832, "s2_id": "c7ee830908a21473a0b2d1d198cd1cce79271f57", "title": "Runtime Performances Benchmark for Knowledge Graph Embedding Methods", "abstract": "This paper wants to focus on providing a characterization of the runtime performances of state-of-the-art implementations of KGE alghoritms, in terms of memory footprint and execution time. Despite the rapidly growing interest in KGE methods, so far little attention has been devoted to their comparison and evaluation; in particular, previous work mainly focused on performance in terms of accuracy in specific tasks, such as link prediction. To this extent, a framework is proposed for evaluating available KGE implementations against graphs with different properties, with a particular focus on the effectiveness of the adopted optimization strategies. Graphs and models have been trained leveraging different architectures, in order to enlighten features and properties of both models and the architectures they have been trained on. Some results enlightened with experiments in this document are the fact that multithreading is efficient, but benefit deacreases as the number of threads grows in case of CPU. GPU proves to be the best architecture for the given task, even if CPU with some vectorized instructions still behaves well. Finally, RAM utilization for the loading of the graph never changes between different architectures and depends only on the type of graph, not on the model.", "venue": "ArXiv", "authors": ["Angelica Sofia Valeriani"], "year": 2020, "n_citations": 0}
{"id": 4981148, "s2_id": "324672a0573324fe4f71c496243773e6cef1148f", "title": "Memory-Aware Partitioning of Machine Learning Applications for Optimal Energy Use in Batteryless Systems", "abstract": "Sensing systems powered by energy harvesting have traditionally been designed to tolerate long periods without energy. As the Internet of Things (IoT) evolves towards a more transient and opportunistic execution paradigm, reducing energy storage costs will be key for its economic and ecologic viability. However, decreasing energy storage in harvesting systems introduces reliability issues. Transducers only produce intermittent energy at low voltage and current levels, making guaranteed task completion a challenge. Existing ad hoc methods overcome this by buering enough energy either for single tasks, incurring large data-retention overheads, or for one full application cycle, requiring a large energy buer.We present Julienning: an automated method for optimizing the total energy cost of batteryless applications. Using a custom specication model, developers can describe transient applications as a set of atomically executed kernels with explicit data dependencies. Our optimization ow can partition dataand energy-intensive applications into multiple execution cycles with bounded energy consumption. By leveraging interkernel data dependencies, these energy-bounded execution cycles minimize the number of system activations and nonvolatile data transfers, and thus the total energy overhead. We validate our methodology with two batteryless cameras running energy-intensive machine learning applications. Results demonstrate that compared to ad hoc solutions, our method can reduce the required energy storage by over 94% while only incurring a 0.12% energy overhead.", "venue": "ArXiv", "authors": ["Andres  Gomez", "Andreas  Tretter", "Pascal Alexander Hager", "Praveenth  Sanmugarajah", "Luca  Benini", "Lothar  Thiele"], "year": 2021, "n_citations": 0}
{"id": 4986202, "s2_id": "b3336a677807cca39ba7657c130e405d8c1232c7", "title": "RAPIDNN: In-Memory Deep Neural Network Acceleration Framework", "abstract": "Deep neural networks (DNN) have demonstrated effectiveness for various applications such as image processing, video segmentation, and speech recognition. Running state-of-the-art DNNs on current systems mostly relies on either generalpurpose processors, ASIC designs, or FPGA accelerators, all of which suffer from data movements due to the limited onchip memory and data transfer bandwidth. In this work, we propose a novel framework, called RAPIDNN, which processes all DNN operations within the memory to minimize the cost of data movement. To enable in-memory processing, RAPIDNN reinterprets a DNN model and maps it into a specialized accelerator, which is designed using non-volatile memory blocks that model four fundamental DNN operations, i.e., multiplication, addition, activation functions, and pooling. The framework extracts representative operands of a DNN model, e.g., weights and input values, using clustering methods to optimize the model for in-memory processing. Then, it maps the extracted operands and their precomputed results into the accelerator memory blocks. At runtime, the accelerator identifies computation results based on efficient in-memory search capability which also provides tunability of approximation to further improve computation efficiency. Our evaluation shows that RAPIDNN achieves 68.4x, 49.5x energy efficiency improvement and 48.1x, 10.9x speedup as compared to ISAAC and PipeLayer, the state-of-the-art DNN accelerators, while ensuring less than 0.3% of quality loss.", "venue": "ArXiv", "authors": ["Mohsen  Imani", "Mohammad  Samragh", "Yeseong  Kim", "Saransh  Gupta", "Farinaz  Koushanfar", "Tajana  Simunic"], "year": 2018, "n_citations": 33}
{"id": 4988188, "s2_id": "2c83da450404c2725bffe2fae70296f9877367bc", "title": "Charon: Load-Aware Load-Balancing in P4", "abstract": "Load-Balancers play an important role in data centers as they distribute network flows across application servers and guarantee per-connection consistency. It is hard however to make fair load balancing decisions so that all resources are efficiently occupied yet not overloaded. Tracking connection states allows to infer server load states and make informed decisions, but at the cost of additional memory space consumption. This makes it hard to implement on programmable hardware, which has constrained memory but offers line-rate performance. This paper presents Charon, a stateless load-aware load balancer that has line-rate performance implemented in P4-NetFPGA. Charon passively collects load states from application servers and employs the power-of-2-choices scheme to make data-driven load balancing decisions and improve resource utilization. Per-connection consistency is preserved statelessly by encoding server ID in a covert channel. The prototype design and implementation details are described in this paper. Simulation results show performance gains in terms of load distribution fairness, quality of service, throughput and processing latency.", "venue": "2021 17th International Conference on Network and Service Management (CNSM)", "authors": ["Carmine  Rizzi", "Zhiyuan  Yao", "Yoann  Desmouceaux", "Mark  Townsley", "Thomas Heide Clausen"], "year": 2021, "n_citations": 0}
{"id": 4988747, "s2_id": "7ae4ffa1d63731fe367f749b3565cb0e40343eb5", "title": "FPGA implementation of the CAR Model of the cochlea", "abstract": "The front end of the human auditory system, the cochlea, converts sound signals from the outside world into neural impulses transmitted along the auditory pathway for further processing. The cochlea senses and separates sound in a nonlinear active fashion, exhibiting remarkable sensitivity and frequency discrimination. Although several electronic models of the cochlea have been proposed and implemented, none of these are able to reproduce all the characteristics of the cochlea, including large dynamic range, large gain and sharp tuning at low sound levels, and low gain and broad tuning at intense sound levels. Here, we implement the `Cascade of Asymmetric Resonators' (CAR) model of the cochlea on an FPGA. CAR represents the basilar membrane filter in the `Cascade of Asymmetric Resonators with Fast-Acting Compression' (CAR-FAC) cochlear model. CAR-FAC is a neuromorphic model of hearing based on a pole-zero filter cascade model of auditory filtering. It uses simple nonlinear extensions of conventional digital filter stages that are well suited to FPGA implementations, so that we are able to implement up to 1224 cochlear sections on Virtex-6 FPGA to process sound data in real time. The FPGA implementation of the electronic cochlea described here may be used as a front-end sound analyser for various machine-hearing applications.", "venue": "2014 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Chetan Singh Thakur", "Tara Julia Hamilton", "Jonathan  Tapson", "Andr\u00e9 van Schaik", "Richard F. Lyon"], "year": 2014, "n_citations": 20}
{"id": 4988915, "s2_id": "ed1feee4a62fe875829f94aef56b677ba885b4ef", "title": "Area/latency optimized early output asynchronous full adders and relative-timed ripple carry adders", "abstract": "This article presents two area/latency optimized gate level asynchronous full adder designs which correspond to early output logic. The proposed full adders are constructed using the delay-insensitive dual-rail code and adhere to the four-phase return-to-zero handshaking. For an asynchronous ripple carry adder (RCA) constructed using the proposed early output full adders, the relative-timing assumption becomes necessary and the inherent advantages of the relative-timed RCA are: (1) computation with valid inputs, i.e., forward latency is data-dependent, and (2) computation with spacer inputs involves a bare minimum constant reverse latency of just one full adder delay, thus resulting in the optimal cycle time. With respect to different 32-bit RCA implementations, and in comparison with the optimized strong-indication, weak-indication, and early output full adder designs, one of the proposed early output full adders achieves respective reductions in latency by 67.8, 12.3 and 6.1\u00a0%, while the other proposed early output full adder achieves corresponding reductions in area by 32.6, 24.6 and 6.9\u00a0%, with practically no power penalty. Further, the proposed early output full adders based asynchronous RCAs enable minimum reductions in cycle time by 83.4, 15, and 8.8\u00a0% when considering carry-propagation over the entire RCA width of 32-bits, and maximum reductions in cycle time by 97.5, 27.4, and 22.4\u00a0% for the consideration of a typical carry chain length of 4 full adder stages, when compared to the least of the cycle time estimates of various strong-indication, weak-indication, and early output asynchronous RCAs of similar size. All the asynchronous full adders and RCAs were realized using standard cells in a semi-custom design fashion based on a 32/28\u00a0nm CMOS process technology.", "venue": "SpringerPlus", "authors": ["P.  Balasubramanian", "Shigeru  Yamashita"], "year": 2016, "n_citations": 17}
{"id": 4995168, "s2_id": "8a6ae996d30a1e734387e1d70b88640a8a600abd", "title": "Six Critical Challenges for 6G Wireless Systems", "abstract": "A large number of papers are now appearing on sixth-generation (6G) wireless systems, covering different aspects, ranging from vision, architecture, applications, and technology breakthroughs. With cellular systems in mind, this paper presents six critical, yet fundamental challenges that must be overcome before development and deployment of 6G systems. These include: Opening the sub-terahertz (sub-THz) spectrum for increased bandwidths and the ability to utilize these bandwidths, pushing the limits of semiconductor technologies for operation within the sub-THz bands, transceiver design and architectures to realize the high peak data rates, and realizations of sub-millisecond latencies at the network-level to achieve the 6G key performance indicators. Additionally, since 6G systems will not be introduced in a green fields environment, backwards compatibility with existing systems is discussed. Where possible, we present practical solutions to realize the identified challenges.", "venue": "ArXiv", "authors": ["Harsh  Tataria", "Mansoor  Shafi", "Mischa  Dohler", "Shu  Sun"], "year": 2021, "n_citations": 0}
{"id": 4996901, "s2_id": "c004d1c562e853cd8b3ff2fd01f4153928a995df", "title": "Numerical model for 32-bit magnonic ripple carry adder", "abstract": "In CMOS-based electronics, the most straightforward way to implement a summation operation is to use the ripple carry adder (RCA). Magnonics, the field of science concerned with data processing by spin-waves and their quanta magnons, recently proposed a magnonic half-adder that can be considered as the simplest magnonic integrated circuit. Here, we develop a computation model for the magnonic basic blocks to enable the design and simulation of magnonic gates and magnonic circuits of arbitrary complexity and demonstrate its functionality on the example of a 32-bit integrated RCA. It is shown that the RCA requires the utilization of additional regenerators based on magnonic directional couplers with embedded amplifiers to normalize the magnon signals in-between the half-adders. The benchmarking of large-scale magnonic integrated circuits is performed. The energy consumption of 30 nm-based magnonic 32-bit adder can be as low as 961 aJ per operation with taking into account all required amplifiers.", "venue": "ArXiv", "authors": ["U.  Garlando", "Q.  Wang", "O. V. Dobrovolskiy", "A. V. Chumak", "F.  Riente"], "year": 2021, "n_citations": 0}
{"id": 4997192, "s2_id": "17f54ffb118694808e6ef2ba260aea9fee7458a3", "title": "FOX: Hardware-Assisted File Auditing for Direct Access NVM-Hosted Filesystems", "abstract": "With emerging non-volatile memories (NVMs) entering the mainstream market, several operating systems start to incorporate new changes and optimizations. One major OS support is the direct-access for files (DAX) feature, which enables efficient access for files hosted in byte-addressable NVM systems. With DAX-enabled filesystems, files can be accessed directly similar to the memory with typical load/store operations. Despite its efficiency, the frequently used system call, dax-mmap, is troublesome for system auditing. File system auditing is mandatory and widely used because auditing logs can help trace potential anomalies, suspicious file accesses, or used as an evidence in digital forensics. However, the frequent and long-time usage of dax-mmap blinds the operating system or file system from tracking process operations to shared files after the initial page faults. This might results in imprecise casualty analysis and leads to false conclusion for attack detection. To remedy the tension between enabling fine-grained and whole file system auditing and leveraging the performance of NVM-hosted file systems, we propose a novel hardware-assisted auditing scheme (FOX). FOX enables file system auditing through lightweight hardware-software changes which can monitor every read/write access event for mmapped files on NVM. Additionally, we propose the optimized schemes, that enable auditing flexibility for selected files/memory range. By prototyping FOX on a fullsystem simulator, Gem5, we observe an average performance overhead of only 9 to 6.4% reduced throughput and 48% extra writes compared to our baseline. Compared to other instrumentation-based software schemes, our scheme is lowoverhead and secure.", "venue": "ArXiv", "authors": ["Mao  Ye"], "year": 2021, "n_citations": 0}
{"id": 5011300, "s2_id": "16e382a1fa7ed1159bf9a9c3d36c2573268368a2", "title": "OpeNPDN: A Neural-network-based Framework for Power Delivery Network Synthesis", "abstract": "Power delivery network (PDN) design is a nontrivial, time-intensive, and iterative task. Correct PDN design must account for considerations related to power bumps, currents, blockages, and signal congestion distribution patterns. This work proposes a machine learning based methodology that employs a set of predefined PDN templates. At the floorplan stage, coarse estimates of current, congestion, macro/blockages, and C4 bump distributions are used to synthesize a grid for early design. At the placement stage, the grid is incrementally refined based on more accurate and fine-grained distributions of current and congestion. At each stage, a convolutional neural network (CNN) selects an appropriate PDN template for each region on the chip, building a safe-by-construction PDN that meets IR drop and electromigration (EM) specifications. The CNN is initially trained using a large synthetically-created dataset, following which transfer learning is leveraged to bridge the gap between real-circuit data (with a limited dataset size) and syntheticallygenerated data. On average, the optimization of the PDN frees thousands of routing tracks in congestion-critical regions, when compared to a globally uniform PDN, while staying within the IR drop and EM limits.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Vidya A. Chhabria", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 0}
{"id": 5011609, "s2_id": "3b8c8758e8e5a9eda1d2e552095cd44ed8236b02", "title": "Self-Adaptive Reconfigurable Arrays (SARA): Using ML to Assist Scaling GEMM Acceleration", "abstract": "With increasing diversity in Deep Neural Network (DNN) models in terms of layer shapes and sizes, the research community has been investigating flexible/reconfigurable accelerator substrates. This line of research has opened up two challenges. The first is to determine the appropriate amount of flexibility within an accelerator array that that can tradeoff the performance benefits versus the area overheads of the reconfigurability. The second is being able to determine the right configuration of the array for the current DNN model and/or layer and reconfigure the accelerator at runtime. This work introduces a new class of accelerators that we call Self Adaptive Reconfigurable Array (SARA). SARA architectures comprise of both a reconfigurable array and a hardware unit capable of determining an optimized configuration for the array at runtime. We demonstrate an instance of SARA with an accelerator we call SAGAR that introduces a novel reconfigurable systolic array that can be configured to work as a distributed collection of smaller arrays of various sizes or as a single array with flexible aspect ratios. We also develop a novel recommendation neural network called ADAPTNET which recommends an array configuration and dataflow for the current layer parameters. An integrated custom hardware ADAPTNETX runs ADAPTNET at runtime and reconfigures the array, making the entire accelerator self sufficient. SAGAR is capable of providing the same mapping flexibility as a collection of 1024 4 \u00d7 4 arrays working as a distributed system while achieving 3.5\u00d7 more power efficiency and 3.2\u00d7 higher compute density Furthermore, when tested over 2 \u00d7 105 cases, the runtimes (GeoMean) achieved on the recommended parameters from ADAPTNET is 99.93% of the best achievable runtime.", "venue": "ArXiv", "authors": ["Ananda  Samajdar", "Michael  Pellauer", "Tushar  Krishna"], "year": 2021, "n_citations": 0}
{"id": 5013085, "s2_id": "8233b38d48aaa0ce80e1b27768dee312a109340c", "title": "CuttleSys: Data-Driven Resource Management forInteractive Applications on Reconfigurable Multicores", "abstract": "Multi-tenancy for latency-critical applications leads to re-source interference and unpredictable performance. Core reconfiguration opens up more opportunities for colocation,as it allows the hardware to adjust to the dynamic performance and power needs of a specific mix of co-scheduled applications. However, reconfigurability also introduces challenges, as even for a small number of reconfigurable cores, exploring the design space becomes more time- and resource-demanding. \nWe present CuttleSys, a runtime for reconfigurable multi-cores that leverages scalable and lightweight data mining to quickly identify suitable core and cache configurations for a set of co-scheduled applications. The runtime combines collaborative filtering to infer the behavior of each job on every core and cache configuration, with Dynamically Dimensioned Search to efficiently explore the configuration space. We evaluate CuttleSys on multicores with tens of reconfigurable cores and show up to 2.46x and 1.55x performance improvements compared to core-level gating and oracle-like asymmetric multicores respectively, under stringent power constraints.", "venue": "ArXiv", "authors": ["Neeraj  Kulkarni", "Gonzalo  Gonzalez-Pumariega", "Amulya  Khurana", "Christine  Shoemaker", "Christina  Delimitrou", "David  Albonesi"], "year": 2020, "n_citations": 0}
{"id": 5013438, "s2_id": "5da7c767febf43e148e23297fabf70940f8df5c5", "title": "Top-down design of a low-power multi-channel 2.5-Gbit/s/channel gated oscillator clock-recovery circuit", "abstract": "We present a complete top-down design of a low-power multi-channel clock recovery circuit based on gated current-controlled oscillators. The flow includes several tools and methods used to specify block constraints, to design and verify the topology down to the transistor level, as well as to achieve a power consumption as low as 5 mW/Gbit/s. Statistical simulation is used to estimate the achievable bit error rate in the presence of phase and frequency errors and to prove the feasibility of the concept. VHDL modeling provides extensive verification of the topology. Thermal noise modeling based on well-known concepts delivers design parameters for the device sizing and biasing. We present two practical examples of possible design improvements analyzed and implemented with this methodology.", "venue": "Design, Automation and Test in Europe", "authors": ["Paul  Muller", "Armin  Tajalli", "Seyed Mojtaba Atarodi", "Yusuf  Leblebici"], "year": 2005, "n_citations": 9}
{"id": 5013727, "s2_id": "c530d95914c7324419a9d70f6cd3753ecce2246c", "title": "On the information engine of circuit design", "abstract": "This paper addresses a new approach to find a spectrum of information measures for the process of digital circuit synthesis. We consider the problem from the information engine point of view. The circuit synthesis as a whole and different steps of the design process (an example of decision diagram is given) are presented via such measurements as entropy, logical work and information vitality. We also introduce new information measures to provide better estimates of synthesis criteria. We show that the basic properties of an information engine, such as the conservation law of information flow and the equilibrium law of information can be formulated.", "venue": "The 2002 45th Midwest Symposium on Circuits and Systems, 2002. MWSCAS-2002.", "authors": ["Denis V. Popel", "Nawar  Al-Hakeem"], "year": 2002, "n_citations": 1}
{"id": 5022803, "s2_id": "bd5ec0ecfa8c4047170faf1dad7f7a174d4738f2", "title": "Ultra-Fast, High-Performance 8x8 Approximate Multipliers by a New Multicolumn 3, 3: 2 Inexact Compressor and its Derivatives", "abstract": "Multiplier, which has a key role in many different applications, is a time-consuming, energy-intensive computation block. Approximate computing is a practical design paradigm that attempts to improve hardware efficacy while keeping computation quality satisfactory. A novel multicolumn 3,3:2 inexact compressor is presented in this paper. It takes three partial products from two adjacent columns each for rapid partial product reduction. The proposed inexact compressor and its derivates enable us to design a high-speed approximate multiplier. Then, another ultra-fast, high-efficient approximate multiplier is achieved by means a systematic truncation strategy. The proposed multipliers accumulate partial products in only two stages, one fewer stage than other approximate multipliers in the literature. Implementation results by Synopsys Design Compiler and 45 nm technology node demonstrates nearly 11.11% higher speed for the second proposed design over the fastest existing approximate multiplier. Furthermore, the new approximate multipliers are applied to the image processing application of image sharpening. Their performance in this application is highly satisfactory. It is shown in this paper that the error pattern of an approximate multiplier, in addition to the mean error distance and error rate, has a direct effect on the outcomes of the image processing application.", "venue": "ArXiv", "authors": ["Fereshteh  Karimi", "Reza Faghih Mirzaee", "Ali  Fakeri-Tabrizi", "Arman  Roohi"], "year": 2021, "n_citations": 0}
{"id": 5024814, "s2_id": "1653bbcf0428c33af34daa8c3feb55924dbc3fa4", "title": "Open-source Hardware: Opportunities and Challenges", "abstract": "Innovation in hardware is slowing due to rising costs of chip design and diminishing benefits from Moore's law and Dennard scaling. Software innovation, on the other hand, is flourishing, helped in good measure by a thriving open-source ecosystem. We believe that open source can similarly help hardware innovation, but has not yet due to several reasons. We identify these reasons and how the industry, academia, and the hardware community at large can come together to address them.", "venue": "ArXiv", "authors": ["Gagan  Gupta", "Tony  Nowatzki", "Vinay  Gangadhar", "Karthikeyan  Sankaralingam"], "year": 2016, "n_citations": 6}
{"id": 5025199, "s2_id": "47298b6e457199cade0433fe83fb216d051ef824", "title": "Solving the Hamiltonian path problem with a light-based computer", "abstract": "In this paper we propose a special computational device which uses light rays for solving the Hamiltonian path problem on a directed graph. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. In each node the rays are uniquely marked so that they can be easily identified. At the destination node we will search only for particular rays that have passed only once through each node. We show that the proposed device can solve small and medium instances of the problem in reasonable time.", "venue": "Natural Computing", "authors": ["Mihai  Oltean"], "year": 2007, "n_citations": 58}
{"id": 5026783, "s2_id": "7e319afc1ad486ca5e74cce15aa57a0fddf13ca0", "title": "Test planning for mixed-signal SOCs with wrapped analog cores", "abstract": "Many SOCs today contain both digital and analog embedded cores. Even though the test cost for such mixed-signal SOCs is significantly higher than that for digital SOCs, most prior research in this area has focused exclusively on digital cores. We propose a low-cost test development methodology for mixed-signal SOCs that allows the analog and digital cores to be tested in a unified manner, thereby minimizing the overall test cost. The analog cores in the SOC are wrapped such that they can be accessed using a digital test access mechanism (TAM). We evaluate the impact of the use of analog test wrappers on area overhead and test time. To reduce area overhead, we present an analog test wrapper optimization technique, which is then combined with TAM optimization in a cost-oriented heuristic approach for test scheduling. We also demonstrate the feasibility of using analog wrappers by presenting transistor-level simulations for an analog wrapper and a representative core. We present experimental results on test scheduling for an ITC'02 benchmark SOC that has been augmented with five analog cores.", "venue": "Design, Automation and Test in Europe", "authors": ["Anuja  Sehgal", "Fang  Liu", "Sule  Ozev", "Krishnendu  Chakrabarty"], "year": 2005, "n_citations": 10}
{"id": 5030193, "s2_id": "4491544a0ba6d6d6514bc1ca7258e3d515a4f75e", "title": "Selective Match-Line Energizer Content Addressable Memory(SMLE -CAM)", "abstract": "A Content Addressable Memory (CAM) is a memory primarily designed for high speed search operation. Parallel search scheme forms the basis of CAM, thus power reduction is the challenge associated with a large amount of parallel active circuits. We are presenting a novel algorithm and architecture described as Selective Match-Line Energizer Content Addressable Memory (SMLE-CAM) which energizes only those MLs (Match-Line) whose first three bits are conditionally matched with corresponding first three search bit using special architecture which comprises of novel XNOR-CAM cell and novel XOR-CAM cell. The rest of the CAM chain is followed by NOR-CAM cell. The 256 X 144 bit SMLE-CAM is implemented in TSMC 90 nm technology and its robustness across PVT variation is verified. The post-layout simulation result shows, it has energy metric of 0.115 fJ/bit/search with search time 361.6 ps, the best reported so far. The maximum operating frequency is 1GHz.", "venue": "ArXiv", "authors": ["V. Mohammed Zackriya", "Harish M. Kittur"], "year": 2014, "n_citations": 7}
{"id": 5031121, "s2_id": "39343d0070691acac716fdccb1e807e69ec8ea02", "title": "Encoder-Decoder Networks for Analyzing Thermal and Power Delivery Networks", "abstract": "Power delivery network (PDN) analysis and thermal analysis are computationally expensive tasks that are essential for successful IC design. Algorithmically, both these analyses have similar computational structure and complexity as they involve the solution to a partial differential equation of the same form. This paper converts these analyses into image-to-image and sequence-to-sequence translation tasks, which allows leveraging a class of machine learning models with an encoder-decoder-based generative (EDGe) architecture to address the time-intensive nature of these tasks. For PDN analysis, we propose two networks: (i) IREDGe: a full-chip static and dynamic IR drop predictor and (ii) EMEDGe: electromigration (EM) hotspot classifier based on input power, power grid distribution, and power pad distribution patterns. For thermal analysis, we propose ThermEDGe, a full-chip static and dynamic temperature estimator based on input power distribution patterns for thermal analysis. These networks are transferable across designs synthesized within the same technology and packing solution. The networks predict on-chip IR drop, EM hotspot locations, and temperature in milliseconds with negligibly small errors against commercial tools requiring several hours.", "venue": "ArXiv", "authors": ["Vidya A. Chhabria", "Vipul  Ahuja", "Ashwath  Prabhu", "Nikhil  Patil", "Palkesh  Jain", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 0}
{"id": 5035979, "s2_id": "405f5905defbb2612b31b0b8e9388c2ce6d4e91a", "title": "JackHammer: Efficient Rowhammer on Heterogeneous FPGA-CPU Platforms", "abstract": "After years of development, FPGAs are finally making an appearance on multi-tenant cloud servers. These heterogeneous FPGA-CPU architectures break common assumptions about isolation and security boundaries. Since the FPGA and CPU architectures share hardware resources, a new class of vulnerabilities requires us to reassess the security and dependability of these platforms. \nIn this work, we analyze the memory and cache subsystem and study Rowhammer and cache attacks enabled on two proposed heterogeneous FPGA-CPU platforms by Intel: the Arria 10 GX with an integrated FPGA-CPU platform, and the Arria 10 GX PAC expansion card which connects the FPGA to the CPU via the PCIe interface. We show that while Intel PACs currently are immune to cache attacks from FPGA to CPU, the integrated platform is indeed vulnerable to Prime and Probe style attacks from the FPGA to the CPU's last level cache. Further, we demonstrate JackHammer, a novel and efficient Rowhammer from the FPGA to the host's main memory. Our results indicate that a malicious FPGA can perform twice as fast as a typical Rowhammer attack from the CPU on the same system and causes around four times as many bit flips as the CPU attack. We demonstrate the efficacy of JackHammer from the FPGA through a realistic fault attack on the WolfSSL RSA signing implementation that reliably causes a fault after an average of fifty-eight RSA signatures, 25% faster than a CPU rowhammer attack. In some scenarios our JackHammer attack produces faulty signatures more than three times more often and almost three times faster than a conventional CPU rowhammer attack.", "venue": "IACR Trans. Cryptogr. Hardw. Embed. Syst.", "authors": ["Zane  Weissman", "Thore  Tiemann", "Daniel  Moghimi", "Evan  Custodio", "Thomas  Eisenbarth", "Berk  Sunar"], "year": 2020, "n_citations": 11}
{"id": 5039010, "s2_id": "547484e1e04b0dcfe9e175610dd5d492e99a3ca7", "title": "Representing Gate-Level SET Faults by Multiple SEU Faults at RTL", "abstract": "The advanced complex electronic systems increasingly demand safer and more secure hardware parts. Correspondingly, fault injection became a major verification milestone for both safety- and security-critical applications. However, fault injection campaigns for gate-level designs suffer from huge execution times. Therefore, designers need to apply early design evaluation techniques to reduce the execution time of fault injection campaigns. In this work, we propose a method to represent gate-level Single-Event Transient (SET) faults by multiple Single-Event Upset (SEU) faults at the Register-Transfer Level. Introduced approach is to identify true and false logic paths for each SET in the flip-flops\u2019 fan-in logic cones to obtain more accurate sets of flip-flops for multiple SEUs injections at RTL. Experimental results demonstrate the feasibility of the proposed method to successfully reduce the fault space and also its advantage with respect to state of the art. It was shown that the approach is able to reduce the fault space, and therefore the fault-injection effort, by up to tens to hundreds of times.", "venue": "2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS)", "authors": ["Ahmet Cagri Bagbaba", "Maksim  Jenihhin", "Raimund  Ubar", "Christian  Sauer"], "year": 2020, "n_citations": 0}
{"id": 5042828, "s2_id": "3f48910a9d62991ff6cceb5cc26a25e6bbb58166", "title": "Foveal-pit inspired filtering of DVS spike response", "abstract": "In this paper, we present results of processing Dynamic Vision Sensor (DVS) recordings of visual patterns with a retinal model based on foveal-pit inspired Difference of Gaussian (DoG) filters. A DVS sensor was stimulated with varying number of vertical white and black bars of different spatial frequencies moving horizontally at a constant velocity. The output spikes generated by the DVS sensor were applied as input to a set of DoG filters inspired by the receptive field structure of the primate visual pathway. In particular, these filters mimic the receptive fields of the midget and parasol ganglion cells (spiking neurons of the retina) that sub-serve the photo-receptors of the foveal-pit. The features extracted with the foveal-pit model are used for further classification using a spiking convolutional neural network trained with a backpropagation variant adapted for spiking neural networks.", "venue": "2021 55th Annual Conference on Information Sciences and Systems (CISS)", "authors": ["Shriya T. P. Gupta", "Pablo  Linares-Serrano", "Basabdatta Sen Bhattacharya", "Teresa  Serrano-Gotarredona"], "year": 2021, "n_citations": 0}
{"id": 5046455, "s2_id": "2e616c45e66b542ccfa2193158bc89d513a6c2f2", "title": "FPGA based agile algorithm-on-demand coprocessor", "abstract": "With the growing computational needs of many real-world applications, frequently changing specifications of standards, and the high design and NRE costs of ASICs, an algorithm-agile FPGA based coprocessor has become a viable alternative. We report the general design of an algorithm-agile coprocessor and the proof-of-concept implementation.", "venue": "Design, Automation and Test in Europe", "authors": ["Ramachandran  Pradeep", "S.  Vinay", "Sanjay  Burman", "V.  Kamakoti"], "year": 2005, "n_citations": 8}
{"id": 5051421, "s2_id": "7c3b0fda8bda8a1236e27326f34625f0d111e678", "title": "On the Resilience of RTL NN Accelerators: Fault Characterization and Mitigation", "abstract": "Machine Learning (ML) is making a strong resurgence in tune with the massive generation of unstructured data which in turn requires massive computational resources. Due to the inherently compute and power-intensive structure of Neural Networks (NNs), hardware accelerators emerge as a promising solution. However, with technology node scaling below 10nm, hardware accelerators become more susceptible to faults, which in turn can impact the NN accuracy. In this paper, we study the resilience aspects of Register-Transfer Level (RTL) model of NN accelerators, in particular, fault characterization and mitigation. By following a High-Level Synthesis (HLS) approach, first, we characterize the vulnerability of various components of RTL NN. We observed that the severity of faults depends on both i) application-level specifications, i.e., NN data (inputs, weights, or intermediate) and NN layers and ii) architectural-level specifications, i.e., data representation model and the parallelism degree of the underlying accelerator. Second, motivated by characterization results, we present a low-overhead fault mitigation technique that can efficiently correct bit flips, by 47.3% better than state-of-the-art methods.", "venue": "2018 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)", "authors": ["Behzad  Salami", "Osman S. Unsal", "Adri\u00e1n  Cristal"], "year": 2018, "n_citations": 34}
{"id": 5054115, "s2_id": "3a8fd354119608d310be342e0e5ecc741d448ac4", "title": "Exploiting Data Longevity for Enhancing the Lifetime of Flash-based Storage Class Memory", "abstract": "This paper proposes to exploit the capability of retention time relaxation in flash memories for improving the lifetime of an SLC-based SSD. The main idea is that as a majority of I/O data in a typical workload do not need a retention time larger than a few days, we can have multiple partial program states in a cell and use every two states to store one-bit data at each time. Thus, we can store multiple bits in a cell (one bit at each time) without erasing it after each write -- that would directly translates into lifetime enhancement. The proposed scheme is called Dense-SLC (D-SLC) flash design which improves SSD lifetime by 5.1X--8.6X.", "venue": "SIGMETRICS 2017", "authors": ["Wonil  Choi", "Mohammad  Arjomand", "Myoungsoo  Jung", "Mahmut T. Kandemir"], "year": 2017, "n_citations": 5}
{"id": 5054522, "s2_id": "8dbbfe744e938b6af4145fe17504ed054b1154bc", "title": "Combinatorics and Geometry for the Many-ported, Distributed and Shared Memory Architecture", "abstract": "Manycore SoC architectures based on on-chip shared memory are preferred for flexible and programmable solutions in many application domains. However, the development of many ported memory is becoming increasingly challenging as we approach the end of Moores Law while systems requirements demand larger shared memory and more access ports. Memory can no longer be designed simply to minimize single transaction access time, but must take into account the functionality on the SoC. In this paper we examine a common large memory usage in SoC, where the memory is used as storage for large buffers that are then moved for time scheduled processing. We merge two aspects of many ported memory design, combinatorial analysis of interconnect, and geometric analysis of critical paths, extending both to show that in this case the SoC performance benefits significantly from a hierarchical, distributed and staged architecture with lower-radix switches and fractal randomization of memory bank addressing, along with judicious and geometry aware application of speed up. The results presented show the new architecture supports 20% higher throughput with 20% lower latency and 30% less interconnection area at approximately the same power consumption. We demonstrate the flexibility and scalability of this architecture on silicon from a physical design perspective by taking the design through layout. The architecture enables a much easier implementation flow that works well with physically irregular port access and memory dominant layout, which is a common issue in real designs.", "venue": "2020 14th IEEE/ACM International Symposium on Networks-on-Chip (NOCS)", "authors": ["Hao  Luan", "Alan  Gatherer"], "year": 2020, "n_citations": 1}
{"id": 5058364, "s2_id": "949da107af472e5c64b6d1edb746b091470d5d7c", "title": "CNN-MERP: An FPGA-based memory-efficient reconfigurable processor for forward and backward propagation of convolutional neural networks", "abstract": "Large-scale deep convolutional neural networks (CNNs) are widely used in machine learning applications. While CNNs involve huge complexity, VLSI (ASIC and FPGA) chips that deliver high-density integration of computational resources are regarded as a promising platform for CNN's implementation. At massive parallelism of computational units, however, the external memory bandwidth, which is constrained by the pin count of the VLSI chip, becomes the system bottleneck. Moreover, VLSI solutions are usually regarded as a lack of the flexibility to be reconfigured for the various parameters of CNNs. This paper presents CNN-MERP to address these issues. CNN-MERP incorporates an efficient memory hierarchy that significantly reduces the bandwidth requirements from multiple optimizations including on/off-chip data allocation, data flow optimization and data reuse. The proposed 2-level reconfigurability is utilized to enable fast and efficient reconfiguration, which is based on the control logic and the multiboot feature of FPGA. As a result, an external memory bandwidth requirement of 1.94MB/GFlop is achieved, which is 55% lower than prior arts. Under limited DRAM bandwidth, a system throughput of 1244GFlop/s is achieved at the Vertex UltraScale platform, which is 5.48 times higher than the state-of-the-art FPGA implementations.", "venue": "2016 IEEE 34th International Conference on Computer Design (ICCD)", "authors": ["Xushen  Han", "Dajiang  Zhou", "Shihao  Wang", "Shinji  Kimura"], "year": 2016, "n_citations": 24}
{"id": 5058815, "s2_id": "f838c7c6f76407aaca0b898a076439ab7d1e7261", "title": "Fourier domain decoding algorithm of non-binary LDPC codes for parallel implementation", "abstract": "For decoding non-binary low-density parity-check (LDPC) codes, logarithm-domain sum-product (Log-SP) algorithms were proposed for reducing quantization effects of SP algorithm in conjunction with FFT. Since FFT is not applicable in the logarithm domain, the computations required at check nodes in the Log-SP algorithms are computationally intensive. What is worse, check nodes usually have higher degree than variable nodes. As a result, most of the time for decoding is used for check node computations, which leads to a bottleneck effect. In this paper, we propose a Log-SP algorithm in the Fourier domain. With this algorithm, the role of variable nodes and check nodes are switched. The intensive computations are spread over lower-degree variable nodes, which can be efficiently calculated in parallel. Furthermore, we develop a fast calculation method for the estimated bits and syndromes in the Fourier domain.", "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Kenta  Kasai", "Kohichi  Sakaniwa"], "year": 2011, "n_citations": 10}
{"id": 5060422, "s2_id": "384d1e83634f39e8813bdb7445b73c445f119b45", "title": "DRAM Failure Prediction in AIOps: EmpiricalEvaluation, Challenges and Opportunities", "abstract": "DRAM failure prediction is a vital task in AIOps, which is crucial to maintain the reliability and sustainable service of large-scale data centers. However, limited work has been done on DRAM failure prediction mainly due to the lack of public available datasets. This paper presents a comprehensive empirical evaluation of diverse machine learning techniques for DRAM failure prediction using a large-scale multisource dataset, including more than three millions of records of kernel, address, and mcelog data, provided by Alibaba Cloud through PAKDD 2021 competition. Particularly, we first formulate the problem as a multiclass classification task and exhaustively evaluate seven popular/stateof-the-art classifiers on both the individual and multiple data sources. We then formulate the problem as an unsupervised anomaly detection task and evaluate three state-of-the-art anomaly detectors. Further, based on the empirical results and our experience of attending this competition, we discuss major challenges and present future research opportunities in this task.", "venue": "ArXiv", "authors": ["Zhiyue  Wu", "Hongzuo  Xu", "Guansong  Pang", "Fengyuan  Yu", "Yijie  Wang", "Songlei  Jian", "Yongjun  Wang"], "year": 2021, "n_citations": 0}
{"id": 5062924, "s2_id": "41d6e810ad96356012bffb90d382d5c1d2726f9d", "title": "Side-Channel Hardware Trojan for Provably-Secure SCA-Protected Implementations", "abstract": "Hardware Trojans have drawn the attention of academia, industry, and government agencies. Effective detection mechanisms and countermeasures against such malicious designs can only be developed when there is a deep understanding of how hardware Trojans can be built in practice, in particular, Trojans specifically designed to avoid detection. In this article, we present a mechanism to introduce an extremely stealthy hardware Trojan into cryptographic primitives equipped with provably-secure first-order side-channel countermeasures. Once the Trojan is triggered, the malicious design exhibits exploitable side-channel leakage, leading to successful key recovery attacks. Generally, such a Trojan requires neither addition nor removal of any logic which makes it extremely hard to detect. On ASICs, it can be inserted by subtle manipulations at the subtransistor level and on FPGAs by changing the routing of particular signals, leading to zero logic overhead. The underlying concept is based on modifying a securely masked hardware implementation in such a way that running the device at a particular clock frequency violates one of its essential properties, leading to exploitable leakage. We apply our technique to a threshold implementation of the PRESENT block cipher realized in two different CMOS technologies and show that triggering the Trojan makes the ASIC prototypes vulnerable.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Samaneh  Ghandali", "Thorben  Moos", "Amir  Moradi", "Christof  Paar"], "year": 2020, "n_citations": 9}
{"id": 5063443, "s2_id": "b2842fa1b14e628da3bc956278b120644268d30f", "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric Bit-Width Operands", "abstract": "Matrix multiplications between asymmetric bit-width operands, especially between 8- and 4-bit operands are likely to become a fundamental kernel of many important workloads including neural networks and machine learning. While existing SIMD matrix multiplication instructions for symmetric bit-width operands can support operands of mixed precision by zero- or sign-extending the narrow operand to match the size of the other operands, they cannot exploit the benefit of narrow bit-width of one of the operands. We propose a new SIMD matrix multiplication instruction that uses mixed precision on its inputs (8- and 4-bit operands) and accumulates product values into narrower 16-bit output accumulators, in turn allowing the SIMD operation at 128-bit vector width to process a greater number of data elements per instruction to improve processing throughput and memory bandwidth utilization without increasing the register read- and write-port bandwidth in CPUs. The proposed asymmetric-operand-size SIMD instruction offers 2x improvement in throughput of matrix multiplication in comparison to throughput obtained using existing symmetric-operand-size instructions while causing negligible (0.05%) overflow from 16-bit accumulators for representative machine learning workloads. The asymmetric-operand-size instruction not only can improve matrix multiplication throughput in CPUs, but also can be effective to support multiply-and-accumulate (MAC) operation between 8- and 4-bit operands in state-of-the-art DNN hardware accelerators (e.g., systolic array microarchitecture in Google TPU, etc.) and offer similar improvement in matrix multiply performance seamlessly without violating the various implementation constraints. We demonstrate how a systolic array architecture designed for symmetric-operand-size instructions could be modified to support an asymmetric-operand-sized instruction.", "venue": "ArXiv", "authors": ["Dibakar  Gope", "Jesse  Beu", "Matthew  Mattina"], "year": 2020, "n_citations": 2}
{"id": 5076790, "s2_id": "69553532bcf11abd4fb8823cdc3aa21174412791", "title": "Embedded Systems Architecture for SLAM Applications", "abstract": "In recent years, we have observed a clear trend in the rapid rise of autonomous vehicles, robotics, virtual reality, and augmented reality. The core technology enabling these applications, Simultaneous Localization And Mapping (SLAM), imposes two main challenges: first, these workloads are computationally intensive and they often have real-time requirements; second, these workloads run on battery-powered mobile devices with limited energy budget. In short, the essence of these challenges is that performance should be improved while simultaneously reducing energy consumption, two rather contradicting goals by conventional wisdom. In this paper, we take a close look at state-of-the-art Simultaneous Localization And Mapping (SLAM) workloads, especially how these workloads behave on mobile devices. Based on the results, we propose a mobile architecture to improve SLAM performance on mobile devices.", "venue": "ArXiv", "authors": ["Jie  Tang", "Shaoshan  Liu", "Jean-Luc  Gaudiot"], "year": 2017, "n_citations": 2}
{"id": 5077588, "s2_id": "77c2faaf3f170822c3e37c4ecb4c8f13894b27ab", "title": "CODIC: A Low-Cost Substrate for Enabling Custom In-DRAM Functionalities and Optimizations", "abstract": "DRAM is the dominant main memory technology used in modern computing systems. Computing systems implement a memory controller that interfaces with DRAM via DRAM commands. DRAM executes the given commands using internal components (e.g., access transistors, sense amplifiers) that are orchestrated by DRAM internal timings, which are fixed for each DRAM command. Unfortunately, the use of fixed internal timings limits the types of operations that DRAM can perform and hinders the implementation of new functionalities and custom mechanisms that improve DRAM reliability, performance and energy. To overcome these limitations, we propose enabling programmable DRAM internal timings for controlling in-DRAM components.To this end, we design CODIC, a new low-cost DRAM substrate that enables fine-grained control over four previously fixed internal DRAM timings that are key to many DRAM operations. We implement CODIC with only minimal changes to the DRAM chip and the DDRx interface. To demonstrate the potential of CODIC, we propose two new CODIC-based security mechanisms that outperform state-of-the-art mechanisms in several ways: (1) a new DRAM Physical Unclonable Function (PUF) that is more robust and has significantly higher throughput than state-of-the-art DRAM PUFs, and (2) the first cold boot attack prevention mechanism that does not introduce any performance or energy overheads at runtime.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Lois  Orosa", "Yaohua  Wang", "Mohammad  Sadrosadati", "Jeremie S. Kim", "Minesh  Patel", "Ivan  Puddu", "Haocong  Luo", "Kaveh  Razavi", "Juan  G'omez-Luna", "Hasan  Hassan", "Nika  Mansouri-Ghiasi", "Saugata  Ghose", "Onur  Mutlu"], "year": 2021, "n_citations": 4}
{"id": 5078190, "s2_id": "c7e9cd46ac0ef83e8d6d2dbc517f4666a113d00a", "title": "Design of a Low-Power 1.65 Gbps Data Channel for HDMI Transmitter", "abstract": "This paper presents a design of low power data channel for application in High Definition Multimedia Interface (HDMI) Transmitter circuit. The input is 10 bit parallel data and output is serial data at 1.65 Gbps. This circuit uses only a single frequency of serial clock input. All other timing signals are derived within the circuit from the serial clock. This design has dedicated lines to disable and enable all its channels within two pixel-clock periods only. A pair of disable and enable functions performed immediately after power-on of the circuit serves as the reset function. The presented design is immune to data-dependent switching spikes in supply current and pushes them in the range of serial frequency and its multiples. Thus filtering requirements are relaxed. The output stage uses a bias voltage of 2.8 volts for a receiver pull-up voltage of 3.3 volts. The reported data channel is designed using UMC 180 nm CMOS Technology. The design is modifiable for other inter-board serial interfaces like USB and LAN with different number of bits at the parallel input.", "venue": "ArXiv", "authors": ["Ajay  Agrawal", "R. S. Gamad"], "year": 2016, "n_citations": 0}
{"id": 5079733, "s2_id": "836d0b3c90c45918efff4ccfb6873bfee36a59f4", "title": "The Validation of Graph Model-Based, Gate Level Low-Dimensional Feature Data for Machine Learning Applications", "abstract": "As an alternative to traditional fault injection-based methodologies and to explore the applicability of modern machine learning algorithms in the field of reliability engineering, this paper proposes a systemic framework that explores gate-level netlist circuit abstractions to extract and exploit relevant feature representations in a low-dimensional vector space. A scalable feature learning method on a graphical domain called node2vec algorithm [6] had been utilized for efficiently extracting structural features of the netlist, providing a valuable database to exercise a selection of machine learning (ML) or deep learning (DL) algorithms aiming at predicting fault propagation metrics. The current work proposes to model the gate-level netlist as a Probabilistic Bayesian Graph (PGB) in the form of a Graph Modeling Language (GML) format. To accomplish this goal, a Verilog Procedural Interface (VPI) library linked to standard simulation tools has been built to map gate-level netlist into the graph model. The extracted features have used for predicting the Functional Derating (FDR) factors of individual flip-flops of a given circuit through Support Vector Machine (SVM) and Deep Neural Network (DNN) algorithms. The results of the approach have been compared against data obtained through first-principles approaches. The whole experiment implemented on the features extracted from the 10-Gigabit Ethernet MAC IEEE 802.3 standard circuit.", "venue": "2019 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International Symposium of System-on-Chip (SoC)", "authors": ["Aneesh  Balakrishnan", "Thomas  Lange", "Maximilien  Glorieux", "Dan  Alexandrescu", "Maksim  Jenihhin"], "year": 2019, "n_citations": 3}
{"id": 5081376, "s2_id": "a5ddb17a3e51365c02079f48a7358d6ba433ec00", "title": "QuTiBench: Benchmarking Neural Networks on Heterogeneous Hardware", "abstract": "Neural Networks have become one of the most successful universal machine learning algorithms. They play a key role in enabling machine vision and speech recognition for example. Their computational complexity is enormous and comes along with equally challenging memory requirements, which limits deployment in particular within energy constrained, embedded environments. In order to address these implementation challenges, a broad spectrum of new customized and heterogeneous hardware architectures have emerged, often accompanied with co-designed algorithms to extract maximum benefit out of the hardware. Furthermore, numerous optimization techniques are being explored for neural networks to reduce compute and memory requirements while maintaining accuracy. This results in an abundance of algorithmic and architectural choices, some of which fit specific use cases better than others. \nFor system level designers, there is currently no good way to compare the variety of hardware, algorithm and optimization options. While there are many benchmarking efforts in this field, they cover only subsections of the embedded design space. None of the existing benchmarks support essential algorithmic optimizations such as quantization, an important technique to stay on chip, or specialized heterogeneous hardware architectures. We propose a novel benchmark suite, QuTiBench, that addresses this need. QuTiBench is a novel multi-tiered benchmarking methodology that supports algorithmic optimizations such as quantization and helps system developers understand the benefits and limitations of these novel compute architectures in regard to specific neural networks and will help drive future innovation. We invite the community to contribute to QuTiBench in order to support the full spectrum of choices in implementing machine learning systems.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Michaela  Blott", "Lisa  Halder", "Miriam  Leeser", "Linda  Doyle"], "year": 2019, "n_citations": 8}
{"id": 5083225, "s2_id": "63186759bc07e09f92d20672417326d60082ed58", "title": "Post-route alleviation of dense meander segments in high-performance printed circuit boards", "abstract": "Length-matching is an important technique to balance delays of bus signals in high-performance PCB routing. Existing routers, however, may generate dense meander segments with small distance. Signals propagating across these meander segments exhibit a speedup effect due to crosstalks between the segments of the same wire, thus leading to mismatch of arrival times even with the same physical wire length. In this paper, we propose a post-processing method to enlarge the width and the distance of meander segments and distribute them more evenly on the board so that the crosstalks can be reduced. In the proposed framework, we model the sharing combinations of available routing areas after removing dense meander segments from the initial routing, as well as the generation of relaxed meander segments and their groups in subareas. Thereafter, this model is transformed into an ILP problem and solved efficiently. Experimental results show that the proposed method can extend the width and the distance of meander segments about two times even under very tight area constraints, so that the crosstalks and thus the speedup effect can be alleviated effectively in high-performance PCB designs.", "venue": "2013 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Tsun-Ming  Tseng", "Bing  Li", "Tsung-Yi  Ho", "Ulf  Schlichtmann"], "year": 2013, "n_citations": 4}
{"id": 5088615, "s2_id": "30805c08d4043ce22cc0ae6e992d51e723bbe03f", "title": "FlexSA: Flexible Systolic Array Architecture for Efficient Pruned DNN Model Training", "abstract": "Modern deep learning models have high memory and computation cost. To make them fast and memory-cost efficient, structured model pruning is commonly used. We find that pruning a model using a common training accelerator with large systolic arrays is extremely performance-inefficient. To make a systolic array efficient for pruning and training, we propose FlexSA, a flexible systolic array architecture. FlexSA dynamically reconfigures the systolic array structure and offers multiple sub-systolic operating modes, which are designed for energy- and memory bandwidth-efficient processing of tensors with different sizes and shapes. We also present a compilation heuristic for tiling matrix-multiplication-and-accumulation operations in a training workload to best utilize the resources of FlexSA. Based on our evaluation, FlexSA with the proposed compilation heuristic improves compute resource utilization of pruning and training modern CNN models by 37% compared to a conventional training accelerator with a large systolic array. FlexSA also improves on-chip data reuse by 1.7X saving 28% energy compared to naive systolic array splitting.", "venue": "ArXiv", "authors": ["Sangkug  Lym", "Mattan  Erez"], "year": 2020, "n_citations": 4}
{"id": 5094355, "s2_id": "1acea19316553c496dafed18933652e7b23de676", "title": "Fast Low-Complexity Decoders for Low-Rate Polar Codes", "abstract": "Polar codes are capacity-achieving error-correcting codes with an explicit construction that can be decoded with low-complexity algorithms. In this work, we show how the state-of-the-art low-complexity decoding algorithm can be improved to better accommodate low-rate codes. More constituent codes are recognized in the updated algorithm and dedicated hardware is added to efficiently decode these new constituent codes. We also alter the polar code construction to further decrease the latency and increase the throughput with little to no noticeable effect on error-correction performance. Rate-flexible decoders for polar codes of length 1024 and 2048 are implemented on FPGA. Over the previous work, they are shown to have from 22 % to 28 % lower latency and 26 % to 34 % greater throughput when decoding low-rate codes. On 65 nm ASIC CMOS technology, the proposed decoder for a (1024, 512) polar code is shown to compare favorably against the state-of-the-art ASIC decoders. With a clock frequency of 400 MHz and a supply voltage of 0.8 V, it has a latency of 0.41 \u03bcs and an area efficiency of 1.8 Gbps/mm 2 for an energy efficiency of 77 pJ/info. bit. At 600 MHz with a supply of 1 V, the latency is reduced to 0.27 \u03bcs and the area efficiency increased to 2.7 Gbps/mm 2 at 115 pJ/info. bit.", "venue": "J. Signal Process. Syst.", "authors": ["Pascal  Giard", "Alexios  Balatsoukas-Stimming", "Gabi  Sarkis", "Claude  Thibeault", "Warren J. Gross"], "year": 2018, "n_citations": 31}
{"id": 5094444, "s2_id": "552b19ca30b1e1287b2c65f9823d0d8664828ee9", "title": "MapVisual: A Visualization Tool for Memory Access Patterns", "abstract": "Memory bandwidth is strongly correlated to the complexity of the memory access pattern of a running application. To improve memory performance of applications with irregular and/or unpredictable memory patterns, we need tools to analyze these patterns during application development. In this work, we present a software tool for the analysis and visualization of memory access patterns. We perform memory tracing and profiling, we do data processing and filtering, and we use visualization algorithms to produce three dimensional graphs that describe the patterns both in space and in time. Finally, we evaluate our toolflow on a variety of applications.", "venue": "ArXiv", "authors": ["Pavlos  Aimoniotis", "Maria Rafaela Gkeka", "Nikolaos  Bellas"], "year": 2021, "n_citations": 0}
{"id": 5096531, "s2_id": "c93da9d6791ec6fdfd2f92c0995440844408a491", "title": "Preemptive thread block scheduling with online structural runtime prediction for concurrent GPGPU kernels", "abstract": "Recent NVIDIA Graphics Processing Units (GPUs) can execute multiple kernels concurrently. On these GPUs, the thread block scheduler (TBS) currently uses the FIFO policy to schedule thread blocks of concurrent kernels. We show that the FIFO policy leaves performance to chance, resulting in significant loss of performance and fairness. To improve performance and fairness, we propose use of the preemptive Shortest Remaining Time First (SRTF) policy instead. Although SRTF requires an estimate of runtime of GPU kernels, we show that such an estimate of the runtime can be easily obtained using online profiling and exploiting a simple observation on GPU kernels' grid structure. Specifically, we propose a novel Structural Runtime Predictor. Using a simple Staircase model of GPU kernel execution, we show that the runtime of a kernel can be predicted by profiling only the first few thread blocks. We evaluate an online predictor based on this model on benchmarks from ERCBench, and find that it can estimate the actual runtime reasonably well after the execution of only a single thread block. Next, we design a thread block scheduler that is both concurrent kernel-aware and uses this predictor. We implement the Shortest Remaining Time First (SRTF) policy and evaluate it on two-program workloads from ER-CBench. SRTF improves STP by 1.18\u00d7 and ANTT by 2.25\u00d7 over FIFO. When compared to MPMax, a state-of-the-art resource allocation policy for concurrent kernels, SRTF improves STP by 1.16\u00d7 and ANTT by 1.3\u00d7. To improve fairness, we also propose SRTF/Adaptive which controls resource usage of concurrently executing kernels to maximize fairness. SRTF/Adaptive improves STP by 1.12\u00d7, ANTT by 2.23\u00d7 and Fairness by 2.95\u00d7 compared to FIFO. Overall, our implementation of SRTF achieves system throughput to within 12.64% of Shortest Job First (SJF, an oracle optimal scheduling policy), bridging 49% of the gap between FIFO and SJF.", "venue": "2014 23rd International Conference on Parallel Architecture and Compilation (PACT)", "authors": ["Sreepathi  Pai", "R.  Govindarajan", "Matthew J. Thazhuthaveetil"], "year": 2014, "n_citations": 13}
{"id": 5101863, "s2_id": "5b040bab987e529c8092047da49971dfb92d4359", "title": "An extension to DNA based Fredkin gate circuits: design of reversible sequential circuits using Fredkin gates", "abstract": "In recent years, reversible logic has emerged as a promising computing paradigm having its applications in low power computing, quantum computing, nanotechnology, optical computing and DNA computing. The classical set of gates such as AND, OR, and EXOR are not reversible. Recently, it has been shown how to encode information in DNA and use DNA amplification to implement Fredkin gates. Furthermore, in the past Fredkin gates have been constructed using DNA, whose outputs are used as inputs for other Fredkin gates. Thus, it can be concluded that arbitrary circuits of Fredkin gates can be constructed using DNA. This paper provides the initial threshold to building of more complex system having reversible sequential circuits and which can execute more complicated operations. The novelty of the paper is the reversible designs of sequential circuits using Fredkin gate. Since, Fredkin gate has already been realized using DNA, it is expected that this work will initiate the building of complex systems using DNA. The reversible circuits designed here are highly optimized in terms of number of gates and garbage outputs. The modularization approach that is synthesizing small circuits and thereafter using them to construct bigger circuits is used for designing the optimal reversible sequential circuits.", "venue": "International Symposium on Optomechatronic Technologies", "authors": ["Himanshu  Thapliyal", "M. B. Srinivas"], "year": 2005, "n_citations": 16}
{"id": 5106210, "s2_id": "23dfce717434b3ca17b3a0044b2a6270378cd840", "title": "TPAD: Hardware Trojan Prevention and Detection for Trusted Integrated Circuits", "abstract": "There are increasing concerns about possible malicious modifications of integrated circuits (ICs) used in critical applications. Such attacks are often referred to as hardware Trojans. While many techniques focus on hardware Trojan detection during IC testing, it is still possible for attacks to go undetected. Using a combination of new design techniques and new memory technologies, we present a new approach that detects a wide variety of hardware Trojans during IC testing and also during system operation in the field. Our approach can also prevent a wide variety of attacks during synthesis, place-and-route, and fabrication of ICs. It can be applied to any digital system, and can be tuned for both traditional and split-manufacturing methods. We demonstrate its applicability for both application-specified integrated circuits and field-programmable gate arrays. Using fabricated test chips with Trojan emulation capabilities and also using simulations, we demonstrate: 1) the area and power costs of our approach can range between 7.4%-165% and 7%-60%, respectively, depending on the design and the attacks targeted; 2) the speed impact can be minimal (close to 0%); 3) our approach can detect 99.998% of Trojans (emulated using test chips) that do not require detailed knowledge of the design being attacked; 4) our approach can prevent 99.98% of specific attacks (simulated) that utilize detailed knowledge of the design being attacked (e.g., through reverse engineering); and 5) our approach never produces any false positives, i.e., it does not report attacks when the IC operates correctly.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Tony F. Wu", "Karthik  Ganesan", "Yunqing Alexander Hu", "H.-S. Philip Wong", "S. Simon Wong", "Subhasish  Mitra"], "year": 2016, "n_citations": 75}
{"id": 5106986, "s2_id": "fd3de18b7819ca3c7798397a734f689b5e7037fa", "title": "High Area/Energy Efficiency RRAM CNN Accelerator with Kernel-Reordering Weight Mapping Scheme Based on Pattern Pruning", "abstract": "Resistive Random Access Memory (RRAM) is an emerging device for processing-in-memory (PIM) architecture to accelerate convolutional neural network (CNN). However, due to the highly coupled crossbar structure in the RRAM array, it is difficult to exploit the sparsity of the network in RRAM-based CNN accelerator. To optimize the weight mapping of sparse network in the RRAM array and achieve high area and energy efficiency, we propose a novel weight mapping scheme and corresponding RRAM-based CNN accelerator architecture based on pattern pruning and Operation Unit(OU) mechanism. Experimental results show that our work can achieve 4.16x-5.20x crossbar area efficiency, 1.98x-2.15x energy efficiency, and 1.15x-1.35x performance speedup in comparison with the traditional weight mapping method.", "venue": "ArXiv", "authors": ["Songming  Yu", "Yongpan  Liu", "Lu  Zhang", "Jingyu  Wang", "Jinshan  Yue", "Zhuqing  Yuan", "Xueqing  Li", "Huazhong  Yang"], "year": 2020, "n_citations": 1}
{"id": 5109241, "s2_id": "116b28c6629dc5d0d10f992f7b6090265c9663dd", "title": "MARS: Middleware for Adaptive Reflective Computer Systems", "abstract": "Self-adaptive approaches for runtime resource management of manycore computing platforms often require a runtime model of the system that represents the software organization or the architecture of the target platform. The increasing heterogeneity in a platform\u2019s resource types and the interactions between resources pose challenges for coordinated model-based decision making in the face of dynamic workloads. Self-awareness properties address these challenges for emerging heterogeneous manycore processing (HMP) platforms through reflective resource managers. However, with HMP computing platform architectures evolving rapidly, porting the self-aware decision logic across different hardware platforms is challenging, requiring resource managers to update their models and platform-specific interfaces. We propose MARS (Middleware for Adaptive and Reflective Systems), a cross-layer and multi-platform framework that allows users to easily create resource managers by composing system models and resource management policies in a flexible and coordinated manner. MARS consists of a generic user-level sensing/actuation interface that allows for portable policy design, and a reflective system model used to coordinate multiple policies. We demonstrate MARS\u2019 interaction across multiple layers of the system stack through a dynamic voltage and frequency scaling (DVFS) policy example which can run on any Linux-based HMP computing platform.", "venue": "ArXiv", "authors": ["Tiago  M\u00fcck", "Bryan  Donyanavard", "Biswadip  Maity", "Kasra  Moazzemi", "Nikil D. Dutt"], "year": 2021, "n_citations": 1}
{"id": 5112921, "s2_id": "8afe137d8f816854e2064a5b3bac218322a3647a", "title": "Fast Modeling L2 Cache Reuse Distance Histograms Using Combined Locality Information from Software Traces", "abstract": "To mitigate the performance gap between the CPU and the main memory, multi-level cache architectures are widely used in modern processors. Therefore, modeling the behaviors of the downstream caches becomes a critical part of the processor performance evaluation in the early stage of Design Space Exploration (DSE). In this paper, we propose a fast and accurate L2 cache reuse distance histogram model, which can be used to predict the behaviors of the multi-level cache architectures where the L1 cache uses the LRU replacement policy and the L2 cache uses LRU/Random replacement policies. We use the L1 reuse distance histogram and two newly proposed metrics, namely the RST table and the Hit-RDH, that describing more detailed information of the software traces as the inputs. The output of our model is the L2 cache reuse distance histogram, based on which the L2 cache miss rates can be evaluated. We compare the L2 cache miss rates with the results from gem5 cycle-accurate simulations of 15 benchmarks chosen from SPEC2006. The average absolute error is less than 5%, while the evaluation time can be sped up by 7.38 times. We also extend our model into a multi-core architecture in which two cores share a unified L2 cache. The error of our model, in this case, is less than 7%.", "venue": "J. Syst. Archit.", "authors": ["Ming  Ling", "Jiancong  Ge", "Guangmin  Wang"], "year": 2020, "n_citations": 4}
{"id": 5115598, "s2_id": "0399d95f18baa03a95d38a9baf4d56c09f5c1408", "title": "An improved distributed routing algorithm for Benes based optical NoC", "abstract": "Integrated optical interconnect is believed to be one of the main technologies to replace electrical wires. Optical Network-on-Chip (ONoC) has attracted more attentions nowadays. Benes topology is a good choice for ONoC for its rearrangeable non-blocking character, multistage feature and easy scalability. Routing algorithm plays an important role in determining the performance of ONoC. But traditional routing algorithms for Benes network are not suitable for ONoC communication, we developed a new distributed routing algorithm for Benes ONoC in this paper. Our algorithm selected the routing path dynamically according to network condition and enables more path choices for the message traveling in the network. We used OPNET to evaluate the performance of our routing algorithm and also compared it with a well-known bit-controlled routing algorithm. ETE delay and throughput were showed under different packet length and network sizes. Simulation results show that our routing algorithm can provide better performance for ONoC.", "venue": "International Conference on Image Processing and Pattern Recognition in Industrial Engineering", "authors": ["Jing  Zhang", "Huaxi  Gu", "Yintang  Yang"], "year": 2010, "n_citations": 3}
{"id": 5118374, "s2_id": "9901af6599d25e9a8349d28def8e4e5a65aeaafa", "title": "Evaluating cache coherent shared virtual memory for heterogeneous multicore chips", "abstract": "Although current homogeneous chips tightly couple the cores with cache-coherent shared virtual memory (CCSVM), this is not the communication paradigm used by any current heterogeneous chip. In this paper, we present a CCSVM design for a CPU/GPU chip, as well as an extension of the pthreads programming model for programming this HMC. We experimentally compare CCSVM/xthreads to a state-of-the-art CPU/GPU chip from AMD that runs OpenCL software. CCSVM's more efficient communication enables far better performance and far fewer DRAM accesses.", "venue": "2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Blake A. Hechtman", "Daniel J. Sorin"], "year": 2013, "n_citations": 17}
{"id": 5118920, "s2_id": "c0524e21b6cbf77f084c9c0679a5e9a80b2d1c0e", "title": "Early DSE and Automatic Generation of Coarse Grained Merged Accelerators", "abstract": "Post-Moore\u2019s law area-constrained systems rely on accelerators to deliver performance enhancements. Coarse grained accelerators can offer substantial domain acceleration, but manual, ad-hoc identification of code to accelerate is prohibitively expensive. Because cycle-accurate simulators and high-level synthesis (HLS) flows are so time-consuming, manual creation of high-utilization accelerators that exploit control and data flow patterns at optimal granularities is rarely successful. To address these challenges, we present AccelMerger, the first automated methodology to create coarse grained, controland data-flow-rich, merged accelerators. AccelMerger uses sequence alignment matching to recognize similar function call-graphs and loops, and neural networks to quickly evaluate their post-HLS characteristics. It accurately identifies which functions to accelerate, and it merges accelerators to respect an area budget and to accommodate system communication characteristics like latency and bandwidth. Merging two accelerators can save as much as 99% of the area of one. The space saved is used by a globally optimal integer linear program to allocate more accelerators for increased performance. We demonstate AccelMerger\u2019s effectiveness using HLS flows without any manual effort to fine-tune the resulting designs. On FPGA-based systems, AccelMerger yields application performance improvements of up to 16.7\u00d7 over software implementations, and 1.91\u00d7 on average with respect to state-ofthe-art early-stage design space exploration tools.", "venue": "ArXiv", "authors": ["Iulian  Brumar", "Georgios  Zacharopoulos", "Yuan  Yao", "Saketh  Rama", "Gu-Yeon  Wei", "David  Brooks"], "year": 2021, "n_citations": 0}
{"id": 5122905, "s2_id": "aa6e3e6477ba118ca033c38cfd87fa3132bd0953", "title": "Partitioned successive-cancellation list decoding of polar codes", "abstract": "Successive-cancellation list (SCL) decoding is an algorithm that provides very good error-correction performance for polar codes. However, its hardware implementation requires a large amount of memory, mainly to store intermediate results. In this paper, a partitioned SCL algorithm is proposed to reduce the large memory requirements of the conventional SCL algorithm. The decoder tree is broken into partitions that are decoded separately. We show that with careful selection of list sizes and number of partitions, the proposed algorithm can outperform conventional SCL while requiring less memory.", "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Seyyed Ali Hashemi", "Alexios  Balatsoukas-Stimming", "Pascal  Giard", "Claude  Thibeault", "Warren J. Gross"], "year": 2016, "n_citations": 38}
{"id": 5123712, "s2_id": "3b1ff936a1ec319a52a1eadbe70d937e350f40f7", "title": "On Timing Model Extraction and Hierarchical Statistical Timing Analysis", "abstract": "In this paper, we investigate the challenges of applying statistical static timing analysis in hierarchical design flow, where modules supplied by IP vendors are used to hide design details for IP protection and to reduce the complexity of design and verification. For the three basic circuit types, combinational, flip-flop-based, and latch-controlled, we propose methods for extracting timing models that contain interfacing and compressed internal constraints. Using these compact timing models, the runtime of full-chip timing analysis can be reduced, while circuit details from IP vendors are not exposed. We also propose a method for reconstructing correlation between modules during full-chip timing analysis. This correlation cannot be incorporated into timing models because it depends on the layout of the corresponding modules in the chip. In addition, we investigate how to apply the extracted timing models with the reconstructed correlation to evaluate the performance of the complete design. Experiments demonstrate that using the extracted timing models and reconstructed correlation full-chip timing analysis can be several times faster than applying the flattened circuit directly, while the accuracy of statistical timing analysis is still well maintained.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Bing  Li", "Ning  Chen", "Yang  Xu", "Ulf  Schlichtmann"], "year": 2013, "n_citations": 13}
{"id": 5125740, "s2_id": "4766b0c7f3b6dbc0f0bf1bd3d9ba6bbec71945aa", "title": "A Simulation Experiment on a Built-In Self Test Equipped with Pseudorandom Test Pattern Generator and Multi-Input Shift Register (MISR)", "abstract": "This paper investigates the impact of the changes of the characteristic polynomials and initial loadings, on behaviour of aliasing errors of parallel signature analyzer (Multi-Input Shift Register), used in an LFSR based digital circuit testing technique. The investigation is carried-out through an extensive simulation study of the effectiveness of the LFSR based digital circuit testing technique. The results of the study show that when the identical characteristic polynomials of order n are used in both pseudo-random test-pattern generator, as well as in Multi-Input Shift Register (MISR) signature analyzer (parallel type) then the probability of aliasing errors remains unchanged due to the changes in the initial loadings of the pseudo-random test-pattern generator.", "venue": "VLSIC 2010", "authors": ["A.  Ahmad"], "year": 2010, "n_citations": 16}
{"id": 5125840, "s2_id": "055273eb20b7fcdbc457e9849657523a7cdac54e", "title": "O-HAS: Optical Hardware Accelerator Search for Boosting Both Acceleration Performance and Development Speed", "abstract": "The recent breakthroughs and prohibitive complexities of Deep Neural Networks (DNNs) have excited extensive interest in domain specific DNN accelerators, among which optical DNN accelerators are particularly promising thanks to their unprecedented potential of achieving superior performance-per-watt. However, the development of optical DNN accelerators is much slower than that of electrical DNN accelerators. One key challenge is that while many techniques have been developed to facilitate the development of electrical DNN accelerators, techniques that support or expedite optical DNN accelerator design remain much less explored, limiting both the achievable performance and the innovation development of optical DNN accelerators. To this end, we develop the first-of-its-kind framework dubbed O-HAS, which for the first time demonstrates automated Optical Hardware Accelerator Search for boosting both the acceleration efficiency and development speed of optical DNN accelerators. Specifically, our O-HAS consists of two integrated enablers: (1) an O-Cost Predictor, which can accurately yet efficiently predict an optical accelerator's energy and latency based on the DNN model parameters and the optical accelerator design; and (2) an O-Search Engine, which can automatically explore the large design space of optical DNN accelerators and identify the optimal accelerators (i.e., the micro-architectures and algorithm-to-accelerator mapping methods) in order to maximize the target acceleration efficiency. Extensive experiments and ablation studies consistently validate the effectiveness of both our O-Cost Predictor and O-Search Engine as well as the excellent efficiency of O-HAS generated optical accelerators.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Mengquan  Li", "Zhongzhi  Yu", "Yongan  Zhang", "Yonggan  Fu", "Yingyan  Lin"], "year": 2021, "n_citations": 1}
{"id": 5125939, "s2_id": "01bb012ec3f0e63e5ad8d7770068b9af67bbefe7", "title": "Boolean Logic Gates from a Single Memristor via Low-Level Sequential Logic", "abstract": "By using the memristor\u2019s memory to both store a bit and perform an operation with a second input bit, simple Boolean logic gates have been built with a single memristor. The operation makes use of the interaction of current spikes (occasionally called current transients) found in both memristors and other devices. The sequential time-based logic methodology allows two logical input bits to be used on a one-port by sending the bits separated in time. The resulting logic gate is faster than one relying on memristor\u2019s state switching, low power and requires only one memristor. We experimentally demonstrate working OR and XOR gates made with a single flexible Titanium dioxide sol-gel memristor.", "venue": "UCNC", "authors": ["Ella  Gale", "Ben de Lacy Costello", "Andrew  Adamatzky"], "year": 2013, "n_citations": 39}
{"id": 5133762, "s2_id": "146966addad25ab92f32e694e5a1af787e460e4d", "title": "Optimality Study of Existing Quantum Computing Layout Synthesis Tools", "abstract": "Layout synthesis, an important step in quantum computing, processes quantum circuits to satisfy device layout constraints. In this paper, we construct QUEKO benchmarks for this problem, which have known optimal depths and gate counts. We use QUEKO to evaluate the optimality of current layout synthesis tools, including Cirq from Google, Qiskit from IBM, <inline-formula><tex-math notation=\"LaTeX\">$\\mathsf {t}|\\mathsf {ket}\\rangle$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"sans-serif\">t</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant=\"sans-serif\">ket</mml:mi><mml:mo>\u232a</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"tan-ieq1-3009140.gif\"/></alternatives></inline-formula> from Cambridge Quantum Computing, and a recent academic work. To our surprise, despite over a decade of research and development by academia and industry on compilation and synthesis for quantum circuits, we are still able to demonstrate large optimality gaps: 1.5-12x on average on a smaller device and 5-45x on average on a larger device. This suggests substantial room for improvement of the efficiency of quantum computer by better layout synthesis tools. Finally, we also prove the NP-completeness of the layout synthesis problem for quantum computing. We have made the QUEKO benchmarks open-source.", "venue": "IEEE Transactions on Computers", "authors": ["Bochen  Tan", "Jason  Cong"], "year": 2021, "n_citations": 20}
{"id": 5133887, "s2_id": "646c0f0489ca439aa47a6b824dbb4601749e0531", "title": "A Lightweight Isolation Mechanism for Secure Branch Predictors", "abstract": "Recently exposed vulnerabilities reveal that branch predictors shared by different processes leave the attackers with the opportunities for malicious training and perception. Instead of flush-based or physical isolation of hardware resources, we want to achieve isolation of the content in these hardware tables with some lightweight processing using randomization as follows. (1) Content encoding. We propose to use hardware-based thread-private random numbers to encode the contents of the branch predictor tables. It achieves a similar effect of logical isolation but adds little in terms of space or time overheads. (2) Index encoding. We propose a randomized index mechanism of the branch predictor. This disrupts the correspondence between the branch instruction address and the branch predictor entry, thus increases the noise for malicious perception attacks. Our analyses using an FPGA-based RISC-V processor prototype and additional auxiliary simulations suggest that the proposed mechanisms incur a very small performance cost while providing strong protection.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Lutan  Zhao", "Peinan  Li", "Rui  Hou", "Michael C. Huang", "Jiazhen  Li", "Lixin  Zhang", "Xuehai  Qian", "Dan  Meng"], "year": 2021, "n_citations": 9}
{"id": 5137126, "s2_id": "ab7c1b208ee75bbc4be020ebf0c85488d867755d", "title": "Empirical Measurements of Disk Failure Rates and Error Rates", "abstract": "The SATA advertised bit error rate of one error in 10 terabytes is frightening. We moved 2 PB through low-cost hardware and saw five disk read error events, several controller failures, and many system reboots caused by security patches. We conclude that SATA uncorrectable read errors are not yet a dominant system-fault source - they happen, but are rare compared to other problems. We also conclude that UER (uncorrectable error rate) is not the relevant metric for our needs. When an uncorrectable read error happens, there are typically several damaged storage blocks (and many uncorrectable read errors.) Also, some uncorrectable read errors may be masked by the operating system. The more meaningful metric for data architects is Mean Time To Data Loss (MTTDL.)", "venue": "ArXiv", "authors": ["Jim  Gray", "Catharine van Ingen"], "year": 2007, "n_citations": 82}
{"id": 5141118, "s2_id": "e257387155dac2eba2c2c3bcbc8d80fd2cb30545", "title": "VLSI Implementation of Novel Class of High Speed Pipelined Digital Signal Processing Filter for Wireless Receivers", "abstract": "The need for high performance transceiver with high Signal to Noise Ratio (SNR) has driven the communication system to utilize latest technique identified as over sampling systems. It was the most economical modulator and decimation in communication system. It has been proven to increase the SNR and is used in many high performance systems such as in the Analog to Digital Converter (ADC) for wireless transceiver. This research work presented the design of the novel class of decimation and its VLSI implementation which was the sub-component in the over sampling technique. The design and realization of main unit of decimation stage that was the Cascaded Integrator Comb (CIC) filter, the associated half band filters and the droop correction are also designed. The Verilog HDL code in Xilinx ISE environment has been derived to describe the proposed advanced CIC filter properties. Consequently, Virtex-II FPGA board was used to implement and test the design on the real hardware. The ASIC design implementation was performed accordingly and resulted power and area measurement on chip core layout. The proposed design focused on the trade-off between the high speed and the low power consumption as well as the silicon area and high resolution for the chip implementation which satisfies wireless communication systems. The synthesis report illustrates the maximum clock frequency of 332 MHz with the active core area of 0.308\u00d70.308 mm2. It can be concluded that VLSI implementation of proposed filter architecture is an enabler in solving problems that affect communication capability in DSP application.", "venue": "ArXiv", "authors": ["Rozita  Teymourzadeh", "Yazan Samir Algnabi", "Masuri  Othman", "Md. Shabiul Islam", "Mok Vee Hong"], "year": 2018, "n_citations": 0}
{"id": 5142424, "s2_id": "3cbbf89cdce242524b6869c85c9862a7766e9f79", "title": "\"It's a Trap!\"-How Speculation Invariance Can Be Abused with Forward Speculative Interference", "abstract": "Side-channel attacks based on speculative execution access sensitive data and use transmitters to leak such data during wrongpath execution. Speculative side-channel defenses have been proposed to prevent such information leakage. In one class of defenses, speculative instructions are considered unsafe and are delayed until they become non-speculative. However, not all speculative instructions are unsafe: Recent work demonstrates that speculative invariant instructions are independent of a speculative control-flow path and are guaranteed to eventually execute and commit, regardless of the outcome of the performed speculation. Compile time information coupled with run-time mechanisms can then selectively lift defenses for Speculative Invariant instructions, regaining some of the performance lost to \u201cdelay\u201d defenses. Unfortunately, speculative invariance can be easily mishandled with Speculative Interference to leak information using a new side-channel that we introduce in this paper. Recent work shows that younger speculative instructions can interfere with older non-speculative instructions that are bound to commit. This \u201cbackward\u201d speculative interference reveals speculatively accessed secrets through the non-speculative instructions, in a way that delay-defenses do not cover, rendering them ineffective for this type of attack. In our work, we show that the counterpart to backward speculative interference, i.e., forward speculative interference, enables older speculative instructions to interfere with younger speculative-invariant (bound-tocommit) instructions, effectively turning them into transmitters for secret data accessed during speculation. We demonstrate forward speculative interference on real hardware, by selectively filling the reorder buffer (ROB) with spurious instructions, pushing speculative-invariant instructions inor-out the ROB on demand, based on a speculatively accessed secret. This reveals the speculatively accessed secret, as the occupancy of the ROB itself becomes a new speculative side-channel. We also demonstrate that it is possible to use the x86 ISA REP prefix, which unrolls as a micro-op loop in the microarchitecture at decode time (before any sidechannel defenses have taken effect), as a method for generating spurious instructions. We propose several mitigations that range from changing compile-time decisions for speculative-invariance to run-time mechanisms that aim to make ROB occupancy operand-independent.", "venue": "ArXiv", "authors": ["Pavlos  Aimoniotis", "Christos  Sakalis", "Magnus  Sj\u00e4lander", "Stefanos  Kaxiras"], "year": 2021, "n_citations": 0}
{"id": 5145204, "s2_id": "e211b4456c004a4f0d4d617677c1cdb290db438b", "title": "A High-Throughput Energy-Efficient Implementation of Successive Cancellation Decoder for Polar Codes Using Combinational Logic", "abstract": "This paper proposes a high-throughput energy-efficient Successive Cancellation (SC) decoder architecture for polar codes based on combinational logic. The proposed combinational architecture operates at relatively low clock frequencies compared to sequential circuits, but takes advantage of the high degree of parallelism inherent in such architectures to provide a favorable tradeoff between throughput and energy efficiency at short to medium block lengths. At longer block lengths, the paper proposes a hybrid-logic SC decoder that combines the advantageous aspects of the combinational decoder with the low-complexity nature of sequential-logic decoders. Performance characteristics on ASIC and FPGA are presented with a detailed power consumption analysis for combinational decoders. Finally, the paper presents an analysis of the complexity and delay of combinational decoders, and of the throughput gains obtained by hybrid-logic decoders with respect to purely synchronous architectures.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Onur  Dizdar", "Erdal  Arikan"], "year": 2016, "n_citations": 69}
{"id": 5146192, "s2_id": "1d7992fe500570139e5f7b2863af0d15a85ea3e7", "title": "Application-Specific System Processor for the SHA-1 Hash Algorithm", "abstract": "This work proposes an Application-Specific System Processor (ASSP) hardware for the Secure Hash Algorithm 1 (SHA-1) algorithm. The proposed hardware was implemented in a Field Programmable Gate Array (FPGA) Xilinx Virtex 6 xc6vlx240t-1ff1156. The throughput and the occupied area were analyzed for several implementations in parallel instances of the hash algorithm. The results showed that the hardware proposed for the SHA-1 achieved a throughput of 0.644 Gbps for a single instance and slightly more than 28 Gbps for 48 instances in a single FPGA. Various applications such as password recovery, password validation, and high volume data integrity checking can be performed efficiently and quickly with an ASSP for SHA1.", "venue": "ArXiv", "authors": ["Carlos E. B. S. J\u00fanior", "Matheus F. Torquato", "Marcelo A. C. Fernandes"], "year": 2019, "n_citations": 0}
{"id": 5160175, "s2_id": "dd7a6b4ed7f875a07d30ba677aca625c7834883c", "title": "On Transformations of Load-Store Maurer Instruction Set Architecture", "abstract": "In this paper, we study how certain conditions can affect the transformations on the states of the memory of a strict load-store Maurer ISA, when half of the data memory serves as the part of the operating unit.", "venue": "ArXiv", "authors": ["Tie  Hou"], "year": 2008, "n_citations": 0}
{"id": 5162314, "s2_id": "d1ddd8b72b54632c0f640fafca9f076efee69ea6", "title": "Energy-Efficient Time-Domain Vector-by-Matrix Multiplier for Neurocomputing and Beyond", "abstract": "We propose an extremely energy-efficient mixed-signal <inline-formula> <tex-math notation=\"LaTeX\">${N} \\times {N}$ </tex-math></inline-formula> vector-by-matrix multiplication (VMM) in a time domain. Multi-bit inputs/outputs are represented with time-encoded digital signals, while multi-bit matrix weights are realized with adjustable current sources, e.g., transistors biased in subthreshold regime. The major advantage of the proposed approach over other types of mixed-signal implementations is very compact peripheral circuits, which would be essential for achieving high energy efficiency and speed at the system level. As a case study, we have designed a multilayer perceptron, based on two layers of 10 <inline-formula> <tex-math notation=\"LaTeX\">$ \\times $ </tex-math></inline-formula> 10 four-quadrant multipliers, in 55-nm process with embedded NOR flash memory technology, which allows for compact implementation of adjustable current sources. Our analysis, based on memory cell measurements, shows that >6 bit operation can be ensured for larger (<inline-formula> <tex-math notation=\"LaTeX\">${N} >$ </tex-math></inline-formula> 50) VMMs. Post-layout estimates for 55-nm 6-bit VMM, which take into account the impact of PVT variations, noise, and overhead of I/O circuitry for converting between conventional digital and time domain representations, show ~7 fJ/Op for <inline-formula> <tex-math notation=\"LaTeX\">${N} >$ </tex-math></inline-formula> 500. The energy efficiency can be further improved to POp/J regime for more optimal and aggressive designs.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Mohammad  Bavandpour", "Mohammad Reza Mahmoodi", "Dmitri B. Strukov"], "year": 2019, "n_citations": 32}
{"id": 5175191, "s2_id": "1b187c5b96ddd94d6163c730e4db3e2c1e583b0f", "title": "Design and Implementation of an Improved Carry Increment Adder", "abstract": "A complex digital circuit comprises of adder as a basic unit. The performance of the circuit depends on the design of this basic adder unit. The speed of operation of a circuit is one of the important performance criteria of many digital circuits which ultimately depends on the delay of the basic adder unit. Many research works have been devoted in improving the delay of the adder circuit. In this paper we have proposed an improved carry increment adder (CIA) that improves the delay performance of the circuit. The improvement is achieved by incorporating carry look adder (CLA) in the design of CIA contrary to the previous design of CIA that employs ripple carry adder (RCA). A simulation study is carried out for comparative analysis. The coding is done in Verilog hardware description language (HDL) and the simulation is carried out in Xilinx ISE 13.1 environment.", "venue": "ArXiv", "authors": ["Aribam Balarampyari Devi", "Manoj  Kumar", "Romesh  Laishram"], "year": 2016, "n_citations": 4}
{"id": 5176036, "s2_id": "09c023e86f591034dcad73e5e80ef6cb26dcf0a0", "title": "A Novel Approximate Hamming Weight Computing for Spiking Neural Networks: an FPGA Friendly Architecture", "abstract": "Hamming weights of sparse and long binary vectors are important modules in many scientific applications, particularly in spiking neural networks that are of our interest. To improve both area and latency of their FPGA implementations, we propose a method inspired from synaptic transmission failure for exploiting FPGA lookup tables to compress long input vectors. To evaluate the effectiveness of this approach, we count the number of \u20181\u2019s of the compressed vector using a simple linear adder. We classify the compressors into shallow ones with up to two levels of lookup tables and deep ones with more than two levels. The architecture generated by this approach shows up to 82% and 35% reductions for different configurations of shallow compressors in area and latency respectively. Moreover, our simulation results show that calculating the Hamming weight of a 1024-bit vector of a spiking neural network by the use of only deep compressors preserves the chaotic behavior of the network while slightly impacts on the learning performance.", "venue": "ArXiv", "authors": ["Kaveh  Akbarzadeh-Sherbaf", "Mikaeel  Bahmani", "Danial  Ghiaseddin", "Saeed  Safari", "Abdol-Hossein  Vahabie"], "year": 2021, "n_citations": 0}
{"id": 5177295, "s2_id": "9c0fcb62039c5813d642ec2699ca2c1437fc5733", "title": "Building Application-Specific Overlays on FPGAs with High-Level Customizable IPs", "abstract": "Overlays are virtual, re-configurable architectures that overlay on top of physical FPGA fabrics. An overlay that is specialized for an application, or a class of applications, offers both fast reconfiguration and minimized performance penalty. Such an overlay is usually implemented by hardware designers in hardware \"assembly\" languages at register-transfer level (RTL). \nThis short article proposes an idea for a software programmer, instead of hardware designers, to quickly implement an application-specific overlay using high-level customizable IPs. These IPs are expressed succinctly by a specification language, whose abstraction level is much higher than RTL but can nonetheless expresses many performance-critical loop and data optimizations on FPGAs, and thus would offer competitively high performance at a much lower cost of maintenance and much easier customizations. \nWe propose new language features to easily put the IPs together into an overlay. A compiler automatically implements the specified optimizations to generate an efficient overlay, exposes a multi-tasking programming interface for the overlay, and inserts a runtime scheduler for scheduling tasks to run on the IPs of the overlay, respecting the dependences between the tasks. While an application written in any language can take advantage of the overlay through the programming interface, we show a particular usage scenario, where the application itself is also succinctly specified in the same language. \nWe describe the new language features for expressing overlays, and illustrate the features with an LU decomposer and a convolutional neural network. A system is under construction to implement the language features and workloads.", "venue": "ArXiv", "authors": ["Hongbo  Rong"], "year": 2020, "n_citations": 0}
{"id": 5179917, "s2_id": "85473dc43751c21e836b0b59c11601bca372b494", "title": "TRINITY: Coordinated Performance, Energy and Temperature Management in 3D Processor-Memory Stacks", "abstract": "The consistent demand for better performance has lead to innovations at hardware and microarchitectural levels. 3D stacking of memory and logic dies delivers an order of magnitude improvement in available memory bandwidth. The price paid however is, tight thermal constraints. \nIn this paper, we study the complex multiphysics interactions between performance, energy and temperature. Using a cache coherent multicore processor cycle level simulator coupled with power and thermal estimation tools, we investigate the interactions between (a) thermal behaviors (b) compute and memory microarchitecture and (c) application workloads. The key insights from this exploration reveal the need to manage performance, energy and temperature in a coordinated fashion. Furthermore, we identify the concept of \"effective heat capacity\" i.e. the heat generated beyond which no further gains in performance is observed with increases in voltage-frequency of the compute logic. Subsequently, a real-time, numerical optimization based, application agnostic controller (TRINITY) is developed which intelligently manages the three parameters of interest. We observe up to $30\\%$ improvement in Energy Delay$^2$ Product and up to $8$ Kelvin lower core temperatures as compared to fixed frequencies. Compared to the \\texttt{ondemand} Linux CPU DVFS governor, for similar energy efficiency, TRINITY keeps the cores cooler by $6$ Kelvin which increases the lifetime reliability by up to 59\\%.", "venue": "ArXiv", "authors": ["Karthik  Rao", "William J. Song", "Yorai  Wardi", "Sudhakar  Yalamanchili"], "year": 2018, "n_citations": 0}
{"id": 5182826, "s2_id": "5d62928330505a0033e013c382c51d006aa9d77d", "title": "Thermal analysis of 3D associative processor", "abstract": "Thermal density and hot spots limit three-dimensional (3D) implementation of massively-parallel SIMD processors and prohibit stacking DRAM dies above them. This study proposes replacing SIMD by an Associative Processor (AP). AP exhibits close to uniform thermal distribution with reduced hot spots. Additionally, AP may outperform SIMD processor when the data set size is sufficiently large, while dissipating less power. Comparative performance and thermal analysis supported by simulation confirm that AP might be preferable over SIMD for 3D implementation of large scale massively parallel processing engines combined with 3D DRAM integration.", "venue": "ArXiv", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar"], "year": 2013, "n_citations": 3}
{"id": 5184152, "s2_id": "bf3e7c0c16ebce780520acaa4b6712c1354c0067", "title": "A low-power Content-Addressable Memory based on clustered-sparse networks", "abstract": "A low-power Content-Addressable Memory (CAM) is introduced employing a new mechanism for associativity between the input tags and the corresponding address of the output data. The proposed architecture is based on a recently developed clustered-sparse network using binary-weighted connections that on-average will eliminate most of the parallel comparisons performed during a search. Therefore, the dynamic energy consumption of the proposed design is significantly lower compared to that of a conventional low-power CAM design. Given an input tag, the proposed architecture computes a few possibilities for the location of the matched tag and performs the comparisons on them to locate a single valid match. A 0.13\u03bcm CMOS technology was used for simulation purposes. The energy consumption and the search delay of the proposed design are 9.5%, and 30.4% of that of the conventional NAND architecture respectively with a 3.4% higher number of transistors.", "venue": "2013 IEEE 24th International Conference on Application-Specific Systems, Architectures and Processors", "authors": ["Hooman  Jarollahi", "Vincent  Gripon", "Naoya  Onizawa", "Warren J. Gross"], "year": 2013, "n_citations": 27}
{"id": 5186480, "s2_id": "6f6b9a8dca40acc0b283802aa08005702ef22d8d", "title": "Relaxed Half-Stochastic Belief Propagation", "abstract": "Low-density parity-check codes are attractive for high throughput applications because of their low decoding complexity per bit, but also because all the codeword bits can be decoded in parallel. However, achieving this in a circuit implementation is complicated by the number of wires required to exchange messages between processing nodes. Decoding algorithms that exchange binary messages are interesting for fully-parallel implementations because they can reduce the number and the length of the wires, and increase logic density. This paper introduces the Relaxed Half-Stochastic (RHS) decoding algorithm, a binary message belief propagation (BP) algorithm that achieves a coding gain comparable to the best known BP algorithms that use real-valued messages. We derive the RHS algorithm by starting from the well-known Sum-Product algorithm, and then derive a low-complexity version suitable for circuit implementation. We present extensive simulation results on two standardized codes having different rates and constructions, including low bit error rate results. These simulations show that RHS can converge faster on average than existing state-of-the-art decoding algorithms, leading to improvements in throughput and energy efficiency.", "venue": "IEEE Transactions on Communications", "authors": ["Fran\u00e7ois  Leduc-Primeau", "Saied  Hemati", "Shie  Mannor", "Warren J. Gross"], "year": 2013, "n_citations": 12}
{"id": 5186659, "s2_id": "67a586e80d9f29519d6218fa03355579f0d88230", "title": "PANTHER: A Programmable Architecture for Neural Network Training Harnessing Energy-Efficient ReRAM", "abstract": "The wide adoption of deep neural networks has been accompanied by ever-increasing energy and performance demands due to the expensive nature of training them. Numerous special-purpose architectures have been proposed to accelerate training: both digital and hybrid digital-analog using resistive RAM (ReRAM) crossbars. ReRAM-based accelerators have demonstrated the effectiveness of ReRAM crossbars at performing matrix-vector multiplication operations that are prevalent in training. However, they still suffer from inefficiency due to the use of serial reads and writes for performing the weight gradient and update step. A few works have demonstrated the possibility of performing outer products in crossbars, which can be used to realize the weight gradient and update step without the use of serial reads and writes. However, these works have been limited to low precision operations which are not sufficient for typical training workloads. Moreover, they have been confined to a limited set of training algorithms for fully-connected layers only. To address these limitations, we propose a bit-slicing technique for enhancing the precision of ReRAM-based outer products, which is substantially different from bit-slicing for matrix-vector multiplication only. We incorporate this technique into a crossbar architecture with three variants catered to different training algorithms. To evaluate our design on different types of layers in neural networks (fully-connected, convolutional, etc.) and training algorithms, we develop PANTHER, an ISA-programmable training accelerator with compiler support. Our design can also be integrated into other accelerators in the literature to enhance their efficiency. Our evaluation shows that PANTHER achieves up to <inline-formula><tex-math notation=\"LaTeX\">$8.02\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>.</mml:mo><mml:mn>02</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq1-2998456.gif\"/></alternatives></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$54.21\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>54</mml:mn><mml:mo>.</mml:mo><mml:mn>21</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq2-2998456.gif\"/></alternatives></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\">$103\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>103</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq3-2998456.gif\"/></alternatives></inline-formula> energy reductions as well as <inline-formula><tex-math notation=\"LaTeX\">$7.16\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>7</mml:mn><mml:mo>.</mml:mo><mml:mn>16</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq4-2998456.gif\"/></alternatives></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$4.02\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>4</mml:mn><mml:mo>.</mml:mo><mml:mn>02</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq5-2998456.gif\"/></alternatives></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\">$16\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>16</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq6-2998456.gif\"/></alternatives></inline-formula> execution time reductions compared to digital accelerators, ReRAM-based accelerators, and GPUs, respectively.", "venue": "IEEE Transactions on Computers", "authors": ["Aayush  Ankit", "Izzat El Hajj", "Sai Rahul Chalamalasetti", "Sapan  Agarwal", "Matthew  Marinella", "Martin  Foltin", "John Paul Strachan", "Dejan  Milojicic", "Wen-Mei  Hwu", "Kaushik  Roy"], "year": 2020, "n_citations": 19}
{"id": 5190282, "s2_id": "5bcd3526add3f3eaf4f1d3091885477c72dcbf62", "title": "ScaleSimulator: A Fast and Cycle-Accurate Parallel Simulator for Architectural Exploration", "abstract": "Design of next generation computer systems should be supported by simulation infrastructure that must achieve a few contradictory goals such as fast execution time, high accuracy, and enough flexibility to allow comparison between large numbers of possible design points. Most existing architecture level simulators are designed to be flexible and to execute the code in parallel for greater efficiency, but at the cost of scarified accuracy. This paper presents the ScaleSimulator simulation environment, which is based on a new design methodology whose goal is to achieve near cycle accuracy while still being flexible enough to simulate many different future system architectures and efficient enough to run meaningful workloads. We achieve these goals by making the parallelism a first-class citizen in our methodology. Thus, this paper focuses mainly on the ScaleSimulator design points that enable better parallel execution while maintaining the scalability and cycle accuracy of a simulated architecture. The paper indicates that the new proposed ScaleSimulator tool can (1) efficiently parallelize the execution of a cycle-accurate architecture simulator, (2) efficiently simulate complex architectures (e.g., out-of-order CPU pipeline, cache coherency protocol, and network) and massive parallel systems, and (3) use meaningful workloads, such as full simulation of OLTP benchmarks, to examine future architectural choices.", "venue": "SimuTools", "authors": ["Chalak  Ori", "Weiguang  Cai", "Wei  Li", "Lei  Fang", "Libing  Zheng", "Jintang  Wang", "Zuguang  Wu", "Xiongli  Gu", "Haibin  Wang", "Avi  Mendelson"], "year": 2017, "n_citations": 0}
{"id": 5192800, "s2_id": "acfe002f7aed451f02453f9d0c473f107a80aba2", "title": "AnalogNets: ML-HW Co-Design of Noise-robust TinyML Models and Always-On Analog Compute-in-Memory Accelerator", "abstract": "Always-on TinyML perception tasks in IoT applications require very high energy efficiency. Analog computein-memory (CiM) using non-volatile memory (NVM) promises high efficiency and also provides self-contained on-chip model storage. However, analog CiM introduces new practical considerations, including conductance drift, read/write noise, fixed analog-to-digital (ADC) converter gain, etc. These additional constraints must be addressed to achieve models that can be deployed on analog CiM with acceptable accuracy loss. This work describes AnalogNets: TinyML models for the popular always-on applications of keyword spotting (KWS) and visual wake words (VWW). The model architectures are specifically designed for analog CiM, and we detail a comprehensive training methodology, to retain accuracy in the face of analog non-idealities, and low-precision data converters at inference time. We also describe AON-CiM, a programmable, minimal-area phase-change memory (PCM) analog CiM accelerator, with a novel layer-serial approach to remove the cost of complex interconnects associated with a fully-pipelined design. We evaluate the AnalogNets on a calibrated simulator, as well as real hardware, and find that accuracy degradation is limited to 0.8%/1.2% after 24 hours of PCM drift (8-bit) for KWS/VWW. AnalogNets running on the 14nm AON-CiM accelerator demonstrate 8.58/4.37 TOPS/W for KWS/VWW workloads using 8-bit activations, respectively, and increasing to 57.39/25.69 TOPS/W with 4-bit activations.", "venue": "ArXiv", "authors": ["Chuteng  Zhou", "Fernando  Garcia-Redondo", "Julian  B\u00fcchel", "Irem  Boybat", "Xavier Timoneda Comas", "S. R. Nandakumar", "Shidhartha  Das", "Abu  Sebastian", "Manuel Le Gallo", "Paul N. Whatmough"], "year": 2021, "n_citations": 0}
{"id": 5193229, "s2_id": "dc0672d8b028580477b5a7b21475d33e77f17295", "title": "FLOWER: A comprehensive dataflow compiler for high-level synthesis", "abstract": "FPGAs have found their way into data centers as accelerator cards, making reconfigurable computing more accessible for high-performance applications. At the same time, new high-level synthesis compilers like Xilinx Vitis and runtime libraries such as XRT attract software programmers into the reconfigurable domain. While software programmers are familiar with task-level and data-parallel programming, FPGAs often require different types of parallelism. For example, data-driven parallelism is mandatory to obtain satisfactory hardware designs for pipelined dataflow architectures. However, software programmers are often not acquainted with dataflow architectures\u2014 resulting in poor hardware designs. In this work we present FLOWER, a comprehensive compiler infrastructure that provides automatic canonical transformations for high-level synthesis from a domain-specific library. This allows programmers to focus on algorithm implementations rather than low-level optimizations for dataflow architectures. We show that FLOWER allows to synthesize efficient implementations for high-performance streaming applications targeting System-on-Chip and FPGA accelerator cards, in the context of image processing and computer vision.", "venue": "2021 International Conference on Field-Programmable Technology (ICFPT)", "authors": ["Puya  Amiri", "Ars\u00e8ne  P\u00e9rard-Gayot", "Richard  Membarth", "Philipp  Slusallek", "Roland  Lei\u00dfa", "Sebastian  Hack"], "year": 2021, "n_citations": 0}
{"id": 5195406, "s2_id": "3f63640a31fb4624aeebc6e7959cb8ac26e54051", "title": "Simultaneous Multi Layer Access: A High Bandwidth and Low Cost 3D-Stacked Memory Interface", "abstract": "Limited memory bandwidth is a critical bottleneck in modern systems. 3D-stacked DRAM enables higher bandwidth by leveraging wider Through-Silicon-Via (TSV) channels, but today\u2019s systems cannot fully exploit them due to the limited internal bandwidth of DRAM. DRAM reads a whole row simultaneously from the cell array to a row buffer, but can transfer only a fraction of the data from the row buffer to peripheral IO circuit, through a limited and expensive set of wires referred to as global bitlines. In presence of wider memory channels, the major bottleneck becomes the limited data transfer capacity through these global bitlines. Our goal in this work is to enable higher bandwidth in 3D-stacked DRAM without the increased cost of adding more global bitlines. We instead exploit otherwise-idle resources, such as global bitlines, already existing within the multiple DRAM layers by accessing the layers simultaneously. Our architecture, Simultaneous Multi Layer Access (SMLA), provides higher bandwidth by aggregating the internal bandwidth of multiple layers and transferring the available data at a higher IO frequency. To implement SMLA, simultaneous data transfer from multiple layers through the same IO TSVs requires coordination between layers to avoid channel conflict. We first study coordination by static partitioning, which we call Dedicated-IO, that assigns groups of TSVs to each layer. We then provide a simple, yet sophisticated mechanism, called Cascaded-IO, which enables simultaneous access to each layer by time-multiplexing the IOs, while also reducing design effort. By operating at a frequency proportional to the number of layers, SMLA provides a higher bandwidth (4X for a four-layer stacked DRAM). Our evaluations show that SMLA provides significant performance improvement and energy reduction (55%/18% on average for multi-programmed workloads, respectively) over a baseline 3D-stacked DRAM with very low area overhead.", "venue": "ArXiv", "authors": ["Donghyuk  Lee", "Gennady  Pekhimenko", "Samira Manabi Khan", "Saugata  Ghose", "Onur  Mutlu"], "year": 2015, "n_citations": 25}
{"id": 5196500, "s2_id": "36c3a377ea91937c7f91ae8f822a65c1ca072e7e", "title": "Tearing Down the Memory Wall", "abstract": "We present a vision for the Erudite architecture that redefines the compute and memory abstractions such that memory bandwidth and capacity become first-class citizens along with compute throughput. In this architecture, we envision coupling a high-density, massively parallel memory technology like Flash with programmable near-data accelerators, like the streaming multiprocessors in modern GPUs. Each accelerator has a local pool of storage-class memory that it can access at high throughput by initiating very large numbers of overlapping requests that help to tolerate long access latency. The accelerators can also communicate with each other and remote memory through a high-throughput low-latency interconnect. As a result, systems based on the Erudite architecture scale compute and memory bandwidth at the same rate, tearing down the notorious memory wall that has plagued computer architecture for generations. In this paper, we present the motivation, rationale, design, benefit, and research challenges for Erudite.", "venue": "ArXiv", "authors": ["Zaid  Qureshi", "Vikram Sharma Mailthody", "Seung Won Min", "I-Hsin  Chung", "Jinjun  Xiong", "Wen-mei  Hwu"], "year": 2020, "n_citations": 2}
{"id": 5198516, "s2_id": "fefd0c9ca04bc18e67de607199b3edf6e3303230", "title": "Synchronizer-Free Digital Link Controller", "abstract": "This work presents a producer-consumer link between two independent clock domains. The link allows for metastability-free, low-latency, high-throughput communication by slight adjustments to the clock frequencies of the producer and consumer domains steered by a controller circuit. Any such controller cannot deterministically avoid, detect, nor resolve metastability. Typically, this is addressed by synchronizers, incurring a larger dead time in the control loop. We follow the approach of Friedrichs et al. (TC 2018) who proposed metastability-containing circuits. The result is a simple control circuit that may become metastable, yet deterministically avoids buffer underrun or overflow. More specifically, the controller output may become metastable, but this may only affect oscillator speeds within specific bounds. In contrast, communication is guaranteed to remain metastability-free. We formally prove correctness of the producer-consumer link and a possible implementation that has only small overhead. With SPICE simulations of the proposed implementation we further substantiate our claims. The simulation uses 65nm process running at roughly 2GHz.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Johannes  Bund", "Matthias  F\u00fcgger", "Christoph  Lenzen", "Moti  Medina"], "year": 2020, "n_citations": 0}
{"id": 5198786, "s2_id": "d93ab16ce677ac2e6e4921810717eb5c2976f69f", "title": "A Time-domain Analog Weighted-sum Calculation Model for Extremely Low Power VLSI Implementation of Multi-layer Neural Networks", "abstract": "A time-domain analog weighted-sum calculation model is proposed based on an integrate-and-fire-type spiking neuron model. The proposed calculation model is applied to multi-layer feedforward networks, in which weighted summations with positive and negative weights are separately performed in each layer and summation results are then fed into the next layers without their subtraction operation. We also propose very large-scale integrated (VLSI) circuits to implement the proposed model. Unlike the conventional analog voltage or current mode circuits, the time-domain analog circuits use transient operation in charging/discharging processes to capacitors. Since the circuits can be designed without operational amplifiers, they can operate with extremely low power consumption. However, they have to use very high resistance devices on the order of G$\\rm \\Omega$. We designed a proof-of-concept (PoC) CMOS VLSI chip to verify weighted-sum operation with the same weights and evaluated it by post-layout circuit simulation using 250-nm fabrication technology. High resistance operation was realized by using the subthreshold operation region of MOS transistors. Simulation results showed that energy efficiency for the weighted-sum calculation was 290~TOPS/W, more than one order of magnitude higher than that in state-of-the-art digital AI processors, even though the minimum width of interconnection used in the PoC chip was several times larger than that in such digital processors. If state-of-the-art VLSI technology is used to implement the proposed model, an energy efficiency of more than 1,000~TOPS/W will be possible. For practical applications, development of emerging analog memory devices such as ferroelectric-gate FETs is necessary.", "venue": "ArXiv", "authors": ["Quan  Wang", "Hakaru  Tamukoh", "Takashi  Morie"], "year": 2018, "n_citations": 12}
{"id": 5198952, "s2_id": "1368381a8268cbd89d557b6f82aedd9ee20dfd7b", "title": "T-count Optimized Quantum Circuits for Bilinear Interpolation", "abstract": "Quantum circuits for basic image processing functions such as bilinear interpolation are required to implement image processing algorithms on quantum computers. In this work, we propose quantum circuits for the bilinear interpolation of NEQR encoded images based on Clifford+T gates. Quantum circuits for the scale up operation and scale down operation are illustrated. The proposed quantum circuits are based on quantum Clifford+T gates and are optimized for T-count. Quantum circuits based on Clifford+T gates can be made fault tolerant but the T gate is very costly to implement. As a result, reducing T-count is an important optimization goal. The proposed quantum bilinear interpolation circuits are based on (i) a quantum adder, (ii) a proposed quantum subtractor, and (iii) a quantum multiplication circuit. Further, both designs are compared and shown to be superior to existing work in terms of T-count. The proposed quantum bilinear interpolation circuits for the scale down operation and for the scale up operation each have a 92.52% improvement in terms of T-count compared to the existing work.", "venue": "2018 Ninth International Green and Sustainable Computing Conference (IGSC)", "authors": ["Edgard  Mu\u00f1oz-Coreas", "Himanshu  Thapliyal"], "year": 2018, "n_citations": 2}
{"id": 5203472, "s2_id": "8608d970bf4840d28bf5c059109c5230c7c53192", "title": "Open-Source Memory Compiler for Automatic RRAM Generation and Verification", "abstract": "The lack of open-source memory compilers in academia typically causes significant delays in research and design implementations. This paper presents an open-source memory compiler that is directly integrated within the Cadence Virtuoso environment using physical verification tools provided by Mentor Graphics (Calibre). It facilitates the entire memory generation process from netlist generation to layout implementation, and physical implementation verification. To the best of our knowledge, this is the first open-source memory compiler that has been developed specifically to automate Resistive Random Access Memory (RRAM) generation. RRAM holds the promise of achieving high speed, high density and non-volatility. A novel RRAM architecture, additionally is proposed, and a number of generated RRAM arrays are evaluated to identify their worst case control line parasitics and worst case settling time across the memristors of their cells. The total capacitance of lines SEL, N and P is 5.83 fF/cell, 3.31 fF/cell and 2.48 fF/cell respectively, while the total calculated resistance for SEL is 1.28 \u2126/cell and 0.14 \u2126/cell for both N and P lines.", "venue": "2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["Dimitrios  Antoniadis", "Peilong  Feng", "Andrea  Mifsud", "Timothy G. Constandinou"], "year": 2021, "n_citations": 1}
{"id": 5204849, "s2_id": "cdef397456269d51f18bd1531b65714538a3187b", "title": "unzipFPGA: Enhancing FPGA-based CNN Engines with On-the-Fly Weights Generation", "abstract": "Single computation engines have become a popular design choice for FPGA-based convolutional neural networks (CNNs) enabling the deployment of diverse models without fabric reconfiguration. This flexibility, however, often comes with significantly reduced performance on memory-bound layers and resource underutilisation due to suboptimal mapping of certain layers on the engine\u2019s fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. To minimise the negative impact of limited bandwidth on memory-bound layers, we present a novel hardware component that enables the on-chip on-the-fly generation of weights. We further introduce an input selective processing element (PE) design that balances the load between PEs on suboptimally mapped layers. Finally, we present unzipFPGA, a framework to train on-the-fly models and traverse the design space to select the highest performing CNN engine configuration. Quantitative evaluation shows that unzipFPGA yields an average speedup of 2.14\u00d7 and 71% over optimised status-quo and pruned CNN engines under constrained bandwidth and up to 3.69\u00d7 higher performance density over the state-of-the-art FPGA-based CNN accelerators.", "venue": "2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Stylianos I. Venieris", "Javier  Fernandez-Marques", "Nicholas D. Lane"], "year": 2021, "n_citations": 1}
{"id": 5205000, "s2_id": "0bf3ae6e2273649686d281d20f0ef4338246dd5b", "title": "Mathematical estimation of logical masking capability of majority/minority gates used in nanoelectronic circuits", "abstract": "In nanoelectronic circuit synthesis, the majority gate and the inverter form the basic combinational logic primitives. This paper deduces the mathematical formulae to estimate the logical masking capability of majority gates, which are used extensively in nanoelectronic digital circuit synthesis. The mathematical formulae derived to evaluate the logical masking capability of majority gates holds well for minority gates, and a comparison with the logical masking capability of conventional gates such as NOT, AND/NAND, OR/NOR, and XOR/XNOR is provided. It is inferred from this research work that the logical masking capability of majority/minority gates is similar to that of XOR/XNOR gates, and with an increase of fan-in the logical masking capability of majority/minority gates also increases.", "venue": "2017 International Conference on Circuits, System and Simulation (ICCSS)", "authors": ["P.  Balasubramanian", "R. T. Naayagi"], "year": 2017, "n_citations": 0}
{"id": 5207434, "s2_id": "5e9bb0f3e74be4d8f75cca6ceb3ec87b3e04d7cc", "title": "PIUMA: Programmable Integrated Unified Memory Architecture", "abstract": "High performance large scale graph analytics is essential to timely analyze relationships in big data sets. Conventional processor architectures suffer from inefficient resource usage and bad scaling on graph workloads. To enable efficient and scalable graph analysis, Intel developed the Programmable Integrated Unified Memory Architecture (PIUMA). PIUMA consists of many multi-threaded cores, fine-grained memory and network accesses, a globally shared address space and powerful offload engines. This paper presents the PIUMA architecture, and provides initial performance estimations, projecting that a PIUMA node will outperform a conventional compute node by one to two orders of magnitude. Furthermore, PIUMA continues to scale across multiple nodes, which is a challenge in conventional multinode setups.", "venue": "ArXiv", "authors": ["Sriram  Aananthakrishnan", "Nesreen K. Ahmed", "Vincent  Cave", "Marcelo  Cintra", "Yigit  Demir", "Kristof Du Bois", "Stijn  Eyerman", "Joshua B. Fryman", "Ivan  Ganev", "Wim  Heirman", "Hans-Christian  Hoppe", "Jason  Howard", "Ibrahim  Hur", "MidhunChandra  Kodiyath", "Samkit  Jain", "Daniel S. Klowden", "Marek M. Landowski", "Laurent  Montigny", "Ankit  More", "Przemyslaw  Ossowski", "Robert  Pawlowski", "Nick  Pepperling", "Fabrizio  Petrini", "Mariusz  Sikora", "Balasubramanian  Seshasayee", "Shaden  Smith", "Sebastian  Szkoda", "Sanjaya  Tayal", "Jesmin Jahan Tithi", "Yves  Vandriessche", "Izajasz P. Wrosz"], "year": 2020, "n_citations": 6}
{"id": 5207646, "s2_id": "7125491ed0a035df16ec57f9a5e1b963421a6854", "title": "A Hardware-Efficient Approach to Computing the Rotation Matrix from a Quaternion", "abstract": "In this paper, we have proposed a novel VLSI-oriented approach to computing the rotation matrix entries from the quaternion coefficients. The advantage of this approach is the complete elimination of multiplications and replacing them by less costly squarings. Our approach uses Logan's identity, which proposes to replace the calculation of the product of two numbers on summing the squares via the Binomial Theorem. Replacing multiplications by squarings implies reducing power consumption as well as decreases hardware circuit complexity.", "venue": "ArXiv", "authors": ["Aleksandr  Cariow", "Galina  Cariowa"], "year": 2016, "n_citations": 1}
{"id": 5209164, "s2_id": "170a3dba6e7a53debc5e5ab797e2c1ba27b9597b", "title": "High Throughput 2D Spatial Image Filters on FPGAs", "abstract": "FPGAs are well established in the signal processing domain, where their fine-grained programmable nature allows the inherent parallelism in these applications to be exploited for enhanced performance. As architectures have evolved, FPGA vendors have added more heterogeneous resources to allow often-used functions to be implemented with higher performance, at lower power and using less area. DSP blocks, for example, have evolved from basic multipliers to support the multiply-accumulate operations that are the core of many signal processing tasks. While more features were added to DSP blocks, their structure and connectivity has been optimised primarily for one-dimensional signal processing. Basic operations in image processing are similar, but performed in a two-dimensional structure, and hence, many of the optimisations in newer DSP blocks are not exploited when mapping image processing algorithms to them. We present a detailed study of two-dimensional spatial filter implementation on FPGAs, showing how to maximise performance through exploitation of DSP block capabilities, while also presenting a lean border pixel management policy.", "venue": "ArXiv", "authors": ["Abdullah  Al-Dujaili", "Suhaib A. Fahmy"], "year": 2017, "n_citations": 2}
{"id": 5214662, "s2_id": "7ce954ac9d596811051633673d6bc8cc0312f851", "title": "DeepNVM++: Cross-Layer Modeling and Optimization Framework of Non-Volatile Memories for Deep Learning", "abstract": "Non-volatile memory (NVM) technologies such as spin-transfer torque magnetic random access memory (STT-MRAM) and spin-orbit torque magnetic random access memory (SOT-MRAM) have significant advantages compared to conventional SRAM due to their non-volatility, higher cell density, and scalability features. While previous work has investigated several architectural implications of NVM for generic applications, in this work we present DeepNVM++, a framework to characterize, model, and analyze NVM-based caches in GPU architectures for deep learning (DL) applications by combining technology-specific circuit-level models and the actual memory behavior of various DL workloads. We present both iso-capacity and iso-area performance and energy analysis for systems whose last-level caches rely on conventional SRAM and emerging STT-MRAM and SOT-MRAM technologies. In the iso-capacity case, STT-MRAM and SOT-MRAM provide up to 3.8x and 4.7x energy-delay product (EDP) reduction and 2.4x and 2.8x area reduction compared to conventional SRAM, respectively. Under iso-area assumptions, STT-MRAM and SOT-MRAM provide up to 2x and 2.3x EDP reduction and accommodate 2.3x and 3.3x cache capacity when compared to SRAM, respectively. We also perform a scalability analysis and show that STT-MRAM and SOT-MRAM achieve orders of magnitude EDP reduction when compared to SRAM for large cache capacities. Our comprehensive cross-layer framework is demonstrated on STT-/SOT-MRAM technologies and can be used for the characterization, modeling, and analysis of any NVM technology for last-level caches in GPUs for DL applications.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Ahmet  Inci", "Mehmet Meric Isgenc", "Diana  Marculescu"], "year": 2021, "n_citations": 3}
{"id": 5214733, "s2_id": "bd996991f3e91e2f757466f21779ed0f8c8d224f", "title": "A Multi-Kernel Multi-Code Polar Decoder Architecture", "abstract": "Polar codes have received increasing attention in the past decade, and have been selected for the next generation of the wireless communication standard. Most research on polar codes has focused on codes constructed from a <inline-formula> <tex-math notation=\"LaTeX\">$2\\times 2$ </tex-math></inline-formula> polarization matrix, called binary kernel: codes constructed from binary kernels have code lengths that are bound to powers of 2. A few recent works have proposed construction methods based on multiple kernels of different dimensions, not only binary ones, allowing code lengths different from powers of 2. In this paper, we design and implement the first multi-kernel successive cancellation polar code decoder in literature. It can decode any code constructed with binary and ternary kernels: the architecture, sized for a maximum code length <inline-formula> <tex-math notation=\"LaTeX\">$N_{\\max }$ </tex-math></inline-formula>, is fully flexible in terms of code length, code rate, and kernel sequence. The decoder can achieve a frequency of over 1 GHz in 65 nm CMOS technology, and a throughput of 615 Mb/s. The area occupation ranges between 0.11 mm<sup>2</sup> for <inline-formula> <tex-math notation=\"LaTeX\">$N_{\\max }=256$ </tex-math></inline-formula> and 2.01 mm<sup>2</sup> for <inline-formula> <tex-math notation=\"LaTeX\">$N_{\\max }=4096$ </tex-math></inline-formula>. Implementation results show an unprecedented degree of flexibility: with <inline-formula> <tex-math notation=\"LaTeX\">$N_{\\max }=4096$ </tex-math></inline-formula>, up to 55 code lengths can be decoded with the same hardware, along with any kernel sequence and code rate.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Gabriele  Coppolino", "Carlo  Condo", "Guido  Masera", "Warren J. Gross"], "year": 2018, "n_citations": 9}
{"id": 5215294, "s2_id": "0fe066a3c79aa70246580da665130f07ba265a50", "title": "A Clock Synchronizer for Repeaterless Low Swing On-Chip Links", "abstract": "A clock synchronizing circuit for repeaterless low swing interconnects is presented in this paper. The circuit uses a delay locked loop (DLL) to generate multiple phases of the clock, of which the one closest to the center of the eye is picked by a phase detector loop. The picked phase is then further fine tuned by an analog voltage controlled delay to position the sampling clock at the center of the eye. A clock domain transfer circuit then transfers the sampled data to the receiver clock domain with a maximum latency of three clock cycles. The proposed synchronizer has been designed and fabricated in 130 nm UMC MM CMOS technology. The circuit consumes 1.4 mW from a 1.2 V supply at a data rate of 1.3 Gbps. Further, the proposed synchronizer has been designed and simulated in TSMC 65 nm CMOS technology. Post layout simulations show that the synchronizer consumes 1.5 mW from a 1 V supply, at a data rate of 4 Gbps in this technology.", "venue": "ArXiv", "authors": ["Naveen  Kadayinti", "Maryam Shojaei Baghini", "Dinesh Kumar Sharma"], "year": 2015, "n_citations": 4}
{"id": 5225089, "s2_id": "ade958407c911b685766243eeabd28227f9f4ae6", "title": "CSM-NN: Current Source Model Based Logic Circuit Simulation - A Neural Network Approach", "abstract": "The miniaturization of transistors down to 5nm and beyond, plus the increasing complexity of integrated circuits, significantly aggravate short channel effects, and demand analysis and optimization of more design corners and modes. Simulators need to model output variables related to circuit timing, power, noise, etc., which exhibit nonlinear behavior. The existing simulation and sign-off tools, based on a combination of closed-form expressions and lookup tables are either inaccurate or slow, when dealing with circuits with more than billions of transistors. In this work, we present CSM-NN, a scalable simulation framework with optimized neural network structures and processing algorithms. CSM-NN is aimed at optimizing the simulation time by accounting for the latency of the required memory query and computation, given the underlying CPU and GPU parallel processing capabilities. Experimental results show that CSM-NN reduces the simulation time by up to 6\u00d7 compared to a state-of-the-art current source model based simulator running on a CPU. This speedup improves by up to 15\u00d7 when running on a GPU. CSM-NN also provides high accuracy levels, with less than 2% error, compared to HSPICE.", "venue": "2019 IEEE 37th International Conference on Computer Design (ICCD)", "authors": ["Mohammad Saeed Abrishami", "Massoud  Pedram", "Shahin  Nazarian"], "year": 2019, "n_citations": 3}
{"id": 5227617, "s2_id": "a58ec2de2d6a60acee04a3b548f595f7d3ab8f9a", "title": "Modeling the temperature bias of power consumption for nanometer-scale CPUs in application processors", "abstract": "We introduce and experimentally validate a new macro-level model of the CPU temperature/power relationship within nanometer-scale application processors or system-on-chips. By adopting a holistic view, this model is able to take into account many of the physical effects that occur within such systems. Together with two algorithms described in the paper, our results can be used, for instance by engineers designing power or thermal management units, to cancel the temperature-induced bias on power measurements. This will help them gather temperature-neutral power data while running multiple instance of their benchmarks. Also power requirements and system failure rates can be decreased by controlling the CPU's thermal behavior. Even though it is usually assumed that the temperature/power relationship is exponentially related, there is however a lack of publicly available physical temperature/power measurements to back up this assumption, something our paper corrects. Via measurements on two pertinent platforms sporting nanometer-scale application processors, we show that the power/temperature relationship is indeed very likely exponential over a 20\u00b0C to 85\u00b0C temperature range. Our data suggest that, for application processors operating between 20\u00b0C and 50\u00b0C, a quadratic model is still accurate and a linear approximation is acceptable.", "venue": "2014 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV)", "authors": ["Karel De Vogeleer", "G\u00e9rard  Memmi", "Pierre  Jouvelot", "Fabien  Coelho"], "year": 2014, "n_citations": 24}
{"id": 5228076, "s2_id": "55e13fe485145ae3bfc2df23e9d6b55b8d9508c1", "title": "Distributed Injection-Locking in Analog Ising Machines to Solve Combinatorial Optimizations", "abstract": "The oscillator-based Ising machine (OIM) is a network of coupled CMOS oscillators that solves combinatorial optimization problems. In this paper, the distribution of the injection-locking oscillations throughout the circuit is proposed to accelerate the phase-locking of the OIM. The implications of the proposed technique theoretically investigated and verified by extensive simulations in EDA tools with a 130 nm PTM model. By distributing the injective signal of the super-harmonic oscillator, the speed is increased by 219.8% with negligible increase in the power dissipation and phase-locking error of the device due to the distributed technique.", "venue": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["M. Ali Vosoughi"], "year": 2020, "n_citations": 1}
{"id": 5229805, "s2_id": "4afafbdc83ec9584f1caed50daa5fdaf989929c8", "title": "Adapting the DMTCP Plugin Model for Checkpointing of Hardware Emulation", "abstract": "Checkpoint-restart is now a mature technology. It allows a user to save and later restore the state of a running process. The new plugin model for the upcoming version 3.0 of DMTCP (Distributed MultiThreaded Checkpointing) is described here. This plugin model allows a target application to disconnect from the hardware emulator at checkpoint time and then re-connect to a possibly different hardware emulator at the time of restart. The DMTCP plugin model is important in allowing three distinct parties to seamlessly inter-operate. The three parties are: the EDA designer, who is concerned with formal verification of a circuit design; the DMTCP developers, who are concerned with providing transparent checkpointing during the circuit emulation; and the hardware emulator vendor, who provides a plugin library that responds to checkpoint, restart, and other events. \nThe new plugin model is an example of process-level virtualization: virtualization of external abstractions from within a process. This capability is motivated by scenarios for testing circuit models with the help of a hardware emulator. The plugin model enables a three-way collaboration: allowing a circuit designer and emulator vendor to each contribute separate proprietary plugins while sharing an open source software framework from the DMTCP developers. This provides a more flexible platform, where different fault injection models based on plugins can be designed within the DMTCP checkpointing framework. After initialization, one restarts from a checkpointed state under the control of the desired plugin. This restart saves the time spent in simulating the initialization phase, while enabling fault injection exactly at the region of interest. Upon restart, one can inject faults or otherwise modify the remainder of the simulation. The work concludes with a brief survey of checkpointing and process-level virtualization.", "venue": "ArXiv", "authors": ["Rohan  Garg", "Kapil  Arya", "Jiajun  Cao", "Gene  Cooperman", "Jeff  Evans", "Ankit  Garg", "Neil A. Rosenberg", "K.  Suresh"], "year": 2017, "n_citations": 2}
{"id": 5230065, "s2_id": "b1be63ed1c633999275d3802d602bcc8432170c1", "title": "Characteristics of Reversible Circuits for Error Detection", "abstract": "In this work, we consider error detection via simulation for reversible circuit architectures. We rigorously prove that reversibility augments the performance of this simple error detection protocol to a considerable degree. A single randomly generated input is guaranteed to unveil a single error with a probability that only depends on the size of the error, not the size of the circuit itself. Empirical studies confirm that this behavior typically extends to multiple errors as well. In conclusion, reversible circuits offer characteristics that reduce masking effects -- a desirable feature that is in stark contrast to irreversible circuit architectures.", "venue": "ArXiv", "authors": ["Lukas  Burgholzer", "Robert  Wille", "Richard  Kueng"], "year": 2020, "n_citations": 0}
{"id": 5231195, "s2_id": "2bc3a1dbdbc36d3cd3368c6a0fe247b16cec9048", "title": "FFTPL: An analytic placement algorithm using fast fourier transform for density equalization", "abstract": "We propose a flat nonlinear placement algorithm FFTPL using fast Fourier transform for density equalization. The placement instance is modeled as an electrostatic system with the analogy of density cost to the potential energy. A well-defined Poisson's equation is proposed for gradient and cost computation. Our placer outperforms state-of-the-art placers with better solution quality and efficiency.", "venue": "2013 IEEE 10th International Conference on ASIC", "authors": ["Jingwei  Lu", "Pengwen  Chen", "Chin-Chih  Chang", "Lu  Sha", "Dennis Jen-Hsin Huang", "Chin-Chi  Teng", "Chung-Kuan  Cheng"], "year": 2013, "n_citations": 6}
{"id": 5234351, "s2_id": "25b12780b43674ce5c680ebdec00f51093c30d39", "title": "VLSI Computational Architectures for the Arithmetic Cosine Transform", "abstract": "The discrete cosine transform (DCT) is a widely-used and important signal processing tool employed in a plethora of applications. Typical fast algorithms for nearly-exact computation of DCT require floating point arithmetic, are multiplier intensive, and accumulate round-off errors. Recently proposed fast algorithm arithmetic cosine transform (ACT) calculates the DCT exactly using only additions and integer constant multiplications, with very low area complexity, for null mean input sequences. The ACT can also be computed non-exactly for any input sequence, with low area complexity and low power consumption, utilizing the novel architecture described. However, as a trade-off, the ACT algorithm requires 10 non-uniformly sampled data points to calculate the eight-point DCT. This requirement can easily be satisfied for applications dealing with spatial signals such as image sensors and biomedical sensor arrays, by placing sensor elements in a non-uniform grid. In this work, a hardware architecture for the computation of the null mean ACT is proposed, followed by a novel architectures that extend the ACT for non-null mean signals. All circuits are physically implemented and tested using the Xilinx XC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for performance assessment.", "venue": "IEEE Transactions on Computers", "authors": ["Nilanka T. Rajapaksha", "Arjuna  Madanayake", "Renato J. Cintra", "Jithra  Adikari", "Vassil S. Dimitrov"], "year": 2015, "n_citations": 4}
{"id": 5234421, "s2_id": "65ff55eac00e415a6be7ff23c6091ca9bee64464", "title": "Ultra-low power on-chip learning of speech commands with phase-change memories", "abstract": "Embedding artificial intelligence at the edge (edge-AI) is an elegant solution to tackle the power and latency issues in the rapidly expanding Internet of Things. As edge devices typically spend most of their time in sleep mode and only wake-up infrequently to collect and process sensor data, non-volatile in-memory computing (NVIMC) is a promising approach to design the next generation of edge-AI devices. Recently, we proposed an NVIMC-based neuromorphic accelerator using the phase change memories (PCMs), which we call as Raven. In this work, we demonstrate the ultra-low-power on-chip training and inference of speech commands using Raven. We showed that Raven can be trained on-chip with power consumption as low as 30~uW, which is suitable for edge applications. Furthermore, we showed that at iso-accuracies, Raven needs 70.36x and 269.23x less number of computations to be performed than a deep neural network (DNN) during inference and training, respectively. Owing to such low power and computational requirements, Raven provides a promising pathway towards ultra-low-power training and inference at the edge.", "venue": "ArXiv", "authors": ["Venkata Pavan Kumar Miriyala", "Masatoshi  Ishii"], "year": 2020, "n_citations": 1}
{"id": 5237707, "s2_id": "3b406813e2711dd4b20eb6fea1742932dce758d4", "title": "Asynchronous Ripple Carry Adder based on Area Optimized Early Output Dual-Bit Full Adder", "abstract": "This technical note presents the design of a new area optimized asynchronous early output dual-bit full adder (DBFA). An asynchronous ripple carry adder (RCA) is constructed based on the new asynchronous DBFAs and existing asynchronous early output single-bit full adders (SBFAs). The asynchronous DBFAs and SBFAs incorporate redundant logic and are encoded using the delay-insensitive dual-rail code (i.e. homogeneous data encoding) and follow a 4-phase return-to-zero handshaking. Compared to the previous asynchronous RCAs involving DBFAs and SBFAs, which are based on homogeneous or heterogeneous delay-insensitive data encodings and which correspond to different timing models, the early output asynchronous RCA incorporating the proposed DBFAs and/or SBFAs is found to result in reduced area for the dual-operand addition operation and feature significantly less latency than the asynchronous RCAs which consist of only SBFAs. The proposed asynchronous DBFA requires 28.6% less silicon than the previously reported asynchronous DBFA. For a 32-bit asynchronous RCA, utilizing 2 stages of SBFAs in the least significant positions and 15 stages of DBFAs in the more significant positions leads to optimization in the latency. The new early output 32-bit asynchronous RCA containing DBFAs and SBFAs reports the following optimizations in design metrics over its counterparts: i) 18.8% reduction in area than a previously reported 32-bit early output asynchronous RCA which also has 15 stages of DBFAs and 2 stages of SBFAs, ii) 29.4% reduction in latency than a 32-bit early output asynchronous RCA containing only SBFAs.", "venue": "ArXiv", "authors": ["P.  Balasubramanian"], "year": 2018, "n_citations": 1}
{"id": 5239777, "s2_id": "3152f43374c47acd25665236c140ca80645cf9d2", "title": "CONTRA: Area-Constrained Technology Mapping Framework For Memristive Memory Processing Unit", "abstract": "Data-intensive applications are poised to benefit directly from processing-in-memory platforms, such as memristive Memory Processing Units, which allow leveraging data locality and performing stateful logic operations. Developing design automation flows for such platforms is a challenging and highly relevant research problem. In this work, we investigate the problem of minimizing delay under arbitrary area constraint for MAGIC-based in-memory computing platforms. We propose an end-to-end area constrained technology mapping framework, CONTRA. CONTRA uses Look-Up Table (LUT) based mapping of the input function on the crossbar array to maximize parallel operations and uses a novel search technique to move data optimally inside the array. CONTRA supports benchmarks in a variety of formats, along with crossbar dimensions as input to generate MAGIC instructions. CONTRA scales for large benchmarks, as demonstrated by our experiments. CONTRA allows mapping benchmarks to smaller crossbar dimensions than achieved by any other technique before, while allowing a wide variety of area-delay trade-offs. CONTRA improves the composite metric of area-delay product by 2.1\u00d7 to 13.1\u00d7 compared to seven existing technology mapping approaches.", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Debjyoti  Bhattacharjee", "Anupam  Chattopadhyay", "Srijit  Dutta", "Ronny  Ronen", "Shahar  Kvatinsky"], "year": 2020, "n_citations": 3}
{"id": 5239943, "s2_id": "abacbca9fa68a820c09d4244ccd5831b1815eb26", "title": "Performance Analysis of Scientific Computing Workloads on Trusted Execution Environments", "abstract": "Author(s): Akram, Ayaz; Giannakou, Anna; Akella, Venkatesh; Lowe-Power, Jason; Peisert, Sean | Abstract: Scientific computing sometimes involves computation on sensitive data. Depending on the data and the execution environment, the HPC (high-performance computing) user or data provider may require confidentiality and/or integrity guarantees. To study the applicability of hardware-based trusted execution environments (TEEs) to enable secure scientific computing, we deeply analyze the performance impact of AMD SEV and Intel SGX for diverse HPC benchmarks including traditional scientific computing, machine learning, graph analytics, and emerging scientific computing workloads. We observe three main findings: 1) SEV requires careful memory placement on large scale NUMA machines (1\u00d7\u22123.4\u00d7 slowdown without and 1\u00d7\u22121.15\u00d7 slowdown with NUMA aware placement), 2) virtualization\u2212a prerequisite for SEV\u2212results in performance degradation for workloads with irregular memory accesses and large working sets (1\u00d7\u22124\u00d7 slowdown compared to native execution for graph applications) and 3) SGX is inappropriate for HPC given its limited secure memory size and inflexible programming model (1.2\u00d7\u2212126\u00d7 slowdown over unsecure execution). Finally, we discuss forthcoming new TEE designs and their potential impact on scientific computing.", "venue": "ArXiv", "authors": ["Ayaz  Akram", "Anna  Giannakou", "Venkatesh  Akella", "Jason  Lowe-Power", "Sean  Peisert"], "year": 2020, "n_citations": 1}
{"id": 5240074, "s2_id": "ed290171d764b24cacd7846ab283a62c0f5d13ce", "title": "DLA: Compiler and FPGA Overlay for Neural Network Inference Acceleration", "abstract": "Overlays have shown significant promise for field-programmable gate-arrays (FPGAs) as they allow for fast development cycles and remove many of the challenges of the traditional FPGA hardware design flow. However, this often comes with a significant performance burden resulting in very little adoption of overlays for practical applications. In this paper, we tailor an overlay to a specific application domain, and we show how we maintain its full programmability without paying for the performance overhead traditionally associated with overlays. Specifically, we introduce an overlay targeted for deep neural network inference with only ~1% overhead to support the control and reprogramming logic using a lightweight very-long instruction word (VLIW) network. Additionally, we implement a sophisticated domain specific graph compiler that compiles deep learning languages such as Caffe or Tensorflow to easily target our overlay. We show how our graph compiler performs architecture-driven software optimizations to significantly boost performance of both convolutional and recurrent neural networks (CNNs/RNNs) \u2013 we demonstrate a 3x improvement on ResNet-101 and a 12x improvement for long short-term memory (LSTM) cells, compared to naive implementations. Finally, we describe how we can tailor our hardware overlay, and use our graph compiler to achieve ~900 fps on GoogLeNet on an Intel Arria 10 1150 \u2013 the fastest ever reported on comparable FPGAs.", "venue": "2018 28th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Mohamed S. Abdelfattah", "David  Han", "Andrew  Bitar", "Roberto  DiCecco", "Shane  O'Connell", "Nitika  Shanker", "Joseph  Chu", "Ian  Prins", "Joshua  Fender", "Andrew C. Ling", "Gordon R. Chiu"], "year": 2018, "n_citations": 37}
{"id": 5245202, "s2_id": "fc84f56b877e73ae57fc408b2abf7e020727535d", "title": "Archer: A Community Distributed Computing Infrastructure for Computer Architecture Research and Education", "abstract": "This paper introduces Archer, a community-based computing infrastructure supporting computer architecture research and education. The Archer system builds on virtualization techniques to provide a collaborative environment that facilitates sharing of computational resources and data among users. It integrates batch scheduling middleware to deliver high-throughput computing services aggregated from resources distributed across wide-area networks and owned by different participating entities in a seamless manner. The paper discusses the motivations that have led to the design of Archer, describes its core middleware components, and presents an analysis of the functionality and performance of the first wide-area deployment of Archer running a representative computer architecture simulation workload.", "venue": "CollaborateCom", "authors": ["Renato J. O. Figueiredo", "P. Oscar Boykin", "Jos\u00e9 A. B. Fortes", "Tao  Li", "Jie-Kwon  Peir", "David  Wolinsky", "Lizy Kurian John", "David R. Kaeli", "David J. Lilja", "Sally A. McKee", "Gokhan  Memik", "Alain J. Roy", "Gary S. Tyson"], "year": 2008, "n_citations": 19}
{"id": 5256237, "s2_id": "d7df3f828d61c8619f504201f01a5554d6e6d4da", "title": "A survey on Dependable Digital Systems using FPGAs: Current Methods and Challenges", "abstract": "Fault tolerance is increasingly being use to design Dependable Digital Systems (DDS), which refers to the capability of a system to keep performing its intended functions in existence of faults. DDS are typically used in Safety-critical system (SCS) such as medical (I&C) devices, Nuclear power Plants (I&C) devices and Aerospace (I&C) systems, the failure in these systems can cause harm to environment, death, injury to people. Different fault tolerance techniques were developed to overcome these issues and that has led to increase the reliability and dependability of applications on Field Programmable Gate Arrays (FPGAs). In this paper, multiple related works are present dealing with different types of faults and fault tolerance methods in FPGA based systems. Furthermore, a comparison between the evaluation metrics of previous works of Fault Tolerant (FT) techniques like hardware redundancy overhead, time delay, reliability, and performance are also present. Keyword: FPGA; fault tolerance; redundancy; critical systems; reliability.", "venue": "ArXiv", "authors": ["Farah Natiq Kassab bashi", "Shawkat S Khairullah"], "year": 2021, "n_citations": 0}
{"id": 5258392, "s2_id": "d76eeefe50ed4c172a3643de8e465aa74649a447", "title": "A Comprehensive and Cross-Platform Test Suite for Memory Safety - Towards an Open Framework for Testing Processor Hardware Supported Security Extensions", "abstract": "Memory safety remains a critical and widely violated property in reality. Numerous defense techniques have been proposed and developed but most of them are not applied or enabled by default in production-ready environment due to their substantial running cost. The situation might change in the near future because the hardware supported defenses against these attacks are finally beginning to be adopted by commercial processors, operating systems and compilers. We then face a question as there is currently no suitable test suite to measure the memory safety extensions supported on different processors. In fact, the issue is not constrained only for memory safety but all aspect of processor security. All of the existing test suites related to processor security lack some of the key properties, such as comprehensiveness, distinguishability and portability. As an initial step, we propose an expandable test framework for measuring the processor security and open source a memory safety test suite utilizing this framework. The framework is deliberately designed to be flexible so it can be gradually extended to all types of hardware supported security extensions in processors. The initial test suite for memory safety currently contains 160 test cases covering spatial and temporal safety of memory, memory access control, pointer integrity and control-flow integrity. Each type of vulnerabilities and their related defenses have been individually evaluated by one or more test cases. The test suite has been ported to three different instruction set architectures (ISAs) and experimented on six different platforms. We have also utilized the test suite to explore the security benefits of applying different sets of compiler flags available on the latest GNU GCC and LLVM compilers.", "venue": "ArXiv", "authors": ["Wei  Song", "Jiameng  Ying", "Sihao  Shen", "Boya  Li", "Hao  Ma", "Peng  Liu"], "year": 2021, "n_citations": 0}
{"id": 5265211, "s2_id": "f052bf9616a0085d50a36b8e147dfb89a9be1a5f", "title": "openwifi CSI fuzzer for authorized sensing and covert channels", "abstract": "CSI (Channel State Information) of WiFi systems contains the environment channel response between the transmitter and the receiver, so the people/objects and their movement in between can be sensed. To get CSI, the receiver performs channel estimation based on the pre-known training field of the transmitted WiFi signal. CSI related technology is useful in many cases, but it also brings concerns on privacy and security. In this paper, we open sourced a CSI fuzzer to enhance the privacy and security of WiFi CSI applications. It is built and embedded into the transmitter of openwifi, which is an open source full-stack WiFi chip design, to prevent unauthorized sensing without sacrificing the WiFi link performance. The CSI fuzzer imposes an artificial channel response to the signal before it is transmitted, so the CSI seen by the receiver will indicate the actual channel response combined with the artificial response. Only the authorized receiver, that knows the artificial response, can calculate the actual channel response and perform the CSI sensing. Another potential application of the CSI fuzzer is covert channels based on a set of pre-defined artificial response patterns. Our work resolves the pain point of implementing the anti-sensing idea based on the commercial off-the-shelf WiFi devices.", "venue": "WISEC", "authors": ["Xianjun  Jiao", "Michael  Mehari", "Wei  Liu", "Muhammad  Aslam", "Ingrid  Moerman"], "year": 2021, "n_citations": 0}
{"id": 5265386, "s2_id": "e7641bd09c784793b6fd8183a07af5159d044cd7", "title": "SparseNN: An energy-efficient neural network accelerator exploiting input and output sparsity", "abstract": "Contemporary Deep Neural Network (DNN) contains millions of synaptic connections with tens to hundreds of layers. The large computational complexity poses a challenge to the hardware design. In this work, we leverage the intrinsic activation sparsity of DNN to substantially reduce the execution cycles and the energy consumption. An end-to-end training algorithm is proposed to develop a lightweight (less than 5% overhead) run-time predictor for the output activation sparsity on the fly. Furthermore, an energy-efficient hardware architecture, SparseNN, is proposed to exploit both the input and output sparsity. SparseNN is a scalable architecture with distributed memories and processing elements connected through a dedicated on-chip network. Compared with the state-of-the-art accelerators which only exploit the input sparsity, SparseNN can achieve a 10%\u201370% improvement in throughput and a power reduction of around 50%.", "venue": "2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Jingyang  Zhu", "Jingbo  Jiang", "Xizi  Chen", "Chi-Ying  Tsui"], "year": 2018, "n_citations": 21}
{"id": 5266690, "s2_id": "6c589d08f9faa74c24e209956d79e5ef42fc7513", "title": "Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems", "abstract": "Superpages have long been used to mitigate address translation overhead in large-memory systems. However, superpages often preclude lightweight page migration, which is crucial for performance and energy efficiency in hybrid memory systems composed of DRAM and non-volatile memory (NVM). In this article, we propose a novel memory management mechanism called Rainbow to bridge this fundamental conflict between superpages and lightweight page migration. Rainbow manages NVM at the superpage granularity, and uses DRAM to cache frequently accessed (hot) small pages within each superpage. Correspondingly, Rainbow utilizes split TLBs to support different page sizes. By introducing an efficient hot page identification mechanism and a novel NVM-to-DRAM address remapping mechanism, Rainbow supports lightweight page migration without splintering superpages. Experiment results show that Rainbow can significantly reduce applications\u2019 TLB misses by 99.9%, and improve application performance (in terms of IPC) by up to 2.9\u00d7 (45.3% on average) when compared to a state-of-the-art memory migration policy without a superpage support.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Xiaoyuan  Wang"], "year": 2019, "n_citations": 11}
{"id": 5267237, "s2_id": "82acd28509ad27373b1287b41d9146490aff5743", "title": "Memory Tagging and how it improves C/C++ memory safety", "abstract": "Memory safety in C and C++ remains largely unresolved. A technique usually called \"memory tagging\" may dramatically improve the situation if implemented in hardware with reasonable overhead. This paper describes two existing implementations of memory tagging: one is the full hardware implementation in SPARC; the other is a partially hardware-assisted compiler-based tool for AArch64. We describe the basic idea, evaluate the two implementations, and explain how they improve memory safety. This paper is intended to initiate a wider discussion of memory tagging and to motivate the CPU and OS vendors to add support for it in the near future.", "venue": "ArXiv", "authors": ["Kostya  Serebryany", "Evgenii  Stepanov", "Aleksey  Shlyapnikov", "Vlad  Tsyrklevich", "Dmitry  Vyukov"], "year": 2018, "n_citations": 30}
{"id": 5272719, "s2_id": "6aa1b80013012b0aa824c8ac27dc9183d7af94cc", "title": "RapidLayout: Fast Hard Block Placement of FPGA-Optimized Systolic Arrays using Evolutionary Algorithms", "abstract": "Evolutionary algorithms can outperform conventional placement algorithms such as simulated annealing, analytical placement as well as manual placement on metrics such as runtime, wirelength, pipelining cost, and clock frequency when mapping FPGA hard block intensive designs such as systolic arrays on Xilinx UltraScale+ FPGAs. For certain hard-block intensive, systolic array accelerator designs, the commercial-grade Xilinx Vivado CAD tool is unable to provide a legal routing solution without tedious manual placement constraints. Instead, we formulate an automatic FPGA placement algorithm for these hard blocks as a multi-objective optimization problem that targets wirelength squared and maximum bounding box size metrics. We build an end-to-end placement and routing flow called RapidLayout using the Xilinx RapidWright framework. RapidLayout runs 5-6 times faster than Vivado with manual constraints and eliminates the weeks-long effort to generate placement constraints manually for the hard blocks. We also perform automated post-placement pipelining of the long wires inside each convolution block to target 650 MHz URAM-limited operation. RapidLayout outperforms (1) the simulated annealer in VPR by 33% in runtime, 1.9-2.4 times in wirelength, and 3-4 times in bounding box size, while also (2) beating the analytical placer UTPlaceF by 9.3 times in runtime, 1.8-2.2 times in wirelength, and 2-2.7 times in bounding box size. We employ transfer learning from a base FPGA device to speed-up placement optimization for similar FPGA devices in the UltraScale+ family by 11-14 times than learning the placements from scratch.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Niansong  Zhang", "Xiang  Chen", "Nachiket  Kapre"], "year": 2020, "n_citations": 2}
{"id": 5273946, "s2_id": "d59056afe85173520c40753decff1b8b6bd61d06", "title": "High Speed VLSI Architecture for 3-D Discrete Wavelet Transform", "abstract": "This paper presents a memory efficient, high throughput parallel lifting based running three dimensional discrete wavelet transform (3-D DWT) architecture. 3-D DWT is constructed by combining the two spatial and four temporal processors. Spatial processor (SP) apply the two dimensional DWT on a frame, using lifting based 9/7 filter bank through the row rocessor (RP) in row direction and then apply in the colum direction through column processor (CP). To reduce the temporal memory and the latency, the temporal processor (TP) has been designed with lifting based 1-D Haar wavelet filter. The proposed architecture replaced the multiplications by pipeline shift-add operations to reduce the CPD. Two spatial processors works simultaneously on two adjacent frames and provide 2-D DWT coefficients as inputs to the temporal processors. TPs apply the one dimensional DWT in temporal direction and provide eight 3-D DWT coefficients per clock (throughput). Higher throughput reduces the computing cycles per frame and enable the lower power consumption. Implementation results shows that the proposed architecture has the advantage in reduced memory, low power consumption, low latency, and high throughput over the existing designs. The RTL of the proposed architecture is described using verilog and synthesized using 90-nm technology CMOS standard cell library and results show that it consumes 43.42 mW power and occupies an area equivalent to 231.45 K equivalent gate at frequency of 200 MHz. The proposed architecture has also been synthesised for the Xilinx zynq 7020 series field programmable gate array (FPGA).", "venue": "ArXiv", "authors": ["Kota Naga Srinivasarao Batta", "Indrajit  Chakrabarti"], "year": 2015, "n_citations": 3}
{"id": 5274523, "s2_id": "2788d57e003bb85b6995a5740b671139a63bd390", "title": "Tb/s Polar Successive Cancellation Decoder 16nm ASIC Implementation", "abstract": "This work presents an efficient ASIC implementation of successive cancellation (SC) decoder for polar codes. SC is a low-complexity depth-first search decoding algorithm, favorable for beyond-5G applications that require extremely high throughput and low power. The ASIC implementation of SC in this work exploits many techniques including pipelining and unrolling to achieve Tb/s data throughput without compromising power and area metrics. To reduce the complexity of the implementation, an adaptive log-likelihood ratio (LLR) quantization scheme is used. This scheme optimizes bit precision of the internal LLRs within the range of 1-5 bits by considering irregular polarization and entropy of LLR distribution in SC decoder. The performance cost of this scheme is less than 0.2 dB when the code block length is 1024 bits and the payload is 854 bits. Furthermore, some computations in SC take large space with high degree of parallelization while others take longer time steps. To optimize these computations and reduce both memory and latency, register reduction/balancing (R-RB) method is used. The final decoder architecture is called optimized polar SC (OPSC). The post-placement-routing results at 16nm FinFet ASIC technology show that OPSC decoder achieves 1.2 Tb/s coded throughput on 0.79 mm$^2$ area with 0.95 pJ/bit energy efficiency.", "venue": "ArXiv", "authors": ["Altug  S\u00fcral", "E. G\u00f6ksu Sezer", "Ertugrul  Kolagasioglu", "Veerle  Derudder", "Kaoutar  Bertrand"], "year": 2020, "n_citations": 3}
{"id": 5276308, "s2_id": "3ea2ed2cb5d0e5878d69f90ea139741e911c6cb5", "title": "SoK: Opportunities for Software-Hardware-Security Codesign for Next Generation Secure Computing", "abstract": "Users are demanding increased data security. As a result, security is rapidly becoming a first-order design constraint in next generation computing systems. Researchers and practitioners are exploring various security technologies to meet user demand such as trusted execution environments (e.g., Intel SGX, ARM TrustZone), homomorphic encryption, and differential privacy. Each technique provides some degree of security, but differs with respect to threat coverage, performance overheads, as well as implementation and deployment challenges. In this paper, we present a systemization of knowledge (SoK) on these design considerations and trade-offs using several prominent security technologies. Our study exposes the need for software-hardware-security codesign to realize efficient and effective solutions of securing user data. In particular, we explore how design considerations across applications, hardware, and security mechanisms must be combined to overcome fundamental limitations in current technologies so that we can minimize performance overhead while achieving sufficient threat model coverage. Finally, we propose a set of guidelines to facilitate putting these secure computing technologies into practice.", "venue": "HASP@MICRO", "authors": ["Deeksha  Dangwal", "Meghan  Cowan", "Armin  Alaghi", "Vincent T. Lee", "Brandon  Reagen", "Caroline  Trippel"], "year": 2020, "n_citations": 0}
{"id": 5279415, "s2_id": "01946be5bc55d8e371c6925b2869d221a6cc639b", "title": "Static Analysis of Lockless Microcontroller C Programs", "abstract": "Concurrently accessing shared data without locking is usually a subject to race conditions resulting in inconsistent or corrupted data. However, there are programs operating correctly without locking by exploiting the atomicity of certain operations on a specific hardware. In this paper, we describe how to precisely analyze lockless microcontroller C programs with interrupts by taking the hardware architecture into account. We evaluate this technique in an octagon-based value range analysis using access-based localization to increase efficiency.", "venue": "SSV", "authors": ["Eva  Beckschulze", "Sebastian  Biallas", "Stefan  Kowalewski"], "year": 2012, "n_citations": 4}
{"id": 5284311, "s2_id": "7884aa04a3ee6059a3018a0763c9542a16f676ed", "title": "Enabling Reusable Physical Design Flows with Modular Flow Generators", "abstract": "Achieving high code reuse in physical design flows is challenging but increasingly necessary to build complex systems. Unfortunately, existing approaches based on parameterized Tcl generators support very limited reuse and struggle to preserve reusable code as designers customize flows for specific designs and technologies. We present a vision and framework based on modular flow generators that encapsulates coarse-grain and fine-grain reusable code in modular nodes and assembles them into complete flows. The key feature is a flow consistency and instrumentation layer embedded in Python, which supports mechanisms for rapid and early feedback on inconsistent composition. The approach gradually types the Tcl language and allows both automatic and user-annotated static assertion checks. We evaluate the design flows of successive generations of silicon prototypes designed in TSMC16, TSMC28, TSMC40, SKY130, and IBM180 technologies, showing how our approach can enable significant code reuse in future flows.", "venue": "ArXiv", "authors": ["Alex  Carsello", "James  Thomas", "Ankita  Nayak", "Po-Han  Chen", "Mark  Horowitz", "Priyanka  Raina", "Christopher  Torng"], "year": 2021, "n_citations": 0}
{"id": 5287667, "s2_id": "b00d87ca1398fe8df674292740f87dfa1fdd5802", "title": "Neuromemristive Circuits for Edge Computing: A Review", "abstract": "The volume, veracity, variability, and velocity of data produced from the ever increasing network of sensors connected to Internet pose challenges for power management, scalability, and sustainability of cloud computing infrastructure. Increasing the data processing capability of edge computing devices at lower power requirements can reduce several overheads for cloud computing solutions. This paper provides the review of neuromorphic CMOS-memristive architectures that can be integrated into edge computing devices. We discuss why the neuromorphic architectures are useful for edge devices and show the advantages, drawbacks, and open problems in the field of neuromemristive circuits for edge computing.", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "authors": ["Olga  Krestinskaya", "Alex Pappachen James", "Leon O. Chua"], "year": 2020, "n_citations": 81}
{"id": 5290822, "s2_id": "da5af671dc98fb069779fabefb251e65217c4a7b", "title": "Practical Data Compression for Modern Memory Hierarchies", "abstract": "In this thesis, we describe a new, practical approach to integrating hardware-based data compression within the memory hierarchy, including on-chip caches, main memory, and both on-chip and off-chip interconnects. This new approach is fast, simple, and effective in saving storage space. A key insight in our approach is that access time (including decompression latency) is critical in modern memory hierarchies. By combining inexpensive hardware support with modest OS support, our holistic approach to compression achieves substantial improvements in performance and energy efficiency across the memory hierarchy. Using this new approach, we make several major contributions in this thesis. First, we propose a new compression algorithm, Base-Delta-Immediate Compression (BDI), that achieves high compression ratio with very low compression/decompression latency. BDI exploits the existing low dynamic range of values present in many cache lines to compress them to smaller sizes using Base+Delta encoding. Second, we observe that the compressed size of a cache block can be indicative of its reuse. We use this observation to develop a new cache insertion policy for compressed caches, the Size-based Insertion Policy (SIP), which uses the size of a compressed block as one of the metrics to predict its potential future reuse. Third, we propose a new main memory compression framework, Linearly Compressed Pages (LCP), that significantly reduces the complexity and power cost of supporting main memory compression. We demonstrate that any compression algorithm can be adapted to fit the requirements of LCP, and that LCP can be efficiently integrated with the existing cache compression designs, avoiding extra compression/decompression.", "venue": "ArXiv", "authors": ["Gennady  Pekhimenko"], "year": 2016, "n_citations": 5}
{"id": 5304911, "s2_id": "cf11a0ed193df2aa00573737136e6f9c98e2c820", "title": "A System-Level Framework for Analytical and Empirical Reliability Exploration of STT-MRAM Caches", "abstract": "Spin-transfer torque magnetic RAM (STT-MRAM) is known as the most promising replacement for static random access memory (SRAM) technology in large last-level cache memories (LLC). Despite its high density, nonvolatility, near-zero leakage power, and immunity to radiation as the major advantages, STT-MRAM-based cache memory suffers from high error rates mainly due to retention failure (RF), read disturbance, and write failure. Existing studies are limited to estimate the rate of only one or two of these error types for STT-MRAM cache. However, the overall vulnerability of STT-MRAM caches, whose estimation is a must to design cost-efficient reliable caches, has not been studied previously. In this paper, we propose a system-level framework for reliability exploration and characterization of errors\u2019 behavior in STT-MRAM caches. To this end, we formulate the cache vulnerability considering the intercorrelation of the error types including RF, read disturbance, and write failure as well as the dependency of error rates to workloads\u2019 behavior and process variations (PVs). Our analysis reveals that STT-MRAM cache vulnerability is highly workload-dependent and varies by orders of magnitude in different cache access patterns. Our analytical study also shows that this vulnerability divergence significantly increases by PVs in STT-MRAM cells. To take the effects of system workloads and PVs into account, we implement the error types in gem5 full-system simulator. The experimental results using a comprehensive set of multiprogrammed workloads from SPEC CPU2006 benchmark suite on a quad-core processor show that the total error rate in a shared STT-MRAM LLC varies by 32.0\u00d7 for different workloads. A further 6.5\u00d7 vulnerability variation is observed when considering PVs in the STT-MRAM cells. In addition, the contribution of each error type in total LLC vulnerability highly varies in different cache access patterns and moreover, error rates are differently affected by PVs. The proposed analytical and empirical studies can significantly help system architects for efficient utilization of error mitigation techniques and designing highly reliable and low-cost STT-MRAM LLCs.", "venue": "IEEE Transactions on Reliability", "authors": ["Elham  Cheshmikhani", "Hamed  Farbeh", "Hossein  Asadi"], "year": 2020, "n_citations": 5}
{"id": 5305907, "s2_id": "222a4c8802ea38e57875227df65bb97350e6dfef", "title": "Elastic Silicon Interconnects: Abstracting Communication in Accelerator Design", "abstract": "ing Communication in Accelerator Design John Demme john.demme@microsoft.com Microsoft, USA", "venue": "ArXiv", "authors": ["John  Demme"], "year": 2021, "n_citations": 0}
{"id": 5310263, "s2_id": "1193e380545ba339ab665b9f00df39fdae1f5836", "title": "Memristor-based circuits for performing basic arithmetic operations", "abstract": "Abstract In almost all of the currently working circuits, especially in analog circuits implementing signal processing applications, basic arithmetic operations such as multiplication, addition, subtraction and division are performed on values which are represented by voltages or currents. However, in this paper, we propose a new and simple method for performing analog arithmetic operations which in this scheme, signals are represented and stored through a memristance of the newly found circuit element, i.e. memristor, instead of voltage or current. Some of these operators such as divider and multiplier are much simpler and faster than their equivalent voltage-based circuits and they require less chip area. In addition, a new circuit is designed for programming the memristance of the memristor with predetermined analog value. Presented simulation results demonstrate the effectiveness and the accuracy of the proposed circuits.", "venue": "WCIT", "authors": ["Farnood  Merrikh-Bayat", "Saeed Bagheri Shouraki"], "year": 2011, "n_citations": 105}
{"id": 5311367, "s2_id": "e27fd0fb459644266c8e7d845222b9e3b7172928", "title": "PIM-Enclave: Bringing Confidential Computation Inside Memory", "abstract": "Demand for data-intensive workloads and confidential computing are the prominent research directions shaping the future of cloud computing. Computer architectures are evolving to accommodate the computing of large data better. Protecting the computation of sensitive data is also an imperative yet challenging objective; processor-supported secure enclaves serve as the key element in confidential computing in the cloud. However, side-channel attacks are threatening their security boundaries. The current processor architectures consume a considerable portion of its cycles in moving data. Near data computation is a promising approach that minimizes redundant data movement by placing computation inside storage. In this paper, we present a novel design for Processing-In-Memory (PIM) as a dataintensive workload accelerator for confidential computing. Based on our observation that moving computation closer to memory can achieve efficiency of computation and confidentiality of the processed information simultaneously, we study the advantages of confidential computing inside memory. We then explain our security model and programming model developed for PIMbased computation offloading. We construct our findings into a software-hardware co-design, which we call PIM-Enclave. Our design illustrates the advantages of PIM-based confidential computing acceleration. Our evaluation shows PIM-Enclave can provide a side-channel resistant secure computation offloading and run data-intensive applications with negligible performance overhead compared to baseline PIM model.", "venue": "ArXiv", "authors": ["Kha Dinh Duy", "Hojoon  Lee"], "year": 2021, "n_citations": 0}
{"id": 5311394, "s2_id": "6fe76cf2e28932b75b49565e0051c85a62a524e5", "title": "From FPGAs to Obfuscated eASICs: Design and Security Trade-offs", "abstract": "Threats associated with the untrusted fabrication of integrated circuits (ICs) are numerous: piracy, overproduction, reverse engineering, hardware trojans, etc. The use of reconfigurable elements (i.e., look-up tables as in FPGAs) is a known obfuscation technique. In the extreme case, when the circuit is entirely implemented as an FPGA, no information is revealed to the adversary but at a high cost in area, power, and performance. In the opposite extreme, when the same circuit is implemented as an ASIC, best-in-class performance is obtained but security is compromised. This paper investigates an intermediate solution between these two. Our results are supported by a custom CAD tool that explores this FPGA-ASIC design space and enables a standardcell based physical synthesis flow that is flexible and compatible with current design practices. Layouts are presented for obfuscated circuits in a 65nm commercial technology, demonstrating the attained obfuscation both graphically and quantitatively. Furthermore, our security analysis revealed that for truly hiding the circuit\u2019s intent (not only portions of its structure), the obfuscated design also has to chiefly resemble an FPGA: only some small amount of logic can be made static for an adversary to remain unaware of what the circuit does.", "venue": "ArXiv", "authors": ["Zain Ul Abideen", "Tiago Diadami Perez", "Samuel  Pagliarini"], "year": 2021, "n_citations": 0}
{"id": 5315701, "s2_id": "13b157838139bbfee70f576aa99511e82f958ec3", "title": "Indicating Asynchronous Multipliers", "abstract": "Multiplication is a basic arithmetic operation that is encountered in almost all general-purpose microprocessing and digital signal processing applications, and multiplication is physically realized using a multiplier. This paper discusses the physical implementation of indicating asynchronous multipliers, which are inherently elastic and are robust to timing, process, and parametric variations, and are modular. We consider the physical implementation of many weak-indication asynchronous multipliers using a 32/28-nm CMOS technology by adopting the array multiplier architecture. The multipliers are synthesized in a semi-custom ASIC-design style. The 4-phase return-to-zero (RTZ) and the 4-phase return-to-one (RTO) handshake protocols are considered for the data communication. The multipliers are realized using strong-indication or weak-indication full adders. Strong-indication 2-input AND function is used to generate the partial products in the case of both RTZ and RTO handshaking. The full adders considered are derived from different indicating asynchronous logic design methods. Among the multipliers considered, a weak-indication asynchronous multiplier utilizing the biased weak-indication full adder is found to be efficient in terms of the cycle time and the power-cycle time product with respect to both RTZ and RTO handshaking. Also, the 4-phase RTO handshake protocol is found to be preferable than the 4-phase RTZ handshake protocol for achieving enhanced optimizations in the design metrics.", "venue": "2018 2nd European Conference on Electrical Engineering and Computer Science (EECS)", "authors": ["P  Balasubramanian", "D L Maskell", "N E Mastorakis"], "year": 2018, "n_citations": 3}
{"id": 5317092, "s2_id": "aff8c5fe07459372e09a1bcb566cf1cd4ced48dc", "title": "Processing Distribution and Architecture Tradeoff for Large Intelligent Surface Implementation", "abstract": "The Large Intelligent Surface (LIS) concept has emerged recently as a new paradigm for wireless communication, remote sensing and positioning. It consists of a continuous radiating surface placed relatively close to the users, which is able to communicate with users by independent transmission and reception (as a Base Station). Despite of its potential, there are a lot of challenges from an implementation point of view, with the interconnection data-rate and computational complexity being the most relevant. Distributed processing techniques and hierarchical architectures are expected to play a vital role addressing this while ensuring scalability. In this paper we perform algorithm-architecture codesign and analyze the hardware requirements and architecture trade-offs for a discrete LIS to perform uplink detection. By doing this, we expect to give concrete case studies and guidelines for efficient implementation of LIS systems.", "venue": "2020 IEEE International Conference on Communications Workshops (ICC Workshops)", "authors": ["Jesus Rodriguez Sanchez", "Ove  Edfors", "Fredrik  Rusek", "Liang  Liu"], "year": 2020, "n_citations": 1}
{"id": 5320292, "s2_id": "1ef99dc01c935be5b5036a8b3b6e2c5e50c8adaf", "title": "Snitch: A Tiny Pseudo Dual-Issue Processor for Area and Energy Efficient Execution of Floating-Point Intensive Workloads", "abstract": "Data-parallel applications, such as data analytics, machine learning, and scientific computing, are placing an ever-growing demand on floating-point operations per second on emerging systems. With increasing integration density, the quest for energy efficiency becomes the number one design concern. While dedicated accelerators provide high energy efficiency, they are over-specialized and hard to adjust to algorithmic changes. We propose an architectural concept that tackles the issues of achieving extreme energy efficiency while still maintaining high flexibility as a general-purpose compute engine. The key idea is to pair a tiny 10kGE (kilo gate equivalent) control core, called Snitch, with a double-precision floating-point unit (FPU) to adjust the compute to control ratio. While traditionally minimizing non-floating-point unit (FPU) area and achieving high floating-point utilization has been a trade-off, with Snitch, we achieve them both, by enhancing the ISA with two minimally intrusive extensions: stream semantic registers (SSR) and a floating-point repetition instruction (FREP). SSRs allow the core to implicitly encode load/store instructions as register reads/writes, eliding many explicit memory instructions. The FREP extension decouples the floating-point and integer pipeline by sequencing instructions from a micro-loop buffer. These ISA extensions significantly reduce the pressure on the core and free it up for other tasks, making Snitch and FPU effectively dual-issue at a minimal incremental cost of 3.2 percent. The two low overhead ISA extensions make Snitch more flexible than a contemporary vector processor lane, achieving a <inline-formula><tex-math notation=\"LaTeX\">$2\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>2</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zaruba-ieq1-3027900.gif\"/></alternatives></inline-formula> energy-efficiency improvement. We have evaluated the proposed core and ISA extensions on an octa-core cluster in 22 nm technology. We achieve more than <inline-formula><tex-math notation=\"LaTeX\">$6\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>6</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zaruba-ieq2-3027900.gif\"/></alternatives></inline-formula> multi-core speed-up and a <inline-formula><tex-math notation=\"LaTeX\">$3.5\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zaruba-ieq3-3027900.gif\"/></alternatives></inline-formula> gain in energy efficiency on several parallel microkernels.", "venue": "IEEE Transactions on Computers", "authors": ["Florian  Zaruba", "Fabian  Schuiki", "Torsten  Hoefler", "Luca  Benini"], "year": 2021, "n_citations": 14}
{"id": 5321949, "s2_id": "905ed42bb9451ff4ef9760bfbf8e825607a2ecf8", "title": "Power Assignment Problems in Wireless Communication", "abstract": "A fundamental class of problems in wireless communication is concerned with the assignment of suitable transmission powers to wireless devices/stations such that the resulting communication graph satisfies certain desired properties and the overall energy consumed is minimized. Many concrete communication tasks in a wireless network like broadcast, multicast, point-to-point routing, creation of a communication backbone, etc. can be regarded as such a power assignment problem. \nThis paper considers several problems of that kind; for example one problem studied before in \\cite{Carrots, Bilo} aims to select and assign powers to $k$ of the stations such that all other stations are within reach of at least one of the selected stations. We improve the running time for obtaining a $(1+\\epsilon)$-approximate solution for this problem from $n^{((\\alpha/\\epsilon)^{O(d)})}$ as reported by Bilo et al. (\\cite{Bilo}) to $O(n+ {(\\frac{k^{2d+1}}{\\epsilon^d})}^{\\min{\\{2k, (\\alpha/\\epsilon)^{O(d)} \\}}})$ that is, we obtain a running time that is \\emph{linear} in the network size. Further results include a constant approximation algorithm for the TSP problem under squared (non-metric!) edge costs, which can be employed to implement a novel data aggregation protocol, as well as efficient schemes to perform $k$-hop multicasts.", "venue": "ArXiv", "authors": ["Stefan  Funke", "S\u00f6ren  Laue", "Zvi  Lotker", "Rouven  Naujoks"], "year": 2006, "n_citations": 1}
{"id": 5332918, "s2_id": "a9be71e416b32a85ba0cf4d5c725ea0eed212e5b", "title": "FreeIMU: An Open Hardware Framework for Orientation and Motion Sensing", "abstract": "Orientation and Motion Sensing are widely implemented on various consumer products, such as mobile phones, tablets and cameras as they enable immediate interaction with virtual information. The prototyping phase of any orientation and motion sensing capable device is however a quite difficult process as it may involve complex hardware designing, math algorithms and programming. \nIn this paper, we present FreeIMU, an Open Hardware Framework for prototyping orientation and motion sensing capable devices. The framework consists in a small circuit board containing various sensors and a software library, built on top of the Arduino platform. Both the hardware and library are released under open licences and supported by an active community allowing to be implemented into research and commercial projects.", "venue": "ArXiv", "authors": ["Fabio  Varesano"], "year": 2013, "n_citations": 14}
{"id": 5333226, "s2_id": "a7bb0f78e78b9e55253c3163e4ab861823bce518", "title": "Cyclic Sequence Generators as Program Counters for High-Speed FPGA-based Processors", "abstract": "This paper compares the performance of conventional radix-2 program counters with program counters based on Feedback Shift Registers (FSRs), a class of cyclic sequence generator. FSR counters have constant time scaling with bit-width, $N$, whereas FPGA-based radix-2 counters typically have $O(N)$ time-complexity due to the carry-chain. Program counter performance is measured by synthesis of standalone counter circuits, as well as synthesis of three FPGA-based processor designs modified to incorporate FSR program counters. Hybrid counters, combining both an FSR and a radix-2 counter, are presented as a solution to the potential cache-coherency issues of FSR program counters. Results show that high-speed processor designs benefit more from FSR program counters, allowing both greater operating frequency and the use of fewer logic resources.", "venue": "ArXiv", "authors": ["P. A. Suggate", "R. W. Ward", "T. C. A. Molteno"], "year": 2019, "n_citations": 0}
{"id": 5333902, "s2_id": "ff92364e36f086a9464f3308f5295d25cadc0903", "title": "Design Principles for Packet Deparsers on FPGAs", "abstract": "The P4 language has drastically changed the networking field as it allows to quickly describe and implement new networking applications. Although a large variety of applications can be described with the P4 language, current programmable switch architectures impose significant constraints on P4 programs. To address this shortcoming, FPGAs have been explored as potential targets for P4 applications. P4 applications are described using three abstractions: a packet parser, match-action tables, and a packet deparser, which reassembles the output packet with the result of the match-action tables. While implementations of packet parsers and match-action tables on FPGAs have been widely covered in the literature, no general design principles have been presented for the packet deparser. Indeed, implementing a high-speed and efficient deparser on FPGAs remains an open issue because it requires a large amount of interconnections and the architecture must be tailored to a P4 program. As a result, in several works where a P4 application is implemented on FPGAs, the deparser consumes a significant proportion of chip resources. Hence, in this paper, we address this issue by presenting design principles for efficient and high-speed deparsers on FPGAs. As an artifact, we introduce a tool that generates an efficient vendor-agnostic deparser architecture from a P4 program.Our design has been validated and simulated with a cocotb-based framework.The resulting architecture is implemented on Xilinx Ultrascale+ FPGAs and supports a throughput of more than 200 Gbps while reducing resource usage by almost 10x compared to other solutions.", "venue": "FPGA", "authors": ["Thomas  Luinaud", "Jeferson Santiago da Silva", "J. M. Pierre Langlois", "Yvon  Savaria"], "year": 2021, "n_citations": 0}
{"id": 5337044, "s2_id": "b703f63ce8bfb2c11c90eb778b72136d71939c88", "title": "CAAD: Computer Architecture for Autonomous Driving", "abstract": "We describe the computing tasks involved in autonomous driving, examine existing autonomous driving computing platform implementations. To enable autonomous driving, the computing stack needs to simultaneously provide high performance, low power consumption, and low thermal dissipation, at low cost. We discuss possible approaches to design computing platforms that will meet these needs.", "venue": "ArXiv", "authors": ["Shaoshan  Liu", "Jie  Tang", "Zhe  Zhang", "Jean-Luc  Gaudiot"], "year": 2017, "n_citations": 20}
{"id": 5341090, "s2_id": "f58b2906b3b8391e534f47ead2264f3dc4498589", "title": "Enabling and Exploiting Partition-Level Parallelism (PALP) in Phase Change Memories", "abstract": "Phase-change memory (PCM) devices have multiple banks to serve memory requests in parallel. Unfortunately, if two requests go to the same bank, they have to be served one after another, leading to lower system performance. We observe that a modern PCM bank is implemented as a collection of partitions that operate mostly independently while sharing a few global peripheral structures, which include the sense amplifiers (to read) and the write drivers (to write). Based on this observation, we propose PALP, a new mechanism that enables partition-level parallelism within each PCM bank, and exploits such parallelism by using the memory controller\u2019s access scheduling decisions. PALP consists of three new contributions. First, we introduce new PCM commands to enable parallelism in a bank\u2019s partitions in order to resolve the read-write bank conflicts, with no changes needed to PCM logic or its interface. Second, we propose simple circuit modifications that introduce a new operating mode for the write drivers, in addition to their default mode of serving write requests. When configured in this new mode, the write drivers can resolve the read-read bank conflicts, working jointly with the sense amplifiers. Finally, we propose a new access scheduling mechanism in PCM that improves performance by prioritizing those requests that exploit partition-level parallelism over other requests, including the long outstanding ones. While doing so, the memory controller also guarantees starvation-freedom and the PCM\u2019s running-average-power-limit (RAPL). We evaluate PALP with workloads from the MiBench and SPEC CPU2017 Benchmark suites. Our results show that PALP reduces average PCM access latency by 23%, and improves average system performance by 28% compared to the state-of-the-art approaches.", "venue": "ACM Trans. Embed. Comput. Syst.", "authors": ["Shihao  Song", "Anup  Das", "Onur  Mutlu", "Nagarajan  Kandasamy"], "year": 2019, "n_citations": 21}
{"id": 5342155, "s2_id": "0b1a602cb8a636d8a65ac78c2091766217253cd4", "title": "Pixie: A heterogeneous Virtual Coarse-Grained Reconfigurable Array for high performance image processing applications", "abstract": "Coarse-Grained Reconfigurable Arrays (CGRAs) enable ease of programmability and result in low development costs. They enable the ease of use specifically in reconfigurable computing applications. The smaller cost of compilation and reduced reconfiguration overhead enables them to become attractive platforms for accelerating high-performance computing applications such as image processing. The CGRAs are ASICs and therefore, expensive to produce. However, Field Programmable Gate Arrays (FPGAs) are relatively cheaper for low volume products but they are not so easily programmable. We combine best of both worlds by implementing a Virtual Coarse-Grained Reconfigurable Array (VCGRA) on FPGA. VCGRAs are a trade off between FPGA with large routing overheads and ASICs. In this perspective we present a novel heterogeneous Virtual Coarse-Grained Reconfigurable Array (VCGRA) called \"Pixie\" which is suitable for implementing high performance image processing applications. The proposed VCGRA contains generic processing elements and virtual channels that are described using the Hardware Description Language VHDL. Both elements have been optimized by using the parameterized configuration tool flow and result in a resource reduction of 24% for each processing elements and 82% for each virtual channels respectively.", "venue": "ArXiv", "authors": ["Amit  Kulkarni", "Dirk  Stroobandt", "Andr\u00e9  Werner", "Florian  Fricke", "Michael  H\u00fcbner"], "year": 2017, "n_citations": 12}
{"id": 5342840, "s2_id": "1ec6fae053444355828b8a98b8a2c0092f5a877a", "title": "Cohmeleon: Learning-Based Orchestration of Accelerator Coherence in Heterogeneous SoCs", "abstract": "One of the most critical aspects of integrating loosely-coupled accelerators in heterogeneous SoC architectures is orchestrating their interactions with the memory hierarchy, especially in terms of navigating the various cache-coherence options: from accelerators accessing off-chip memory directly, bypassing the cache hierarchy, to accelerators having their own private cache. By running real-size applications on FPGA-based prototypes of many-accelerator multi-core SoCs, we show that the best cache-coherence mode for a given accelerator varies at runtime, depending on the accelerator\u2019s characteristics, the workload size, and the overall SoC status. Cohmeleon applies reinforcement learning to select the best coherence mode for each accelerator dynamically at runtime, as opposed to statically at design time. It makes these selections adaptively, by continuously observing the system and measuring its performance. Cohmeleon is accelerator-agnostic, architecture-independent, and it requires minimal hardware support. Cohmeleon is also transparent to application programmers and has a negligible software overhead. FPGA-based experiments show that our runtime approach offers, on average, a 38% speedup with a 66% reduction of off-chip memory accesses compared to state-of-the-art design-time approaches. Moreover, it can match runtime solutions that are manually tuned for the target architecture.", "venue": "MICRO", "authors": ["Joseph  Zuckerman", "Davide  Giri", "Jihye  Kwon", "Paolo  Mantovani", "Luca P. Carloni"], "year": 2021, "n_citations": 0}
{"id": 5343010, "s2_id": "a0796edf402715d2f0aae5168c1554ab5ebc26f2", "title": "Reducing DRAM Refresh Overheads with Refresh-Access Parallelism", "abstract": "This article summarizes the idea of \"refresh-access parallelism,\" which was published in HPCA 2014, and examines the work's significance and future potential. The overarching objective of our HPCA 2014 paper is to reduce the significant negative performance impact of DRAM refresh with intelligent memory controller mechanisms. \nTo mitigate the negative performance impact of DRAM refresh, our HPCA 2014 paper proposes two complementary mechanisms, DARP (Dynamic Access Refresh Parallelization) and SARP (Subarray Access Refresh Parallelization). The goal is to address the drawbacks of state-of-the-art per-bank refresh mechanism by building more efficient techniques to parallelize refreshes and accesses within DRAM. First, instead of issuing per-bank refreshes in a round-robin order, as it is done today, DARP issues per-bank refreshes to idle banks in an out-of-order manner. Furthermore, DARP proactively schedules refreshes during intervals when a batch of writes are draining to DRAM. Second, SARP exploits the existence of mostly-independent subarrays within a bank. With minor modifications to DRAM organization, it allows a bank to serve memory accesses to an idle subarray while another subarray is being refreshed. Our extensive evaluations on a wide variety of workloads and systems show that our mechanisms improve system performance (and energy efficiency) compared to three state-of-the-art refresh policies, and their performance bene ts increase as DRAM density increases.", "venue": "ArXiv", "authors": ["Kevin K. Chang", "Donghyuk  Lee", "Zeshan  Chishti", "Alaa R. Alameldeen", "Chris  Wilkerson", "Yoongu  Kim", "Onur  Mutlu"], "year": 2018, "n_citations": 1}
{"id": 5344555, "s2_id": "378a8f63c948ca70180f4898b37cf32392a09cdf", "title": "Parallel Interleaver Design for a High throughput HSPA+/LTE Multi-Standard Turbo Decoder", "abstract": "To meet the evolving data rate requirements of emerging wireless communication technologies, many parallel architectures have been proposed to implement high throughput turbo decoders. However, concurrent memory reading/writing in parallel turbo decoding architectures leads to severe memory conflict problem, which has become a major bottleneck for high throughput turbo decoders. In this paper, we propose a flexible and efficient VLSI architecture to solve the memory conflict problem for highly parallel turbo decoders targeting multi-standard 3G/4G wireless communication systems. To demonstrate the effectiveness of the proposed parallel interleaver architecture, we implemented an HSPA +/LTE/LTE-Advanced multi-standard turbo decoder with a 45 nm CMOS technology. The implemented turbo decoder consists of 16 Radix-4 MAP decoder cores, and the chip core area is 2.43 mm 2. When clocked at 600 MHz, this turbo decoder can achieve a maximum decoding throughput of 826 Mbps in the HSPA+ mode and 1.67 Gbps in the LTE/LTE-Advanced mode, exceeding the peak data rate requirements for both standards.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Guohui  Wang", "Hao  Shen", "Yang  Sun", "Joseph R. Cavallaro", "Aida  Vosoughi", "Yuanbin  Guo"], "year": 2014, "n_citations": 39}
{"id": 5345542, "s2_id": "57a4c3c65c4176e617b0162fdafefad9b6a8dbda", "title": "HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks", "abstract": "DNNs are known to be vulnerable to so-called adversarial attacks, in which inputs are carefully manipulated to induce misclassification. Existing defenses are mostly softwarebased and come with high overheads or other limitations. This paper presents HASI, a hardware-accelerated defense that uses a process we call stochastic inference to detect adversarial inputs. HASI carefully injects noise into the model at inference time and used the model\u2019s response to differentiate adversarial inputs from benign ones. We show an adversarial detection rate of average 87% which exceeds the detection rate of the state of the art approaches, with a much lower overhead. We demonstrate a software/hardware-accelerated co-design, which reduces the performance impact of stochastic inference to 1.58\u00d7\u22122\u00d7 relative to the unprotected baseline, compared to 14 \u00d7 \u221220\u00d7 overhead for a software-only GPU implementation.", "venue": "ArXiv", "authors": ["Mohammad Hossein Samavatian", "Saikat  Majumdar", "Kristin  Barber", "Radu  Teodorescu"], "year": 2021, "n_citations": 0}
{"id": 5347598, "s2_id": "65ffad694f9384f06b65691bcad15c9f25645315", "title": "Ethernet Packet Processor for SoC Application", "abstract": "As the demand for Internet expands significantly in numbers of users, servers, IP addresses, switches and routers, the IP based network architecture must evolve and change. The design of domain specific processors that require high performance, low power and high degree of programmability is the bottleneck in many processor based applications. This paper describes the design of ethernet packet processor for system-on-chip (SoC) which performs all core packet processing functions, including segmentation and reassembly, packetization classification, route and queue management which will speedup switching/routing performance. Our design has been configured for use with multiple projects ttargeted to a commercial configurable logic device the system is designed to support 10/100/1000 links with a speed advantage. VHDL has been used to implement and simulated the required functions in FPGA.", "venue": "ArXiv", "authors": ["Raja Jitendra Nayaka", "Rajashekhar C. Biradar"], "year": 2012, "n_citations": 0}
{"id": 5349081, "s2_id": "66df0d6b0a807b99c444a2d2ce4778372207355b", "title": "Understanding system characteristics of online erasure coding on scalable, distributed and large-scale SSD array systems", "abstract": "Large-scale systems with arrays of solid state disks (SSDs) have become increasingly common in many computing segments. To make such systems resilient, we can adopt erasure coding such as Reed-Solomon (RS) code as an alternative to replication because erasure coding can offer a significantly lower storage cost than replication. To understand the impact of using erasure coding on system performance and other system aspects such as CPU utilization and network traffic, we build a storage cluster consisting of approximately one hundred processor cores with more than fifty high-performance SSDs, and evaluate the cluster with a popular open-source distributed parallel file system, Ceph. Then we analyze behaviors of systems adopting erasure coding from the following five viewpoints, compared with those of systems using replication: (1) storage system I/O performance; (2) computing and software overheads; (3) I/O amplification; (4) network traffic among storage nodes; (5) the impact of physical data layout on performance of RS-coded SSD arrays. For all these analyses, we examine two representative RS configurations, which are used by Google and Facebook file systems, and compare them with triple replication that a typical parallel file system employs as a default fault tolerance mechanism. Lastly, we collect 54 block-level traces from the cluster and make them available for other researchers.", "venue": "2017 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Sungjoon  Koh", "Jie  Zhang", "Miryeong  Kwon", "Jungyeon  Yoon", "David  Donofrio", "Nam Sung Kim", "Myoungsoo  Jung"], "year": 2017, "n_citations": 2}
{"id": 5350182, "s2_id": "ca9cb0a12935cec61ad4597098d27a1069c20128", "title": "Building Toffoli Network for Reversible Logic Synthesis Based on Swapping Bit Strings", "abstract": "In this paper, we have implemented and designed a sorting network for reversible logic circuits synthesis in terms of n*n Toffoli gates. The algorithm presented in this paper constructs a Toffoli Network based on swapping bit strings. Reduction rules are then applied by simple template matching and removing useless gates from the network. Random selection of bit strings and reduction of control inputs are used to minimize both the number of gates and gate width. The method produces near optimal results for up to 3-input 3-output circuits.", "venue": "ArXiv", "authors": ["Hafiz Md. Hasan Babu", "Md. Saiful Islam", "Md. Rafiqul Islam", "Lafifa  Jamal", "Abu Ahmed Ferdaus", "Muhammad Rezaul Karim", "Abdullah Al Mahmud"], "year": 2010, "n_citations": 0}
{"id": 5352572, "s2_id": "22fba3619b56a90cb5475fc9ec3303f5cdfac847", "title": "Optimized generation of data-path from C codes for FPGAs", "abstract": "FPGAs, as computing devices, offer significant speedup over microprocessors. Furthermore, their configurability offers an advantage over traditional ASICs. However, they do not yet enjoy high-level language programmability, as microprocessors do. This has become the main obstacle for their wider acceptance by application designers. ROCCC is a compiler designed to generate circuits from C source code to execute on FPGAs, more specifically on CSoCs. It generates RTL level HDLs from frequently executing kernels in an application. In this paper, we describe the ROCCC's system overview and focus on its data path generation. We compare the performance of ROCCC-generated VHDL code with that of Xilinx IPs. The synthesis result shows that the ROCCC-generated circuit takes around 2/spl times//spl sim/3/spl times/ the area and runs at a comparable clock rate.", "venue": "Design, Automation and Test in Europe", "authors": ["Zhi  Guo", "Betul  Buyukkurt", "Walid A. Najjar", "Kees A. Vissers"], "year": 2005, "n_citations": 124}
{"id": 5352918, "s2_id": "627f68b5cb68fb7af91190dbbcaa6a362767cb6e", "title": "MWQ: Multiscale Wavelet Quantized Neural Networks", "abstract": "Model quantization can reduce the model size and computational latency, it has become an essential technique for the deployment of deep neural networks on resourceconstrained hardware (e.g., mobile phones and embedded devices). The existing quantization methods mainly consider the numerical elements of the weights and activation values, ignoring the relationship between elements. The decline of representation ability and information loss usually lead to the performance degradation. Inspired by the characteristics of images in the frequency domain, we propose a novel multiscale wavelet quantization (MWQ) method. This method decomposes original data into multiscale frequency components by wavelet transform, and then quantizes the components of different scales, respectively. It exploits the multiscale frequency and spatial information to alleviate the information loss caused by quantization in the spatial domain. Because of the flexibility of MWQ, we demonstrate three applications (e.g., model compression, quantized network optimization, and information enhancement) on the ImageNet and COCO datasets. Experimental results show that our method has stronger representation ability and can play an effective role in quantized neural networks.", "venue": "ArXiv", "authors": ["Qigong  Sun", "Yan  Ren", "Licheng  Jiao", "Xiufang  Li", "Fanhua  Shang", "Fang  Liu"], "year": 2021, "n_citations": 2}
{"id": 5358675, "s2_id": "34093abe9e32f68f25c7a28b17550318aab46a64", "title": "Fault Detection for RC4 Algorithm and its Implementation on FPGA Platform", "abstract": "In hardware implementation of a cryptographic algorithm, one may achieve leakage of secret information by creating scopes to introduce controlled faulty bit(s) even though the algorithm is mathematically a secured one. The technique is very effective in respect of crypto processors embedded in smart cards. In this paper few fault detecting archi- tectures for RC4 algorithm are designed and implemented on Virtex5(ML505) FPGA. The results indicate that the proposed architectures can handle most of the faults without loss of throughput consuming marginally additional hardware and power.", "venue": "ArXiv", "authors": ["Rourab  Paul", "Amlan  Chakrabarti", "Ranjan  Ghosh"], "year": 2014, "n_citations": 1}
{"id": 5361950, "s2_id": "83fbb0c19511a3126bd2dc348db29c4b0131cf46", "title": "Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator", "abstract": "Due to their growing popularity and computational cost, deep neural networks (DNNs) are being targeted for hardware acceleration. A popular architecture for DNN acceleration, adopted by the Google Tensor Processing Unit (TPU), utilizes a systolic array based matrix multiplication unit at its core. This paper deals with the design of fault-tolerant, systolic array based DNN accelerators for high defect rate technologies. To this end, we empirically show that the classification accuracy of a baseline TPU drops significantly even at extremely low fault rates (as low as 0.006%). We then propose two novel strategies, fault-aware pruning (FAP) and fault-aware pruning+retraining (FAP+T), that enable the TPU to operate at fault rates of up to 50%, with negligible drop in classification accuracy (as low as 0.1%) and no run-time performance overhead. The FAP+T does introduce a one-time retraining penalty per TPU chip before it is deployed, but we propose optimizations that reduce this one-time penalty to under 12 minutes. The penalty is then amortized over the entire lifetime of the TPU's operation.", "venue": "2018 IEEE 36th VLSI Test Symposium (VTS)", "authors": ["Jeff  Zhang", "Tianyu  Gu", "Kanad  Basu", "Siddharth  Garg"], "year": 2018, "n_citations": 48}
{"id": 5364245, "s2_id": "ec62e9f366890face0d14a642e898aa6676dffc2", "title": "Accelerating non-volatile/hybrid processor cache design space exploration for application specific embedded systems", "abstract": "In this article, we propose a technique to accelerate non-volatile/hybrid of volatile and non-volatile processor cache design space exploration for application specific embedded systems. Utilizing a novel cache behavior modeling equation and a new accurate cache miss prediction mechanism, our proposed technique can accelerate NVM/hybrid FIFO processor cache design space exploration for SPEC CPU 2000 applications up to 249 times compared to the conventional approach.", "venue": "The 20th Asia and South Pacific Design Automation Conference", "authors": ["Mohammad Shihabul Haque", "Ang  Li", "Akash  Kumar", "Qingsong  Wei"], "year": 2015, "n_citations": 6}
{"id": 5371064, "s2_id": "b3ab6a6c8d7893ba88b417e74d88fb8b0b87782f", "title": "Exposing Software Defined Radio Functionality To Native Operating System Applications via Virtual Devices", "abstract": "Many reconfigurable platforms require that applications be written specifically to take advantage of the reconfigurable hardware. In a PC-based environment, this presents an undesirable constraint in that the many already available applications cannot leverage on such hardware. Greatest benefit can only be derived from reconfigurable devices if even native OS applications can transparently utilize reconfigurable devices as they would normal full-fledged hardware devices. This paper presents how Proteus Virtual Devices are used to expose reconfigurable hardware in a transparent manner for use by typical native OS applications.", "venue": "ArXiv", "authors": ["Darran  Nathan"], "year": 2004, "n_citations": 0}
{"id": 5372374, "s2_id": "ff6aa772063da75cc6960aaa6bba110141ead0a0", "title": "Multi-Valued Routing Tracks for FPGAs in 28nm FDSOI Technology", "abstract": "In this paper we present quaternary and ternary routing tracks for FPGAs, and their implementation in 28nm FDSOI technology. We discuss the transistor level design of multi-valued repeaters, multiplexers and translators, and specific features of FDSOI technology which make it possible. Next we compare the multi-valued routing architectures with equivalent single driver two-valued routing architectures. We show that for long tracks, it is possible to achieve upto 3x reduction in dynamic switching energy, upto 2x reduction in routing wire area and 10% reduction in area dedicated to routing resources. The multi-valued tracks are slightly more susceptible to process variation. We present a layout method for multivalued standard cells and determine the layout overhead.We conclude with various usage scenarios of these tracks.", "venue": "ArXiv", "authors": ["Sumanta  Chaudhuri", "Tarik  Graba", "Yves  Mathieu"], "year": 2016, "n_citations": 1}
{"id": 5374128, "s2_id": "abc2cfac65c6302b58a6a1e8438f4f5df15399dc", "title": "Themis: A Network Bandwidth-Aware Collective Scheduling Policy for Distributed Training of DL Models", "abstract": "The continuous growth in both size and training data for modern Deep Neural Networks (DNNs) models has led to training tasks taking days or even months. Distributed training is a solution to reduce training time by splitting the task across multiple NPUs (e.g., GPU/TPU). However, distributed training adds communication overhead between the NPUs in order to synchronize the gradients and/or activation, depending on the parallelization strategy. In today\u2019s datacenters, for training at scale, NPUs are connected through multi-dimensional interconnection links with different bandwidth and latency. Hence, keeping all network dimensions busy and maximizing the network BW is a challenging task in such a hybrid network environment, as this work identifies. We propose Themis, a novel collective scheduling scheme that dynamically schedules collectives (divided into chunks) to balance the communication loads across all dimensions, further improving the network BW utilization. Our results show that on average, Themis can improve the network BW utilization of single All-Reduce by 1.88\u00d7 (2.92\u00d7 max), and improve the end-to-end training iteration performance of real workloads such as ResNet-50, GNMT, DLRM, and Transformer1T by 1.49\u00d7 (1.96\u00d7 max), 1.41\u00d7 (1.81\u00d7 max), 1.42\u00d7 (1.80\u00d7 max), and 1.35\u00d7 (1.78\u00d7 max), respectively.", "venue": "ArXiv", "authors": ["Saeed  Rashidi", "William  Won", "Sudarshan  Srinivasan", "Srinivas  Sridharan", "Tushar  Krishna"], "year": 2021, "n_citations": 0}
{"id": 5375367, "s2_id": "84dfc3ef4cbb24cf1a42203b15397d36983c00df", "title": "LISA: Increasing Internal Connectivity in DRAM for Fast Data Movement and Low Latency", "abstract": "This paper summarizes the idea of Low-Cost Interlinked Subarrays (LISA), which was published in HPCA 2016, and examines the work's significance and future potential. Contemporary systems perform bulk data movement movement inefficiently, by transferring data from DRAM to the processor, and then back to DRAM, across a narrow off-chip channel. The use of this narrow channel results in high latency and energy consumption. Prior work proposes to avoid these high costs by exploiting the existing wide internal DRAM bandwidth for bulk data movement, but the limited connectivity of wires within DRAM allows fast data movement within only a single DRAM subarray. Each subarray is only a few megabytes in size, greatly restricting the range over which fast bulk data movement can happen within DRAM. \nOur HPCA 2016 paper proposes a new DRAM substrate, Low-Cost Inter-Linked Subarrays (LISA), whose goal is to enable fast and efficient data movement across a large range of memory at low cost. LISA adds low-cost connections between adjacent subarrays. By using these connections to interconnect the existing internal wires (bitlines) of adjacent subarrays, LISA enables wide-bandwidth data transfer across multiple subarrays with little (only 0.8%) DRAM area overhead. As a DRAM substrate, LISA is versatile, enabling a variety of new applications. We describe and evaluate three such applications in detail: (1) fast inter-subarray bulk data copy, (2) in-DRAM caching using a DRAM architecture whose rows have heterogeneous access latencies, and (3) accelerated bitline precharging by linking multiple precharge units together. Our extensive evaluations show that each of LISA's three applications significantly improves performance and memory energy efficiency on a variety of workloads and system configurations.", "venue": "ArXiv", "authors": ["Kevin K. Chang", "Prashant J. Nair", "Saugata  Ghose", "Donghyuk  Lee", "Moinuddin K. Qureshi", "Onur  Mutlu"], "year": 2018, "n_citations": 1}
{"id": 5377499, "s2_id": "b0f68778910cf10ed06ea29fd5e7eb124f6e6b36", "title": "Exploiting Fine-Grain Ordered Parallelism in Dense Matrix Algorithms", "abstract": "Dense linear algebra kernels are critical for wireless applications, and the oncoming proliferation of 5G only amplifies their importance. Many such matrix algorithms are inductive, and exhibit ample amounts of fine-grain ordered parallelism -- when multiple computations flow with fine-grain producer/consumer dependences, and where the iteration domain is not easily tileable. Synchronization overheads make multi-core parallelism ineffective and the non-tileable iterations make the vector-VLIW approach less effective, especially for the typically modest-sized matrices. Because CPUs and DSPs lose order-of-magnitude performance/hardware utilization, costly and inflexible ASICs are often employed in signal processing pipelines. A programmable accelerator with similar performance/power/area would be highly desirable. We find that fine-grain ordered parallelism can be exploited by supporting: 1. fine-grain stream-based communication/synchronization; 2. inductive data-reuse and memory access patterns; 3. implicit vector-masking for partial vectors; 4. hardware specialization of dataflow criticality. In this work, we propose, REVEL, as a next-generation DSP architecture. It supports the above features in its ISA and microarchitecture, and further uses a novel vector-stream control paradigm to reduce control overheads. Across a suite of linear algebra kernels, REVEL outperforms equally provisioned DSPs by 4.6x-37x in latency and achieves a performance per mm 2 of 8.3x. It is only 2.2x higher power to achieve the same performance as ideal ASICs, at about 55% of the combined area.", "venue": "ArXiv", "authors": ["Jian  Weng", "Vidushi  Dadu", "Tony  Nowatzki"], "year": 2019, "n_citations": 1}
{"id": 5380306, "s2_id": "95dcc04bb3f85209282a203d22714ee673b195c9", "title": "Twin-Load: Building a Scalable Memory System over the Non-Scalable Interface", "abstract": "Commodity memory interfaces have difficulty in scaling memory capacity to meet the needs of modern multicore and big data systems. DRAM device density and maximum device count are constrained by technology, package, and signal in- tegrity issues that limit total memory capacity. Synchronous DRAM protocols require data to be returned within a fixed latency, and thus memory extension methods over commodity DDRx interfaces fail to support scalable topologies. Current extension approaches either use slow PCIe interfaces, or require expensive changes to the memory interface, which limits commercial adoptability. Here we propose twin-load, a lightweight asynchronous memory access mechanism over the synchronous DDRx interface. Twin-load uses two special loads to accomplish one access request to extended memory, the first serves as a prefetch command to the DRAM system, and the second asynchronously gets the required data. Twin-load requires no hardware changes on the processor side and only slight soft- ware modifications. We emulate this system on a prototype to demonstrate the feasibility of our approach. Twin-load has comparable performance to NUMA extended memory and outperforms a page-swapping PCIe-based system by several orders of magnitude. Twin-load thus enables instant capacity increases on commodity platforms, but more importantly, our architecture opens opportunities for the design of novel, efficient, scalable, cost-effective memory subsystems.", "venue": "ArXiv", "authors": ["Zehan  Cui", "Tianyue  Lu", "Haiyang  Pan", "Sally A. McKee", "Mingyu  Chen"], "year": 2015, "n_citations": 0}
{"id": 5383625, "s2_id": "f0704077b5acd416fd6bab9f10e38880cc1868e3", "title": "Automatic Nested Loop Acceleration on FPGAs Using Soft CGRA Overlay", "abstract": "Offloading compute intensive nested loops to execute on FPGA accelerators have been demonstrated by numerous researchers as an effective performance enhancement technique across numerous application domains. To construct such accelerators with high design productivity, researchers have increasingly turned to the use of overlay architectures as an intermediate generation target built on top of off-the-shelf FPGAs. However, achieving the desired performance-overhead trade-off remains a major productivity challenge as complex application-specific customizations over a large design space covering multiple architectural parameters are needed. \nIn this work, an automatic nested loop acceleration framework utilizing a regular soft coarse-grained reconfigurable array (SCGRA) overlay is presented. Given high-level resource constraints, the framework automatically customizes the overlay architectural design parameters, high-level compilation options as well as communication between the accelerator and the host processor for optimized performance specifically to the given application. In our experiments, at a cost of 10 to 20 minutes additional tools run time, the proposed customization process resulted in up to 5 times additional speedup over a baseline accelerator generated by the same framework without customization. Overall, when compared to the equivalent software running on the host ARM processor alone on the Zedboard, the resulting accelerators achieved up to 10 times speedup.", "venue": "ArXiv", "authors": ["Cheng  Liu", "Ho-Cheung  Ng", "Hayden Kwok-Hay So"], "year": 2015, "n_citations": 16}
{"id": 5383872, "s2_id": "3ae7f8b29faecca030b137ce84b7bbe4e75f5ee3", "title": "Classifying Application Phases in Asymmetric Chip Multiprocessors", "abstract": "In present study, in order to improve the performance and reduce the amount of power which is dissipated in heterogeneous multicore processors, the ability of detecting the program execution phases is investigated. The programs execution intervals have been classified in different phases based on their throughput and the utilization of the cores. The results of implementing the phase detection technique are investigated on a single core processor and also on a multicore processor. To minimize the profiling overhead, an algorithm for the dynamic adjustment of the profiling intervals is presented. It is based on the behavior of the program and reduces the profiling overhead more than three fold. The results are obtained from executing multiprocessor benchmarks on a given processor. In order to show the program phases clearly, throughput and utilization of execution intervals are presented on a scatter plot. The results are presented for both fixed and variable intervals.", "venue": "ArXiv", "authors": ["A. Z. Jooya", "M.  Analoui"], "year": 2010, "n_citations": 0}
{"id": 5384370, "s2_id": "664f751d95e2c9005b6b482aac71f8b0a3bceb4b", "title": "Why systems-on-chip needs more UML like a hole in the head", "abstract": "SoC can most certainly make use of UML; SoC just does not need more UML, or even all of it. The advent of model mappings, coupled with marks that indicate which mapping rule to apply, enable a major simplification of the rise of UML in SoC.", "venue": "Design, Automation and Test in Europe", "authors": ["Stephen J. Mellor", "John R. Wolfe", "Campbell  McCausland"], "year": 2005, "n_citations": 10}
{"id": 5385137, "s2_id": "9f840be023309cc957dc741dce85dfc6b1a3b486", "title": "NPE: An FPGA-based Overlay Processor for Natural Language Processing", "abstract": "In recent years, transformer-based models have shown state-of-the-art results for Natural Language Processing (NLP). In particular, the introduction of the BERT language model brought with it breakthroughs in tasks such as question answering and natural language inference, advancing applications that allow humans to interact naturally with embedded devices. FPGA-based overlay processors have been shown as effective solutions for edge image and video processing applications, which mostly rely on low precision linear matrix operations. In contrast, transformer-based NLP techniques employ a variety of higher precision nonlinear operations with significantly higher frequency. We present NPE, an FPGA-based overlay processor that can efficiently execute a variety of NLP models. NPE offers software-like programmability to the end user and, unlike FPGA designs that implement specialized accelerators for each nonlinear function, can be upgraded for future NLP models without requiring reconfiguration. NPE can meet real-time conversational AI latency targets for the BERT language model with 4x lower power than CPUs and 6x lower power than GPUs. We also show NPE uses 3x fewer FPGA resources relative to comparable BERT network-specific accelerators in the literature. NPE provides a cost-effective and power-efficient FPGA-based solution for Natural Language Processing at the edge.", "venue": "FPGA", "authors": ["Hamza  Khan", "Asma  Khan", "Zainab  Khan", "Lun Bin Huang", "Kun  Wang", "Lei  He"], "year": 2021, "n_citations": 2}
{"id": 5387504, "s2_id": "ae98b156c6edfa53a0f162ed0603e97cbf1c3936", "title": "Ternary circuits: why R=3 is not the Optimal Radix for Computation", "abstract": "A demonstration that e=2.718 rounded to 3 is the best radix for computation is disproved. The MOSFET-like CNTFET technology is used to compare inverters, Nand, adders, multipliers, D Flip-Flops and SRAM cells. The transistor count ratio between ternary and binary circuits is generally greater than the log(3)/log(2) information ratio. The only exceptions concern a circuit approach that combines two circuit drawbacks (an additional power supply and a circuit conflict between transistors) and only when it implements circuits based on the ternary inverter. For arithmetic circuits such as adders and multipliers, the ternary circuits are always outperformed by the binary ones using the same technology.", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2019, "n_citations": 4}
{"id": 5391206, "s2_id": "1b810bcc3eda4a3a90488b14a4787ca7769d0421", "title": "Abusing Cache Line Dirty States to Leak Information in Commercial Processors", "abstract": "Caches have been used to construct various types of covert and side channels to leak information. Most of the previous cache channels exploit the timing difference between cache hits and cache misses. However, we introduce a new and broader classification of cache covert channel attacks: Hit+Miss, Hit+Hit, Miss+Miss. We highlight that cache misses (or cache hits) in different states may have more significant time differences, which can be used as timing channels. Based on the classification, We propose a new type of stable and stealthy Miss+Miss cache channel. The write-back caches are widely deployed in modern processors. This paper presents in detail how to use replacement latency difference to construct timing-based channels (calles WB channel) to leak information in the writeback cache: any modification to a cache line by a sender will set the cache line to the dirty state, and the receiver can observe this through measuring the latency to replace this cache set. We also demonstrate how senders could exploit a different number of dirty cache lines in a cache set to improve transmission bandwidth with symbols encoding multiple bits. The peak transmission bandwidths of the WB channels in commercial systems can vary between 1300 to 4400 Kbps per cache set in the hyper-threaded setting without shared memory between the sender and the receiver. Different from most existing cache channels that always target specific memory addresses, the new WB channels focus on the cache set and cache line states, making the channel hard to be disturbed by other processes on the core and can still work in the cache using a random replacement policy. We also analyzed the stealthiness of WB channels from the perspective of the number of cache loads and cache miss rates. Further, This paper discusses and evaluates possible defenses. The paper finishes by discussing various forms of side-channel attacks.", "venue": "ArXiv", "authors": ["Yujie  Cui", "Xu  Cheng"], "year": 2021, "n_citations": 0}
{"id": 5392428, "s2_id": "4426f024e4341cf91f4bc69a3a02ae89337cfd7a", "title": "DSL-based Design Space Exploration for Temporal and Spatial Parallelism of Custom Stream Computing", "abstract": "Stream computation is one of the approaches suitable for FPGA-based custom computing due to its high throughput capability brought by pipelining with regular memory access. To increase performance of iterative stream computation, we can exploit both temporal and spatial parallelism by deepening and duplicating pipelines, respectively. However, the performance is constrained by several factors including available hardware resources on FPGA, an external memory bandwidth, and utilization of pipeline stages, and therefore we need to find the best mix of the different parallelism to achieve the highest performance per power. In this paper, we present a domain-specific language (DSL) based design space exploration for temporally and/or spatially parallel stream computation with FPGA. We define a DSL where we can easily design a hierarchical structure of parallel stream computation with abstract description of computation. For iterative stream computation of fluid dynamics simulation, we design hardware structures with a different mix of the temporal and spatial parallelism. By measuring the performance and the power consumption, we find the best among them.", "venue": "ArXiv", "authors": ["Kentaro  Sano"], "year": 2015, "n_citations": 11}
{"id": 5392557, "s2_id": "d5f79a30b3975e447cede805cb24d9879d6c140a", "title": "Systolic Tensor Array: An Efficient Structured-Sparse GEMM Accelerator for Mobile CNN Inference", "abstract": "Convolutional neural network (CNN) inference on mobile devices demands efficient hardware acceleration of low-precision (INT8) general matrix multiplication (GEMM). The systolic array (SA) is a pipelined 2D array of processing elements (PEs), with very efficient local data movement, well suited to accelerating GEMM, and widely deployed in industry. In this letter, we describe two significant improvements to the traditional SA architecture, to specifically optimize for CNN inference. First, we generalize the traditional scalar PE, into a Tensor-PE, which gives rise to a family of new Systolic Tensor Array (STA) microarchitectures. The STA family increases intra-PE operand reuse and datapath efficiency, resulting in circuit area and power dissipation reduction of as much as 2.08\u00d7 and 1.36\u00d7 respectively, compared to the conventional SA at iso-throughput with INT8 operands. Second, we extend this design to support a novel block-sparse data format called density-bound block (DBB). This variant (STA-DBB) achieves a 3.14\u00d7 and 1.97\u00d7 improvement over the SA baseline at iso-throughput in area and power respectively, when processing specially-trained DBB-sparse models, while remaining fully backwards compatible with dense models.", "venue": "IEEE Computer Architecture Letters", "authors": ["Zhi-Gang  Liu", "Paul N. Whatmough", "Matthew  Mattina"], "year": 2020, "n_citations": 20}
{"id": 5392690, "s2_id": "90900d041383340f7777675fa85b63cc11d3016e", "title": "A Survey on Tiering and Caching in High-Performance Storage Systems", "abstract": "Although every individual invented storage technology made a big step towards perfection, none of them is spotless. Different data store essentials such as performance, availability, and recovery requirements have not met together in a single economically affordable medium, yet. One of the most influential factors is price. So, there has always been a trade-off between having a desired set of storage choices and the costs. To address this issue, a network of various types of storing media is used to deliver the high performance of expensive devices such as solid state drives and non-volatile memories, along with the high capacity of inexpensive ones like hard disk drives. In software, caching and tiering are long-established concepts for handling file operations and moving data automatically within such a storage network and manage data backup in low-cost media. Intelligently moving data around different devices based on the needs is the key insight for this matter. In this survey, we discuss some recent pieces of research that have been done to improve high-performance storage systems with caching and tiering techniques.", "venue": "ArXiv", "authors": ["Morteza  Hoseinzadeh"], "year": 2019, "n_citations": 8}
{"id": 5394769, "s2_id": "433ff377a029ac72d9c503b46753baf7d909b84b", "title": "An Open-source Library of Large Integer Polynomial Multipliers", "abstract": "Polynomial multiplication is a bottleneck in most of the public-key cryptography protocols, including Elliptic-curve cryptography and several of the post-quantum cryptography algorithms presently being studied. In this paper, we present a library of various large integer polynomial multipliers to be used in hardware cryptocores. Our library contains both digitized and non-digitized multiplier flavours for circuit designers to choose from. The library is supported by a C++ generator that automatically produces the multipliers\u2019 logic in Verilog HDL that is amenable for FPGA and ASIC designs. Moreover, for ASICs, it also generates configurable and parameterizable synthesis scripts. The features of the generator allow for a quick generation and assessment of several architectures at the same time, thus allowing a designer to easily explore the (complex) optimization search space of polynomial multiplication.", "venue": "2021 24th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)", "authors": ["Malik  Imran", "Zain Ul Abideen", "Samuel  Pagliarini"], "year": 2021, "n_citations": 1}
{"id": 5406447, "s2_id": "bf9e5879c75c980415ff2122d005c2a293971eb2", "title": "An Efficient Framework for Floor-plan Prediction of Dynamic Runtime Reconfigurable Systems", "abstract": "Several embedded application domains for reconfigurable systems tend to combine frequent changes with high performance demands of their workloads such as image processing, wearable computing and network processors.\u00a0 Time multiplexing of reconfigurable hardware resources raises a number of new issues, ranging from run-time systems to complex programming models that usually form a Reconfigurable hardware Operating System (ROS).\u00a0 The Operating System performs online task scheduling and handles resource management. There are many challenges in adaptive computing and dynamic reconfigurable systems. One of the major understudied challenges is estimating the required resources in terms of soft cores, Programmable Reconfigurable Regions (PRRs), the appropriate communication infrastructure, and to predict a near optimal layout and floor-plan of the reconfigurable logic fabric. Some of these issues are specific to the application being designed, while others are more general and relate to the underlying run-time environment. Static resource allocation for Run-Time Reconfiguration (RTR) often leads to inferior and unacceptable results. In this paper, we present a novel adaptive and dynamic methodology, based on a Machine Learning approach, for predicting and estimating the necessary resources for an application based on past historical information. An important feature of the proposed methodology is that the system is able to learn and generalize and, therefore, is expected to improve its accuracy over time.\u00a0 The goal of the entire process is to extract useful hidden knowledge from the data. This knowledge is the prediction and estimation of the necessary resources for an unknown or not previously seen application. Full Text: PDF", "venue": "ArXiv", "authors": ["Ahmed  Al-Wattar", "Shawki  Areibi", "Gary William Grewal"], "year": 2016, "n_citations": 0}
{"id": 5412137, "s2_id": "f5dad676ec540638aca19a32c777e58d0558151b", "title": "An efficient FPGA implementation of MRI image filtering and tumor characterization using Xilinx system generator", "abstract": "This paper presents an efficient architecture for various image filtering algorithms and tumor characterization using Xilinx System Generator (XSG). This architecture offers an alternative through a graphical user interface that combines MATLAB, Simulink and XSG and explores important aspects concerned to hardware implementation. Performance of this architecture implemented in SPARTAN-3E Starter kit (XC3S500E-FG320) exceeds those of similar or greater resources architectures. The proposed architecture reduces the resources available on target device by 50%.", "venue": "VLSIC 2011", "authors": ["S. Allin Christe", "M.  Vignesh", "A.  Kandaswamy"], "year": 2011, "n_citations": 49}
{"id": 5416850, "s2_id": "2cd94cd32979952e76711a8ed3a43d7f0c7a532a", "title": "Isolation-Aware Timing Analysis and Design Space Exploration for Predictable and Composable Many-Core Systems", "abstract": "Composable many-core systems enable the independent development and analysis of applications which will be executed on a shared platform where the mix of concurrently executed applications may change dynamically at run time. For each individual application, an off-line Design Space Exploration (DSE) is performed to compute several mapping alternatives on the platform, offering Pareto-optimal trade-offs in terms of real-time guarantees, resource usage, etc. At run time, one mapping is then chosen to launch the application on demand. In this context, to enable an independent analysis of each individual application at design time, so-called inter-application isolation schemes are applied which specify temporal or spatial isolation policies between applications. S.o.t.a. composable many-core systems are developed based on a fixed isolation scheme that is exclusively applied to every resource in every mapping of every application and use a timing analysis tailored to that isolation scheme to derive timing guarantees for each mapping. A fixed isolation scheme, however, heavily restricts the explored space of solutions and can, therefore, lead to suboptimality. Lifting this restriction necessitates a timing analysis that is applicable to mappings with an arbitrary mix of isolation schemes on different resources. To address this issue, we present an isolation-aware timing analysis that unlike existing analyses can handle multiple isolation schemes in combination within one mapping and delivers safe yet tight timing bounds by identifying and excluding interference scenarios that can never happen under the given combination of isolation schemes. Based on the timing analysis, we present a DSE which explores the choices of isolation scheme per resource within each mapping. Experimental results demonstrate the advantage of the proposed approach over approaches based on a fixed isolation scheme.", "venue": "ECRTS", "authors": ["Behnaz  Pourmohseni", "Fedor  Smirnov", "Stefan  Wildermann", "Jurgen  Teich"], "year": 2019, "n_citations": 11}
{"id": 5423589, "s2_id": "13e0e93c10c5b2b8d617f5772c61aa3d18d5b1f2", "title": "2-in-1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency", "abstract": "The recent breakthroughs of deep neural networks (DNNs) and the advent of billions of Internet of Things (IoT) devices have excited an explosive demand for intelligent IoT devices equipped with domain-specific DNN accelerators. However, the deployment of DNN accelerator enabled intelligent functionality into real-world IoT devices still remains particularly challenging. First, powerful DNNs often come at prohibitive complexities, whereas IoT devices often suffer from stringent resource constraints. Second, while DNNs are vulnerable to adversarial attacks especially on IoT devices exposed to complex real-world environments, many IoT applications require strict security. Existing DNN accelerators mostly tackle only one of the two aforementioned challenges (i.e., efficiency or adversarial robustness) while neglecting or even sacrificing the other. To this end, we propose a 2-in-1 Accelerator, an integrated algorithm-accelerator co-design framework aiming at winning both the adversarial robustness and efficiency of DNN accelerators. Specifically, we first propose a Random Precision Switch (RPS) algorithm that can effectively defend DNNs against adversarial attacks by enabling random DNN quantization as an in-situ model switch during training and inference. Furthermore, we propose a new precision-scalable accelerator featuring (1) a new precision-scalable MAC unit architecture which spatially tiles the temporal MAC units to boost both the achievable efficiency and flexibility and (2) a systematically optimized dataflow that is searched by our generic accelerator optimizer. Extensive experiments and ablation studies validate that our 2-in-1 Accelerator can not only aggressively boost both the adversarial robustness and efficiency of DNN accelerators under various attacks, but also naturally support instantaneous robustness-efficiency trade-offs adapting to varied resources without the necessity of DNN retraining. We believe our 2-in-1 Accelerator has opened up an exciting perspective for robust and efficient accelerator design.", "venue": "MICRO", "authors": ["Yonggan  Fu", "Yang  Zhao", "Qixuan  Yu", "Chaojian  Li", "Yingyan  Lin"], "year": 2021, "n_citations": 1}
{"id": 5424663, "s2_id": "27412aa75a67b258918c96f9d510ff035694b263", "title": "SqueezeJet: High-Level Synthesis Accelerator Design for Deep Convolutional Neural Networks", "abstract": "Deep convolutional neural networks have dominated the pattern recognition scene by providing much more accurate solutions in computer vision problems such as object recognition and object detection. Most of these solutions come at a huge computational cost, requiring billions of multiply-accumulate operations and, thus, making their use quite challenging in real-time applications that run on embedded mobile (resource-power constrained) hardware. This work presents the architecture, the high-level synthesis design, and the implementation of SqueezeJet, an FPGA accelerator for the inference phase of the SqueezeNet DCNN architecture, which is designed specifically for use in embedded systems. Results show that SqueezeJet can achieve 15.16 times speed-up compared to the software implementation of SqueezeNet running on an embedded mobile processor with less than 1% drop in top-5 accuracy.", "venue": "ARC", "authors": ["Panagiotis G. Mousouliotis", "Loukas P. Petrou"], "year": 2018, "n_citations": 9}
{"id": 5437964, "s2_id": "0ced1b8877cbbaa3dee9f18f0b1dfae6a615d1a0", "title": "DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks", "abstract": "Data movement between the CPU and main memory is a first-order obstacle against improv ing performance, scalability, and energy efficiency in modern systems. Computer systems employ a range of techniques to reduce overheads tied to data movement, spanning from traditional mechanisms (e.g., deep multi-level cache hierarch ies, aggressive hardware prefetcher s) to emerging techniques such as Near-Data Processing (NDP), where some computation is moved close to memory. Prior NDP works investigate the root causes of data movement bottlenecks using different profiling methodologies and tools. However, there is still a lack of understanding about the key metrics that can identify different data movement bottlenecks and their relation to traditional and emerging data movement mitigation mechanisms. Our goal is to methodically identify potential sources of data movement over a broad set of applications and to comprehensively compare traditional compute-centric data movement mitigation techniques (e.g., cach ing and prefetch ing) to more memory-centric techniques (e.g., NDP), thereby developing a rigorous understanding of the best techniques to mitigate each source of data movement. With this goal in mind, we perform the first large-scale characterization of a wide variety of applications, across a wide range of application domains, to identify fundamental program properties that lead to data movement to/from main memory. We develop the first systematic methodology to classify applications based on the sources contributing to data movement bottlenecks. From our large-scale characterization of 77K functions across 345 applications, we select 144 functions to form the first open-source benchmark suite (DAMOV) for main memory data movement studies. We select a diverse range of functions that (1) represent different types of data movement bottlenecks, and (2) come from a wide range of application domains. Using NDP as a case study, we identify new insights about the different data movement bottlenecks and use these insights to determine the most suitable data movement mitigation mechanism for a particular application. We open-source DAMOV and the complete source code for our new characterization methodology at https://github.com/CMU-SAFARI/DAMOV.", "venue": "IEEE Access", "authors": ["Geraldo F. Oliveira", "Juan  G\u00f3mez-Luna", "Lois  Orosa", "Saugata  Ghose", "Nandita  Vijaykumar", "Ivan  Fernandez", "Mohammad  Sadrosadati", "Onur  Mutlu"], "year": 2021, "n_citations": 10}
{"id": 5439772, "s2_id": "a3fa729175f1fc336f6cd3c0dc96607822ff047e", "title": "MIMS: Towards a Message Interface Based Memory System", "abstract": "The decades-old synchronous memory bus interface has restricted many innovations in the memory system, which is facing various challenges (or walls) in the era of multi-core and big data. In this paper, we argue that a message based interface should be adopted to replace the traditional bus-based interface in the memory system. A novel message interface based memory system called MIMS is proposed. The key innovation of MIMS is that processors communicate with the memory system through a universal and flexible message packet interface. Each message packet is allowed to encapsulate multiple memory requests (or commands) and additional semantic information. The memory system is more intelligent and active by equipping with a local buffer scheduler, which is responsible for processing packets, scheduling memory requests, preparing responses, and executing specific commands with the help of semantic information. Under the MIMS framework, many previous innovations on memory architecture as well as new optimization opportunities such as address compression and continuous requests combination can be naturally incorporated. The experimental results on a 16-core cycle-detailed simulation system show that: with accurate granularity message, MIMS can improve system performance by 53.21% and reduce energy delay product (EDP) by 55.90%. Furthermore, it can improve effective bandwidth utilization by 62.42% and reduce memory access latency by 51% on average.", "venue": "Journal of Computer Science and Technology", "authors": ["Licheng  Chen", "Mingyu  Chen", "Yuan  Ruan", "Yongbing  Huang", "Zehan  Cui", "Tianyue  Lu", "Yungang  Bao"], "year": 2014, "n_citations": 10}
{"id": 5440321, "s2_id": "ea600ea44a52b68520d07856c6b69843d0c456f4", "title": "A Deep Learning Inference Scheme Based on Pipelined Matrix Multiplication Acceleration Design and Non-uniform Quantization", "abstract": "Matrix multiplication is the bedrock in Deep Learning inference application. When it comes to hardware acceleration on edge computing devices, matrix multiplication often takes up a great majority of the time. To achieve better performance in edge computing, we introduce a low-power Multi-layer Perceptron (MLP) accelerator based on a pipelined matrix multiplication scheme and a nonuniform quantization methodology. The implementation is running on Field-programmable Gate Array (FPGA) devices and tested its performance on handwritten digit classification and Q-learning tasks. Results show that our method can achieve better performance with fewer power consumption.", "venue": "ArXiv", "authors": ["Yuyang  Zhang", "Dik Hin Leung", "Min  Guo", "Yijia  Xiao", "Haoyue  Liu", "Yunfei  Li", "Jiyuan  Zhang", "Guan  Wang", "Zhen  Chen"], "year": 2021, "n_citations": 0}
{"id": 5440705, "s2_id": "e75c4b676b53eeb8ae97e535a3f6cf3fd7439a4e", "title": "CARGO : Context Augmented Critical Region Offload for Network-bound datacenter Workloads", "abstract": "Network bound applications, like a database server executing OLTP queries or a caching server storing objects for a dynamic web applications, are essential services that consumers and businesses use daily. These services run on a large datacenters and are required to meet predefined Service Level Objectives (SLO), or latency targets, with high probability. Thus, efficient datacenter applications should optimize their execution in terms of power and performance. However, to support large scale data storage, these workloads make heavy use of pointer connected data structures (e.g., hash table, large fan-out tree, trie) and exhibit poor instruction and memory level parallelism. Our experiments show that due to long memory access latency, these workloads occupy processor resources (e.g., ROB entries, RS buffers, LS queue entries etc.) for a prolonged period of time that delay the processing of subsequent requests. Delayed execution not only increases request processing latency, but also severely effects an application throughput and power-efficiency. To overcome this limitation, we present CARGO, a novel mechanism to overlap queuing latency and request processing by executing select instructions on an application critical path at the network interface card (NIC) while requests wait for processor resources to become available. Our mechanism dynamically identifies the critical instructions and includes the register state needed to compute the long latency memory accesses. This context-augmented critical region is often executed at the NIC well before execution begins at the core, effectively prefetching the data ahead of time. Across a variety of interactive datacenter applications, our proposal improves latency, throughput, and power efficiency by 2.7X, 2.7X, and 1.5X, respectively, while incurring a modest amount storage overhead.", "venue": "ArXiv", "authors": ["Siddharth  Rai", "Trevor E. Carlson"], "year": 2020, "n_citations": 0}
{"id": 5440797, "s2_id": "209c0b9b3410c6b3ba1f371e7b938a339466bc0d", "title": "A Data-Center FPGA Acceleration Platform for Convolutional Neural Networks", "abstract": "Intensive computation is entering data centers with multiple workloads of deep learning. To balance the compute efficiency, performance, and total cost of ownership (TCO), the use of a field-programmable gate array (FPGA) with reconfigurable logic provides an acceptable acceleration capacity and is compatible with diverse computation-sensitive tasks in the cloud. In this paper, we develop an FPGA acceleration platform that leverages a unified framework architecture for general-purpose convolutional neural network (CNN) inference acceleration at a data center. To overcome the computation bound, 4,096 DSPs are assembled and shaped as supertile units (SUs) for different types of convolution, which provide up to 4.2 TOP/s 16-bit fixed-point performance at 500 MHz. The interleaved-task-dispatching method is proposed to map the computation across the SUs, and the memory bound is solved by a dispatching-assembling buffering model and broadcast caches. For various non-convolution operators, a filter processing unit is designed for general-purpose filter-like/pointwise operators. In the experiment, the performances of CNN models running on server-class CPUs, a GPU, and an FPGA are compared. The results show that our design achieves the best FPGA peak performance and a throughput at the same level as that of the state-of-the-art GPU in data centers, with more than 50 times lower latency.", "venue": "2019 29th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Xiaoyu  Yu", "Yuwei  Wang", "Jie  Miao", "Ephrem  Wu", "Heng  Zhang", "Yu  Meng", "Bo  Zhang", "Biao  Min", "Dewei  Chen", "Jianlin  Gao"], "year": 2019, "n_citations": 11}
{"id": 5441895, "s2_id": "620719e46c1ce7d9c65384c4b0580e8d704f8938", "title": "Hardware-Efficient Schemes of Quaternion Multiplying Units for 2D Discrete Quaternion Fourier Transform Processors", "abstract": "In this paper, we offer and discuss three efficient structural solutions for the hardware-oriented implementation of discrete quaternion Fourier transform basic operations with reduced implementation complexities. The first solution: a scheme for calculating sq product, the second solution: a scheme for calculating qt product, and the third solution: a scheme for calculating sqt product, where s is a so-called i-quaternion, t is an j-quaternion, and q is an usual quaternion. The direct multiplication of two usual quaternions requires 16 real multiplications (or two-operand multipliers in the case of fully parallel hardware implementation) and 12 real additions (or binary adders). At the same time, our solutions allow to design the computation units, which consume only 6 multipliers plus 6 two input adders for implementation of sq or qt basic operations and 9 binary multipliers plus 6 two-input adders and 4 four-input adders for implementation of sqt basic operation.", "venue": "ArXiv", "authors": ["Aleksandr  Cariow", "Galina  Cariowa", "Marina  Chicheva"], "year": 2017, "n_citations": 1}
{"id": 5445419, "s2_id": "fc8beac929b44781435acad2a4dc13e79de0d6dc", "title": "BSC: Block-based Stochastic Computing to Enable Accurate and Efficient TinyML", "abstract": "Along with the progress of AI democratization, machine learning (ML) has been successfully applied to edge applications, such as smart phones and automated driving. Nowadays, more applications require ML on tiny devices with extremely limited resources, like implantable cardioverter defibrillator (ICD), which is known as TinyML. Unlike ML on the edge, TinyML with a limited energy supply has higher demands on low-power execution. Stochastic computing (SC) using bitstreams for data representation is promising for TinyML since it can perform the fundamental ML operations using simple logical gates, instead of the complicated binary adder and multiplier. However, SC commonly suffers from low accuracy for ML tasks due to low data precision and inaccuracy of arithmetic units. Increasing the length of the bitstream in the existing works can mitigate the precision issue but incur higher latency. In this work, we propose a novel SC architecture, namely Block-based Stochastic Computing (BSC). BSC divides inputs into blocks, such that the latency can be reduced by exploiting high data parallelism. Moreover, optimized arithmetic units and output revision (OUR) scheme are proposed to improve accuracy. On top of it, a global optimization approach is devised to determine the number of blocks, which can make a better latency-power trade-off. Experimental results show that BSC can outperform the existing designs in achieving over 10% higher accuracy on ML tasks and over 6\u00d7 power reduction.", "venue": "ArXiv", "authors": ["Yuhong  Song", "Edwin Hsing-Mean Sha", "Qingfeng  Zhuge", "Rui  Xu", "Yongzhuo  Zhang", "Bingzhe  Li", "Lei  Yang"], "year": 2021, "n_citations": 0}
{"id": 5450240, "s2_id": "5b9271c27acab2a9dcc9feada884f3df380a9286", "title": "Open Cores for Digital Signal Processing", "abstract": "This paper presents the design and implementation of three System on Chip (SoC) cores, which implement the Digital Signal Processing (DSP) functions: Finite Impulse Response (FIR) filter, Infinite Impulse Response (IIR) filter and Fast Fourier Transform (FFT). The FIR filter core is based on the symmetrical realization form, the IIR filter core is based on the Second Order Sections (SOS) architecture and the FFT core is based on the Radix $2^2$ Single Delay Feedback (R$2^2$SDF) architecture. The three cores are compatible with the Wishbone SoC bus and they were described using generic and structural VHDL. In system hardware verification was performed by using an OpenRisc-based SoC synthesized on an Altera FPGA, the tests showed that the designed DSP cores are suitable for building SoC based on the OpenRisc processor and the Wishbone bus.", "venue": "ArXiv", "authors": ["Juan Camilo Valderrama-Cuervo", "Alexander  L\u00f3pez-Parrado"], "year": 2014, "n_citations": 0}
{"id": 5452908, "s2_id": "df63fdb20669a8b7d3ba7dafebf75b9a94d7ba46", "title": "Exploiting the DRAM Microarchitecture to Increase Memory-Level Parallelism", "abstract": "This paper summarizes the idea of Subarray-Level Parallelism (SALP) in DRAM, which was published in ISCA 2012, and examines the work's significance and future potential. Modern DRAMs have multiple banks to serve multiple memory requests in parallel. However, when two requests go to the same bank, they have to be served serially, exacerbating the high latency of on-chip memory. Adding more banks to the system to mitigate this problem incurs high system cost. Our goal in this work is to achieve the benefits of increasing the number of banks with a low-cost approach. To this end, we propose three new mechanisms, SALP-1, SALP-2, and MASA (Multitude of Activated Subarrays), to reduce the serialization of different requests that go to the same bank. The key observation exploited by our mechanisms is that a modern DRAM bank is implemented as a collection of subarrays that operate largely independently while sharing few global peripheral structures. \nOur three proposed mechanisms mitigate the negative impact of bank serialization by overlapping different components of the bank access latencies of multiple requests that go to different subarrays within the same bank. SALP-1 requires no changes to the existing DRAM structure, and needs to only reinterpret some of the existing DRAM timing parameters. SALP-2 and MASA require only modest changes (< 0.15% area overhead) to the DRAM peripheral structures, which are much less design constrained than the DRAM core. Our evaluations show that SALP-1, SALP-2 and MASA significantly improve performance for both single-core systems (7%/13%/17%) and multi-core systems (15%/16%/20%), averaged across a wide range of workloads. We also demonstrate that our mechanisms can be combined with application-aware memory request scheduling in multicore systems to further improve performance and fairness.", "venue": "ArXiv", "authors": ["Yoongu  Kim", "Vivek  Seshadri", "Donghyuk  Lee", "Jamie  Liu", "Onur  Mutlu"], "year": 2018, "n_citations": 3}
{"id": 5455000, "s2_id": "929300c0bafec19922b2c7623630bf53f9167492", "title": "CoMeT: An Integrated Interval Thermal Simulation Toolchain for 2D, 2.5D, and 3D Processor-Memory Systems", "abstract": "LOKESH SIDDHU, Department of CSE, Indian Institute of Technology Delhi, India RAJESH KEDIA, Khosla School of IT, Indian Institute of Technology Delhi, India SHAILJA PANDEY, Department of CSE, Indian Institute of Technology Delhi, India MARTIN RAPP, Chair for Embedded System (CES), Karlsruhe Institute of Technology (KIT), Germany ANUJ PATHANIA, Informatics Departments, University of Amsterdam, Netherlands J\u00d6RG HENKEL, Chair for Embedded System (CES), Karlsruhe Institute of Technology (KIT), Germany PREETI RANJAN PANDA, Department of CSE, Indian Institute of Technology Delhi, India", "venue": "ArXiv", "authors": ["Lokesh  Siddhu", "Rajesh  Kedia", "Shailja  Pandey", "Martin  Rapp", "Anuj  Pathania", "Jorg  Henkel", "Preeti Ranjan Panda"], "year": 2021, "n_citations": 0}
{"id": 5455167, "s2_id": "e68c261d54a247ddd17b964b3c997c01f28342f5", "title": "CARLA: A Convolution Accelerator With a Reconfigurable and Low-Energy Architecture", "abstract": "Convolutional Neural Networks (CNNs) have proven to be extremely accurate for image recognition, even outperforming human recognition capability. When deployed on battery\u2013powered mobile devices, efficient computer architectures are required to enable fast and energy-efficient computation of costly convolution operations. Despite recent advances in hardware accelerator design for CNNs, two major problems have not yet been addressed effectively, particularly when the convolution layers have highly diverse structures: (1) minimizing energy-hungry off-chip DRAM data movements; (2) maximizing the utilization factor of processing resources to perform convolutions. This work thus proposes an energy-efficient architecture equipped with several optimized dataflows to support the structural diversity of modern CNNs. The proposed approach is evaluated on convolutional layers of VGGNet-16 and ResNet-50. Results show that the architecture achieves a Processing Element (PE) utilization factor of 98% for the majority of $3\\times 3$ and $1\\times 1$ convolutional layers, while limiting latency to 396.9 ms and 92.7 ms when performing convolutional layers of VGGNet-16 and ResNet-50, respectively. In addition, the proposed architecture benefits from the structured sparsity in ResNet-50 to reduce the latency to 42.5 ms when half of the channels are pruned.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Mehdi  Ahmadi", "Shervin  Vakili", "J. M. Pierre Langlois"], "year": 2021, "n_citations": 2}
{"id": 5461051, "s2_id": "a0662311018081ac0af1e43d3870190deec9de4c", "title": "ShiftsReduce: Minimizing Shifts in Racetrack Memory 4.0", "abstract": "Racetrack memories (RMs) have significantly evolved since their conception in 2008, making them a serious contender in the field of emerging memory technologies. Despite key technological advancements, the access latency and energy consumption of an RM-based system are still highly influenced by the number of shift operations. These operations are required to move bits to the right positions in the racetracks. This paper presents data placement techniques for RMs that maximize the likelihood that consecutive references access nearby memory locations at runtime thereby minimizing the number of shifts. We present an integer linear programming (ILP) formulation for optimal data placement in RMs, and revisit existing offset assignment heuristics, originally proposed for random-access memories. We introduce a novel heuristic tailored to a realistic RM and combine it with a genetic search to further improve the solution. We show a reduction in the number of shifts of up to 52.5%, outperforming the state of the art by up to 16.1%.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Asif Ali Khan", "Fazal  Hameed", "Robin  Blaesing", "Stuart  Parkin", "Jer\u00f3nimo  Castrill\u00f3n"], "year": 2020, "n_citations": 16}
{"id": 5465984, "s2_id": "afca81836c4cac4bcfbb62489145ca30c9a34fe8", "title": "A low-overhead soft\u2013hard fault-tolerant architecture, design and management scheme for reliable high-performance many-core 3D-NoC systems", "abstract": "The Network-on-Chip (NoC) paradigm has been proposed as a favorable solution to handle the strict communication requirements between the increasingly large number of cores on a single chip. However, NoC systems are exposed to the aggressive scaling down of transistors, low operating voltages, and high integration and power densities, making them vulnerable to permanent (hard) faults and transient (soft) errors. A hard fault in a NoC can lead to external blocking, causing congestion across the whole network. A soft error is more challenging because of its silent data corruption, which leads to a large area of erroneous data due to error propagation, packet re-transmission, and deadlock. In this paper, we present the architecture and design of a comprehensive soft error and hard fault-tolerant 3D-NoC system, named 3D-Hard-Fault-Soft-Error-Tolerant-OASIS-NoC (3D-FETO). With the aid of efficient mechanisms and algorithms, 3D-FETO is capable of detecting and recovering from soft errors which occur in the routing pipeline stages and leverages reconfigurable components to handle permanent faults in links, input buffers, and crossbars. In-depth evaluation results show that the 3D-FETO system is able to work around different kinds of hard faults and soft errors, ensuring graceful performance degradation, while minimizing additional hardware complexity and remaining power efficient.", "venue": "The Journal of Supercomputing", "authors": ["Khanh N. Dang", "Michael Conrad Meyer", "Yuichi  Okuyama", "Ben A. Abderazek"], "year": 2016, "n_citations": 13}
{"id": 5469461, "s2_id": "128e8e6736f658f890460b267b5596403d873b0f", "title": "TS Cache: A Fast Cache With Timing-Speculation Mechanism Under Low Supply Voltages", "abstract": "To mitigate the ever-worsening \u201cpower wall\u201d problem, more and more applications need to expand their working voltage to the wide-voltage range including the near-threshold region. However, the read delay distribution of the static random access memory (SRAM) cells under the near-threshold voltage shows a more serious long-tail characteristic than that under the nominal voltage due to the process fluctuation. Such degradation of SRAM delay makes the SRAM-based cache a performance bottleneck of systems as well. To avoid unreliable data reading, circuit-level studies use larger/more transistors in a bitcell by sacrificing chip area and the static power of cache arrays. Architectural studies propose the auxiliary error correction or block disabling/remapping methods in fault-tolerant caches, which worsen both the hit latency and energy efficiency due to the complex accessing logic. This article proposes a timing-speculation (TS) cache to boost the cache frequency and improve energy efficiency under low supply voltages. In the TS cache, the voltage differences of bitlines (BLs) are continuously evaluated twice by a sense amplifier (SA), and the access timing error can be detected much earlier than that in prior methods. According to the measurement results from the fabricated chips, the TS L1 cache aggressively increases its frequency to $1.62\\times $ and $1.92\\times $ compared with the conventional scheme at 0.5- and 0.6-V supply voltages, respectively.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Shan  Shen", "Tianxiang  Shao", "Xiaojing  Shang", "Yichen  Guo", "Ming  Ling", "Jun  Yang", "Longxing  Shi"], "year": 2020, "n_citations": 5}
{"id": 5470163, "s2_id": "e653e3112199bc7e7e0aabf84f310c03d3d9b7d0", "title": "A Graph Deep Learning Framework for High-Level Synthesis Design Space Exploration", "abstract": "The design of efficient hardware accelerators for high-throughput data-processing applications, e.g., deep neural networks, is a challenging task in computer architecture design. In this regard, High-Level Synthesis (HLS) emerges as a solution for fast prototyping application-specific hardware starting from a C/C++ behavioural description of the application computational flow. In the accelerator synthesis phase, designers apply HLS directives to optimize the hardware implementation, by trading-off cost and performance. This Design-Space Exploration (DSE) aims at identifying Pareto optimal synthesis configurations whose exhaustive search is often unfeasible due to the design-space dimensionality and the prohibitive computational cost of the synthesis process. Within this framework, we effectively and efficiently address the design problem by proposing, for the first time in the literature, graph neural networks that jointly predict acceleration performance and hardware costs of a synthesized behavioral specification given optimization directives. The learned model can be used to rapidly approach the Pareto curve by guiding the DSE, taking into account performance and cost estimates. The proposed method outperforms traditional HLS-driven DSE approaches, by accounting for arbitrary length of computer programs and the invariant properties of the input. We propose a novel hybrid control and data flow graph representation that enables training the graph neural network on specifications of different hardware accelerators; the methodology naturally transfers to unseen data-processing applications too. Moreover, we show that our approach achieves prediction accuracy comparable with that of commonly used simulators without having access to analytical models of the HLS compiler and the target FPGA, while being orders of magnitude faster. Finally, the learned representation can be exploited for DSE in unexplored configuration spaces by fine-tuning on a small number of samples from the new target domain. The outcome of the empirical evaluation of this transfer learning shows strong results against state-of-the-art baselines in relevant benchmarks including neural processing.", "venue": "ArXiv", "authors": ["Lorenzo  Ferretti", "Andrea  Cini", "Georgios  Zacharopoulos", "Cesare  Alippi", "Laura  Pozzi"], "year": 2021, "n_citations": 0}
{"id": 5471553, "s2_id": "0b2361895f53f6c36d83c48bd717cf05793b5692", "title": "Statistical modeling of pipeline delay and design of pipeline under process variation to enhance yield in sub-100nm technologies", "abstract": "Operating frequency of a pipelined circuit is determined by the of the slowest pipeline stage. However, under statistical delay variation in sub-100 nm technology regime, the slowest stage is not readily identifiable and the estimation of the pipeline yield with respect to a target delay is a challenging problem. We have proposed analytical models to estimate yield for a pipelined design based on delay distributions of individual pipe stages. Using the proposed models, we have shown that change in logic depth and imbalance between the stage delays can improve the yield of a pipeline. A statistical methodology has been developed to optimally design a pipeline circuit for enhancing yield. Optimization results show that, proper imbalance among the stage delays in a pipeline improves design yield by 9% for the same area and performance (and area reduction by about 8.4% under a yield constraint) over a balanced design.", "venue": "Design, Automation and Test in Europe", "authors": ["Animesh  Datta", "Swarup  Bhunia", "Saibal  Mukhopadhyay", "Nilanjan  Banerjee", "Kaushik  Roy"], "year": 2005, "n_citations": 38}
{"id": 5471831, "s2_id": "1e270c3bdf6825be9ec9e22e014dd221626f24fb", "title": "Decting Errors in Reversible Circuits With Invariant Relationships", "abstract": "Reversible logic is experience renewed interest as we are approach the limits of CMOS technologies. While physical implementations of reversible gates have yet to materialize, it is safe to assume that they will rely on faulty individual components. In this work we present a present a method to provide fault tolerance to a reversible circuit based on invariant relationships.", "venue": "ArXiv", "authors": ["Nuno  Alves"], "year": 2008, "n_citations": 1}
{"id": 5474137, "s2_id": "74771bef41b6d9dd6605e1a2f5a767efd72b9680", "title": "A Low-Power Accelerator for Deep Neural Networks with Enlarged Near-Zero Sparsity", "abstract": "It remains a challenge to run Deep Learning in devices with stringent power budget in the Internet-of-Things. This paper presents a low-power accelerator for processing Deep Neural Networks in the embedded devices. The power reduction is realized by avoiding multiplications of near-zero valued data. The near-zero approximation and a dedicated Near-Zero Approximation Unit (NZAU) are proposed to predict and skip the nearzero multiplications under certain thresholds. Compared with skipping zero-valued computations, our design achieves 1.92X and 1.51X further reduction of the total multiplications in LeNet5 and Alexnet respectively, with negligible lose of accuracy. In the proposed accelerator, 256 multipliers are grouped into 16 independent Processing Lanes (PL) to support up to 16 neuron activations simultaneously. With the help of data pre-processing and buffering in each PL, multipliers can be clock-gated in most of the time even the data is excessively streaming in. Designed and simulated in UMC 65 nm process, the accelerator operating at 500 MHz is > 4X faster than the mobile GPU Tegra K1 in processing the fully-connected layer FC8 of Alexnet, while consuming 717X less energy.", "venue": "ArXiv", "authors": ["Yuxiang  Huan", "Yifan  Qin", "Yantian  You", "Li-Rong  Zheng", "Zhuo  Zou"], "year": 2017, "n_citations": 4}
{"id": 5476787, "s2_id": "af0d32e2e87cfb78f2e28b6333ea953f4b239d8a", "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its Performance on PIUMA and Xeon CPU", "abstract": "The Word Movers Distance (WMD) measures the semantic dissimilarity between two text documents by computing the cost of optimally moving all words of a source/query document to the most similar words of a target document. Computing WMD between two documents is costly because it requires solving an optimization problem that costs O (V 3log(V )) where V is the number of unique words in the document. Fortunately, WMD can be framed as an Earth Mover\u2019s Distance (EMD) for which the algorithmic complexity can be reduced to O (V 2) by adding an entropy penalty to the optimization problem and solving it using the Sinkhorn-Knopp algorithm. Additionally, the computation can be made highly parallel by computing the WMD of a single query document against multiple target documents at once, for example by finding whether a given tweet is similar to any other tweets of a given day. In this paper, we first present a shared-memory parallel SinkhornKnopp algorithm to compute the WMD of one document against many other documents by adopting theO (V 2) EMD algorithm. We then algorithmically transform the original O (V 2) dense computeheavy version into an equivalent sparse one which is mapped onto the new Intel Programmable Integrated Unified Memory Architecture (PIUMA) system. The WMD parallel implementation achieves 67\u00d7 speedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We also show that PIUMA cores are around 1.2 \u2014 2.6\u00d7 faster than Xeon cores on Sinkhorn-WMD and also provide better strong scaling.", "venue": "ArXiv", "authors": ["Jesmin Jahan Tithi", "Fabrizio  Petrini"], "year": 2021, "n_citations": 0}
{"id": 5477386, "s2_id": "776daf2716144f2a8b3a9e93b3abce1a0cdc3bd3", "title": "SPARTA: A Divide and Conquer Approach to Address Translation for Accelerators", "abstract": "Virtual memory (VM) is critical to the usability and programmability of hardware accelerators. Unfortunately, implementing accelerator VM efficiently is challenging because the area and power constraints make it difficult to employ the large multi-level TLBs used in general-purpose CPUs. Recent research proposals advocate a number of restrictions on virtual-to-physical address mappings in order to reduce the TLB size or increase its reach. However, such restrictions are unattractive because they forgo many of the original benefits of traditional VM, such as demand paging and copy-on-write. \nWe propose SPARTA, a divide and conquer approach to address translation. SPARTA splits the address translation into accelerator-side and memory-side parts. The accelerator-side translation hardware consists of a tiny TLB covering only the accelerator's cache hierarchy (if any), while the translation for main memory accesses is performed by shared memory-side TLBs. Performing the translation for memory accesses on the memory side allows SPARTA to overlap data fetch with translation, and avoids the replication of TLB entries for data shared among accelerators. To further improve the performance and efficiency of the memory-side translation, SPARTA logically partitions the memory space, delegating translation to small and efficient per-partition translation hardware. Our evaluation on index-traversal accelerators shows that SPARTA virtually eliminates translation overhead, reducing it by over 30x on average (up to 47x) and improving performance by 57%. At the same time, SPARTA requires minimal accelerator-side translation hardware, reduces the total number of TLB entries in the system, gracefully scales with memory size, and preserves all key VM functionalities.", "venue": "ArXiv", "authors": ["Javier  Picorel", "Seyed Alireza Sanaee Kohroudi", "Zi  Yan", "Abhishek  Bhattacharjee", "Babak  Falsafi", "Djordje  Jevdjic"], "year": 2020, "n_citations": 1}
{"id": 5481530, "s2_id": "30ac5481d8ae2476051b499352a0468b1e082913", "title": "Towards Creating a Deployable Grasp Type Probability Estimator for a Prosthetic Hand", "abstract": "For lower arm amputees, prosthetic hands promise to restore most of physical interaction capabilities. This requires to accurately predict hand gestures capable of grabbing varying objects and execute them timely as intended by the user. Current approaches often rely on physiological signal inputs such as Electromyography (EMG) signal from residual limb muscles to infer the intended motion. However, limited signal quality, user diversity and high variability adversely affect the system robustness. Instead of solely relying on EMG signals, our work enables augmenting EMG intent inference with physical state probability through machine learning and computer vision method. To this end, we: (1) study state-of-the-art deep neural network architectures to select a performant source of knowledge transfer for the prosthetic hand, (2) use a dataset containing object images and probability distribution of grasp types as a new form of labeling where instead of using absolute values of zero and one as the conventional classification labels, our labels are a set of probabilities whose sum is 1. The proposed method generates probabilistic predictions which could be fused with EMG prediction of probabilities over grasps by using the visual information from the palm camera of a prosthetic hand. Our results demonstrate that InceptionV3 achieves highest accuracy with 0.95 angular similarity followed by 1.4 MobileNetV2 with 0.93 at \\(\\sim \\)20% the amount of operations.", "venue": "CyPhy/WESE", "authors": ["Mehrshad  Zandigohar", "Mo  Han", "Deniz  Erdogmus", "Gunar  Schirner"], "year": 2019, "n_citations": 2}
{"id": 5487532, "s2_id": "6a6445056498d6445886d57a5c5a30d13acf38ab", "title": "Tensor Yard: One-Shot Algorithm of Hardware-Friendly Tensor-Train Decomposition for Convolutional Neural Networks", "abstract": "Nowadays Deep Learning became widely used in many economic, technical and scientific areas of human interest. It is clear that efficiency of solutions based on Deep Neural Networks should consider not only quality metric for the target task, but also latency and constraints of target platform design should be taken into account. In this paper we present novel hardware-friendly Tensor-Train decomposition implementation for Convolutional Neural Networks together with Tensor Yard \u2014 one-shot training algorithm which optimizes an order of decomposition of network layers. These ideas allow to accelerate ResNet models on Ascend 310 NPU devices without significant loss of accuracy. For example we accelerate ResNet-101 by 14.6% with drop by 0.5 of top-1 ImageNet accuracy.", "venue": "ArXiv", "authors": ["Anuar  Taskynov", "Vladimir  Korviakov", "Ivan  Mazurenko", "Yepan  Xiong"], "year": 2021, "n_citations": 1}
{"id": 5487598, "s2_id": "55f799b9471c0623c36a57e5cef967db462371dd", "title": "Worst-case and average-case analysis of n-detection test sets", "abstract": "Test sets that detect each target fault n times (n-detection test sets) are typically generated for restricted values of n due to the increase in test set size with n. We perform both a worst-case analysis and an average-case analysis to check the effect of restricting n on the unmodeled fault coverage of an (arbitrary) n-detection test set. Our analysis is independent of any particular test set or test generation approach. It is based on a specific set of target faults and a specific set of untargeted faults. It shows that, depending on the circuit, very large values of n may be needed to guarantee the detection of all the untargeted faults. We discuss the implications of these results.", "venue": "Design, Automation and Test in Europe", "authors": ["Irith  Pomeranz", "Sudhakar M. Reddy"], "year": 2005, "n_citations": 10}
{"id": 5489876, "s2_id": "57169892a94fc885c82351d3394fad71c9d3c2a7", "title": "Exploring Fault-Energy Trade-offs in Approximate DNN Hardware Accelerators", "abstract": "Systolic array-based deep neural network (DNN) accelerators have recently gained prominence for their low computational cost. However, their high energy consumption poses a bottleneck to their deployment in energy-constrained devices. To address this problem, approximate computing can be employed at the cost of some tolerable accuracy loss. However, such small accuracy variations may increase the sensitivity of DNNs towards undesired subtle disturbances, such as permanent faults. The impact of permanent faults in accurate DNNs has been thoroughly investigated in the literature. Conversely, the impact of permanent faults in approximate DNN accelerators (AxDNNs) is yet under-explored. The impact of such faults may vary with the fault bit positions, activation functions and approximation errors in AxDNN layers. Such dynamacity poses a considerable challenge to exploring the trade-off between their energy efficiency and fault resilience in AxDNNs. Towards this, we present an extensive layer-wise and bit-wise fault resilience and energy analysis of different AxDNNs, using the state-of-the-art Evoapprox8b signed multipliers. In particular, we vary the stuck-at-0, stuck-at-1 fault-bit positions, and activation functions to study their impact using the most widely used MNIST and Fashion-MNIST datasets. Our quantitative analysis shows that the permanent faults exacerbate the accuracy loss in AxDNNs when compared to the accurate DNN accelerators. For instance, a permanent fault in AxDNNs can lead up to 66% accuracy loss, whereas the same faulty bit can lead to only 9% accuracy loss in an accurate DNN accelerator. Our results demonstrate that the fault resilience in AxDNNs is orthogonal to the energy efficiency.", "venue": "2021 22nd International Symposium on Quality Electronic Design (ISQED)", "authors": ["Ayesha  Siddique", "Kanad  Basu", "Khaza Anuarul Hoque"], "year": 2021, "n_citations": 1}
{"id": 5490297, "s2_id": "2cdaad38ba0cd9a8b3dcecef4e0550bf674b7855", "title": "Customizing Trusted AI Accelerators for Efficient Privacy-Preserving Machine Learning", "abstract": "The use of trusted hardware has become a promising solution to enable privacy-preserving machine learning. In particular, users can upload their private data and models to a hardware-enforced trusted execution environment (e.g. an enclave in Intel SGX-enabled CPUs) and run machine learning tasks in it with confidentiality and integrity guaranteed. To improve performance, AI accelerators have been widely employed for modern machine learning tasks. However, how to protect privacy on an AI accelerator remains an open question. To address this question, we propose a solution for efficient privacy-preserving machine learning based on an unmodified trusted CPU and a customized trusted AI accelerator. We carefully leverage cryptographic primitives to establish trust and protect the channel between the CPU and the accelerator. As a case study, we demonstrate our solution based on the open-source versatile tensor accelerator. The result of evaluation shows that the proposed solution provides efficient privacy-preserving machine learning at a small design cost and moderate performance overhead.", "venue": "ArXiv", "authors": ["Peichen  Xie", "Xuanle  Ren", "Guangyu  Sun"], "year": 2020, "n_citations": 0}
{"id": 5490871, "s2_id": "3eb871533f94e7cca0fac69f9742ec272fd69c4a", "title": "A Comparative Study between HLS and HDL on SoC for Image Processing Applications", "abstract": "The increasing complexity in today's systems and the limited market times demand new development tools for FPGA. Currently, in addition to traditional hardware description languages (HDLs), there are high-level synthesis (HLS) tools that increase the abstraction level in system development. Despite the greater simplicity of design and testing, HLS has some drawbacks in describing harware. This paper presents a comparative study between HLS and HDL for FPGA, using a Sobel filter as a case study in the image processing field. The results show that the HDL implementation is slightly better than the HLS version considering resource usage and response time. However, the programming effort required in the HDL solution is significantly larger than in the HLS counterpart.", "venue": "ArXiv", "authors": ["Roberto  Millon", "Emmanuel  Frati", "Enzo  Rucci"], "year": 2020, "n_citations": 0}
{"id": 5492895, "s2_id": "4ea8b538d7f66103187c68a05db57facf5f69b41", "title": "Search for Optimal Systolic Arrays: A Comprehensive Automated Exploration Framework and Lessons Learned", "abstract": "Systolic arrays have been widely used for accelerating HPC and deep learning applications. There is a plethora of previous works on the performance tuning of systolic arrays, but usually based on a number of oversimplified assumptions (e.g., only considering divisors for loop tiling, pruning based on off-chip data communication) to reduce the design space. In this paper, we present a comprehensive design space exploration tool named Odyssey for systolic array optimization. Odyssey does not rely on artificial assumptions to limit the design space, and yet it is highly efficient and scalable with a hybrid optimization technique. For example, for a 1024\u00d71024\u00d71024 matrix multiplication, it finds designs that reach 90% of the optimal performance in 5 seconds with a single CPU thread. Moreover, using Odyssey, we unveil and quantify the suboptimality introduced by multiple commonly used oversimplifications in prior studies for systolic array design space exploration. For example, Odyssey results show that limiting to divisors for loop tiling leads to a 39% performance loss, and pruning based on off-chip data movement results in a 45% performance loss. We applied Odyssey to explore the architecture trade-offs for matrix multiplication and convolutional neural network, providing inspiration into possible optimizations for these two applications.", "venue": "ArXiv", "authors": ["Jie  Wang", "Jason  Cong"], "year": 2021, "n_citations": 0}
{"id": 5494271, "s2_id": "39fc07c6bad01315794f314437846690cade70db", "title": "Overview of Swallow - A Scalable 480-core System for Investigating the Performance and Energy Efficiency of Many-core Applications and Operating Systems", "abstract": "We present Swallow, a scalable many-core architecture, with a current configuration of 480 x 32-bit processors. \nSwallow is an open-source architecture, designed from the ground up to deliver scalable increases in usable computational power to allow experimentation with many-core applications and the operating systems that support them. \nScalability is enabled by the creation of a tile-able system with a low-latency interconnect, featuring an attractive communication-to-computation ratio and the use of a distributed memory configuration. \nWe analyse the energy and computational and communication performances of Swallow. The system provides 240GIPS with each core consuming 71--193mW, dependent on workload. Power consumption per instruction is lower than almost all systems of comparable scale. \nWe also show how the use of a distributed operating system (nOS) allows the easy creation of scalable software to exploit Swallow's potential. Finally, we show two use case studies: modelling neurons and the overlay of shared memory on a distributed memory system.", "venue": "ArXiv", "authors": ["Simon J. Hollis", "Steve  Kerrison"], "year": 2015, "n_citations": 3}
{"id": 5496139, "s2_id": "3eb5d40c0483e1f877d1bdd27963eaf509178818", "title": "Temperature-Based Hardware Trojan For Ring-Oscillator-Based TRNGs", "abstract": "True random number generators (TRNGs) are essential components of cryptographic designs, which are used to generate private keys for encryption and authentication, and are used in masking countermeasures. In this work, we present a mechanism to design a stealthy parametric hardware Trojan for a ring oscillator based TRNG architecture proposed by Yang et al. at ISSCC 2014. Once the Trojan is triggered the malicious TRNG generates predictable non-random outputs. Such a Trojan does not require any additional logic (even a single gate) and is purely based on subtle manipulations on the sub-transistor level. The underlying concept is to disable the entropy source at high temperature to trigger the Trojan, while ensuring that Trojan-infected TRNG works correctly under normal conditions. We show how an attack can be performed with the Trojan-infected TRNG design in which the attacker uses a stochastic Markov Chain model to predict its reduced-entropy outputs.", "venue": "ArXiv", "authors": ["Samaneh  Ghandali", "Daniel  Holcomb", "Christof  Paar"], "year": 2019, "n_citations": 1}
{"id": 5503925, "s2_id": "64736a0c0e8d86a572aa61bb5b02ecafc9af2b1c", "title": "Partial Reconfiguration for Design Optimization", "abstract": "FPGA designers have traditionally shared a similar design methodology with ASIC designers. Most notably, at design time, FPGA designers commit to a fixed allocation of logic resources to modules in a design. At runtime, some of the occupied resources could be left under-utilized due to hard-to-avoid sources of inefficiencies (e.g., operation dependencies, unbalanced pipelines). With partial reconfiguration (PR), FPGA resources can be re-allocated over time. Therefore, using PR, a designer can attempt to reduce under-utilization with better area-time scheduling. In this paper, we offer definitions, insights, and equations to explain when, how, and why PR-style designs can improve over the performance-area Pareto front of ASIC-style designs (without PR). We first introduce the concept of area-time volume to explain why PR-style designs can improve upon ASIC-style designs. We identify resource under-utilization as an opportunity that can be exploited by PR-style designs. We then present a first-order analytical model to help a designer decide if a PR-style design can be beneficial. When it is the case, the model points to the most suitable PR execution strategy and provides an estimate of the improvement. The model is validated in a case study.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Marie  Nguyen", "Nathan  Serafin", "James C. Hoe"], "year": 2020, "n_citations": 1}
{"id": 5505261, "s2_id": "5b6302334e302afa708900a4ddcb4ab396b1a3c0", "title": "Vector Symbolic Architectures as a Computing Framework for Nanoscale Hardware", "abstract": "This article reviews recent progress in the development of the computing framework Vector Symbolic Architectures (also known as Hyperdimensional Computing). This framework is well suited for implementation in stochastic, nanoscale hardware and it naturally expresses the types of cognitive operations required for Artificial Intelligence (AI). We demonstrate in this article that the ring-like algebraic structure of Vector Symbolic Architectures offers simple but powerful operations on highdimensional vectors that can support all data structures and manipulations relevant in modern computing. In addition, we illustrate the distinguishing feature of Vector Symbolic Architectures, \u201ccomputing in superposition,\u201d which sets it apart from conventional computing. This latter property opens the door to efficient solutions to the difficult combinatorial search problems inherent in AI applications. Vector Symbolic Architectures are Turing complete, as we show, and we see them acting as a framework for computing with distributed representations in myriad AI settings. This paper serves as a reference for computer architects by illustrating techniques and philosophy of VSAs for distributed computing and relevance to emerging computing hardware, such as neuromorphic computing.", "venue": "ArXiv", "authors": ["Denis  Kleyko", "Mike  Davies", "E. Paxon Frady", "Pentti  Kanerva", "Spencer J. Kent", "Bruno A. Olshausen", "Evgeny  Osipov", "Jan M. Rabaey", "Dmitri A. Rachkovskij", "Abbas  Rahimi", "Friedrich T. Sommer"], "year": 2021, "n_citations": 8}
{"id": 5506766, "s2_id": "f9897e8d8ed938af522d85c212027432390620f9", "title": "Android OS CASE STUDY", "abstract": "Android is a mobile operating system based on a modified version of the Linux kernel and other open source software, designed primarily for touchscreen mobile devices such as smartphones and tablets. It is an operating system for low powered devices that run on battery and are full of hardware like Global Positioning System (GPS) receivers, cameras, light and orientation sensors, Wi-Fi and LTE (4G telephony) connectivity and a touch screen. Like all operating systems, Android enables applications to make use of the hardware features through abstraction and provide a defined environment for applications. The study includes following topic: Background And History Android Architecture Kernel And StartUp Process Process Management Deadlock CPU Scheduling Memory Management Storage Management I/O Battery Optimization", "venue": "ArXiv", "authors": ["Mayank  Goel", "Gourav  Singal"], "year": 2021, "n_citations": 0}
{"id": 5507006, "s2_id": "b811ed4bf1cbbff2f08157c8bfe66ccc8ca6e9f4", "title": "Ring-mesh: a scalable and high-performance approach for manycore accelerators", "abstract": "There is increasing number of works addressing the design challenges of fast, scalable solutions for the growing number of new type of applications. Recently, many of the solutions aimed at improving processing element capabilities to speed up the execution of machine learning application domain. However, only a few works focused on the interconnection subsystem as a potential source of performance improvement. Wrapping many cores together offer excellent parallelism, but it brings other challenges (e.g. adequate interconnections). Scalable, power-aware interconnects are required to support such a growing number of processing elements, as well as modern applications. In this paper, we propose a scalable and energy-efficient network-on-chip architecture fusing the advantages of rings as well as the 2D mesh without using any bridge router to provide high performance. A dynamic adaptation mechanism allows to better adapt to the application requirements. Simulation results show efficient power consumption (up to 141.3%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$141.3\\%$$\\end{document} saving for connecting 1024 cores), 2\u00d7\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2{\\times}$$\\end{document} (on average) throughput growth with better scalability (up to 1024 processing elements) compared to popular 2D mesh while tested in multiple statistical traffic pattern scenarios.", "venue": "The Journal of Supercomputing", "authors": ["Somnath  Mazumdar", "Alberto  Scionti"], "year": 2019, "n_citations": 0}
{"id": 5510622, "s2_id": "9fbc271f7f6ddf6ef7fc30bf6a50ef7a511ce336", "title": "GREENER: A Tool for Improving Energy Efficiency of Register Files", "abstract": "Graphics Processing Units (GPUs) maintain a large register file to increase the thread level parallelism (TLP). To increase the TLP further, recent GPUs have increased the number of on-chip registers in every generation. However, with the increase in the register file size, the leakage power increases. Also, with the technology advances, the leakage power component has increased and has become an important consideration for the manufacturing process. The leakage power of a register file can be reduced by turning infrequently used registers into low power (drowsy or off) state after accessing them. A major challenge in doing so is the lack of runtime register access information. \nThis paper proposes GREENER (GPU REgister file ENErgy Reducer): a system to minimize leakage energy of the register file of GPUs. GREENER employs a compile-time analysis to estimate the run-time register access information. The result of the analysis is used to determine the power state of the registers (ON, SLEEP, or OFF) after each instruction. We propose a power optimized assembly instruction set that allows GREENER to encode the power state of the registers in the executable itself. The modified assembly, along with a run-time optimization to update the power state of a register during execution, results in significant power reduction. \nWe implemented GREENER in GPGPU-Sim simulator, and used GPUWattch framework to measure the register file's leakage power. Evaluation of GREENER on 21 kernels from CUDASDK, GPGPU-SIM, Parboil, and Rodinia benchmarks suites shows an average reduction of register leakage energy by 69.04% and maximum reduction of 87.95% with a negligible number of simulation cycles overhead (0.53% on average).", "venue": "ArXiv", "authors": ["Vishwesh  Jatala", "Jayvant  Anantpur", "Amey  Karkare"], "year": 2017, "n_citations": 2}
{"id": 5518947, "s2_id": "efd8b1269f21fd35ef7e5628ab970ab223bee53d", "title": "Testing logic cores using a BIST P1500 compliant approach: a case of study", "abstract": "In this paper we describe how we applied a BIST-based approach to the test of a logic core to be included in system-on-a-chip (SoC) environments. The approach advantages are the ability to protect the core IP, the simple test interface (thanks also to the adoption of the P1500 standard), the possibility to run the test at-speed, the reduced test time, and the good diagnostic capabilities. The paper reports figures of the achieved fault coverage, the required area overhead, and the performance slowdown, and compares the figures with those for alternative approaches, such as those based on full scan and sequential ATPG.", "venue": "Design, Automation and Test in Europe", "authors": ["Paolo  Bernardi", "Guido  Masera", "Federico  Quaglio", "Matteo Sonza Reorda"], "year": 2005, "n_citations": 12}
{"id": 5519802, "s2_id": "4ce2697cdb6566945bfaaf35f02e71cc06c8acdc", "title": "Low-latency list decoding of polar codes with double thresholding", "abstract": "For polar codes with short-to-medium code length, list successive cancellation decoding is used to achieve a good error-correcting performance. However, list pruning in the current list decoding is based on the sorting strategy and its timing complexity is high. This results in a long decoding latency for large list size. In this work, aiming at a low-latency list decoding implementation, a double thresholding algorithm is proposed for a fast list pruning. As a result, with a negligible performance degradation, the list pruning delay is greatly reduced. Based on the double thresholding, a low-latency list decoding architecture is proposed and implemented using a UMC 90nm CMOS technology. Synthesis results show that, even for a large list size of 16, the proposed low-latency architecture achieves a decoding throughput of 220 Mbps at a frequency of 641 MHz.", "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["YouZhe  Fan", "Ji  Chen", "ChenYang  Xia", "Chi-Ying  Tsui", "Jie  Jin", "Hui  Shen", "Bin  Li"], "year": 2015, "n_citations": 32}
{"id": 5521023, "s2_id": "9dbd99f76e29d2326498ba60cbf3daa21f81f2ab", "title": "Synthesis of Predictable Global NoC by Abutment in Synchoros VLSI Design", "abstract": "Synchoros VLSI design style has been proposed as an alternative to the standard cell-based design style; the word synchoros is derived from the Greek word choros for space. Synchoricity discretises space with a virtual grid, the way synchronicity discretises time with clock ticks. SiLago (Silicon Lego) blocks are atomic synchoros building blocks like Lego bricks. SiLago blocks absorb all metal layer details, i.e., all wires, to enable composition by abutment of valid; valid in the sense of being technology design rules compliant, timing clean and OCV ruggedized. Effectively, composition by abutment eliminates logic and physical synthesis for the end user. Like Lego system, synchoricity does need a finite number of SiLago block types to cater to different types of designs. Global NoCs are important system level design components. In this paper, we show, how with a small library of SiLago blocks for global NoCs, it is possible to automatically synthesize arbitrary global NoCs of different types, dimensions, and topology. The synthesized global NoCs are not only valid VLSI designs, but their cost metrics (area, latency, and energy) are known with post-layout accuracy in linear time. We argue that this is essential to be able to do chip-level design space exploration. We show how the abstract timing model of such global NoC SiLago blocks can be built and used to analyse the timing of global NoC links with post layout accuracy and in linear time. We validate this claim by subjecting the same VLSI designs of global NoC to commercial EDA\u2019s static timing analysis and show that the abstract timing analysis enabled by synchoros VLSI design gives the same results as the commercial EDA tools.", "venue": "2021 15th IEEE/ACM International Symposium on Networks-on-Chip (NOCS)", "authors": ["Jordi Altay'o Gonz'alez", "Dimitrios  Stathis", "Ahmed  Hemani"], "year": 2021, "n_citations": 1}
{"id": 5537224, "s2_id": "908097087a7cb2185149cb812417274a3cf92645", "title": "Threshold Logic Computing: Memristive-CMOS Circuits for Fast Fourier Transform and Vedic Multiplication", "abstract": "Brain-inspired circuits can provide an alternative solution to implement computing architectures taking advantage of fault tolerance and generalization ability of logic gates. In this brief, we advance over the memristive threshold circuit configuration consisting of memristive averaging circuit in combination with operational amplifier and/or CMOS inverters in application to realizing complex computing circuits. The developed memristive threshold logic gates are used for designing fast Fourier transform and multiplication circuits useful for modern microprocessors. Overall, the proposed threshold logic outperforms previous memristive-CMOS logic cells on every aspect, however, they indicate a lower chip area, lower total harmonic distortion, and controllable leakage power, but a higher power dissipation with respect to CMOS logic.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Alex Pappachen James", "Dinesh Sasi Kumar", "Arun  Ajayan"], "year": 2015, "n_citations": 27}
{"id": 5541391, "s2_id": "e478edf0426e743a5ae6e1ffc04c88e682e263d8", "title": "Neural Storage: A New Paradigm of Elastic Memory", "abstract": "Storage and retrieval of data in a computer memory plays a major role in system performance. Traditionally, computer memory organization is \u2018static\u2019 \u2013 i.e., they do not change based on the applicationspecific characteristics in memory access behaviour during system operation. Specifically, the association of a data block with a search pattern (or cues) as well as the granularity of a stored data do not evolve. Such a static nature of computer memory, we observe, not only limits the amount of data we can store in a given physical storage, but it also misses the opportunity for dramatic performance improvement in various applications. On the contrary, human memory is characterized by seemingly infinite plasticity in storing and retrieving data \u2013 as well as dynamically creating/updating the associations between data and corresponding cues. In this paper, we introduce Neural Storage (NS), a braininspired learning memory paradigm that organizes the memory as a flexible neural memory network. In NS, the network structure, strength of associations, and granularity of the data adjust continuously during system operation, providing unprecedented plasticity and performance benefits. We present the associated storage/retrieval/retention algorithms in NS, which integrate a formalized learning process. Using a full-blown operational model, we demonstrate that NS achieves an order of magnitude improvement in memory access performance for two representative applications when compared to traditional content-based memory.", "venue": "ArXiv", "authors": ["Prabuddha  Chakraborty", "Swarup  Bhunia"], "year": 2021, "n_citations": 0}
{"id": 5543476, "s2_id": "e9190ad5b97ae20e13750fe2fb8b69aee2ee2616", "title": "On the design and analysis of quaternary serial and parallel adders", "abstract": "Optimization techniques for decreasing the time and chip area of adder circuits have been thoroughly studied for years mostly in binary logic system. In this paper, we provide the necessary equations required to design a full adder in quaternary logic system. We provide the design of a logarithmic stage parallel adder which can compute the carries within log2(n) time delay for n qudits. At last, we compare the gate delays of full adder and logarithmic stage parallel adder with the help of mathematical expressions.", "venue": "TENCON 2010 - 2010 IEEE Region 10 Conference", "authors": ["Anindya  Das", "Ifat  Jahangir", "Masud  Hasan"], "year": 2010, "n_citations": 6}
{"id": 5551696, "s2_id": "1db51642aac42a0f1422228205307ccc42de5d9c", "title": "Flashabacus: a self-governing flash-based accelerator for low-power systems", "abstract": "Energy efficiency and computing flexibility are some of the primary design constraints of heterogeneous computing. In this paper, we present FlashAbacus, a data-processing accelerator that self-governs heterogeneous kernel executions and data storage accesses by integrating many flash modules in lightweight multiprocessors. The proposed accelerator can simultaneously process data from different applications with diverse types of operational functions, and it allows multiple kernels to directly access flash without the assistance of a host-level file system or an I/O runtime library. We prototype FlashAbacus on a multicore-based PCIe platform that connects to FPGA-based flash controllers with a 20 nm node process. The evaluation results show that FlashAbacus can improve the bandwidth of data processing by 127%, while reducing energy consumption by 78.4%, as compared to a conventional method of heterogeneous computing.", "venue": "EuroSys", "authors": ["Jie  Zhang", "Myoungsoo  Jung"], "year": 2018, "n_citations": 8}
{"id": 5554197, "s2_id": "1ce8618f38957eb36cac1e72118baf00f7224082", "title": "Packet Chasing: Spying on Network Packets over a Cache Side-Channel", "abstract": "This paper presents Packet Chasing, an attack on the network that does not require access to the network, and works regardless of the privilege level of the process receiving the packets. A spy process can easily probe and discover the exact cache location of each buffer used by the network driver. Even more useful, it can discover the exact sequence in which those buffers are used to receive packets. This then enables packet frequency and packet sizes to be monitored through cache side channels. This allows both covert channels between a sender and a remote spy with no access to the network, as well as direct attacks that can identify, among other things, the web page access patterns of a victim on the network. In addition to identifying the potential attack, this work proposes a software-based short-term mitigation as well as a light-weight, adaptive, cache partitioning mitigation that blocks the interference of I/O and CPU requests in the last-level cache.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Mohammadkazem  Taram", "Ashish  Venkat", "Dean  Tullsen"], "year": 2020, "n_citations": 6}
{"id": 5554421, "s2_id": "b8ba4eae7d3e03f188ffc2056ba913bad2ad0066", "title": "SIMCNN - Exploiting Computational Similarity to Accelerate CNN Training in Hardware", "abstract": "Convolution neural networks (CNN) are computation intensive to train. It consists of a substantial number of multidimensional dot products between many kernels and inputs. We observe that there are notable similarities among the vectors extracted from inputs (i.e., input vectors). If one input vector is similar to another one, its computations with the kernels are also similar to those of the other and therefore, can be skipped by reusing the already-computed results. Based on this insight, we propose a novel scheme based on locality sensitive hashing (LSH) to exploit the similarity of computations during CNN training in a hardware accelerator. The proposed scheme, called SIMCNN, uses a cache (SIMCACHE) to store LSH signatures of recent input vectors along with the computed results. If the LSH signature of a new input vector matches with that of an already existing vector in the SIMCACHE, the alreadycomputed result is reused for the new vector. SIMCNN is the first work that exploits computational similarity for accelerating CNN training in hardware. The paper presents a detailed design, workflow, and implementation of SIMCNN. Our experimental evaluation with four different deep learning models shows that SIMCNN saves a significant number of computations and therefore, improves training time up to 43%.", "venue": "ArXiv", "authors": ["Vahid  Janfaza", "Kevin  Weston", "Moein  Razavi", "Shantanu  Mandal", "Abdullah  Muzahid"], "year": 2021, "n_citations": 0}
{"id": 5560392, "s2_id": "85d61bfa01d98640e791f54c52b06d91c9723997", "title": "A configurable accelerator for manycores: the Explicitly Many-Processor Approach", "abstract": "A new approach to designing processor accelerators is presented. A new computing model and a special kind of accelerator with dynamic (end-user programmable) architecture is suggested. The new model considers a processor, in which a newly introduced supervisor layer coordinates the job of the cores. The cores have the ability (based on the parallelization information provided by the compiler, and using the help of the supervisor) to outsource part of the job they received to some neighbouring core. The introduced changes essentially and advantageously modify the architecture and operation of the computing systems. The computing throughput drastically increases, the efficiency of the technological implementation (computing performance per logic gates) increases, the non-payload activity for using operating system services decreases, the real-time behavior changes advantageously, and connecting accelerators to the processor greatly simplifies. Here only some details of the architecture and operation of the processor are discussed, the rest is described elsewhere.", "venue": "ArXiv", "authors": ["J\u00e1nos  V\u00e9gh"], "year": 2016, "n_citations": 4}
{"id": 5560743, "s2_id": "9165f068eb635accf10cc5e0243dcc791a74e24e", "title": "The design of a Network-On-Chip architecture based on an avionic protocol", "abstract": "When the Network-On-Chip (NoC) paradigm was introduced, many researchers have proposed many novelistic NoC architectures, tools and design strategies. In this paper we introduce a new approach in the field of designing Network-On-Chip (NoC). Our inspiration came from an avionic protocol which is the AFDX protocol. The proposed NoC architecture is a switch centric architecture, with exclusive shortcuts between hosts and utilizes the flexibility, the reliability and the performances offered by AFDX.", "venue": "2014 World Symposium on Computer Applications & Research (WSCAR)", "authors": ["Ahmed Ben Achballah", "Slim Ben Saoud"], "year": 2014, "n_citations": 3}
{"id": 5570710, "s2_id": "12757911169ae6a3cb6e476219afd174afe66cf7", "title": "Optimization Techniques to Improve Inference Performance of a Forward Propagating Neural Network on an FPGA", "abstract": "This paper describes an optimized implementation of a Forward Propagating Classification Neural Network which has been previously trained. The implementation described highlights a novel means of using Python scripts to generate a Verilog hardware implementation. The characteristics of this implementation include optimizations to scale input data, use selected addends instead of multiplication functions, hardware friendly activation functions and simplified output selection. Inference performance comparison of a 28x28 pixel 'hand-written' recognition NN between a software implementation on an Intel i7 vs a Xilinx FPGA will be detailed.", "venue": "ArXiv", "authors": ["Matthew Joseph Adiletta", "Brian  Flanagan"], "year": 2020, "n_citations": 0}
{"id": 5572280, "s2_id": "2d9b321274f663e6f54fdc7b6c23862ca486bd72", "title": "Cost Efficient Design of Reversible Adder Circuits for Low Power Applications", "abstract": "A large amount of research is currently going on in the field of reversible logic, which have low heat dissipation, low power consumption, which is the main factor to apply reversible in digital VLSI circuit design. This paper introduces reversible gate named as Inventive0 gate. The novel gate is synthesis the efficient adder modules with minimum garbage output and gate count. The Inventive0 gate capable of implementing a 4-bit ripple carry adder and carry skip adders.It is presented that Inventive0 gate is much more efficient and optimized approach as compared to their existing design, in terms of gate count, garbage outputs and constant inputs. In addition, some popular available reversible gates are implemented in the MOS transistor design the implementation kept in mind for minimum MOS transistor count and are completely reversible in behavior more precise forward and backward computation. Lesser architectural complexity show that the novel designs are compact, fast as well as low power.", "venue": "ArXiv", "authors": ["Neeraj Kumar Misra", "Mukesh Kumar Kushwaha", "Subodh  Wairya", "Amit  Kumar"], "year": 2015, "n_citations": 10}
{"id": 5572373, "s2_id": "bf05584448acb175f1ba20319d4ab22710591df9", "title": "Faster and Low Power Twin Precision Multiplier", "abstract": "In this work faster unsigned multiplication has been achieved by using a combination of High Performance Multiplication [HPM] column reduction technique and implementing a N-bit multiplier using 4 N/2-bit multipliers (recursive multiplication) and acceleration of the final addition using a hybrid adder. Low power has been achieved by using clock gating technique. Based on the proposed technique 16 and 32-bit multipliers are developed. The performance of the proposed multiplier is analyzed by evaluating the delay, area and power, with TCBNPHP 90 nm process technology on interconnect and layout using Cadence NC launch, RTL compiler and ENCOUNTER tools. The results show that the 32-bit proposed multiplier is as much as 22% faster, occupies only 3% more area and consumes 30% lesser power with respect to the recently reported twin precision multiplier.", "venue": "ArXiv", "authors": ["V.  Sreedeep", "B.  Ramkumar", "Harish M. Kittur"], "year": 2011, "n_citations": 0}
{"id": 5575272, "s2_id": "feecd7badb21917a9e7b87f6d3558e8a684eb916", "title": "Side-Channel Attacks on RISC-V Processors: Current Progress, Challenges, and Opportunities", "abstract": "Side-channel attacks on microprocessors, like the RISC-V, exhibit security vulnerabilities that lead to several design challenges. Hence, it is imperative to study and analyze these security vulnerabilities comprehensively. In this paper, we present a brief yet comprehensive study of the security vulnerabilities in modern microprocessors with respect to side-channel attacks and their respective mitigation techniques. The focus of this paper is to analyze the hardware-exploitable side-channel attack using power consumption and software-exploitable side-channel attacks to manipulate cache. Towards this, we perform an in-depth analysis of the applicability and practical implications of cache attacks on RISC-V microprocessors and their associated challenges. Finally, based on the comparative study and our analysis, we highlight some key research directions to develop robust RISC-V microprocessors that are resilient to side-channel attacks. Keywords\u2013RISC-V; Side-channel; Secure ISA; microprocessors; cache; hardware security.", "venue": "ArXiv", "authors": ["Mahya Morid Ahmadi", "Faiq  Khalid", "Muhammad  Shafique"], "year": 2021, "n_citations": 0}
{"id": 5584475, "s2_id": "8a7b5f48548b88a38733f858c05abc5418e69124", "title": "Hardware Implementation of A Non-RLL Soft-decoding Beacon-based Visible Light Communication Receiver", "abstract": "Visible light communication (VLC)-based beacon systems, which usually transmit identification (ID) information in small-size data frames are applied widely in indoor localization systems. There is one fact that flicker of LED light should be avoided in any VLC systems. Current flicker mitigation solutions based on run-length limited (RLL) codes suffer from reduced code rates or are limited to hard-decoding forward error correction (FEC) decoders. Recently, soft-decoding techniques of RLL- codes are proposed to support soft-decoding FEC algorithms, but they include high-complexity and time-consuming computations. Fortunately, non-RLL direct-current (DC)-balance solutions can overcome the drawbacks of RLL-based algorithms. In this paper, we introduce a non-RLL flicker mitigation method for the VLC- based beacon systems that combines a simple pre-scrambler with a Polar encoder to keep the DC-balance of VLC system even in short data frames. Also, we have proposed a hardware architecture for the proposed non-RLL VLC receiver for the first time. Moreover, to enable the soft-decoding of the Polar decoder, we introduce a 3-bit soft-decision filter which helps improve the bit-error-rate performance of the system.", "venue": "2018 International Conference on Advanced Technologies for Communications (ATC)", "authors": ["Duc Phuc Nguyen", "Dinh-Dung  Le", "Thi Hong Tran", "Huu-Thuan  Huynh", "Yasuhiko  Nakashima"], "year": 2018, "n_citations": 6}
{"id": 5586933, "s2_id": "30e3d8f9299148b8f5e5a53e348ae7c6d98792be", "title": "Optimisation of job scheduling for supercomputers with burst buffers", "abstract": "The ever-increasing gap between compute and I/O performance in HPC platforms, together with the development of novel NVMe storage devices (NVRAM), led to the emergence of the burst buffer concept - an intermediate persistent storage layer logically positioned between random-access main memory and a parallel file system. Since the appearance of this technology, numerous supercomputers have been equipped with burst buffers exploring various architectures. Despite the development of real-world architectures as well as research concepts, Resource and Job Management Systems, such as Slurm, provide only marginal support for scheduling jobs with burst buffer requirements. This research is primarily motivated by the alerting observation that burst buffers are omitted from reservations in the procedure of backfilling in existing job schedulers. In this dissertation, we forge a detailed supercomputer simulator based on Batsim and SimGrid, which is capable of simulating I/O contention and I/O congestion effects. Due to the lack of publicly available workloads with burst buffer requests, we create a burst buffer request distribution model derived from Parallel Workload Archive logs. We investigate the impact of burst buffer reservations on the overall efficiency of online job scheduling for canonical algorithms: First-Come-First-Served (FCFS) and Shortest-Job-First (SJF) EASY-backfilling. Our results indicate that the lack of burst buffer reservations in backfilling may significantly deteriorate the performance of scheduling. [...] Furthermore, this lack of reservations may cause the starvation of medium-size and wide jobs. Finally, we propose a burst-buffer-aware plan-based scheduling algorithm with simulated annealing optimisation, which improves the mean waiting time by over 20% and mean bounded slowdown by 27% compared to the SJF EASY-backfilling.", "venue": "ArXiv", "authors": ["Jan  Kopanski"], "year": 2021, "n_citations": 0}
{"id": 5587651, "s2_id": "1e8ba7124ec76fb8643eb200e1cef930df681b1b", "title": "A study of the speedups and competitiveness of FPGA soft processor cores using dynamic hardware/software partitioning", "abstract": "Field programmable gate arrays (FPGAs) provide designers with the ability to create hardware circuits quickly. Increases in FPGA configurable logic capacity and decreasing FPGA costs have enabled designers to incorporate FPGAs more readily in their designs. FPGA vendors have begun providing configurable soft processor cores that can be synthesized onto their FPGA products. While FPGAs with soft processor cores provide designers with increased flexibility, such processors typically have degraded performance and energy consumption compared to hard-core processors. Previously, we proposed warp processing, a technique capable of optimizing a software application by dynamically and transparently re-implementing critical software kernels as custom circuits in on-chip configurable logic. We now study the potential of a MicroBlaze soft-core based warp processing system to eliminate the performance and energy overhead of a soft-core processor compared to a hard-core processor. We demonstrate that the soft-core based warp processor achieves average speedups of 5.8 and energy reductions of 57% compared to the soft core alone. Our data shows that a soft-core based warp processor yields performance and energy consumption competitive with existing hard-core processors, thus expanding the usefulness of soft processor cores on FPGAs to a broader range of applications.", "venue": "Design, Automation and Test in Europe", "authors": ["Roman L. Lysecky", "Frank  Vahid"], "year": 2005, "n_citations": 120}
{"id": 5587802, "s2_id": "8d3759b2f4ea65907b1fb5d102c42e68ef234766", "title": "Multi-voltage and level-shifter assignment driven floorplanning", "abstract": "As technology scales, low power design has become a significant requirement for SOC designers. Among the existing techniques, Multiple-Supply Voltage (MSV) is a popular and effective method to reduce both dynamic and static power. Besides, level shifters consume area and delay, and should be considered during floorplanning. In this paper, we present a new floorplanning system, called MVLSAF, to solve multi-voltage and level shifter assignment problem. We use a convex cost network flow algorithm to assign arbitrary number of legal working voltages and a minimum cost flow algorithm to handle level-shifter assignment. The experimental results show MVLSAF is effective1.", "venue": "2009 IEEE 8th International Conference on ASIC", "authors": ["Bei  Yu", "Sheqin  Dong", "Satoshi  Goto"], "year": 2009, "n_citations": 6}
{"id": 5593082, "s2_id": "674770476f10d8680364c4a5699d28476b579cba", "title": "On the Low-Complexity, Hardware-Friendly Tridiagonal Matrix Inversion for Correlated Massive MIMO Systems", "abstract": "In massive multiple-input and multiple-output (M-MIMO) systems, one of the key challenges in the implementation is the large-scale matrix inversion operation, as widely used in channel estimation, equalization, detection, and decoding procedures. Traditionally, to handle this complexity issue, several low-complexity matrix inversion approximation methods have been proposed, including the classic Cholesky decomposition and the Neumann series expansion (NSE). However, the conventional approaches failed to exploit neither the special structure of channel matrices nor the critical issues in the hardware implementation, which results in poorer throughput performance and longer processing delay. In this paper, by targeting at the correlated M-MIMO systems, we propose a modified NSE based on tridiagonal matrix inversion approximation (TMA) to accommodate the complexity as well as the performance issue in the conventional hardware implementation, and analyze the corresponding approximation errors. Meanwhile, we investigate the very-large-scale integration implementation for the proposed detection algorithm based on a Xilinx Virtex-7 XC7VX690T FPGA platform. It is shown that for correlated massive MIMO systems, it can achieve near minimum mean square error performance and 630\u00a0Mb/s throughput. Compared with other benchmark systems, the proposed pipelined TMA detector can get high throughput-to-hardware ratio. Finally, we also propose a fast iteration structure for further research.", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Chuan  Zhang", "Xiao  Liang", "Zhizhen  Wu", "Feng  Wang", "Shunqing  Zhang", "Zaichen  Zhang", "Xiaohu  You"], "year": 2019, "n_citations": 16}
{"id": 5594499, "s2_id": "11dbd1d758b72715b580e9d5d6c0bff180041a32", "title": "ISEGEN: generation of high-quality instruction set extensions by iterative improvement", "abstract": "Customization of processor architectures through instruction set extensions (ISEs) is an effective way to meet the growing performance demands of embedded applications. A high-quality ISE generation approach needs to obtain results close to those achieved by experienced designers, particularly for complex applications that exhibit regularity; expert designers are able to exploit manually such regularity in the data flow graphs to generate high-quality ISEs. We present ISEGEN, an approach that identifies high-quality ISEs by iterative improvement following the basic principles of the well-known Kernighan-Lin (K-L) min-cut heuristic. Experimental results on a number of MediaBench, EEMBC and cryptographic applications show that our approach matches the quality of the optimal solution obtained by exhaustive search. We also show that our ISEGEN technique is on average 20 times faster than a genetic formulation that generates equivalent solutions. Furthermore, the ISEs identified by our technique exhibit 35% more speedup than the genetic solution on a large cryptographic application (AES) by effectively exploiting its regular structure.", "venue": "Design, Automation and Test in Europe", "authors": ["Partha  Biswas", "Sudarshan  Banerjee", "Nikil D. Dutt", "Laura  Pozzi", "Paolo  Ienne"], "year": 2005, "n_citations": 55}
{"id": 5600558, "s2_id": "8bb494c6d5aa81cba9d18059cf10d2114750f103", "title": "MultiNoC: a multiprocessing system enabled by a network on chip", "abstract": "The MultiNoC system implements a programmable onchip multiprocessing platform built on top of an efficient, low area overhead intra-chip interconnection scheme. The employed interconnection structure is a network on chip, or NoC. NoC are emerging as a viable alternative to increasing demands on interconnection architectures, due to the following characteristics: (i) energy efficiency and reliability; (ii) scalability of bandwidth, when compared to traditional bus architectures; (iii) reusability; (iv) distributed routing decisions. An external host computer feeds MultiNoC with application instructions and data. After this initialization procedure, MultiNoC executes some algorithm. After finishing execution of the algorithm, output data can be read back by the host. Sequential or parallel algorithms conveniently adapted to the MultiNoC structure can be executed. The main motivation to propose this design is to enable the investigation of current trends to increase the number of embedded processors in SoC, leading to the concept of \"sea of processors\" systems.", "venue": "Design, Automation and Test in Europe", "authors": ["Aline  Mello", "Leandro  M\u00f6ller", "Ney Laert Vilar Calazans", "Fernando Gehm Moraes"], "year": 2005, "n_citations": 13}
{"id": 5600712, "s2_id": "f686c13e7bf934638086d99742b3199bc0662913", "title": "A Benes Based NoC Switching Architecture for Mixed Criticality Embedded Systems", "abstract": "Multi-core, Mixed Criticality Embedded (MCE) real-time systems require high timing precision and predictability to guarantee there will be no interference between tasks. These guarantees are necessary in application areas such as avionics and automotive, where task interference or missed deadlines could be catastrophic, and safety requirements are strict. In modern multi-core systems, the interconnect becomes a potential point of uncertainty, introducing major challenges in proving behaviour is always within specified constraints, limiting the means of growing system performance to add more tasks, or provide more computational resources to existing tasks. We present MCENoC, a Network-on-Chip (NoC) switching architecture that provides innovations to overcome this with predictable, formally verifiable timing behaviour that is consistent across the whole NoC. We show how the fundamental properties of Benes networks benefit MCE applications and meet our architecture requirements. Using SystemVerilog Assertions (SVA), formal properties are defined that aid the refinement of the specification of the design as well as enabling the implementation to be exhaustively formally verified. We demonstrate the performance of the design in terms of size, throughput and predictability, and discuss the application level considerations needed to exploit this architecture.", "venue": "2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)", "authors": ["Steve  Kerrison", "David  May", "Kerstin  Eder"], "year": 2016, "n_citations": 6}
{"id": 5601280, "s2_id": "64c8a739fef8f1ea61e2e6e6db72557c2a69cbc0", "title": "An Improved GEF Fast Addition Algorithm", "abstract": "In this paper, an improved GEF fast addition algorithm is proposed. The proposed algorithm reduces time and memory space. In this algorithm, carry is calculated on the basis of arrival timing of the operand's bits without overhead of sorting. Intermediate terms are generated from the most significant bit and the carry is generated from the least significant bit using the functions of efficient operators. This algorithm shows better performance for use in the fastest computational devices of the near future.", "venue": "ArXiv", "authors": ["Md. Mizanur Rahman", "Md. Shahadat Hossain", "Md. Rakib Hassan", "M. M. A. Hashem"], "year": 2013, "n_citations": 0}
{"id": 5604875, "s2_id": "c59b9077a071305af0efdb89baf7bae8e6b640ab", "title": "A Survey of Selected Algorithms Used in Military Applications from the Viewpoints of Dataflow and GaAs", "abstract": "This is a short survey of ten algorithms that are often used for military purposes, followed by analysis of their potential suitability for dataflow and GaAs, which are a specific architecture and technology for supercomputers on a chip, respectively. Whenever an algorithm or a device is used in military settings, it is natural to assume strict requirements related to speed, reliability, scale, energy, size, and accuracy. The two aforementioned paradigms seem to be promising in fulfilling most of these requirements.", "venue": "ArXiv", "authors": ["Ilir  Capuni", "Veljko  Milutinovic"], "year": 2021, "n_citations": 0}
{"id": 5610333, "s2_id": "7af3992959ff24e369da7c514c041062f5249b1e", "title": "SpArch: Efficient Architecture for Sparse Matrix Multiplication", "abstract": "Generalized Sparse Matrix-Matrix Multiplication (SpGEMM) is a ubiquitous task in various engineering and scientific applications. However, inner product based SpGEMM introduces redundant input fetches for mismatched nonzero operands, while outer product based approach suffers from poor output locality due to numerous partial product matrices. Inefficiency in the reuse of either inputs or outputs data leads to extensive and expensive DRAM access. To address this problem, this paper proposes an efficient sparse matrix multiplication accelerator architecture, SpArch, which jointly optimizes the data locality for both input and output matrices. We first design a highly parallelized streaming-based merger to pipeline the multiply and merge stage of partial matrices so that partial matrices are merged on chip immediately after produced. We then propose a condensed matrix representation that reduces the number of partial matrices by three orders of magnitude and thus reduces DRAM access by 5.4x. We further develop a Huffman tree scheduler to improve the scalability of the merger for larger sparse matrices, which reduces the DRAM access by another 1.8x. We also resolve the increased input matrix read induced by the new representation using a row prefetcher with near-optimal buffer replacement policy, further reducing the DRAM access by 1.5x. Evaluated on 20 benchmarks, SpArch reduces the total DRAM access by 2.8x over previous state-of-the-art. On average, SpArch achieves 4x, 19x, 18x, 17x, 1285x speedup and 6x, 164x, 435x, 307x, 62x energy savings over OuterSpace, MKL, cuSPARSE, CUSP, and ARM Armadillo, respectively.", "venue": "2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Zhekai  Zhang", "Hanrui  Wang", "Song  Han", "William J. Dally"], "year": 2020, "n_citations": 52}
{"id": 5611897, "s2_id": "409c64378371eeceb32f4b48e3768057e6d6ef5b", "title": "FUSE: Fusing STT-MRAM into GPUs to Alleviate Off-Chip Memory Access Overheads", "abstract": "In this work, we propose FUSE, a novel GPU cache system that integrates spin-transfer torque magnetic random-access memory (STT-MRAM) into the on-chip L1D cache. FUSE can minimize the number of outgoing memory accesses over the interconnection network of GPU's multiprocessors, which in turn can considerably improve the level of massive computing parallelism in GPUs. Specifically, FUSE predicts a read-level of GPU memory accesses by extracting GPU runtime information and places write-once-read-multiple (WORM) data blocks into the STT-MRAM, while accommodating write-multiple data blocks over a small portion of SRAM in the L1D cache. To further reduce the off-chip memory accesses, FUSE also allows WORM data blocks to be allocated anywhere in the STT-MRAM by approximating the associativity with the limited number of tag comparators and I/O peripherals. Our evaluation results show that, in comparison to a traditional GPU cache, our proposed heterogeneous cache reduces the number of outgoing memory references by 32% across the interconnection network, thereby improving the overall performance by 217% and reducing energy cost by 53%.", "venue": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)", "authors": ["Jie  Zhang", "Myoungsoo  Jung", "Mahmut T. Kandemir"], "year": 2019, "n_citations": 3}
{"id": 5615260, "s2_id": "06b7fc5c9f6e8d67328dbb69fb4ae4da58975435", "title": "When to use 3D Die-Stacked Memory for Bandwidth-Constrained Big Data Workloads", "abstract": "Response time requirements for big data processing systems are shrinking. To meet this strict response time requirement, many big data systems store all or most of their data in main memory to reduce the access latency. Main memory capacities have grown, and systems with 2 TB of main memory capacity available today. However, the rate at which processors can access this data--the memory bandwidth--has not grown at the same rate. In fact, some of these big-memory systems can access less than 10% of their main memory capacity in one second (billions of processor cycles). \n3D die-stacking is one promising solution to this bandwidth problem, and industry is investing significantly in 3D die-stacking. We use a simple back-of-the-envelope-style model to characterize if and when the 3D die-stacked architecture is more cost-effective than current architectures for in-memory big data workloads. We find that die-stacking has much higher performance than current systems (up to 256x lower response times), and it does not require expensive memory over provisioning to meet real-time (10 ms) response time service-level agreements. However, the power requirements of the die-stacked systems are significantly higher (up to 50x) than current systems, and its memory capacity is lower in many cases. Even in this limited case study, we find 3D die-stacking is not a panacea. Today, die-stacking is the most cost-effective solution for strict SLAs and by reducing the power of the compute chip and increasing memory densities die-stacking can be cost-effective under other constraints in the future.", "venue": "ArXiv", "authors": ["Jason  Lowe-Power", "Mark D. Hill", "David A. Wood"], "year": 2016, "n_citations": 9}
{"id": 5615852, "s2_id": "cbc8737feaf51332b89e11e9f8e1fd9d70abeacd", "title": "Machines and Algorithms", "abstract": "I discuss the evolution of computer architectures with a focus on QCD and with reference to the interplay between architecture, engineering, data motion and algorithms. \nNew architectures are discussed and recent performance results are displayed. \nI also review recent progress in multilevel solver and integation algorithms.", "venue": "ArXiv", "authors": ["Peter A. Boyle"], "year": 2017, "n_citations": 8}
{"id": 5620401, "s2_id": "6236d9545a2ffd91e5abd6417f41b37827b81080", "title": "Architecting Optically-Controlled Phase Change Memory", "abstract": "Phase Change Memory (PCM) is an attractive candidate for main memory as it offers non-volatility and zero leakage power, while providing higher cell densities, longer data retention time, and higher capacity scaling compared to DRAM. In PCM, data is stored in the crystalline or amorphous state of the phase change material. The typical electrically-controlled PCM (EPCM), however, suffers from longer write latency and higher write energy compared to DRAM and limited multi-level cell (MLC) capacities. These challenges limit the performance of data-intensive applications running on computing systems with EPCMs. Recently, researchers demonstrated optically-controlled PCM (OPCM) cells, with support for 5 bits/cell in contrast to 2 bits/cell in EPCM. These OPCM cells can be accessed directly with optical signals that are multiplexed in high-bandwidthdensity silicon-photonic links. The higher MLC capacity in OPCM and the direct cell access using optical signals enable an increased read/write throughput and lower energy per access than EPCM. However, due to the direct cell access using optical signals, OPCM systems cannot be designed using conventional memory architecture. We need a complete redesign of the memory architecture that is tailored to the properties of OPCM technology. This paper presents the design of a unified network and main memory system called COSMOS that combines OPCM and silicon-photonic links to achieve high memory throughput. COSMOS is composed of a hierarchical multi-banked OPCM array with novel read and write access protocols. COSMOS uses an Electrical-Optical-Electrical (E-O-E) control unit to map standard DRAM read/write commands (sent in electrical domain) from the memory controller on to optical signals that access the OPCM cells. Our evaluation of a 2.5D-integrated system containing a processor and COSMOS demonstrates 2.14\u00d7 average speedup across graph and HPC workloads compared to an EPCM system. COSMOS consumes 3.8\u00d7 lower read energy-per-bit and 5.97\u00d7 lower write energy-per-bit compared to EPCM. COSMOS is the first non-volatile memory that provides comparable performance and energy consumption as DDR4 in addition to increased bit density, higher area efficiency and improved scalability.", "venue": "ArXiv", "authors": ["Aditya  Narayan", "Yvain  Thonnart", "Pascal  Vivet", "Ayse K. Coskun", "Ajay  Joshi"], "year": 2021, "n_citations": 0}
{"id": 5621341, "s2_id": "6d6029810d1c7990408a7b4d5e6a53c958577992", "title": "Limited Associativity Makes Concurrent Software Caches a Breeze", "abstract": "Software caches optimize the performance of diverse storage systems, databases and other software systems. Existing works on software caches automatically resort to fully associative cache designs. Our work shows that limited associativity caches are a promising direction for concurrent software caches. Specifically, we demonstrate that limited associativity enables simple yet efficient realizations of multiple cache management schemes that can be trivially parallelized. We show that the obtained hit ratio is usually similar to fully associative caches of the same management policy, but the throughput is improved by up to x5 compared to production-grade caching libraries, especially in multi-threaded executions.", "venue": "ArXiv", "authors": ["Dolev  Adas", "Gil  Einziger", "Roy  Friedman"], "year": 2021, "n_citations": 1}
{"id": 5622339, "s2_id": "8ff46a92e82d8618c65a1468c6200115ccffe038", "title": "Power Gating Structure for Reversible Programmable Logic Array", "abstract": "Throughout the world, the numbers of researchers or hardware designer struggle for the reducing of power dissipation in low power VLSI systems. This paper presented an idea of using the power gating structure for reducing the sub threshold leakage in the reversible system. This concept presented in the paper is entirely new and presented in the literature of reversible logics. By using the reversible logics for the digital systems, the energy can be saved up to the gate level implementation. But at the physical level designing of the reversible logics by the modern CMOS technology the heat or energy is dissipated due the sub-threshold leakage at the time of inactivity or standby mode. The Reversible Programming logic array (RPLA) is one of the important parts of the low power industrial applications and in this paper the physical design of the RPLA is presented by using the sleep transistor and the results is shown with the help of TINA- PRO software. The results for the proposed design is also compare with the CMOS design and shown that of 40.8% of energy saving. The Transient response is also produces in the paper for the switching activity and showing that the proposed design is much better that the modern CMOS design of the RPLA.", "venue": "ArXiv", "authors": ["Pradeep  Singla"], "year": 2016, "n_citations": 0}
{"id": 5627514, "s2_id": "5d204405713f402782fd3bf1b4b57ec96b8913ee", "title": "Reducing solid-state drive read latency by optimizing read-retry", "abstract": "3D NAND flash memory with advanced multi-level cell techniques provides high storage density, but suffers from significant performance degradation due to a large number of read-retry operations. Although the read-retry mechanism is essential to ensuring the reliability of modern NAND flash memory, it can significantly in-crease the read latency of an SSD by introducing multiple retry steps that read the target page again with adjusted read-reference voltage values. Through a detailed analysis of the read mechanism and rigorous characterization of 160 real 3D NAND flash memory chips, we find new opportunities to reduce the read-retry latency by exploiting two advanced features widely adopted in modern NAND flash-based SSDs: 1) the CACHE READ command and 2) strong ECC engine. First, we can reduce the read-retry latency using the advanced CACHE READ command that allows a NAND flash chip to perform consecutive reads in a pipelined manner. Second, there exists a large ECC-capability margin in the final retry step that can be used for reducing the chip-level read latency. Based on our new findings, we develop two new techniques that effectively reduce the read-retry latency: 1) Pipelined Read-Retry (PR\u00b2) and 2) Adaptive Read-Retry (AR\u00b2). PR\u00b2 reduces the latency of a read-retry operation by pipelining consecutive retry steps using the CACHE READ command. AR\u00b2 shortens the latency of each retry step by dynamically reducing the chip-level read latency depending on the current operating conditions that determine the ECC-capability margin. Our evaluation using twelve real-world workloads shows that our proposal improves SSD response time by up to 31.5% (17% on average)over a state-of-the-art baseline with only small changes to the SSD controller.", "venue": "ASPLOS", "authors": ["Jisung  Park", "Myungsuk  Kim", "Myoungjun  Chun", "Lois  Orosa", "Jihong  Kim", "Onur  Mutlu"], "year": 2021, "n_citations": 1}
{"id": 5630671, "s2_id": "bfd6de179949b03865aceba03893709e36d094c7", "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration", "abstract": "DNN accelerators are often developed and evaluated in isolation without considering the cross-stack, system-level effects in real-world environments. This makes it difficult to appreciate the impact of Systemon-Chip (SoC) resource contention, OS overheads, and programming-stack inefficiencies on overall performance/energy-efficiency. To address this challenge, we present Gemmini, an open-source, full-stack DNN accelerator generator. Gemmini generates a wide design-space of efficient ASIC accelerators from a flexible architectural template, together with flexible programming stacks and full SoCs with shared resources that capture system-level effects. Gemmini-generated accelerators have also been fabricated, delivering up to three orders-of-magnitude speedups over high-performance CPUs on various DNN benchmarks.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Hasan  Genc", "Seah  Kim", "Alon  Amid", "Ameer  Haj-Ali", "Vighnesh  Iyer", "Pranav  Prakash", "Jerry  Zhao", "Daniel  Grubb", "Harrison  Liew", "Howard  Mao", "Albert J. Ou", "Colin  Schmidt", "Samuel  Steffl", "John  Wright", "Ion  Stoica", "Jonathan  Ragan-Kelley", "Krste  Asanovic", "Borivoje  Nikolic", "Yakun Sophia Shao"], "year": 2021, "n_citations": 5}
{"id": 5631922, "s2_id": "406be75a74863e801046b501d4da7ca11e190302", "title": "Implementing a Scalable and Elastic Computing Environment Based on Cloud Containers", "abstract": "In this article we look at the potential of cloud containers and we provide some guidelines for companies and organisations that are starting to look at how to migrate their legacy infrastructure to something modern, reliable and scalable. We propose an architecture that has an excellent relationship between the cost of implementation and the benefits it can bring, based on the \u201dPilot Light\u201d topology. The services are reconfigured inside small docker containers and the workload is balanced using load balancers that allow horizontal autoscaling techniques to be exploited in the future. By generating additional containers and utilizing the possibilities given by load balancers, companies and network systems experts may model and calibrate infrastructures based on the projected number of users. Containers offer the opportunity to expand the infrastructure and increase processing capacity in a very short time. The proposed approach results in an easily maintainable and faulttolerant system that could help and simplify the work in particular of small and medium-sized organisations.", "venue": "ICCSA", "authors": ["Damiano  Perri", "Marco  Simonetti", "Sergio  Tasso", "Federico  Ragni", "Osvaldo  Gervasi"], "year": 2021, "n_citations": 0}
{"id": 5632050, "s2_id": "c1bb2afa0a1bb935a3c9263e5a11278bc5250fcb", "title": "Design and Implementation of Multistage Interconnection Networks for SoC Networks", "abstract": "In this paper the focus is on a family of Interconnection Networks (INs) known as Multistage Interconnection Networks (MINs). When it is exploited in Network-on-Chip (NoC) architecture designs, smaller circuit area, lower power consumption, less junctions and broader bandwidth can be achieved. Each MIN can be considered as an alternative for an NoC architecture design for its simple topology and easy scalability with low degree. This paper includes two major contributions. First, it compares the performance of seven prominent MINs (i.e. Omega, Butterfly, Flattened Butterfly, Flattened Baseline, Generalized Cube, Bene\\v{s} and Clos networks) based on 45nm-CMOS technology and under different types of Synthetic and Trace-driven workloads. Second, a network called Meta-Flattened Network (MFN), was introduced that can decrease the blocking probability by means of reduction the number of hops and increase the intermediate paths between stages. This is also led into significant decrease in power consumption.", "venue": "ArXiv", "authors": ["Mahsa  Moazez", "Farshad  Safaei", "Majid  Rezazadeh"], "year": 2012, "n_citations": 4}
{"id": 5638804, "s2_id": "bce7b421938347db4540376aa9e1a6a0ad486cab", "title": "Model-based Hardware Design for FPGAs using Folding Transformations based on Subcircuits", "abstract": "We present a tool flow and results for a model- based hardware design for FPGAs from Simulink descriptions which nicely integrates into existing environments. While current commercial tools do not exploit some high-level optimizations, we investigate the promising approach of using reusable subcircuits for folding transformations to control embedded multiplier usage and to optimize logic block usage. We show that resource improvements of up to 70% compared to the original model are possible, but it is also shown that subcircuit selection is a critical task. While our tool flow provides good results already, the investigation and optimization of subcircuit selection is clearly identified as an additional keypoint to extend high-level control on low-level FPGA mapping properties. I. INTRODUCTION The use of domain-specific modeling tools like Mat- lab/Simulink is a common way to describe (and test) data flow dominated applications, commonly denoted as model-based design. It has proven successful in automatic code generation which is the de-facto standard, e. g., in the automotive domain for many years. Up to 80% of the processor code in todays embedded control units is generated from Matlab/Simulink (1). The increasing demand for processing high sample frequencies recently lead to performance requirements that exceed the capabilities of embedded CPUs. FPGAs provide a solution for this problem as they yield the required computational power. However, typical sample frequencies are still much lower than the FPGAs' system clock frequency. This opens the opportunity to reduce FPGA resources by computing parts of the design using time-multiplexing while sharing the computation modules. A well known method to automatically transform a parallel data flow graph (DFG) into a sequential circuit is folding (2). During the folding transformation, common operators like, e. g., multipliers are implemented only once and shared by using multiplexers and additional registers. These resources and the required controller introduce an overhead which has to be lower than the resources saved due to sharing to gain any benefit (3). As the number of multiplexers directly scales with the number of inputs of shared operands, an overhead reduction could be obtained when operations are combined to larger subcircuits instead of equipping each single operation with multiplexers and registers. In principle, the same solution could be found with the right operator selection, scheduling and binding, but for this the right parameters have to be known. The use of common subcircuits instead removes multiplexers and registers per construction. A subcircuit as defined in this work corresponds to a subgraph of the DFG. The more frequently a subcircuit occurs, the more resources can be saved due to sharing. However, the larger a common subcircuit is, the smaller is typically its frequency of occurrence. In addition, several independent common subcircuits may exist in the design which may even partly overlap, leading to a large design space. The task is thus related to a subgraph par- titioning problem based on sets of isomorphic subgraphs. The target function on the other hand is based on implementation costs which are not directly related to individual subgraphs or to partition properties like size or number. Besides this, subcircuits can be used to reach a resource optimal point in the design space which meets the throughput requirements. The idea behind our investigation is to apply well known high-level transformations to the Simulink description target- ing resource reductions at the lowest register transfer/FPGA level. We present a tool flow which automatically utilizes the folding transformation to share arbitrary common subcircuits and show the benefits for this approach by a design space ex- ploration of several benchmark circuits. The main contribution of this work is an extensive analysis of the results which were generated during this exploration. Besides this, we show that the results obtained with our tool flow are always better in terms of slices than the folding transformations of the Matlab HDL coder (4), which was taken as state-of-the-art reference in this work.", "venue": "ArXiv", "authors": ["Konrad  M\u00f6ller", "Martin  Kumm", "Charles-Frederic  M\u00fcller", "Peter  Zipf"], "year": 2015, "n_citations": 3}
{"id": 5639986, "s2_id": "0ea47f109a6a1c922050c6f0a2a165cb493a0dd7", "title": "Transfer and Online Reinforcement Learning in STT-MRAM Based Embedded Systems for Autonomous Drones", "abstract": "In this paper we present an algorithm-hardware co-design for camera-based autonomous flight in small drones. We show that the large write-latency and write-energy for nonvolatile memory (NVM) based embedded systems makes them unsuitable for real-time reinforcement learning (RL). We address this by performing transfer learning (TL) on meta-environments and RL on the last few layers of a deep convolutional network. While the NVM stores the meta-model from TL, an on-die SRAM stores the weights of the last few layers. Thus all the real-time updates via RL are carried out on the SRAM arrays. This provides us with a practical platform with comparable performance as end-to-end RL and 83.4% lower energy per image frame.", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Insik  Yoon", "Malik Aqeel Anwar", "Titash  Rakshit", "Arijit  Raychowdhury"], "year": 2019, "n_citations": 4}
{"id": 5641457, "s2_id": "52f7a0e1ffa4f52a03a33b2a49657aac243db3c8", "title": "Metal Fillers as Potential Low Cost Countermeasure against Optical Fault Injection Attacks", "abstract": "Physically accessible devices such as sensor nodes in Wireless Sensor Networks or \"smart\" devices in the Internet of Things have to be resistant to a broad spectrum of physical attacks, for example to Side Channel Analysis and to Fault Injection attacks. In this work we concentrate on the vulnerability of ASICs to precise optical Fault Injection attacks. Here we propose to use metal fillers as potential low-cost countermeasure that may be effective against a broad spectrum of physical attacks. In our future work we plan to evaluate different methods of metal fillers placement, to select an effective one and to integrate it as additional design rules into automated design flows.", "venue": "2020 IEEE East-West Design & Test Symposium (EWDTS)", "authors": ["Dmytro  Petryk", "Zoya  Dyka", "Jens  Katzer", "Peter  Langend\u00f6rfer"], "year": 2020, "n_citations": 2}
{"id": 5646734, "s2_id": "fd129d4e0f85194ef3a77f924b5e1df74f35a918", "title": "Designing Hardware/Software Systems for Embedded High-Performance Computing", "abstract": "In this work, we propose an architecture and methodology to design hardware/software systems for high-performance embedded computing on FPGA. The hardware side is based on a many-core architecture whose design is generated automatically given a set of architectural parameters. Both the architecture and the methodology were evaluated running dense matrix multiplication and sparse matrix-vector multiplication on a ZYNQ-7020 FPGA platform. The results show that using a system-level design of the system avoids complex hardware design and still provides good performance results.", "venue": "ArXiv", "authors": ["M\u00e1rio P. V\u00e9stias", "Rui Policarpo Duarte", "Hor\u00e1cio C. Neto"], "year": 2015, "n_citations": 0}
{"id": 5648582, "s2_id": "997c887d8701d9e86e9ff9104c4c3e124d1641d8", "title": "Extending High-Level Synthesis for Task-Parallel Programs", "abstract": "C/C++/OpenCL-based high-level synthesis (HLS) becomes more and more popular for field-programmable gate array (FPGA) accelerators in many application domains in recent years, thanks to its competitive quality of result (QoR) and short development cycle compared with the traditional register-transfer level (RTL) design approach. Yet, limited by the sequential C semantics, it remains challenging to adopt the same highly productive high-level programming approach in many other application domains, where coarse-grained tasks run in parallel and communicate with each other at a fine-grained level. While current HLS tools support task-parallel programs, the productivity is greatly limited in the code development, correctness verification, and QoR tuning cycles, due to the poor programmability, restricted software simulation, and slow code generation, respectively. Such limited productivity often defeats the purpose of HLS and hinder programmers from adopting HLS for task-parallel FPGA accelerators. In this paper, we extend the HLS C++ language and present a fully automated framework with programmer-friendly interfaces, universal software simulation, and fast code generation to overcome these limitations. Experimental results based on a wide range of real-world task-parallel programs show that, on average, the lines of kernel and host code are reduced by 22% and 51%, respectively, which considerably improves the programmability. The correctness verification and the iterative QoR tuning cycles are both greatly accelerated by 3.2\u00d7 and 6.8\u00d7, respectively.", "venue": "FPGA", "authors": ["Yuze  Chi", "Licheng  Guo", "Young-kyu  Choi", "Jie  Wang", "Jason  Cong"], "year": 2021, "n_citations": 4}
{"id": 5650730, "s2_id": "1e43cff50e40e8169bf87cdad611c85613a37b4f", "title": "DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates", "abstract": "CPU simulators are useful tools for modeling CPU execution behavior. However, they suffer from inaccuracies due to the cost and complexity of setting their fine-grained parameters, such as the latencies of individual instructions. This complexity arises from the expertise required to design benchmarks and measurement frameworks that can precisely measure the values of parameters at such fine granularity. In some cases, these parameters do not necessarily have a physical realization and are therefore fundamentally approximate, or even unmeasurable.In this paper we present DiffTune, a system for learning the parameters of x86 basic block CPU simulators from coarse-grained end-to-end measurements. Given a simulator, DiffTune learns its parameters by first replacing the original simulator with a differentiable surrogate, another function that approximates the original function; by making the surrogate differentiable, DiffTune is then able to apply gradient-based optimization techniques even when the original function is non-differentiable, such as is the case with CPU simulators. With this differentiable surrogate, DiffTune then applies gradient-based optimization to produce values of the simulator\u2019s parameters that minimize the simulator\u2019s error on a dataset of ground truth end-to-end performance measurements. Finally, the learned parameters are plugged back into the original simulator.DiffTune is able to automatically learn the entire set of microarchitecture-specific parameters within the Intel x86 simulation model of llvm-mca, a basic block CPU simulator based on LLVM\u2019s instruction scheduling model. DiffTune\u2019s learned parameters lead llvm-mca to an average error that not only matches but lowers that of its original, expert-provided parameter values.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Alex  Renda", "Yishen  Chen", "Charith  Mendis", "Michael  Carbin"], "year": 2020, "n_citations": 10}
{"id": 5653795, "s2_id": "b68413a4fa532d4c2546d15cf07885eaa310b4ae", "title": "Generic and universal parallel matrix summation with a flexible compression goal for Xilinx FPGAs", "abstract": "Bit matrix compression is a highly relevant operation in computer arithmetic. Essentially being a multi-operand addition, it is the key operation behind fast multiplication and many higher-level operations such as multiply-accumulate, the computation of the dot product or the implementation of FIR filters. Compressor implementations have been constantly evolving for greater efficiency both in general and in the context of concrete applications or specific implementation technologies. This paper is building on this history and describes a generic implementation of a bit matrix compressor for Xilinx FPGAs, which does not require a generator tool. It contributes FPGA-oriented metrics for the evaluation of elementary parallel bit counters, a systematic analysis and partial decomposition of previously proposed counters and a fully implemented construction heuristic with a flexible compression target matching the device capabilities. The generic implementation is agnostic of the aspect ratio of the input matrix and can be used for multiplication the same way as it can be for single-column population count operations.", "venue": "2017 27th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Thomas B. Preu\u00dfer"], "year": 2017, "n_citations": 9}
{"id": 5654248, "s2_id": "a8b65a2d5f86eb174553442771c07bdcb485744a", "title": "Simultaneous reduction of dynamic and static power in scan structures", "abstract": "Power dissipation during test is a major challenge in testing integrated circuits. Dynamic power has been the dominant part of power dissipation in CMOS circuits, however, in future technologies the static portion of power dissipation will outreach the dynamic portion. This paper proposes an efficient technique to reduce both dynamic and static power dissipation in scan structures. Scan cell outputs which are not on the critical path(s) are multiplexed to fixed values during scan mode. These constant values and primary inputs are selected such that the transitions occurring on nonmultiplexed scan cells are suppressed and the leakage current during scan mode is decreased. A method for finding these vectors is also proposed. The effectiveness of this technique is proved by experiments performed on ISCAS89 benchmark circuits.", "venue": "Design, Automation and Test in Europe", "authors": ["Shervin  Sharifi", "Javid  Jaffari", "Mohammad  Hosseinabady", "Ali  Afzali-Kusha", "Zainalabedin  Navabi"], "year": 2005, "n_citations": 40}
{"id": 5659290, "s2_id": "255fb549ae859b86fa7f84a5705369598431866c", "title": "PowerNet: Transferable Dynamic IR Drop Estimation via Maximum Convolutional Neural Network", "abstract": "IR drop is a fundamental constraint required by almost all chip designs. However, its evaluation usually takes a long time that hinders mitigation techniques for fixing its violations. In this work, we develop a fast dynamic IR drop estimation technique, named PowerNet, based on a convolutional neural network (CNN). It can handle both vector-based and vectorless IR analyses. Moreover, the proposed CNN model is general and transferable to different designs. This is in contrast to most existing machine learning (ML) approaches, where a model is applicable only to a specific design. Experimental results show that PowerNet outperforms the latest ML method by 9% in accuracy for the challenging case of vectorless IR drop and achieves a 30\u00d7 speedup compared to an accurate IR drop commercial tool. Further, a mitigation tool guided by PowerNet reduces IR drop hotspots by 26% and 31% on two industrial designs, respectively, with very limited modification on their power grids.", "venue": "2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Zhiyao  Xie", "Haoxing  Ren", "Brucek  Khailany", "Ye  Sheng", "Santosh  Santosh", "Jiang  Hu", "Yiran  Chen"], "year": 2020, "n_citations": 13}
{"id": 5670290, "s2_id": "ae64adf6fc9df5a3b24bd2c5152cda68323deb81", "title": "Modeling Deep Learning Accelerator Enabled GPUs", "abstract": "The efficacy of deep learning has resulted in its use in a growing number of applications. The Volta graphics processor unit (GPU) architecture from NVIDIA introduced a specialized functional unit, the \u201ctensor core\u201d, that helps meet the growing demand for higher performance for deep learning. In this paper we study the design of the tensor cores in NVIDIA's Volta and Turing architectures. We further propose an architectural model for the tensor cores in Volta. When implemented a GPU simulator, GPGPU-Sim, our tensor core model achieves 99.6% correlation versus an NVIDIA Titan V GPU in terms of average instructions per cycle when running tensor core enabled GEMM workloads. We also describe support added to enable GPGPU-Sim to run CUTLASS, an open-source CUDA C++ template library providing customizable GEMM templates that utilize tensor cores.", "venue": "2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Md Aamir Raihan", "Negar  Goli", "Tor M. Aamodt"], "year": 2019, "n_citations": 31}
{"id": 5674124, "s2_id": "08daf8dfa01b31d8c486c31eeb39416f7731c68c", "title": "The Accuracy and Efficiency of Posit Arithmetic", "abstract": "Motivated by the increasing interest in the posit numeric format, in this paper we evaluate the accuracy and efficiency of posit arithmetic in contrast to the traditional IEEE 754 32-bit floating-point (FP32) arithmetic. We first design and implement a Posit Arithmetic Unit (PAU), called POSAR, with flexible bit-sized arithmetic suitable for applications that can trade accuracy for savings in chip area. Next, we analyze the accuracy and efficiency of POSAR with a series of benchmarks including mathematical computations, ML kernels, NAS Parallel Benchmarks (NPB), and Cifar-10 CNN. This analysis is done on our implementation of POSAR integrated into a RISC-V Rocket Chip core in comparison with the IEEE 754-based Floting Point Unit (FPU) of Rocket Chip. Our analysis shows that POSAR can outperform the FPU, but the results are not spectacular. For NPB, 32-bit posit achieves better accuracy than FP32 and improves the execution by up to 2%. However, POSAR with 32-bit posit needs 30% more FPGA resources compared to the FPU. For classic ML algorithms, we find that 8-bit posits are not suitable to replace FP32 because they exhibit low accuracy leading to wrong results. Instead, 16-bit posit offers the best option in terms of accuracy and efficiency. For example, 16-bit posit achieves the same Top-1 accuracy as FP32 on a Cifar-10 CNN with a speedup of 18%.", "venue": "2021 IEEE 39th International Conference on Computer Design (ICCD)", "authors": ["Stefan Dan Ciocirlan", "Dumitrel  Loghin", "Lavanya  Ramapantulu", "Nicolae  Tapus", "Yong Meng Teo"], "year": 2021, "n_citations": 2}
{"id": 5676048, "s2_id": "de6a490b8380088ec9067251a5afef059e634fe0", "title": "TraceTracker: Hardware/software co-evaluation for large-scale I/O workload reconstruction", "abstract": "Block traces are widely used for system studies, model verifications, and design analyses in both industry and academia. While such traces include detailed block access patterns, existing trace-driven research unfortunately often fails to find true-north due to a lack of runtime contexts such as user idle periods and system delays, which are fundamentally linked to the characteristics of target storage hardware. In this work, we propose TraceTracker, a novel hardware/software co-evaluation method that allows users to reuse a broad range of the existing block traces by keeping most their execution contexts and user scenarios while adjusting them with new system information. Specifically, our TraceTracker's software evaluation model can infer CPU burst times and user idle periods from old storage traces, whereas its hardware evaluation method remasters the storage traces by interoperating the inferred time information, and updates all inter-arrival times by making them aware of the target storage system. We apply the proposed co-evaluation model to 577 traces, which were collected by servers from different institutions and locations a decade ago, and revive the traces on a high-performance flash-based storage array. The evaluation results reveal that the accuracy of the execution contexts reconstructed by TraceTracker is on average 99% and 96% with regard to the frequency of idle operations and the total idle periods, respectively.", "venue": "2017 IEEE International Symposium on Workload Characterization (IISWC)", "authors": ["Miryeong  Kwon", "Jie  Zhang", "Gyuyoung  Park", "Wonil  Choi", "David  Donofrio", "John  Shalf", "Mahmut T. Kandemir", "Myoungsoo  Jung"], "year": 2017, "n_citations": 14}
{"id": 5676421, "s2_id": "80c53bf932b1bfa7b9f1452dd1e7571c946a59b0", "title": "High-Resolution Waveform Capture Device on a Cyclone-V FPGA", "abstract": "We introduce the waveform capture device (WCD), a flexible measurement system capable of recording complex digital signals on trillionth-of-a-second (ps) time scales. The WCD is implemented via modular code on an off-the-shelf field-programmable gate-array (FPGA, Intel/Altera Cyclone V), and incorporates both time-to-digital converter (TDC) and digital storage oscilloscope (DSO) functionality. The device captures a waveform by taking snapshots of a signal as it propagates down an ultra-fast transmission line known as a carry chain (CC). It is calibrated via a novel dynamic phase-shifting (DPS) method that requires substantially less data and resources than the state-of-the-art. Using DPS, we find the measurement resolution - or mean propagation delay from one CC element to the next - to be 4.91\u00b10.04 ps (4.54\u00b10.02 ps) for a pulse of logic high (low). Similarly, we find the single-shot precision - or mean error on the timing of the waveform - to be 29.52 ps (27.14 ps) for pulses of logic high (low). We verify these findings by reproducing commercial oscilloscope measurements of asynchronous ring-oscillators on FPGAs, finding the mean pulse width to be 0.240 \u00b1 0.002 ns per inverter gate. Finally, we present a careful analysis of design constraints, introduce a novel error correction algorithm, and sketch a simple extension to the analog domain. We also provide the Verilog code instantiating our design\u2019s hardware primitives in an Appendix, and make our FPGA interfacing methods available as an open-source Python library at https://github.com/Noeloikeau/fpyga.", "venue": "IEEE Access", "authors": ["Noeloikeau F. Charlot", "Daniel J. Gauthier", "Andrew  Pomerance"], "year": 2021, "n_citations": 0}
{"id": 5680078, "s2_id": "f77a451dcd82fb6a35f079ac50bda2dfe3ef31a7", "title": "HARP: Practically and Effectively Identifying\u00a0Uncorrectable\u00a0Errors\u00a0in\u00a0Memory\u00a0Chips That Use On-Die Error-Correcting Codes", "abstract": "Aggressive storage density scaling in modern main memories causes increasing error rates that are addressed using error-mitigation techniques. State-of-the-art techniques for addressing high error rates identify and repair bits that are at risk of error from within the memory controller. Unfortunately, modern main memory chips internally use on-die error correcting codes (on-die ECC) that obfuscate the memory controller\u2019s view of errors, complicating the process of identifying at-risk bits (i.e., error profiling). To understand the problems that on-die ECC causes for error profiling, we analytically study how on-die ECC changes the way that memory errors appear outside of the memory chip (e.g., to the memory controller). We show that on-die ECC introduces statistical dependence between errors in different bit positions, raising three key challenges for practical and effective error profiling: on-die ECC (1) exponentially increases the number of at-risk bits the profiler must identify; (2) makes individual at-risk bits more difficult to identify; and (3) interferes with commonly-used memory data patterns that are designed to make at-risk bits easier to identify. To address the three challenges, we introduce Hybrid Active-Reactive Profiling (HARP), a new error profiling algorithm that rapidly achieves full coverage of at-risk bits based on two key insights. First, errors that on-die ECC fails to correct have two sources: (1) direct errors from raw bit errors in the data portion of the ECC word and (2) indirect errors that on-die ECC introduces when facing uncorrectable errors. Second, the maximum number of indirect errors that can occur concurrently is limited to the correction capability of on-die ECC. HARP\u2019s key idea is to first identify all bits at risk of direct errors using existing profiling techniques with the help of small modifications to the on-die ECC mechanism. Then, a secondary ECC within the memory controller with correction capability equal to or greater than that of on-die ECC can safely identify bits at-risk of indirect errors, if and when they fail. We evaluate HARP in simulation relative to two state-of-the-art baseline error profiling algorithms. We show that HARP achieves full coverage of all at-risk bits faster (e.g., 99th-percentile coverage 20.6%/36.4%/52.9%/62.1% faster, on average, given 2/3/4/5 raw bit errors per ECC word) than the baseline algorithms, which sometimes fail to achieve full coverage. We perform a case study of how each profiler impacts the system\u2019s overall bit error rate (BER) when using a repair mechanism to tolerate DRAM data-retention errors. We show that HARP identifies all errors faster than the best-performing baseline algorithm (e.g., by 3.7 \u00d7 for a raw per-bit error probability of 0.75). We conclude that HARP effectively overcomes the three error profiling challenges introduced by on-die ECC.", "venue": "MICRO", "authors": ["Minesh  Patel", "Geraldo F. Oliveira", "Onur  Mutlu"], "year": 2021, "n_citations": 1}
{"id": 5685907, "s2_id": "28b69138ea97cc83fbd012bae3853662090f74f4", "title": "LW-GCN: A Lightweight FPGA-based Graph Convolutional Network Accelerator", "abstract": "Graph convolutional networks (GCNs) have been introduced to effectively process non-euclidean graph data. However, GCNs incur large amounts of irregularity in computation and memory access, which prevents efficient use of traditional neural network accelerators. Moreover, existing dedicated GCN accelerators demand high memory volumes and are difficult to implement onto resource limited edge devices. In this work, we propose LW-GCN, a lightweight FPGA-based accelerator with a software-hardware co-designed process to tackle irregularity in computation and memory access in GCN inference. LW-GCN decomposes the main GCN operations into sparse-dense matrix multiplication (SDMM) and dense matrix multiplication (DMM). We propose a novel compression format to balance workload across PEs and prevent data hazards. Moreover, we apply data quantization and workload tiling, and map both SDMM and DMM of GCN inference onto a uniform architecture on resource limited hardware. Evaluation on GCN and GraphSAGE are performed on Xilinx Kintex-7 FPGA with three popular datasets. Compared to existing CPU, GPU, and state-of-the-art FPGA-based accelerator, LW-GCN reduces latency by up to 60x, 12x and 1.7x and increases power efficiency by up to 912x., 511x and 3.87x, respectively. Furthermore, compared with NVIDIA\u2019s latest edge GPU Jetson Xavier NX, LW-GCN achieves speedup and energy savings of 32x and 84x, respectively.", "venue": "ArXiv", "authors": ["Zhuofu  Tao", "Chen  Wu", "Yuan  Liang", "Lei  He"], "year": 2021, "n_citations": 0}
{"id": 5704196, "s2_id": "a6cc5c7503e23f2ca6b9e4ad95a9a1418aa733da", "title": "SECDA: Efficient Hardware/Software Co-Design of FPGA-based DNN Accelerators for Edge Inference", "abstract": "Edge computing devices inherently face tight resource constraints, which is especially apparent when deploying Deep Neural Networks (DNN) with high memory and compute demands. FPGAs are commonly available in edge devices. Since these reconfigurable circuits can achieve higher throughput and lower power consumption than general purpose processors, they are especially well-suited for DNN acceleration. However, existing solutions for designing FPGA-based DNN accelerators for edge devices come with high development overheads, given the cost of repeated FPGA synthesis passes, reimplementation in a Hardware Description Language (HDL) of the simulated design, and accelerator system integration. In this paper we propose SECDA, a new hardware/software co-design methodology to reduce design time of optimized DNN inference accelerators on edge devices with FPGAs. SECDA combines cost-effective SystemC simulation with hardware execution, streamlining design space exploration and the development process via reduced design evaluation time. As a case study, we use SECDA to efficiently develop two different DNN accelerator designs on a PYNQ-Z1 board, a platform that includes an edge FPGA. We quickly and iteratively explore the system's hardware/software stack, while identifying and mitigating performance bottlenecks. We evaluate the two accelerator designs with four common DNN models, achieving an average performance speedup across models of up to 3.5\u00d7 with a 2.9\u00d7 reduction in energy consumption over CPU-only inference. Our code is available at https://github.com/gicLAB/SECDA", "venue": "2021 IEEE 33rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)", "authors": ["Jude  Haris", "Perry  Gibson", "Jos'e  Cano", "Nicolas Bohm Agostini", "David  Kaeli"], "year": 2021, "n_citations": 0}
{"id": 5706840, "s2_id": "68dc599735e8a56f7af618938e7c97f570236a97", "title": "Layering the monitoring action for improved flexibility and overhead control: work-in-progress", "abstract": "With the diffusion of complex heterogeneous platforms and their need of characterization, monitoring the system gained increasing interest. This work proposes a framework to build custom and modular monitoring systems, flexible enough to face the heterogeneity of modern platforms, offering a predictable HW/SW impact.", "venue": "2020 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)", "authors": ["Giacomo  Valente", "Tiziana  Fanni", "Carlo  Sau", "Francesco Di Battista"], "year": 2020, "n_citations": 2}
{"id": 5711341, "s2_id": "4b3196cf16d9ec995bddbc7974823ccefbe9e2bc", "title": "Design-Phase Buffer Allocation for Post-Silicon Clock Binning by Iterative Learning", "abstract": "At submicrometer manufacturing technology nodes, process variations affect circuit performance significantly. To counter these variations, engineers are reserving more timing margin to maintain yield, leading to an unaffordable overdesign. Most of these margins, however, are wasted after manufacturing, because process variations cause only some chips to be really slow, while other chips can easily meet given timing specifications. To reduce this pessimism, we can reserve less timing margin and tune failed chips after manufacturing with clock buffers to make them meet timing specifications. With this post-silicon clock tuning, critical paths can be balanced with neighboring paths in each chip specifically to counter the effect of process variations. Consequently, chips with timing failures can be rescued and the yield can thus be improved. This is specially useful in high-performance designs, e.g., high-end CPUs, where clock binning makes chips with higher performance much more profitable. In this paper, we propose a method to determine where to insert post-silicon tuning buffers during the design phase to improve the overall profit with clock binning. This method learns the buffer locations with a Sobol sequence iteratively and reduces the buffer ranges afterward with tuning concentration and buffer grouping. Experimental results demonstrate that the proposed method can achieve a profit improvement of about 14% on average and up to 26%, with only a small number of tuning buffers inserted into the circuit.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Grace Li Zhang", "Bing  Li", "Jinglan  Liu", "Yiyu  Shi", "Ulf  Schlichtmann"], "year": 2018, "n_citations": 18}
{"id": 5719235, "s2_id": "23001e0d175e952fbb753712dfa7bf5ee1e32b1a", "title": "Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory Machines", "abstract": "Multi-socket machines with 1-100 TBs of physical memory are becoming prevalent. Applications running on such multi-socket machines suffer non-uniform bandwidth and latency when accessing physical memory. Decades of research have focused on data allocation and placement policies in NUMA settings, but there have been no studies on the question of how to place page-tables amongst sockets. We make the case for explicit page-table allocation policies and show that page-table placement is becoming crucial to overall performance. We propose Mitosis to mitigate NUMA effects on page-table walks by transparently replicating and migrating page-tables across sockets without application changes. This reduces the frequency of accesses to remote NUMA nodes when performing page-table walks. Mitosis uses two components: (i) a mechanism to efficiently enable and (ii) policies to effectively control -- page-table replication and migration. We implement Mitosis in Linux and evaluate its benefits on real hardware. Mitosis improves performance for large-scale multi-socket workloads by up to 1.34x by replicating page-tables across sockets. Moreover, it improves performance by up to 3.24x in cases when the OS migrates a process across sockets by enabling cross-socket page-table migration.", "venue": "ASPLOS", "authors": ["Reto  Achermann", "Ashish  Panwar", "Abhishek  Bhattacharjee", "Timothy  Roscoe", "Jayneel  Gandhi"], "year": 2020, "n_citations": 14}
{"id": 5719528, "s2_id": "5e0856fdd629317d6226e81628a80b98f14d4660", "title": "Building fast Bayesian computing machines out of intentionally stochastic, digital parts", "abstract": "The brain interprets ambiguous sensory information faster and more reliably than modern computers, using neurons that are slower and less reliable than logic gates. But Bayesian inference, which underpins many computational models of perception and cognition, appears computationally challenging even given modern transistor speeds and energy budgets. The computational principles and structures needed to narrow this gap are unknown. Here we show how to build fast Bayesian computing machines using intentionally stochastic, digital parts, narrowing this efficiency gap by multiple orders of magnitude. We find that by connecting stochastic digital components according to simple mathematical rules, one can build massively parallel, low precision circuits that solve Bayesian inference problems and are compatible with the Poisson firing statistics of cortical neurons. We evaluate circuits for depth and motion perception, perceptual learning and causal reasoning, each performing inference over 10,000+ latent variables in real time - a 1,000x speed advantage over commodity microprocessors. These results suggest a new role for randomness in the engineering and reverse-engineering of intelligent computation.", "venue": "ArXiv", "authors": ["Vikash K. Mansinghka", "Eric  Jonas"], "year": 2014, "n_citations": 19}
{"id": 5720502, "s2_id": "9e4e5759d2a3aaa03c0cca601f29d3f733395b10", "title": "A Study of Runtime Adaptive Prefetching for STTRAM L1 Caches", "abstract": "Spin- Transfer Torque RAM (STTRAM) is a promising alternative to SRAM in on-chip caches due to several advantages. These advantages include non-volatility, low leakage, high integration density, and CMOS compatibility. Prior studies have shown that relaxing and adapting the STTRAM retention time to runtime application needs can substantially reduce overall cache energy without significant latency overheads, due to the lower STTRAM write energy and latency in shorter retention times. In this paper, as a first step towards efficient prefetching across the STTRAM cache hierarchy, we study prefetching in reduced retention STTRAM L1 caches. Using SPEC CPU 2017 benchmarks, we analyze the energy and latency impact of different prefetch distances in different STTRAM cache retention times for different applications. We show that expired _unused _prefetches\u2190 the number of unused prefetches expired by the reduced retention time STTRAM cache-can accurately determine the best retention time for energy consumption and access latency. This new metric can also provide insights into the best prefetch distance for memory bandwidth consumption and prefetch accuracy. Based on our analysis and insights, we propose Prefetch-Aware Retention time Tuning (PART) and Retention time-based Prefetch Control (RPC). Compared to a base STTRAM cache, PART and RPC collectively reduced the average cache energy and latency by 22.24 % and 24.59 %, respectively. When the base architecture was augmented with the state-of-the-art near-side prefetch throttling (NST), PART+RPC reduced the average cache energy and latency by 3.50 % and 3.59 %, respectively, and reduced the hardware overhead by 54.55 %.", "venue": "2020 IEEE 38th International Conference on Computer Design (ICCD)", "authors": ["Kyle  Kuan", "Tosiron  Adegbija"], "year": 2020, "n_citations": 1}
{"id": 5720511, "s2_id": "c3ad9c32e43b968d8a7a68bd9f3da507ceb4f6d4", "title": "Die-Stacked DRAM: Memory, Cache, or MemCache?", "abstract": "Die-stacked DRAM is a promising solution for satisfying the ever-increasing memory bandwidth requirements of multi-core processors. Manufacturing technology has enabled stacking several gigabytes of DRAM modules on the active die, thereby providing orders of magnitude higher bandwidth as compared to the conventional DIMM-based DDR memories. Nevertheless, die-stacked DRAM, due to its limited capacity, cannot accommodate entire datasets of modern big-data applications. Therefore, prior proposals use it either as a sizable memory-side cache or as a part of the software-visible main memory. Cache designs can adapt themselves to the dynamic variations of applications but suffer from the tag storage/latency/bandwidth overhead. On the other hand, memory designs eliminate the need for tags, and hence, provide efficient access to data, but are unable to capture the dynamic behaviors of applications due to their static nature. \nIn this work, we make a case for using the die-stacked DRAM partly as main memory and partly as a cache. We observe that in modern big-data applications there are many hot pages with a large number of accesses. Based on this observation, we propose to use a portion of the die-stacked DRAM as main memory to host hot pages, enabling serving a significant number of the accesses from the high-bandwidth DRAM without the overhead of tag-checking, and manage the rest of the DRAM as a cache, for capturing the dynamic behavior of applications. In this proposal, a software procedure pre-processes the application and determines hot pages, then asks the OS to map them to the memory portion of the die-stacked DRAM. The cache portion of the die-stacked DRAM is managed by hardware, caching data allocated in the off-chip memory.", "venue": "ArXiv", "authors": ["Mohammad  Bakhshalipour", "HamidReza  Zare", "Pejman  Lotfi-Kamran", "Hamid  Sarbazi-Azad"], "year": 2018, "n_citations": 7}
{"id": 5720971, "s2_id": "bb1d19aa6b9b3b3aec2f85e98c70097e3e74e7c6", "title": "A floating point division unit based on Taylor-Series expansion algorithm and Iterative Logarithmic Multiplier", "abstract": "Floating point division, even though being an infrequent operation in the traditional sense, is indis- pensable when it comes to a range of non-traditional applications such as K-Means Clustering and QR Decomposition just to name a few. In such applications, hardware support for floating point division would boost the performance of the entire system. In this paper, we present a novel architecture for a floating point division unit based on the Taylor-series expansion algorithm. We show that the Iterative Logarithmic Multiplier is very well suited to be used as a part of this architecture. We propose an implementation of the powering unit that can calculate an odd power and an even power of a number simultaneously, meanwhile having little hardware overhead when compared to the Iterative Logarithmic Multiplier.", "venue": "ArXiv", "authors": ["Riyansh K. Karani", "Akash K. Rana", "Dhruv H. Reshamwala", "Kishore  Saldanha"], "year": 2017, "n_citations": 0}
{"id": 5721609, "s2_id": "cde13fa528f1f1fb1915a3b7e25efd897635484d", "title": "Handover Experiments with UAVs: Software Radio Tools and Experimental Research Platform", "abstract": "Mobility management is the key feature of cellular networks. When integrating unmanned aerial vehicles (UAVs) into cellular networks, their cell association needs to be carefully managed for coexistence with other cellular users. UAVs move in three dimensions and may traverse several cells on their flight path, and so may be subject to several handovers. In order to enable research on mobility management with UAV users, this paper describes the design, implementation, and testing methodology for handover experiments with aerial users. We leverage software-defined radios (SDRs) and implement a series of tools for preparing the experiment in the laboratory and for taking it outdoors for field testing. We use solely commercial off-the-shelf hardware, open-source software, and an experimental license to enable reproducible and scalable experiments. Our initial outdoor results with two SDR base stations connected to an open-source software core network, implementing the 4G long-term evolution protocol, and one low altitude UAV user equipment demonstrate the handover process.", "venue": "WiNTECH@MOBICOM", "authors": ["Keith  Powell", "Andrew L. Yingst", "Talha Faizur Rahman", "Vuk  Marojevic"], "year": 2021, "n_citations": 0}
{"id": 5721628, "s2_id": "86231647995501937e4b9597c0b457b47f33d753", "title": "A Fast-and-Effective Early-Stage Multi-level Cache Optimization Method Based on Reuse-Distance Analysis", "abstract": "In this paper we propose a practical and effective approach allowing designers to optimize multi-level cache size at early system design phase. Our key contribution is to generalize the reuse distance analysis method and develop an effective and practical cache design optimization approach. We adopt a simple scanning search method to locate optimal cache solution in terms of cache size, power consumption or average data access delay. The proposed approach is particularly useful for early-phase system designers and is verified to be 150 to 250 times faster than the traditional simulation-based approach. In addition, we also introduce a simplified analytical model and provide designers insights about how cache design parameters may affect the expected results. As a result, designers can make adequate decision at early system design phase. Keywords\u2014multi-level caches; reuse distance;", "venue": "ArXiv", "authors": ["Cheng-Lin  Tsai", "Ren-Song  Tsay"], "year": 2021, "n_citations": 1}
{"id": 5721945, "s2_id": "9a20a15950b897a57f48e9d6c3cbb6d5a40fce06", "title": "SPRING: A Sparsity-Aware Reduced-Precision Monolithic 3D CNN Accelerator Architecture for Training and Inference", "abstract": "CNNs outperform traditional machine learning algorithms across a wide range of applications. However, their computational complexity makes it necessary to design efficient hardware accelerators. Most CNN accelerators focus on exploring dataflow styles that exploit computational parallelism. However, potential performance speedup from sparsity has not been adequately addressed. The computation and memory footprint of CNNs can be significantly reduced if sparsity is exploited in network evaluations. To take advantage of sparsity, some accelerator designs explore sparsity encoding and evaluation on CNN accelerators. However, sparsity encoding is just performed on activation or weight and only in inference. It has been shown that activation and weight also have high sparsity levels during training. Hence, sparsity-aware computation should also be considered in training. To further improve performance and energy efficiency, some accelerators evaluate CNNs with limited precision. However, this is limited to the inference since reduced precision sacrifices network accuracy if used in training. In addition, CNN evaluation is usually memory-intensive, especially in training. In this paper, we propose SPRING, a SParsity-aware Reduced-precision Monolithic 3D CNN accelerator for trainING and inference. SPRING supports both CNN training and inference. It uses a binary mask scheme to encode sparsities in activation and weight. It uses the stochastic rounding algorithm to train CNNs with reduced precision without accuracy loss. To alleviate the memory bottleneck in CNN evaluation, especially in training, SPRING uses an efficient monolithic 3D NVM interface to increase memory bandwidth. Compared to GTX 1080 Ti, SPRING achieves 15.6X, 4.2X and 66.0X improvements in performance, power reduction, and energy efficiency, respectively, for CNN training, and 15.5X, 4.5X and 69.1X improvements for inference.", "venue": "ArXiv", "authors": ["Ye  Yu", "Niraj K. Jha"], "year": 2019, "n_citations": 3}
{"id": 5727399, "s2_id": "1eb9e5fbc5406b2cbf853bef5d5165a7c2c33ce1", "title": "TriCheck: Memory Model Verification at the Trisection of Software, Hardware, and ISA", "abstract": "Memory consistency models (MCMs) which govern inter-module interactions in a shared memory system, are a significant, yet often under-appreciated, aspect of system design. MCMs are defined at the various layers of the hardware-software stack, requiring thoroughly verified specifications, compilers, and implementations at the interfaces between layers. Current verification techniques evaluate segments of the system stack in isolation, such as proving compiler mappings from a high-level language (HLL) to an ISA or proving validity of a microarchitectural implementation of an ISA. This paper makes a case for full-stack MCM verification and provides a toolflow, TriCheck, capable of verifying that the HLL, compiler, ISA, and implementation collectively uphold MCM requirements. The work showcases TriCheck's ability to evaluate a proposed ISA MCM in order to ensure that each layer and each mapping is correct and complete. Specifically, we apply TriCheck to the open source RISC-V ISA [55], seeking to verify accurate, efficient, and legal compilations from C11. We uncover under-specifications and potential inefficiencies in the current RISC-V ISA documentation and identify possible solutions for each. As an example, we find that a RISC-V-compliant microarchitecture allows 144 outcomes forbidden by C11 to be observed out of 1,701 litmus tests examined. Overall, this paper demonstrates the necessity of full-stack verification for detecting MCM-related bugs in the hardware-software stack.", "venue": "ASPLOS 2017", "authors": ["Caroline  Trippel", "Yatin A. Manerkar", "Daniel  Lustig", "Michael  Pellauer", "Margaret  Martonosi"], "year": 2016, "n_citations": 36}
{"id": 5729895, "s2_id": "16d49978356f9441ae6ace4a9172f511e8e8abd1", "title": "QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits", "abstract": "Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ) computers. Previous work for mitigating noise has primarily focused on gate-level or pulse-level noise-adaptive compilation. However, limited research efforts have explored a higher level of optimization by making the quantum circuits themselves resilient to noise. In this paper, we propose QuantumNAS, a comprehensive framework for noise-adaptive co-search of the variational circuit and qubit mapping. Variational quantum circuits are a promising approach for constructing quantum neural networks for machine learning and variational ansatzes for quantum simulation. However, finding the best variational circuit and its optimal parameters is challenging due to the large design space and parameter training cost. We propose to decouple the circuit search and parameter training by introducing a novel SuperCircuit. The SuperCircuit is constructed with multiple layers of pre-defined parameterized gates (e.g., U3 and CU3) and trained by iteratively sampling and updating the parameter subsets (SubCircuits) of it. It provides an accurate estimation of SubCircuits performance trained from scratch. Then we perform an evolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit performance is estimated with parameters inherited from SuperCircuit and simulated with real device noise models. Finally, we perform iterative gate pruning and finetuning to remove redundant gates in a fine-grained manner. Extensively evaluated with 12 quantum machine learning (QML) and variational quantum eigensolver (VQE) benchmarks on 10 quantum computers, QuantumNAS significantly outperforms noise-unaware search, human, random, and existing noiseadaptive qubit mapping baselines. For QML tasks, QuantumNAS is the first to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class classification accuracy on real quantum computers. It also achieves the lowest eigenvalue for VQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD baselines. We also open-source QuantumEngine for fast training of parameterized quantum circuits to facilitate future research.", "venue": "ArXiv", "authors": ["Hanrui  Wang", "Yongshan  Ding", "Jiaqi  Gu", "Yujun  Lin", "David Z. Pan", "Frederic T. Chong", "Song  Han"], "year": 2021, "n_citations": 4}
{"id": 5732886, "s2_id": "46ab1dd2b16c43e38bb665e642d19aa5f68a0dc2", "title": "Efficient Non-linear Calculators", "abstract": "A novel algorithm for producing smooth nonlinearities on digital hardware is presented. The non-linearities are inherently quadratic and have both symmetrical and asymmetrical variants. The integer (and fixed point) implementation is highly amenable for use with digital gates on an ASIC or FPGA. The implementations are multiplier-less. Scaling of the non-linear output, as required in an LSTM cell, is integrated into the implementation. This too does not require a multiplier. The non-linearities are useful as activation functions in a variety of ANN architectures. The floating point mappings have been compared with other non-linearities and have been benchmarked. Results show that these functions should be considered in the ANN design phase. The hardware resource usage of the implementations have been thoroughly investigated. Our results make a strong case for implementations in edge applications. This document summarizes the findings and serves to give a quick overview of the outcomes of our research.", "venue": "ArXiv", "authors": ["Adedamola  Wuraola", "Nitish  Patel"], "year": 2021, "n_citations": 0}
{"id": 5735444, "s2_id": "0cf58b41c70b85b41a0c50ea30923ec9ba0f5b3f", "title": "A High-Throughput Architecture of List Successive Cancellation Polar Codes Decoder With Large List Size", "abstract": "As the first kind of forward error correction (FEC) codes that achieve channel capacity, polar codes have attracted much research interest recently. Compared with other popular FEC codes, polar codes decoded by list successive cancellation decoding (LSCD) with a large list size have better error correction performance. However, due to the serial decoding nature of LSCD and the high complexity of list management, the decoding latency is high, which limits the usage of polar codes in practical applications that require low latency and high throughput. In this paper, we study the high-throughput implementation of LSCD with a large list size. Specifically, at the algorithmic level, to achieve a low-decoding latency with moderate hardware complexity, two decoding schemes, a multibit double thresholding scheme and a partial G-node look-ahead scheme, are proposed. Then, a high-throughput VLSI architecture implementing the proposed algorithms is developed with optimizations on different computation modules. From the implementation results on United Microelectronics Corporation (UMC) 90\u00a0nm complementary metal oxide semiconductor (CMOS) technology, the proposed architecture achieves decoding throughputs of 1.103\u00a0Gb/s, 977\u00a0Mb/s, and 827\u00a0Mb/s when the list sizes are 8, 16, and 32, respectively.", "venue": "IEEE Transactions on Signal Processing", "authors": ["ChenYang  Xia", "Ji  Chen", "YouZhe  Fan", "Chi-ying  Tsui", "Jie  Jin", "Hui  Shen", "Bin  Li"], "year": 2018, "n_citations": 13}
{"id": 5735625, "s2_id": "6b168cb2542876582ed4fd1aaaed12b3c8613b47", "title": "Asynchronous Early Output Dual-Bit Full Adders Based on Homogeneous and Heterogeneous Delay-Insensitive Data Encoding", "abstract": "This paper presents the designs of asynchronous early output dual-bit full adders without and with redundant logic (implicit) corresponding to homogeneous and heterogeneous delay-insensitive data encoding. For homogeneous delay-insensitive data encoding only dual-rail i.e. 1-of-2 code is used, and for heterogeneous delay-insensitive data encoding 1-of-2 and 1-of-4 codes are used. The 4-phase return-to-zero protocol is used for handshaking. To demonstrate the merits of the proposed dual-bit full adder designs, 32-bit ripple carry adders (RCAs) are constructed comprising dual-bit full adders. The proposed dual-bit full adders based 32-bit RCAs incorporating redundant logic feature reduced latency and area compared to their non-redundant counterparts with no accompanying power penalty. In comparison with the weakly indicating 32-bit RCA constructed using homogeneously encoded dual-bit full adders containing redundant logic, the early output 32-bit RCA comprising the proposed homogeneously encoded dual-bit full adders with redundant logic reports corresponding reductions in latency and area by 22.2% and 15.1% with no associated power penalty. On the other hand, the early output 32-bit RCA constructed using the proposed heterogeneously encoded dual-bit full adder which incorporates redundant logic reports respective decreases in latency and area than the weakly indicating 32-bit RCA that consists of heterogeneously encoded dual-bit full adders with redundant logic by 21.5% and 21.3% with nil power overhead. The simulation results obtained are based on a 32/28nm CMOS process technology.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "K.  Prasad"], "year": 2017, "n_citations": 5}
{"id": 5736016, "s2_id": "a99d0538ca913418108d85bb42f949752247d101", "title": "Optimizing scrubbing by netlist analysis for FPGA configuration bit classification and floorplanning", "abstract": "Existing scrubbing techniques for SEU mitigation on FPGAs do not guarantee an error-free operation after SEU recovering if the affected configuration bits do belong to feedback loops of the implemented circuits. In this paper, we a) provide a netlist-based circuit analysis technique to distinguish so-called critical configuration bits from essential bits in order to identify configuration bits which will need also state-restoring actions after a recovered SEU and which not. Furthermore, b) an alternative classification approach using fault injection is developed in order to compare both classification techniques. Moreover, c) we will propose a floorplanning approach for reducing the effective number of scrubbed frames and d), experimental results will give evidence that our optimization methodology not only allows to detect errors earlier but also to minimize the Mean-Time-To-Repair (MTTR) of a circuit considerably. In particular, we show that by using our approach, the MTTR for datapath-intensive circuits can be reduced by up to 48.5% in comparison to standard approaches.", "venue": "Integr.", "authors": ["Bernhard  Schmidt", "Daniel  Ziener", "J\u00fcrgen  Teich", "Christian  Z\u00f6llner"], "year": 2017, "n_citations": 2}
{"id": 5738562, "s2_id": "3ec7c1c75468dc87864d6c84302afad8173d2e93", "title": "Appearances of the Birthday Paradox in High Performance Computing", "abstract": "We give an elementary statistical analysis of two High Performance Computing issues, processor cache mapping and network port mapping. In both cases we find that, as in the birthday paradox, random assignment leads to more frequent coincidences than one expects a priori. Since these correspond to contention for limited resources, this phenomenon has important consequences for performance.", "venue": "ArXiv", "authors": ["Victor  Eijkhout", "Margaret  Myers", "John  McCalpin"], "year": 2019, "n_citations": 0}
{"id": 5743899, "s2_id": "61e9c930efef0de4df0689d504acf354f728649f", "title": "Programmable Memristive Threshold Logic Gate Array", "abstract": "This paper proposes the implementation of programmable threshold logic gate (TLG) crossbar array based on modified TLG cells for high speed processing and computation. The proposed TLG array operation does not depend on input signal and time pulses, comparing to the existing architectures. The circuit is implemented using TSMC 180nm CMOS technology. The on-chip area and power dissipation of the simulated 3 \u00d7 4 TLG array is 1463\u00b5m2 and 425\u00b5W, respectively.", "venue": "2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)", "authors": ["Olga  Krestinskaya", "Akshay Kumar Maan", "Alex Pappachen James"], "year": 2018, "n_citations": 3}
{"id": 5745143, "s2_id": "df7a98650a945717b8fd9cf7e9945d149fa704ba", "title": "On Resistive Memories: One Step Row Readout Technique and Sensing Circuitry", "abstract": "Transistor-based memories are rapidly approaching their maximum density per unit area. Resistive crossbar arrays enable denser memory due to the small size of switching devices. However, due to the resistive nature of these memories, they suffer from current sneak paths complicating the readout procedure. In this paper, we propose a row readout technique with circuitry that can be used to read {selector-less} resistive crossbar based memories. High throughput reading and writing techniques are needed to overcome the memory-wall bottleneck problem and to enable near memory computing paradigm. The proposed technique can read the entire row of dense crossbar arrays in one cycle, unlike previously published techniques. The requirements for the readout circuitry are discussed and satisfied in the proposed circuit. Additionally, an approximated expression for the power consumed while reading the array is derived. A figure of merit is defined and used to compare the proposed approach with existing reading techniques. Finally, a quantitative analysis of the effect of biasing mismatch on the array size is discussed.", "venue": "ArXiv", "authors": ["Mohammed E. Fouda", "Ahmed M. Eltawil", "Fadi J. Kurdahi"], "year": 2019, "n_citations": 4}
{"id": 5745637, "s2_id": "2363351cd5eeab09ba54a70d578b5d8740e6d054", "title": "A high-performance MEMRISTOR-based Smith-Waterman DNA sequence alignment Using FPNI structure", "abstract": "This paper aims to present a new re-configuration sequencing method for difference of read lengths that may take place as input data in which is crucial drawbacks lay impact on DNA sequencing methods.", "venue": "ArXiv", "authors": ["Mahdi  Taheri", "Hamed  Zandevakili", "Ali  Mahani"], "year": 2020, "n_citations": 0}
{"id": 5747721, "s2_id": "99cbadbb66be1d54166213e586700a75e842136f", "title": "Automated Seed Quality Testing System using GAN & Active Learning", "abstract": "Quality assessment of agricultural produce is a crucial step in minimizing food stock wastage. However, this is currently done manually and often requires expert supervision, especially in smaller seeds like corn. We propose a novel computer vision-based system for automating this process. We build a novel seed image acquisition setup, which captures both the top and bottom views. Dataset collection for this problem has challenges of data annotation costs/time and class imbalance. We address these challenges by i.) using a Conditional Generative Adversarial Network (CGAN) to generate real-looking images for the classes with lesser images and ii.) annotate a large dataset with minimal expert human intervention by using a Batch Active Learning (BAL) based annotation tool. We benchmark different image classification models on the dataset obtained. We are able to get accuracies of up to 91.6% for testing the physical purity of seed samples.", "venue": "ArXiv", "authors": ["Sandeep  Nagar", "Prateek  Pani", "Raj  Nair", "Girish  Varma"], "year": 2021, "n_citations": 0}
{"id": 5748314, "s2_id": "3f797b9ea2d95313064ea49fb5067b590e8133ec", "title": "Voltage and Level-Shifter Assignment Driven Floorplanning", "abstract": "As technology scales, low power design has become a significant requirement for SOC designers. Among the existing techniques, Multiple-Supply Voltage (MSV) is a popular and effective method to reduce both dynamic and static power. Besides, level shifters consume area and delay, and should be considered during floorplanning. In this paper, we present a new floorplanning system, called MVLSAF, to solve multi-voltage and level shifter assignment problem. We use a convex cost network flow algorithm to assign arbitrary number of legal working voltages and a minimum cost flow algorithm to handle level-shifter assignment. The experimental results show MVLSAF is effective1.", "venue": "IEICE Trans. Fundam. Electron. Commun. Comput. Sci.", "authors": ["Bei  Yu", "Sheqin  Dong", "Song  Chen", "Satoshi  Goto"], "year": 2009, "n_citations": 6}
{"id": 5749363, "s2_id": "ed5cb4ddd422480b712d1b3182a7240213ae438b", "title": "HL-Pow: A Learning-Based Power Modeling Framework for High-Level Synthesis", "abstract": "High-level synthesis (HLS) enables designers to customize hardware designs efficiently. However, it is still challenging to foresee the correlation between power consumption and HLS-based applications at an early design stage. To overcome this problem, we introduce HL-Pow, a power modeling framework for FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow incorporates an automated feature construction flow to efficiently identify and extract features that exert a major influence on power consumption, simply based upon HLS results, and a modeling flow that can build an accurate and generic power model applicable to a variety of designs with HLS. By using HL-Pow, the power evaluation process for FPGA designs can be significantly expedited because the power inference of HL-Pow is established on HLS instead of the time-consuming register-transfer level (RTL) implementation flow. Experimental results demonstrate that HL-Pow can achieve accurate power modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To further facilitate power-oriented optimizations, we describe a novel design space exploration (DSE) algorithm built on top of HL-Pow to trade off between latency and power consumption. This algorithm can reach a close approximation of the real Pareto frontier while only requiring running HLS flow for 20% of design points in the entire design space.", "venue": "2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Zhe  Lin", "Jieru  Zhao", "Sharad  Sinha", "Wei  Zhang"], "year": 2020, "n_citations": 3}
{"id": 5755715, "s2_id": "fccac157a71bb1c39556fdf18b1db9f67d9e40e9", "title": "Towards Energy-Efficient and Secure Edge AI: A Cross-Layer Framework", "abstract": "The security and privacy concerns along with the amount of data that is required to be processed on regular basis has pushed processing to the edge of the computing systems. Deploying advanced Neural Networks (NN), such as deep neural networks (DNNs) and spiking neural networks (SNNs), that offer state-of-the-art results on resourceconstrained edge devices is challenging due to the stringent memory and power/energy constraints. Moreover, these systems are required to maintain correct functionality under diverse security and reliability threats. This paper first discusses existing approaches to address energy efficiency, reliability, and security issues at different system layers, i.e., hardware (HW) and software (SW). Afterward, we discuss how to further improve the performance (latency) and the energy efficiency of Edge AI systems through HW/SW-level optimizations, such as pruning, quantization, and approximation. To address reliability threats (like permanent and transient faults), we highlight cost-effective mitigation techniques, like fault-aware training and mapping. Moreover, we briefly discuss effective detection and protection techniques to address security threats (like model and data corruption). Towards the end, we discuss how these techniques can be combined in an integrated cross-layer framework for realizing robust and energy-efficient Edge AI systems.", "venue": "ArXiv", "authors": ["Muhammad  Shafique", "Alberto  Marchisio", "Rachmad Vidya Wicaksana Putra", "Muhammad Abdullah Hanif"], "year": 2021, "n_citations": 0}
{"id": 5764781, "s2_id": "402f850dff86fb601d34b2841e6083ac0f928edd", "title": "SCNN: An accelerator for compressed-sparse convolutional neural networks", "abstract": "Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to a multiplier array, where they are extensively reused; product accumulation is performed in a novel accumulator array. On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7\u00d7 and 2.3\u00d7, respectively, over a comparably provisioned dense CNN accelerator.", "venue": "2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Angshuman  Parashar", "Minsoo  Rhu", "Anurag  Mukkara", "Antonio  Puglielli", "Rangharajan  Venkatesan", "Brucek  Khailany", "Joel S. Emer", "Stephen W. Keckler", "William J. Dally"], "year": 2017, "n_citations": 668}
{"id": 5765736, "s2_id": "d44054a2f4ce53609fde93ba45205ccc4162a5b2", "title": "Run-Time-Reconfigurable Multi-Precision Floating-Point Matrix Multiplier Intellectual Property Core on FPGA", "abstract": "In today\u2019s world, high-power computing applications such as image processing, digital signal processing, graphics, robotics require enormous computing power. These applications use matrix operations, especially matrix multiplication. Multiplication operations require a lot of computational time and are also complex in design. We can use field-programmable gate arrays as low-cost hardware accelerators along with a low-cost general-purpose processor instead of a high-cost application-specific processor for such applications. In this work, we employ an efficient Strassen\u2019s algorithm for matrix multiplication and a highly efficient run-time-reconfigurable floating-point multiplier for matrix element multiplication. The run-time-reconfigurable floating-point multiplier is implemented with custom floating-point format for variable-precision applications. A very efficient combination of Karatsuba algorithm and Urdhva Tiryagbhyam algorithm is used to implement the binary multiplier. This design can effectively adjust the power and delay requirements according to different accuracy requirements by reconfiguring itself during run time.", "venue": "Circuits Syst. Signal Process.", "authors": ["S.  Arish", "R. K. Sharma"], "year": 2017, "n_citations": 2}
{"id": 5766797, "s2_id": "559b29dcba034e9bfa06ef4636ed5f142ef7bd9b", "title": "Virtual-Threading: Advanced General Purpose Processors Architecture", "abstract": "The paper describes the new computers architecture, the main features of which has been claimed in the Russian Federation patent 2312388 and in the US patent application 11/991331. This architecture is intended to effective support of the General Purpose Parallel Computing (GPPC), the essence of which is extremely frequent switching of threads between states of activity and states of viewed in the paper the algorithmic latency. To emphasize the same impact of the architectural latency and the algorithmic latency upon GPPC, is introduced the new notion of the generalized latency and is defined its quantitative measure - the Generalized Latency Tolerance (GLT). It is shown that a well suited for GPPC implementation architecture should have high level of GLT and is described such architecture, which is called the Virtual-Threaded Machine. This architecture originates a processor virtualization in the direction of activities virtualization, which is orthogonal to the well-known direction of memory virtualization. The key elements of the architecture are 1) the distributed fine grain representation of the architectural register file, which elements are hardware swapped through levels of a microarchitectural memory, 2) the prioritized fine grain direct hardware multiprogramming, 3) the access controlled virtual addressing and 4) the hardware driven semaphores. The composition of these features lets to introduce new styles of operating system (OS) programming, which is free of interruptions, and of applied programming with a very rare using the OS services.", "venue": "ArXiv", "authors": ["Andrei I. Yafimau"], "year": 2009, "n_citations": 0}
{"id": 5768669, "s2_id": "fd9fd8bc23d9395fb686c807081a3fbc7ded409b", "title": "Multiplierless modules for forward and backward integer wavelet transform", "abstract": "This article is about the architecture of a wavelet filter bank with reprogrammable logic. It is based on second generation of wavelets with a reduced of number of operations. A new basic structure for parallel architecture and modules to forward and backward integer discrete wavelet transform is proposed.", "venue": "CompSysTech '03", "authors": ["Vasil  Kolev"], "year": 2003, "n_citations": 3}
{"id": 5769591, "s2_id": "572b9c77192ffa8ab000048b19df2582254b8050", "title": "A novel reconfigurable architecture of a DSP processor for efficient mapping of DSP functions using field programmable DSP arrays", "abstract": "Development of modern integrated circuit technologies makes it feasible to develop cheaper, faster and smaller special purpose signal processing function circuits. Digital Signal processing functions are generally implemented either on ASICs with inflexibility, or on FPGAs with bottlenecks of relatively smaller utilization factor or lower speed compared to ASIC. Field Programmable DSP Array (FPDA) is the proposed DSP dedicated device, redolent to FPGA, but with basic fixed common modules (CMs) (like adders, subtractors, multipliers, scaling units, shifters) instead of CLBs. This paper introduces the development of reconfigurable system architecture with a focus on FPDA that integrates different DSP functions like DFT, FFT, DCT, FIR, IIR, and DWT etc. The switching between DSP functions is occurred by reconfiguring the interconnection between CMs. Validation of the proposed architecture has been achieved on Virtex5 FPGA. The architecture provides sufficient amount of flexibility, parallelism and scalability.", "venue": "CARN", "authors": ["Amitabha  Sinha", "Mitrava  Sarkar", "Soumojit  Acharyya", "Suranjan  Chakraborty"], "year": 2013, "n_citations": 2}
{"id": 5770171, "s2_id": "a5266677b9d76abbbd3805701c3a78104564e7ac", "title": "Are We Susceptible to Rowhammer? An End-to-End Methodology for Cloud Providers", "abstract": "Cloud providers are concerned that Rowhammer poses a potentially critical threat to their servers, yet today they lack a systematic way to test whether the DRAM used in their servers is vulnerable to Rowhammer attacks. This paper presents an endto-end methodology to determine if cloud servers are susceptible to these attacks. With our methodology, a cloud provider can construct worst-case testing conditions for DRAM.We apply our methodology to three classes of servers from a major cloud provider. Our findings show that none of the CPU instruction sequences used in prior work to mount Rowhammer attacks create worst-case DRAM testing conditions. To address this limitation, we develop an instruction sequence that leverages microarchitectural side-effects to \"hammer\" DRAM at a near-optimal rate on modern Intel Skylake and Cascade Lake platforms. We also design a DDR4 fault injector that can reverse engineer row adjacency for any DDR4 DIMM. When applied to our cloud provider\u2019s DIMMs, we find that DRAM rows do not always follow a linear map.", "venue": "2020 IEEE Symposium on Security and Privacy (SP)", "authors": ["Lucian  Cojocar", "Jeremie  Kim", "Minesh  Patel", "Lillian  Tsai", "Stefan  Saroiu", "Alec  Wolman", "Onur  Mutlu"], "year": 2020, "n_citations": 29}
{"id": 5771145, "s2_id": "55ec076ae74dfa122c133ab1e5e059db28e13341", "title": "Scalability Terminology: Farms, Clones, Partitions, Packs, RACS and RAPS", "abstract": "Defines a vocabulary for scaleable systems: Geoplexes, Farms, Clones, RACS, RAPS, clones, partitions, and packs and dicusses the design tradeoffs of using clones, partitons, and packs.", "venue": "ArXiv", "authors": ["Bill  Devlin", "Jim  Gray", "Bill  Laing", "George  Spix"], "year": 1999, "n_citations": 56}
{"id": 5775128, "s2_id": "5d42b5b19e9b14b06075a680c3d93409cb27a193", "title": "HCIC: Hardware-Assisted Control-Flow Integrity Checking", "abstract": "Recently, code reuse attacks (CRAs), such as return-oriented programming (ROP) and jump-oriented programming (JOP), have emerged as a new class of ingenious security threats. Attackers can utilize CRAs to hijack the control flow of programs to perform malicious actions without injecting any codes. Many defenses, classed into software-based and hardware-based, have been proposed. However, software-based methods are difficult to be deployed in practical systems due to high performance overhead. Hardware-based methods can reduce performance overhead but may require extending instruction set architectures (ISAs) and modifying the compiler or suffer the vulnerability of key leakage. To tackle these issues, this paper proposes a new hardware-assisted control flow checking method to resist CRAs with negligible performance overhead without extending ISAs, modifying the compiler or leaking the encryption/decryption key. The key technique involves two control flow checking mechanisms. The first one is the encrypted Hamming distances matching between the physical unclonable function (PUF) response and the return addresses, which prevents attackers from returning between gadgets so long as the PUF response is secret, thus resisting ROP attacks. The second one is the linear encryption/decryption operation (XOR) between the PUF response and the instructions at target addresses of call and jmp instructions to defeat JOP attacks. Advanced return-based full-function reuse attacks will be prevented with the dynamic key-updating method. Experimental evaluations on benchmarks demonstrate that the proposed method introduces negligible 0.95% runtime overhead and 0.78% binary size overhead on average.", "venue": "IEEE Internet of Things Journal", "authors": ["Jiliang  Zhang", "Binhang  Qi", "Zheng  Qin", "Gang  Qu"], "year": 2019, "n_citations": 35}
{"id": 5777482, "s2_id": "8f524f2c024119d2cc7bf40187f8152544790f47", "title": "CLEAR: Cross-layer exploration for architecting resilience: Combining hardware and software techniques to tolerate soft errors in processor cores", "abstract": "We present a first of its kind framework which overcomes a major challenge in the design of digital systems that are resilient to reliability failures: achieve desired resilience targets at minimal costs (energy, power, execution time, area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, algorithm). This is also referred to as cross-layer resilience. In this paper, we focus on radiation-induced soft errors in processor cores. We address both single-event upsets (SEUs) and single-event multiple upsets (SEMUs) in terrestrial environments. Our framework automatically and systematically explores the large space of comprehensive resilience techniques and their combinations across various layers of the system stack (798 cross-layer combinations in this paper), derives cost-effective solutions that achieve resilience targets at minimal costs, and provides guidelines for the design of new resilience techniques. We demonstrate the practicality and effectiveness of our framework using two diverse designs: a simple, in-order processor core and a complex, out-of-order processor core. Our results demonstrate that a carefully optimized combination of circuit-level hardening, logic-level parity checking, and micro-architectural recovery provides a highly cost-effective soft error resilience solution for general-purpose processor cores. For example, a 50\u00d7 improvement in silent data corruption rate is achieved at only 2.1% energy cost for an out-of-order core (6.1% for an in-order core) with no speed impact. However, selective circuit-level hardening alone, guided by a thorough analysis of the effects of soft errors on application benchmarks, provides a cost-effective soft error resilience solution as well (with ~1% additional energy cost for a 50\u00d7 improvement in silent data corruption rate).", "venue": "2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)", "authors": ["Eric  Cheng", "Shahrzad  Mirkhani", "Lukasz G. Szafaryn", "Chen-Yong  Cher", "Hyungmin  Cho", "Kevin  Skadron", "Mircea R. Stan", "Klas  Lilja", "Jacob A. Abraham", "Pradip  Bose", "Subhasish  Mitra"], "year": 2016, "n_citations": 56}
{"id": 5778110, "s2_id": "51c7a5380b73e43db6aee3c95a3a53f1f7e25a45", "title": "Understanding Tool Synthesis Behavior and Safe Finite State Machine Design", "abstract": "High-reliability design requires understanding synthesis tool behavior and best practices. Detection and protection against illegal states and transitions is important for critical Finite State Machines (FSMs) within high reliability applications. Single Event Upsets (SEUs) probability is increasing with decreasing circuit dimensions and voltage [1]. SEU handling must be analyzed post optimization to ensure designed protections are still functional. In this work the default behavior of three synthesis tools interacting with high reliability FSMs is discussed. Post-synthesis netlists of test FSMs are analyzed for optimization induced changes that affect reliability during a SEU. Best practices are proposed to curtail aggressive optimizers. Keywords\u2014 Finite State Machine, Single Event Upset, HighReliability, Synthesis", "venue": "ArXiv", "authors": ["Katie  Liszewski", "Timothy  McDonley"], "year": 2021, "n_citations": 0}
{"id": 5778710, "s2_id": "ec879f785f051ce6d6c54b995446fd346654ac42", "title": "A Memory Aware High Level Synthesis Too", "abstract": "We introduce a new approach to take into account the memory architecture and the memory mapping in High- Level Synthesis for data intensive applications. We formalize the memory mapping as a set of constraints for the synthesis, and defined a Memory Constraint Graph and an accessibility criterion to be used in the scheduling step. We use a memory mapping file to include those memory constraints in our HLS tool GAUT. It is possible, with the help of GAUT, to explore a wide range of solutions, and to reach a good tradeoff between time, power-consumption, and area.", "venue": "ArXiv", "authors": ["Gwenol\u00e9  Corre", "Nathalie  Julien", "Eric  Senn", "Eric  Martin"], "year": 2006, "n_citations": 0}
{"id": 5783545, "s2_id": "93904c92422d6460e0b8690b15af6d6770541a38", "title": "REQ-YOLO: A Resource-Aware, Efficient Quantization Framework for Object Detection on FPGAs", "abstract": "Deep neural networks (DNNs), as the basis of object detection, will play a key role in the development of future autonomous systems with full autonomy. The autonomous systems have special requirements of real-time, energy-e cient implementations of DNNs on a power-budgeted system. Two research thrusts are dedicated to per- formance and energy e ciency enhancement of the inference phase of DNNs. The first one is model compression techniques while the second is e cient hardware implementations. Recent researches on extremely-low-bit CNNs such as binary neural network (BNN) and XNOR-Net replace the traditional oating point operations with bi- nary bit operations, signi cantly reducing memory bandwidth and storage requirement, whereas suffering non-negligible accuracy loss and waste of digital signal processing (DSP) blocks on FPGAs. To overcome these limitations, this paper proposes REQ-YOLO, a resource aware, systematic weight quantization framework for object detection, considering both algorithm and hardware resource aspects in object detection. We adopt the block-circulant matrix method and propose a heterogeneous weight quantization using Alternative Direction Method of Multipliers (ADMM), an e ective optimization technique for general, non-convex optimization problems. To achieve real-time, highly efficient implementations on FPGA, we present the detailed hardware implementation of block circulant matrices on CONV layers and de- velop an e cient processing element (PE) structure supporting the heterogeneous weight quantization, CONV data ow and pipelining techniques, design optimization, and a template-based automatic synthesis framework to optimally exploit hardware resource. Experimental results show that our proposed REQ-YOLO framework can signi cantly compress the YOLO model while introducing very small accuracy degradation. The related codes are here: https://github.com/Anonymous788/heterogeneous_ADMM_YOLO.", "venue": "FPGA", "authors": ["Caiwen  Ding", "Shuo  Wang", "Ning  Liu", "Kaidi  Xu", "Yanzhi  Wang", "Yun  Liang"], "year": 2019, "n_citations": 34}
{"id": 5787069, "s2_id": "9066cfda55fd4ac68fb1c81315622aab78c198cf", "title": "Energy-Efficiency Prediction of Multithreaded Workloads on Heterogeneous Composite Cores Architectures using Machine Learning Techniques", "abstract": "Heterogeneous architectures have emerged as a promising alternative for homogeneous architectures to improve the energy-efficiency of computer systems. Composite Cores Architecture (CCA), a class of dynamic heterogeneous architectures enabling the computer system to construct the right core at run-time for each application by composing cores together to build larger core or decomposing a large core into multiple smaller cores. While this architecture provides more flexibility for the running application to find the best run-time settings to maximize energy-efficiency, due to the interdependence of various tuning parameters such as the type of the core, run-time voltage and frequency and the number of threads, it makes it more challenging for scheduling. Prior studies mainly addressed the scheduling problem in CCAs by looking at one or two of these tuning parameters. However, as we will show in this paper, it is important to concurrently optimize and fine-tune these parameters. In addition, most previous works on CCA mainly study traditional single threaded CPU applications. This paper describes a systematic approach to predict the right configurations for running multithreaded workloads on CCAs. It achieves this by developing a machine learning-based approach to predict core type, voltage and frequency setting to maximize the energy-efficiency. Our predictor learns offline from an extensive set of training multithreaded workloads. It is then applied to predict the optimal processor configuration at run-time by taking into account the multithreaded application characteristics and the optimization objective. For this purpose, five well-known machine learning models are implemented for energy-efficiency optimization and precisely compared in terms of accuracy and hardware overhead to guide the scheduling decisions in a CCA.", "venue": "ArXiv", "authors": ["Hossein  Sayadi"], "year": 2018, "n_citations": 0}
{"id": 5789331, "s2_id": "30e2d87072aea1226f532b890cc6393a70fddf36", "title": "CRT-Based High-Speed Parallel Architecture for Long BCH Encoding", "abstract": "Bose-Chaudhuri-Hocquenghen (BCH) error-correcting codes are now widely used in communication system and digital technology. The direct linear feedback shifted register (LFSR)-based encoding of a long BCH code suffers from the large fan-out effect of some XOR gates. This makes the LFSR-based encoders of long BCH codes not keep up with the data transmission speed in some applications. The technique for eliminating the large fan-out effect by J-unfolding method and some algebraic manipulation has been proposed. In this brief, we present a Chinese remainder theorem (CRT)-based parallel architecture for long BCH encoding. Our novel technique can be used to eliminate the fan-out bottleneck. The only restriction on the speed of long BCH encoding of our CRT-based architecture is log2 N, where N is the length of the BCH code.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Hao  Chen"], "year": 2009, "n_citations": 26}
{"id": 5793669, "s2_id": "b297e5d214fb066bf14cf103d3b7286cb903906e", "title": "Flexible Support for Fast Parallel Commutative Updates", "abstract": "Privatizing data is a useful strategy for increasing parallelism in a shared memory multithreaded program. Independent cores can compute independently on duplicates of shared data, combining their results at the end of their computations. Conventional approaches to privatization, however, rely on explicit static or dynamic memory allocation for duplicated state, increasing memory footprint and contention for cache resources, especially in shared caches. In this work, we describe CCache, a system for on-demand privatization of data manipulated by commutative operations. CCache garners the benefits of privatization, without the increase in memory footprint or cache occupancy. Each core in CCache dynamically privatizes commutatively manipulated data, operating on a copy. Periodically or at the end of its computation, the core merges its value with the value resident in memory, and when all cores have merged, the in-memory copy contains the up-to-date value. We describe a low-complexity architectural implementation of CCache that extends a conventional multicore to support on-demand privatization without using additional memory for private copies. We evaluate CCache on several high-value applications, including random access key-value store, clustering, breadth first search and graph ranking, showing speedups upto 3.2X.", "venue": "ArXiv", "authors": ["Vignesh  Balaji", "Dhruva  Tirumala", "Brandon  Lucia"], "year": 2017, "n_citations": 1}
{"id": 5799087, "s2_id": "730a60e9996e3b938527e890e98dd51511a432cb", "title": "Evaluation of silicon consumption for a connectionless Network-on-Chip", "abstract": "We present the design and evaluation of a predictable Network-on-Chip (NoC) to interconnect processing units running multimedia applications with variable-bit-rate. The design is based on a connectionless strategy in which flits from different communication flows are interleaved in the same communication channel between routers. Each flit carries routing information used by routers to perform arbitration and scheduling of the corresponding output communication channel. Analytic comparisons show that our approach keeps average latency lower than a network based on resource reservation, when both networks are working over 80% of offered load. We also evaluate the proposed NoC on FPGA and ASIC technologies to understand the trade-off due to our approach, in terms of silicon consumption.", "venue": "ArXiv", "authors": ["Marcelo Daniel Berejuck", "Ant\u00f4nio Augusto Fr\u00f6hlich"], "year": 2014, "n_citations": 4}
{"id": 5801412, "s2_id": "d0fd258ca82d5b3fb9029411285aa5fae22bd802", "title": "Load Driven Branch Predictor (LDBP)", "abstract": "Branch instructions dependent on hard-to-predict load data are the leading branch misprediction contributors. Current state-of-the-art history-based branch predictors have poor prediction accuracy for these branches. Prior research backs this observation by showing that increasing the size of a 256-KBit history-based branch predictor to its 1-MBit variant has just a 10% reduction in branch mispredictions. \nWe present the novel Load Driven Branch Predictor(LDBP) specifically targeting hard-to-predict branches dependent on a load instruction. Though random load data determines the outcome for these branches, the load address for most of these data has a predictable pattern. This is an observable template in data structures like arrays and maps. Our predictor model exploits this behavior to trigger future loads associated with branches ahead of time and use its data to predict the branch's outcome. The predictable loads are tracked, and the precomputed outcomes of the branch instruction are buffered for making predictions. Our experimental results show that compared to a standalone 256-Kbit IMLI predictor, when LDBP is augmented with a 150-Kbit IMLI, it reduces the average branch mispredictions by 20% and improves average IPC by 13.1% for benchmarks from SPEC CINT2006 and GAP benchmark suite.", "venue": "ArXiv", "authors": ["Akash  Sridhar", "Nursultan  Kabylkas", "Jose  Renau"], "year": 2020, "n_citations": 0}
{"id": 5807945, "s2_id": "99da0b2d51618f5201c72f0f469212d2489612cd", "title": "Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN Inference Engine", "abstract": "Deep neural networks have achieved impressive results in computer vision and machine learning. Unfortunately, state-of-the-art networks are extremely compute and memory intensive, which makes them unsuitable for mW-devices such as IoT end-nodes. Aggressive quantization of these networks dramatically reduces the computation and memory footprint. Binary-weight neural networks (BWNs) follow this trend, pushing weight quantization to the limit. Hardware accelerators for BWNs presented up to now have focused on core efficiency, disregarding I/O bandwidth, and system-level efficiency that are crucial for the deployment of accelerators in ultra-low power devices. We present Hyperdrive: a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel binary-weight streaming approach, which can be used for an arbitrarily sized convolutional neural network architecture and input resolution by exploiting the natural scalability of the compute units both at chip-level and system-level by arranging Hyperdrive chips systolically in a 2D mesh while processing the entire feature map together in parallel. Hyperdrive achieves 4.3 TOp/s/W system-level efficiency (i.e., including I/Os)\u2014 $3.1\\times $ higher than state-of-the-art BWN accelerators, even if its core uses resource-intensive FP16 arithmetic for increased robustness.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Renzo  Andri", "Lukas  Cavigelli", "Davide  Rossi", "Luca  Benini"], "year": 2019, "n_citations": 11}
{"id": 5809507, "s2_id": "b8355cd273944185e71c9946fa279d5e30700029", "title": "Architectural Design of a RAM Arbiter", "abstract": "Standard memory modules to store (and access) data are designed for use with a single system accessing it. More complicated memory modules would be accessed through a memory controller, which are also designed for one system. For multiple systems to access a single memory module there must be some facilitation that allows them to access the memory without overriding or corrupting the access from the others. This was done with the use of a memory arbiter, which controls the flow of traffic into the memory controller. The arbiter has a set of rules to abide to in order to choose which system gets through to the memory controller. In this project, a regular RAM module is designed for use with one system. Furthermore, a memory arbiter is also designed in Verilog that allows for more than one system to use a single RAM module in a controlled and synchronized manner. The arbiter uses a fixed priority scheme to avoid starvation of the system. In addition one of the major problems associated with such systems i.e. The Address Clash Problem has been nicely tackled and solved. The design is verified in simulation and validated on a Xilinx ML605 evaluation board with a Virtex 6 FPGA.", "venue": "ArXiv", "authors": ["Sourangsu  Banerji"], "year": 2014, "n_citations": 0}
{"id": 5810648, "s2_id": "dda7c10868d398a61d113a0671491422b57f9f93", "title": "Characterizing, Exploiting, and Mitigating Vulnerabilities in MLC NAND Flash Memory Programming", "abstract": "This paper summarizes our work on experimentally analyzing, exploiting, and addressing vulnerabilities in multi-level cell NAND flash memory programming, which was published in the industrial session of HPCA 2017, and examines the work's significance and future potential. Modern NAND flash memory chips use multi-level cells (MLC), which store two bits of data in each cell, to improve chip density. As MLC NAND flash memory scaled down to smaller manufacturing process technologies, manufacturers adopted a two-step programming method to improve reliability. In two-step programming, the two bits of a multi-level cell are programmed using two separate steps, in order to minimize the amount of cell-to-cell program interference induced on neighboring flash cells. \nIn this work, we demonstrate that two-step programming exposes new reliability and security vulnerabilities in state-of-the-art MLC NAND flash memory. We experimentally characterize contemporary 1X-nm (i.e., 15--19nm) flash memory chips, and find that a partially-programmed flash cell (i.e., a cell where the second programming step has not yet been performed) is much more vulnerable to cell-to-cell interference and read disturb than a fully-programmed cell. We show that it is possible to exploit these vulnerabilities on solid-state drives (SSDs) to alter the partially-programmed data, causing (potentially malicious) data corruption. Based on our observations, we propose several new mechanisms that eliminate or mitigate these vulnerabilities in partially-programmed cells, and at the same time increase flash memory lifetime by 16%.", "venue": "ArXiv", "authors": ["Yu  Cai", "Saugata  Ghose", "Yixin  Luo", "Ken  Mai", "Onur  Mutlu", "Erich F. Haratsch"], "year": 2018, "n_citations": 3}
{"id": 5810742, "s2_id": "723c474564ecaf409ac79b948eef4b5104a59dc2", "title": "AddNet: Deep Neural Networks Using FPGA-Optimized Multipliers", "abstract": "Low-precision arithmetic operations to accelerate deep-learning applications on field-programmable gate arrays (FPGAs) have been studied extensively, because they offer the potential to save silicon area or increase throughput. However, these benefits come at the cost of a decrease in accuracy. In this article, we demonstrate that reconfigurable constant coefficient multipliers (RCCMs) offer a better alternative for saving the silicon area than utilizing low-precision arithmetic. RCCMs multiply input values by a restricted choice of coefficients using only adders, subtractors, bit shifts, and multiplexers (MUXes), meaning that they can be heavily optimized for FPGAs. We propose a family of RCCMs tailored to FPGA logic elements to ensure their efficient utilization. To minimize information loss from quantization, we then develop novel training techniques that map the possible coefficient representations of the RCCMs to neural network weight parameter distributions. This enables the usage of the RCCMs in hardware, while maintaining high accuracy. We demonstrate the benefits of these techniques using AlexNet, ResNet-18, and ResNet-50 networks. The resulting implementations achieve up to 50% resource savings over traditional 8-bit quantized networks, translating to significant speedups and power savings. Our RCCM with the lowest resource requirements exceeds 6-bit fixed point accuracy, while all other implementations with RCCMs achieve at least similar accuracy to an 8-bit uniformly quantized design, while achieving significant resource savings.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Julian  Faraone", "Martin  Kumm", "Martin  Hardieck", "Peter  Zipf", "Xueyuan  Liu", "David  Boland", "Philip H. W. Leong"], "year": 2020, "n_citations": 12}
{"id": 5817437, "s2_id": "a12fc67fec8f0e3153c96fa39ec761e3d24b0f4f", "title": "High-Precision Tuning of State for Memristive Devices by Adaptable Variation-Tolerant Algorithm", "abstract": "Using memristive properties common for titanium dioxide thin film devices, we designed a simple write algorithm to tune device conductance at a specific bias point to 1% relative accuracy (which is roughly equivalent to seven-bit precision) within its dynamic range even in the presence of large variations in switching behavior. The high precision state is nonvolatile and the results are likely to be sustained for nanoscale memristive devices because of the inherent filamentary nature of the resistive switching. The proposed functionality of memristive devices is especially attractive for analog computing with low precision data. As one representative example we demonstrate hybrid circuitry consisting of an integrated circuit summing amplifier and two memristive devices to perform the analog multiply-and-add (dot-product) computation, which is a typical bottleneck operation in information processing.", "venue": "Nanotechnology", "authors": ["Fabien  Alibart", "Ligang  Gao", "Brian  Hoskins", "Dmitri B. Strukov"], "year": 2012, "n_citations": 415}
{"id": 5818298, "s2_id": "48832a0a86267c298e2ec5d073ad521417cdb8da", "title": "The Bitlet Model: A Parameterized Analytical Model to Compare PIM and CPU Systems", "abstract": "RONNY RONEN, Technion Israel Institute of Technology, Israel ADI ELIAHU, Technion Israel Institute of Technology, Israel ORIAN LEITERSDORF, Technion Israel Institute of Technology, Israel NATAN PELED, Technion Israel Institute of Technology, Israel KUNAL KORGAONKAR, Technion Israel Institute of Technology, Israel ANUPAM CHATTOPADHYAY, Nanyang Technological University, Singapore BEN PERACH, Technion Israel Institute of Technology, Israel SHAHAR KVATINSKY, Technion Israel Institute of Technology, Israel", "venue": "ArXiv", "authors": ["Ronny  Ronen", "Adi  Eliahu", "Orian  Leitersdorf", "Natan  Peled", "Kunal  Korgaonkar", "Anupam  Chattopadhyay", "Ben  Perach", "Shahar  Kvatinsky"], "year": 2021, "n_citations": 1}
{"id": 5819795, "s2_id": "b9e0ca57bfd17041e8a5f1b5436721869749561e", "title": "FPGA Design for Pseudorandom Number Generator Based on Chaotic Iteration used in Information Hiding Application", "abstract": "Lots of researches indicate that the inefficient generation of random numbers is a significant bottleneck for information communication applications. Therefore, Field Programmable Gate Array (FPGA) is developed to process a scalable fixed-point method for random streams generation. In our previous researches, we have proposed a technique by applying some well-defined discrete chaotic iterations that satisfy the reputed Devaney's definition of chaos, namely chaotic iterations (CI). We have formerly proven that the generator with CI can provide qualified chaotic random numbers. In this paper, this generator based on chaotic iterations is optimally redesigned for FPGA device. By doing so, the generation rate can be largely improved. Analyses show that these hardware generators can also provide good statistical chaotic random bits and can be cryptographically secure too. An application in the information hiding security field is finally given as an illustrative example.", "venue": "ArXiv", "authors": ["Jacques M. Bahi", "Xiaole  Fang", "Christophe  Guyeux", "Laurent  Larger"], "year": 2016, "n_citations": 24}
{"id": 5822027, "s2_id": "2e44ca7bd477b3e99f064bb48f29134d76bbd31f", "title": "Long Range Communication on Batteryless Devices", "abstract": "Bulk of the existing Wireless Sensor Network (WSN) nodes are usually battery powered, stationary and mostly designed for short distance communication, with little to no consideration for constrained devices that operate solely on harvested energy. On many occasions, batteries and beefy super-capacitors are used to power these WSN, but these systems are prone to service-life degradation and current-leakages. Most of the systems implementing super capacitors do not account for leakages after exceeding the charge cycle threshold. Frequent battery maintenance and replacement at scale is non-trivial, labor-intensive and challenging task, especially on sensing nodes deployed in extreme harsh environments with limited human intervention. In this paper, we present the technique for achieving Kilometer range communication on batteryless constraint devices by harnessing the capabilities of LoRa technology.", "venue": "ArXiv", "authors": ["Simeon  Babatunde", "Nirnay  Jain", "Vishwas  Powar"], "year": 2020, "n_citations": 0}
{"id": 5827649, "s2_id": "d963bb242073441f209f68c1ebebe61e19ae1d2b", "title": "T-count and Qubit Optimized Quantum Circuit Design of the Non-Restoring Square Root Algorithm", "abstract": "Quantum circuits for basic mathematical functions such as the square root are required to implement scientific computing algorithms on quantum computers. Quantum circuits that are based on Clifford+T gates can easily be made fault tolerant, but the T gate is very costly to implement. As a result, reducing T-count has become an important optimization goal. Further, quantum circuits with many qubits are difficult to realize, making designs that save qubits and produce no garbage outputs desirable. In this work, we present a T-count optimized quantum square root circuit with only 2 \u1e61 n + 1 qubits and no garbage output. To make a fair comparison against existing work, the Bennett\u2019s garbage removal scheme is used to remove garbage output from existing works. We determined that our proposed design achieves an average T-count savings of 43.44%, 98.95%, 41.06%, and 20.28% as well as qubit savings of 85.46%, 95.16%, 90.59%, and 86.77% compared to existing works.", "venue": "ACM J. Emerg. Technol. Comput. Syst.", "authors": ["Edgard  Mu\u00f1oz-Coreas", "Himanshu  Thapliyal"], "year": 2018, "n_citations": 12}
{"id": 5828595, "s2_id": "af388a04cad0470b881b809926ff00b1cc6fa26e", "title": "Encrypted Data Processing", "abstract": "In this paper, we present a comprehensive architecture for confidential computing, which we show to be general purpose and quite efficient. It executes the application as is, without any added burden or discipline requirements from the application developers. Furthermore, it does not require the trust of system software at the computing server and does not impose any added burden on the communication subsystem. The proposed Encrypted Data Processing (EDAP) architecture accomplishes confidentiality, authenticity, and freshness of the key-based cryptographic data protection by adopting data encryption with a multi-level key protection scheme. It guarantees that the user data is visible only in non-privileged mode to a designated program trusted by the data owner on a designated hardware, thus protecting the data from an untrusted hardware, hypervisor, OS, or other users\u2019 applications. The cryptographic keys and protocols used for achieving these confidential computing requirements are described in a use case example. Encrypting and decrypting data in an EDAPenabled processor can lead to performance degradation as it adds cycle time to the overall execution. However, our simulation result shows that the slowdown is only 6% on average across a collection of commercial workloads when the data encryption engine is placed between the L1 and L2 cache. We demonstrate that the EDAP architecture is valuable and practicable in the modern cloud environment for confidential computing. EDAP delivers a zero trust model of computing where the user software does not trust system software and vice versa.", "venue": "ArXiv", "authors": ["Jessica  Tseng", "Gianfranco  Bilardi", "Kattamuri  Ekanadham", "Manoj  Kumar", "Jose  Moreira", "P. C. Pattnaik"], "year": 2021, "n_citations": 1}
{"id": 5829019, "s2_id": "34c3096432e53d4c9412c568040b07c658da090b", "title": "Hardware Implementation of Neural Self-Interference Cancellation", "abstract": "In-band full-duplex systems can transmit and receive information simultaneously and on the same frequency band. However, due to the strong self-interference caused by the transmitter to its own receiver, the use of non-linear digital self-interference cancellation is essential. In this work, we describe a hardware architecture for a neural network-based non-linear self-interference (SI) canceller and we compare it with our own hardware implementation of a conventional polynomial based SI canceller. Our results show that, for the same SI cancellation performance, the neural network canceller has an <inline-formula> <tex-math notation=\"LaTeX\">$8.1\\times $ </tex-math></inline-formula> smaller area and requires <inline-formula> <tex-math notation=\"LaTeX\">$7.7\\times $ </tex-math></inline-formula> less power than the polynomial canceller. Moreover, the neural network canceller can achieve 7 dB more SI cancellation while still being <inline-formula> <tex-math notation=\"LaTeX\">$1.2\\times $ </tex-math></inline-formula> smaller than the polynomial canceller and only requiring <inline-formula> <tex-math notation=\"LaTeX\">$1.3\\times $ </tex-math></inline-formula> more power. These results show that NN-based methods applied to communications are not only useful from a performance perspective, but can also lead to order-of-magnitude implementation complexity reductions.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Yann  Kurzo", "Andreas Toftegaard Kristensen", "Andreas  Burg", "Alexios  Balatsoukas-Stimming"], "year": 2020, "n_citations": 8}
{"id": 5830866, "s2_id": "bbf3506f29b7d1928f53356cdb50dfc3a227e57e", "title": "picoArray technology: the tool's story", "abstract": "This paper briefly describes the picoArray/spl trade/ architecture, and in particular the deterministic internal communication fabric. The methods that have been developed for debugging and verifying systems using devices from the picoArray family are explained. In order to maximize the computational ability of these devices, hardware debugging support has been kept to a minimum and methods and tools developed to take this into account.", "venue": "Design, Automation and Test in Europe", "authors": ["Andrew  Duller", "Daniel  Towner", "Gajinder  Panesar", "Alan  Gray", "Will  Robbins"], "year": 2005, "n_citations": 40}
{"id": 5832916, "s2_id": "a1d597cf0522e38d55bfaebd7fc5a891ef9485bc", "title": "VLSI Design Of Advanced Digital Filters", "abstract": "The Cascaded Integrator Comb filters (CIC) find many applications in recent electronic devices such as frequency selection functions in a digital radio or modem and any filter structure that is required to efficiently process large sample rate factor. These filters are normally located after the sigma delta modulator and have regular structure. These types of filters do not require multipliers and the coefficient storage unlike in the normal digital FIR and IIR filters because of all filter coefficients are unity. Hence, it can be efficiently implemented to operate at high speed. Hence, this book describes the Very Large Scale Integration (VLSI) implementation of the CIC filters that are suitable for high performance audio applications. A total of five cascaded integrators and comb pairs were chosen to meet the design requirement, particularly the effect of signal aliasing to increase the performance. The CIC filter makes use of pipeline architecture that consists of fast adders.", "venue": "ArXiv", "authors": ["Rozita  Teymourzadeh"], "year": 2018, "n_citations": 1}
{"id": 5834145, "s2_id": "9401131bfa58ae96a167d8865ea01cfb7891f61c", "title": "A Survey on Recent Hardware Data Prefetching Approaches with An Emphasis on Servers", "abstract": "Data prefetching, i.e., the act of predicting application's future memory accesses and fetching those that are not in the on-chip caches, is a well-known and widely-used approach to hide the long latency of memory accesses. The fruitfulness of data prefetching is evident to both industry and academy: nowadays, almost every high-performance processor incorporates a few data prefetchers for capturing various access patterns of applications; besides, there is a myriad of proposals for data prefetching in the research literature, where each proposal enhances the efficiency of prefetching in a specific way. In this survey, we discuss the fundamental concepts in data prefetching and study state-of-the-art hardware data prefetching approaches. Additional Key Words and Phrases: Data Prefetching, Scale-Out Workloads, Server Processors, and Spatio-Temporal Correlation.", "venue": "ArXiv", "authors": ["Mohammad  Bakhshalipour", "Mehran  Shakerinava", "Fatemeh  Golshan", "Ali  Ansari", "Pejman  Lotfi-Kamran", "Hamid  Sarbazi-Azad"], "year": 2020, "n_citations": 0}
{"id": 5835715, "s2_id": "0c8392acaad4bf4bd98c82124042a9d8d611cecc", "title": "Solving the subset-sum problem with a light-based device", "abstract": "We propose an optical computational device which uses light rays for solving the subset-sum problem. The device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible subsets of the given set. To each arc we assign either a number from the given set or a predefined constant. When the light is passing through an arc it is delayed by the amount of time indicated by the number placed in that arc. At the destination node we will check if there is a ray whose total delay is equal to the target value of the subset sum problem (plus some constants). The proposed optical solution solves a NP-complete problem in time proportional with the target sum, but requires an exponential amount of energy.", "venue": "Natural Computing", "authors": ["Mihai  Oltean", "Oana  Muntean"], "year": 2007, "n_citations": 34}
{"id": 5836064, "s2_id": "2fb4e087aaad47f7cf195b3543f71824c6c2377b", "title": "A Survey of Resource Management for Processing-in-Memory and Near-Memory Processing Architectures", "abstract": "Due to the amount of data involved in emerging deep learning and big data applications, operations related to data movement have quickly become a bottleneck. Data-centric computing (DCC), as enabled by processing-in-memory (PIM) and near-memory processing (NMP) paradigms, aims to accelerate these types of applications by moving the computation closer to the data. Over the past few years, researchers have proposed various memory architectures that enable DCC systems, such as logic layers in 3D-stacked memories or charge-sharing-based bitwise operations in dynamic random-access memory (DRAM). However, application-specific memory access patterns, power and thermal concerns, memory technology limitations, and inconsistent performance gains complicate the offloading of computation in DCC systems. Therefore, designing intelligent resource management techniques for computation offloading is vital for leveraging the potential offered by this new paradigm. In this article, we survey the major trends in managing PIM and NMP-based DCC systems and provide a review of the landscape of resource management techniques employed by system designers for such systems. Additionally, we discuss the future challenges and opportunities in DCC management.", "venue": "ArXiv", "authors": ["Kamil  Khan", "Sudeep  Pasricha", "Ryan Gary Kim"], "year": 2020, "n_citations": 1}
{"id": 5837305, "s2_id": "d8b8af0eed2aab53ac7fa5be8deccf102f02bae4", "title": "Auto-generation of pipelined hardware designs for polar encoder", "abstract": "This paper presents a general framework for auto-generation of pipelined polar encoder architectures. The proposed framework could be well represented by a general formula. Given arbitrary code length N and the level of parallelism M, the formula could specify the corresponding hardware architecture. We have written a compiler which could read the formula and then automatically generate its register-transfer level (RTL) description suitable for FPGA or ASIC implementation. With this hardware generation system, one could explore the design space and make a trade-off between cost and performance. Our experimental results have demonstrated the efficiency of this auto-generator for polar encoder architectures.", "venue": "2018 China Semiconductor Technology International Conference (CSTIC)", "authors": ["Zhiwei  Zhong", "Xiaohu  You", "Chuan  Zhang"], "year": 2018, "n_citations": 4}
{"id": 5837696, "s2_id": "ec4ea33f7aa400fdeb790951600103f67cb677c7", "title": "Efficient Hardware Design and Implementation of Encrypted MIPS Processor", "abstract": "The paper describes the design and hardware implementation of 32-bit encrypted MIPS processor based on MIPS pipeline architecture. The organization of pipeline stages in such a way that pipeline can be clocked at high frequency. Encryption and Decryption blocks of data encryption standard (DES) cryptosystem and dependency among themselves are explained in detail with the help of block diagram. In order to increase the processor functionality and performance, especially for security applications we include three new instructions 32-bit LKLW, LKUW and CRYPT. The design has been synthesized at 40nm process technology targeting using Xilinx Virtex-6 device. The encrypted MIPS pipeline processor can work at 218MHz at synthesis level and 744MHz at simulation level.", "venue": "ArXiv", "authors": ["Kirat Pal Singh", "Dilip  Kumar"], "year": 2015, "n_citations": 1}
{"id": 5850284, "s2_id": "2fd5afc4333d00acc2c9a50c717925b98fadb5d9", "title": "BSSSN: Bit String Swapping Sorting Network for Reversible Logic Synthesis", "abstract": "In this paper, we have introduced the notion of UselessGate and ReverseOperation. We have also given an algorithm to implement a sorting network for reversible logic synthesis based on swapping bit strings. The network is constructed in terms of n*n Toffoli Gates read from left to right and it has shown that there will be no more gates than the number of swappings the algorithm requires. The gate complexity of the network is O(n2). The number of gates in the network can be further reduced by template reduction technique and removing UselessGate from the network.", "venue": "ArXiv", "authors": ["Md. Saiful Islam"], "year": 2010, "n_citations": 6}
{"id": 5851454, "s2_id": "93943a87eb451d11163d99a7ecf06944ba414c2a", "title": "Occlum: Secure and Efficient Multitasking Inside a Single Enclave of Intel SGX", "abstract": "Intel Software Guard Extensions (SGX) enables user-level code to create private memory regions called enclaves, whose code and data are protected by the CPU from software and hardware attacks outside the enclaves. Recent work introduces library operating systems (LibOSes) to SGX so that legacy applications can run inside enclaves with few or even no modifications. As virtually any non-trivial application demands multiple processes, it is essential for LibOSes to support multitasking. However, none of the existing SGX LibOSes support multitasking both securely and efficiently. This paper presents Occlum, a system that enables secure and efficient multitasking on SGX. We implement the LibOS processes as SFI-Isolated Processes (SIPs). SFI is a software instrumentation technique for sandboxing untrusted modules (called domains). We design a novel SFI scheme named MPX-based, Multi-Domain SFI (MMDSFI) and leverage MMDSFI to enforce the isolation of SIPs. We also design an independent verifier to ensure the security guarantees of MMDSFI. With SIPs safely sharing the single address space of an enclave, the LibOS can implement multitasking efficiently. The Occlum LibOS outperforms the state-of-the-art SGX LibOS on multitasking-heavy workloads by up to 6,600x on micro-benchmarks and up to 500x on application benchmarks.", "venue": "ASPLOS", "authors": ["Youren  Shen", "Hongliang  Tian", "Yu  Chen", "Kang  Chen", "Runji  Wang", "Yi  Xu", "Yubin  Xia"], "year": 2020, "n_citations": 26}
{"id": 5856075, "s2_id": "887dde12447a0050ea30d5d5b02440ff8fcd7285", "title": "ScalaBFS: A Scalable BFS Accelerator on HBM-Enhanced FPGAs", "abstract": "High Bandwidth Memory (HBM) provides massive aggregated memory bandwidth by exposing multiple memory channels to the processing units. To achieve high performance, an accelerator built on top of an FPGA configured with HBM (i.e., FPGA-HBM platform) needs to scale its performance according to the available memory channels. In this paper, we propose an accelerator for BFS (Breadth-First Search) algorithm, named as ScalaBFS, that builds multiple processing elements to sufficiently exploit the high bandwidth of HBM to improve efficiency. We implement the prototype system of ScalaBFS and conduct BFS in both real-world and synthetic scale-free graphs on Xilinx Alveo U280 FPGA card (real hardware). The experimental results show that ScalaBFS scales its performance almost linearly according to the available memory pseudo channels (PCs) from the HBM2 subsystem of U280. By fully using the 32 PCs and building 64 processing elements (PEs) on U280, ScalaBFS achieves a performance up to 19.7 GTEPS (Giga Traversed Edges Per Second). When conducting BFS in sparse real-world graphs, ScalaBFS achieves equivalent GTEPS to Gunrock running on the state-of-art Nvidia V100 GPU that features 64-PC HBM2 (twice memory bandwidth than U280).", "venue": "ArXiv", "authors": ["Chenhao  Liu", "Zhiyuan  Shao", "Zeke  Wang", "Kexin  Li", "Minkang  Wu", "Jiajie  Chen", "Xiaofei  Liao", "Hai  Jin"], "year": 2021, "n_citations": 0}
{"id": 5858925, "s2_id": "7da4337c0747070242ce041089da9c7b827f344c", "title": "Blockchain Machine: A Network-Attached Hardware Accelerator for Hyperledger Fabric", "abstract": "In this paper, we demonstrate how Hyperledger Fabric, one of the most popular permissioned blockchains, can benefit from networkattached acceleration. The scalability and peak performance of Fabric is primarily limited by the bottlenecks present in its block validation/commit phase. We propose Blockchain Machine, a hardware accelerator coupled with a hardware-friendly communication protocol, to act as the validator peer. It can be adapted to applications and their smart contracts, and is targeted for a server with network-attached FPGA acceleration card. The BlockchainMachine retrieves blocks and their transactions in hardware directly from the network interface, which are then validated through a configurable and efficient block-level and transaction-level pipeline. The validation results are then transferred to the host CPU where non-bottleneck operations are executed. From our implementation integrated with Fabric v1.4 LTS, we observed up to 12\u00d7 speedup in block validation when compared to software-only validator peer, with commit throughput of up to 68,900 tps. Our work provides an acceleration platform that will foster further research on hardware acceleration of permissioned blockchains.", "venue": "ArXiv", "authors": ["Haris  Javaid", "Ji  Yang", "Nathania  Santoso", "Mohit  Upadhyay", "Sundararajarao  Mohan", "Chengchen  Hu", "Gordon  Brebner"], "year": 2021, "n_citations": 1}
{"id": 5862600, "s2_id": "fe63080debb0020d585c68a311116550f31a2d62", "title": "BISMO: A Scalable Bit-Serial Matrix Multiplication Overlay for Reconfigurable Computing", "abstract": "Matrix-matrix multiplication is a key computational kernel for numerous applications in science and engineering, with ample parallelism and data locality that lends itself well to high-performance implementations. Many matrix multiplication-dependent applications can use reduced-precision integer or fixed-point representations to increase their performance and energy efficiency while still offering adequate quality of results. However, precision requirements may vary between different application phases or depend on input data, rendering constant-precision solutions ineffective. We present BISMO, a vectorized bit-serial matrix multiplication overlay for reconfigurable computing. BISMO utilizes the excellent binary-operation performance of FPGAs to offer a matrix multiplication performance that scales with required precision and parallelism. We characterize the resource usage and performance of BISMO across a range of parameters to build a hardware cost model, and demonstrate a peak performance of 6.5 TOPS on the Xilinx PYNQ-Z1 board.", "venue": "2018 28th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Yaman  Umuroglu", "Lahiru  Rasnayake", "Magnus  Sj\u00e4lander"], "year": 2018, "n_citations": 43}
{"id": 5863171, "s2_id": "f8d021c1c501e8d8a950c9d67aa52071a18bc1ea", "title": "MeltdownPrime and SpectrePrime: Automatically-Synthesized Attacks Exploiting Invalidation-Based Coherence Protocols", "abstract": "The recent Meltdown and Spectre attacks highlight the importance of automated verification techniques for identifying hardware security vulnerabilities. We have developed a tool for synthesizing microarchitecture-specific programs capable of producing any user-specified hardware execution pattern of interest. Our tool takes two inputs: a formal description of (i) a microarchitecture in a domain-specific language, and (ii) a microarchitectural execution pattern of interest, e.g. a threat pattern. All programs synthesized by our tool are capable of producing the specified execution pattern on the supplied microarchitecture. \nWe used our tool to specify a hardware execution pattern common to Flush+Reload attacks and automatically synthesized security litmus tests representative of those that have been publicly disclosed for conducting Meltdown and Spectre attacks. We also formulated a Prime+Probe threat pattern, enabling our tool to synthesize a new variant of each---MeltdownPrime and SpectrePrime. Both of these new exploits use Prime+Probe approaches to conduct the timing attack. They are both also novel in that they are 2-core attacks which leverage the cache line invalidation mechanism in modern cache coherence protocols. These are the first proposed Prime+Probe variants of Meltdown and Spectre. But more importantly, both Prime attacks exploit invalidation-based coherence protocols to achieve the same level of precision as a Flush+Reload attack. While mitigation techniques in software (e.g., barriers that prevent speculation) will likely be the same for our Prime variants as for original Spectre and Meltdown, we believe that hardware protection against them will be distinct. As a proof of concept, we implemented SpectrePrime as a C program and ran it on an Intel x86 processor, averaging about the same accuracy as Spectre over 100 runs---97.9% for Spectre and 99.95% for SpectrePrime.", "venue": "ArXiv", "authors": ["Caroline  Trippel", "Daniel  Lustig", "Margaret  Martonosi"], "year": 2018, "n_citations": 50}
{"id": 5872950, "s2_id": "0a6fbcf5c3141f46b0206031d3b63a657a4e3654", "title": "A High Dynamic Range 3-Moduli-Set with Efficient Reverse Converter", "abstract": "-Residue Number System (RNS) is a valuable tool for fast and parallel arithmetic. It has a wide application in digital signal processing, fault tolerant systems, etc. In this work, we introduce the 3-moduli set {2^n, 2^{2n}-1, 2^{2n}+1} and propose its residue to binary converter using the Chinese Remainder Theorem. We present its simple hardware implementation that mainly includes one Carry Save Adder (CSA) and a Modular Adder (MA). We compare the performance and area utilization of our reverse converter to the reverse converters of the moduli sets {2^n-1, 2^n, 2^n+1, 2^{2n}+1} and {2^n-1, 2^n, 2^n+1, 2^n-2^{(n+1)/2}+1, 2^n+2^{(n+1)/2}+1} that have the same dynamic range and we demonstrate that our architecture is better in terms of performance and area utilization. Also, we show that our reverse converter is faster than the reverse converter of {2^n-1, 2^n, 2^n+1} for dynamic ranges like 8-bit, 16-bit, 32-bit and 64-bit however it requires more area.", "venue": "ArXiv", "authors": ["Arash  Hariri", "Keivan  Navi", "Reza  Rastegar"], "year": 2009, "n_citations": 4}
{"id": 5873372, "s2_id": "9571eb24abe0b19ad38357420358197d979cfaea", "title": "L-shape based layout fracturing for e-beam lithography", "abstract": "Layout fracturing is a fundamental step in mask data preparation and e-beam lithography (EBL) writing. To increase EBL throughput, recently a new L-shape writing strategy is proposed, which calls for new L-shape fracturing, versus the conventional rectangular fracturing. Meanwhile, during layout fracturing, one must minimize very small/narrow features, also called slivers, due to manufacturability concern. This paper addresses this new research problem of how to perform L-shaped fracturing with sliver minimization. We propose two novel algorithms. The first one, rectangular merging (RM), starts from a set of rectangular fractures and merges them optimally to form L-shape fracturing. The second algorithm, direct L-shape fracturing (DLF), directly and effectively fractures the input layouts into L-shapes with sliver minimization. The experimental results show that our algorithms are very effective.", "venue": "2013 18th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Bei  Yu", "Jhih-Rong  Gao", "David Z. Pan"], "year": 2013, "n_citations": 14}
{"id": 5876605, "s2_id": "a2c19aabbdd96040e704236c8ea1f203ebf84bc3", "title": "Enabling Resource-Aware Mapping of Spiking Neural Networks via Spatial Decomposition", "abstract": "With growing model complexity, mapping spiking neural network (SNN)-based applications to tile-based neuromorphic hardware is becoming increasingly challenging. This is because the synaptic storage resources on a tile, viz., a crossbar, can accommodate only a fixed number of presynaptic connections per postsynaptic neuron. For complex SNN models that have many presynaptic connections per neuron, some connections may need to be pruned after training to fit onto the tile resources, leading to a loss in the model quality, e.g., accuracy. In this letter, we propose a novel unrolling technique that decomposes a neuron function with many presynaptic connections into a sequence of homogeneous neural units, where each neural unit is a function computation node, with two presynaptic connections. This spatial decomposition technique significantly improves crossbar utilization and retains all presynaptic connections, resulting in no loss of the model quality derived from connection pruning. We integrate the proposed technique within an existing SNN mapping framework and evaluate it using machine learning applications on the DYNAP-SE state-of-the-art neuromorphic hardware. Our results demonstrate an average 60% lower crossbar requirement, $9\\times $ higher synapse utilization, 62% lower wasted energy on the hardware, and between 0.8% and 4.6% increase in the model quality.", "venue": "IEEE Embedded Systems Letters", "authors": ["Adarsha  Balaji", "Shihao  Song", "Anup  Das", "Jeffrey  Krichmar", "Nikil  Dutt", "James  Shackleford", "Nagarajan  Kandasamy", "Francky  Catthoor"], "year": 2021, "n_citations": 14}
{"id": 5895793, "s2_id": "59a6a16001a90909ae2bb2ce218ac9c64aa934fb", "title": "Interfacing the ControlLogix PLC over Ethernet/IP", "abstract": "The Allen-Bradley ControlLogix [1] line of programmable logic controllers (PLCs) offers several interfaces: Ethernet, ControlNet, DeviceNet, RS-232 and others. The ControlLogix Ethernet interface module 1756-ENET uses EtherNet/IP, the ControlNet protocol [2], encapsulated in Ethernet packages, with specific service codes [3]. A driver for the Experimental Physics and Industrial Control System (EPICS) has been developed that utilizes this EtherNet/IP protocol for controllers running the vxWorks RTOS as well as a Win32 and Unix/Linux test program. Features, performance and limitations of this interface are presented.", "venue": "ArXiv", "authors": ["K.-U.  Kasemir", "L. R. Dalesio"], "year": 2001, "n_citations": 12}
{"id": 5902352, "s2_id": "6f933b5b7d30c80a88ed70dab8103330af97ad2c", "title": "Leakage-aware interconnect for on-chip network", "abstract": "On-chip networks have been proposed as the interconnect fabric for future systems-on-chip and multi-processors on chip. Power is one of the main constraints of these systems and the interconnect consumes a significant portion of the power budget. In this paper, we propose four leakage-aware interconnect schemes. Our schemes achieve 10.13%-63.57% active leakage savings and 12.35%-95.96% standby leakage savings across schemes while the delay penalty ranges from 0% to 4.69%.", "venue": "Design, Automation and Test in Europe", "authors": ["Yuh-Fang  Tsai", "Narayanan  Vijaykrishnan", "Yuan  Xie", "Mary Jane Irwin"], "year": 2005, "n_citations": 4}
{"id": 5903267, "s2_id": "ff4bb7fce973c58619d033978dbc18936ae24103", "title": "A General Neural Network Hardware Architecture on FPGA", "abstract": "Field Programmable Gate Arrays (FPGAs) plays an increasingly important role in data sampling and processing industries due to its highly parallel architecture, low power consumption, and flexibility in custom algorithms. Especially, in the artificial intelligence field, for training and implement the neural networks and machine learning algorithms, high energy efficiency hardware implement and massively parallel computing capacity are heavily demanded. Therefore, many global companies have applied FPGAs into AI and Machine learning fields such as autonomous driving and Automatic Spoken Language Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3]. Considering the FPGAs great potential in these fields, we tend to implement a general neural network hardware architecture on XILINX ZU9CG System On Chip (SOC) platform [4], which contains abundant hardware resource and powerful processing capacity. The general neural network architecture on the FPGA SOC platform can perform forward and backward algorithms in deep neural networks (DNN) with high performance and easily be adjusted according to the type and scale of the neural networks.", "venue": "ArXiv", "authors": ["Yufeng  Hao"], "year": 2017, "n_citations": 6}
{"id": 5904578, "s2_id": "3a4b0cfcb04b1af6497e2b129461977af8b1f12e", "title": "Parameters Affecting the Resilience of Scale-Free Networks to Random Failures", "abstract": "It is commonly believed that scale-free networks are robust to massive numbers of random node deletions. For example, Cohen et al. in (1) study scale-free networks including some which approximate the measured degree distribution of the Internet. Their results suggest that if each node in this network failed independently with probability 0.99, most of the remaining nodes would still be connected in a giant component. In this paper, we show that a large and important subclass of scale-free networks are not robust to massive numbers of random node deletions. In particular, we study scale-free networks which have minimum node degree of 1 and a power-law degree distribution beginning with nodes of degree 1 (power-law networks). We show that, in a power-law network approximating the Internet's reported distribution, when the probability of deletion of each node is 0.5 only about 25% of the surviving nodes in the network remain connected in a giant component, and the giant component does not persist beyond a critical failure rate of 0.9. The new result is partially due to improved analytical accommodation of the large number of degree-0 nodes that result after node deletions. Our results apply to power-law networks with a wide range ofmore\u00a0\u00bb power-law exponents, including Internet-like networks. We give both analytical and empirical evidence that such networks are not generally robust to massive random node deletions.\u00ab\u00a0less", "venue": "ArXiv", "authors": ["Hamilton  Link", "Randall A. LaViolette", "Jared  Saia", "Terran  Lane"], "year": 2005, "n_citations": 4}
{"id": 5906387, "s2_id": "16124ea857f8fdee68e7ac84f56a612bb728ec47", "title": "VeRLPy: Python Library for Verification of Digital Designs with Reinforcement Learning", "abstract": "Digital hardware is verified by comparing its behavior against a reference model on a range of randomly generated input signals. The random generation of the inputs hopes to achieve sufficient coverage of the different parts of the design. However, such coverage is often difficult to achieve, amounting to large verification efforts and delays. An alternative is to use Reinforcement Learning (RL) to generate the inputs by learning to prioritize those inputs which can more efficiently explore the design under test. In this work, we present VeRLPy [3], an open-source library to allow RL-driven verification with limited additional engineering overhead. This contributes to two broad movements within the EDA community of (a) moving to open-source tool chains and (b) reducing barriers for development with Python support. We also demonstrate the use of VeRLPy for a few designs and establish its value over randomly generated input signals.", "venue": "AIMLSystems", "authors": ["Aebel Joe Shibu", "S  Sadhana", "N  Shilpa", "Pratyush  Kumar"], "year": 2021, "n_citations": 0}
{"id": 5906460, "s2_id": "9679c690e1d9a2f4fe3b7040c41a0581c5ba40c6", "title": "Memristive Threshold Logic Circuit Design of Fast Moving Object Detection", "abstract": "Real-time detection of moving objects involves memorization of features in the template image and their comparison with those in the test image. At high sampling rates, such techniques face the problems of high algorithmic complexity and component delays. We present a new resistive switching-based threshold logic cell which encodes the pixels of a template image. The cell comprises a voltage divider circuit that programs the resistances of the memristors arranged in a single-node threshold logic network, and the output is encoded as a binary value by using a CMOS inverter gate. When a test image is applied to the template-programmed cell, a mismatch in the respective pixels is seen as a change in the output voltage of the cell. The proposed cell when compared with CMOS equivalent implementation shows improved performance in area, leakage power, power dissipation, and delay.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Akshay Kumar Maan", "Dinesh Sasi Kumar", "Sherin  Sugathan", "Alex Pappachen James"], "year": 2015, "n_citations": 28}
{"id": 5910847, "s2_id": "3c0cd65e7ef97c2d109b38c6ae25191709d04e51", "title": "FPGA Implementation of pipeline Digit-Slicing Multiplier-Less Radix 2 power of 2 DIF SDF Butterfly for Fourier Transform Structure", "abstract": "The need for wireless communication has driven the communication systems to high performance. However, the main bottleneck that affects the communication capability is the Fast Fourier Transform (FFT), which is the core of most modulators. This paper presents FPGA implementation of pipeline digit-slicing multiplier-less radix 22 DIF (Decimation In Frequency) SDF (single path delay feedback) butterfly for FFT structure. The approach is taken, in order to reduce computation complexity in butterfly multiplier, the digit-slicing multiplier-less technique was utilized in the critical path of pipeline Radix-22 DIF SDF FFT structure. The proposed design focused on the trade-off between the speed and active silicon area for the chip implementation. The multiplier input data was sliced into four blocks each one with four bits to process at the same time in parallel. The new architecture was investigated and simulated with MATLAB software. The Verilog HDL code in Xilinx ISE environment was derived to describe the FFT Butterfly functionality and was downloaded to Virtex II FPGA board. Consequently, the Virtex-II FG456 Proto board was used to implement and test the design on the real hardware. As a result, from the findings, the synthesis report indicates the maximum clock frequency of 555.75 MHz with the total equivalent gate count of 32,146 is a marked and significant improvement over Radix 22 DIF SDF FFT butterfly. In comparison with the conventional butterfly architecture design which can only run at a maximum clock frequency of 200.102 MHz and the conventional multiplier can only run at a maximum clock frequency of 221.140 MHz, the proposed system exhibits better results.", "venue": "ArXiv", "authors": ["Yazan Samir Algnabi", "Rozita  Teymourzadeh", "Masuri  Othman", "Md. Shabiul Islam"], "year": 2018, "n_citations": 0}
{"id": 5914622, "s2_id": "5e9881a39961c3d4ea937488e736f4dcb9771c60", "title": "Analytical formulations of Peer-to-Peer Connection Efficiency", "abstract": "Use of Peer-to-Peer (P2P) service networks introduces a new communication paradigm because peers are both clients and servers and so each peer may provide/request services to/from other peers. Empirical studies of P2P networks have been undertaken and reveal useful characteristics. However there is to date little analytical work to describe P2P networks with respect to their communication paradigm and their interconnections. This paper provides an analytical formulation and optimisation of peer connection efficiency, in terms of minimising the fraction of wasted connection time. Peer connection efficiency is analysed for both a uni- and multi-connected peer. Given this fundamental optimisation, the paper optimises the number of connections that peers should make use of as a function of network load, in terms of minimising the total queue size that requests in the P2P network experience. The results of this paper provide a basis for engineering high performance P2P interconnection networks. The optimisations are useful for reducing bandwidth and power consumption, e.g. in the case of peers being mobile devices with a limited power supply. Also these results could be used to determine when a (virtual) circuit should be switched to support a connection.", "venue": "ArXiv", "authors": ["Aaron  Harwood"], "year": 2003, "n_citations": 0}
{"id": 5916006, "s2_id": "be627831520fe1d11227bf73b87e54ffe21a7f7e", "title": "Virtual Machine Support for Many-Core Architectures: Decoupling Abstract from Concrete Concurrency Models", "abstract": "The upcoming many-core architectures require software developers to exploit concurrency to utilize available computational power. Today's high-level language virtual machines (VMs), which are a cornerstone of software development, do not provide sufficient abstraction for concurrency concepts. We analyze concrete and abstract concurrency models and identify the challenges they impose for VMs. To provide sufficient concurrency support in VMs, we propose to integrate concurrency operations into VM instruction sets. Since there will always be VMs optimized for special purposes, our goal is to develop a methodology to design instruction sets with concurrency support. Therefore, we also propose a list of trade-offs that have to be investigated to advise the design of such instruction sets. As a first experiment, we implemented one instruction set extension for shared memory and one for non-shared memory concurrency. From our experimental results, we derived a list of requirements for a full-grown experimental environment for further research.", "venue": "PLACES", "authors": ["Stefan  Marr", "Michael  Haupt", "Stijn  Timbermont", "Bram  Adams", "Theo  D'Hondt", "Pascal  Costanza", "Wolfgang De Meuter"], "year": 2009, "n_citations": 13}
{"id": 5921357, "s2_id": "3d2741ac332b7909e0e848998979597cc3d3596f", "title": "Wrangling Rogues: Managing Experimental Post-Moore Architectures", "abstract": "The Rogues Gallery is a new experimental testbed that is focused on tackling \"rogue\" architectures for the Post-Moore era of computing. While some of these devices have roots in the embedded and high-performance computing spaces, managing current and emerging technologies provides a challenge for system administration that are not always foreseen in traditional data center environments. \nWe present an overview of the motivations and design of the initial Rogues Gallery testbed and cover some of the unique challenges that we have seen and foresee with upcoming hardware prototypes for future post-Moore research. Specifically, we cover the networking, identity management, scheduling of resources, and tools and sensor access aspects of the Rogues Gallery and techniques we have developed to manage these new platforms.", "venue": "ArXiv", "authors": ["Will  Powell", "E. Jason Riedy", "Jeffrey S. Young"], "year": 2018, "n_citations": 0}
{"id": 5926539, "s2_id": "eaf281dc1a52e0d4add2cd387a224ce613d7d1c5", "title": "Dynamic RF Combining for Multi-Antenna Ambient Energy Harvesting", "abstract": "Ambient radio frequency (RF) energy harvesting (EH) technology is key to realize self-sustainable, always-on, low-power, massive Internet of Things networks. Typically, rigid (non-adaptable to channel fluctuations) multi-antenna receive architectures are proposed to support reliable EH operation. Herein, we introduce a dynamic RF combining architecture for ambient RF EH use cases, and exemplify the attainable performance gains via three simple phase shifts\u2019 exploration mechanisms, namely, brute force (BF), sequential testing (ST) and codebook based (CB). Among the proposed mechanisms, BF demands the highest power consumption, while CB requires the highest-resolution phase shifters, thus tipping the scales in favor of ST. Finally, we show that the performance gains of ST over a rigid RF combining scheme increase with the number of receive antennas and energy transmitters\u2019 deployment density.", "venue": "IEEE Wireless Communications Letters", "authors": ["Onel Luis Alcaraz L'opez", "Bruno  Clerckx", "Matti  Latva-aho"], "year": 2021, "n_citations": 0}
{"id": 5927569, "s2_id": "be46519e31c757bedd4af969a98ab33283466f7d", "title": "A low-voltage, Low-Power 4-bit BCD adder, designed using the Clock Gated Power Gating, and the DVT scheme", "abstract": "This paper proposes a Low-Power, Energy Efficient 4bit Binary Coded Decimal (BCD) adder design where the conventional 4-bit BCD adder has been modified with the Clock Gated Power Gating Technique. Moreover, the concept of DVT (Dual-vth) scheme has been introduced while designing the full adder blocks to reduce the Leakage Power, as well as, to maintain the overall performance of the entire circuit. The reported architecture of 4-bit BCD adder is designed using 45 nm technology and it consumes 1.384 \u03bcWatt of Average Power while operating with a frequency of 200 MHz, and a Supply Voltage (Vdd) of 1 Volt. The results obtained from different simulation runs on SPICE, indicate the superiority of the proposed design compared to the conventional 4-bit BCD adder. Considering the product of Average Power and Delay, for the operating frequency of 200 MHz, a fair 47.41% reduction compared to the conventional design has been achieved with this proposed scheme.", "venue": "2013 IEEE International Conference on Signal Processing, Computing and Control (ISPCC)", "authors": ["Dipankar  Saha", "Subhramita  Basak", "Sagar  Mukherjee", "Chandan Kumar Sarkar"], "year": 2013, "n_citations": 2}
{"id": 5929606, "s2_id": "372130006b7624d568110f337a0e3c7b529ed912", "title": "Optimizing Memory Performance of Xilinx FPGAs under Vitis", "abstract": "Plenty of research efforts have been devoted to FPGA-based acceleration, due to its low latency and high energy efficiency. However, using the original low-level hardware description languages like Verilog to program FPGAs requires generally good knowledge of hardware design details and hand-on experiences. Fortunately, the FPGA community intends to address this low programmability issues. For example, , with the intention that programming FPGAs is just as easy as programming GPUs. Even though Vitis is proven to increase programmability, we cannot directly obtain high performance without careful design regarding hardware pipeline and memory this http URL this paper, we focus on the memory subsystem, comprehensively and systematically benchmarking the effect of optimization methods on memory performance. Upon benchmarking, we quantitatively analyze the typical memory access patterns for a broad range of applications, including AI, HPC, and database. Further, we also provide the corresponding optimization direction for each memory access pattern so as to improve overall performance.", "venue": "ArXiv", "authors": ["Ruoshi  Li", "Hongjing  Huang", "Zeke  Wang", "Zhiyuan  Shao", "Xiaofei  Liao", "Hai  Jin"], "year": 2020, "n_citations": 3}
{"id": 5930467, "s2_id": "67a926fb6c6af6beb7b5452509783a97daede4e4", "title": "Stochastic fuzzy controller", "abstract": "A standard approach to building a fuzzy controller based on stochastic logic uses binary random signals with an average (expected value of a random variable) in the range [0, 1]. A different approach is presented, founded on a representation of the membership functions with the probability density functions.", "venue": "ArXiv", "authors": ["Franc  Jurkovic"], "year": 2004, "n_citations": 0}
{"id": 5935477, "s2_id": "1627a88a0381273e11e2bd4ffa21d907dcaa0a02", "title": "Buffer insertion for bridges and optimal buffer sizing for communication sub-system of systems-on-chip", "abstract": "We have presented an optimal buffer sizing and buffer insertion methodology which uses stochastic models of the architecture and continuous time Markov decision processes CTMDP. Such a methodology is useful in managing the scarce buffer resources available on chip as compared to network based data communication which can have large buffer space. The modeling of this problem in terms of a CTMDP framework lead to a nonlinear formulation due to usage of bridges in the bus architecture. We present a methodology to split the problem into several smaller though linear systems and we then solve these subsystems.", "venue": "Design, Automation and Test in Europe", "authors": ["Sankalp S. Kallakuri", "Alex  Doboli", "Eugene A. Feinberg"], "year": 2005, "n_citations": 1}
{"id": 5936035, "s2_id": "1878d8479794f1d1c5ac318111bfa889fe0a7a3e", "title": "A Ternary Digital to Analog Converter with High Power Output and 170-dB Dynamic Range", "abstract": "A prototype of a very high dynamic range 32-bits Digital to Analog Converter (DAC) was designed and built for the purpose of direct auditory stimulus generation. It provides signals from less than 100 nV up to 50 Watts peak power output, driving a 32-Ohms earphone or speaker. The use of ternary cells makes possible a 170 dB dynamic range that is basically limited by thermal noise only.", "venue": "ArXiv", "authors": ["Guido  Stolfi"], "year": 2012, "n_citations": 1}
{"id": 5938272, "s2_id": "376ba8942956f3e78506aa5ae27d0520691b3478", "title": "A Soft Processor Overlay with Tightly-coupled FPGA Accelerator", "abstract": "FPGA overlays are commonly implemented as coarse-grained reconfigurable architectures with a goal to improve designers' productivity through balancing flexibility and ease of configuration of the underlying fabric. To truly facilitate full application acceleration, it is often necessary to also include a highly efficient processor that integrates and collaborates with the accelerators while maintaining the benefits of being implemented within the same overlay framework. This paper presents an open-source soft processor that is designed to tightly-couple with FPGA accelerators as part of an overlay framework. RISC-V is chosen as the instruction set for its openness and portability, and the soft processor is designed as a 4-stage pipeline to balance resource consumption and performance when implemented on FPGAs. The processor is generically implemented so as to promote design portability and compatibility across different FPGA platforms. Experimental results show that integrated software-hardware applications using the proposed tightly-coupled architecture achieve comparable performance as hardware-only accelerators while the proposed architecture provides additional run-time flexibility. The processor has been synthesized to both low-end and high-performance FPGA families from different vendors, achieving the highest frequency of 268.67MHz and resource consumption comparable to existing RISC-V designs.", "venue": "ArXiv", "authors": ["Ho-Cheung  Ng", "Cheng  Liu", "Hayden Kwok-Hay So"], "year": 2015, "n_citations": 11}
{"id": 5939426, "s2_id": "f9cfeef228b1e2ec4e0c7098bbe8a43ab03210c0", "title": "Performance Implications of NoCs on 3D-Stacked Memories: Insights from the Hybrid Memory Cube", "abstract": "Three-dimensional (3D)-stacked memories, such as the Hybrid Memory Cube (HMC), provide a promising solution for overcoming the bandwidth wall between processors and memory by integrating memory and logic dies in a single stack. Such memories also utilize a network-on-chip (NoC) to connect their internal structural elements and to enable scalability. This novel usage of NoCs enables numerous benefits such as high bandwidth and memory-level parallelism and creates future possibilities for efficient processing-in-memory techniques. However, the implications of such NoC integration on the performance characteristics of 3D-stacked memories in terms of memory access latency and bandwidth have not been fully explored. This paper addresses this knowledge gap (i) by characterizing an HMC prototype using Micron's AC-510 accelerator board and by revealing its access latency and bandwidth behaviors; and (ii) by investigating the implications of such behaviors on system- and software-level designs. Compared to traditional DDR-based memories, our examinations reveal the performance impacts of NoCs for current and future 3D-stacked memories and demonstrate how the packet-based protocol, internal queuing characteristics, traffic conditions, and other unique features of the HMC affects the performance of applications.", "venue": "2018 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Ramyad  Hadidi", "Bahar  Asgari", "Jeffrey S. Young", "Burhan Ahmad Mudassar", "Kartikay  Garg", "Tushar  Krishna", "Hyesoon  Kim"], "year": 2018, "n_citations": 21}
{"id": 5940085, "s2_id": "7cb9d4dd9ecb58434ea11332d102188fbe9f1ec9", "title": "Enhanced Multigradient Dilution Preparation", "abstract": "In our paper the new algorithm enhanced multi gradient Dilution Preparation (EMDP) is discussed. This new algorithm is reported with a lab on chip or digital Microfluidic biochip to operate multiple operation on a tiny chip. We can use Digital Microfluidic biochip to operate multiple operation on a tiny chip. Samples are very costly which are used in any Biochemical laboratory Protocols. For the case of fast and high throughput application, It is essential to minimize the cost of operations and the time of operations and that is why one of the most challenging and important phase is sample preparation. In our proposed algorithm, we have hide to reduce sample droplets and waste droplets and for this purpose waste recycling is used, when different series of multi gradient targets concentration factors (CFS) are generated. We have compared our proposed algorithm with recent dilution techniques such as MTC, REMIA, and WARA. For the storage of intermediate droplets which, and generated during this process, on chip storage space 0(n) is needed.", "venue": "ArXiv", "authors": ["Meenakshi  Sanyal", "Somenath  Chakraborty"], "year": 2021, "n_citations": 0}
{"id": 5941007, "s2_id": "0c4fe1f1a8043e8f4175b21faca1b72bff8033e6", "title": "Enabling the Adoption of Processing-in-Memory: Challenges, Mechanisms, Future Research Directions", "abstract": "Poor DRAM technology scaling over the course of many years has caused DRAM-based main memory to increasingly become a larger system bottleneck. A major reason for the bottleneck is that data stored within DRAM must be moved across a pin-limited memory channel to the CPU before any computation can take place. This requires a high latency and energy overhead, and the data often cannot benefit from caching in the CPU, making it difficult to amortize the overhead. \nModern 3D-stacked DRAM architectures include a logic layer, where compute logic can be integrated underneath multiple layers of DRAM cell arrays within the same chip. Architects can take advantage of the logic layer to perform processing-in-memory (PIM), or near-data processing. In a PIM architecture, the logic layer within DRAM has access to the high internal bandwidth available within 3D-stacked DRAM (which is much greater than the bandwidth available between DRAM and the CPU). Thus, PIM architectures can effectively free up valuable memory channel bandwidth while reducing system energy consumption. \nA number of important issues arise when we add compute logic to DRAM. In particular, the logic does not have low-latency access to common CPU structures that are essential for modern application execution, such as the virtual memory and cache coherence mechanisms. To ease the widespread adoption of PIM, we ideally would like to maintain traditional virtual memory abstractions and the shared memory programming model. This requires efficient mechanisms that can provide logic in DRAM with access to CPU structures without having to communicate frequently with the CPU. To this end, we propose and evaluate two general-purpose solutions that minimize unnecessary off-chip communication for PIM architectures. We show that both mechanisms improve the performance and energy consumption of many important memory-intensive applications.", "venue": "ArXiv", "authors": ["Saugata  Ghose", "Kevin  Hsieh", "Amirali  Boroumand", "Rachata  Ausavarungnirun", "Onur  Mutlu"], "year": 2018, "n_citations": 37}
{"id": 5945392, "s2_id": "57f81a5bbda92f0ddf8101d9f4d2512f3c609ef8", "title": "Storage Class Memory: Principles, Problems, and Possibilities", "abstract": "Storage Class Memory (SCM) is a class of memory technology which has recently become viable for use. Their namearises from the fact that they exhibit non-volatility of data, similar to secondary storage while also having latencies comparable toprimary memory and byte-addressibility. In this area, Phase Change Memory (PCM), Spin-Transfer-Torque Random Access Memory(STT-RAM), and Resistive RAM (ReRAM) have emerged as the major contenders for commercial and industrial use. In this paper, wedescribe how these memory types function, while highlighting the problems of endurance and performance that these memory typesface. We also discuss the future possibilities of Multi-Level Cells (MLCs), as well as how SCM can be used to construct accelerators.", "venue": "ArXiv", "authors": ["Aditya K Kamath", "Leslie  Monis", "A Tarun Karthik", "Basavaraj  Talawar"], "year": 2019, "n_citations": 2}
{"id": 5949009, "s2_id": "d4daf543892d10a61bd9150cd5918d38c3996a71", "title": "Proceedings of the 2nd International Workshop on Overlay Architectures for FPGAs (OLAF 2016)", "abstract": "The 2nd International Workshop on Overlay Architectures for FPGAs (OLAF 2016) was held on 21 Mar, 2016 as a co-located workshop at the 24th ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA 2016). This year, the program committee selected 6 papers and 3 extended abstracts to be presented at the workshop, which are subsequently collected in this online volume.", "venue": "ArXiv", "authors": ["Hayden Kwok-Hay So", "John  Wawrzynek"], "year": 2016, "n_citations": 0}
{"id": 5949793, "s2_id": "5772e0e0f8f3aa00ef15d151a489c85f15ff93de", "title": "Towards a Multi-array Architecture for Accelerating Large-scale Matrix Multiplication on FPGAs", "abstract": "Large-scale floating-point matrix multiplication is a fundamental kernel in many scientific and engineering applications. Most existing work only focus on accelerating matrix multiplication on FPGA by adopting a linear systolic array. This paper towards the extension of this architecture by proposing a scalable and highly configurable multi-array architecture. In addition, we propose a work-stealing scheme to ensure the equality in the workload partition among multiple linear arrays. Furthermore, an analytical model is developed to determine the optimal design parameters. Experiments on a real-life convolutional neural network (CNN) show that we can obtain the optimal extension of the linear array architecture.", "venue": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "authors": ["Junzhong  Shen", "Yuran  Qiao", "You  Huang", "Mei  Wen", "Chunyuan  Zhang"], "year": 2018, "n_citations": 18}
{"id": 5959599, "s2_id": "b44b12d1b23f5cbc516a933a0517585b5ea928c7", "title": "DAS: Dynamic Adaptive Scheduling for Energy-Efficient Heterogeneous SoCs", "abstract": "Domain-specific systems-on-chip (DSSoCs) aim at bridging the gap between application-specific integrated circuits (ASICs) and general-purpose processors. Traditional operating system (OS) schedulers can undermine the potential of DSSoCs since their execution times can be orders of magnitude larger than the execution time of the task itself. To address this problem, we propose a dynamic adaptive scheduling (DAS) framework that combines the benefits of a fast (low-overhead) scheduler and a slow (sophisticated, high-performance but high-overhead) scheduler. Experiments with five real-world streaming applications show that DAS consistently outperforms both the fast and slow schedulers. For 40 different workloads, DAS achieves on average 1.29\u00d7 speedup and 45% lower EDP compared to the sophisticated scheduler at low data rates and 1.28\u00d7 speedup and 37% lower EDP than the fast scheduler when the workload complexity increases.", "venue": "IEEE Embedded Systems Letters", "authors": ["A. Alper Goksoy", "Anish  Krishnakumar", "Md Sahil Hassan", "Allen J. Farcas", "Ali  Akoglu", "Radu  Marculescu", "Umit Y. Ogras"], "year": 2021, "n_citations": 0}
{"id": 5959710, "s2_id": "04400a01703ff92e8730c7164bf9a6fa2dabd37d", "title": "DReAM: Dynamic Re-arrangement of Address Mapping to Improve the Performance of DRAMs", "abstract": "The initial location of data in DRAMs is determined and controlled by the 'address-mapping' and even modern memory controllers use a fixed and run-time-agnostic address mapping. On the other hand, the memory access pattern seen at the memory interface level will dynamically change at run-time. This dynamic nature of memory access pattern and the fixed behavior of address mapping process in DRAM controllers, implied by using a fixed address mapping scheme, means that DRAM performance cannot be exploited efficiently. DReAM is a novel hardware technique that can detect a workload-specific address mapping at run-time based on the application access pattern which improves the performance of DRAMs. The experimental results show that DReAM outperforms the best evaluated address mapping on average by 9%, for mapping-sensitive workloads, by 2% for mapping-insensitive workloads, and up to 28% across all the workloads. DReAM can be seen as an insurance policy capable of detecting which scenarios are not well served by the predefined address mapping.", "venue": "MEMSYS", "authors": ["Mohsen  Ghasempour", "Jim  Garside", "Aamer  Jaleel", "Mikel  Luj'an"], "year": 2016, "n_citations": 21}
{"id": 5962411, "s2_id": "696d4f3ac0f7093d9de403f4fbbccefd05071d1b", "title": "New approximate multiplier for low power digital signal processing", "abstract": "In this paper a low power multiplier is proposed. The proposed multiplier utilizes Broken-Array Multiplier approximation method on the conventional modified Booth multiplier. This method reduces the total power consumption of multiplier up to 58% at the cost of a small decrease in output accuracy. The proposed multiplier is compared with other approximate multipliers in terms of power consumption and accuracy. Furthermore, to have a better evaluation of the proposed multiplier efficiency, it has been used in designing a 30-tap low-pass FIR filter and the power consumption and accuracy are compared with that of a filter with conventional booth multipliers. The simulation results show a 17.1% power reduction at the cost of only 0.4 dB decrease in the output SNR.", "venue": "The 17th CSI International Symposium on Computer Architecture & Digital Systems (CADS 2013)", "authors": ["Farzad  Farshchi", "Muhammad Saeed Abrishami", "Sied Mehdi Fakhraie"], "year": 2013, "n_citations": 38}
{"id": 5963744, "s2_id": "b06b556169d8b55d6d8058164dd599c67c50c430", "title": "Understanding Reduced-Voltage Operation in Modern DRAM Chips: Characterization, Analysis, and Mechanisms", "abstract": "The energy consumption of DRAM is a critical concern in modern computing systems. Improvements in manufacturing process technology have allowed DRAM vendors to lower the DRAM supply voltage conservatively, which reduces some of the DRAM energy consumption. We would like to reduce the DRAM supply voltage more aggressively, to further reduce energy. Aggressive supply voltage reduction requires a thorough understanding of the effect voltage scaling has on DRAM access latency and DRAM reliability. \nIn this paper, we take a comprehensive approach to understanding and exploiting the latency and reliability characteristics of modern DRAM when the supply voltage is lowered below the nominal voltage level specified by DRAM standards. Using an FPGA-based testing platform, we perform an experimental study of 124 real DDR3L (low-voltage) DRAM chips manufactured recently by three major DRAM vendors. We find that reducing the supply voltage below a certain point introduces bit errors in the data, and we comprehensively characterize the behavior of these errors. We discover that these errors can be avoided by increasing the latency of three major DRAM operations (activation, restoration, and precharge). We perform detailed DRAM circuit simulations to validate and explain our experimental findings. We also characterize the various relationships between reduced supply voltage and error locations, stored data patterns, DRAM temperature, and data retention. \nBased on our observations, we propose a new DRAM energy reduction mechanism, called Voltron. The key idea of Voltron is to use a performance model to determine by how much we can reduce the supply voltage without introducing errors and without exceeding a user-specified threshold for performance loss. Voltron reduces the average system energy by 7.3% while limiting the average system performance loss to only 1.8%, for a variety of workloads.", "venue": "ArXiv", "authors": ["Kevin K. Chang", "Abdullah Giray Yaglik\u00e7i", "Saugata  Ghose", "Aditya  Agrawal", "Niladrish  Chatterjee", "Abhijith  Kashyap", "Donghyuk  Lee", "Mike  O'Connor", "Hasan  Hassan", "Onur  Mutlu"], "year": 2017, "n_citations": 8}
{"id": 5964579, "s2_id": "de738426bd2b832fbf855f521664d95c322bd6e8", "title": "RT-RCG: Neural Network and Accelerator Search Towards Effective and Real-time ECG Reconstruction from Intracardiac Electrograms", "abstract": "There exists a gap in terms of the signals provided by pacemakers (i.e., intracardiac electrogram (EGM)) and the signals doctors use (i.e., 12-lead electrocardiogram (ECG)) to diagnose abnormal rhythms. Therefore, the former, even if remotely transmitted, are not sufficient for doctors to provide a precise diagnosis, let alone make a timely intervention. To close this gap and make a heuristic step towards real-time critical intervention in instant response to irregular and infrequent ventricular rhythms, we propose a new framework dubbed RT-RCG to automatically search for (1) efficient Deep Neural Network (DNN) structures and then (2) corresponding accelerators, to enable Real-Time and high-quality Reconstruction of ECG signals from EGM signals. Specifically, RT-RCG proposes a new DNN search space tailored for ECG reconstruction from EGM signals, and incorporates a differentiable acceleration search (DAS) engine to efficiently navigate over the large and discrete accelerator design space to generate optimized accelerators. Extensive experiments and ablation studies under various settings consistently validate the effectiveness of our RT-RCG. To the best of our knowledge, RT-RCG is the first to leverage neural architecture search (NAS) to simultaneously tackle both reconstruction efficacy and efficiency.", "venue": "ArXiv", "authors": ["Yongan  Zhang", "Anton  Banta", "Yonggan  Fu", "Mathews M. John", "Allison  Post", "Mehdi  Razavi", "Joseph  Cavallaro", "Behnaam  Aazhang", "Yingyan  Lin"], "year": 2021, "n_citations": 1}
{"id": 5966974, "s2_id": "44ab0a2c3213279927bdb4ca08b4869d3c67cb7e", "title": "Medusa: A Scalable Interconnect for Many-Port DNN Accelerators and Wide DRAM Controller Interfaces", "abstract": "To cope with the increasing demand and computational intensity of deep neural networks (DNNs), industry and academia have turned to accelerator technologies. In particular, FPGAs have been shown to provide a good balance between performance and energy efficiency for accelerating DNNs. While significant research has focused on how to build efficient layer processors, the computational building blocks of DNN accelerators, relatively little attention has been paid to the on-chip interconnects that sit between the layer processors and the FPGA's DRAM controller. We observe a disparity between DNN accelerator interfaces, which tend to comprise many narrow ports, and FPGA DRAM controller interfaces, which tend to be wide buses. This mismatch causes traditional interconnects to consume significant FPGA resources. To address this problem, we designed Medusa: an optimized FPGA memory interconnect which transposes data in the interconnect fabric, tailoring the interconnect to the needs of DNN layer processors. Compared to a traditional FPGA interconnect, our design can reduce LUT and FF use by 4.7x and 6.0x, and improves frequency by 1.8x.", "venue": "2018 28th International Conference on Field Programmable Logic and Applications (FPL)", "authors": ["Yongming  Shen", "Tianchu  Ji", "Michael  Ferdman", "Peter A. Milder"], "year": 2018, "n_citations": 2}
{"id": 5968989, "s2_id": "840a6ffaf2cff29703006b13ffb9dcd78759d4f1", "title": "A survey of techniques for improving energy efficiency in embedded computing systems", "abstract": "Recent technological advances have greatly improved the performance and features of embedded systems. With the number of just mobile devices now reaching nearly equal to the population of Earth, embedded systems have truly become ubiquitous. These trends, however, have also made the task of managing their power consumption extremely challenging. In recent years, several techniques have been proposed to address this issue. In this paper, we survey the techniques for managing power consumption of embedded systems. We discuss the need of power management and provide a classification of the techniques on several important parameters to highlight their similarities and differences. This paper is intended to help the researchers and application-developers in gaining insights into the working of power management techniques and designing even more efficient high-performance embedded systems of tomorrow.", "venue": "Int. J. Comput. Aided Eng. Technol.", "authors": ["Sparsh  Mittal"], "year": 2014, "n_citations": 146}
{"id": 5971287, "s2_id": "0974d532c31d4b299d32eebb4512ff79f4f382dc", "title": "MFAGAN: A Compression Framework for Memory-Efficient On-Device Super-Resolution GAN", "abstract": "Generative adversarial networks (GANs) have promoted remarkable advances in single-image super-resolution (SR) by recovering photo-realistic images. However, high memory consumption of GAN-based SR (usually generators) causes performance degradation and more energy consumption, hindering the deployment of GAN-based SR into resource-constricted mobile devices. In this paper, we propose a novel compression framework Multi-scale Feature Aggregation Net based GAN (MFAGAN) for reducing the memory access cost of the generator. First, to overcome the memory explosion of dense connections, we utilize a memory-efficient multi-scale feature aggregation net as the generator. Second, for faster and more stable training, our method introduces the PatchGAN discriminator. Third, to balance the student discriminator and the compressed generator, we distill both the generator and the discriminator. Finally, we perform a hardware-aware neural architecture search (NAS) to find a specialized SubGenerator for the target mobile phone. Benefiting from these improvements, the proposed MFAGAN achieves up to 8.3\u00d7memory saving and 42.9\u00d7 computation reduction, with only minor visual quality degradation, compared with ESRGAN. Empirical studies also show \u223c70 milliseconds latency on Qualcomm Snapdragon 865 chipset.", "venue": "ArXiv", "authors": ["Wenlong  Cheng", "Mingbo  Zhao", "Zhiling  Ye", "Shuhang  Gu"], "year": 2021, "n_citations": 5}
{"id": 5972597, "s2_id": "ecf5efd5fe18860b42a1abd198e94a868dbf944c", "title": "Zorua: Enhancing Programming Ease, Portability, and Performance in GPUs by Decoupling Programming Models from Resource Management", "abstract": "The application resource specification--a static specification of several parameters such as the number of threads and the scratchpad memory usage per thread block--forms a critical component of the existing GPU programming models. This specification determines the performance of the application during execution because the corresponding on-chip hardware resources are allocated and managed purely based on this specification. This tight coupling between the software-provided resource specification and resource management in hardware leads to significant challenges in programming ease, portability, and performance, as we demonstrate in this work. \nOur goal in this work is to reduce the dependence of performance on the software-provided resource specification to simultaneously alleviate the above challenges. To this end, we introduce Zorua, a new resource virtualization framework, that decouples the programmer-specified resource usage of a GPU application from the actual allocation in the on-chip hardware resources. Zorua enables this decoupling by virtualizing each resource transparently to the programmer. \nWe demonstrate that by providing the illusion of more resources than physically available, Zorua offers several important benefits: (i) Programming Ease: Zorua eases the burden on the programmer to provide code that is tuned to efficiently utilize the physically available on-chip resources. (ii) Portability: Zorua alleviates the necessity of re-tuning an application's resource usage when porting the application across GPU generations. (iii) Performance: By dynamically allocating resources and carefully oversubscribing them when necessary, Zorua improves or retains the performance of applications that are already highly tuned to best utilize the resources. The holistic virtualization provided by Zorua has many other potential uses which we describe in this paper.", "venue": "ArXiv", "authors": ["Nandita  Vijaykumar", "Kevin  Hsieh", "Gennady  Pekhimenko", "Samira Manabi Khan", "Ashish  Shrestha", "Saugata  Ghose", "Phillip B. Gibbons", "Onur  Mutlu"], "year": 2018, "n_citations": 0}
{"id": 5973490, "s2_id": "0fe40f0eab56f602ef259d734990cd347dd399cf", "title": "Performance Enhancement of Routers in Networks-on-Chip Using Dynamic Virtual Channels Allocation", "abstract": "This study proposes a new router architecture to improve the performance of dynamic allocation of virtual channels. The proposed router is designed to reduce the hardware complexity and to improve power and area consumption, simultaneously. In the new structure of the proposed router, all of the controlling components have been implemented sequentially inside the allocator router modules. This optimizes communications between the controlling components and eliminates the most of hardware overloads of modular communications. Eliminating additional communications also reduces the hardware complexity. In order to show the validity of the proposed design in real hardware resources, the proposed router has been implemented onto a Field-Programmable Gate Array (FPGA). Since the implementation of a Network-on-Chip (NoC) requires certain amount of area on the chip, the suggested approach is also able to reduce the demand of hardware resources. In this method, the internal memory of the FPGA is used for implementing control units. This memory is faster and can be used with specific patterns. The use of the FPGA memory saves the hardware resources and allows the implementation of NoC based FPGA.", "venue": "ArXiv", "authors": ["Salman  Onsori", "Farshad  Safaei"], "year": 2014, "n_citations": 0}
{"id": 5976806, "s2_id": "3956864b69b9995da5b6431f7462522e69b89e8b", "title": "Optimal cache-aware suffix selection", "abstract": "Given string $S[1..N]$ and integer $k$, the {\\em suffix selection} problem is to determine the $k$th lexicographically smallest amongst the suffixes $S[i\\ldots N]$, $1 \\leq i \\leq N$. We study the suffix selection problem in the cache-aware model that captures two-level memory inherent in computing systems, for a \\emph{cache} of limited size $M$ and block size $B$. The complexity of interest is the number of block transfers. We present an optimal suffix selection algorithm in the cache-aware model, requiring $\\Theta\\left(N/B\\right)$ block transfers, for any string $S$ over an unbounded alphabet (where characters can only be compared), under the common tall-cache assumption (i.e. $M=\\Omega\\left(B^{1+\\epsilon}\\right)$, where $\\epsilon<1$). Our algorithm beats the bottleneck bound for permuting an input array to the desired output array, which holds for nearly any nontrivial problem in hierarchical memory models.", "venue": "STACS", "authors": ["Gianni  Franceschini", "Roberto  Grossi", "S.  Muthukrishnan"], "year": 2009, "n_citations": 4}
{"id": 5985282, "s2_id": "137f856b9fd5fb33079354c85a4005dd9ac434fd", "title": "BLASYS: Approximate Logic Synthesis Using Boolean Matrix Factorization", "abstract": "Approximate computing is an emerging paradigm where design accuracy can be traded off for benefits in design metrics such as design area, power consumption or circuit complexity. In this work, we present a novel paradigm to synthesize approximate circuits using Boolean matrix factorization (BMF). In our methodology the truth table of a sub-circuit of the design is approximated using BMF to a controllable approximation degree, and the results of the factorization are used to synthesize a less complex subcircuit. To scale our technique to large circuits, we devise a circuit decomposition method and a subcircuit design-space exploration technique to identify the best order for subcircuit approximations. Our method leads to a smooth trade-off between accuracy and full circuit complexity as measured by design area and power consumption. Using an industrial strength design flow, we extensively evaluate our methodology on a number of testcases, where we demonstrate that the proposed methodology can achieve up to 63% in power savings, while introducing an average relative error of 5%. We also compare our work to previous works in Boolean circuit synthesis and demonstrate significant improvements in design metrics for same accuracy targets.", "venue": "2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)", "authors": ["Soheil  Hashemi", "Hokchhay  Tann", "Sherief  Reda"], "year": 2018, "n_citations": 19}
{"id": 5986558, "s2_id": "3e520e13d4019e29ca84fdf065424c15c015907e", "title": "A Novel Hierarchical Circuit LUT Model for SOI Technology for Rapid Prototyping", "abstract": "In this paper, a new look-up table (LUT) method is proposed to reduce the simulation time and the run time memory requirement for large logic and mixed signal simulations. In the proposed method, for the first time, circuit with multiple devices is replaced by one LUT model, called circuit LUT. The replacement results in significant reduction of the run time memory requirement. The replacement also reduces the number of interpolation steps to be performed at every Newton\u2013Raphson iteration during the simulation that results in significant reduction of simulation time. With the proposed method, the simulation speed is improved by two times over the conventional LUT models developed for devices. In addition, 25% reduction in the run time memory requirement is also achieved by the proposed method.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Sitansusekhar  Roymohapatra", "Ganesh R. Gore", "Akanksha  Yadav", "Mahesh B. Patil", "Krishnan S. Rengarajan", "Subramanian S. Iyer", "Maryam Shojaei Baghini"], "year": 2020, "n_citations": 2}
{"id": 5989314, "s2_id": "502f104835eaab0c0f6a2c6bf032678000dea4a0", "title": "OpenRISC System-on-Chip Design Emulation", "abstract": "Recently the hardware emulation technique has emerged as a promising approach to accelerating hardware verification/debugging process. To fully evaluate the powerfulness of the emulation approach and demonstrate its potential impact, we propose to emulate a system-on-chip (SoC) design using Mentor Graphics Veloce emulation platform. This article presents our project setup and the results we have achieved. The results are encouraging. ORPSoC emulation with Veloce has more than ten times faster than hardware simulation. Our experimental results demonstrate that Mentor Graphics Veloce has major advantages in emulation, verification, and debugging of complicated real hardware designs, especially in the context of SoC complexity. Through our three major tasks, we will demonstrate that (1) Veloce can successfully emulate large-scale SoC designs; (2) it has much better performance comparing to the state-of-the-art simulation tools; (3) it can significantly accelerate the process of hardware verification and debugging while maintaining full signal visibility.", "venue": "ArXiv", "authors": ["Kai  Cong", "Li  Lei", "Zhenkun  Yang", "Fei  Xie"], "year": 2016, "n_citations": 0}
{"id": 5991248, "s2_id": "d3e4af065758d4df925ad265c2a8cac5fcc64cd7", "title": "Design and Implementation of Bit Transition Counter", "abstract": "In today\u2019s VLSI system design, power consumption is gaining more attention as compared to performance and area. This is due to battery life in portable devices and operating frequency of the design. Power consumption mainly consists of static power, dynam ic power, leakage power and short circuit power. Dynamic power is dominant among all which depends on many factors viz. power supply, load capacitance and frequency. Switching activity also affects dynamic power consumption of bus which is determined by calculating the number of bit transitions on bus. The purpose of this paper is to design a bit transition counter which can be used to calculate the switching activity of the circuit nodes. The novel feature is that it can be inserted at any node of the circuit, thus helpful for calculating power consumption of bus.", "venue": "ArXiv", "authors": ["Amandeep  Singh", "Balwinder  Singh"], "year": 2014, "n_citations": 0}
{"id": 5994272, "s2_id": "6efdc31f39c3150c16a28736fb1f6a05d98e9884", "title": "Reducing DRAM Latency at Low Cost by Exploiting Heterogeneity", "abstract": "In modern systems, DRAM-based main memory is signi?cantly slower than the processor.Consequently, processors spend a long time waiting to access data from main memory, makingthe long main memory access latency one of the most critical bottlenecks to achieving highsystem performance. Unfortunately, the latency of DRAM has remained almost constant inthe past decade. This is mainly because DRAM has been optimized for cost-per-bit, ratherthan access latency. As a result, DRAM latency is not reducing with technology scaling, andcontinues to be an important performance bottleneck in modern and future systems.This dissertation seeks to achieve low latency DRAM-based memory systems at low costin three major directions. The key idea of these three major directions is to enable and ex-ploit latency heterogeneity in DRAM architecture. First, based on the observation that longbitlines in DRAM are one of the dominant sources of DRAM latency, we propose a newDRAM architecture, Tiered-Latency DRAM (TL-DRAM), which divides the long bitline intotwo shorter segments using an isolation transistor, allowing one segment to be accessed withreduced latency. Second, we propose a ?ne-grained DRAM latency reduction mechanism,Adaptive-Latency DRAM, which optimizes DRAM latency for the common operating conditions for individual DRAM module. We observe that DRAM manufacturers incorporate a very large timing margin as a provision against the worst-case operating conditions, whichis accessing the slowest cell across all DRAM products with the worst latency at the highesttemperature, even though such a slowest cell and such an operating condition are rare. Ourmechanism dynamically optimizes DRAM latency to the current operating condition of theaccessed DRAM module, thereby reliably improving system performance. Third, we observethat cells closer to the peripheral logic can be much faster than cells farther from the peripherallogic (a phenomenon we call architectural variation). Based on this observation, we propose anew technique, Architectural-Variation-Aware DRAM (AVA-DRAM), which reduces DRAMlatency at low cost, by pro?ling and identifying only the inherently slower regions in DRAMto dynamically determine the lowest latency DRAM can operate at without causing failures.This dissertation provides a detailed analysis of DRAM latency by using both circuit-levelsimulation with a detailed DRAM model and FPGA-based pro?ling of real DRAM modules.Our latency analysis shows that our low latency DRAM mechanisms enable significant latencyreductions, leading to large improvement in both system performance and energy e?fficiencyacross a variety of workloads in our evaluated systems, while ensuring reliable DRAM operation.", "venue": "ArXiv", "authors": ["Donghyuk  Lee"], "year": 2016, "n_citations": 35}
{"id": 5996874, "s2_id": "833f26fa8b38dc74e9952303c48af19465fd13f9", "title": "Prophet: A Speculative Multi-threading Execution Model with Architectural Support Based on CMP", "abstract": "Speculative Multithreading (SpMT) has been proposed as a perspective method for sequential programs to benefit from the increasing computing resources provided by Chip Multiprocessors (CMP). This paper analyzes the extraction of ihread-level parallelism from general-purpose programs and presents a speculative multi-threading execution model, Prophet. The architectural support for Prophet execution model is designed based on CMP. In Prophet the inter-thread data dependences are reduced by precomputation slice (p-slice). Multi-versioning Cache system along with thread state control mechanism are designed for buffering the speculative data and also a snooping bus based cache coherence protocol is used to detect data dependence violation. The experiment results show that Prophet system could achieve significant speedup for general-purpose programs.", "venue": "2009 International Conference on Scalable Computing and Communications; Eighth International Conference on Embedded Computing", "authors": ["Zhaoyu  Dong", "Yinliang  Zhao", "Yuanke  Wei", "Xuhao  Wang", "Shaolong  Song"], "year": 2009, "n_citations": 24}
{"id": 5998636, "s2_id": "bd55a827eab27cacfec26b2cabebdd91ffb93041", "title": "Dagger: Accelerating RPCs in Cloud Microservices Through Tightly-Coupled Reconfigurable NICs", "abstract": "The ongoing shift of cloud services from monolithic designs to microservices creates high demand for efficient and high performance datacenter networking stacks, optimized for fine-grained workloads. Commodity networking systems based on software stacks and peripheral NICs introduce high overheads when it comes to delivering small messages. We present Dagger, a hardware acceleration fabric for cloud RPCs based on FPGAs, where the accelerator is closely-coupled with the host processor over a configurable memory interconnect. The three key design principle of Dagger are: (1) offloading the entire RPC stack to an FPGA-based NIC, (2) leveraging memory interconnects instead of PCIe buses as the interface with the host CPU, and (3) making the acceleration fabric reconfigurable, so it can accommodate the diverse needs of microservices. We show that the combination of these principles significantly improves the efficiency and performance of cloud RPC systems while preserving their generality. Dagger achieves 1.3 \u2212 3.8\u00d7 higher per-core RPC throughput compared to both highly-optimized software stacks, and systems using specialized RDMA adapters. It also scales up to 84 Mrps with 8 threads on 4 CPU cores, while maintaining state-ofthe-art \u03bcs-scale tail latency. We also demonstrate that large thirdparty applications, like memcached and MICA KVS, can be easily ported on Dagger with minimal changes to their codebase, bringing their median and tail KVS access latency down to 2.8 \u2212 3.5 us and 5.4 \u2212 7.8 us, respectively. Finally, we show that Dagger is beneficial for multi-tier end-to-end microservices with different threading models by evaluating it using an 8-tier application implementing a flight check-in service.", "venue": "ArXiv", "authors": ["Nikita  Lazarev", "Shaojie  Xiang", "Neil  Adit", "Zhiru  Zhang", "Christina  Delimitrou"], "year": 2021, "n_citations": 0}
{"id": 6001835, "s2_id": "6ee7cc7835b9244b739e7f5be518effa1996e83f", "title": "SIMDRAM: a framework for bit-serial SIMD processing using DRAM", "abstract": "Processing-using-DRAM has been proposed for a limited set of basic operations (i.e., logic operations, addition). However, in order to enable full adoption of processing-using-DRAM, it is necessary to provide support for more complex operations. In this paper, we propose SIMDRAM, a flexible general-purpose processing-using-DRAM framework that (1) enables the efficient implementation of complex operations, and (2) provides a flexible mechanism tosupport the implementation of arbitrary user-defined operations. The SIMDRAM framework comprises three key steps. The first step builds an efficient MAJ/NOT representation of a given desired operation. The second step allocates DRAM rows that are reserved for computation to the operation\u2019s input and output operands, and generates the required sequence of DRAM commands to perform the MAJ/NOT implementation of the desired operation in DRAM. The third step uses the SIMDRAM control unit located inside the memory controller to manage the computation of the operation from start to end, by executing the DRAM commands generated in the second step of the framework. We design the hardware and ISA support for SIMDRAM framework to (1) address key system integration challenges, and (2) allow programmers to employ new SIMDRAM operations without hardware changes. We evaluate SIMDRAM for reliability, area overhead, throughput, and energy efficiency using a wide range of operations and seven real-world applications to demonstrate SIMDRAM\u2019s generality. Our evaluations using a single DRAM bank show that (1) over 16 operations, SIMDRAM provides 2.0X the throughput and 2.6X the energy efficiency of Ambit, a state-of-the-art processing-using-DRAM mechanism; (2) over seven real-world applications, SIMDRAM provides 2.5X the performance of Ambit. Using 16 DRAM banks, SIMDRAM provides (1) 88X and 5.8X the throughput, and 257X and 31X the energy efficiency, of a CPU and a high-end GPU, respectively, over 16 operations; (2) 21X and 2.1X the performance of the CPU and GPU, over seven real-world applications. SIMDRAM incurs an area overhead of only 0.2% in a high-end CPU.", "venue": "ASPLOS", "authors": ["Nastaran  Hajinazar", "Geraldo F. Oliveira", "Sven  Gregorio", "Jo\u00e3o Dinis Ferreira", "Nika  Mansouri-Ghiasi", "Minesh  Patel", "Mohammed  Alser", "Saugata  Ghose", "Juan  G\u00f3mez-Luna", "Onur  Mutlu"], "year": 2021, "n_citations": 14}
{"id": 6015096, "s2_id": "97447dcfc33841df9ca42048969672e7007c5711", "title": "Microcontroller Based Testing of Digital IP-Core", "abstract": "Testing core based System on Chip [1] is a challenge for the test engineers. To test the complete SOC at one time with maximum fault coverage, test engineers prefer to test each IP-core separately. At speed testing using external testers is more expensive because of gigahertz processor. The purpose of this paper is to develop cost efficient and flexible test methodology for testing digital IP-cores [2]. The prominent feature of the approach is to use microcontroller to test IP-core. The novel feature is that there is no need of test pattern generator and output response analyzer as microcontroller performs the function of both. This approach has various advantages such as at speed testing, low cost, less area overhead and greater flexibility since most of the testing process is based on software.", "venue": "VLSIC 2012", "authors": ["Amandeep  Singh", "Balwinder  Singh"], "year": 2012, "n_citations": 1}
{"id": 6018663, "s2_id": "7025420c2a8814687a36a1b7bc5afadfacaa5899", "title": "High Performance Reconfigurable Computing Systems", "abstract": "The rapid progress and advancement in electronic chips technology provide a variety of new implementation options for system engineers. The choice varies between the flexible programs running on a general-purpose processor (GPP) and the fixed hardware implementation using an application specific integrated circuit (ASIC). Many other implementation options present, for instance, a system with a RISC processor and a DSP core. Other options include graphics processors and microcontrollers. Specialist processors certainly improve performance over general-purpose ones, but this comes as a quid pro quo for flexibility. Combining the flexibility of GPPs and the high performance of ASICs leads to the introduction of reconfigurable computing (RC) as a new implementation option with a balance between versatility and speed. The focus of this chapter is on introducing reconfigurable computers as modern super computing architectures. The chapter also investigates the main reasons behind the current advancement in the development of RC-systems. Furthermore, a technical survey of various RC-systems is included laying common grounds for comparisons. In addition, this chapter mainly presents case studies implemented under the MorphoSys RC-system. The selected case studies belong to different areas of application, such as, computer graphics and information coding. Parallel versions of the studied algorithms are developed to match the topologies supported by the MorphoSys. Performance evaluation and results analyses are included for implementations with different characteristics.", "venue": "ArXiv", "authors": ["Issam W. Damaj"], "year": 2019, "n_citations": 0}
{"id": 6019166, "s2_id": "ca7665207c7e79fbdc8491bfdd89379bedb1faa1", "title": "A Resolution for Shared Memory Conflict in Multiprocessor System-on-a-Chip", "abstract": "Now days, manufacturers are focusing on increasing the concurrency in multiprocessor system-on-a-chip (MPSoC) architecture instead of increasing clock speed, for embedded systems. Traditionally lock-based synchronization is provided to support concurrency; as managing locks can be very difficult and error prone. Transactional memories and lock based systems have been extensively used to provide synchronization between multiple processors (1) in general-purpose systems. It has been shown that locks have numerous shortcomings over transactional memory in terms of power consumption, ease of programming and performance. In this paper, we propose a new semaphore scheme for synchronization in shared cache memory in an MPSoC. Moreover, we have evaluated and compared our scheme with locks and transactions in terms of energy consumption and cache miss rate using SimpleScalar functional simulator.", "venue": "ArXiv", "authors": ["Shaily  Mittal", "Nitin"], "year": 2012, "n_citations": 0}
{"id": 6021037, "s2_id": "8596804f9cd6ca8756be4d986375a739308e1d74", "title": "Turbo NOC: A Framework for the Design of Network-on-Chip-Based Turbo Decoder Architectures", "abstract": "This paper proposes a general framework for the design and simulation of network-on-chip-based turbo decoder architectures. Several parameters in the design space are investigated, namely, network topology, parallelism degree, the rate at which messages are sent by processing nodes over the network, and routing strategy. The main results of this analysis are as follows: 1) the most suited topologies to achieve high throughput with a limited complexity overhead are generalized de Bruijn and generalized Kautz topologies and 2) depending on the throughput requirements, different parallelism degrees, message injection rates, and routing algorithms can be used to minimize the network area overhead.", "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers", "authors": ["Maurizio  Martina", "Guido  Masera"], "year": 2010, "n_citations": 45}
{"id": 6022213, "s2_id": "2bd5240e94177910e7d6f2ca17b6105ccd7b068a", "title": "Implementation of float-float operators on graphics hardware", "abstract": "The Graphic Processing Unit (GPU) has evolved into a powerful and flexible processor. The latest graphic processors provide fully programmable vertex and pixel processing units that support vector operations up to single floating-point precision. This computational power is now being used for general-purpose computations. However, some applications require higher precision than single precision. This paper describes the emulation of a 44-bit floating-point number format and its corresponding operations. An implementation is presented along with performance and accuracy results.", "venue": "ArXiv", "authors": ["Guillaume Da Gra\u00e7a", "David  Defour"], "year": 2006, "n_citations": 33}
{"id": 6024373, "s2_id": "b5df2a619e8751b2d344c4360f7e87b214506b85", "title": "MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture", "abstract": "With the imminent slowing down of DRAM scaling, Phase Change Memory (PCM) is emerging as a lead alternative for main memory technology. While PCM achieves low energy due to various technology-specific advantages, PCM is significantly slower than DRAM (especially for writes) and can endure far fewer writes before wearing out. Previous work has proposed to use a large, DRAM-based hardware cache to absorb writes and provide faster access. However, due to ineffectual caching where blocks are evicted before sufficient number of accesses, hardware caches incur significant overheads in energy and bandwidth, two key but scarce resources in modern multicores. Because using hardware for detecting and removing such ineffectual caching would incur additional hardware cost and complexity, we leverage the OS virtual memory support for this purpose. We propose a DRAM-PCM hybrid memory architecture where the OS migrates pages on demand from the PCM to DRAM. We call the DRAM part of our memory as MigrantStore which includes two ideas. First, to reduce the energy, bandwidth, and wear overhead of ineffectual migrations, we propose migration hysteresis. Second, to reduce the software overhead of good replacement policies, we propose recently- accessed-page-id (RAPid) buffer, a hardware buffer to track the addresses of recently-accessed MigrantStore pages.", "venue": "ArXiv", "authors": ["Hamza Bin Sohail", "Balajee  Vamanan", "T. N. Vijaykumar"], "year": 2015, "n_citations": 11}
{"id": 6027400, "s2_id": "cc9bf28adf2332dd72b4880dae965ac10c7d4557", "title": "Characterizing Concurrency Mechanisms for NVIDIA GPUs under Deep Learning Workloads", "abstract": "We investigate the performance of the concurrency mechanisms available on NVIDIA\u2019s new Ampere GPU microarchitecture under deep learning training and inference workloads. In contrast to previous studies that treat the GPU as a black box, we examine scheduling at the microarchitectural level. We find that the lack of fine-grained preemption mechanisms, robust task prioritization options, and contention-aware thread block placement policies limits the effectiveness of NVIDIA\u2019s concurrency mechanisms. In summary, the sequential nature of deep learning workloads and their fluctuating resource requirements and kernel runtimes make executing such workloads while maintaining consistently high utilization and low, predictable turnaround times difficult on current NVIDIA hardware.", "venue": "Perform. Evaluation", "authors": ["Guin  Gilman", "Robert J. Walls"], "year": 2021, "n_citations": 0}
{"id": 6033877, "s2_id": "e69e9dafb65bd1b6ca61567b748af72eb1bbb2e2", "title": "Sextans: A Streaming Accelerator for General-Purpose Sparse-Matrix Dense-Matrix Multiplication", "abstract": "Sparse-Matrix Dense-Matrix multiplication (SpMM) is the key operator for a wide range of applications including scientific computing, graph processing, and deep learning. Architecting accelerators for SpMM is faced with three challenges \u2013 (1) the random memory accessing and unbalanced load in processing because of random distribution of elements in sparse matrices, (2) inefficient data handling of the large matrices which can not be fit on-chip, and (3) a non-general-purpose accelerator design where one accelerator can only process a fixed-size problem. In this paper, we present Sextans, an accelerator for generalpurpose SpMM processing. Sextans accelerator features (1) fast random access using on-chip memory, (2) streaming access to offchip large matrices, (3) PE-aware non-zero scheduling for balanced workload with an II=1 pipeline, and (4) hardware flexibility to enable prototyping the hardware once to support SpMMs of different size as a general-purpose accelerator. We leverage high bandwidth memory (HBM) for the efficient accessing of both sparse and dense matrices. In the evaluation, we present an FPGA prototype Sextans which is executable on a Xilinx U280 HBM FPGA board and a projected prototype Sextans-P with higher bandwidth comparable to V100 and more frequency optimization. We conduct a comprehensive evaluation on 1,400 SpMMs on a wide range of sparse matrices including 50 matrices from SNAP and 150 from SuiteSparse. We compare Sextans with NVIDIA K80 and V100 GPUs. Sextans achieves a 2.50x geomean speedup over K80 GPU and Sextans-P achieves a 1.14x geomean speedup over V100 GPU (4.94x over K80).", "venue": "ArXiv", "authors": ["Linghao  Song", "Yuze  Chi", "Atefeh  Sohrabizadeh", "Young-kyu  Choi", "Jason  Lau", "Jason  Cong"], "year": 2021, "n_citations": 2}
{"id": 6036346, "s2_id": "5a258af8352389dadd0ed107060f68702be2a290", "title": "Fast polar codes for terabits-per-second throughput communications", "abstract": "Targeting high-throughput and low-power communications, we implement two successive cancellation (SC) decoders for polar codes. With 16nm ASIC technology, the area efficiency and energy efficiency are 4Tbps/mm and 0.63pJ/bit, respectively, for the unrolled decoder, and 561Gbps/mm and 1.21pJ/bit, respectively, for the recursive decoder. To achieve such a high throughput, a novel code construction, coined as fast polar codes, is proposed and jointly optimized with a highly-parallel SC decoding architecture. First, we reuse existing modules to fast decode more outer code blocks, and then modify code construction to facilitate faster decoding for all outer code blocks up to a degree of parallelism of 16. Furthermore, parallel comparison circuits and bit quantization schemes are customized for hardware implementation. Collectively, they contribute to an 2.66\u00d7 area efficiency improvement and 33% energy saving over the state of the art.", "venue": "ArXiv", "authors": ["Jiajie  Tong", "Xianbin  Wang", "Qifan  Zhang", "Huazi  Zhang", "Rong  Li", "Jun  Wang", "Wen  Tong"], "year": 2021, "n_citations": 1}
{"id": 6038007, "s2_id": "275af2d59af46866b7a99559af399b613661812f", "title": "Dependability modeling and optimization of triple modular redundancy partitioning for SRAM-based FPGAs", "abstract": "Abstract SRAM-based FPGAs are popular in the aerospace industry for their field programmability and low cost. However, they suffer from cosmic radiation-induced Single Event Upsets (SEUs). Triple Modular Redundancy (TMR) is a well-known technique to mitigate SEUs in FPGAs that is often used with another SEU mitigation technique known as configuration scrubbing. Traditional TMR provides protection against a single fault at a time, while partitioned TMR provides improved reliability and availability. In this paper, we present a methodology to analyze TMR partitioning at early design stage using probabilistic model checking. The proposed formal model can capture both single and multiple-cell upset scenarios, regardless of any assumption of equal partition sizes. Starting with a high-level description of a design, a Markov model is constructed from the Data Flow Graph (DFG) using a specified number of partitions, a component characterization library and a user defined scrub rate. Such a model and exhaustive analysis captures all the considered failures and repairs possible in the system within the radiation environment. Various reliability and availability properties are then verified automatically using the PRISM model checker exploring the relationship between the scrub frequency and the number of TMR partitions required to meet the design requirements. Also, the reported results show that based on a known voter failure rate, it is possible to find an optimal number of partitions at early design stages using our proposed method.", "venue": "Reliab. Eng. Syst. Saf.", "authors": ["Khaza Anuarul Hoque", "Otmane A\u00eft Mohamed", "Yvon  Savaria"], "year": 2019, "n_citations": 1}
{"id": 6038674, "s2_id": "a4ef23dd82a9b58695976adfe41d27a7f5b1d6e6", "title": "Quantum Algorithm Processor For Finding Exact Divisors", "abstract": "Wiring diagrams are given for a quantum algorithm processor in CMOS to compute, in parallel, all divisors of an n-bit integer. Lines required in a wiring diagram are proportional to n. Execution time is proportional to the square of n.", "venue": "ArXiv", "authors": ["John Robert Burger"], "year": 2005, "n_citations": 2}
{"id": 6042219, "s2_id": "60faa537b0d28abc8ba13e2e978d0690e43be395", "title": "Dopant network processing units: towards efficient neural network emulators with high-capacity nanoelectronic nodes", "abstract": "The rapidly growing computational demands of deep neural networks require novel hardware designs. Recently, tuneable nanoelectronic devices were developed based on hopping electrons through a network of dopant atoms in silicon. These \u2018dopant network processing units\u2019 (DNPUs) are highly energy-efficient and have potentially very high throughput. By adapting the control voltages applied to its electrodes, a single DNPU can solve a variety of linearly non-separable classification problems. However, using a single device has limitations due to the implicit single-node architecture. This paper presents a promising novel approach to neural information processing by introducing DNPUs as high-capacity neurons and moving from a single to a multi-neuron framework. By implementing and testing a small multi-DNPU classifier in hardware, we show that feed-forward DNPU networks improve the performance of a single DNPU from 77% to 94% test accuracy on a binary classification task with concentric classes on a plane. Furthermore, motivated by the integration of DNPUs with memristor crossbar arrays, we study the potential of using DNPUs in combination with linear layers. We show by simulation that an MNIST classifier with only 10 DNPU nodes achieves over 96% test accuracy. Our results pave the road towards hardware neural network emulators that offer atomic-scale information processing with low latency and energy consumption.", "venue": "Neuromorph. Comput. Eng.", "authors": ["Hans-Christian  Ruiz-Euler", "Unai  Alegre-Ibarra", "Bram  van de Ven", "Hajo  Broersma", "Peter A Bobbert", "Wilfred G van der Wiel"], "year": 2021, "n_citations": 2}
{"id": 6045684, "s2_id": "62c3716971ddc968af4d11ad6a4a3e4b743bf6d6", "title": "Low-Latency SC Decoder Architectures for Polar Codes", "abstract": "Nowadays polar codes are becoming one of the most favorable capacity achieving error correction codes for their low encoding and decoding complexity. However, due to the large code length required by practical applications, the few existing successive cancellation (SC) decoder implementations still suffer from not only the high hardware cost but also the long decoding latency. This paper presents novel several approaches to design low-latency decoders for polar codes based on look-ahead techniques. Look-ahead techniques can be employed to reschedule the decoding process of polar decoder in numerous approaches. However, among those approaches, only well-arranged ones can achieve good performance in terms of both latency and hardware complexity. By revealing the recurrence property of SC decoding chart, the authors succeed in reducing the decoding latency by 50% with look-ahead techniques. With the help of VLSI-DSP design techniques such as pipelining, folding, unfolding, and parallel processing, methodologies for four different polar decoder architectures have been proposed to meet various application demands. Sub-structure sharing scheme has been adopted to design the merged processing element (PE) for further hardware reduction. In addition, systematic methods for construction refined pipelining decoder (2nd design) and the input generating circuits (ICG) block have been given. Detailed gate-level analysis has demonstrated that the proposed designs show latency advantages over conventional ones with similar hardware cost.", "venue": "ArXiv", "authors": ["Chuan  Zhang", "Bo  Yuan", "Keshab K. Parhi"], "year": 2011, "n_citations": 4}
{"id": 6049360, "s2_id": "90b13eeb37d4c2f92d9140209ef13b0ac42a0170", "title": "MIRAGE: Mitigating Conflict-Based Cache Attacks with a Practical Fully-Associative Design", "abstract": "Shared caches in modern processors are vulnerable to conflict-based attacks, whereby an attacker monitors the access pattern of a victim by engineering cache-set conflicts. Recent mitigations propose a randomized mapping of addresses to cache locations to obfuscate addresses that can conflict with a target address. Unfortunately, such designs continue to select eviction candidates from a small subset of the resident cache lines, which makes such designs vulnerable to algorithms that can quickly identify the conflicting addresses. \nThis paper presents Mirage, a practical design for a fully associative cache, wherein eviction candidates are selected randomly from among all the lines resident in the cache, to be immune to set-conflicts. A key challenge in naively adopting such designs for large shared caches (containing tens of thousands of lines) is the complexity of cache-lookup, as that can require searching through all the lines resident in the cache in such designs. Mirage practically enables a fully-associative design, while maintaining the access latency similar to a traditional set-associative cache using: (1) Pointer-based indirection from the tag-store to the data-store, which allows a newly installed address to evict data of any resident line, (2) Skewed-associative tag-store with extra invalid tags, wherein incoming addresses can be installed without set-conflicts, and (3) Load-aware placement that maximizes the availability of sets with invalid tags, to eliminate set-conflicts. Our analysis shows Mirage provides the global-eviction property of a fully-associative cache throughout the system lifetime (violations of full-associativity, i.e set-conflicts, occur less than once in 10^4 to 10^17 years), offering a principled defense against set-conflict based attacks. Mirage incurs negligible slowdown (0.3%) and 12-15% extra storage compared to the recently proposed Scatter-Cache.", "venue": "USENIX Security Symposium", "authors": ["Gururaj  Saileshwar", "Moinuddin  Qureshi"], "year": 2021, "n_citations": 4}
{"id": 6049555, "s2_id": "acd24031625bc830b77aea36e58d41f5488bb63e", "title": "A Design Flow for Mapping Spiking Neural Networks to Many-Core Neuromorphic Hardware", "abstract": "The design of many-core neuromorphic hardware is becoming increasingly complex as these systems are now expected to execute large machine-learning models. A predictable design flow is needed to guarantee real-time performance such as latency and throughput without significantly increasing the buffer requirement of computing cores. Synchronous Data Flow Graphs (SDFGs) have been previously used for predictable mapping of streaming applications to multiprocessor systems. We propose an SDFG-based design flow to map spiking neural networks (SNNs) to many-core neuromorphic hardware with the objective of exploring the tradeoff between throughput and buffer-size requirements. The proposed design flow integrates an iterative partitioning approach based on Kernighan-Lin graph partitioning heuristic to create SNN clusters such that each cluster can be mapped to a core of the hardware. The partitioning approach minimizes inter-cluster spike communication, which improves latency on the shared interconnect of the hardware. Next, the design flow maps clusters to cores using Particle Swarm Optimization (PSO), an evolutionary algorithm, while exploring the design space of throughput and buffer size. Pareto-optimal mappings are retained from the design flow, allowing system designers to select a Pareto mapping that satisfies throughput and buffer-size requirements of the design. We evaluated the developed design flow using five large-scale convolutional neural network (CNN) models. Results demonstrate 63% higher maximum throughput and 10% lower buffer-size requirement compared to state-of-the-art dataflow-based mapping solutions.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Shihao  Song", "M. Lakshmi Varshika", "Anup  Das", "Nagarajan  Kandasamy"], "year": 2021, "n_citations": 4}
{"id": 6055634, "s2_id": "32b1363aec329718a01fac3ac0581b9fdfb0c6fb", "title": "A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs", "abstract": "Although high-performance deep neural networks are in high demand in edge environments, computation resources are strictly limited in edge devices, and light-weight neural network techniques, such as Depthwise Separable Convolution (DSC), have been developed. ResNet is one of conventional deep neural network models that stack a lot of layers and parameters for a higher accuracy. To reduce the parameter size of ResNet, by utilizing a similarity to ODE (Ordinary Differential Equation), Neural ODE repeatedly uses most of weight parameters instead of having a lot of different parameters. Thus, Neural ODE becomes significantly small compared to that of ResNet so that it can be implemented in resource-limited edge devices. In this paper, a combination of Neural ODE and DSC, called dsODENet, is designed and implemented for FPGAs (Field-Programmable Gate Arrays). dsODENet is then applied to edge domain adaptation as a practical use case and evaluated with image classification datasets. It is implemented on Xilinx ZCU104 board and evaluated in terms of domain adaptation accuracy, training speed, FPGA resource utilization, and speedup rate compared to a software execution. The results demonstrate that dsODENet is comparable to or slightly better than our baseline Neural ODE implementation in terms of domain adaptation accuracy, while the total parameter size without preand post-processing layers is reduced by 54.2% to 79.8%. The FPGA implementation accelerates the prediction tasks by 27.9 times faster than a software implementation.", "venue": "ArXiv", "authors": ["Hiroki  Kawakami", "Hirohisa  Watanabe", "Keisuke  Sugiura", "Hiroki  Matsutani"], "year": 2021, "n_citations": 0}
{"id": 6056427, "s2_id": "8a5949348dc16596a3f6a449e8ed4bcb4e6295a1", "title": "G-TADOC: Enabling Efficient GPU-Based Text Analytics without Decompression", "abstract": "Text analytics directly on compression (TADOC) has proven to be a promising technology for big data analytics. GPUs are extremely popular accelerators for data analytics systems. Unfortunately, no work so far shows how to utilize GPUs to accelerate TADOC. We describe G-TADOC, the first framework that provides GPU-based text analytics directly on compression, effectively enabling efficient text analytics on GPUs without decompressing the input data.G-TADOC solves three major challenges. First, TADOC involves a large amount of dependencies, which makes it difficult to exploit massive parallelism on a GPU. We develop a novel fine-grained thread-level workload scheduling strategy for GPU threads, which partitions heavily-dependent loads adaptively in a fine-grained manner. Second, in developing G-TADOC, thousands of GPU threads writing to the same result buffer leads to inconsistency while directly using locks and atomic operations lead to large synchronization overheads. We develop a memory pool with thread-safe data structures on GPUs to handle such difficulties. Third, maintaining the sequence information among words is essential for lossless compression. We design a sequence-support strategy, which maintains high GPU parallelism while ensuring sequence information.Our experimental evaluations show that G-TADOC provides 31.1\u00d7 average speedup compared to state-of-the-art TADOC.", "venue": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "authors": ["Feng  Zhang", "Zaifeng  Pan", "Yanliang  Zhou", "Jidong  Zhai", "Xipeng  Shen", "Onur  Mutlu", "Xiaoyong  Du"], "year": 2021, "n_citations": 0}
{"id": 6058112, "s2_id": "a2c9756c81d42464b2d969884b53db81cba16bd9", "title": "Proceedings of the Second International Workshop on FPGAs for Software Programmers (FSP 2015)", "abstract": "This volume contains the papers accepted at the Second International Workshop on FPGAs for Software Programmers (FSP 2015), held in London, United Kingdom, September 1st, 2015. FSP 2015 was co-located with the International Conference on Field Programmable Logic and Applications (FPL).", "venue": "ArXiv", "authors": ["Frank  Hannig", "Dirk  Koch", "Daniel  Ziener"], "year": 2015, "n_citations": 0}
{"id": 6063674, "s2_id": "eb5af4c6eea7daea78dcb7931577348ee21dceee", "title": "A Foray into Efficient Mapping of Algorithms to Hardware Platforms on Heterogeneous Systems", "abstract": "Heterogeneous computing can potentially offer significant performance and performance per watt improvements over homogeneous computing, but the question \"what is the ideal mapping of algorithms to architectures?\" remains an open one. In the past couple of years new types of computing devices such as FPGAs have come into general computing use. In this work we attempt to add to the body of scientific knowledge by comparing Kernel performance and performance per watt of seven key algorithms according to Berkley's dwarf taxonomy. We do so using the Rodinia benchmark suite on three different high-end hardware architecture representatives from the CPU, GPU and FPGA families. We find results that support some distinct mappings between the architecture and performance per watt. Perhaps the most interesting finding is that, for our specific hardware representatives, FPGAs should be considered as alternatives to GPUs and CPUs in several key algorithms: N-body simulations, dense linear algebra and structured grid.", "venue": "ArXiv", "authors": ["Oren  Segal", "Nasibeh  Nasiri", "Martin  Margala"], "year": 2016, "n_citations": 0}
{"id": 6063698, "s2_id": "669a726a6a4fd3f38954559951f474cef6b0b92b", "title": "HTS: A Hardware Task Scheduler for Heterogeneous Systems", "abstract": "As the Moore's scaling era comes to an end, application specific hardware accelerators appear as an attractive way to improve the performance and power efficiency of our computing systems. A massively heterogeneous system with a large number of hardware accelerators along with multiple general purpose CPUs is a promising direction, but pose several challenges in terms of the run-time scheduling of tasks on the accelerators and design granularity of accelerators. This paper addresses these challenges by developing an example heterogeneous system to enable multiple applications to share the available accelerators. We propose to design accelerators at a lower abstraction to enable applications to be broken down into tasks that can be mapped on several accelerators. We observe that several real-life workloads can be broken down into common primitives that are shared across many workloads. Finally, we propose and design a hardware task scheduler inspired by the hardware schedulers in out-of-order superscalar processors to efficiently utilize the accelerators in the system by scheduling tasks in out-of-order and even speculatively. We evaluate the proposed system on both real-life and synthetic benchmarks based on Digital Signal Processing~(DSP) applications. Compared to executing the benchmark on a system with sequential scheduling, proposed scheduler achieves up to 12x improvement in performance.", "venue": "ArXiv", "authors": ["Kartik  Hegde", "Abhishek  Srivastava", "Rohit  Agrawal"], "year": 2019, "n_citations": 0}
{"id": 6066123, "s2_id": "c5246061f3f3cbd575ff72c7e0da4a789af31854", "title": "Ozone: Efficient Execution with Zero Timing Leakage for Modern Microarchitectures", "abstract": "Time variation during program execution can leak sensitive information. Time variations due to program control flow and hardware resource contention have been used to steal encryption keys in cipher implementations such as AES and RSA. A number of approaches to mitigate timing-based side-channel attacks have been proposed including cache partitioning, control-flow obfuscation and injecting timing noise into the outputs of code. While these techniques make timing-based side-channel attacks more difficult, they do not eliminate the risks. Prior techniques are either too specific or too expensive, and all leave remnants of the original timing side channel for later attackers to attempt to exploit. \nIn this work, we show that the state-of-the-art techniques in timing side-channel protection, which limit timing leakage but do not eliminate it, still have significant vulnerabilities to timing-based side-channel attacks. To provide a means for total protection from timing-based side-channel attacks, we develop Ozone, the first zero timing leakage execution resource for a modern microarchitecture. Code in Ozone execute under a special hardware thread that gains exclusive access to a single core's resources for a fixed (and limited) number of cycles during which it cannot be interrupted. Memory access under Ozone thread execution is limited to a fixed size uncached scratchpad memory, and all Ozone threads begin execution with a known fixed microarchitectural state. We evaluate Ozone using a number of security sensitive kernels that have previously been targets of timing side-channel attacks, and show that Ozone eliminates timing leakage with minimal performance overhead.", "venue": "HOST", "authors": ["Zelalem Birhanu Aweke", "Todd M. Austin"], "year": 2017, "n_citations": 5}
{"id": 6066246, "s2_id": "13d79f13e5dc04cc2a1064acb801c76f609e0b25", "title": "Revisiting FPGA Acceleration of Molecular Dynamics Simulation with Dynamic Data Flow Behavior in High-Level Synthesis", "abstract": "Molecular dynamics (MD) simulation is one of the past decade's most important tools for enabling biology scientists and researchers to explore human health and diseases. However, due to the computation complexity of the MD algorithm, it takes weeks or even months to simulate a comparatively simple biology entity on conventional multicore processors. The critical path in molecular dynamics simulations is the force calculation between particles inside the simulated environment, which has abundant parallelism. Among various acceleration platforms, FPGA is an attractive alternative because of its low power and high energy efficiency. However, due to its high programming cost using RTL, none of the mainstream MD software packages has yet adopted FPGA for acceleration. \nIn this paper we revisit the FPGA acceleration of MD in high-level synthesis (HLS) so as to provide affordable programming cost. Our experience with the MD acceleration demonstrates that HLS optimizations such as loop pipelining, module duplication and memory partitioning are essential to improve the performance, achieving a speedup of 9.5X compared to a 12-core CPU. More importantly, we observe that even the fully optimized HLS design can still be 2X slower than the reference RTL architecture due to the common dynamic (conditional) data flow behavior that is not yet supported by current HLS tools. To support such behavior, we further customize an array of processing elements together with a data-driven streaming network through a common RTL template, and fully automate the design flow. Our final experimental results demonstrate a 19.4X performance speedup and 39X energy efficiency for the widely used ApoA1 MD benchmark on the Convey HC1ex FPGA compared to a 12-core Intel Xeon server.", "venue": "ArXiv", "authors": ["Jason  Cong", "Zhenman  Fang", "Hassan  Kianinejad", "Peng  Wei"], "year": 2016, "n_citations": 5}
{"id": 6066306, "s2_id": "0d048a5476ab8d03ce121ef1cc5eb94a81a93002", "title": "SeMPE: Secure Multi Path Execution Architecture for Removing Conditional Branch Side Channels", "abstract": "One prevalent source of side channel vulnerabilities is the secret-dependent behavior of conditional branches (SDBCB). The state-of-the-art solution relies on Constant-Time Expressions, which require high programming effort and incur high performance overheads. In this paper, we propose SeMPE, an architecture support to eliminate SDBCB without requiring much programming effort while incurring low performance overheads. When a secret-dependent branch is encountered, SeMPE fetches, executes, and commits both paths of the branch, preventing the adversary from inferring secret values from the branching behavior of the program. SeMPE outperforms code generated by FaCT, a constant-time expression language, by up to 18\u00d7.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Andrea  Mondelli", "Paul  Gazzillo", "Yan  Solihin"], "year": 2021, "n_citations": 0}
{"id": 6069117, "s2_id": "1f4206f60ce42e0968d0ce1cfce18ee1a22ed32d", "title": "VIBNN: Hardware Acceleration of Bayesian Neural Networks", "abstract": "Bayesian Neural Networks (BNNs) have been proposed to address the problem of model uncertainty in training and inference. By introducing weights associated with conditioned probability distributions, BNNs are capable of resolving the overfitting issue commonly seen in conventional neural networks and allow for small-data training, through the variational inference process. Frequent usage of Gaussian random variables in this process requires a properly optimized Gaussian Random Number Generator (GRNG). The high hardware cost of conventional GRNG makes the hardware implementation of BNNs challenging. In this paper, we propose VIBNN, an FPGA-based hardware accelerator design for variational inference on BNNs. We explore the design space for massive amount of Gaussian variable sampling tasks in BNNs. Specifically, we introduce two high performance Gaussian (pseudo) random number generators: 1) the RAM-based Linear Feedback Gaussian Random Number Generator (RLF-GRNG), which is inspired by the properties of binomial distribution and linear feedback logics; and 2) the Bayesian Neural Network-oriented Wallace Gaussian Random Number Generator. To achieve high scalability and efficient memory access, we propose a deep pipelined accelerator architecture with fast execution and good hardware utilization. Experimental results demonstrate that the proposed VIBNN implementations on an FPGA can achieve throughput of 321,543.4 Images/s and energy efficiency upto 52,694.8 Images/J while maintaining similar accuracy as its software counterpart.", "venue": "ASPLOS 2018", "authors": ["Ruizhe  Cai", "Ao  Ren", "Ning  Liu", "Caiwen  Ding", "Luhao  Wang", "Xuehai  Qian", "Massoud  Pedram", "Yanzhi  Wang"], "year": 2018, "n_citations": 40}
{"id": 6070935, "s2_id": "a827c4650858d432c1eacf8cebfa4af7b1b0eeee", "title": "Architectures for High Performance Computing and Data Systems using Byte-Addressable Persistent Memory", "abstract": "Non-volatile, byte addressable, memory technology with performance close to main memory promises to revolutionise computing systems in the near future. Such memory technology provides the potential for extremely large memory regions (i.e. > 3TB per server), very high performance I/O, and new ways of storing and sharing data for applications and workflows. This paper outlines an architecture that has been designed to exploit such memory for High Performance Computing and High Performance Data Analytics systems, along with descriptions of how applications could benefit from such hardware.", "venue": "ISC Workshops", "authors": ["Adrian  Jackson", "Mich\u00e8le  Weiland", "Mark  Parsons", "Bernhard  Homoelle"], "year": 2019, "n_citations": 2}
{"id": 6073348, "s2_id": "2c406ff8b76cee17085ea3ffea5b1464928167a9", "title": "HyGain: High Performance, Energy-Efficient Hybrid Gain Cell based Cache Hierarchy", "abstract": "In this paper, we propose a \u201cfull-stack\u201d solution to designing high capacity and low latency on-chip cache hierarchies by starting at the circuit level of the hardware design stack. First, we propose a novel Gain Cell (GC) design using FDSOI. The GC has several desirable characteristics, including ~50% higher storage density and ~50% lower dynamic energy as compared to the traditional 6T SRAM, even after accounting for peripheral circuit overheads. We also exploit back-gate bias to increase retention time to 1.12 ms (~60\u00d7 of eDRAM) which, combined with optimizations like staggered refresh, makes it an ideal candidate to architect all levels of on-chip caches. We show that compared to 6T SRAM, for a given area budget, GC based caches, on average, provide 29% and 36% increase in IPC for singleand multi-programmed workloads, respectively on contemporary workloads including SPEC CPU 2017. We also observe dynamic energy savings of 42% and 34% for singleand multi-programmed workloads, respectively. We utilize the inherent properties of the proposed GC, including decoupled read and write bitlines to devise optimizations to save precharge energy and architect GC caches with better energy and performance characteristics. Finally, in a quest to utilize the best of all worlds, we combine GC with STT-RAM to create hybrid hierarchies. We show that a hybrid hierarchy with GC caches at L1 and L2, and an LLC split between GC and STT-RAM, with asymmetric write optimization enabled, is able to provide a 54% benefit in energy-delay product (EDP) as compared to an all-SRAM design, and 13% as compared to an all-GC cache hierarchy, averaged across multi-programmed workloads.", "venue": "ArXiv", "authors": ["Sarabjeet  Singh", "Neelam  Surana", "Pranjali  Jain", "Joycee  Mekie", "Manu  Awasthi"], "year": 2021, "n_citations": 0}
{"id": 6081855, "s2_id": "3739596988af8ba5acaf65e9a7843fb4e7df700b", "title": "Hardware Implementation of Hyperbolic Tangent Function using Catmull-Rom Spline Interpolation", "abstract": "Deep neural networks yield the state of the art results in many computer vision and human machine interface tasks such as object recognition, speech recognition etc. Since, these networks are computationally expensive, customized accelerators are designed for achieving the required performance at lower cost and power. One of the key building blocks of these neural networks is non-linear activation function such as sigmoid, hyperbolic tangent (tanh), and ReLU. A low complexity accurate hardware implementation of the activation function is required to meet the performance and area targets of the neural network accelerators. This paper presents an implementation of tanh function using the Catmull-Rom spline interpolation. State of the art results are achieved using this method with comparatively smaller logic area.", "venue": "ArXiv", "authors": ["Mahesh  Chandra"], "year": 2020, "n_citations": 1}
{"id": 6085444, "s2_id": "083a6188fa6b0ed96adbf718e0a9dd6838e22af2", "title": "A concept of a measuring system for probe kinesthetic parameters identification during echocardiography examination", "abstract": "Echocardiography is the most commonly used imaging technique in clinical cardiology. Due to the high demand for this type of examination and the small number of specialists, there is a need to support the examination process through telemedicine. Moreover, specialist training can be supported by appropriate simulation systems. For (i) creating tailor-made tele-echo robots, (ii) creating echo system simulators, and (iii) conducting echo examination with local or remote expert assistance, knowledge about echo probe kinesthetic parameters during echocardiography examination is advisable. The article describes the concept of a measuring system for obtaining such data.", "venue": "ArXiv", "authors": ["Tomasz  Winiarski", "Pawel  Balsam", "Maciej  Wegierek", "Sonia  Borodzicz-Jazdzyk", "Wojciech  Dudek", "Konrad  Banachowicz", "Janusz  Kochanowski", "Michal  Marchel"], "year": 2020, "n_citations": 0}
{"id": 6086332, "s2_id": "9e3c61c16f1c1834c430c6e18281851c53719d66", "title": "A Post-Silicon Trace Analysis Approach for System-on-Chip Protocol Debug", "abstract": "Reconstructing system-level behavior from silicon traces is a critical problem in post-silicon validation of System-on-Chip designs. Current industrial practice in this area is primarily manual, depending on collaborative insights of the architects, designers, and validators. This paper presents a trace analysis approach that exploits architectural models of system-level protocols to reconstruct design behavior from partially observed silicon traces in the presence of ambiguous and noisy data. The output of the approach is a set of all potential interpretations of a system's internal execution abstracted to system-level protocols. To support the trace analysis approach, a companion trace signal selection framework guided by system-level protocols is also presented, and its impacts on the complexity and accuracy of the analysis approach are discussed. That approach and the framework have been evaluated on a multi-core System-on-Chip prototype that implements a set of common industrial system-level protocols.", "venue": "2017 IEEE International Conference on Computer Design (ICCD)", "authors": ["Yuting  Cao", "Hao  Zheng", "Hernan M. Palombo", "Sandip  Ray", "Jin  Yang"], "year": 2017, "n_citations": 4}
{"id": 6086534, "s2_id": "27cc581928fdd504d60c041671df5c185f966491", "title": "FORMS: Fine-grained Polarized ReRAM-based In-situ Computation for Mixed-signal DNN Accelerator", "abstract": "Recent work demonstrated the promise of using resistive random access memory (ReRAM) as an emerging technology to perform inherently parallel analog domain in-situ matrix-vector multiplication\u2014the intensive and key computation in deep neural networks (DNNs). One key problem is the weights that are signed values. However, in a ReRAM crossbar, weights are stored as conductance of the crossbar cells, and the in-situ computation assumes all cells on each crossbar column are of the same sign. The current architectures either use two ReRAM crossbars for positive and negative weights (PRIME), or add an offset to weights so that all values become positive (ISAAC). Neither solution is ideal: they either double the cost of crossbars, or incur extra offset circuity. To better address this problem, we propose FORMS, a fine-grained ReRAM-based DNN accelerator with algorithm/hardware co-design. Instead of trying to represent the positive/negative weights, our key design principle is to enforce exactly what is assumed in the in-situ computation\u2014 ensuring that all weights in the same column of a crossbar have the same sign. It naturally avoids the cost of an additional crossbar. Such polarized weights can be nicely generated using alternating direction method of multipliers (ADMM) regularized optimization during the DNN training, which can exactly enforce certain patterns in DNN weights. To achieve high accuracy, we divide the crossbar into logical sub-arrays and only enforce this property within the fine-grained sub-array columns. Crucially, the small sub-arrays provides a unique opportunity for input zero-skipping, which can significantly avoid unnecessary computations and reduce computation time. At the same time, it also makes the hardware much easier to implement and is less susceptible to non-idealities and noise than coarse-grained architectures. Putting all together, with the same optimized DNN models, FORMS achieves 1.50\u00d7 and 1.93\u00d7 throughput improvement in terms of $\\frac{{GOPs}}{{s \\times m{m^2}}}$ and $\\frac{{GOPs}}{W}$ compared to ISAAC, and 1.12\u00d7 ~2.4 \u00d7 speed up in terms of frame per second over optimized ISAAC with almost the same power/area cost. Interestingly, FORMS optimization framework can even speed up the original ISAAC from 10.7 \u00d7 up to 377.9\u00d7, reflecting the importance of software/hardware co-design optimizations.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Geng  Yuan", "Payman  Behnam", "Zhengang  Li", "Ali  Shafiee", "Sheng  Lin", "Xiaolong  Ma", "Hang  Liu", "Xuehai  Qian", "Mahdi Nazm Bojnordi", "Yanzhi  Wang", "Caiwen  Ding"], "year": 2021, "n_citations": 2}
{"id": 6087296, "s2_id": "78688f65a6c10ee18bf8c99a1c926383d966e2a3", "title": "Gain and Pain of a Reliable Delay Model", "abstract": "Simulation on small circuits reveal, that the commonly used methods fail\n to provide a comprehensive picture of the possible behavior. The \nInvolution Delay Model, however, manages to fill this gap at a cost of \nincreased simulation time.", "venue": "ArXiv", "authors": ["Jurgen  Maier"], "year": 2021, "n_citations": 0}
{"id": 6092640, "s2_id": "c1fe35e3ca313c0308d722366181f617c7fd9c11", "title": "Maximizing CNN accelerator efficiency through resource partitioning", "abstract": "Convolutional neural networks (CNNs) are revolutionizing machine learning, but they present significant computational challenges. Recently, many FPGA-based accelerators have been proposed to improve the performance and efficiency of CNNs. Current approaches construct a single processor that computes the CNN layers one at a time; the processor is optimized to maximize the throughput at which the collection of layers is computed. However, this approach leads to inefficient designs because the same processor structure is used to compute CNN layers of radically varying dimensions. We present a new CNN accelerator paradigm and an accompanying automated design methodology that partitions the available FPGA resources into multiple processors, each of which is tailored for a different subset of the CNN convolutional layers. Using the same FPGA resources as a single large processor, multiple smaller specialized processors increase computational efficiency and lead to a higher overall throughput. Our design methodology achieves 3.8x higher throughput than the state-of-the-art approach on evaluating the popular AlexNet CNN on a Xilinx Virtex-7 FPGA. For the more recent SqueezeNet and GoogLeNet, the speedups are 2.2x and 2.0x.", "venue": "2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Yongming  Shen", "Michael  Ferdman", "Peter A. Milder"], "year": 2017, "n_citations": 199}
{"id": 6092690, "s2_id": "8900ea4b1cfec02a2b57ec9ee09e25e5c8001b0e", "title": "An event-based architecture for solving constraint satisfaction problems", "abstract": "Constraint satisfaction problems are ubiquitous in many domains. They are typically solved using conventional digital computing architectures that do not reflect the distributed nature of many of these problems, and are thus ill-suited for solving them. Here we present a parallel analogue/digital hardware architecture specifically designed to solve such problems. We cast constraint satisfaction problems as networks of stereotyped nodes that communicate using digital pulses, or events. Each node contains an oscillator implemented using analogue circuits. The non-repeating phase relations among the oscillators drive the exploration of the solution space. We show that this hardware architecture can yield state-of-the-art performance on random SAT problems under reasonable assumptions on the implementation. We present measurements from a prototype electronic chip to demonstrate that a physical implementation of the proposed architecture is robust to practical non-idealities and to validate the theory proposed.", "venue": "Nature communications", "authors": ["Hesham  Mostafa", "Lorenz K. M\u00fcller", "Giacomo  Indiveri"], "year": 2015, "n_citations": 33}
{"id": 6094827, "s2_id": "f8482efd0ce52c85201c41cf99f8076dc1403310", "title": "RISC and CISC", "abstract": "Comparison of RISC & CISC in details, encompassing the addressing modes, evolution, definitions and characteristics. Pre - RISC design is also elaborated. Both the architectures are explained with the help of example. Analysis is made based on performance.", "venue": "ArXiv", "authors": ["Farhat  Masood"], "year": 2011, "n_citations": 5}
{"id": 6095131, "s2_id": "fd3f5ab74dd45dc1011755ed5bcae70f226bcb29", "title": "In-RDBMS hardware acceleration of advanced analytics", "abstract": "The data revolution is fueled by advances in machine learning, databases, and hardware design. Programmable accelerators are making their way into each of these areas independently. As such, there ...", "venue": "VLDB 2018", "authors": ["MahajanDivya", "KimJoon  Kyung", "SacksJacob", "ArdalanAdel", "KumarArun", "EsmaeilzadehHadi"], "year": 2018, "n_citations": 0}
{"id": 6103861, "s2_id": "3010350a7ae3907173642cc9e6e7cf63164ed804", "title": "Novel Reversible `TSG' Gate and Its Application for Designing Components of Primitive Reversible/Quantum ALU", "abstract": "In recent years, reversible logic has emerged as a promising computing paradigm having application in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. This paper utilizes a new 4*4 reversible gate called TSG gate to build the components of a primitive reversible/quantum ALU. The most significant aspect of the TSG gate is that it can work singly as a reversible full adder, that is reversible full adder can now be implemented with a single gate only. A Novel reversible 4:2 compressor is also designed from the TSG gate which is later used to design a novel 8times8 reversible Wallace tree multiplier. It is proved that the adder, 4:2 compressor and multiplier architectures designed using the TSG gate are better than their counterparts available in literature, in terms of number of reversible gates and garbage outputs. This is perhaps, the first attempt to design a reversible 4:2 compressor and a reversible Wallace tree multiplier as far as existing literature and our knowledge is concerned. Thus, this paper provides an initial threshold to build more complex systems which can execute complicated operations using reversible logic", "venue": "2005 5th International Conference on Information Communications & Signal Processing", "authors": ["Himanshu  Thapliyal", "M. B. Srinivas"], "year": 2005, "n_citations": 59}
{"id": 6103895, "s2_id": "ab79d521655f654e8f62d74ec001d00f007bb31a", "title": "MultPIM: Fast Stateful Multiplication for Processing-in-Memory", "abstract": "Processing-in-memory (PIM) seeks to eliminate computation/memory data transfer using devices that support both storage and logic. Stateful logic techniques such as IMPLY, MAGIC and FELIX can perform logic gates within memristive crossbar arrays with massive parallelism. Multiplication via stateful logic is an active field of research due to the wide implications. Recently, RIME has become the state-of-the-art algorithm for stateful single-row multiplication by using memristive partitions, reducing the latency of the previous state-of-the-art by 5.1\u00d7. In this paper, we begin by proposing novel partitionbased computation techniques for broadcasting and shifting data. Then, we design an in-memory multiplication algorithm based on the carry-save add-shift (CSAS) technique. Finally, we detail specific logic optimizations to the algorithm that further reduce latency. These contributions constitute MultPIM, a multiplier that reduces state-of-the-art time complexity from quadratic to linear-log. For 32-bit numbers, MultPIM improves latency by an additional 3.8\u00d7 over RIME, while even slightly reducing area overhead. Furthermore, we optimize MultPIM for full-precision matrix-vector multiplication and demonstrate 22.0\u00d7 latency improvement over FloatPIM matrix-vector multiplication.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Orian  Leitersdorf", "Ronny  Ronen", "Shahar  Kvatinsky"], "year": 2021, "n_citations": 1}
{"id": 6104173, "s2_id": "b5db1ab7ad113f4b5ad86607192abc80835c3fae", "title": "An Efficient Partial Sums Generator for Constituent Code based Successive Cancellation Decoding of Polar Codes", "abstract": "This paper proposes the architecture of partial sum generator for constituent codes based polar code decoder. Constituent codes based polar code decoder has the advantage of low latency. However, no purposefully designed partial sum generator design exists that can yield desired timing for the decoder. We first derive the mathematical presentation with the partial sums set $\\bm{\\beta^c}$ which is corresponding to each constituent codes. From this, we concoct a shift-register based partial sum generator. Next, the overall architecture and design details are described, and the overhead compared with conventional partial sum generator is evaluated. Finally, the implementation results with both ASIC and FPGA technology and relevant discussions are presented.", "venue": "ArXiv", "authors": ["Tiben  Che", "Gwan S. Choi"], "year": 2016, "n_citations": 1}
{"id": 6106770, "s2_id": "8c529a8103c9256484632c3f3563bcf447522b6a", "title": "Open Tiled Manycore System-on-Chip", "abstract": "Manycore System-on-Chip include an increasing amount of processing elements and have become an important research topic for improvements of both hardware and software. While research can be conducted using system simulators, prototyping requires a variety of components and is very time consuming. With the Open Tiled Manycore System-on-Chip (OpTiMSoC) we aim at building such an environment for use in our and other research projects as prototyping platform. \nThis paper describes the project goals and aspects of OpTiMSoC and summarizes the current status and ideas.", "venue": "ArXiv", "authors": ["Stefan  Wallentowitz", "Philipp  Wagner", "Michael  Tempelmeier", "Thomas  Wild", "Andreas  Herkersdorf"], "year": 2013, "n_citations": 8}
{"id": 6109961, "s2_id": "11638320039aff2d9e07915d5fcf239148989503", "title": "FIST: A Feature-Importance Sampling and Tree-Based Method for Automatic Design Flow Parameter Tuning", "abstract": "Design flow parameters are of utmost importance to chip design quality and require a painfully long time to evaluate their effects. In reality, flow parameter tuning is usually performed manually based on designers\u2019 experience in an ad hoc manner. In this work, we introduce a machine learning-based automatic parameter tuning methodology that aims to find the best design quality with a limited number of trials. Instead of merely plugging in machine learning engines, we develop clustering and approximate sampling techniques for improving tuning efficiency. The feature extraction in this method can reuse knowledge from prior designs. Furthermore, we leverage a state-of-the-art XGBoost model and propose a novel dynamic tree technique to overcome overfitting. Experimental results on benchmark circuits show that our approach achieves 25% improvement in design quality or 37% reduction in sampling cost compared to random forest method, which is the kernel of a highly cited previous work. Our approach is further validated on two industrial designs. By sampling less than 0.02% of possible parameter sets, it reduces area by 1.83% and 1.43% compared to the best solutions hand-tuned by experienced designers.", "venue": "2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Zhiyao  Xie", "Guan-Qi  Fang", "Yu-Hung  Huang", "Haoxing  Ren", "Yanqing  Zhang", "Brucek  Khailany", "Shao-Yun  Fang", "Jiang  Hu", "Yiran  Chen", "Erick Carvajal Barboza"], "year": 2020, "n_citations": 1}
{"id": 6114497, "s2_id": "70e5b9bd2b2d402b6d2a903df17f3272a8a1c598", "title": "Being-ahead: Benchmarking and Exploring Accelerators for Hardware-Efficient AI Deployment", "abstract": "Customized hardware accelerators have been developed to provide improved performance and efficiency for DNN inference and training. However, the existing hardware accelerators may not always be suitable for handling various DNN models as their architecture paradigms and configuration tradeoffs are highly application-specific. It is important to benchmark the accelerator candidates in the earliest stage to gather comprehensive performance metrics and locate the potential bottlenecks. Further demands also emerge after benchmarking, which require adequate solutions to address the bottlenecks and improve the current designs for targeted workloads. To achieve these goals, in this paper, we leverage an automation tool called DNNExplorer [1] for benchmarking customized DNN hardware accelerators and exploring novel accelerator designs with improved performance and efficiency. Key features include (1) direct support to popular machine learning frameworks for DNN workload analysis and accurate analytical models for fast accelerator benchmarking; (2) a novel accelerator design paradigm with high-dimensional design space support and fine-grained adjustability to overcome the existing design drawbacks; and (3) a design space exploration (DSE) engine to generate optimized accelerators by considering targeted AI workloads and available hardware resources. Results show that accelerators adopting the proposed novel paradigm can deliver up to 4.2\u00d7 higher throughput (GOP/s) than the state-of-the-art pipeline design in [2] and up to 2.0\u00d7 improved efficiency than the recently published generic design in [3]) given the same DNN model and resource budgets. With DNNExplorer\u2019s benchmarking and exploration features, we can be ahead at building and optimizing customized AI accelerators and enable more efficient AI applications.", "venue": "ArXiv", "authors": ["Xiaofan  Zhang", "Hanchen  Ye", "Deming  Chen"], "year": 2021, "n_citations": 0}
{"id": 6114655, "s2_id": "f89cb09ddc564ae13687e0c7d9a981363f2a5aa6", "title": "Energy Efficient Computing Systems: Architectures, Abstractions and Modeling to Techniques and Standards", "abstract": "Computing systems have undergone several inflexion points - while Moore's law guided the semiconductor industry to cram more and more transistors and logic into the same volume, the limits of instruction-level parallelism (ILP) and the end of Dennard's scaling drove the industry towards multi-core chips. We have now entered the era of domain-specific architectures for new workloads like AI and ML. These trends continue, arguably with other limits, along with challenges imposed by tighter integration, extreme form factors and diverse workloads, making systems more complex from an energy efficiency perspective. Many research surveys have covered different aspects of techniques in hardware and microarchitecture across devices, servers, HPC, data center systems along with software, algorithms, frameworks for energy efficiency and thermal management. Somewhat in parallel, the semiconductor industry has developed techniques and standards around specification, modeling and verification of complex chips; these areas have not been addressed in detail by previous research surveys. This survey aims to bring these domains together and is composed of a systematic categorization of key aspects of building energy efficient systems - (a) specification - the ability to precisely specify the power intent or properties at different layers (b) modeling and simulation of the entire system or subsystem (hardware or software or both) so as to be able to perform what-if analysis, (c) techniques used for implementing energy efficiency at different levels of the stack, (d) verification techniques used to provide guarantees that the functionality of complex designs are preserved, and (e) energy efficiency standards and consortiums that aim to standardize different aspects of energy efficiency, including cross-layer optimizations.", "venue": "ArXiv", "authors": ["Rajeev  Muralidhar", "Renata  Borovica", "Rajkumar  Buyya"], "year": 2020, "n_citations": 1}
{"id": 6115596, "s2_id": "2be5b430dd9a08b9aa6b46b74d3e5222859a30db", "title": "Thermal and IR Drop Analysis Using Convolutional Encoder-Decoder Networks", "abstract": "Computationally expensive temperature and power grid analyses are required during the design cycle to guide IC design. This paper employs encoder-decoder based generative (EDGe) networks to map these analyses to fast and accurate image-to-image and sequence-to-sequence translation tasks. The network takes a power map as input and outputs the temperature or IR drop map. We propose two networks: (i) ThermEDGe: a static and dynamic full-chip temperature estimator and (ii) IREDGe: a full-chip static IR drop predictor based on input power, power grid distribution, and power pad distribution patterns. The models are design-independent and must be trained just once for a particular technology and packaging solution. ThermEDGe and IREDGe are demonstrated to rapidly predict on-chip temperature and IR drop contours in milliseconds (in contrast with commercial tools that require several hours or more) and provide an average error of 0.6% and 0.008% respectively.", "venue": "2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Vidya A. Chhabria", "Vipul  Ahuja", "Ashwath  Prabhu", "Nikhil  Patil", "Palkesh  Jain", "Sachin S. Sapatnekar"], "year": 2021, "n_citations": 4}
{"id": 6116082, "s2_id": "ae2412d0e9dadbac2082eec1bc55db5456c91e1f", "title": "A Unified Model for Gate Level Propagation Analysis", "abstract": "Classic hardware verification techniques (e.g., X-propagation and fault-propagation) and more recent hardware security verification techniques based on information flow tracking (IFT) aim to understand how information passes, affects, and otherwise modifies a circuit. These techniques all have separate usage scenarios, but when dissected into their core functionality, they relate in a fundamental manner. In this paper, we develop a common framework for gate level propagation analysis. We use our model to generate synthesizable propagation logic to use in standard EDA tools. To justify our model, we prove that Precise Hardware IFT is equivalent to gate level X-propagation and imprecise fault propagation. We also show that the difference between Precise Hardware IFT and fault propagation is not significant for 74X-series and '85 ISCAS benchmarks with more than 313 gates and the difference between imprecise hardware IFT and Precise Hardware IFT is almost always significant regardless of size.", "venue": "ArXiv", "authors": ["Jeremy  Blackstone", "Wei  Hu", "Alric  Althoff", "Armaiti  Ardeshiricham", "Lu  Zhang", "Ryan  Kastner"], "year": 2020, "n_citations": 1}
{"id": 6117501, "s2_id": "d1af2532e706536b4ecd740eefab287a8e4204f7", "title": "Comments on \u201cIEEE 1588 Clock Synchronization Using Dual Slave Clocks in a Slave\u201d", "abstract": "In the above letter, Chin and Chen proposed an IEEE 1588 clock synchronization method based on dual slave clocks, where they claim that multiple unknown parameters-i.e., clock offset, clock skew, and master-to-slave delay-can be estimated with only one-way time transfers using more equations than usual. This comment investigates Chin and Chen's dual clock scheme with detailed models for a master and dual slave clocks and shows that the formulation of multi-parameter estimation is invalid, which affirms that it is impossible to distinguish the effect of delay from that of clock offset at a slave even with dual slave clocks.", "venue": "IEEE Communications Letters", "authors": ["Kyeong Soo Kim"], "year": 2014, "n_citations": 7}
{"id": 6119130, "s2_id": "217b3a70213a945ffcf87b1036dbd73225f90306", "title": "Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning", "abstract": "Past research has proposed numerous hardware prefetching techniques, most of which rely on exploiting one specific type of program context information (e.g., program counter, cacheline address, or delta between cacheline addresses) to predict future memory accesses. These techniques either completely neglect a prefetcher\u2019s undesirable effects (e.g., memory bandwidth usage) on the overall system, or incorporate system-level feedback as an afterthought to a system-unaware prefetch algorithm. We show that prior prefetchers often lose their performance benefit over a wide range of workloads and system configurations due to their inherent inability to take multiple different types of program context and system-level feedback information into account while prefetching. In this paper, we make a case for designing a holistic prefetch algorithm that learns to prefetch using multiple different types of program context and system-level feedback information inherent to its design. To this end, we propose Pythia, which formulates the prefetcher as a reinforcement learning agent. For every demand request, Pythia observes multiple different types of program context information to make a prefetch decision. For every prefetch decision, Pythia receives a numerical reward that evaluates prefetch quality under the current memory bandwidth usage. Pythia uses this reward to reinforce the correlation between program context information and prefetch decision to generate highly accurate, timely, and system-aware prefetch requests in the future. Our extensive evaluations using simulation and hardware synthesis show that Pythia outperforms two state-of-the-art prefetchers (MLOP and Bingo) by 3.4% and 3.8% in single-core, 7.7% and 9.6% in twelve-core, and 16.9% and 20.2% in bandwidth-constrained core configurations, while incurring only 1.03% area overhead over a desktop-class processor and no software changes in workloads. The source code of Pythia can be freely downloaded from https://github.com/CMU-SAFARI/Pythia.", "venue": "MICRO", "authors": ["Rahul  Bera", "Konstantinos  Kanellopoulos", "Anant V. Nori", "Taha  Shahroodi", "Sreenivas  Subramoney", "Onur  Mutlu"], "year": 2021, "n_citations": 0}
{"id": 6123487, "s2_id": "98254da9c33130b99ecba523efca61346c2be461", "title": "FP-Stereo: Hardware-Efficient Stereo Vision for Embedded Applications", "abstract": "Fast and accurate depth estimation, or stereo matching, is essential in embedded stereo vision systems, requiring substantial design effort to achieve an appropriate balance among accuracy, speed and hardware cost. To reduce the design effort and achieve the right balance, we propose FP-Stereo for building high-performance stereo matching pipelines on FPGAs automatically. FP-Stereo consists of an open-source hardware-efficient library, allowing designers to obtain the desired implementation instantly. Diverse methods are supported in our library for each stage of the stereo matching pipeline and a series of techniques are developed to exploit the parallelism and reduce the resource overhead. To improve the usability, FP-Stereo can generate synthesizable C code of the FPGA accelerator with our optimized HLS templates automatically. To guide users for the right design choice meeting specific application requirements, detailed comparisons are performed on various configurations of our library to investigate the accuracy/speed/cost trade-off. Experimental results also show that FP-Stereo outperforms the state-of-the-art FPGA design from all aspects, including 6.08% lower error, 2x faster speed, 30% less resource usage and 40% less energy consumption. Compared to GPU designs, FP-Stereo achieves the same accuracy at a competitive speed while consuming much less energy.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Jieru  Zhao", "Tingyuan  Liang", "Liang  Feng", "Wenchao  Ding", "Sharad  Sinha", "Wei  Zhang", "Shaojie  Shen"], "year": 2020, "n_citations": 4}
{"id": 6125462, "s2_id": "13824ae084e3869bcb7964d6da51ae126b653d60", "title": "Gain and Pain of a Reliable Delay Model", "abstract": "In this paper we evaluate a promising delay estimation method, the Involution Delay Model. We apply it to three simple circuits (a combinatorial loop, an SR latch and an adder), interpret the delivered results and determine realistic overhead estimations. Comparisons to analog SPICE simulations reveal fine-grained behavioral coverage, whereat the commonly used digital inertial delay model shows severe shortcomings. Overall, the Involution Delay Model is able to identify a wide range of malicious behavior and is thus a viable upgrade to available delay models in modern digital timing simulation tools.", "venue": "2021 24th Euromicro Conference on Digital System Design (DSD)", "authors": ["J\u00fcrgen  Maier"], "year": 2021, "n_citations": 1}
{"id": 6127733, "s2_id": "f3561793929f0b853b32a01a3b967dcf26840a8e", "title": "CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models for Full-Chip Parasitic Extraction", "abstract": "Accurate capacitance extraction is becoming more important for designing integrated circuits under advanced process technology. The pattern matching based full-chip extraction methodology delivers fast computational speed, but suffers from large error, and tedious efforts on building capacitance models of the increasing structure patterns. In this work, we propose an effective method for building convolutional neural network (CNN) based capacitance models (called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance extraction. With a novel grid-based data representation, the proposed method is able to model the pattern with a variable number of conductors, so as to largely reduce the number of patterns. Based on the ability of ResNet architecture on capturing spatial information and the proposed training skills, the obtained CNN-Cap exhibits much better performance over the multilayer perception neural network based capacitance model while being more versatile. Extensive experiments on a 55nm and a 15nm process technologies have demonstrated that the error of total capacitance produced with CNN-Cap is always within 1.3% and the error of produced coupling capacitance is less than 10% in over 99.5% probability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU server, while it consumes negligible memory compared to the look-up table based capacitance model.", "venue": "2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Dingcheng  Yang", "Wenjian  Yu", "Yuanbo  Guo", "Wenjie  Liang"], "year": 2021, "n_citations": 0}
{"id": 6128815, "s2_id": "dbb90b01430e0b6a62f0fe28eefae9c604a79815", "title": "ACTreS: Analog Clock Tree Synthesis", "abstract": "This paper describes a graph-theoretic formalism and a flow that, to a great extent, automate the design of clock trees in Sampled-Data Analog Circuits (SDACs). The current practice for clock tree design of SDACs is a manual process, which is time-consuming and error-prone. Clock tree design in digital domain, however, is fully automated and is carried out by Clock Tree Synthesis (CTS) software. In spite of critical differences, SDAC clock tree design problem has fundamental similarities with its digital counterpart. We exploited these similarities and built a design flow and tool set, which uses commercial digital CTS software as an intermediate step. We will explain our flow using a 0.18 micron 10-bit 60 MHz 2-stage pipelined differential-input flash analog-to-digital converter as a test circuit.", "venue": "ArXiv", "authors": ["Bilgiday  Yuce", "H. Fatih Ugurdag", "Iskender  Agi", "Gokhan  Guner", "Vahap Baris Esen", "Seyrani  Korkmaz", "I. Faik Baskaya", "Gunhan  Dundar"], "year": 2021, "n_citations": 0}
{"id": 6130718, "s2_id": "7b701e943158cd77f475667cd8150710be5cf09c", "title": "Design and Development of Low Cost Multi-Channel USB Data", "abstract": "ABSTRACT This paper describes the design and development of low cost USB Data Acquisition System (DAS) for the measurement of physical parameters. Physical parameters such as temperature, humidity, light intensity etc., which are generally slowly varying signals are sensed by respective sensors or integrated sensors and converted into voltages. The DAS is designed using PIC18F4550 microcontroller, communicating with Personal Computer (PC) through USB (Universal Serial Bus). The designed DAS has been tested with the application program developed in Visual Basic, which allows online monitoring in graphical as well as numerical display. General Terms Data Acquisition System (DAS), Universal Serial Bus (USB), Microcontroller. Keywords Data Acquisition System (DAS), temperature, humidity, online monitoring. 1. INTRODUCTION As the computer technology advances, the performance and the availability of the PCs and Laptop become reliable, common and also the prices are falling drastically. Thus, the design and development of the low cost PC based DAS using microcontrollers for use in various fields has been a challenging task. Research is going on in various fields for the design and development of low cost real time DAS [1-5]. Physical parameters such as temperature, pressure, humidity, light intensity etc., are generally slowly varying signals. They can be sensed by respective sensor or transducer giving changes in electrical parameters. In the laboratories or industrial environment, it is very much essential to monitor and/or control such physical parameters. Manual observation and recording of such parameters for continuously for a long time is almost impossible and it cannot fulfill the current requirements in terms of the accuracy and time duration. The efficient solution of this problem is to develope data logger or DAS [6-7]. The present work is to explore the design and development of the low cost multi channel USB DAS using PIC18F4550 microcontroller for continuous monitoring and storing of the physical parameters such as temperature, humidity, etc. Most of the researcher develop application program to customize the readymade data acquisition (DAQ) cards for their specific application. The unique feature of our designed DAS is that, it is designed and developed with commonly available components in the market at low cost; firmware and application program are also developed and are user friendly.", "venue": "ArXiv", "authors": ["Nungleppam Monoranjan Singh", "Kanak Chandra Sarma", "Nungleppam Gopil Singh"], "year": 2012, "n_citations": 3}
{"id": 6135640, "s2_id": "7005b6523562b2612a01e667d3fc401e0a0e8711", "title": "Janus: An FPGA-Based System for High-Performance Scientific Computing", "abstract": "Janus is a modular, massively parallel, and reconfigurable FPGA-based computing system. Each Janus module has one computational core and one host. Janus is tailored to, but not limited to, the needs of a class of hard scientific applications characterized by regular code structure, unconventional data-manipulation requirements, and a few Megabits database. The authors discuss this configurable system's architecture and focus on its use for Monte Carlo simulations of statistical mechanics, as Janus performs impressively on this class of application.", "venue": "Computing in Science & Engineering", "authors": ["Francesco  Belletti", "Maria  Cotallo", "Andr\u00e9s Cruz Flor", "Luis Antonio Fern\u00e1ndez", "Antonio  Gordillo", "Andrea  Maiorano", "Filippo  Mantovani", "Enzo  Marinari", "Victor  Martin-Mayor", "Antonio Mu\u00f1oz Sudupe", "Denis  Navarro", "Sergio Perez Gaviro", "Mauro  Rossi", "Juan Jesus Ruiz-Lorenzo", "Sebastiano Fabio Schifano", "Daniele  Sciretti", "Alfonso  Taranc\u00f3n", "Raffaele  Tripiccione", "Jose Luis Velasco"], "year": 2009, "n_citations": 64}
{"id": 6135858, "s2_id": "7df9df33438c5617c3554e44f6719526b66201e5", "title": "P3FA: Unified Unicast/Multicast Forwarding with Low Egress Diversities", "abstract": "Multicast is an efficient way to realize one-to-many group communications in large-scale networks such as the Internet. However, the deployment of IP multicast services over the Internet has not been as rapid as expected and needed. Excepting the fatal defects in designing IPv4 address structure. Another main reason that contributes to this slow deployment is the lack of carrier-grade multicast-enabled switches and routers that can be as to scale as their unicast counterparts. Implementing a high-performance switch/router relies on a polynomial-time group membership query algorithm within the Packet Forwarding Engines (PFEs) to determine whether or not a packet is forwarded through an egress. Among these, Bloom filter (BF)-based and Residue Number System (RNS)-based are being considered as two representations of the membership query algorithms. However, both approaches suffer from some fatal weaknesses such as space and time inefficiencies, especially for a carrier-grade PFE with high port-density features. According to similar properties of the prime theorem, we propose a simplified forwarding scheme in this paper, named Per-Port Prime Filter Array (P3FA). The simulation results indicate that the P3FA can significantly improve space efficiencies under specific lower egress-diversities conditions. Under the same space constraints, compared with the SVRF, the multicast time efficiencies, the unicast time efficiency of the P3FA are respectively increased by 12x-17234x and 19x-2038x at a range of port-densities 16-1024, but at the expense of hardware cost, which increased by \u03c1/2x. A PFE designer that attempts to adopt P3FA should trade-off between required performance and cost.", "venue": "ArXiv", "authors": ["Zhu  Jin", "Wen-Kang  Jia"], "year": 2021, "n_citations": 0}
{"id": 6139996, "s2_id": "8dc8ce191a565f665f2319b024baac9a7cbe521f", "title": "An Efficient Hybrid I/O Caching Architecture Using Heterogeneous SSDs", "abstract": "Storage subsystem is considered as the performance bottleneck of computer systems in data-intensive applications. Solid-State Drives (SSDs) are emerging storage devices which unlike Hard Disk Drives (HDDs), do not have mechanical parts and therefore, have superior performance compared to HDDs. Due to the high cost of SSDs, entirely replacing HDDs with SSDs is not economically justified. Additionally, SSDs can endure a limited number of writes before failing. To mitigate the shortcomings of SSDs while taking advantage of their high performance, SSD caching is practiced in both academia and industry. Previously proposed caching architectures have only focused on either performance or endurance and neglected to address both parameters in suggested architectures. Moreover, the cost, reliability, and power consumption of such architectures is not evaluated. This paper proposes a hybrid I/O caching architecture that while offers higher performance than previous studies, it also improves power consumption with a similar budget. The proposed architecture uses DRAM, Read-Optimized SSD (RO-SSD), and Write-Optimized SSD (WO-SSD) in a three-level cache hierarchy and tries to efficiently redirect read requests to either DRAM or RO-SSD while sending writes to WO-SSD. To provide high reliability, dirty pages are written to at least two devices which removes any single point of failure. The power consumption is also managed by reducing the number of accesses issued to SSDs. The proposed architecture reconfigures itself between performance- and endurance-optimized policies based on the workload characteristics to maintain an effective tradeoff between performance and endurance. We have implemented the proposed architecture on a server equipped with industrial SSDs and HDDs. The experimental results show that as compared to state-of-the-art studies, the proposed architecture improves performance and power consumption by an average of 8 and 28 percent, respectively, and reduces the cost by 5 percent while increasing the endurance cost by 4.7 percent and negligible reliability penalty.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Reza  Salkhordeh", "Mostafa  Hadizadeh", "Hossein  Asadi"], "year": 2019, "n_citations": 9}
{"id": 6140114, "s2_id": "86a89c8b0ac11424fd18a34d786b06c785d36160", "title": "A Novel Quantum Cost Efficient Reversible Full Adder Gate in Nanotechnology", "abstract": "Reversible logic has become one of the promising research directions in low power dissipating circuit design in the past few years and has found its applications in low power CMOS design, cryptography, optical information processing and nanotechnology. This paper presents a novel and quantum cost efficient reversible full adder gate in nanotechnology. This gate can work singly as a reversible full adder unit and requires only one clock cycle. The proposed gate is a universal gate in the sense that it can be used to synthesize any arbitrary Boolean functions. It has been demonstrated that the hardware complexity offered by the proposed gate is less than the existing counterparts. The proposed reversible full adder gate also adheres to the theoretical minimum established by the researchers.", "venue": "ArXiv", "authors": ["Md. Saiful Islam"], "year": 2010, "n_citations": 29}
{"id": 6146733, "s2_id": "30943fc2c894332a9666e30aae90984e307597df", "title": "Efficient Instruction Scheduling using Real-time Load Delay Tracking", "abstract": "Many hardware structures in today\u2019s highperformance out-of-order processors do not scale in an efficient way. To address this, different solutions have been proposed that build execution schedules in an energy-efficient manner. Issue time prediction processors are one such solution that use dataflow dependencies and predefined instruction latencies to predict issue times of repeated instructions. In this work, we aim to improve their accuracy, and consequently their performance, in an energy efficient way. We accomplish this by taking advantage of two key observations. First, memory accesses often take additional time to arrive than the static, predefined access latency that is used to describe these systems. This is due to contention in the memory hierarchy and variability in DRAM access times. The use of this observed delay is important to optimize a processor\u2019s execution schedule, as previous works that use predefined information demonstrate performance losses as high as 25%. Second, we find that these memory access delays often repeat across iterations of the same code. This, in turn, allows us to predict the arrival time of these accesses. In this work, we introduce a new processor microarchitecture, that replaces a complex reservation-station-based scheduler with an efficient, scalable alternative. Our proposed scheduling technique tracks real-time delays of loads to accurately predict instruction issue times, and uses a reordering mechanism to prioritize instructions based on that prediction, achieving closeto-out-of-order processor performance. To accomplish this in an energy-efficient manner we introduce: (1) an instruction delay learning mechanism that monitors repeated load instructions and learns their latest delay, (2) an issue time predictor that uses learned delays and data-flow dependencies to predict instruction issue times and (3) priority queues that reorder instructions based on their issue time prediction. Together, our processor achieves 86.2% of the performance of a traditional out-of-order processor, higher than previous efficient scheduler proposals, while still consuming 30% less power.", "venue": "ArXiv", "authors": ["Andreas  Diavastos", "Trevor E. Carlson"], "year": 2021, "n_citations": 0}
{"id": 6147518, "s2_id": "21a2f344cecab0925a5d0332a9e9200e3a4badff", "title": "FPGA implementation of high speed reconfigurable filter bank for multi-standard wireless communication receivers", "abstract": "In next generation wireless communication system, wireless transceivers should be able to handle wideband input signals compromising of multiple communication standards. Such multi-standard wireless communication receivers (MWCRs) need filter bank to extract the desired signal of interest from wideband input spectrum and bring it to the baseband for further signal processing tasks such as spectrum sensing, modulation classification, demodulation etc. In MWCRs, rather any wireless receivers, modulated filter banks, such as Discrete Fourier Transform Filter Banks (DFTFB), are preferred due to their advantages such as lower area, delay and power requirements. To support multi-standard operation, reconfigurable DFTFB (RDFTFB) was proposed by integrating DFTFB with the coefficient decimation method. In this paper, an efficient high speed implementation of RDFTFB on Virtex-7 field programmable gate arrays (FPGA) has been proposed. The proposed approach minimizes the critical path delay between clocked registers thereby leading to significant improvement in the maximum operating frequency of the RDFTFB. Numerically, the proposed implementation leads to 89.7% improvement in the maximum frequency at which RDFTFB can be clocked. Furthermore, proposed implementation leads to 18.5% reduction in the dynamic power consumption.", "venue": "2016 20th International Symposium on VLSI Design and Test (VDAT)", "authors": ["Sasha  Garg", "Sumit Jagdish Darak"], "year": 2016, "n_citations": 3}
{"id": 6150864, "s2_id": "1f57d5a73f3eedfd699a47312528d1c2b2dff566", "title": "Design of adiabatic MTJ-CMOS hybrid circuits", "abstract": "Low-power designs are a necessity with the increasing demand of portable devices which are battery operated. In many of such devices the operational speed is not as important as battery life. Logic-in-memory structures using nano-devices and adiabatic designs are two methods to reduce the static and dynamic power consumption respectively. Magnetic tunnel junction (MTJ) is an emerging technology which has many advantages when used in logic-in-memory structures in conjunction with CMOS. In this paper, we introduce a novel adiabatic hybrid MTJ/CMOS structure which is used to design AND/NAND, XOR/XNOR and 1-bit full adder circuits. We simulate the designs using HSPICE with 32nm CMOS technology and compared it with a non-adiabatic hybrid MTJ/CMOS circuits. The proposed adiabatic MTJ/CMOS full adder design has more than 7 times lower power consumtion compared to the previous MTJ/CMOS full adder.", "venue": "2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["Fazel  Sharifi", "Z. M. Saifullah", "Abdel-Hameed A. Badawy"], "year": 2017, "n_citations": 1}
{"id": 6151750, "s2_id": "6c688369a155c72bcfb154e5b4d219bb09dcbf1b", "title": "An Artificial Neural Networks based Temperature Prediction Framework for Network-on-Chip based Multicore Platform", "abstract": "Continuous improvement in silicon process technologies has made possible the integration of hundreds of cores on a single chip. However, power and heat have become dominant constraints in designing these massive multicore chips causing issues with reliability, timing variations and reduced lifetime of the chips. Dynamic Thermal Management (DTM) is a solution to avoid high temperatures on the die. Typical DTM schemes only address core level thermal issues. However, the Network-on-chip (NoC) paradigm, which has emerged as an enabling methodology for integrating hundreds to thousands of cores on the same die can contribute significantly to the thermal issues. Moreover, the typical DTM is triggered reactively based on temperature measurements from on-chip thermal sensor requiring long reaction times whereas predictive DTM method estimates future temperature in advance, eliminating the chance of temperature overshoot. Artificial Neural Networks (ANNs) have been used in various domains for modeling and prediction with high accuracy due to its ability to learn and adapt. This thesis concentrates on designing an ANN prediction engine to predict the thermal profile of the cores and Network-on-Chip elements of the chip. This thermal profile of the chip is then used by the predictive DTM that combines both core level and network level DTM techniques. On-chip wireless interconnect which is recently envisioned to enable energy-efficient data exchange between cores in a multicore environment, will be used to provide a broadcast-capable medium to efficiently distribute thermal control messages to trigger and manage the DTM schemes.", "venue": "ArXiv", "authors": ["Sandeep Aswath Narayana"], "year": 2016, "n_citations": 4}
{"id": 6153681, "s2_id": "909a6813a5cafa96c3cff6239fa71d053757051f", "title": "A geographic directed preferential Internet topology model", "abstract": "The goal of this work is to model the peering arrangements between autonomous systems (ASes). Most existing models of the AS-graph assume an undirected graph. However, peering arrangements are mostly asymmetric customer-provider arrangements, which are better modeled as directed edges. Furthermore, it is well known that the AS-graph, and in particular its clustering structure, is influenced by geography. We introduce a new model that describes the AS-graph as a directed graph, with an edge going from the customer to the provider, but also models symmetric peer-to-peer arrangements. In addition, our model takes geography into account. We are able to mathematically analyze its power-law exponent and number of leaves. Beyond the analysis, we have implemented our model as a synthetic network generator called GDNG. Experimentation with GDNG shows that the networks it produces are more realistic than those generated by other network generators, in terms of its power-law exponent, fractions of customer-provider and symmetric peering arrangements, and the size of its dense core. We believe that our model is the first to manifest realistic regional dense cores that have a clear geographic flavor. Our synthetic networks also exhibit path inflation effects that are similar to those observed in the real AS graph.", "venue": "13th IEEE International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems", "authors": ["Sagy  Bar", "Mira  Gonen", "Avishai  Wool"], "year": 2005, "n_citations": 38}
{"id": 6153771, "s2_id": "7ce0f44a286d430d11196cd72a790de9a76c753e", "title": "Multiplierless MP-Kernel Machine For Energy-efficient Edge Devices", "abstract": "We present a novel framework for designing multiplierless kernel machines that can be used on resourceconstrained platforms like intelligent edge devices. The framework uses a piecewise linear (PWL) approximation based on a margin propagation (MP) technique and uses only addition/subtraction, shift, comparison, and register underflow/overflow operations. We propose a hardware-friendly MPbased inference and online training algorithm that has been optimized for a Field Programmable Gate Array (FPGA) platform. Our FPGA implementation eliminates the need for DSP units and reduces the number of LUTs. By reusing the same hardware for inference and training, we show that the platform can overcome classification errors and local minima artifacts that result from the MP approximation. Using the FPGA platform, we also show that the proposed multiplierless MP-kernel machine demonstrates superior performance in terms of power, performance, and area compared to other comparable implementations.", "venue": "ArXiv", "authors": ["Abhishek Ramdas Nair", "Pallab Kumar Nath", "Shantanu  Chakrabartty", "Chetan Singh Thakur"], "year": 2021, "n_citations": 0}
{"id": 6153982, "s2_id": "aaa0aad7de6bbd7d6559f1f17ba6e40c2445ea26", "title": "Leaked-Web: Accurate and Efficient Machine Learning-Based Website Fingerprinting Attack through Hardware Performance Counters", "abstract": "Users\u2019 website browsing history contains sensitive information, like health conditions, political interests, financial situations, etc. In order to cope with the potential website behavior leakage and enhance the browsing security, some defense mechanisms such as SSH tunnels and anonymity networks (e.g., Tor) have been proposed. Nevertheless, some recent studies have demonstrated the possibility of inferring website fingerprints based on important usage information such as traffic, cache usage, memory usage, CPU activity, power consumption, and hardware performance counters information. However, existing website fingerprinting attacks demand high sampling rate which causes high performance overheads and large network traffic, and/or they require launching an additional malicious website by the user which is not guaranteed. As a result, such drawbacks make the existing attacks more noticeable to users and corresponding fingerprinting detection mechanisms. In response, in this work we propose Leaked-Web, a novel accurate and efficient machine learning-based website fingerprinting attack through processor\u2019s Hardware Performance Counters (HPCs). Leaked-Web efficiently collects hardware performance counters in users\u2019 computer system at a significantly low granularity monitoring rate and sends the samples to the remote attack\u2019s server for further classification. Leaked-Web examines the web browsers\u2019 microarchitectural features using various advanced machine learning algorithms ranging from classical, boosting, deep learning, and time-series models. Our experimental results indicate that Leaked-Web based on a LogitBoost ML classifier using only the top 4 HPC features achieves 91% classification accuracy outperforming the state-of-the-art attacks by nearly 5%. Furthermore, our proposed attack obtains a negligible performance overhead (only <1%) which is around 12% lower than the existing hardware-assisted website fingerprinting attacks.", "venue": "ArXiv", "authors": ["Han  Wang"], "year": 2021, "n_citations": 0}
{"id": 6154212, "s2_id": "09e0624775766f9300a09eff2fc1a8efc8a6ee62", "title": "HALCONE : A Hardware-Level Timestamp-based Cache Coherence Scheme for Multi-GPU systems", "abstract": "While multi-GPU (MGPU) systems are extremely popular for compute-intensive workloads, several inefficiencies in the memory hierarchy and data movement result in a waste of GPU resources and difficulties in programming MGPU systems. First, due to the lack of hardware-level coherence, the MGPU programming model requires the programmer to replicate and repeatedly transfer data between the GPUs' memory. This leads to inefficient use of precious GPU memory. Second, to maintain coherency across an MGPU system, transferring data using low-bandwidth and high-latency off-chip links leads to degradation in system performance. Third, since the programmer needs to manually maintain data coherence, the programming of an MGPU system to maximize its throughput is extremely challenging. To address the above issues, we propose a novel lightweight timestamp-based coherence protocol, HALCONE, for MGPU systems and modify the memory hierarchy of the GPUs to support physically shared memory. HALCONE replaces the Compute Unit (CU) level logical time counters with cache level logical time counters to reduce coherence traffic. Furthermore, HALCONE introduces a novel timestamp storage unit (TSU) with no additional performance overhead in the main memory to perform coherence actions. Our proposed HALCONE protocol maintains the data coherence in the memory hierarchy of the MGPU with minimal performance overhead (less than 1\\%). Using a set of standard MGPU benchmarks, we observe that a 4-GPU MGPU system with shared memory and HALCONE performs, on average, 4.6$\\times$ and 3$\\times$ better than a 4-GPU MGPU system with existing RDMA and with the recently proposed HMG coherence protocol, respectively. We demonstrate the scalability of HALCONE using different GPU counts (2, 4, 8, and 16) and different CU counts (32, 48, and 64 CUs per GPU) for 11 standard benchmarks.", "venue": "ArXiv", "authors": ["Saiful A. Mojumder", "Yifan  Sun", "Leila  Delshadtehrani", "Yenai  Ma", "Trinayan  Baruah", "Jos\u00e9 L. Abell\u00e1n", "John  Kim", "David  Kaeli", "Ajay  Joshi"], "year": 2020, "n_citations": 2}
{"id": 6155558, "s2_id": "4f5e5dd7ba1e46f46ed3a07050c111088eaeb4f2", "title": "NaNet: a Low-Latency, Real-Time, Multi-Standard Network Interface Card with GPUDirect Features", "abstract": "While the GPGPU paradigm is widely recognized as an effective approach to high performance computing, its adoption in low-latency, real-time systems is still in its early stages. \nAlthough GPUs typically show deterministic behaviour in terms of latency in executing computational kernels as soon as data is available in their internal memories, assessment of real-time features of a standard GPGPU system needs careful characterization of all subsystems along data stream path. \nThe networking subsystem results in being the most critical one in terms of absolute value and fluctuations of its response latency. \nOur envisioned solution to this issue is NaNet, a FPGA-based PCIe Network Interface Card (NIC) design featuring a configurable and extensible set of network channels with direct access through GPUDirect to NVIDIA Fermi/Kepler GPU memories. \nNaNet design currently supports both standard - GbE (1000BASE-T) and 10GbE (10Base-R) - and custom - 34~Gbps APElink and 2.5~Gbps deterministic latency KM3link - channels, but its modularity allows for a straightforward inclusion of other link technologies. \nTo avoid host OS intervention on data stream and remove a possible source of jitter, the design includes a network/transport layer offload module with cycle-accurate, upper-bound latency, supporting UDP, KM3link Time Division Multiplexing and APElink protocols. \nAfter NaNet architecture description and its latency/bandwidth characterization for all supported links, two real world use cases will be presented: the GPU-based low level trigger for the RICH detector in the NA62 experiment at CERN and the on-/off-shore data link for KM3 underwater neutrino telescope.", "venue": "ArXiv", "authors": ["Alessandro  Lonardo", "F.  Ameli", "Roberto  Ammendola", "Andrea  Biagioni", "Ottorino  Frezza", "G.  Lamanna", "Francesca Lo Cicero", "Maurizio  Martinelli", "Pier Stanislao Paolucci", "Elena  Pastorelli", "Luca  Pontisso", "Davide  Rossetti", "F.  Simeone", "Francesco  Simula", "M.  Sozzi", "Laura  Tosoratto", "Piero  Vicini"], "year": 2014, "n_citations": 0}
{"id": 6155931, "s2_id": "6a604b8c1ac40fb2b8f7597b7aea8cb13fb32381", "title": "A Dual Digital Signal Processor VME Board For Instrumentation And Control Applications", "abstract": "A Dual Digital Signal Processing VME Board was developed for the Continuous Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at Jefferson Lab. It is a versatile general-purpose digital signal processing board using an open architecture, which allows for adaptation to various applications. The base design uses two independent Texas Instrument (TI) TMS320C6711, which are 900 MFLOPS floating-point digital signal processors (DSP). Applications that require a fixed point DSP can be implemented by replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The design can be manufactured with a reduced chip set without redesigning the printed circuit board. For example it can be implemented as a single-channel DSP with no analog I/O.", "venue": "ArXiv", "authors": ["Hai  Dong", "Roger  Flood", "C.  Hovater", "J.  Musson"], "year": 2001, "n_citations": 4}
{"id": 6159316, "s2_id": "381a957155b2661bdb450cf670c6886295acf5ad", "title": "Energy-efficient wireless interconnection framework for multichip systems with in-package memory stacks", "abstract": "Multichip systems with memory stacks and various processing chips are at the heart of platform based designs such as servers and embedded systems. Full utilization of the benefits of these integrated multichip systems need a seamless, and scalable in-package interconnection framework. However, state-of-the-art inter-chip communication requires long wireline channels which increases energy consumption and latency while decreasing data bandwidth. Here, we propose the design of an energy-efficient, seamless wireless interconnection network for multichip systems. We demonstrate with cycle-accurate simulations that such a design reduces the energy consumption and latency while increasing the bandwidth in comparison to modern multichip integration systems.", "venue": "2017 30th IEEE International System-on-Chip Conference (SOCC)", "authors": ["Md Shahriar Shamim", "M. Meraj Ahmed", "Naseef  Mansoor", "Amlan  Ganguly"], "year": 2017, "n_citations": 7}
{"id": 6166342, "s2_id": "b51426e91f1192de8b7c06609b53ed1f24c7f506", "title": "Isadora: Automated Information Flow Property Generation for Hardware Designs", "abstract": "Isadora is a methodology for creating information flow specifications of hardware designs. The methodology combines information flow tracking and specification mining to produce a set of information flow properties that are suitable for use during the security validation process, and which support a better understanding of the security posture of the design. Isadora is fully automated; the user provides only the design under consideration and a testbench and need not supply a threat model nor security specifications. We evaluate Isadora on a RISC-V processor plus two designs related to SoC access control. Isadora generates security properties that align with those suggested by the Common Weakness Enumerations (CWEs), and in the case of the SoC designs, align with the properties written manually by security experts.", "venue": "ASHES@CCS", "authors": ["Calvin  Deutschbein", "Andres  Meza", "Francesco  Restuccia", "Ryan  Kastner", "Cynthia  Sturton"], "year": 2021, "n_citations": 0}
{"id": 6177950, "s2_id": "402b6eed11b6c8718b4ae319703caf93d62b68bf", "title": "uops.info: Characterizing Latency, Throughput, and Port Usage of Instructions on Intel Microarchitectures", "abstract": "Modern microarchitectures are some of the world's most complex man-made systems. As a consequence, it is increasingly difficult to predict, explain, let alone optimize the performance of software running on such microarchitectures. As a basis for performance predictions and optimizations, we would need faithful models of their behavior, which are, unfortunately, seldom available. In this paper, we present the design and implementation of a tool to construct faithful models of the latency, throughput, and port usage of x86 instructions. To this end, we first discuss common notions of instruction throughput and port usage, and introduce a more precise definition of latency that, in contrast to previous definitions, considers dependencies between different pairs of input and output operands. We then develop novel algorithms to infer the latency, throughput, and port usage based on automatically-generated microbenchmarks that are more accurate and precise than existing work. To facilitate the rapid construction of optimizing compilers and tools for performance prediction, the output of our tool is provided in a machine-readable format. We provide experimental results for processors of all generations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake, and discuss various cases where the output of our tool differs considerably from prior work.", "venue": "ASPLOS", "authors": ["Andreas  Abel", "Jan  Reineke"], "year": 2019, "n_citations": 29}
{"id": 6184616, "s2_id": "a7487a7a2e92a740bad790eb986087f0f5cd694c", "title": "DPU: DAG Processing Unit for Irregular Graphs with Precision-Scalable Posit Arithmetic in 28nm", "abstract": "Computation in several real-world applications like probabilistic machine learning, sparse linear algebra, and robotic navigation, can be modeled as irregular directed acyclic graphs (DAGs). The irregular data dependencies in DAGs pose challenges to parallel execution on general-purpose CPUs and GPUs, resulting in severe under-utilization of the hardware. This paper proposes DPU, a specialized processor designed for the efficient execution of irregular DAGs. The DPU is equipped with parallel compute units that execute different subgraphs of a DAG independently. The compute units can synchronize within a cycle using a hardware-supported synchronization primitive, and communicate via an efficient interconnect to a global banked scratchpad. Furthermore, a precision-scalable positTM arithmetic unit is developed to enable application-dependent precision. The DPU is taped-out in 28nm CMOS, achieving a speedup of 5.1\u00d7 and 20.6\u00d7 over state-of-the-art CPU and GPU implementations on DAGs of sparse linear algebra and probabilistic machine learning workloads. This performance is achieved while operating at a power budget of 0.23W, as opposed to 55W and 98W of the CPU and GPU, resulting in a peak efficiency of 538 GOPS/W with DPU, which is 1350\u00d7 and 9000\u00d7 higher than the CPU and GPU, respectively. Thus, with specialized architecture, DPU enables low-power execution of irregular DAG workloads.", "venue": "IEEE Journal of Solid-State Circuits", "authors": ["Nimish  Shah", "Laura Isabel Galindez Olascoaga", "Shirui  Zhao", "Wannes  Meert", "Marian  Verhelst"], "year": 2021, "n_citations": 0}
{"id": 6185916, "s2_id": "b5ca992907cd1f81bea60aa041d6a49373655237", "title": "Collaborative storage management in sensor networks", "abstract": "In this paper, we consider a class of sensor networks where the data is not required in real-time by an observer; for example: a sensor network monitoring a scientific phenomenon for later play back and analysis. In such networks, the data must be stored in the network. Thus, in addition to battery power, storage is a primary resource; the useful lifetime of the network is constrained by its ability to store the generated data samples. We explore the use of collaborative storage techniques to efficiently manage data in storage constrained sensor networks. The proposed collaborative storage technique takes advantage of spatial correlation among the data collected by nearby sensors to significantly reduce the size of the data near the data sources. In addition, local coordination can be used to adjust the sampling rate to match the required application fidelity. We show that the proposed approach provides significant savings in the size of the stored data vs. local buffering. These savings allow the network to operate for a longer time without exhausting storage space. Furthermore, the savings reduce the amount of data that will eventually be relayed in response to queries or upon eventual collection of the data. In addition, collaborative storage performs load balancing of the available storage space if data generation rates are not uniform across sensors (as would be the case in an event driven sensor network), or if the available storage varies across the network.", "venue": "Int. J. Ad Hoc Ubiquitous Comput.", "authors": ["Sameer  Tilak", "Nael B. Abu-Ghazaleh", "Wendi B. Heinzelman"], "year": 2005, "n_citations": 30}
{"id": 6187442, "s2_id": "f4c8a4c1cf58b5a61f419a1987338c364f76833d", "title": "VLSI Implementation of Cryptographic Algorithms & Techniques: A Literature Review", "abstract": "Through the years, the flow of Data and its transmission have increased tremendously and so has the security issues to it. Cryptography in recent years with the advancement of VLSI has led to its implementation of Encryption and Decryption techniques, where the process of translating and converting plaintext into cypher text and vice versa is made possible. In this paper, the review of various aspects of VLSI's implementation of encryption and decryption are covered. To systemize the material, the information about topics such as Private Key Encryption, Index Technique, Blowfish Algorithm, DNA cryptography, and many more are reviewed. Ultimately, with this review, the basic understanding of different VLSI techniques of Encryption and Decryption can be studied and", "venue": "ArXiv", "authors": ["Favin  Fernandes", "Gauravi  Dungarwal", "Aishwariya  Gaikwad", "Ishan  Kareliya", "Swati  Shilaskar"], "year": 2021, "n_citations": 0}
{"id": 6192851, "s2_id": "d33ee960e6f5f0959ed4bcd176d364fa99bab961", "title": "Modified Micropipline Architecture for Synthesizable Asynchronous FIR Filter Design", "abstract": "The use of asynchronous design approaches to construct digital signal processing (DSP) systems is a rapidly growing research area driven by a wide range of emerging energy constrained applications such as wireless sensor network, portable medical devices and brain implants. The asynchronous design techniques allow the construction of systems which are samples driven, which means they only dissipate dynamic energy when there processing data and idle otherwise. This inherent advantage of asynchronous design over conventional synchronous circuits allows them to be energy efficient. However the implementation flow of asynchronous systems is still difficult due to its lack of compatibility with industry-standard synchronous design tools and modelling languages. This paper devises a novel asynchronous design for a finite impulse response (FIR) filter, an essential building block of DSP systems, which is synthesizable and suitable for implementation using conventional synchronous systems design flow and tools. The proposed design is based on a modified version of the micropipline architecture and it is constructed using four phase bundled data protocol. A hardware prototype of the proposed filter has been developed on an FPGA, and systematically verified. The results prove correct functionality of the novel design and a superior performance compared to a synchronous FIR implementation. The findings of this work will allow a wider adoption of asynchronous circuits by DSP designers to harness their energy and performance benefits.", "venue": "ArXiv", "authors": ["Basel  Halak", "Hsien-Chih  Chiu"], "year": 2016, "n_citations": 1}
{"id": 6193783, "s2_id": "4cd454d5c68652e9cd9a8ee072534331348ea2f0", "title": "Memory Slices: A Modular Building Block for Scalable, Intelligent Memory Systems", "abstract": "While reduction in feature size makes computation cheaper in terms of latency, area, and power consumption, performance of emerging data-intensive applications is determined by data movement. These trends have introduced the concept of scalability as reaching a desirable performance per unit cost by using as few number of units as possible. Many proposals have moved compute closer to the memory. However, these efforts ignored maintaining a balance between bandwidth and compute rate of an architecture, with those of applications, which is a key principle in designing scalable large systems. This paper proposes the use of memory slices, a modular building block for scalable memory systems integrated with compute, in which performance scales with memory size (and volume of data). The slice architecture utilizes a programmable memory interface feeding a systolic compute engine with high reuse rate. The modularity feature of slice-based systems is exploited with a partitioning and data mapping strategy across allocated memory slices where training performance scales with the data size. These features enable shifting the most pressure to cheap compute units rather than expensive memory accesses or transfers via interconnection network. An application of the memory slices to a scale-out memory system is accelerating the training of recurrent, convolutional, and hybrid neural networks (RNNs and RNNs+CNN) that are forming cloud workloads. The results of our cycle-level simulations show that memory slices exhibits a superlinear speedup when the number of slices increases. Furthermore, memory slices improve power efficiency to 747 GFLOPs/J for training LSTMs. While our current evaluation uses memory slices with 3D packaging, a major value is that slices can also be constructed with a variety of packaging options, for example with DDR-based memory units.", "venue": "ArXiv", "authors": ["Bahar  Asgari", "Saibal  Mukhopadhyay", "Sudhakar  Yalamanchili"], "year": 2018, "n_citations": 1}
{"id": 6199633, "s2_id": "d2610e4f4969c04f1673f3c50b3eabe1abebfbd3", "title": "Exploiting Data Longevity for Enhancing the Lifetime of Flash-based Storage Class Memory", "abstract": "Storage-class memory (SCM) combines the benefits of a solid-state memory, such as high-performance and robustness, with the archival capabilities and low cost of conventional hard-disk magnetic storage. Among candidate solid-state nonvolatile memory technologies that could potentially be used to construct SCM, flash memory is a well-established technology and have been widely used in commercially available SCM incarnations. Flash-based SCM enables much better tradeoffs between performance, space and power than disk-based systems. However, write endurance is a significant challenge for a flash-based SCM (each act of writing a bit may slightly damage a cell, so one flash cell can be written 10^4-10^5 times, depending on the flash technology, before it becomes unusable). This is a well-documented problem and has received a lot of attention by manufactures that are using some combination of write reduction and wear-leveling techniques for achieving longer lifetime. In an effort to improve flash lifetime, first, by quantifying data longevity in an SCM, we show that a majority of the data stored in a solid-state SCM do not require long retention times provided by flash memory (i.e., up to 10 years in modern devices); second, by exploiting retention time relaxation, we propose a novel mechanism, called Dense-SLC (D-SLC), which enables us perform multiple writes into a cell during each erase cycle for lifetime extension; and finally, we discuss the required changes in the flash management software (FTL) in order to use D-SLC mechanism for extending the lifetime of the solid-state part of an SCM. Using an extensive simulation-based analysis of an SLC flash-based SCM, we demonstrate that D-SLC is able to significantly improve device lifetime (between 5.1X and 8.6X) with no performance overhead and also very small changes at the FTL software.", "venue": "Proc. ACM Meas. Anal. Comput. Syst.", "authors": ["Wonil  Choi", "Mohammad  Arjomand", "Myoungsoo  Jung", "Mahmut T. Kandemir"], "year": 2017, "n_citations": 3}
{"id": 6200243, "s2_id": "2a01b9536b048dee97de03963115efa47959d827", "title": "45-year CPU evolution: one law and two equations", "abstract": "Moore's law and two equations allow to explain the main trends of CPU evolution since MOS technologies have been used to implement microprocessors.", "venue": "ArXiv", "authors": ["Daniel  Etiemble"], "year": 2018, "n_citations": 6}
{"id": 6203453, "s2_id": "6f651d5f2caeb84bb22f356f78e010b4634751a8", "title": "TiM-DNN: Ternary In-Memory Accelerator for Deep Neural Networks", "abstract": "The use of lower precision has emerged as a popular technique to optimize the compute and storage requirements of complex deep neural networks (DNNs). In the quest for lower precision, recent studies have shown that ternary DNNs (which represent weights and activations by signed ternary values) represent a promising sweet spot, achieving accuracy close to full-precision networks on complex tasks. We propose TiM-DNN, a programmable in-memory accelerator that is specifically designed to execute ternary DNNs. TiM-DNN supports various ternary representations including unweighted {\u22121, 0, 1}, symmetric weighted <inline-formula> <tex-math notation=\"LaTeX\">$\\{-a,0,a\\}$ </tex-math></inline-formula>, and asymmetric weighted <inline-formula> <tex-math notation=\"LaTeX\">$\\{-a,0,b\\}$ </tex-math></inline-formula> ternary systems. The building blocks of TiM-DNN are TiM tiles\u2014specialized memory arrays that perform massively parallel signed ternary vector\u2013matrix multiplications with a single access. TiM tiles are in turn composed of ternary processing cells (TPCs), bit-cells that function as both ternary storage units and signed ternary multiplication units. We evaluate an implementation of TiM-DNN in 32-nm technology using an architectural simulator calibrated with SPICE simulations and RTL synthesis. We evaluate TiM-DNN across a suite of state-of-the-art DNN benchmarks including both deep convolutional and recurrent neural networks. A 32-tile instance of TiM-DNN achieves a peak performance of 114 TOPs/s, consumes 0.9-W power, and occupies 1.96 mm<sup>2</sup> chip area, representing a <inline-formula> <tex-math notation=\"LaTeX\">$300\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$388\\times $ </tex-math></inline-formula> improvement in TOPS/W and TOPS/mm<sup>2</sup>, respectively, compared to an NVIDIA Tesla V100 GPU. In comparison to specialized DNN accelerators, TiM-DNN achieves <inline-formula> <tex-math notation=\"LaTeX\">$55\\times $ </tex-math></inline-formula>-<inline-formula> <tex-math notation=\"LaTeX\">$240\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$160\\times $ </tex-math></inline-formula>-<inline-formula> <tex-math notation=\"LaTeX\">$291\\times $ </tex-math></inline-formula> improvement in TOPS/W and TOPS/mm<sup>2</sup>, respectively. Finally, when compared to a well-optimized near-memory accelerator for ternary DNNs, TiM-DNN demonstrates <inline-formula> <tex-math notation=\"LaTeX\">$3.9\\times $ </tex-math></inline-formula>-<inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> improvement in system-level energy and <inline-formula> <tex-math notation=\"LaTeX\">$3.2\\times $ </tex-math></inline-formula>-<inline-formula> <tex-math notation=\"LaTeX\">$4.2\\times $ </tex-math></inline-formula> speedup, underscoring the potential of in-memory computing for ternary DNNs.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Shubham  Jain", "Sumeet Kumar Gupta", "Anand  Raghunathan"], "year": 2020, "n_citations": 6}
{"id": 6206118, "s2_id": "158255ad941939d480fadfa5599b7c90e0394de6", "title": "On the Difficulty of Designing Processor Arrays for Deep Neural Networks", "abstract": "Systolic arrays are a promising computing concept which is in particular inline with CMOS technology trends and linear algebra operations found in the processing of artificial neural networks. The recent success of such deep learning methods in a wide set of applications has led to a variety of models, which albeit conceptual similar as based on convolutions and fully-connected layers, in detail show a huge diversity in operations due to a large design space: An operand's dimension varies substantially since it depends on design principles such as receptive field size, number of features, striding, dilating and grouping of features. Last, recent networks extent previously plain feedforward models by various connectivity, such as in ResNet or DenseNet. The problem of choosing an optimal systolic array configuration cannot be solved analytically, thus instead methods and tools are required that facilitate a fast and accurate reasoning about optimality in terms of total cycles, utilization, and amount of data movements. In this work we introduce Camuy, a lightweight model of a weight-stationary systolic array for linear algebra operations that allows quick explorations of different configurations, such as systolic array dimensions and input/output bitwidths. Camuy aids accelerator designers in either finding optimal configurations for a particular network architecture or for robust performance across a variety of network architectures. It offers simple integration into existing machine learning tool stacks (e.g TensorFlow) through custom operators. We present an analysis of popular DNN models to illustrate how it can estimate required cycles, data movement costs, as well as systolic array utilization, and show how the progress in network architecture design impacts the efficiency of inference on accelerators based on systolic arrays.", "venue": "IoT Streams/ITEM@PKDD/ECML", "authors": ["Kevin  Stehle", "G\u00fcnther  Schindler", "Holger  Fr\u00f6ning"], "year": 2020, "n_citations": 0}
{"id": 6206369, "s2_id": "c5f671b8f04d272df66f4bba25680dabc3f7cfed", "title": "Continual Learning Approach for Improving the Data and Computation Mapping in Near-Memory Processing System", "abstract": "1. ABSTRACT The resurgence of near-memory processing (NMP) with the advent of big data has shifted the computation paradigm from processor-centric to memory-centric computing. To meet the bandwidth and capacity demands of memory-centric computing, 3D memory has been adopted to form a scalable memory-cube network. Along with NMP and memory system development, the mapping for placing data and guiding computation in the memory-cube network has become crucial in driving the performance improvement in NMP. However, it is very challenging to design a universal optimal mapping for all applications due to unique application behavior and intractable decision space. In this paper, we propose an artificially intelligent memory mapping scheme, AIMM, that optimizes data placement and resource utilization through page and computation remapping. Our proposed technique involves continuously evaluating and learning the impact of mapping decisions on system performance for any application. AIMM uses a neural network to achieve a near-optimal mapping during execution, trained using a reinforcement learning algorithm that is known to be effective for exploring a vast design space. We also provide a detailed AIMM hardware design that can be adopted as a plugin module for various NMP systems. Our experimental evaluation shows that AIMM improves the baseline NMP performance in single and multiple program scenario by up to 70% and 50%, respectively.", "venue": "ArXiv", "authors": ["Pritam  Majumder", "Jiayi  Huang", "Sungkeun  Kim", "Abdullah  Muzahid", "Dylan  Siegers", "Chia-Che  Tsai", "Eun Jung Kim"], "year": 2021, "n_citations": 0}
{"id": 6211765, "s2_id": "5de5fbe940f95e385b79982fdcbd74e9d3f72340", "title": "A Scalable Decoder Micro-architecture for Fault-Tolerant Quantum Computing", "abstract": "Quantum computation promises significant computational advantages over classical computation for some problems. However, quantum hardware suffers from much higher error rates than in classical hardware. As a result, extensive quantum error correction is required to execute a useful quantum algorithm. The decoder is a key component of the error correction scheme whose role is to identify errors faster than they accumulate in the quantum computer and that must be implemented with minimum hardware resources in order to scale to the regime of practical applications. In this work, we consider surface code error correction, which is the most popular family of error correcting codes for quantum computing, and we design a decoder micro-architecture for the Union-Find decoding algorithm. We propose a three-stage fully pipelined hardware implementation of the decoder that significantly speeds up the decoder. Then, we optimize the amount of decoding hardware required to perform error correction simultaneously over all the logical qubits of the quantum computer. By sharing resources between logical qubits, we obtain a 67% reduction of the number of hardware units and the memory capacity is reduced by 70%. Moreover, we reduce the bandwidth required for the decoding process by a factor at least 30x using low-overhead compression algorithms. Finally, we provide numerical evidence that our optimized micro-architecture can be executed fast enough to correct errors in a quantum computer.", "venue": "ArXiv", "authors": ["Poulami  Das", "Christopher A. Pattison", "Srilatha  Manne", "Douglas  Carmean", "Krysta  Svore", "Moinuddin  Qureshi", "Nicolas  Delfosse"], "year": 2020, "n_citations": 14}
{"id": 6212762, "s2_id": "3416758841aefb9b7e9b9d1857c893b512c5897a", "title": "Application-aware Retiming of Accelerators: A High-level Data-driven Approach", "abstract": "Flexibility at hardware level is the main driving force behind adaptive systems whose aim is to realise microarhitecture deconfiguration 'online'. This feature allows the software/hardware stack to tolerate drastic changes of the workload in data centres. With emerge of FPGA reconfigurablity this technology is becoming a mainstream computing paradigm. Adaptivity is usually accompanied by the high-level tools to facilitate multi-dimensional space exploration. An essential aspect in this space is memory orchestration where on-chip and off-chip memory distribution significantly influences the architecture in coping with the critical spatial and timing constraints, e.g. Place and Route. This paper proposes a memory smart technique for a particular class of adaptive systems: Elastic Circuits which enjoy slack elasticity at fine level of granularity. We explore retiming of a set of popular benchmarks via investigating the memory distribution within and among accelerators. The area, performance and power patterns are adopted by our high-level synthesis framework, with respect to the behaviour of the input descriptions, to improve the quality of the synthesised elastic circuits.", "venue": "ArXiv", "authors": ["Ana  Lava", "Mahdi Jelodari Mamaghani", "Siamak  Mohammadi", "Steve B. Furber"], "year": 2016, "n_citations": 0}
{"id": 6213183, "s2_id": "4679a5b41a52b5d61d3008d7fa832d1427630a4a", "title": "FlexWatts: A Power- and Workload-Aware Hybrid Power Delivery Network for Energy-Efficient Microprocessors", "abstract": "Modern client processors typically use one of three commonly-used power delivery network (PDN) architectures: 1) motherboard voltage regulators (MBVR), 2) integrated voltage regulators (IVR), and 3) low dropout voltage regulators (LDO). We observe that the energy-efficiency of each of these PDNs varies with the processor power (e.g, thermal design power (TDP) and dynamic power-state) and workload characteristics (e.g., work-load type and computational intensity). This leads to energy-inefficiency and performance loss, as modern client processors operate across a wide spectrum of power consumption and execute a wide variety of workloads.To address this inefficiency, we propose FlexWatts, a hybrid adaptive PDN for modern client processors whose goal is to provide high energy-efficiency across the processor\u2019s wide range of power consumption and workloads. FlexWatts provides high energy-efficiency by intelligently and dynamically allocating PDNs to processor domains depending on the processor\u2019s power consumption and workload. FlexWatts is based on three key ideas. First, FlexWatts combines IVRs and LDOs in a novel way to share multiple on-chip and off-chip resources and thus reduce cost, as well as board and die area overheads. This hybrid PDN is allocated for processor domains with a wide power consumption range (e.g., CPU cores and graphics engines) and it dynamically switches between two modes: IVR-Mode and LDO-Mode, depending on the power consumption. Second, for all other processor domains (that have a low and narrow power range, e.g., the IO domain), FlexWatts statically allocates off-chip VRs, which have high energy-efficiency for low and narrow power ranges. Third, FlexWatts introduces a novel prediction algorithm that automatically switches the hybrid PDN to the mode (IVR-Mode or LDO-Mode) that is the most beneficial based on processor power consumption and workload characteristics.To evaluate the tradeoffs of PDNs, we develop and open-source PDNspot, the first validated architectural PDN model that enables quantitative analysis of PDN metrics. Using PDNspot, we evaluate FlexWatts on a wide variety of SPEC CPU2006, graphics (3DMark06), and battery life (e.g., video playback) workloads against IVR, the state-of-the-art PDN in modern client processors. For a 4 W thermal design power (TDP) processor, FlexWatts improves the average performance of the SPEC CPU2006 and 3DMark06 workloads by 22% and 25%, respectively. For battery life workloads, FlexWatts reduces the average power consumption of video playback by 11% across all tested TDPs (4W\u201350W). FlexWatts has comparable cost and area overhead to IVR. We conclude that FlexWatts provides high energy-efficiency across a modern client processor\u2019s wide range of power consumption and wide variety of workloads, with minimal overhead.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Jawad  Haj-Yahya", "Mohammed  Alser", "Jeremie S. Kim", "Lois  Orosa", "Efraim  Rotem", "Avi  Mendelson", "Anupam  Chattopadhyay", "Onur  Mutlu"], "year": 2020, "n_citations": 3}
{"id": 6214655, "s2_id": "bcf72453ef36e3f77d805fb30fa38c63ab7308bb", "title": "A Novel Reconfigurable Hardware Design for Speech Enhancement Based on Multi-Band Spectral Subtraction Involving Magnitude and Phase Components", "abstract": "This paper proposes an efficient reconfigurable hardware design for speech enhancement based on multi band spectral subtraction algorithm and involving both magnitude and phase components. Our proposed design is novel as it estimates environmental noise from speech adaptively utilizing both magnitude and phase components of the speech spectrum. We performed multi-band spectrum subtraction by dividing the noisy speech spectrum into different non-uniform frequency bands having varying signal to noise ratio (SNR) and subtracting the estimated noise from each of these frequency bands. This results to the elimination of noise from both high SNR and low SNR signal components for all the frequency bands. We have coined our proposed speech enhancement technique as Multi Band Magnitude Phase Spectral Subtraction (MBMPSS). The magnitude and phase operations are executed concurrently exploiting the parallel logic blocks of Field Programmable Gate Array (FPGA), thus increasing the throughput of the system to a great extent. We have implemented our design on Spartan6 Lx45 FPGA and presented the implementation result in terms of resource utilization and delay information for the different blocks of our design. To the best of our best knowledge, this is a new type of hardware design for speech enhancement application and also a first of its kind implementation on reconfigurable hardware. We have used benchmark audio data for the evaluation of the proposed hardware and the experimental results show that our hardware shows a better SNR value compared to the existing state of the art research works.", "venue": "ArXiv", "authors": ["Tanmay  Biswas", "Sudhindu Bikash Mandal", "Debasree  Saha", "Amlan  Chakrabarti"], "year": 2015, "n_citations": 2}
{"id": 6214885, "s2_id": "9e7b1b221fcf2c20f8cc79a7c56e5162d113e248", "title": "ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning", "abstract": "DNN accelerators provide efficiency by leveraging reuse of activations/weights/outputs during the DNN computations to reduce data movement from DRAM to the chip. The reuse is captured by the accelerator\u2019s dataflow. While there has been significant prior work in exploring and comparing various dataflows, the strategy for assigning on-chip hardware resources (i.e., compute and memory) given a dataflow that can optimize for performance/energy while meeting platform constraints of area/power for DNN(s) of interest is still relatively unexplored. The design-space of choices for balancing compute and memory explodes combinatorially, as we show in this work (e.g., as large as O(1072) choices for running MobileNet-V2), making it infeasible to do manual-tuning via exhaustive searches. It is also difficult to come up with a specific heuristic given that different DNNs and layer types exhibit different amounts of reuse.In this paper, we propose an autonomous strategy called Con-fuciuX to find optimized HW resource assignments for a given model and dataflow style. ConfuciuX leverages a reinforcement learning method, REINFORCE, to guide the search process, leveraging a detailed HW performance cost model within the training loop to estimate rewards. We also augment the RL approach with a genetic algorithm for further fine-tuning. Con-fuciuX demonstrates the highest sample-efficiency for training compared to other techniques such as Bayesian optimization, genetic algorithm, simulated annealing, and other RL methods. It converges to the optimized hardware configuration 4.7 to 24 times faster than alternate techniques.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Sheng-Chun  Kao", "Geonhwa  Jeong", "Tushar  Krishna"], "year": 2020, "n_citations": 15}
{"id": 6215068, "s2_id": "0fdc1a05a67631d61dd4e6cf1b513c02403bfcb5", "title": "NeuMMU: Architectural Support for Efficient Address Translations in Neural Processing Units", "abstract": "To satisfy the compute and memory demands of deep neural networks (DNNs), neural processing units (NPUs) are widely being utilized for accelerating DNNs. Similar to how GPUs have evolved from a slave device into a mainstream processor architecture, it is likely that NPUs will become first-class citizens in this fast-evolving heterogeneous architecture space. This paper makes a case for enabling address translation in NPUs to decouple the virtual and physical memory address space. Through a careful data-driven application characterization study, we root-cause several limitations of prior GPU-centric address translation schemes and propose a memory management unit (MMU) that is tailored for NPUs. Compared to an oracular MMU design point, our proposal incurs only an average 0.06% performance overhead.", "venue": "ASPLOS", "authors": ["Bongjoon  Hyun", "Youngeun  Kwon", "Yujeong  Choi", "John  Kim", "Minsoo  Rhu"], "year": 2020, "n_citations": 8}
{"id": 6219124, "s2_id": "c03fdf0f43f393e04743f9858d0950c28256560e", "title": "Feature extraction without learning in an analog spatial pooler memristive-CMOS circuit design of hierarchical temporal memory", "abstract": "Hierarchical temporal memory (HTM) is a neuromorphic algorithm that emulates sparsity, hierarchy and modularity resembling the working principles of neocortex. Feature encoding is an important step to create sparse binary patterns. This sparsity is introduced by the binary weights and random weight assignment in the initialization stage of the HTM. We propose the alternative deterministic method for the HTM initialization stage, which connects the HTM weights to the input data and preserves natural sparsity of the input information. Further, we introduce the hardware implementation of the deterministic approach and compare it to the traditional HTM and existing hardware implementation. We test the proposed approach on the face recognition problem and show that it outperforms the conventional HTM approach.", "venue": "ArXiv", "authors": ["Olga  Krestinskaya", "Alex Pappachen James"], "year": 2018, "n_citations": 20}
{"id": 6224416, "s2_id": "1bbcf06899f1d053b2fba248e69da7886674288b", "title": "Accelerating BLAS on Custom Architecture through Algorithm-Architecture Co-design", "abstract": "Basic Linear Algebra Subprograms (BLAS) play key role in high performance and scientific computing applications. Experimentally, yesteryear multicore and General Purpose Graphics Processing Units (GPGPUs) are capable of achieving up to 15 to 57% of the theoretical peak performance at 65W to 240W respectively for compute bound operations like Double/Single Precision General Matrix Multiplication (XGEMM). For bandwidth bound operations like Single/Double precision Matrix-vector Multiplication (XGEMV) the performance is merely 5 to 7% of the theoretical peak performance in multicores and GPGPUs respectively. Achieving performance in BLAS requires moving away from conventional wisdom and evolving towards customized accelerator tailored for BLAS through algorithm-architecture co-design. In this paper, we present acceleration of Level-1 (vector operations), Level-2 (matrix-vector operations), and Level-3 (matrix-matrix operations) BLAS through algorithm architecture co-design on a Coarse-grained Reconfigurable Architecture (CGRA). We choose REDEFINE CGRA as a platform for our experiments since REDEFINE can be adapted to support domain of interest through tailor-made Custom Function Units (CFUs). For efficient sequential realization of BLAS, we present design of a Processing Element (PE) and perform micro-architectural enhancements in the PE to achieve up-to 74% of the theoretical peak performance of PE in DGEMM, 40% in DGEMV and 20% in double precision inner product (DDOT). We attach this PE to REDEFINE CGRA as a CFU and show the scalability of our solution. Finally, we show performance improvement of 3-140x in PE over commercially available Intel micro-architectures, ClearSpeed CSX700, FPGA, and Nvidia GPGPUs.", "venue": "ArXiv", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan"], "year": 2016, "n_citations": 3}
{"id": 6231925, "s2_id": "7b556b349119c6d520b88151d6a700c604b79b0e", "title": "Trends in Processor Architecture", "abstract": "This chapter presents an overview of the main trends in processor architecture. It starts with an analysis of the past evolution of processors and the main driving forces behind it, and then it focuses on a description of the main architectural features of current processors. Finally, it presents a discussion on some promising directions for future evolution of processor architectures.", "venue": "Harnessing Performance Variability in Embedded and High-performance Many/Multi-core Platforms", "authors": ["Antonio  Gonzalez"], "year": 2018, "n_citations": 4}
{"id": 6234846, "s2_id": "b76b84218ac7740bb5ca9f15daf8865395a006f9", "title": "Hardware-efficient Residual Networks for FPGAs", "abstract": "Residual networks (ResNets) employ skip connections in their networks\u2014reusing activations from previous layers\u2014to improve training convergence, but these skip connections create challenges for hardware implementations of ResNets. The hardware must either wait for skip connections to be processed before processing more incoming data or buffer them elsewhere. Without skip connections, ResNets would be more hardware-efficient. Thus, we present the teacher-student learning method to gradually prune away all of a ResNet\u2019s skip connections, constructing a network we call NonResNet. We show that when implemented for FPGAs, NonResNet decreases ResNet\u2019s BRAM utilization by 9% and LUT utilization by 3% and increases throughput by 5%.", "venue": "ArXiv", "authors": ["Olivia  Weng", "Alireza  Khodamoradi", "Ryan  Kastner"], "year": 2021, "n_citations": 0}
{"id": 6236145, "s2_id": "33c7f478737068adbfc713f6d595f3f8d917e17d", "title": "Field-Programmable Crossbar Array (FPCA) for Reconfigurable Computing", "abstract": "For decades, advances in electronics were directly driven by the scaling of CMOS transistors according to Moore's law. However, both the CMOS scaling and the classical computer architecture are approaching fundamental and practical limits, and new computing architectures based on emerging devices, such as resistive random-access memory (RRAM) devices, are expected to sustain the exponential growth of computing capability. Here, we propose a novel memory-centric, reconfigurable, general purpose computing platform that is capable of handling the explosive amount of data in a fast and energy-efficient manner. The proposed computing architecture is based on a uniform, physical, resistive, memory-centric fabric that can be optimally reconfigured and utilized to perform different computing and data storage tasks in a massively parallel approach. The system can be tailored to achieve maximal energy efficiency based on the data flow by dynamically allocating the basic computing fabric for storage, arithmetic, and analog computing including neuromorphic computing tasks.", "venue": "IEEE Transactions on Multi-Scale Computing Systems", "authors": ["Mohammed A. Zidan", "YeonJoo  Jeong", "Jong Hoon Shin", "Chao  Du", "Zhengya  Zhang", "Wei D. Lu"], "year": 2018, "n_citations": 32}
{"id": 6239459, "s2_id": "0cebdff5e48e290e682739704f7da791c0742517", "title": "Nature-Inspired Interconnects for Self-Assembled Large-Scale Network-on-Chip Designs", "abstract": "Future nanoscale electronics built up from an Avogadro number of components need efficient, highly scalable, and robust means of communication in order to be competitive with traditional silicon approaches. In recent years, the networks-on-chip (NoC) paradigm emerged as a promising solution to interconnect challenges in silicon-based electronics. Current NoC architectures are either highly regular or fully customized, both of which represent implausible assumptions for emerging bottom-up self-assembled molecular electronics that are generally assumed to have a high degree of irregularity and imperfection. Here, we pragmatically and experimentally investigate important design tradeoffs and properties of an irregular, abstract, yet physically plausible three-dimensional (3D) small-world interconnect fabric that is inspired by modern network-on-chip paradigms. We vary the framework's key parameters, such as the connectivity, number of switch nodes, and distribution of long- versus short-range connections, and measure the network's relevant communication characteristics. We further explore the robustness against link failures and the ability and efficiency to solve a simple toy problem, the synchronization task. The results confirm that (1) computation in irregular assemblies is a promising and disruptive computing paradigm for self-assembled nanoscale electronics and (2) that 3D small-world interconnect fabrics with a power-law decaying distribution of shortcut lengths are physically plausible and have major advantages over local two-dimensional and 3D regular topologies.", "venue": "Chaos", "authors": ["Christof  Teuscher"], "year": 2007, "n_citations": 56}
{"id": 6240255, "s2_id": "58ffd627fd8d382d6deaff71fb1af905464dabc6", "title": "Bridging the Architecture Gap: Abstracting Performance-Relevant Properties of Modern Server Processors", "abstract": "We describe a universal modeling approach for predicting single- and multicore runtime of steady-state loops on server processors. To this end we strictly differentiate between application and machine models: An application model comprises the loop code, problem sizes, and other runtime parameters, while a machine model is an abstraction of all performance-relevant properties of a CPU. We introduce a generic method for determining machine models and present results for relevant server-processor architectures by Intel, AMD, IBM, and Marvell/Cavium. Considering this wide range of architectures, the set of features required for adequate performance modeling is surprisingly small. To validate our approach, we compare performance predictions to empirical data for an OpenMP-parallel preconditioned CG algorithm, which includes compute- and memory-bound kernels. Both single- and multicore analysis shows that the model exhibits average and maximum relative errors of 5% and 10%. Deviations from the model and insights gained are discussed in detail.", "venue": "Supercomput. Front. Innov.", "authors": ["Johannes  Hofmann", "Christie L. Alappat", "Georg  Hager", "Dietmar  Fey", "Gerhard  Wellein"], "year": 2020, "n_citations": 14}
{"id": 6242520, "s2_id": "8dd6c84dce582389df1c0b72471d7ab1e0b9f27b", "title": "Understanding Power Consumption and Reliability of High-Bandwidth Memory with Voltage Underscaling", "abstract": "Modern computing devices employ High-Bandwidth Memory (HBM) to meet their memory bandwidth requirements. An HBM-enabled device consists of multiple DRAM layers stacked on top of one another next to a compute chip (e.g, CPU, GPU, and FPGA) in the same package. Although such HBM structures provide high bandwidth at a small form factor, the stacked memory layers consume a substantial portion of the package's power budget. Therefore, power-saving techniques that preserve the performance of HBM are desirable. Undervolting is one such technique: it reduces the supply voltage to decrease power consumption without reducing the device's operating frequency to avoid performance loss. Undervolting takes advantage of voltage guardbands put in place by manufacturers to ensure correct operation under all environmental conditions. However, reducing voltage without changing frequency can lead to reliability issues manifested as unwanted bit flips. In this paper, we provide the first experimental study of real HBM chips under reduced-voltage conditions. We show that the guardband regions for our HBM chips constitute 19% of the nominal voltage. Pushing the supply voltage down within the guardband region reduces power consumption by a factor of 1.5X for all bandwidth utilization rates. Pushing the voltage down further by 11% leads to a total of 2.3X power savings at the cost of unwanted bit flips. We explore and characterize the rate and types of these reduced-voltage-induced bit flips and present a fault map that enables the possibility of a three-factor trade-off among power, memory capacity, and fault rate.", "venue": "2021 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "authors": ["Seyed Saber Nabavi Larimi", "Behzad  Salami", "Osman S. Unsal", "Adri\u00e1n  Cristal", "Hamid  Sarbazi-Azad", "Onur  Mutlu"], "year": 2021, "n_citations": 2}
{"id": 6243283, "s2_id": "98894fd9043b98c14104871c7d101f41b7fcf83b", "title": "Run-time reconfigurable multi-precision floating point multiplier design for high speed, low-power applications", "abstract": "Floating point multiplication is one of the crucial operations in many application domains such as image processing, signal processing etc. But every application requires different working features. Some need high precision, some need low power consumption, low latency etc. But IEEE-754 format is not really flexible for these specifications and also design is complex. Optimal run-time reconfigurable hardware implementations may need the use of custom floating-point formats that do not necessarily follow IEEE specified sizes. In this paper, we present a run-time-reconfigurable floating point multiplier implemented on FPGA with custom floating point format for different applications. This floating point multiplier can have 6 modes of operations depending on the accuracy or application requirement. With the use of optimal design with custom IPs (Intellectual Properties), a better implementation is done by truncating the inputs before multiplication. And a combination of Karatsuba algorithm and Urdhva-Tiryagbhyam algorithm (Vedic Mathematics) is used to implement unsigned binary multiplier. This further increases the efficiency of the multiplier.", "venue": "2015 2nd International Conference on Signal Processing and Integrated Networks (SPIN)", "authors": ["S.  Arish", "R. K. Sharma"], "year": 2015, "n_citations": 7}
{"id": 6246719, "s2_id": "ff699f5528f4f0f81ac7c03962a8c74886e0c029", "title": "An ECG-SoC with 535nW/channel lossless data compression for wearable sensors", "abstract": "This paper presents a low power ECG recording Sys-tem-on-Chip (SoC) with on-chip low complexity lossless ECG compression for data reduction in wireless/ambulatory ECG sensor devices. The proposed algorithm uses a linear slope predictor to estimate the ECG samples, and uses a novel low complexity dynamic coding-packaging scheme to frame the resulting estimation error into fixed-length 16-bit format. The proposed technique achieves an average compression ratio of 2.25\u00d7 on MIT/BIH ECG database. Implemented in 0.35 \u03bcm process, the compressor uses 0.565 K gates/channel occupying 0.4 mm2 for 4-channel, and consumes 535 nW/channel at 2.4V for ECG sampled at 512 Hz. Small size and ultra-low power consumption makes the proposed technique suitable for wearable ECG sensor application.", "venue": "2013 IEEE Asian Solid-State Circuits Conference (A-SSCC)", "authors": ["Chacko John Deepu"], "year": 2013, "n_citations": 20}
{"id": 6248517, "s2_id": "6e857bd1e68f07fcf8e1220c1b01223e55751f73", "title": "Memristor crossbar-based hardware implementation of fuzzy membership functions", "abstract": "In May 1, 2008, researchers at Hewlett Packard (HP) announced the first physical realization of a fundamental circuit element called memristor that attracted so much interest worldwide. This newly found element can easily be combined with crossbar interconnect technology which this new structure has opened a new field in designing configurable or programmable electronic systems. These systems in return can have applications in signal processing and artificial intelligence. In this paper, based on the simple memristor crossbar structure, we propose new and simple circuits for hardware implementation of fuzzy membership functions. In our proposed circuits, these fuzzy membership functions can have any shapes and resolutions. In addition, these circuits can be used as a basis for the construction of evolutionary systems.", "venue": "2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)", "authors": ["Farnood  Merrikh-Bayat", "Saeed Bagheri Shouraki", "Farshad  Merrikh-Bayat"], "year": 2011, "n_citations": 8}
{"id": 6255460, "s2_id": "0b2024d06a15b57e326b8e1e4ac56b2740f17d2c", "title": "Silent Data Corruptions at Scale", "abstract": "Silent Data Corruption (SDC) can have negative impact on largescale infrastructure services. SDCs are not captured by error reporting mechanisms within a Central Processing Unit (CPU) and hence are not traceable at the hardware level. However, the data corruptions propagate across the stack and manifest as applicationlevel problems. These types of errors can result in data loss and can require months of debug engineering time. In this paper, we describe common defect types observed in silicon manufacturing that leads to SDCs. We discuss a real-world example of silent data corruption within a datacenter application. We provide the debug flow followed to root-cause and triage faulty instructions within a CPU using a case study, as an illustration on how to debug this class of errors. We provide a high-level overview of themitigations to reduce the risk of silent data corruptions within a large production fleet. In our large-scale infrastructure, we have run a vast library of silent error test scenarios across hundreds of thousands of machines in our fleet. This has resulted in hundreds of CPUs detected for these errors, showing that SDCs are a systemic issue across generations. We have monitored SDCs for a period longer than 18 months. Based on this experience, we determine that reducing silent data corruptions requires not only hardware resiliency and production detection mechanisms, but also robust fault-tolerant software architectures.", "venue": "ArXiv", "authors": ["Harish Dattatraya Dixit", "Sneha  Pendharkar", "Matt  Beadon", "Chris  Mason", "Tejasvi  Chakravarthy", "Bharath  Muthiah", "Sriram  Sankar"], "year": 2021, "n_citations": 10}
{"id": 6258612, "s2_id": "52c06a43eeddd2a0666b7cd3ab9fd41686bc7db4", "title": "GPU Domain Specialization via Composable On-Package Architecture", "abstract": "\n As GPUs scale their low-precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and memory system capabilities. We demonstrate that a converged GPU design trying to address diverging architectural requirements between FP32 (or larger)-based HPC and FP16 (or smaller)-based DL workloads results in sub-optimal configurations for either of the application domains. We argue that a\n C\n omposable\n O\n n-\n PA\n ckage\n GPU\n (COPA-GPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with memory system specialization per application domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4\u00d7 higher off-die bandwidth, 32\u00d7 larger on-package cache, and 2.3\u00d7 higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-optimized COPA-GPU featuring a combination of 16\u00d7 larger cache capacity and 1.6\u00d7 higher DRAM bandwidth scales per-GPU training and inference performance by 31% and 35%, respectively, and reduces the number of GPU instances by 50% in scale-out training scenarios.\n", "venue": "ACM Transactions on Architecture and Code Optimization", "authors": ["Yaosheng  Fu", "Evgeny  Bolotin", "Niladrish  Chatterjee", "David  Nellans", "Stephen W. Keckler"], "year": 2022, "n_citations": 0}
{"id": 6262079, "s2_id": "cd301871f581c6e03fa2483172e4909eea51299e", "title": "Design of a virtual component neutral network-on-chip transaction layer", "abstract": "Research studies have demonstrated the feasibility and advantages of network-on-chip (NoC) over traditional bus-based architectures but have not focused on compatibility of communication standards. This paper describes a number of issues faced when designing a VC-neutral NoC, i.e., compatible with standards such as AHB 2.0, AXI, VCI, OCP, and various other proprietary protocols, and how a layered approach to communication helps solve these issues.", "venue": "Design, Automation and Test in Europe", "authors": ["Philippe  Martin"], "year": 2005, "n_citations": 17}
{"id": 6264172, "s2_id": "4704199ffcff03166398f3a3984917cb359237ee", "title": "ARENA: Asynchronous Reconfigurable Accelerator Ring to Enable Data-Centric Parallel Computing", "abstract": "The next generation HPC and data centers are likely to be reconfigurable and data-centric due to the trend of hardware specialization and the emergence of data-driven applications. In this article, we propose ARENA \u2013 an asynchronous reconfigurable accelerator ring architecture as a potential scenario on how the future HPC and data centers will be like. Despite using the coarse-grained reconfigurable arrays (CGRAs) as the substrate platform, our key contribution is not only the CGRA-cluster design itself, but also the ensemble of a new architecture and programming model that enables asynchronous tasking across a cluster of reconfigurable nodes, so as to bring specialized computation to the data rather than the reverse. We presume distributed data storage without asserting any prior knowledge on the data distribution. Hardware specialization occurs at runtime when a task finds the majority of data it requires are available at the present node. In other words, we dynamically generate specialized CGRA accelerators where the data reside. The asynchronous tasking for bringing computation to data is achieved by circulating the task token, which describes the dataflow graphs to be executed for a task, among the CGRA cluster connected by a fast ring network. Evaluations on a set of HPC and data-driven applications across different domains show that ARENA can provide better parallel scalability with reduced data movement (53.9 percent). Compared with contemporary compute-centric parallel models, ARENA can bring on average 4.37\u00d7 speedup. The synthesized CGRAs and their task-dispatchers only occupy 2.93mm<inline-formula><tex-math notation=\"LaTeX\">$^2$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"tan-ieq1-3081074.gif\"/></alternatives></inline-formula> chip area under 45nm process technology and can run at 800MHz with on average 759.8mW power consumption. ARENA also supports the concurrent execution of multi-applications, offering ideal architectural support for future high-performance parallel computing and data analytics systems.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Cheng  Tan", "Chenhao  Xie", "Tong  Geng", "Andres  Marquez", "Antonino  Tumeo", "Kevin  Barker", "Ang  Li"], "year": 2021, "n_citations": 0}
{"id": 6269844, "s2_id": "ce55dcf1de50b08d996630b8207124fa43c0bf56", "title": "Brain-like infrastructure for embedded SoC diagnosis", "abstract": "This article describes high-speed multiprocessor architecture for the concurrent analyzing information represented in analytic, graph- and table forms of associative relations to search, recognize and make a decision in n-dimensional vector discrete space. Vector-logical process models of actual applications, for which the quality of solution is estimated by the proposed integral non-arithmetical metric of the interaction between Boolean vectors, are described.", "venue": "2010 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)", "authors": ["Vladimir  Hahanov", "Wajeb  Gharibi", "Olesya  Guz"], "year": 2010, "n_citations": 0}
{"id": 6270286, "s2_id": "b1edd1391a89ba5ff0df847909a76179dd87dc7b", "title": "Enabling Effective FPGA Debug using Overlays: Opportunities and Challenges", "abstract": "FPGAs are going mainstream. Major companies that were not traditionally FPGA-focused are now seeking ways to exploit the benefits of reconfigurable technology and provide it to their customers. In order to do so, a debug ecosystem that provides for effective visibility into a working design and quick debug turn-around times is essential. Overlays have the opportunity to play a key role in this ecosystem. In this overview paper, we discuss how an overlay fabric that allows the user to rapidly add debug instrumentation to a design can be created and exploited. We discuss the requirements of such an overlay and some of the research challenges and opportunities that need to be addressed. To make our exposition concrete, we use two previously-published examples of overlays that have been developed to implement debug instrumentation.", "venue": "ArXiv", "authors": ["Fatemeh  Eslami", "Eddie  Hung", "Steven J. E. Wilton"], "year": 2016, "n_citations": 8}
{"id": 6272099, "s2_id": "288829068a5352073a2ba634429c8330077f443a", "title": "Hardware accelerated power estimation", "abstract": "In this paper, we present power emulation, a novel design paradigm that utilizes hardware acceleration for the purpose of fast power estimation. Power emulation is based on the observation that the functions necessary for power estimation (power model evaluation, aggregation, etc.) can be implemented as hardware circuits. Therefore, we can enhance any given design with \"power estimation hardware\", map it to a prototyping platform, and exercise it with any given test stimuli to obtain power consumption estimates. Our empirical studies with industrial designs reveal that power emulation can achieve significant speedups (10X to 500X) over state-of-the-art commercial register-transfer level (RTL) power estimation tools.", "venue": "Design, Automation and Test in Europe", "authors": ["Joel  Coburn", "Srivaths  Ravi", "Anand  Raghunathan"], "year": 2005, "n_citations": 9}
{"id": 6272993, "s2_id": "c7a770d5ae827e7a82a7b50253a902a309835c25", "title": "fpgaConvNet: A Toolflow for Mapping Diverse Convolutional Neural Networks on Embedded FPGAs", "abstract": "In recent years, Convolutional Neural Networks (ConvNets) have become an enabling technology for a wide range of novel embedded Artificial Intelligence systems. Across the range of applications, the performance needs vary significantly, from high-throughput video surveillance to the very low-latency requirements of autonomous cars. In this context, FPGAs can provide a potential platform that can be optimally configured based on the different performance needs. However, the complexity of ConvNet models keeps increasing making their mapping to an FPGA device a challenging task. This work presents fpgaConvNet, an end-to-end framework for mapping ConvNets on FPGAs. The proposed framework employs an automated design methodology based on the Synchronous Dataflow (SDF) paradigm and defines a set of SDF transformations in order to efficiently explore the architectural design space. By selectively optimising for throughput, latency or multiobjective criteria, the presented tool is able to efficiently explore the design space and generate hardware designs from high-level ConvNet specifications, explicitly optimised for the performance metric of interest. Overall, our framework yields designs that improve the performance by up to 6.65x over highly optimised embedded GPU designs for the same power constraints in embedded environments.", "venue": "ArXiv", "authors": ["Stylianos I. Venieris", "Christos-Savvas  Bouganis"], "year": 2017, "n_citations": 22}
{"id": 6283918, "s2_id": "fc9bcc15dfeb60e123f3169b85fde0b12f6d6706", "title": "Hardware design for binarization and thinning of fingerprint images", "abstract": "Two critical steps in fingerprint recognition are binarization and thinning of the image. The need for real time processing motivates us to select local adaptive thresholding approach for the binarization step. We introduce a new hardware for this purpose based on pipeline architecture. We propose a formula for selecting an optimal block size for the thresholding purpose. To decrease minutiae false detection, the binarized image is dilated. We also present in this paper a new pipeline structure for implementing the thinning algorithm", "venue": "ArXiv", "authors": ["Farshad  Kheiri", "Shadrokh  Samavi", "Nader  Karimi"], "year": 2017, "n_citations": 2}
{"id": 6285844, "s2_id": "fd0394807e914970bbf14d9b015ab95bc5b0e8d2", "title": "3D cache hierarchy optimization", "abstract": "3D integration has the potential to improve the scalability and performance of Chip Multiprocessors (CMP). A closed form analytical solution for optimizing 3D CMP cache hierarchy is developed. It allows optimal partitioning of the cache hierarchy levels into 3D silicon layers and optimal allocation of area among cache hierarchy levels under constrained area and power budgets. The optimization framework is extended by incorporating the impact of multithreaded data sharing on the private cache miss rate. An analytical model for cache access time as a function of cache size and a number of 3D partitions is proposed and verified using CACTI simulation.", "venue": "2013 IEEE International 3D Systems Integration Conference (3DIC)", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar"], "year": 2013, "n_citations": 7}
{"id": 6290442, "s2_id": "6313f104df803b1cb6667dbb6e0b2f52fa000dfa", "title": "First experiences with the Intel MIC architecture at LRZ", "abstract": "With the rapidly growing demand for computing power new accelerator based architectures have entered the world of high performance computing since around 5 years. In particular GPGPUs have recently become very popular, however programming GPGPUs using programming languages like CUDA or OpenCL is cumbersome and error-prone. Trying to overcome these difficulties, Intel developed their own Many Integrated Core (MIC) architecture which can be programmed using standard parallel programming techniques like OpenMP and MPI. In the beginning of 2013, the first production-level cards named Intel Xeon Phi came on the market. LRZ has been considered by Intel as a leading research centre for evaluating coprocessors based on the MIC architecture since 2010 under strict NDA. Since the Intel Xeon Phi is now generally available, we can share our experience on programming Intel's new MIC architecture.", "venue": "ArXiv", "authors": ["Volker  Weinberg", "Momme  Allalen"], "year": 2013, "n_citations": 0}
{"id": 6292861, "s2_id": "0dd02d585a6931192b47d5e13feac2bc0e630381", "title": "Extending High-Level Synthesis for Task-Parallel Programs", "abstract": "C/C++/OpenCL-based high-level synthesis (HLS) becomes more and more popular for field-programmable gate array (FPGA) accelerators in many application domains in recent years, thanks to its competitive quality of results (QoR) and short development cycles compared with the traditional register-transfer level design approach. Yet, limited by the sequential C semantics, it remains challenging to adopt the same highly productive high-level programming approach in many other application domains, where coarse-grained tasks run in parallel and communicate with each other at a fine-grained level. While current HLS tools do support task-parallel programs, the productivity is greatly limited in the code development cycle due to the poor programmability, in the correctness verification cycle due to restricted software simulation, and in the QoR tuning cycle due to slow code generation. Such limited productivity often defeats the purpose of HLS and hinder programmers from adopting HLS for task-parallel FPGA accelerators.In this paper, we extend the HLS C++ language and present a fully automated framework with programmer-friendly interfaces, unconstrained software simulation, and fast hierarchical code generation to overcome these limitations and demonstrate how task-parallel programs can be productively supported in HLS. Experimental results based on a wide range of real-world task-parallel programs show that, on average, the lines of kernel and host code are reduced by 22% and 51%, respectively, which considerably improves the programmability. The correctness verification and the iterative QoR tuning cycles are both greatly shortened by 3.2\u00d7 and 6.8\u00d7, respectively. Our work is open-source at https://github.com/UCLA-VAST/tapa/.", "venue": "2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Yuze  Chi", "Licheng  Guo", "Jason  Lau", "Young-kyu  Choi", "Jie  Wang", "Jason  Cong"], "year": 2021, "n_citations": 1}
{"id": 6293679, "s2_id": "0881d8beaabfe1a41f29ca7d9cc14982139bef9a", "title": "Condensation-Net: Memory-Efficient Network Architecture With Cross-Channel Pooling Layers and Virtual Feature Maps", "abstract": "Lightweight convolutional neural networks is an important research topic in the field of embedded vision. To implement image recognition tasks on a resource-limited hardware platform, it is necessary to reduce the memory size and the computational cost. The contribution of this paper is stated as follows. First, we propose an algorithm to process a specific network architecture (Condensation-Net) without increasing the maximum memory storage for feature maps. The architecture for virtual feature maps saves 26.5% of memory bandwidth by calculating the results of cross-channel pooling before storing the feature map into the memory. Second, we show that cross-channel pooling can improve the accuracy of object detection tasks, such as face detection, because it increases the number of filter weights. Compared with Tiny-YOLOv2, the improvement of accuracy is 2.0% for quantized networks and 1.5% for full-precision networks when the false-positive rate is 0.1. Last but not the least, the analysis results show that the overhead to support the cross-channel pooling with the proposed hardware architecture is negligible small. The extra memory cost to support Condensation-Net is 0.2% of the total size, and the extra gate count is only 2.1% of the total size.", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": ["Tse-Wei  Chen", "Motoki  Yoshinaga", "Hongxing  Gao", "Wei  Tao", "Dongchao  Wen", "Junjie  Liu", "Kinya  Osa", "Masami  Kato"], "year": 2019, "n_citations": 1}
{"id": 6294477, "s2_id": "cde84fac57c83ed28b19e15325f5efb1cb0c0ed0", "title": "Algebra-logical repair method for FPGA logic blocks", "abstract": "An algebra-logical repair method for FPGA functional logic blocks on the basis of solving the coverage problem is proposed. It is focused on implementation into Infrastructure IP for system-on-a chip and system-in-package. A method is designed for providing the operability of FPGA blocks and digital system as a whole. It enables to obtain exact and optimal solution associated with the minimum number of spares needed to repair the FPGA logic components with multiple faults.", "venue": "2010 East-West Design & Test Symposium (EWDTS)", "authors": ["Vladimir  Hahanov", "Sergey  Galagan", "Vitaliy  Olchovoy", "Aleksey  Priymak"], "year": 2010, "n_citations": 4}
{"id": 6297665, "s2_id": "d7c01803a82ebe0c3e5dc1a0d142c34f93490590", "title": "Performance-Optimum Superscalar Architecture for Embedded Applications", "abstract": "Embedded applications are widely used in portable devices such as wireless phones, personal digital assistants, laptops, etc. High throughput and real time requirements are especially important in such data-intensive tasks. Therefore, architectures that provide the required performance are the most desirable. On the other hand, processor performance is severely related to the average memory access delay, number of processor registers and also size of the instruction window and superscalar parameters. Therefore, cache, register file and superscalar parameters are the major architectural concerns in designing a superscalar architecture for embedded processors. Although increasing cache and register file size leads to performance improvements in high performance embedded processors, the increased area, power consumption and memory delay are the overheads of these techniques. This paper explores the effect of cache, register file and superscalar parameters on the processor performance to specify the optimum size of these parameters for embedded applications. Experimental results show that although having bigger size of these parameters is one of the performance improvement approaches in embedded processors, however, by increasing the size of some parameters over a threshold value, performance improvement is saturated and especially in cache size, increments over this threshold value decrease the performance.", "venue": "ArXiv", "authors": ["Mehdi  Alipour", "Mostafa E. Salehi"], "year": 2012, "n_citations": 0}
{"id": 6298120, "s2_id": "cc5b779ca1cfe3d7088ef672f86871bf66ce2fa3", "title": "On the optimal design of triple modular redundancy logic for SRAM-based FPGAs", "abstract": "Triple modular redundancy (TMR) is a suitable fault tolerant technique for SRAM-based FPGA. However, one of the main challenges in achieving 100% robustness in designs protected by TMR running on programmable platforms is to prevent upsets in the routing from provoking undesirable connections between signals from distinct redundant logic parts, which can generate an error in the output. This paper investigates the optimal design of the TMR logic (e.g., by cleverly inserting voters) to ensure robustness. Four different versions of a TMR digital filter were analyzed by fault injection. Faults were randomly inserted straight into the bitstream of the FPGA. The experimental results presented in this paper demonstrate that the number and placement of voters in the TMR design can directly affect the fault tolerance, ranging from 4.03% to 0.98% the number of upsets in the routing able to cause an error in the TMR circuit.", "venue": "Design, Automation and Test in Europe", "authors": ["Fernanda Gusm\u00e3o de Lima Kastensmidt", "Luca  Sterpone", "Luigi  Carro", "Matteo Sonza Reorda"], "year": 2005, "n_citations": 242}
{"id": 6304703, "s2_id": "e41068f5865f76da8f62b37e479bfb9ee6e818f0", "title": "Design of an Audio Interface for Patmos", "abstract": "This paper describes the design and implementation of an audio interface for the Patmos processor, which runs on an Altera DE2-115 FPGA board. This board has an audio codec included, the WM8731. The interface described in this work allows to receive and send audio from and to the WM8731, and to synthesize, store or manipulate audio signals writing C programs for Patmos. The structure of this project is integrated with the Patmos project: new hardware modules have been added as IOs, which allow the communication between the processor and the audio codec. These modules include a clock generator for the audio chip, ADC and DAC modules for the audio conversion from analog to digital and vice versa, and an IC module which allows setting configuration parameters on the audio codec. Moreover, a top module has been created, which connects all the modules previously mentioned between them, to Patmos and to the WM8731, using the external pins of the FPGA. Simulations have been done using the Patmos emulator to check the correct functionality of all the modules. After that, the Patmos processor with the audio interface has been synthesized and loaded into the Altera FPGA. C programs have been written, and SignalTap, the logic analyzer from Quartus, has been used to verify that the modules work as expected. The results show that the audio interface works as desired: the implemented C programs show that the audio is input and output to and from Patmos correctly, with the right format and the expected sampling frequency. The analogue audio signal output from the FPGA has been recorded using an external audio interface, and the signals look exactly as anticipated.", "venue": "ArXiv", "authors": ["Daniel Sanz Ausin", "Fabian  Goerge"], "year": 2017, "n_citations": 1}
{"id": 6304963, "s2_id": "f396aeb9a8519f9cbb18f3b9957f9e88ae79a7b6", "title": "Using Virtual Addresses with Communication Channels", "abstract": "While for single processor and SMP machines, memory is the allocatable quantity, for machines made up of large amounts of parallel computing units, each with its own local memory, the allocatable quantity is a single computing unit. Where virtual address management is used to keep memory coherent and allow allocation of more than physical memory is actually available, virtual communication channel references can be used to make computing units stay connected across allocation and swapping.", "venue": "ArXiv", "authors": ["Oskar  Schirmer"], "year": 2013, "n_citations": 0}
{"id": 6306880, "s2_id": "3719fddef51053ca16e7b9a2702ebe0a703856ff", "title": "2D Discrete Fourier Transform with simultaneous edge artifact removal for real-time applications", "abstract": "Two-Dimensional (2D) Discrete Fourier Transform (DFT) is a basic and computationally intensive algorithm, with a vast variety of applications. 2D images are, in general, non-periodic, but are assumed to be periodic while calculating their DFTs. This leads to cross-shaped artifacts in the frequency domain due to spectral leakage. These artifacts can have critical consequences if the DFTs are being used for further processing. In this paper we present a novel FPGA-based design to calculate high-throughput 2D DFTs with simultaneous edge artifact removal. Standard approaches for removing these artifacts using apodization functions or mirroring, either involve removing critical frequencies or a surge in computation by increasing image size. We use a periodic-plus-smooth decomposition based artifact removal algorithm optimized for FPGA implementation, while still achieving real-time (\u226523 frames per second) performance for a 512\u00d7512 size image stream. Our optimization approach leads to a significant decrease in external memory utilization thereby avoiding memory conflicts and simplifies the design. We have tested our design on a PXIe based Xilinx Kintex 7 FPGA system communicating with a host PC which gives us the advantage to further expand the design for industrial applications.", "venue": "2015 International Conference on Field Programmable Technology (FPT)", "authors": ["Faisal  Mahmood", "Mart  Toots", "Lars-Goran  Ofverstedt", "Ulf  Skoglund"], "year": 2015, "n_citations": 13}
{"id": 6310706, "s2_id": "33a0fbb85a76e33b353e29b7dc4aea9253f2baf0", "title": "SNS Timing System", "abstract": "A modern physics facility must synchronize the operation of equipment over a wide area. The primary purpose of the site wide SNS synchronization and timing system is to synchronize the operation of the LINAC, accumulator ring and neutron choppers and to distribute appropriate timing signals to accelerator systems, including the Injector, LINAC, Accumulator Ring and Experimental Facilities. Signals to be distributed include the ring RF clock, real-time timing triggers, machine mode and other informational events. Timing triggers and clocks from the SNS synchronization and timing system are used to synchronize hardware operations including the MEBT beam chopper, RF turn on, synchronous equipment state changes, as well as data acquisition for power supplies and beam diagnostics equipment. This paper will describe the timing equipment being designed for the SNS facility and discuss the tradeoffs between conflicting demands of the accelerator and neutron chopper performance due to AC power grid frequency fluctuations.", "venue": "ArXiv", "authors": ["B.  Oerter", "R.  Nelson", "T.  Shea", "C.  Sibley"], "year": 2001, "n_citations": 6}
{"id": 6314201, "s2_id": "ed69c4d7ba11bf033495f0432ea5c21200bbb484", "title": "Sorting in Memristive Memory", "abstract": "Sorting is needed in many application domains. The data is read from memory and sent to a general purpose processor or application specific hardware for sorting. The sorted data is then written back to the memory. Reading/writing data from/to memory and transferring data between memory and processing unit incur a large latency and energy overhead. In this work, we develop, to the best of our knowledge, the first architectures for in-memory sorting of data. We propose two architectures. The first architecture is applicable to the conventional format of representing data, weighted binary radix. The second architecture is proposed for the developing unary processing systems where data is encoded as uniform unary bitstreams. The two architectures have different advantages and disadvantages, making one or the other more suitable for a specific application. However, the common property of both is a significant reduction in the processing time compared to prior sorting designs. Our evaluations show on average 37\u00d7 and 138\u00d7 energy reduction for binary and unary designs, respectively, as compared to conventional CMOS off-memory sorting systems.", "venue": "ArXiv", "authors": ["Mohsen Riahi Alam", "M. Hassan Najafi", "Nima  TaheriNejad"], "year": 2020, "n_citations": 2}
{"id": 6314613, "s2_id": "7e464743385e532ddbce2ba483d16556f20448d8", "title": "Design of a High Speed XAUI Based on Dynamic Reconfigurable Transceiver IP Core", "abstract": "By using the dynamic reconfigurable transceiver in high speed interface design, designer can solve critical technology problems such as ensuring signal integrity conveniently, with lower error binary rate. In this paper, we designed a high speed XAUI (10Gbps Ethernet Attachment Unit Interface) to transparently extend the physical reach of the XGMII. The following points are focused: (1) IP (Intellectual Property) core usage. Altera Co. offers two transceiver IP cores in Quartus II MegaWizard Plug-In Manager for XAUI design which is featured of dynamic reconfiguration performance, that is, ALTGX_RECO?FIG instance and ALTGX instance, we can get various groups by changing settings of the devices without power off. These two blocks can accomplish function of PCS (Physical Coding Sub-layer) and PMA (Physical Medium Attachment), however, with higher efficiency and reliability. (2) 1+1 protection. In our design, two ALTGX IP cores are used to work in parallel, which named XAUI0 and XAUI1. The former works as the main channel while the latter redundant channel. When XAUI0 is out of service for some reasons, XAUI1 will start to work to keep the business. (3) RTL (Register Transfer Level) coding with Verilog HDL and simulation. Create the ALTGX_RECO?FIG instance and ALTGX instance, enable dynamic reconfiguration in the ALTGXB Megafunction, then connect the ALTGX_RECO?FIG with the ALTGX instances. After RTL coding, the design was simulated on VCS simulator. The validated result indicates that the packets are transferred efficiently. FPGA makes high-speed optical communication system design simplified.", "venue": "SOCO 2012", "authors": ["Haipeng  Zhang", "Lingjun  Kong", "Xiuju  Huang", "Mengmeng  Cao"], "year": 2012, "n_citations": 1}
{"id": 6315138, "s2_id": "9ebc06a944b5d3d04aa5c991b41d12d2962610e9", "title": "Statistical timing based optimization using gate sizing", "abstract": "The increased dominance of intra-die process variations has motivated the field of statistical static timing analysis (SSTA) and has raised the need for SSTA-based circuit optimization. We propose a new sensitivity based, statistical gate sizing method. Since brute-force computation of the change in circuit delay distribution to gate size change is computationally expensive, we propose an efficient and exact pruning algorithm. The pruning algorithm is based on a novel theory of perturbation bounds which are shown to decrease as they propagate through the circuit. This allows pruning of gate sensitivities without complete propagation of their perturbations. We apply our proposed optimization algorithm to ISCAS benchmark circuits and demonstrate the accuracy and efficiency of the proposed method. Our results show an improvement of up to 10.5% in the 99-percentile circuit delay for the same circuit area, using the proposed statistical optimizer and a run time improvement of up to 56/spl times/ compared to the brute-force approach.", "venue": "Design, Automation and Test in Europe", "authors": ["Aseem  Agarwal", "Kaviraj  Chopra", "David  Blaauw"], "year": 2005, "n_citations": 49}
{"id": 6319029, "s2_id": "77835f0049433f7f0d1c6c4fb9dbbeebf31b82d0", "title": "Adaptive-Latency DRAM: Reducing DRAM Latency by Exploiting Timing Margins", "abstract": "This paper summarizes the idea of Adaptive-Latency DRAM (AL-DRAM), which was published in HPCA 2015, and examines the work's significance and future potential. AL-DRAM is a mechanism that optimizes DRAM latency based on the DRAM module and the operating temperature, by exploiting the extra margin that is built into the DRAM timing parameters. DRAM manufacturers provide a large margin for the timing parameters as a provision against two worst-case scenarios. First, due to process variation, some outlier DRAM chips are much slower than others. Second, chips become slower at higher temperatures. The timing parameter margin ensures that the slow outlier chips operate reliably at the worst-case temperature, and hence leads to a high access latency. \nUsing an FPGA-based DRAM testing platform, our work first characterizes the extra margin for 115 DRAM modules from three major manufacturers. The experimental results demonstrate that it is possible to reduce four of the most critical timing parameters by a minimum/maximum of 17.3%/54.8% at 55C while maintaining reliable operation. AL-DRAM uses these observations to adaptively select reliable DRAM timing parameters for each DRAM module based on the module's current operating conditions. AL-DRAM does not require any changes to the DRAM chip or its interface; it only requires multiple different timing parameters to be specified and supported by the memory controller. Our real system evaluations show that AL-DRAM improves the performance of memory-intensive workloads by an average of 14% without introducing any errors. Our characterization and proposed techniques have inspired several other works on analyzing and/or exploiting different sources of latency and performance variation within DRAM chips.", "venue": "ArXiv", "authors": ["Donghyuk  Lee", "Yoongu  Kim", "Gennady  Pekhimenko", "Samira Manabi Khan", "Vivek  Seshadri", "Kevin K. Chang", "Onur  Mutlu"], "year": 2018, "n_citations": 2}
{"id": 6320841, "s2_id": "39b4d495c9f1e3f9bca0a260bf1dfc7281802461", "title": "Algorithm and Hardware Co-design for Reconfigurable CNN Accelerator", "abstract": "Recent advances in algorithm-hardware co-design for deep neural networks (DNNs) have demonstrated their potential in automatically designing neural architectures and hardware designs. Nevertheless, it is still a challenging optimization problem due to the expensive training cost and the time-consuming hardware implementation, which makes the exploration on the vast design space of neural architecture and hardware design intractable. In this paper, we demonstrate that our proposed approach is capable of locating designs on the Pareto frontier. This capability is enabled by a novel three-phase co-design framework, with the following new features: (a) decoupling DNN training from the design space exploration of hardware architecture and neural architecture, (b) providing a hardwarefriendly neural architecture space by considering hardware characteristics in constructing the search cells, (c) adopting Gaussian process to predict accuracy, latency and power consumption to avoid time-consuming synthesis and place-and-route processes. In comparison with the manually-designed ResNet101, InceptionV2 and MobileNetV2, we can achieve up to 5% higher accuracy with up to 3\u00d7 speed up on the ImageNet dataset. Compared with other state-of-the-art co-design frameworks, our found network and hardware configuration can achieve 2% \u223c 6% higher accuracy, 2\u00d7\u223c 26\u00d7 smaller latency and 8.5\u00d7 higher energy efficiency.", "venue": "ArXiv", "authors": ["Hongxiang  Fan", "Martin  Ferianc", "Zhiqiang  Que", "He  Li", "Shuanglong  Liu", "Xinyu  Niu", "Wayne  Luk"], "year": 2021, "n_citations": 0}
{"id": 6321670, "s2_id": "ec3d6f0d4b9b3dc01c7c28cf25ac7862344bae3b", "title": "MemPool-3D: Boosting Performance and Efficiency of Shared-L1 Memory Many-Core Clusters with 3D Integration", "abstract": "Three-dimensional integrated circuits promise power, performance, and footprint gains compared to their 2D counterparts, thanks to drastic reductions in the interconnects\u2019 length through their smaller form factor. We can leverage the potential of 3D integration by enhancing MemPool, an open-source manycore design with 256 cores and a shared pool of L1 scratchpad memory connected with a low-latency interconnect. MemPool\u2019s baseline 2D design is severely limited by routing congestion and wire propagation delay, making the design ideal for 3D integration. In architectural terms, we increase MemPool\u2019s scratchpad memory capacity beyond the sweet spot for 2D designs, improving performance in a common digital signal processing kernel. We propose a 3D MemPool design that leverages a smart partitioning of the memory resources across two layers to balance the size and utilization of the stacked dies. In this paper, we explore the architectural and the technology parameter spaces by analyzing the power, performance, area, and energy efficiency of MemPool instances in 2D and 3D with 1 MiB, 2 MiB, 4 MiB, and 8 MiB of scratchpad memory in a commercial 28 nm technology node. We observe a performance gain of 9.1 % when running a matrix multiplication on the MemPool-3D design with 4 MiB of scratchpad memory compared to the MemPool 2D counterpart. In terms of energy efficiency, we can implement the MemPool-3D instance with 4 MiB of L1 memory on an energy budget 15 % smaller than its 2D counterpart, and even 3.7 % smaller than the MemPool-2D instance with one-fourth of the L1 scratchpad memory capacity.", "venue": "ArXiv", "authors": ["Matheus  Cavalcante", "Anthony  Agnesina", "Samuel  Riedel", "Moritz  Brunion", "Alberto  Garcia-Ortiz", "Dragomir  Milojevic", "Francky  Catthoor", "Sung Kyu Lim", "Luca  Benini"], "year": 2021, "n_citations": 0}
{"id": 6321694, "s2_id": "c23ecde0553fba8fcd9fcbafec2948ab56da7198", "title": "Configurable memory systems for embedded many-core processors", "abstract": "The memory system of a modern embedded processor consumes a large fraction of total system energy. We explore a range of different configuration options and show that a reconfigurable design can make better use of the resources available to it than any fixed implementation, and provide large improvements in both performance and energy consumption. Reconfigurability becomes increasingly useful as resources become more constrained, so is particularly relevant in the embedded space. \nFor an optimised architectural configuration, we show that a configurable cache system performs an average of 20% (maximum 70%) better than the best fixed implementation when two programs are competing for the same resources, and reduces cache miss rate by an average of 70% (maximum 90%). We then present a case study of AES encryption and decryption, and find that a custom memory configuration can almost double performance, with further benefits being achieved by specialising the task of each core when parallelising the program.", "venue": "ArXiv", "authors": ["Daniel  Bates", "Alex  Chadwick", "Robert  Mullins"], "year": 2016, "n_citations": 3}
{"id": 6324283, "s2_id": "13fec5228f8d8abe25c5a6c9960c1d2681a929cb", "title": "A Dynamic Overlay Supporting Just-In-Time Assembly to Construct Customized Hardware Accelerators", "abstract": "Barriers that prevent programmers from using FPGAs include the need to work within vendor specific CAD tools, knowledge of hardware programming models, and the requirement to pass each design through synthesis, place and route. In this work, a dynamic overlay is designed to support Just- In-Time assembly by composing hardware operators to construct full accelerators. The hardware operators are pre-synthesized bit- streams and can be downloaded to Partially Reconfigurable(PR) regions at runtime.", "venue": "ArXiv", "authors": ["Zeyad  Aklah", "Sen  Ma", "David  Andrews"], "year": 2016, "n_citations": 3}
{"id": 6327124, "s2_id": "2a13aba1a5d10d0ba0f833c4c7629896d760d85b", "title": "Physically unclonable function using initial waveform of ring oscillators on 65 nm CMOS technology", "abstract": "A silicon physically unclonable function (PUF) using ring oscillators (ROs) has the advantage of easy application in both an application specific integrated circuit (ASIC) and a field-programmable gate array (FPGA). Here, we provide a RO-PUF using the initial waveform of the ROs based on 65 nm CMOS technology. Compared with the conventional RO-PUF, the number of ROs is greatly reduced and the time needed to generate an ID is within a couple of system clocks.", "venue": "ArXiv", "authors": ["Tetsufumi  Tanamoto", "Satoshi  Takaya", "Nobuaki  Sakamoto", "Hirotsugu  Kasho", "Shinichi  Yasuda", "Takao  Marukame", "Shinobu  Fujita", "Yuichiro  Mitani"], "year": 2017, "n_citations": 3}
{"id": 6331502, "s2_id": "0488df3167b65cb35f7975c9d1a4fd29d970cfe5", "title": "Easily testable logical networks based on a 'widened long flip-flop'", "abstract": "The article describes an attempt to solve at once three basic problems arising at testing a complex digital equipment for defects: 1) the problem of an exponential increasing of the complexity of testing the equipment with the complexity of the equipment; 2) the problem of testing of the tester; 3) the problem of a mutual masking of defects. The proposed solution is nothing more than using certain limitations for connections between usual logical gates. Arbitrary multiple stuck-at-faults are supposed as defects.", "venue": "ArXiv", "authors": ["Nick  Stukach"], "year": 2008, "n_citations": 0}
{"id": 6332625, "s2_id": "42f70e0ffb2077cd3719f537dd0f280bdb44d616", "title": "HALLS: An Energy-Efficient Highly Adaptable Last Level STT-RAM Cache for Multicore Systems", "abstract": "Spin-Transfer Torque RAM (STT-RAM) is widely considered a promising alternative to SRAM in the memory hierarchy due to STT-RAM's non-volatility, low leakage power, high density, and fast read speed. The STT-RAM's small feature size is particularly desirable for the last-level cache (LLC), which typically consumes a large area of silicon die. However, long write latency and high write energy still remain challenges of implementing STT-RAMs in the CPU cache. An increasingly popular method for addressing this challenge involves trading off the non-volatility for reduced write speed and write energy by relaxing the STT-RAM's data retention time. However, in order to maximize energy saving potential, the cache configurations, including STT-RAM's retention time, must be dynamically adapted to executing applications\u2019 variable memory needs. In this paper, we propose a highly adaptable last level STT-RAM cache (HALLS) that allows the LLC configurations and retention time to be adapted to applications\u2019 runtime execution requirements. We also propose low-overhead runtime tuning algorithms to dynamically determine the best (lowest energy) cache configurations and retention times for executing applications. Compared to prior work, HALLS reduced the average energy consumption by 60.57 percent in a quad-core system, while introducing marginal latency overhead.", "venue": "IEEE Transactions on Computers", "authors": ["Kyle  Kuan", "Tosiron  Adegbija"], "year": 2019, "n_citations": 9}
{"id": 6340901, "s2_id": "3374c4f48015f325ff7cd6a768c8a743d35e1f58", "title": "MGSim + MGMark: A Framework for Multi-GPU System Research", "abstract": "The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of GPUs (Graphics Processing Units). As single-GPU systems struggle to satisfy the performance demands, multi-GPU systems have begun to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabrics, runtime libraries and associated programming models. The research community currently lacks a publically available and comprehensive multi-GPU simulation framework and benchmark suite to evaluate multi-GPU system design solutions. \nIn this work, we present MGSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. We complement MGSim with MGMark, a suite of multi-GPU workloads that explores multi-GPU collaborative execution patterns. Our simulator is scalable and comes with in-built support for multi-threaded execution to enable fast and efficient simulations. In terms of performance accuracy, MGSim differs $5.5\\%$ on average when compared against actual GPU hardware. We also achieve a $3.5\\times$ and a $2.5\\times$ average speedup in function emulation and architectural simulation with 4 CPU cores, while delivering the same accuracy as the serial simulation. \nWe illustrate the novel simulation capabilities provided by our simulator through a case study exploring programming models based on a unified multi-GPU system~(U-MGPU) and a discrete multi-GPU system~(D-MGPU) that both utilize unified memory space and cross-GPU memory access. We evaluate the design implications from our case study, suggesting that D-MGPU is an attractive programming model for future multi-GPU systems.", "venue": "ArXiv", "authors": ["Yifan  Sun", "Trinayan  Baruah", "Saiful A. Mojumder", "Shi  Dong", "Rafael  Ubal", "Xiang  Gong", "Shane  Treadway", "Yuhui  Bao", "Vincent  Zhao", "Jos\u00e9 L. Abell\u00e1n", "John  Kim", "Ajay  Joshi", "David R. Kaeli"], "year": 2018, "n_citations": 7}
{"id": 6343028, "s2_id": "b44df9bc93ff378e93307cc5b0b269f6f17b857b", "title": "BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant Weight Matrices", "abstract": "In recent years, Graph Neural Networks (GNNs) appear to be state-of-the-art algorithms for analyzing non-euclidean graph data. By applying deep-learning to extract high-level representations from graph structures, GNNs achieve extraordinary accuracy and great generalization ability in various tasks. However, with the ever-increasing graph sizes, more and more complicated GNN layers, and higher feature dimensions, the computational complexity of GNNs grows exponentially. How to inference GNNs in real time has become a challenging problem, especially for some resource-limited edge-computing platforms.To tackle this challenge, we propose BlockGNN, a software-hardware co-design approach to realize efficient GNN acceleration. At the algorithm level, we propose to leverage block-circulant weight matrices to greatly reduce the complexity of various GNN models. At the hardware design level, we propose a pipelined CirCore architecture, which supports efficient block-circulant matrices computation. Basing on CirCore, we present a novel BlockGNN accelerator to compute various GNNs with low latency. Moreover, to determine the optimal configurations for diverse deployed tasks, we also introduce a performance and resource model that helps choose the optimal hardware parameters automatically. Comprehensive experiments on the ZC706 FPGA platform demonstrate that on various GNN tasks, BlockGNN achieves up to 8.3\u00d7 speedup compared to the baseline HyGCN architecture and 111.9\u00d7 energy reduction compared to the Intel Xeon CPU platform.", "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Zhe  Zhou", "Bizhao  Shi", "Zhe  Zhang", "Yijin  Guan", "Guangyu  Sun", "Guojie  Luo"], "year": 2021, "n_citations": 1}
{"id": 6344115, "s2_id": "f11a7062d5a0a361d75d93194c72ae91568206c4", "title": "Why is FPGA-GPU Heterogeneity the Best Option for Embedded Deep Neural Networks?", "abstract": "Graphics Processing Units (GPUs) are currently the dominating programmable architecture for Deep Learning (DL) accelerators. The adoption of Field Programmable Gate Arrays (FPGAs) in DL accelerators is however getting momentum. In this paper, we demonstrate that Direct Hardware Mapping (DHM) of a Convolutional Neural Network (CNN) on an embedded FPGA substantially outperforms a GPU implementation in terms of energy efficiency and execution time. However, DHM is highly resource intensive and cannot fully substitute the GPU when implementing a state-of-the-art CNN. We thus propose a hybrid FPGA-GPU DL acceleration method and demonstrate that heterogeneous acceleration outperforms GPU acceleration even including communication overheads. Experimental results are conducted on a heterogeneous multi-platform setup embedding an Nvidia\u00ae Jetson TX2 CPUGPU board and an Intel\u00ae Cyclone10GX FPGA board. The SqueezeNet, MobileNetv2, and ShuffleNetv2 mobile-oriented CNNs are experimented. We show that heterogeneous FPGAGPU acceleration outperforms GPU acceleration for classification inference task over MobileNetv2 (12%-30% energy reduction, 4% to 26% latency reduction), SqueezeNet (21%-28% energy reduction, same latency), and ShuffleNetv2 (25% energy reduction, 21% latency reduction).", "venue": "ArXiv", "authors": ["Walther  Carballo-Hern'andez", "Maxime  Pelcat", "Franccois  Berry"], "year": 2021, "n_citations": 1}
{"id": 6344768, "s2_id": "7b52d7339cbb9fd635a9884b5c2736d3fb8f0d62", "title": "Banshee: Bandwidth-Efficient DRAM Caching via Software/Hardware Cooperation", "abstract": "Placing the DRAM in the same package as a processor enables several times higher memory bandwidth than conventional off-package DRAM. Yet, the latency of in-package DRAM is not appreciably lower than that of off-package DRAM. A promising use of in-package DRAM is as a large cache. Unfortunately, most previous DRAM cache designs optimize mainly for cache hit latency and do not consider bandwidth efficiency as a first-class design constraint. Hence, as we show in this paper, these designs are suboptimal for use with in-package DRAM.We propose a new DRAM cache design, Banshee, that optimizes for both in-package and off-package DRAM bandwidth efficiency without degrading access latency. Banshee is based on two key ideas. First, it eliminates the tag lookup overhead by tracking the contents of the DRAM cache using TLBs and page table entries, which is efficiently enabled by a new lightweight TLB coherence protocol we introduce. Second, it reduces unnecessary DRAM cache replacement traffic with a new bandwidth-aware frequency-based replacement policy. Our evaluations show that Banshee significantly improves performance (15% on average) and reduces DRAM traffic (35.8% on average) over the best-previous latency-optimized DRAM cache design.CCS CONCEPTS\u2022Computersystemsorganization \u2192 Multicore architectures; {\\it Heterogeneous (hybrid) systems;", "venue": "2017 50th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Xiangyao  Yu", "Christopher J. Hughes", "Nadathur  Satish", "Onur  Mutlu", "Srinivas  Devadas"], "year": 2017, "n_citations": 55}
{"id": 6345055, "s2_id": "0fc93abe2606fad14c42692dbe22989a4e53b088", "title": "FpSynt: A fixed-point datapath synthesis tool for embedded systems", "abstract": "Digital mobile systems must function with low power, small size and weight, and low cost. High-performance desktop microprocessors, with built-in floating point hardware, are not suitable in these cases. For embedded systems, it can be advantageous to implement these calculations with fixed point arithmetic instead. We present an automated fixed-point data path synthesis tool FpSynt for designing embedded applications in fixed-point domain with sufficient accuracy for most applications.", "venue": "2013 IEEE 56th International Midwest Symposium on Circuits and Systems (MWSCAS)", "authors": ["Ilya Y. Zhbannikov", "Gregory W. Donohoe"], "year": 2013, "n_citations": 0}
{"id": 6347589, "s2_id": "33468f9ebb9e2cbbf523ac25c0070a3fccdc735f", "title": "Transport Triggered Array Processor for Vision Applications", "abstract": "Low-level sensory data processing in many Internet-of-Things (IoT) devices pursue energy efficiency by utilizing sleep modes or slowing the clocking to the minimum. To curb the share of stand-by power dissipation in those designs, near-threshold/sub-threshold operational points or ultra-low-leakage processes in fabrication are employed. Those limit the clocking rates significantly, reducing the computing throughputs of individual processing cores. In this contribution we explore compensating for the performance loss of operating in near-threshold region (Vdd =0.6V) through massive parallelization. Benefits of near-threshold operation and massive parallelism are optimum energy consumption per instruction operation and minimized memory roundtrips, respectively. The Processing Elements (PE) of the design are based on Transport Triggered Architecture. The fine grained programmable parallel solution allows for fast and efficient computation of learnable low-level features (e.g. local binary descriptors and convolutions). Other operations, including Max-pooling have also been implemented. The programmable design achieves excellent energy efficiency for Local Binary Patterns computations.", "venue": "SAMOS", "authors": ["Mehdi  Safarpour", "Ilkka  Hautala", "Miguel Bordallo L\u00f3pez", "Olli  Silv\u00e9n"], "year": 2019, "n_citations": 0}
{"id": 6347623, "s2_id": "bf03153df049914aba5080efeb30e3107be5dd2f", "title": "Effect of Data Sharing on Private Cache Design in Chip Multiprocessors", "abstract": "In multithreaded applications with high degree of data sharing, the miss rate of private cache is shown to exhibit a compulsory miss component. It manifests because at least some of the shared data originates from other cores and can only be accessed in a shared cache. The compulsory component does not change with the private cache size, causing its miss rate to diminish slower as the cache size grows. As a result, the peak performance of a Chip Multiprocessor (CMP) for workloads with high degree of data sharing is achieved with a smaller private cache, compared to workloads with no data sharing. The CMP performance can be improved by reassigning some of the constrained area or power resource from private cache to core. Alternatively, the area or power budget of a CMP can be reduced without a performance hit.", "venue": "ArXiv", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar"], "year": 2016, "n_citations": 1}
{"id": 6349911, "s2_id": "e29466b2b03949ba65c5106b426fc5586b8d2caa", "title": "ZIPPER: Exploiting Tile- and Operator-level Parallelism for General and Scalable Graph Neural Network Acceleration", "abstract": "Graph neural networks (GNNs) start to gain momentum after showing significant performance improvement in a variety of domains including molecular science, recommendation, and transportation. Turning such performance improvement of GNNs into practical applications relies on effective and efficient execution, especially for inference. However, neither CPU nor GPU can meet these needs if considering both performance and energy efficiency. That\u2019s because accelerating GNNs is challenging due to their excessive memory usage and arbitrary interleaving of diverse operations. Besides, the semantics gap between the high-level GNN programming model and efficient hardware makes it difficult in accelerating general-domain GNNs. To address the challenge, we propose ZIPPER, an efficient yet general acceleration system for GNNs. The keys to ZIPPER include a graph-native intermediate representation (IR) and the associated compiler. By capturing GNN primitive operations and representing with GNN IR, ZIPPER is able to fit GNN semantics into hardware structure for efficient execution. The IR also enables GNN-specific optimizations including sparse graph tiling and redundant operation elimination. We further present an hardware architecture design consisting of dedicated blocks for different primitive operations, along with a run-time scheduler to map a IR program to the hardware blocks. Our evaluation shows that ZIPPER achieves 93.6\u00d7 speedup and 147\u00d7 energy reduction over Intel Xeon CPU, and 1.56\u00d7 speedup and 4.85\u00d7 energy reduction over NVIDIA V100 GPU on averages.", "venue": "ArXiv", "authors": ["Zhihui  Zhang", "Jingwen  Leng", "Shuwen  Lu", "Youshan  Miao", "Yijia  Diao", "Minyi  Guo", "Chao  Li", "Yuhao  Zhu"], "year": 2021, "n_citations": 0}
{"id": 6356794, "s2_id": "d34a954172c3abbc1462ac1d43a6391f5acf1730", "title": "Freezer: A Specialized NVM Backup Controller for Intermittently Powered Systems", "abstract": "The explosion of IoT and wearable devices determined a rising attention toward energy harvesting as source for powering these systems. In this context, many applications cannot afford the presence of a battery because of size, weight, and cost issues. Therefore, due to the intermittent nature of ambient energy sources, these systems must be able to save and restore their state, in order to guarantee progress across power interruptions. In this work, we propose a specialized backup/restore controller that dynamically tracks the memory accesses during the execution of the program. The controller then commits the changes to a snapshot in a nonvolatile memory (NVM) when a power failure is detected. Our approach does not require complex hybrid memories and can be implemented with standard components. Results on a set of benchmarks show an average <inline-formula> <tex-math notation=\"LaTeX\">$8\\times $ </tex-math></inline-formula> reduction in backup size. Thanks to our dedicated controller, the backup time is further reduced by more than <inline-formula> <tex-math notation=\"LaTeX\">$100\\times $ </tex-math></inline-formula>, with an area and power overhead of only 0.4% and 0.8%, respectively, with respect to a low-end IoT node.", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Davide  Pala", "Ivan  Miro-Panades", "Olivier  Sentieys"], "year": 2021, "n_citations": 0}
{"id": 6360979, "s2_id": "89731326a4d32950c8e835113120f72998720774", "title": "Efficient network for non-binary QC-LDPC decoder", "abstract": "This paper presents approaches to develop efficient network for non-binary quasi-cyclic LDPC (QC-LDPC) decoders. By exploiting the intrinsic shifting and symmetry properties of the check matrices, significant reduction of memory size and routing complexity can be achieved. Two different efficient network architectures for Class-I and Class-II non-binary QC-LDPC decoders have been proposed, respectively. Comparison results have shown that for the code of the 64-ary (1260, 630) rate-0.5 Class-I code, the proposed scheme can save more than 70.6% hardware required by shuffle network than the state-of-the-art designs. The proposed decoder example for the 32-ary (992, 496) rate-0.5 Class-II code can achieve a 93.8% shuffle network reduction compared with the conventional ones. Meanwhile, based on the similarity of Class-I and Class-II codes, similar shuffle network is further developed to incorporate both classes of codes at a very low cost.", "venue": "2012 IEEE International Symposium on Circuits and Systems", "authors": ["Chuan  Zhang", "Jin  Sha"], "year": 2012, "n_citations": 1}
{"id": 6361517, "s2_id": "35039faf66e34a1df2501cd0d7ca696826bd8c6b", "title": "Using System Hyper Pipelining (SHP) to Improve the Performance of a Coarse-Grained Reconfigurable Architecture (CGRA) Mapped on an FPGA", "abstract": "The well known method C-Slow Retiming (CSR) can be used to automatically convert a given CPU into a multithreaded CPU with independent threads. These CPUs are then called streaming or barrel processors. System Hyper Pipelining (SHP) adds a new flexibility on top of CSR by allowing a dynamic number of threads to be executed and by enabling the threads to be stalled, bypassed and reordered. SHP is now applied on the programming elements (PE) of a coarse-grained reconfigurable architecture (CGRA). By using SHP, more performance can be achieved per PE. Fork-Join operations can be implemented on a PE using the flexibility provided by SHP to dynamically adjust the number of threads per PE. Multiple threads can share the same data locally, which greatly reduces the data traffic load on the CGRA's routing structure. The paper shows the results of a CGRA using SHP-ed RISC-V cores as PEs implemented on a FPGA.", "venue": "ArXiv", "authors": ["Tobias  Strauch"], "year": 2015, "n_citations": 0}
{"id": 6362941, "s2_id": "0e6895fd48f602b01280051ec5c817a08d33a15f", "title": "von Neumann\u2019s missing \"Second Draft\": what it should contain", "abstract": "Computing science is based on a computing paradigm that is not valid anymore for today\u2019s technological conditions. The reason is that the transmission time even inside the processor chip, but especially between the system\u2019s components, is not negligible anymore. The paper introduces a quantitative measure for dispersion, which is vital for computing performance and energy consumption, and demonstrates how its value increased with the changing technology. The temporal behavior (including the dispersion of the commonly used synchronization clock time) of computing components has a critical impact on the system\u2019s performance at all levels, as demonstrated from gate-level operation to supercomputing. The same effect limits the utility of the researched new materials/effects if the related transfer time cannot be proportionally mitigated. Von Neumann\u2019s model is perfect, but now it is used outside of its range of validity. The correct procedure to consider the transfer time for the present technological background is also derived.", "venue": "2020 International Conference on Computational Science and Computational Intelligence (CSCI)", "authors": ["J'anos  V'egh"], "year": 2020, "n_citations": 3}
{"id": 6365397, "s2_id": "97cf1eb36c6ce70671306d8ad3ab00a3e9c81461", "title": "QPACE - a QCD parallel computer based on Cell processors", "abstract": "QPACE is a novel parallel computer which has been developed to be primarily used for lattice QCD simulations. The compute power is provided by the IBM PowerXCell 8i processor, an enhanced version of the Cell processor that is used in the Playstation 3. The QPACE nodes are interconnected by a custom, application optimized 3-dimensional torus network implemented on an FPGA. To achieve the very high packaging density of 26 TFlops per rack a new water cooling concept has been developed and successfully realized. In this paper we give an overview of the architecture and highlight some important technical details of the system. Furthermore, we provide initial performance results and report on the installation of 8 QPACE racks providing an aggregate peak performance of 200 TFlops.", "venue": "ArXiv", "authors": ["Heinz  Baier", "Hans  Boettiger", "Matthias  Drochner", "Norbert  Eicker", "Uwe  Fischer", "Zolt\u00e1n  Fodor", "Andreas  Frommer", "Claude  Gomez", "Gottfried  Goldrian", "Simon  Heybrock", "Dieter  Hierl", "Matthias  H\u00fcsken", "Thomas H. Huth", "Benjamin  Krill", "Jack  Lauritsen", "Thomas  Lippert", "Thilo  Maurer", "Bernhard  Mendl", "Nils  Meyer", "Andrea  Nobile", "Ibrahim  Ouda", "Marcello  Pivanti", "Dirk  Pleiter", "Manfred  Ries", "Andreas  Sch\u00e4fer", "Heiko  Schick", "Sebastiano Fabio Schifano", "Hubert  Simma", "Stefan  Solbrig", "Thomas  Streuer", "Karl-Heinz  Sulanke", "Raffaele  Tripiccione", "J\u00f6rg-Stephan  Vogt", "Tilo  Wettig", "Frank  Winter"], "year": 2009, "n_citations": 31}
{"id": 6365409, "s2_id": "80f91416fe4ccd67509a5494268114c763cc8f37", "title": "FPGA Acceleration of Sequence Alignment: A Survey", "abstract": "Genomics is changing our understanding of humans, evolution, diseases, and medicines to name but a few. As sequencing technology is developed collecting DNA sequences takes less time thereby generating more genetic data every day. Today the rate of generating genetic data is outpacing the rate of computation power growth. Current sequencing machines can sequence 50 humans genome per day; however, aligning the read sequences against a reference genome and assembling the genome will take 1300 CPU hours. The main step in constructing the genome is aligning the reads against a reference genome. Numerous accelerators have been proposed to accelerate the DNA alignment process. Providing massive parallelism, FPGA-based accelerators have shown great performance in accelerating DNA alignment algorithms. Additionally, FPGA-based accelerators provide better energy efficiency than general-purpose processors. In this survey, we introduce three main DNA alignment algorithms and FPGA-based implementation of these algorithms to accelerate the DNA alignment. We also, compare these three alignment categories and show how accelerators are developing during the time.", "venue": "ArXiv", "authors": ["Sahand  Salamat", "Tajana  Simunic"], "year": 2020, "n_citations": 2}
{"id": 6378870, "s2_id": "612fd0daee47ee82a0b4485098774545016461fc", "title": "Rethinking Floating Point Overheads for Mixed Precision DNN Accelerators", "abstract": "In this paper, we propose a mixed-precision convolution unit architecture which supports different integer and floating point (FP) precisions. The proposed architecture is based on low-bit inner product units and realizes higher precision based on temporal decomposition. We illustrate how to integrate FP computations on integer-based architecture and evaluate overheads incurred by FP arithmetic support. We argue that alignment and addition overhead for FP inner product can be significant since the maximum exponent difference could be up to 58 bits, which results into a large alignment logic. To address this issue, we illustrate empirically that at least 8 bits of alignment logic are required to maintain inference accuracy. We present novel optimizations based on the above observations to reduce the FP arithmetic hardware overheads. Our empirical results, based on simulation and hardware implementation, show significant reduction in FP16 overhead. Over a typical mixed precision implementation, the proposed architecture achieves area improvements of up to 25% in TFLOPS/mm and up to 46% in TOPS/mm with power efficiency improvements of up to 40% in TFLOPS/W and up to 63% in TOPS/W.", "venue": "ArXiv", "authors": ["Hamzah  Abdel-Aziz", "Ali  Shafiee", "Jong Hoon Shin", "Ardavan  Pedram", "Joseph H. Hassoun"], "year": 2021, "n_citations": 1}
{"id": 6380624, "s2_id": "eb4675aa98d54785c085f21251a7dd62d5b2ea52", "title": "Allowing Software Developers to Debug HLS Hardware", "abstract": "High-Level Synthesis (HLS) is emerging as a mainstream design methodology, allowing software designers to enjoy the benefits of a hardware implementation. Significant work has led to effective compilers that produce high-quality hardware designs from software specifications. However, in order to fully benefit from the promise of HLS, a complete ecosystem that provides the ability to analyze, debug, and optimize designs is essential. This ecosystem has to be accessible to software designers. This is challenging, since software developers view their designs very differently than how they are physically implemented on-chip. Rather than individual sequential lines of code, the implementation consists of gates operating in parallel across multiple clock cycles. In this paper, we report on our efforts to create an ecosystem that allows software designers to debug HLS-generated circuits in a familiar manner. We have implemented our ideas in a debug framework that will be included in the next release of the popular LegUp high-level synthesis tool.", "venue": "ArXiv", "authors": ["Jeffrey B. Goeders", "Steven J. E. Wilton"], "year": 2015, "n_citations": 10}
{"id": 6381856, "s2_id": "6f6acb433dd1932ff1fcd726fb1c59d6c2d7e6a1", "title": "Fault tolerant reversible logic synthesis: Carry look-ahead and carry-skip adders", "abstract": "Irreversible logic circuits dissipate heat for every bit of information that is lost. Information is lost when the input vector cannot be recovered from its corresponding output vector. Reversible logic circuit naturally takes care of heating because it implements only the functions that have one-to-one mapping between its input and output vectors. Therefore reversible logic design becomes one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient approaches for designing reversible fast adders that implement carry look-ahead and carry-skip logic. The proposed 16-bit high speed reversible adder will include IG gates for the realization of its basic building block. The IG gate is universal in the sense that it can be used to synthesize any arbitrary Boolean-functions. The IG gate is parity preserving, that is, the parity of the inputs matches the parity of the outputs. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. Therefore, the proposed high speed adders will have the inherent opportunity of detecting errors in its output side. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.", "venue": "2009 International Conference on Advances in Computational Tools for Engineering Applications", "authors": ["Md. Saiful Islam", "Muhammad Mahbubur Rahman", "Zerina  Begum", "Mohd. Zulfiquar Hafiz"], "year": 2009, "n_citations": 63}
{"id": 6382340, "s2_id": "9962fdf6b0914f2270f54c86297dc958f7c43214", "title": "Generating and evaluating application-specific hardware extensions", "abstract": "Modern platform-based design involves the application-specific extension of embedded processors to fit customer requirements. To accomplish this task, the possibilities offered by recent custom/extensible processors for tuning their instruction set and microarchitecture to the applications of interest have to be exploited. A significant factor often determining the success of this process is the utomation available in application analysis and custom instruction generation. \nIn this paper we present YARDstick, a design automation tool for custom processor development flows that focuses on generating and evaluating application-specific hardware extensions. YARDstick is a building block for ASIP development, integrating application analysis, custom instruction generation and selection with user-defined compiler intermediate representations. In a YARDstick-enabled environment, practical issues in traditional ASIP design are confronted efficiently; the exploration infrastructure is liberated from compiler and simulator idiosyncrasies, since the ASIP designer is empowered with the freedom of specifying the target architectures of choice and adding new implementations of analyses and custom instruction generation/selection methods. To illustrate the capabilities of the YARDstick approach, we present interesting exploration scenarios: quantifying the effect of machine-dependent compiler optimizations and the selection of the target architecture in terms of operation set and memory model on custom instruction generation/selection under different input/output constraints.", "venue": "ArXiv", "authors": ["Nikolaos  Kavvadias"], "year": 2014, "n_citations": 0}
{"id": 6382551, "s2_id": "a9f40e709bad9d55209ca5387043e6038cccf207", "title": "LLR-Based Successive-Cancellation List Decoder for Polar Codes With Multibit Decision", "abstract": "Due to their capacity-achieving property, polar codes have become one of the most attractive channel codes. To date, the successive-cancellation list (SCL) decoding algorithm is the primary approach that can guarantee outstanding error-correcting performance of polar codes. However, the hardware designs of the original SCL decoder have a large silicon area and a long decoding latency. Although some recent efforts can reduce either the area or latency of SCL decoders, these two metrics still cannot be optimized at the same time. This brief, for the first time, proposes a general log-likelihood-ratio (LLR) based SCL decoding algorithm with multibit decision. This new algorithm, referred to as LLR - 2K b-SCL, can determine 2K bits simultaneously for arbitrary K with the use of LLR messages. In addition, a reduced-data-width scheme is presented to reduce the critical path of the sorting block. Then, based on the proposed algorithm, a VLSI architecture of the new SCL decoder is developed. Synthesis results show that, for an example (1024, 512) polar code with list size 4, the proposed LLR - 2K b - SCL decoders achieve a significant reduction in both area and latency as compared to prior works. As a result, the hardware efficiencies of the proposed designs with K = 2 and 3 are 2.33 times and 3.32 times of that of the state-of-the-art works, respectively.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Bo  Yuan", "Keshab K. Parhi"], "year": 2017, "n_citations": 26}
{"id": 6383707, "s2_id": "345558c4b4305241b18d4113c810402084c412bc", "title": "Near-Precise Parameter Approximation for Multiple Multiplications on A Single DSP Block", "abstract": "A multiply-accumulate (MAC) operation is the main computation unit for DSP applications. DSP blocks are one of the efficient solutions to implement MACs in FPGA\u2019s. However, since the DSP blocks have wide multiplier and adder blocks, MAC operations using low bit-length parameters lead to an underutilization problem. Hence, an efficient approximation technique is introduced. The technique includes manipulation and approximation of the low bit-length fixed-point parameters based upon a Single DSP Multiple Multiplication (SDMM) execution. The SDMM changes the traditional MAC implementation in the DSP block by separating multiplication and accumulation operations. While the accumulator hardware available in the DSP block is used for multiple parameter multiplication, parallel LUTs are employed for the accumulation part of the MAC operation. The accuracy of the developed optimization technique was evaluated for different CNN weight bit precisions using the Alexnet and VGG-16 networks and the Tiny ImageNet dataset. The optimization can be implemented without loss of accuracy in almost all cases, while it causes slight accuracy losses in a few cases. Through these optimizations, the SDMM is performed at the cost of a small hardware overhead. For example, a single DSP block executes 3 8-bit fixed-point parameter multiplications. As a result of our optimizations, the parameters are represented in a different format on off-chip memory, providing up to 33% compression without any hardware cost. The compression rate can be further increased by up to 97% when used in conjunction with other compression methods for the VGG-16. Reaching this compression rate requires extra hardware cost. A prototype systolic array architecture was implemented employing our optimizations on a Xilinx Zynq FPGA. It reduced the number of DSP blocks by 66.6%, 75%, and 83.3% for 8, 6, and 4-bit input variables, respectively.", "venue": "IEEE Transactions on Computers", "authors": ["Ercan  Kalali", "Rene van Leuken"], "year": 2021, "n_citations": 0}
{"id": 6385934, "s2_id": "32f6e3b8f0bdbbe14b2574cad492fd26e5225d28", "title": "In-Field Logic Repair of Deep Sub-Micron CMOS Processors", "abstract": "Ultra Deep-Sub-Micron CMOS chips have to function correctly and reliably, not only during their early post-fabrication life, but also for their entire life span. In this paper, we present an architectural-level in-field repair technique. The key idea is to trade area for reliability by adding repair features to the system while keeping the power and the performance overheads as low as possible. In the case of permanent faults, spare blocks will replace the faulty blocks on the fly. Meanwhile by shutting down the main logic blocks, partial threshold voltage recovery can be achieved which will alleviate the ageing-related delays and timing issues. The technique can avoid fatal shut-downs in the system and will decrease the down-time, hence the availability of such a system will be preserved. We have implemented the proposed idea on a pipelined processor core using a conventional ASIC design flow. The simulation results show that by tolerating about 70% area overhead and less than 18% power overhead we can dramatically increase the reliability and decrease the downtime of the processor.", "venue": "ArXiv", "authors": ["Massoud Mokhtarpour Ghahroodi", "Mark  Zwolinski"], "year": 2015, "n_citations": 0}
{"id": 6391934, "s2_id": "be6b8fdaf8265a632395b9310c76695a9a4da28e", "title": "TA-LRW: A Replacement Policy for Error Rate Reduction in STT-MRAM Caches", "abstract": "As technology process node scales down, on-chip SRAM caches lose their efficiency because of their low scalability, high leakage power, and increasing rate of soft errors. Among emerging memory technologies, <italic><inline-formula><tex-math notation=\"LaTeX\">$Spin$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq1-2875439.gif\"/></alternatives></inline-formula></italic>-<italic><inline-formula><tex-math notation=\"LaTeX\">$Transfer\\; Torque\\; Magnetic\\; RAM$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq2-2875439.gif\"/></alternatives></inline-formula></italic> (STT-MRAM) is known as the most promising replacement for SRAM-based cache memories. The main advantages of STT-MRAM are its non-volatility, near-zero leakage power, higher density, soft-error immunity, and higher scalability. Despite these advantages, high error rate in STT-MRAM cells due to <italic><inline-formula><tex-math notation=\"LaTeX\">$retention\\; failure$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq3-2875439.gif\"/></alternatives></inline-formula></italic>, <italic><inline-formula><tex-math notation=\"LaTeX\">$write\\; failure$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq4-2875439.gif\"/></alternatives></inline-formula></italic>, and <italic><inline-formula><tex-math notation=\"LaTeX\">$read\\; disturbance$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq5-2875439.gif\"/></alternatives></inline-formula></italic> threatens the reliability of cache memories built upon STT-MRAM technology. The error rate is significantly increased in higher temperature, which further affects the reliability of STT-MRAM-based cache memories. The major source of heat generation and temperature increase in STT-MRAM cache memories is write operations, which are managed by cache <italic><inline-formula><tex-math notation=\"LaTeX\">$replacement\\; policy$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq6-2875439.gif\"/></alternatives></inline-formula></italic>. To the best of our knowledge, none of previous studies have attempted to mitigate heat generation and high temperature of STT-MRAM cache memories using replacement policy. In this paper, we first analyze the cache behavior in conventional <italic><inline-formula><tex-math notation=\"LaTeX\">$Least$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq7-2875439.gif\"/></alternatives></inline-formula></italic>-<italic><inline-formula><tex-math notation=\"LaTeX\">$Recently\\; Used$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq8-2875439.gif\"/></alternatives></inline-formula></italic> (LRU) replacement policy and demonstrate that the majority of consecutive write operations (more than 66 percent) are committed to adjacent cache blocks. These adjacent write operations cause accumulated heat and increased temperature, which significantly increase the cache error rate. To eliminate heat accumulation and the adjacency of consecutive writes, we propose a cache replacement policy, named <italic><inline-formula><tex-math notation=\"LaTeX\">$Thermal$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq9-2875439.gif\"/></alternatives></inline-formula></italic>-<italic><inline-formula><tex-math notation=\"LaTeX\">$Aware\\; Least$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq10-2875439.gif\"/></alternatives></inline-formula></italic>-<italic><inline-formula><tex-math notation=\"LaTeX\">$Recently\\; Written$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq11-2875439.gif\"/></alternatives></inline-formula></italic> (TA-LRW), to smoothly distribute the generated heat by conducting consecutive write operations in distant cache blocks. TA-LRW guarantees the distance of at least three blocks for each two consecutive write operations in an 8-way associative cache. This distant write scheme reduces the temperature-induced error rate by 94.8 percent, on average, compared with the conventional LRU policy, which results in 6.9x reduction in cache error rate. The implementation cost and complexity of TA-LRW is as low as <italic><inline-formula><tex-math notation=\"LaTeX\">$First$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq12-2875439.gif\"/></alternatives></inline-formula></italic>-<italic><inline-formula><tex-math notation=\"LaTeX\">$In,\\; First$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq13-2875439.gif\"/></alternatives></inline-formula></italic>-<italic><inline-formula><tex-math notation=\"LaTeX\">$Out$</tex-math><alternatives><inline-graphic xlink:href=\"asadi-ieq14-2875439.gif\"/></alternatives></inline-formula></italic> (FIFO) policy while providing a near-LRU performance, having the advantages of both replacement policies. The significantly reduced error rate is achieved by imposing only 2.3 percent performance overhead compared with the LRU policy.", "venue": "IEEE Transactions on Computers", "authors": ["Elham  Cheshmikhani", "Hamed  Farbeh", "Seyed Ghassem Miremadi", "Hossein  Asadi"], "year": 2019, "n_citations": 19}
{"id": 6392052, "s2_id": "120fc89c5db4718a7087279067d6816d44e7322b", "title": "Low Cost PC Based Real Time Data Logging System Using PCs Parallel Port For Slowly Varying Signals", "abstract": "A low cost PC based real time data logging system can be used in the laboratories for the measurement, monitoring and storage of the data for slowly varying signals in science and engineering stream. This can be designed and interfaced to the PCs Parallel Port, which is common to all desktop computers or Personal Computers (PCs). By the use of this data logging system one can monitor, measure and store data for slowly varying signals, which is hard to visualise the signal waveforms by ordinary CRO (Cathode Ray Oscilloscope) and DSO (Digital Storage Oscilloscope). The data so stored can be used for further study and analysis. It can be used for a wide range of applications to monitor and store data of temperature, humidity, light intensity, ECG signals etc. with proper signal conditioning circuitry.", "venue": "ArXiv", "authors": ["N. Monoranjan Singh", "K. C. Sarma"], "year": 2012, "n_citations": 9}
{"id": 6393926, "s2_id": "97edc231c5ffa1f3c9c5c6cdb81a640f97373561", "title": "Block-matching in FPGA", "abstract": "Block-matching and 3D filtering (BM3D) is an image denoising algorithm that works in two similar steps. Both of these steps need to perform grouping by block-matching. We implement the block-matching in an FPGA, leveraging its ability to perform parallel computations. Our goal is to enable other researchers to use our solution in the future for real-time video denoising in video cameras that use FPGAs (such as the AXIOM Beta).", "venue": "ArXiv", "authors": ["Rafael Pizarro Solar", "Michal  Pleskowicz"], "year": 2020, "n_citations": 0}
{"id": 6395228, "s2_id": "bb876adcd9efdb5c3caccb07c74b02f8caa2c6c1", "title": "Virtualized Logical Qubits: A 2.5D Architecture for Error-Corrected Quantum Computing", "abstract": "Current, near-term quantum devices have shown great progress in the last several years culminating recently with a demonstration of quantum supremacy. In the medium-term, however, quantum machines will need to transition to greater reliability through error correction, likely through promising techniques like surface codes which are well suited for near-term devices with limited qubit connectivity. We discover quantum memory, particularly resonant cavities with transmon qubits arranged in a 2.5D architecture, can efficiently implement surface codes with substantial hardware savings and performance/fidelity gains. Specifically, we virtualize logical qubits by storing them in layers of qubit memories connected to each transmon.Surprisingly, distributing each logical qubit across many memories has a minimal impact on fault tolerance and results in substantially more efficient operations. Our design permits fast transversal application of CNOT operations between logical qubits sharing the same physical address (same set of cavities) which are 6x faster than standard lattice surgery CNOTs. We develop a novel embedding which saves approximately 10x in transmons with another 2x savings from an additional optimization for compactness.Although qubit virtualization pays a 10x penalty in serialization, advantages in the transversal CNOT and in area efficiency result in fault-tolerance and performance comparable to conventional 2D transmon-only architectures. Our simulations show our system can achieve fault tolerance comparable to conventional two-dimensional grids while saving substantial hardware. Furthermore, our architecture can produce magic states at 1.22x the baseline rate given a fixed number of transmon qubits. This is a critical benchmark for future fault-tolerant quantum computers as magic states are essential and machines will spend the majority of their resources continuously producing them. This architecture substantially reduces the hardware requirements for fault-tolerant quantum computing and puts within reach a proof-of-concept experimental demonstration of around 10 logical qubits, requiring only 11 transmons and 9 attached cavities in total.", "venue": "2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)", "authors": ["Casey  Duckering", "Jonathan M. Baker", "David I. Schuster", "Frederic T. Chong"], "year": 2020, "n_citations": 1}
{"id": 6401354, "s2_id": "7c14bdb8e311281efb4372c1ee47984e7684e050", "title": "An Efficient I/O Architecture for RAM-Based Content-Addressable Memory on FPGA", "abstract": "Despite the impressive search rate of one key per clock cycle, the update stage of a random-access-memory-based content-addressable-memory (RAM-based CAM) always suffers high latency. Two primary causes of such latency include: 1) the compulsory erasing stage along with the writing stage and 2) the major difference in data width between the RAM-based CAM (e.g., 8-bit width) and the modern systems (e.g., 256-bit width). This brief, therefore, proposes an efficient input/output (I/O) architecture of RAM-based binary CAM (RCAM) for low-latency update. To achieve this goal, three techniques, namely centralized erase RAM, bit-sliced, and hierarchical-partitioning, are proposed to eliminate the latency of the erasing stage, as well as to allow RCAM to exploit the bandwidth of modern systems effectively. Several RCAMs, whose data width ranges from 8 bits to 64 bits, were integrated into a 256-bit system for the evaluation. The experimental results in an Intel Arria V 5ASTFD5 field-programmable gate array prove that, at 100 MHz, the proposed designs achieve at least 9.6 times higher I/O efficiency as compared to the traditional RCAM.", "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs", "authors": ["Xuan-Thuan  Nguyen", "Trong-Thuc  Hoang", "Hong-Thu  Nguyen", "Katsumi  Inoue", "Cong-Kha  Pham"], "year": 2019, "n_citations": 5}
{"id": 6404208, "s2_id": "18bf4edb33545b406fd0c40255e3a42bd14655f2", "title": "Information theory as a means of determining the main factors affecting the processors architecture", "abstract": "\u0412 \u0440\u0430\u0431\u043e\u0442\u0435 \u0438\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043e\u0432 \u0437\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 \u0434\u0435\u0441\u044f\u0442\u0438\u043b\u0435\u0442\u0438\u044f \u0441 \u0446\u0435\u043b\u044c\u044e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u043b\u0438\u044f\u044e\u0449\u0438\u0445 \u043d\u0430 \u043d\u0435\u0433\u043e \u0444\u0430\u043a\u0442\u043e\u0440\u043e\u0432. \u041e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0441\u0430\u043c\u0438 \u0444\u0430\u043a\u0442\u043e\u0440\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0431\u0443\u0434\u0443\u0449\u0438\u0445 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043e\u043a. \u0414\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u044f \u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0421\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043e\u0432 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438, \u043e\u043f\u0438\u0440\u0430\u044f\u0441\u044c \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043d\u0430 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0438\u0445 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b.\n In this article we are investigating the computers development process in the past decades in order to identify the factors that influence it the most. We describe such factors and use them to predict the direction of further development. To solve these problems, we use the concept of the Computer Capacity, which allows us to estimate the performance of computers theoretically, relying only on the description of its architecture.", "venue": "\u0412\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438", "authors": ["\u0410\u043d\u0442\u043e\u043d \u0410\u043d\u0434\u0440\u0435\u0435\u0432\u0438\u0447 \u0420\u0430\u043a\u0438\u0442\u0441\u043a\u0438\u0439", "\u0411\u043e\u0440\u0438\u0441 \u042f\u043a\u043e\u0432\u043b\u0435\u0432\u0438\u0447 \u0420\u044f\u0431\u043a\u043e"], "year": 2021, "n_citations": 0}
{"id": 6406556, "s2_id": "4fa6f029af3916a5a25336f28aa8ac00927db6e9", "title": "Fast statistical timing analysis for circuits with Post-Silicon Tunable clock buffers", "abstract": "Post-Silicon Tunable (PST) clock buffers are widely used in high performance designs to counter process variations. By allowing delay compensation between consecutive register stages, PST buffers can effectively improve the yield of digital circuits. To date, the evaluation of manufacturing yield in the presence of PST buffers is only possible using Monte Carlo simulation. In this paper, we propose an alternative method based on graph transformations, which is much faster, more than 1000 times, and computes a parametric minimum clock period. It also identifies the gates which are most critical to the circuit performance, therefore enabling a fast analysis-optimization flow.", "venue": "2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Bing  Li", "Ning  Chen"], "year": 2011, "n_citations": 15}
{"id": 6409532, "s2_id": "15550b1c92a8cc911ec0819d741fcd7894ae8547", "title": "Scale-Out Processors & Energy Efficiency", "abstract": "Scale-out workloads like media streaming or Web search serve millions of users and operate on a massive amount of data, and hence, require enormous computational power. As the number of users is increasing and the size of data is expanding, even more computational power is necessary for powering up such workloads. Data centers with thousands of servers are providing the computational power necessary for executing scale-out workloads. As operating data centers requires enormous capital outlay, it is important to optimize them to execute scale-out workloads efficiently. Server processors contribute significantly to the data center capital outlay, and hence, are a prime candidate for optimizations. While data centers are constrained with power, and power consumption is one of the major components contributing to the total cost of ownership (TCO), a recently-introduced scale-out design methodology optimizes server processors for data centers using performance per unit area. In this work, we use a more relevant performance-per-power metric as the optimization criterion for optimizing server processors and reevaluate the scale-out design methodology. Interestingly, we show that a scale-out processor that delivers the maximum performance per unit area, also delivers the highest performance per unit power.", "venue": "ArXiv", "authors": ["Pouya  Esmaili-Dokht", "Mohammad  Bakhshalipour", "Behnam  Khodabandeloo", "Pejman  Lotfi-Kamran", "Hamid  Sarbazi-Azad"], "year": 2018, "n_citations": 7}
{"id": 6410508, "s2_id": "3d8ce8ee81456fdaf8115ef08b5d91d066033a28", "title": "PIM-DRAM: Accelerating Machine Learning Workloads Using Processing in Commodity DRAM", "abstract": "Deep Neural Networks (DNNs) have transformed the field of machine learning (ML) and are widely deployed in many applications involving image, video, speech and natural language processing. The increasing compute demands of DNNs have been widely addressed through Graphics Processing Units (GPUs) and specialized accelerators. However, as model sizes grow, these von Neumann architectures require very high off-chip memory bandwidth to keep the processing elements utilized, as a majority of the data resides in the main memory. Processing in memory is actively being explored as a promising solution to the memory wall bottleneck for ML workloads. In this work, we propose a new DRAM-based processing-in-memory (PIM) multiplication primitive coupled with intra-bank accumulation to accelerate matrix vector multiply operations in ML workloads. The proposed multiplication primitive adds <1% area overhead and does not require any change to the DRAM peripherals. Subsequently, we design a DRAM-based PIM architecture (PIM-DRAM) and a data mapping scheme for executing DNNs on the proposed architecture. System evaluations performed on the AlexNet, VGG16 and ResNet18 DNNs show that the proposed architecture, mapping, and data flow can provide up to 19.5x speedup over an NVIDIA Titan Xp GPU, highlighting the potential of processing in memory for future generations of DNN hardware.", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "authors": ["Sourjya  Roy", "Mustafa  Ali", "Anand  Raghunathan"], "year": 2021, "n_citations": 0}
{"id": 6411597, "s2_id": "0eb982d25b0a8698e09ce730c20686ebdd1e63af", "title": "A practical approach for circuit routing on dynamic reconfigurable devices", "abstract": "Management of communication by on-line routing in new FPGAs with a large amount of logic resources and partial re configurability is a new challenging problem. A network-on-chip (NoC) typically uses packet routing mechanism, which has often unsafe data transfers, and network interface overhead. In this paper, circuit routing for such dynamic NoCs is investigated, and a practical 1-dimensional network with an efficient routing algorithm is proposed and implemented. Also, this concept has been extended to the 2-dimensional case. The implementation results show the low area overhead and high performance of this network.", "venue": "16th IEEE International Workshop on Rapid System Prototyping (RSP'05)", "authors": ["Ali  Ahmadinia", "Christophe  Bobda", "Ji  Ding", "Mateusz  Majer", "J\u00fcrgen  Teich", "S\u00e1ndor P. Fekete", "Jan van der Veen"], "year": 2005, "n_citations": 60}
{"id": 6414987, "s2_id": "f19229819a583b832f5652df6d8650585998e434", "title": "A High-Performance Solid-State Disk with Double-Data-Rate NAND Flash Memory", "abstract": "We propose a novel solid-state disk (SSD) architecture that utilizes a double-data-rate synchronous NAND flash interface for improving read and write performance. Unlike the conventional design, the data transfer rate in the proposed design is doubled in harmony with synchronous signaling. The new architecture does not require any extra pins with respect to the conventional architecture, thereby guaranteeing backward compatibility. For performance evaluation, we simulated various SSD designs that adopt the proposed architecture and measured their performance in terms of read/write bandwidths and energy consumption. Both NAND flash cell types, namely single-level cells (SLCs) and multi-level cells (MLCs), were considered. In the experiments using SLC-type NAND flash chips, the read and write speeds of the proposed architecture were 1.65-2.76 times and 1.09-2.45 times faster than those of the conventional architecture, respectively. Similar improvements were observed for the MLC-based architectures tested. It was particularly effective to combine the proposed architecture with the way-interleaving technique that multiplexes the data channel between the controller and each flash chip. For a reasonably high degree of way interleaving, the read/write performance and the energy consumption of our approach were notably better than those of the conventional design.", "venue": "ArXiv", "authors": ["Eui-Young  Chung", "Chang-Il  Son", "Kwanhu  Bang", "Dong  Kim", "Soong-Mann  Shin", "Sungroh  Yoon"], "year": 2015, "n_citations": 2}
{"id": 6415826, "s2_id": "c29dea0aa1d32d44dd6471029c7c10113ecaeb60", "title": "Low-power Programmable Processor for Fast Fourier Transform Based on Transport Triggered Architecture", "abstract": "This paper describes a low-power processor tailored for fast Fourier transform computations where transport triggering template is exploited. The processor is software-programmable while retaining an energy-efficiency comparable to existing fixed-function implementations. The power savings are achieved by compressing the computation kernel into one instruction word. The word is stored in an instruction loop buffer, which is more power-efficient than regular instruction memory storage. The processor supports all power-of-two FFT sizes from 64 to 16384 and given 1 mJ of energy, it can compute 20916 transforms of size 1024.", "venue": "ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Jakub  Z\u00e1dn\u00edk", "Jarmo  Takala"], "year": 2019, "n_citations": 1}
{"id": 6416050, "s2_id": "d72872fc227c8a3f134803fe0468b488065e4ca9", "title": "An Efficient Reconfigurable FIR Digital Filter Using Modified Distribute Arithmetic Technique", "abstract": "This paper provides modified Distributed Arithmetic based technique to compute sum of products saving appreciable number of Multiply And accumulation blocks and this consecutively reduces circuit size. In this technique multiplexer based structure is used to reuse the blocks so as to reduce the required memory locations. In this technique a Carry Look Ahead based adder tree is used to have better area-delay product. Designing of FIR filter is done using VHDL and synthesized using Xilinx 12.2 synthesis tool and ISIM simulator. The power analysis is done using Xilinx Xpower analyzer. The proposed structure requires nearly 42% less cells, 40% less LUT flip-flop pairs used, and also 2% less power compared with existing structure.", "venue": "ArXiv", "authors": ["Naveen S. Naik", "Kiran A. Gupta"], "year": 2017, "n_citations": 4}
{"id": 6416169, "s2_id": "db63342d30079e90c570988920623f29d6e9d354", "title": "Designing Efficient and High-Performance AI Accelerators With Customized STT-MRAM", "abstract": "We demonstrate the design of efficient and high-performance artificial intelligence (AI)/deep learning accelerators with customized spin transfer torque (STT)-MRAM (STT-MRAM) and a reconfigurable core. Based on model-driven detailed design space exploration, we present the design methodology of an innovative scratchpad-assisted on-chip STT-MRAM-based buffer system for high-performance accelerators. Using analytically derived expression of memory occupancy time of AI model weights and activation maps, the volatility of STT-MRAM is adjusted with process and temperature variation aware scaling of thermal stability factor to optimize the retention time, energy, read/write latency, and area of STT-MRAM. From the analysis of AI workloads and accelerator implementation in 14-nm technology, we verify the efficacy of our AI accelerator with STT-MRAM (STT-AI). Compared to an SRAM-based implementation, the STT-AI accelerator achieves 75% area and 3% power savings at isoaccuracy. Furthermore, with a relaxed bit error rate and negligible AI accuracy tradeoff, the designed STT-AI Ultra accelerator achieves 75.4% and 3.5% savings in area and power, respectively, over regular SRAM-based accelerators.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Kaniz  Mishty", "Mehdi  Sadi"], "year": 2021, "n_citations": 1}
{"id": 6416525, "s2_id": "782ef4b913ed4fb34be27bb4d3e7515119055c79", "title": "XNORBIN: A 95 TOp/s/W hardware accelerator for binary convolutional neural networks", "abstract": "Deploying state-of-the-art CNNs requires power-hungry processors and off-chip memory. This precludes the implementation of CNNs in low-power embedded systems. Recent research shows CNNs sustain extreme quantization, binarizing their weights and intermediate feature maps, thereby saving 8\u201332x memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount operations. We present XNORBIN, a flexible accelerator for binary CNNs with computation tightly coupled to memory for aggressive data reuse supporting even non-trivial network topologies with large feature map volumes. Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of 2.0TOp/s/MGE at 0.8 V.", "venue": "2018 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS)", "authors": ["Andrawes Al Bahou", "Geethan  Karunaratne", "Renzo  Andri", "Lukas  Cavigelli", "Luca  Benini"], "year": 2018, "n_citations": 29}
{"id": 6423298, "s2_id": "ccd94e90a7f34c1061e417788e0b0ddeb8f61a4f", "title": "Boosting XML Filtering with a Scalable FPGA-based Architecture", "abstract": "growing amount of XML encoded data exchanged over the In- ternet increases the importance of XML based publish-subscribe (pub-sub) and content based routing systems. The input in such systems typically consists of a stream of XML documents and a set of user subscriptions expressed as XML queries. The pub-sub system then filters the published documents and passes them t o the subscribers. Pub-sub systems are characterized by very high input ratios, therefore the processing time is critical. In this p aper we propose a \"pure hardware\" based solution, which utilizes XPath query blocks on FPGA to solve the filtering problem. By utiliz - ing the high throughput that an FPGA provides for parallel pro- cessing, our approach achieves drastically better through put than the existing software or mixed (hardware/software) architectures. The XPath queries (subscriptions) are translated to regular expres- sions which are then mapped to FPGA devices. By introducing stacks within the FPGA we are able to express and process a wide range of path queries very efficiently, on a scalable environ ment. Moreover, the fact that the parser and the filter processing a re per- formed on the same FPGA chip, eliminates expensive communi- cation costs (that a multi-core system would need) thus enabling very fast and efficient pipelining. Our experimental evalua tion re- veals more than one order of magnitude improvement compared to traditional pub/sub systems.", "venue": "CIDR 2009", "authors": ["Abhishek  Mitra", "Marcos R. Vieira", "Petko  Bakalov", "Walid A. Najjar", "Vassilis J. Tsotras"], "year": 2009, "n_citations": 48}
{"id": 6424023, "s2_id": "b236960ba2e34a0d9d5a698578b7eb82cb40122c", "title": "A Scalable, Low-Overhead Finite-State Machine Overlay for Rapid FPGA Application Development", "abstract": "Productivity issues such as lengthy compilation and limited code reuse have restricted usage of field-programmable gate arrays (FPGAs), despite significant technical advantages. Recent work into overlays -- virtual coarse-grained architectures implemented atop FPGAs -- has aimed to address these concerns through abstraction, but have mostly focused on pipelined applications with minimal control requirements. Although research has introduced overlays for finite-state machines, those architectures suffer from limited scalability and flexibility, which we address with a new overlay architecture using memory decomposition on transitional logic. Although our overlay provides modest average improvements of 15% to 29% fewer lookup tables for individual finite-state machines, for the more common usage of an overlay supporting different finite-state machines, our overlay achieves a 77% to 99% reduction in lookup tables. In addition, our overlay reduces compilation time to tenths of a second to enable rapid iterative-development methodologies.", "venue": "ArXiv", "authors": ["David  Wilson", "Greg  Stitt"], "year": 2017, "n_citations": 1}
{"id": 6424930, "s2_id": "ae355309df8c0786bc47e3de1cc1a2d717dd4a6f", "title": "Validating Simplified Processor Models in Architectural Studies", "abstract": "Cycle-accurate software simulation of multicores with complex microarchitectures is often excruciatingly slow. People use simplified core models to gain simulation speed. However, a persistent question is to what extent the results derived from a simplified core model can be used to characterize the behavior of a real machine. \nWe propose a new methodology of validating simplified simulation models, which focuses on the trends of metric values across benchmarks and architectures, instead of errors of absolute metric values. To illustrate this methodology, we conduct a case study using an FPGA-accelerated cycle-accurate full system simulator. We evaluated three cache replacement polices on a 10-stage in-order core model, and then re-conducted all the experiments by substituting a 1-IPC core model for the 10-stage core model. We found that the 1-IPC core model generally produces qualitatively the same results as the accurate core model except for a few mismatches. We argue that most observed mismatches were either indistinguishable from experimental noise or corresponded to the cases where the policy differences even in the accurate model showed inconclusive results. We think it is fair to use simplified core models to study a feature once the influence of the simplification is understood. Additional studies on branch predictors and scaling properties of multithread benchmarks reinforce our argument. However, the validation of a simplified model requires a detailed cycle-accurate model!", "venue": "ArXiv", "authors": ["Sizhuo  Zhang", "Andrew  Wright", "Daniel  S\u00e1nchez", "Arvind"], "year": 2016, "n_citations": 2}
{"id": 6428320, "s2_id": "2393478369b748b9dae91f48f8fef5c5dab63869", "title": "DPA on quasi delay insensitive asynchronous circuits: formalization and improvement", "abstract": "The paper formally specifies a flow devoted to the design of differential power analysis (DPA) resistant QDI (quasi delay insensitive) asynchronous circuits. The paper first proposes a formal modeling of the electrical signature of QDI asynchronous circuits. The DPA is then applied to the formal model in order to identify the source of leakage of this type of circuit. Finally, a complete design flow is specified to minimize the information leakage. The relevancy and efficiency of the approach is demonstrated using the design of an AES crypto-processor.", "venue": "Design, Automation and Test in Europe", "authors": ["G. Fraidy Bouesse", "Marc  Renaudin", "Sophie  Dumont", "Fabien  Germain"], "year": 2005, "n_citations": 43}
{"id": 6432177, "s2_id": "deab8e1ca3a10528a20c045c1537f0d943ab3958", "title": "A Fault Tolerance Improved Majority Voter for TMR System Architectures", "abstract": "For digital system designs, triple modular redundancy (TMR), which is a 3-tuple version of N-modular redundancy is widely preferred for many mission-control and safety-critical applications. The TMR scheme involves two-times duplication of the simplex system hardware, with a majority voter ensuring correctness provided at least two out of three copies of the system remain operational. Thus the majority voter plays a pivotal role in ensuring the correct operation of the system. The fundamental assumption implicit in the TMR scheme is that the majority voter does not become faulty, which may not hold well for implementations based on latest technology nodes with dimensions of the order of just tens of nanometers. To overcome the drawbacks of the classical majority voter some new voter designs were put forward in the literature with the aim of enhancing the fault tolerance. However, these voter designs generally ensure the correct system operation in the presence of either a faulty function module or the faulty voter, considered only in isolation. Since multiple faults may no longer be excluded in the nanoelectronics regime, simultaneous fault occurrences on both the function module and the voter should be considered, and the fault tolerance of the voters have to be analyzed under such a scenario. In this context, this article proposes a new fault-tolerant majority voter which is found to be more robust to faults than the existing voters in the presence of faults occurring internally and/or externally to the voter. Moreover, the proposed voter features less power dissipation, delay, and area metrics based on the simulation results obtained by using a 32/28nm CMOS process.", "venue": "ArXiv", "authors": ["P.  Balasubramanian", "K.  Prasad", "Nikos E. Mastorakis"], "year": 2016, "n_citations": 36}
{"id": 6437632, "s2_id": "4cb1d78ab12c74e905b80c1af6ce674321e4178f", "title": "Exact Cover with Light", "abstract": "We suggest a new optical solution for solving the YES/NO version of the Exact Cover problem by using the massive parallelism of light. The idea is to build an optical device which can generate all possible solutions of the problem and then to pick the correct one. In our case the device has a graph-like representation and the light is traversing it by following the routes given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible covers (exact or not) of the given set. For selecting the correct solution we assign to each item, from the set to be covered, a special integer number. These numbers will actually represent delays induced to light when it passes through arcs. The solution is represented as a subray arriving at a certain moment in the destination node. This will tell us if an exact cover does exist or not.", "venue": "New Generation Computing", "authors": ["Mihai  Oltean", "Oana  Muntean"], "year": 2008, "n_citations": 24}
{"id": 6448776, "s2_id": "1b9d80ce816fc4adaee5d69caa957f5fc1f144aa", "title": "Efficient On-Chip Multicast Routing based on Dynamic Partition Merging", "abstract": "Networks-on-chips (NoCs) have become the mainstream communication infrastructure for chip multiprocessors (CMPs) and many-core systems. The commonly used parallel applications and emerging machine learning-based applications involve a significant amount of collective communication patterns. In CMP applications, multicast is widely used in multithreaded programs and protocols for barrier/clock synchronization and cache coherence. Multicast routing plays an important role on the system performance of a CMP. Existing partition-based multicast routing algorithms all use static destination set partition strategy which lacks the global view of path optimization. In this paper, we propose an efficient Dynamic Partition Merging (DPM)-based multicast routing algorithm. The proposed algorithm divides the multicast destination set into partitions dynamically by comparing the routing cost of different partition merging options and selecting the merged partitions with lower cost. The simulation results of synthetic traffic and PARSEC benchmark applications confirm that the proposed algorithm outperforms the existing path-based routing algorithms. The proposed algorithm is able to improve up to 23% in average packet latency and 14% in power consumption against the existing multipath routing algorithm when tested in PARSEC benchmark workloads.", "venue": "2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)", "authors": ["Binayak  Tiwari", "Mei  Yang", "Yingtao  Jiang", "Xiaohang  Wang"], "year": 2020, "n_citations": 1}
{"id": 6450639, "s2_id": "ac2794063570989de72850cb56b7c7dd6d965a10", "title": "SparseTrain: Exploiting Dataflow Sparsity for Efficient Convolutional Neural Networks Training", "abstract": "Training Convolutional Neural Networks (CNNs) usually requires a large number of computational resources. In this paper, SparseTrain is proposed to accelerate CNN training by fully exploiting the sparsity. It mainly involves three levels of innovations: activation gradients pruning algorithm, sparse training dataflow, and accelerator architecture. By applying a stochastic pruning algorithm on each layer, the sparsity of back-propagation gradients can be increased dramatically without degrading training accuracy and convergence rate. Moreover, to utilize both natural sparsity (resulted from ReLU or Pooling layers) and artificial sparsity (brought by pruning algorithm), a sparse-aware architecture is proposed for training acceleration. This architecture supports forward and back-propagation of CNN by adopting 1-Dimensional convolution dataflow. We have built a cycle-accurate architecture simulator to evaluate the performance and efficiency based on the synthesized design with 14nm FinFET technologies. Evaluation results on AlexNet/ResNet show that SparseTrain could achieve about 2.7\u00d7 speedup and 2.2\u00d7 energy efficiency improvement on average compared with the original training process.", "venue": "2020 57th ACM/IEEE Design Automation Conference (DAC)", "authors": ["Pengcheng  Dai", "Jianlei  Yang", "Xucheng  Ye", "Xingzhou  Cheng", "Junyu  Luo", "Linghao  Song", "Yiran  Chen", "Weisheng  Zhao"], "year": 2020, "n_citations": 2}
{"id": 6460115, "s2_id": "a9075d76a02499a57ca263f63787a06b4e4cf0a2", "title": "Tolerating Soft Errors in Processor Cores Using CLEAR (Cross-Layer Exploration for Architecting Resilience)", "abstract": "We present cross-layer exploration for architecting resilience, a first of its kind framework which overcomes a major challenge in the design of digital systems that are resilient to reliability failures: achieve desired resilience targets at minimal costs (energy, power, execution time, and area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, and algorithm). This is also referred to as cross-layer resilience. In this paper, we focus on radiation-induced soft errors in processor cores. We address both single-event upsets and single-event multiple upsets in terrestrial environments. Our framework automatically and systematically explores the large space of comprehensive resilience techniques and their combinations across various layers of the system stack (586 cross-layer combinations in this paper), derives cost-effective solutions that achieve resilience targets at minimal costs, and provides guidelines for the design of new resilience techniques. Our results demonstrate that a carefully optimized combination of circuit-level hardening, logic-level parity checking, and micro-architectural recovery provides a highly cost-effective soft error resilience solution for general-purpose processor cores. For example, a  $50 {\\times }$  improvement in silent data corruption (SDC) rate is achieved at only 2.1% energy cost for an out-of-order core (6.1% for an in-order core) with no speed impact. However, (application-aware) selective circuit-level hardening alone, guided by a thorough analysis of the effects of soft errors on application benchmarks, provides a cost-effective soft error resilience solution as well (with ~1% additional energy cost for a  $50{\\times }$  improvement in SDC rate).", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": ["Eric  Cheng", "Shahrzad  Mirkhani", "Lukasz G. Szafaryn", "Chen-Yong  Cher", "Hyungmin  Cho", "Kevin  Skadron", "Mircea R. Stan", "Klas  Lilja", "Jacob A. Abraham", "Pradip  Bose", "Subhasish  Mitra"], "year": 2018, "n_citations": 13}
{"id": 6462844, "s2_id": "865851fd107850c92e0a5cce6fba58f396913c6c", "title": "Dynamic Lockstep Processors for Applications with Functional Safety Relevance", "abstract": "Lockstep processing is a recognized technique for helping to secure functional-safety relevant processing against, for instance, single upset errors that might cause faulty execution of code. Lockstepping processors does however bind processing resources in a fashion not beneficial to architectures and applications that would benefit from multi-core/-processors. We propose a novel on-demand synchronizing of cores/processors for lock-step operation featuring post-processing resource release, a concept that facilitates the implementation of modularly redundant core/processor arrays. We discuss the fundamentals of the design and some implementation notes on work achieved to date.", "venue": "2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )", "authors": ["Hans Dermot Doran", "Timo  Lang"], "year": 2021, "n_citations": 0}
{"id": 6465514, "s2_id": "13fb624f219c7ac64308acfe7c79c77ae14af3ac", "title": "A Reconfigurable Vector Instruction Processor for Accelerating a Convection Parametrization Model on FPGAs", "abstract": "High Performance Computing (HPC) platforms allow scientists to model computationally intensive algorithms. HPC clusters increasingly use General-Purpose Graphics Processing Units (GPGPUs) as accelerators; FPGAs provide an attractive alternative to GPGPUs for use as co-processors, but they are still far from being mainstream due to a number of challenges faced when using FPGA-based platforms. Our research aims to make FPGA-based high performance computing more accessible to the scientific community. In this work we present the results of investigating the acceleration of a particular atmospheric model, Flexpart, on FPGAs. We focus on accelerating the most computationally intensive kernel from this model. The key contribution of our work is the architectural exploration we undertook to arrive at a solution that best exploits the parallelism available in the legacy code, and is also convenient to program, so that eventually the compilation of high-level legacy code to our architecture can be fully automated. We present the three different types of architecture, comparing their resource utilization and performance, and propose that an architecture where there are a number of computational cores, each built along the lines of a vector instruction processor, works best in this particular scenario, and is a promising candidate for a generic FPGA-based platform for scientific computation. We also present the results of experiments done with various configuration parameters of the proposed architecture, to show its utility in adapting to a range of scientific applications.", "venue": "ArXiv", "authors": ["Syed Waqar Nabi", "Saji  Hameed", "Wim  Vanderbauwhede"], "year": 2015, "n_citations": 1}
{"id": 6466014, "s2_id": "9069df4c9298da8ccdae1b04e156247133adb868", "title": "A Communication Model for Adaptive Service Provisioning in Hybrid Wireless Networks", "abstract": "Mobile entities with wireless links are able to form a mobile ad-hoc network. Such an infrastructureless network does not have to be administrated. However, self-organizing principles have to be applied to deal with upcoming problems, e.g. information dissemination. These kinds of problems are not easy to tackle, requiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks is arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could eliminate the need for any fixed infrastructure, has been damped. The goal is to overcome the limitations of pure ad-hoc networks by augmenting them with instant Internet access, e.g. via integration of UMTS respectively GSM links. However, this raises multiple questions at the technical as well as the organizational level. Motivated by characteristics of small-world networks that describe an efficient network even without central or organized design, this paper proposes to combine mobile ad-hoc networks and infrastructured networks to form hybrid wireless networks. One main objective is to investigate how this approach can reduce the costs of a permanent backbone link and providing in the same way the benefits of useful information from Internet connectivity or service providers. For the purpose of bridging between the different types of networks, an adequate middleware service is the focus of our investigation. This paper shows our first steps forward to this middleware by introducing the Injection Communication paradigm as principal concept.", "venue": "ArXiv", "authors": ["Matthias R. Brust", "Steffen  Rothkugel"], "year": 2007, "n_citations": 5}
{"id": 6469313, "s2_id": "e37ab8b3f9ad3e75055a9baa9960580aa3692ab3", "title": "Cycle accurate binary translation for simulation acceleration in rapid prototyping of SoCs", "abstract": "The application of a cycle accurate binary translator for rapid prototyping of SoCs is presented. This translator generates code to run on a rapid prototyping system consisting of a VLIW processor and FPGAs. The generated code is annotated with information that triggers cycle generation for the hardware in parallel with the execution of the translated program. The VLIW processor executes the translated program whereas the FPGAs contain the hardware for the parallel cycle generation and the bits interface that adapts the bits of the VLIW processor to the SoC bits of the emulated processor core.", "venue": "Design, Automation and Test in Europe", "authors": ["J\u00fcrgen  Schnerr", "Oliver  Bringmann", "Wolfgang  Rosenstiel"], "year": 2005, "n_citations": 25}
{"id": 6472153, "s2_id": "17cf6eaad65a3faede88e90479fd07a455aab158", "title": "Fast Thresholded SC-Flip Decoding of Polar Codes", "abstract": "SC-Flip (SCF) decoding algorithm shares the attention with the common polar code decoding approaches due to its low-complexity and improved error-correction performance. However, the inefficient criterion for locating the correct bit-flipping position in SCF decoding limits its improvements. Due to its improved bit-flipping criterion, Thresholded SCF (TSCF) decoding algorithm exhibits a superior error-correction performance and lower computational complexity than SCF decoding. However, the parameters of TSCF decoding depend on multiple channel and code parameters, and are obtained via Monte-Carlo simulations. Our main goal is to realize TSCF decoding as a practical polar decoder implementation. To this end, we first realize an approximated threshold value that is independent of the code parameters and precomputations. The proposed approximation has negligible error-correction performance degradation on the TSCF decoding. Then, we validate an alternative approach for forming a critical set that does not require precomputations, which also paves the way to the implementation of the Fast-TSCF decoder. Compared to the existing fast SCF implementations, the proposed Fast-TSCF decoder has 0.24 to 0.41 dB performance gain at frame error rate of $10^{-3}$, without any extra cost. Compared to the TSCF decoding, Fast-TSCF does not depend on precomputations and requires 87% fewer decoding steps. Finally, implementation results in TSMC 65nm CMOS technology show that the Fast-TSCF decoder is 20% and 82% more area-efficient than the state-of-the-art fast SCF and fast SC-List decoder architectures, respectively.", "venue": "ICC 2020 - 2020 IEEE International Conference on Communications (ICC)", "authors": ["Furkan  Ercan", "Warren J. Gross"], "year": 2020, "n_citations": 3}
{"id": 6482969, "s2_id": "bebb2377824a5792d9168f002caf8d6c4e8ba993", "title": "Optimal Layout Synthesis for Quantum Computing", "abstract": "Recent years have witnessed the fast development of quantum computing. Researchers around the world are eager to run larger and larger quantum algorithms that promise speedups impossible to any classical algorithm. However, the available quantum computers are still volatile and error-prone. Thus, layout synthesis, which transforms quantum programs to meet these hardware limitations, is a crucial step in the realization of quantum computing. In this paper, we present two synthesizers, one optimal and one approximate but nearly optimal. Although a few optimal approaches to this problem have been published, our optimal synthesizer explores a larger solution space, thus is optimal in a stronger sense. In addition, it reduces time and space complexity exponentially compared to some leading optimal approaches. The key to this success is a more efficient spacetime-based variable encoding of the layout synthesis problem as a mathematical programming problem. By slightly changing our formulation, we arrive at an approximate synthesizer that is even more efficient and outperforms some leading heuristic approaches, in terms of additional gate cost, by up to 100%, and also fidelity by up to 10x on a comprehensive set of benchmark programs and architectures. For a specific family of quantum programs named QAOA, which is deemed to be a promising application for near-term quantum computers, we further adjust the approximate synthesizer by taking commutation into consideration, achieving up to 75% reduction in depth and up to 65% reduction in additional cost compared to the tool used in a leading QAOA study.", "venue": "2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)", "authors": ["Bochen  Tan", "Jason  Cong"], "year": 2020, "n_citations": 24}
{"id": 6483573, "s2_id": "82cb568d6852e07cb57825c63a43c9a37ed1ab4e", "title": "TRISHUL: A single-pass optimal two-level inclusive data cache hierarchy selection process for real-time MPSoCs", "abstract": "Hitherto discovered approaches analyze the execution time of a real-time application on all the possible cache hierarchy setups to find the application specific optimal two-level inclusive data cache hierarchy to reduce cost, space and energy consumption while satisfying the time deadline in real-time Multi-Processor Systems on Chip (MPSoC). These brute-force like approaches can take years to complete. Alternatively, application's memory access trace driven crude estimation methods can find a cache hierarchy quickly by compromising the accuracy of results. In this article, for the first time, we propose a fast and accurate application's trace driven approach to find the optimal real-time application specific two-level inclusive data cache hierarchy. Our proposed approach \u201cTRISHUL\u201d predicts the optimal cache hierarchy performance first and then utilizes that information to find the optimal cache hierarchy quickly. TRISHUL can suggest a cache hierarchy, which has up to 128 times smaller size, up to 7 times faster compared to the suggestion of the state-of-the-art crude trace driven two-level inclusive cache hierarchy selection approach for the application traces analyzed.", "venue": "2013 18th Asia and South Pacific Design Automation Conference (ASP-DAC)", "authors": ["Mohammad Shihabul Haque", "Akash  Kumar", "Yajun  Ha", "Qiang  Wu", "Shaobo  Luo"], "year": 2013, "n_citations": 4}
{"id": 6483930, "s2_id": "a529b377140dccec50c7bb84aed68414f629b973", "title": "Dynamic FPGA Detection and Protection of Hardware Trojan: A Comparative Analysis", "abstract": "Hardware Trojan detection and protection is becoming more crucial as more untrusted third parties manufacture many parts of critical systems nowadays. The most common way to detect hardware Trojans is comparing the untrusted design with a golden (trusted) one. However, third-party intellectual properties (IPs) are black boxes with no golden IPs to trust. So, previous attempts to detect hardware Trojans will not work with third-party IPs. In this work, we present novel methods for Trojan protection and detection on field programmable gate arrays (FPGAs) without the need for golden chips. Presented methods work at runtime instead of test time. We provide a wide spectrum of Trojan detection and protection methods. While the simplest methods have low overhead and provide limited protection mechanisms, more sophisticated and costly techniques are introduced that can detect hardware Trojans and even clean up the system from infected IPs. Moreover, we study the cost of using the FPGA partial reconfiguration feature to get rid of infected IPs. In addition, we discuss the possibility to construct IP core certificate authority that maintains a centralized database of unsafe vendors and IPs. We show the practicality of the introduced schemes by implementing the different methodologies on FPGAs. Results show that simple methods present negligible overheads and as we try to increase security the delay and power overheads increase.", "venue": "ArXiv", "authors": ["Amr  Al-Anwar", "Mona A. Aboelnaga", "Yousra  Alkabani", "M. Watheq El-Kharashi", "Hassan  Bedour"], "year": 2017, "n_citations": 5}
{"id": 6484588, "s2_id": "e7cce43b29e982b0abd79f7540f96b176be03824", "title": "On the likelihood of multiple bit upsets in logic circuits", "abstract": "Soft errors have a significant impact on the circuit reliability at nanoscale technologies. At the architectural level, soft errors are commonly modeled by a probabilistic bit-flip model. In developing such abstract fault models, an important issue to consider is the likelihood of multiple bit errors caused by particle strikes. This likelihood has been studied to a great extent in memories, but has not been understood to the same extent in logic circuits. In this paper, we attempt to quantify the likelihood that a single transient event can cause multiple bit errors in logic circuits consisting of combinational gates and flip-flops. In particular, we calculate the conditional probability of multiple bit-flips given that a single bit flips as a result of the transient. To calculate this conditional probability, we use a Monte Carlo technique in which samples are generated using detailed post-layout circuit simulations. Our experiments on the ISCAS'85 benchmarks and a few other circuits indicate that, this conditional probability is quite significant and can be as high as 0.31. Thus we conclude that multiple bit-flips must necessarily be considered in order to obtain a realistic architectural fault model for soft errors.", "venue": "ArXiv", "authors": ["Nanditha P. Rao", "Shahbaz  Sarik", "Madhav P. Desai"], "year": 2014, "n_citations": 6}
{"id": 6488793, "s2_id": "24d2bd18f707e281b1bde2eef1911eb1950ef5f0", "title": "A Custom 7nm CMOS Standard Cell Library for Implementing TNN-based Neuromorphic Processors", "abstract": "A set of highly-optimized custom macro extensions is developed for a 7nm CMOS cell library for implementing Temporal Neural Networks (TNNs) that can mimic brain-like sensory processing with extreme energy efficiency. A TNN prototype (13,750 neurons and 315,000 synapses) for MNIST requires only 1.56mm2 die area and consumes only 1.69mW.", "venue": "ArXiv", "authors": ["Harideep  Nair", "Prabhu  Vellaisamy", "Santha  Bhasuthkar", "John Paul Shen"], "year": 2020, "n_citations": 0}
{"id": 6489165, "s2_id": "ae3b4864d33d9771c836037b5a89d02e0a462178", "title": "SystemC analysis of a new dynamic power management architecture", "abstract": "The paper presents a new dynamic power management architecture for a system-on-chip. The power state machine, which describes the status of the core, follows the recommendations of the ACPI (advanced configuration and power interface) standard. The algorithm controls the power states of each block on the basis of battery status, chip temperature and a user defined task priority.", "venue": "Design, Automation and Test in Europe", "authors": ["Massimo  Conti"], "year": 2005, "n_citations": 0}
{"id": 6496866, "s2_id": "947859f54ebf4a7bd7a8d2d753f865123ecd614b", "title": "Enabling Efficient and Flexible FPGA Virtualization for Deep Learning in the Cloud", "abstract": "FPGAs have shown great potential in providing low-latency and energy-efficient solutions for deep neural network (DNN) inference applications. Currently, the majority of FPGA-based DNN accelerators in the cloud run in a time-division multiplexing way for multiple users sharing a single FPGA, and require re-compilation with $\\sim$100s overhead. Such designs lead to poor isolation and heavy performance loss for multiple users, which are far away from providing efficient and flexible FPGA virtualization for neither public nor private cloud scenarios. To solve these problems, we introduce a novel virtualization framework for instruction architecture set (ISA) based on DNN accelerators by sharing a single FPGA. We enable the isolation by introducing a two-level instruction dispatch module and a multi-core based hardware resources pool. Such designs provide isolated and runtime-programmable hardware resources, further leading to performance isolation for multiple users. On the other hand, to overcome the heavy re-compilation overheads, we propose a tiling-based instruction frame package design and two-stage static-dynamic compilation. Only the light-weight runtime information is re-compiled with $\\sim$1 ms overhead, thus the performance is guaranteed for the private cloud. Our extensive experimental results show that the proposed virtualization design achieves 1.07-1.69x and 1.88-3.12x throughput improvement over previous static designs using the single-core and the multi-core architectures, respectively.", "venue": "2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)", "authors": ["Shulin  Zeng", "Guohao  Dai", "Hanbo  Sun", "Kai  Zhong", "Guangjun  Ge", "Kaiyuan  Guo", "Yu  Wang", "Huazhong  Yang"], "year": 2020, "n_citations": 6}
{"id": 6501613, "s2_id": "d285ec7909da9c59fc8a92a5c63b82d586a04552", "title": "In-storage embedded accelerator for sparse pattern processing", "abstract": "We present a novel architecture for sparse pattern processing, using flash storage with embedded accelerators. Sparse pattern processing on large data sets is the essence of applications such as document search, natural language processing, bioinformatics, subgraph matching, machine learning, and graph processing. One slice of our prototype accelerator is capable of handling up to 1TB of data, and experiments show that it can outperform C/C++ software solutions on a 16-core system at a fraction of the power and cost; an optimized version of the accelerator can match the performance of a 48-core server.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Sang Woo Jun", "Huy T. Nguyen", "Vijay  Gadepally", "Arvind"], "year": 2016, "n_citations": 4}
{"id": 6503235, "s2_id": "68584a2156056718c734fa38cdc08d414cbcfc00", "title": "Buddy Compression: Enabling Larger Memory for Deep Learning and HPC Workloads on GPUs", "abstract": "GPUs accelerate high-throughput applications, which require orders-of-magnitude higher memory bandwidth than traditional CPU-only systems. However, the capacity of such high-bandwidth memory tends to be relatively small. Buddy Compression is an architecture that makes novel use of compression to utilize a larger buddy-memory from the host or disaggregated memory, effectively increasing the memory capacity of the GPU. Buddy Compression splits each compressed 128B memory-entry between the high-bandwidth GPU memory and a slower-but-larger buddy memory such that compressible memory-entries are accessed completely from GPU memory, while incompressible entries source some of their data from off-GPU memory. With Buddy Compression, compressibility changes never result in expensive page movement or re-allocation. Buddy Compression achieves on average $1.9 \\times$ effective GPU memory expansion for representative HPC applications and $1.5 \\times$ for deep learning training, performing within 2% of an unrealistic system with no memory limit. This makes Buddy Compression attractive for performance-conscious developers that require additional GPU memory capacity.", "venue": "2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Esha  Choukse", "Michael B. Sullivan", "Mike  O'Connor", "Mattan  Erez", "Jeff  Pool", "David W. Nellans", "Stephen W. Keckler"], "year": 2020, "n_citations": 14}
{"id": 6503722, "s2_id": "18412159667da160a43bcc58de07a477014ed16d", "title": "A 2.48Gb/s QC-LDPC Decoder Implementation on the NI USRP-2953R", "abstract": "The increasing data rates expected to be of the order of Gb/s for future wireless systems directly impact the throughput requirements of the modulation and coding subsystems of the physical layer. In an effort to design a suitable channel coding solution for 5G wireless systems, in this brief we present a massively-parallel 2.48Gb/s Quasi-Cyclic Low-Density Parity-Check (QC-LDPC) decoder implementation operating at 200MHz on the NI USRP-2953R, on a single FPGA. The high-level description of the entire massively-parallel decoder was translated to a Hardware Description Language (HDL), namely VHDL, using the algorithmic compiler in the National Instruments LabVIEW Communication System Design Suite (CSDS) in approximately 2 minutes. This implementation not only demonstrates the scalability of our decoder architecture but also, the rapid prototyping capability of the LabVIEW CSDS tools. As per our knowledge, at the time of writing this paper, this is the fastest implementation of a standard compliant QC-LDPC decoder on a USRP using an algorithmic compiler.", "venue": "ArXiv", "authors": ["Swapnil  Mhaske", "David  Uliana", "Hojin  Kee", "Tai  Ly", "Ahsan  Aziz", "Predrag  Spasojevic"], "year": 2015, "n_citations": 6}
{"id": 6511073, "s2_id": "1c1b24f9adfaefbf5c365a2661a877132e9fadcc", "title": "Memory testing under different stress conditions: an industrial evaluation", "abstract": "This paper presents the effectiveness of various stress conditions (mainly voltage and frequency) on detecting the resistive shorts and open defects in deep sub-micron embedded memories in an industrial environment. Simulation studies on very-low voltage, high voltage and at-speed testing show the need of the stress conditions for high quality products; i.e., low defect-per-million (DPM) level, which is driving the semiconductor market today. The above test conditions have been validated to screen out bad devices on real silicon (a test-chip) built on CMOS 0.18 /spl mu/m technology. The IFA (inductive fault analysis) based simulation technique leads to an efficient fault coverage and DPM estimator, which helps the customers upfront to make decisions on test algorithm implementations under different stress conditions in order to reduce the number of test escapes.", "venue": "Design, Automation and Test in Europe", "authors": ["Ananta K. Majhi", "Mohamed  Azimane", "Guido  Gronthoud", "Maurice  Lousberg", "Stefan  Eichenberger", "Fred  Bowen"], "year": 2005, "n_citations": 14}
{"id": 6514049, "s2_id": "35ed4e95c65d15e682d239be14bc8bd2ffbc3422", "title": "Improving GPU Performance Through Resource Sharing", "abstract": "Graphics Processing Units (GPUs) consisting of Streaming Multiprocessors (SMs) achieve high throughput by running a large number of threads and context switching among them to hide execution latencies. The number of thread blocks, and hence the number of threads that can be launched on an SM, depends on the resource usage--e.g. number of registers, amount of shared memory--of the thread blocks. Since the allocation of threads to an SM is at the thread block granularity, some of the resources may not be used up completely and hence will be wasted. We propose an approach that shares the resources of SM to utilize the wasted resources by launching more thread blocks. We show the effectiveness of our approach for two resources: register sharing, and scratchpad (shared memory) sharing. We further propose optimizations to hide long execution latencies, thus reducing the number of stall cycles. We implemented our approach in GPGPU-Sim simulator and experimentally validated it on 19 applications from 4 different benchmark suites: GPGPU-Sim, Rodinia, CUDA-SDK, and Parboil. We observed that applications that underutilize register resource show a maximum improvement of 24% and an average improvement of 11% with register sharing. Similarly, the applications that underutilize scratchpad resource show a maximum improvement of 30% and an average improvement of 12.5% with scratchpad sharing. The remaining applications, which do not waste any resources, perform similar to the baseline approach.", "venue": "HPDC", "authors": ["Vishwesh  Jatala", "Jayvant  Anantpur", "Amey  Karkare"], "year": 2016, "n_citations": 8}
{"id": 6516896, "s2_id": "a4225561b93392c44151deebafe75d11f35208fc", "title": "SpectralFly: Ramanujan Graphs as Flexible and Efficient Interconnection Networks", "abstract": "In recent years, graph theoretic considerations have become increasingly important in the design of HPC interconnection topologies. One approach is to seek optimal or near-optimal families of graphs with respect to a particular graph theoretic property, such as diameter. In this work, we consider topologies which optimize the spectral gap. In particular, we study a novel HPC topology, SpectralFly, designed around the Ramanujan graph construction of Lubotzky, Phillips, and Sarnak (LPS). We show combinatorial properties, such as diameter, bisection bandwidth, average path length, and resilience to link failure, of SpectralFly topologies are better than, or comparable to, similarly constrained DragonFly, SlimFly, and BundleFly topologies. Additionally, we simulate the performance of SpectralFly topologies on a representative sample of physics-inspired HPC workloads using the Structure Simulation Toolkit Macroscale Element Library simulator and demonstrate considerable benefit to using the LPS construction as the basis of the SpectralFly topology.", "venue": "ArXiv", "authors": ["Sinan  Aksoy", "Stephen  Young", "Jesun  Firoz", "Roberto  Gioiosa", "Mark  Raugas", "Juan  Escobedo"], "year": 2021, "n_citations": 0}
{"id": 6521098, "s2_id": "817dd0fd0eaa9082f949204241c17560e80bd547", "title": "NERO: A Near High-Bandwidth Memory Stencil Accelerator for Weather Prediction Modeling", "abstract": "Ongoing climate change calls for fast and accurate weather and climate modeling. However, when solving large-scale weather prediction simulations, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. These implementations are dominated by complex irregular memory access patterns and low arithmetic intensity that pose fundamental challenges to acceleration. To overcome these challenges, we propose and evaluate the use of near-memory acceleration using a reconfigurable fabric with high-bandwidth memory (HBM). We focus on compound stencils that are fundamental kernels in weather prediction models. By using high-level synthesis techniques, we develop NERO, an FPGA+HBM-based accelerator connected through IBM CAPI2 (Coherent Accelerator Processor Interface) to an IBM POWER9 host system. Our experimental results show that NERO outperforms a 16-core POWER9 system by 4.2x and 8.3x when running two different compound stencil kernels. NERO reduces the energy consumption by 22x and 29x for the same two kernels over the POWER9 system with an energy efficiency of 1.5 GFLOPS/Watt and 17.3 GFLOPS/Watt. We conclude that employing near-memory acceleration solutions for weather prediction modeling is promising as a means to achieve both high performance and high energy efficiency.", "venue": "2020 30th International Conference on Field-Programmable Logic and Applications (FPL)", "authors": ["Gagandeep  Singh", "Dionysios  Diamantopoulos", "Christoph  Hagleitner", "Juan  Gomez-Luna", "Sander  Stuijk", "Onur  Mutlu", "Henk  Corporaal"], "year": 2020, "n_citations": 15}
{"id": 6521756, "s2_id": "c9b17100e8fe18a443a674c3a1823decc62dbbfb", "title": "LECTOR Based Clock Gating for Low Power Multi-Stage Flip Flop Applications", "abstract": "Power dissipation in integrated circuits is one of the major concerns to the research community, at the verge when more number of transistors are integrated on a single chip. The substantial source of power dissipation in sequential elements of the integrated circuit is due to the fast switching of high frequency clock signals. These signals do not carry any information and are mainly intended to synchronize the operation of sequential components. This unnecessary switching of Clock, during the HOLD phase of either logic 1 or logic 0, may be eliminated using a technique, called Clock Gating. In this paper, we have incorporated a recent clock gating style called LECTOR based clock gating LB CG to drive multi stage architecture and simulated its performance using 90nm CMOS Predictive Technology Model PTM with a power supply of 1.1V at 18GHz clock frequency. A substantial savings in terms of average power in comparison to its non gated correspondent have been observed.", "venue": "ArXiv", "authors": ["Pritam  Bhattacharjee", "Bipasha  Nath", "Alak  Majumder"], "year": 2018, "n_citations": 2}
{"id": 6525016, "s2_id": "cbf9bfaef3a899fd83d724243d6a291d5a820b20", "title": "Read-Tuned STT-RAM and eDRAM Cache Hierarchies for Throughput and Energy Enhancement", "abstract": "As capacity and complexity of on-chip cache memory hierarchy increases, the service cost to the critical loads from Last Level Cache (LLC), which are frequently repeated, has become a major concern. The processor may stall for a considerable interval while waiting to access the data stored in the cache blocks in LLC, if there are no independent instructions to execute. To provide accelerated service to the critical loads requests from LLC, this work concentrates on leveraging the additional capacity offered by replacing SRAM-based L2 with Spin-Transfer Torque Random Access Memory (STT-RAM) to accommodate frequently accessed cache blocks in exclusive read mode in favor of reducing the overall read service time. Our proposed technique partitions L2 cache into two STT-RAM arrangements with different write performance and data retention time. The retention-relaxed STT-RAM arrays are utilized to effectively deal with the regular L2 cache requests while the high retention STT-RAM arrays in L2 are selected for maintaining repeatedly read accessed cache blocks from LLC by incurring negligible energy consumption for data retention. Our experimental results show that the proposed technique can reduce the mean L2 read miss ratio by 51.4% and increase the IPC by 11.7% on average across PARSEC benchmark suite while significantly decreasing the total L2 energy consumption compared to conventional SRAM-based L2 design.", "venue": "ArXiv", "authors": ["Navid  Khoshavi", "Xunchao  Chen", "Jun  Wang", "Ronald F. DeMara"], "year": 2016, "n_citations": 39}
{"id": 6526226, "s2_id": "3fa95189ef390e8c6b4d50f6aff0c6ead3ac533a", "title": "Reuse Distance-based Copy-backs of Clean Cache Lines to Lower-level Caches", "abstract": "Cache plays a critical role in reducing the performance gap between CPU and main memory. A modern multi-core CPU generally employs a multi-level hierarchy of caches, through which the most recently and frequently used data are maintained in each core\u2019s local private caches while all cores share the lastlevel cache (LLC). For inclusive caches, clean cache lines replaced in higher-level caches are not necessarily copied back to lower levels, as the inclusiveness implies their existences in lower levels. For exclusive and non-inclusive caches that are widely utilized by Intel, AMD, and ARM today, either indiscriminately copying back all or none of replaced clean cache lines to lower levels raises no violation to exclusiveness and non-inclusiveness definitions. We have conducted a quantitative study and found that, copying back all or none of clean cache lines to lower-level cache of exclusive caches entails suboptimal performance. The reason is that only a part of cache lines would be reused and others turn to be dead in a long run. This observation motivates us to selectively copy back some clean cache lines to LLC in an architecture of exclusive or non-inclusive caches. We revisit the concept of reuse distance of cache lines. In a nutshell, a clean cache line with a shorter reuse distance is copied back to lower-level cache as it is likely to be re-referenced in the near future, while cache lines with much longer reuse distances would be discarded or sent to memory if they are dirty. We have implemented and evaluated our proposal with non-volatile (STT-MRAM) LLC. Experimental results with gem5 and SPEC CPU 2017 benchmarks show that on average our proposal yields up to 12.8% higher throughput of IPC (instructions per cycle) than the least-recently-used (LRU) replacement policy with copying back all clean cache lines for STT-MRAM LLC.", "venue": "ArXiv", "authors": ["Rui  Wang", "Chundong  Wang", "Chongnan  Ye"], "year": 2021, "n_citations": 0}
{"id": 6527540, "s2_id": "ad39278ea271977a78c99a1f18fd16458b4abacf", "title": "A Survey of Machine Learning Applied to Computer Architecture Design", "abstract": "Machine learning has enabled significant benefits in diverse fields, but, with a few exceptions, has had limited impact on computer architecture. Recent work, however, has explored broader applicability for design, optimization, and simulation. Notably, machine learning based strategies often surpass prior state-of-the-art analytical, heuristic, and human-expert approaches. This paper reviews machine learning applied system-wide to simulation and run-time optimization, and in many individual components, including memory systems, branch predictors, networks-on-chip, and GPUs. The paper further analyzes current practice to highlight useful design strategies and identify areas for future work, based on optimized implementation strategies, opportune extensions to existing work, and ambitious long term possibilities. Taken together, these strategies and techniques present a promising future for increasingly automated architectural design.", "venue": "ArXiv", "authors": ["Drew D. Penney", "Lizhong  Chen"], "year": 2019, "n_citations": 12}
{"id": 6531988, "s2_id": "b61eff863f419f13b9dc520d19dbd352f9cb2a9f", "title": "Designing a WISHBONE Protocol Network Adapter for an Asynchronous Network-on-Chip", "abstract": "The Scaling of microchip technologies, from micron to submicron and now to deep sub-micron (DSM) range, has enabled large scale systems-on-chip (SoC). In future deep submicron (DSM) designs, the interconnect effect will definitely dominate performance. Network-on-Chip (NoC) has become a promising solution to bus-based communication infrastructure limitations. NoC designs usually targets Application Specific Integrated Circuits (ASICs), however, the fabrication process costs a lot. Implementing a NoC on an FPGA does not only reduce the cost but also decreases programming and verification cycles. In this paper, an Asynchronous NoC has been implemented on a SPARTAN-3E\u00ae device. The NoC supports basic transactions of both widely used on-chip interconnection standards, the Open Core Protocol (OCP) and the WISHBONE Protocol. Although, FPGA devices are synchronous in nature, it has been shown that they can be used to prototype a Global Asynchronous Local Synchronous (GALS) systems, comprising an Asynchronous NoC connecting IP cores operating in different clock domains.", "venue": "ArXiv", "authors": ["Ahmed H. M. Soliman", "E. M. Saad", "M.  El-Bably", "Hesham M. A. M. Keshk"], "year": 2012, "n_citations": 6}
{"id": 6532136, "s2_id": "b1a06348636b68a68e301645e35b33073eea3aca", "title": "A Single-Channel Architecture for Algebraic Integer-Based 8 $\\,\\times\\,$8 2-D DCT Computation", "abstract": "An area efficient row-parallel architecture is proposed for the real-time implementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine transform (DCT) for image and video processing. The proposed architecture computes 8 \u00d7 8 2-D DCT transform based on the Arai DCT algorithm. An improved fast algorithm for AI-based 1-D DCT computation is proposed along with a single channel 2-D DCT architecture. The design improves on the four-channel AI DCT architecture that was published recently by reducing the number of integer channels to one and the number of eight-point 1-D DCT cores from five down to two. The architecture offers exact computation of 8 \u00d7 8 blocks of the 2-D DCT coefficients up to the FRS, which converts the coefficients from the AI representation to fixed-point format using the method of expansion factors. Prototype circuits corresponding to FRS blocks based on two expansion factors are realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6 XC6VLX240T device. Post place-and-route results show a 20% reduction in terms of area compared to the 2-D DCT architecture requiring five 1-D AI cores. The area-time and area-time2 complexity metrics are also reduced by 23% and 22% respectively for designs with eight-bit input word length. The digital realizations are simulated up to place and route for ASICs using 45 nm CMOS standard cells. The maximum estimated clock rate is 951 MHz for the CMOS realizations indicating 7.608\u00b7109 pixels/s and a 8 \u00d7 8 block rate of 118.875 MHz.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "authors": ["Amila  Edirisuriya", "Arjuna  Madanayake", "Renato J. Cintra", "Vassil S. Dimitrov", "Nilanka T. Rajapaksha"], "year": 2013, "n_citations": 11}
{"id": 6537273, "s2_id": "845cc509db8c059b913c7316ba0c5d231c0ef4be", "title": "ERSFQ 8-bit Parallel Binary Shifter for Energy-Efficient Superconducting CPU", "abstract": "We have designed and tested a parallel 8-bit ERSFQ binary shifter that is one of the essential circuits in the design of the energy-efficient superconducting CPU. The binary shifter performs a bidirectional SHIFT instruction of an 8-bit argument. It consists of a bidirectional triple-port shift register controlled by two (left and right) shift pulse generators asynchronously generating a set number of shift pulses. At first clock cycle, an 8-bit word is loaded into the binary shifter and a 3-bit shift argument is loaded into the desired shift-pulse generator. Next, the generator produces the required number of shift SFQ pulses (from 0 to 7) asynchronously, with a repetition rate set by the internal generator delay of \u223c 30 ps. These SFQ pulses are applied to the left (positive) or the right (negative) input of the binary shifter. Finally, after the shift operation is completed, the resulting 8-bit word goes to the parallel output. The complete 8-bit ERSFQ binary shifter, consisting of 820 Josephson junctions, was simulated and optimized using PSCAN2. It was fabricated in MIT Lincoln Lab's 10-kA/cm2 SFQ5ee fabrication process with a high-kinetic inductance layer. We have successfully tested the binary shifter at both the LSB-to-MSB and MSB-to-LSB propagation regimes for all eight shift arguments. A single shift operation on a single input word demonstrated operational margins of \u00b116% of the dc bias current. The correct functionality of the 8-bit ERSFQ binary shifter with the large, exhaustive data pattern was observed within \u00b110% margins of the dc bias current. In this paper, we describe the design and present the test results for the ERSFQ 8-bit parallel binary shifter.", "venue": "IEEE Transactions on Applied Superconductivity", "authors": ["Alex F. Kirichenko", "Michael Y. Kamkar", "Jason  Walter", "Igor V. Vernik"], "year": 2019, "n_citations": 2}
{"id": 6537663, "s2_id": "b3e4be34133c50c4ece92eed0ce5e93f6bb4812d", "title": "Fast TLB Simulation for RISC-V Systems", "abstract": "Address translation and protection play important roles in today's processors, supporting multiprocessing and enforcing security. Historically, the design of the address translation mechanisms has been closely tied to the instruction set. In contrast, RISC-V defines its privileged specification in a way that permits a variety of designs. \nAn important part of the design space is the organisation of Translation Lookaside Buffers (TLBs). This paper presents our recent work on simulating TLB behaviours in multi-core RISC-V systems. Our TLB simulation framework allows rapid, flexible and versatile prototyping of various hardware TLB design choices, and enables validation, profiling and benchmarking of software running on RISC-V systems. We show how this framework can be integrated with the dynamic binary translated emulator QEMU to perform online simulation. When simulating complicated multi-level shared TLB designs, the framework runs at around 400 million instructions per second (MIPS) when simulating an 8-core system. The performance overhead compared to unmodified QEMU is only 18% when the benchmark's L1 TLB miss rate is 1%. \nWe also demonstrate how this tool can be used to explore the instruction-set level design space. We test a shared last-level TLB design that is not currently permitted by the RISC-V's privileged specification. We then propose an extension to RISC-V's virtual memory system design based on these experimental results.", "venue": "ArXiv", "authors": ["Xuan  Guo", "Robert  Mullins"], "year": 2019, "n_citations": 2}
{"id": 6539233, "s2_id": "af1e53253ef10ce0cdfd14fbb874aefdce459538", "title": "Dynamic Power Reduction in a Novel CMOS 5T-SRAM for Low-Power SoC", "abstract": "This paper addresses a novel five-transistor (5T) CMOS SRAM design with high performance and reliability in 65nm CMOS, and illustrates how it reduces the dynamic power consumption in comparison with the conventional and low-power 6T SRAM counterparts. This design can be used as cache memory in processors and low-power portable devices. The proposed SRAM cell features ~13% area reduction compared to a conventional 6T cell, and features a unique bit-line and negative supply voltage biasing methodology and ground control architecture to enhance performance, and suppress standby leakage power.", "venue": "CDES", "authors": ["Hooman  Jarollahi", "Richard F. Hobson"], "year": 2010, "n_citations": 1}
{"id": 6541581, "s2_id": "35cbedf61261efc1b842d15cda1a0454c7e0434a", "title": "Area-efficient selective multi-threshold CMOS design methodology for standby leakage power reduction", "abstract": "This paper presents a design flow for an improved selective multi-threshold (selective-MT) circuit. The selective-MT circuit is improved so that plural MT-cells can share one switch transistor. We propose the design methodology from RTL (register transfer level) to final layout with optimizing switch transistor structure.", "venue": "Design, Automation and Test in Europe", "authors": ["Takeshi  Kitahara", "Naoyuki  Kawabe", "Fumihiro  Minami", "Katsuhiro  Seta", "Toshiyuki  Furusawa"], "year": 2005, "n_citations": 9}
{"id": 6547437, "s2_id": "98bfbe9ab20e0e710220d09c1d8db288172bf92f", "title": "Reduced Area Low Power High Throughput BCD Adders for IEEE 754r Format", "abstract": "IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and a major enhancement to the standard is the addition of decimal format. Firstly, this paper proposes novel two transistor AND and OR gates. The proposed AND gate has no power supply, thus it can be referred as the Powerless AND gate. Similarly, the proposed two transistor OR gate has no ground and can be referred as Groundless OR. Secondly for IEEE 754r format, two novel BCD adders called carry skip and carry look-ahead BCD adders are also proposed in this paper. In order to design the carry look-ahead BCD adder, a novel 4 bit carry look-ahead adder called NCLA is proposed which forms the basic building block of the proposed carry look-ahead BCD adder. Finally, the proposed two transistors AND and OR gates are used to provide the optimized small area low power high throughput circuitries of the proposed BCD adders.", "venue": "ArXiv", "authors": ["Himanshu  Thapliyal", "Hamid R. Arabnia", "M. B. Srinivas"], "year": 2006, "n_citations": 7}
{"id": 6556661, "s2_id": "bdb0aa91fd02b45ca859ec1b63b1761183e7288a", "title": "AutoSoC: Automating Algorithm-SOC Co-design for Aerial Robots", "abstract": "Aerial autonomous machines (Drones) has a plethora of promising applications and use cases. While the popularity of these autonomous machines continues to grow, there are many challenges, such as endurance and agility, that could hinder the practical deployment of these machines. The closed-loop control frequency must be high to achieve high agility. However, given the resource-constrained nature of the aerial robot, achieving high control loop frequency is hugely challenging and requires careful co-design of algorithm and onboard computer. Such an effort requires infrastructures that bridge various domains, namely robotics, machine learning, and system architecture design. To that end, we present AutoSoC, a framework for co-designing algorithms as well as hardware accelerator systems for end-to-end learning-based aerial autonomous machines. We demonstrate the efficacy of the framework by training an obstacle avoidance algorithm for aerial robots to navigate in a densely cluttered environment. For the best performing algorithm, our framework generates various accelerator design candidates with varying performance, area, and power consumption. The framework also runs the ASIC flow of place and route and generates a layout of the floor-planed accelerator, which can be used to tape-out the final hardware chip.", "venue": "ArXiv", "authors": ["Srivatsan  Krishnan", "Thierry  Tambe", "Zishen  Wan", "Vijay Janapa Reddi"], "year": 2021, "n_citations": 2}
{"id": 6557612, "s2_id": "21f3dbdb5ec30c45fde20237d6a92c831a8c2246", "title": "Cycle-Accurate Evaluation of Software-Hardware Co-Design of Decimal Computation in RISC-V Ecosystem", "abstract": "Software-hardware co-design solutions for decimal computation can provide several Pareto points to development of embedded systems in terms of hardware cost and performance. This paper demonstrates how to accurately evaluate such co-design solutions using RISC-V ecosystem. In a software-hardware co-design solution, a part of solution requires dedicated hardware. In our evaluation framework, we develop new decimal oriented instructions supported by an accelerator. The framework can realize cycle-accurate analysis for performance as well as hardware overhead for co-design solutions for decimal computation. The obtained performance result is compared with an estimation with dummy functions.", "venue": "2019 32nd IEEE International System-on-Chip Conference (SOCC)", "authors": ["Riaz-ul-haque  Mian", "Michihiro  Shintani", "Michiko  Inoue"], "year": 2019, "n_citations": 0}
{"id": 6558500, "s2_id": "b605b0d87b2b0b081c08caac6426e4c27d6a2d34", "title": "Minimum energy quantized neural networks", "abstract": "This work targets the automated minimum-energy optimization of Quantized Neural Networks (QNNs) \u2014 networks using low precision weights and activations. These networks are trained from scratch at an arbitrary fixed point precision. At iso-accuracy, QNNs using fewer bits require deeper and wider network architectures than networks using higher precision operators, while they require less complex arithmetic and less bits per weights. This fundamental trade-off is analyzed and quantified to find the minimum energy QNN for any benchmark and hence optimize energy-efficiency. To this end, the energy consumption of inference is modeled for a generic hardware platform. This allows drawing several conclusions across different benchmarks. First, energy consumption varies orders of magnitude at iso-accuracy depending on the number of bits used in the QNN. Second, in a typical system, BinaryNets or int4 implementations lead to the minimum energy solution, outperforming int8 networks up to 2\u201310\u00d7 at iso-accuracy. All code used for QNN training is available from https://github.com/BertMoons/.", "venue": "2017 51st Asilomar Conference on Signals, Systems, and Computers", "authors": ["Bert  Moons", "Koen  Goetschalckx", "Nick Van Berckelaer", "Marian  Verhelst"], "year": 2017, "n_citations": 64}
{"id": 6559486, "s2_id": "4c10904dc05ea9b1c998be7d16f2cc54efe5fbb2", "title": "A Fast Finite Field Multiplier for SIKE", "abstract": "Various post-quantum cryptography algorithms have been recently proposed. Supersingluar isogeny Diffie-Hellman key exchange (SIKE) is one of the most promising candidates due to its small key size. However, the SIKE scheme requires numerous finite field multiplications for its isogeny computation, and hence suffers from slow encryption and decryption process. In this paper, we propose a fast finite field multiplier design that performs multiplications in GF(p) with high throughput and low latency. The design accelerates the computation by adopting deep pipelining, and achieves high hardware utilization through data interleaving. The proposed finite field multiplier demonstrates 4.48 times higher throughput than prior work based on the identical fast multiplication algorithm and 1.43 times higher throughput than the state-of-the-art fast finite field multiplier design aimed at SIKE.", "venue": "ArXiv", "authors": ["Yeonsoo  Jeon", "Dongsuk  Jeon"], "year": 2020, "n_citations": 0}
{"id": 6562422, "s2_id": "55a065fb46a62a61e6c0bf47886f72b911538011", "title": "Building Beyond HLS: Graph Analysis and Others", "abstract": "High-Level Synthesis has introduced reconfigurable logic to a new world \u2013 that of software development. The newest wave of HLS tools has been successful, and the future looks bright. But is HLS the end-all-be-all to FPGA acceleration? Is it enough to allow nonexperts to program FPGAs successfully, even when dealing with troublesome data structures and complex control flows \u2013 such as those often encountered in graph algorithms? We take a look at the panorama of adoption of HLS by the software community, focusing on graph analysis in particular in order to generalise to FPGA-unfriendly problems. We argue for the existence of shortcomings in current HLS development flowswhich hinder adoption, and present our perspective on the path forward, including how these issues may be remedied via higher-level tooling.", "venue": "ArXiv", "authors": ["Pedro Filipe Silva", "Joao  Bispo", "Nuno  Paulino"], "year": 2021, "n_citations": 0}
{"id": 6569357, "s2_id": "be5944e2e21a205faf72bf754a78d9d07d8399f1", "title": "Computational RAM to Accelerate String Matching at Scale", "abstract": "Traditional Von Neumann computing is falling apart in the era of exploding data volumes as the overhead of data transfer becomes forbidding. Instead, it is more energy-efficient to fuse compute capability with memory where the data reside. This is particularly critical for pattern matching, a key computational step in large-scale data analytics, which involves repetitive search over very large databases residing in memory. Emerging spintronic technologies show remarkable versatility for the tight integration of logic and memory. In this paper, we introduce CRAM-PM, a novel high-density, reconfigurable spintronic in-memory compute substrate for pattern matching.", "venue": "ArXiv", "authors": ["Zamshed I. Chowdhury", "S. Karen Khatamifard", "Zhengyang  Zhao", "Masoud  Zabihi", "Salonik  Resch", "Meisam  Razaviyayn", "Jianping  Wang", "Sachin S. Sapatnekar", "Ulya R. Karpuzcu"], "year": 2018, "n_citations": 1}
{"id": 6572009, "s2_id": "4b00f266041db0004e9c46e2398cc5d9773dcd6c", "title": "Hotspot prevention through runtime reconfiguration in network-on-chip", "abstract": "Many existing thermal management techniques focus on reducing the overall power consumption of the chip, and do not address location-specific temperature problems referred to as hotspots. We propose the use of dynamic runtime reconfiguration to shift the hotspot-inducing computation periodically and make the thermal profile more uniform. Our analysis shows that dynamic reconfiguration is an effective technique in reducing hotspots for NoC.", "venue": "Design, Automation and Test in Europe", "authors": ["Greg M. Link", "Narayanan  Vijaykrishnan"], "year": 2005, "n_citations": 38}
{"id": 6577865, "s2_id": "3d5aa7dd7bb355d71627e3f0920556e047c30d5c", "title": "A Flexible and Fast PyTorch Toolkit for Simulating Training and Inference on Analog Crossbar Arrays", "abstract": "We introduce the IBM ANALOG HARDWARE ACCELERATION KIT, a new and first of a kind open source toolkit to simulate analog crossbar arrays in a convenient fashion from within PYTORCH (freely available at https://github.com/IBM/aihwkit). The toolkit is under active development and is centered around the concept of an \u201canalog tile\u201d which captures the computations performed on a crossbar array. Analog tiles are building blocks that can be used to extend existing network modules with analog components and compose arbitrary artificial neural networks (ANNs) using the flexibility of the PYTORCH framework. Analog tiles can be conveniently configured to emulate a plethora of different analog hardware characteristics and their non-idealities, such as device-to-device and cycle-to-cycle variations, resistive device response curves, and weight and output noise. Additionally, the toolkit makes it possible to design custom unit cell configurations and to use advanced analog optimization algorithms such as Tiki-Taka. Moreover, the backward and update behavior can be set to \u201cideal\" to enable hardware-aware training features for chips that target inference acceleration only. To evaluate the inference accuracy of such chips over time, we provide statistical programming noise and drift models calibrated on phase-change memory hardware. Our new toolkit is fully GPU accelerated and can be used to conveniently estimate the impact of material properties and non-idealities of future analog technology on the accuracy for arbitrary ANNs.", "venue": "2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS)", "authors": ["Malte J. Rasch", "Diego  Moreda", "Tayfun  Gokmen", "Manuel Le Gallo", "Fabio  Carta", "Cindy  Goldberg", "Kaoutar El Maghraoui", "Abu  Sebastian", "Vijay  Narayanan"], "year": 2021, "n_citations": 4}
{"id": 6587756, "s2_id": "a837c9a13fbc6231d50c22395afd336fded0f1b8", "title": "FPGA Based Efficient Multiplier for Image Processing Applications Using Recursive Error Free Mitchell Log Multiplier and KOM Architecture", "abstract": "The Digital Image processing applications like medical imaging, satellite imaging, Biometric trait images etc., rely on multipliers to improve the quality of image. However, existing multiplication techniques introduce errors in the output with consumption of more time, hence error free high speed multipliers has to be designed. In this paper we propose FPGA based Recursive Error Free Mitchell Log Multiplier (REFMLM) for image Filters. The 2x2 error free Mitchell log multiplier is designed with zero error by introducing error correction term is used in higher order Karastuba-Ofman Multiplier (KOM) Architectures. The higher order KOM multipliers is decomposed into number of lower order multipliers using radix 2 till basic multiplier block of order 2x2 which is designed by error free Mitchell log multiplier. The 8x8 REFMLM is tested for Gaussian filter to remove noise in fingerprint image. The Multiplier is synthesized using Spartan 3 FPGA family device XC3S1500-5fg320. It is observed that the performance parameters such as area utilization, speed, error and PSNR are better in the case of proposed architecture compared to existing architectures", "venue": "ArXiv", "authors": ["Satish S. Bhairannawar", "R.  Rathan", "K. B. Raja", "K. R. Venugopal", "Lalit M. Patnaik"], "year": 2014, "n_citations": 2}
{"id": 6590010, "s2_id": "39100767be0b939f8635ca32f0328808df2c9b78", "title": "A PGAS Communication Library for Heterogeneous Clusters", "abstract": "This work presents a heterogeneous communication library for clusters of processors and FPGAs. This library, Shoal, supports the Partitioned Global Address Space (PGAS) memory model for applications. PGAS is a shared memory model for clusters that creates a distinction between local and remote memory access. Through Shoal and its common application programming interface for hardware and software, applications can be more freely migrated to the optimal platform and deployed onto dynamic cluster topologies. The library is tested using a thorough suite of microbenchmarks to establish latency and throughput performance. We also show an implementation of the Jacobi iterative method that demonstrates the ease with which applications can be moved between platforms to yield faster run times. Through this work, we have demonstrated the feasibility of using a PGAS programming model for multi-node heterogeneous platforms.", "venue": "ArXiv", "authors": ["Varun  Sharma", "Paul  Chow"], "year": 2021, "n_citations": 0}
{"id": 6590167, "s2_id": "b6187e239c447c7bf6ce1c450e0549a43274ed09", "title": "New Attacks and Defenses for Randomized Caches", "abstract": "The last level cache is vulnerable to timing based side channel attacks because it is shared by the attacker and the victim processes even if they are located on different cores. These timing attacks evict the victim cache lines using small conflict groups(SCG), and monitor the cache to observe when the victim uses these cache lines again. A conflict group is a collection of cache lines which will evict the target cache line. Randomization is often used by defenses to prevent creation of SCGs. \nWe introduce new attacks to demonstrate that the current randomization schemes require an extremely high refresh rate to be secure, on average a 15\\% performance overhead, and upto 50\\% in the worst case. Next, we propose a new randomization strategy using an indirection table, which mitigates this issue. Addresses of cache lines are encrypted and used to lookup the indirection table entry. Each indirection table entry stores a mapping to a randomly chosen cache set. The cache line is placed into this randomly chosen set. The encryption key changes upto 50x faster than CEASER's default rate, by using evictions to trigger the re-randomization. Instead of moving cache lines, this mechanism re-randomizes one iTable entry at a time, whenever the cache lines corresponding to the iTable entry are naturally evicted. Thus, the miss rate is not much worse than the baseline. \nWe quantitatively show that our scheme does almost as well as a fully associative cache to defend against these attacks. We also demonstrate new attacks that target the iTable by oversubscribing its entries, and quantitatively show that our scheme is resilient against new attacks for trillions of years. We estimate low area ( < 7\\%) and power overhead compared to a baseline inclusive last-level cache. Lastly, we evaluate a low performance overhead (<4%) using the SPECrate 2017 and PARSEC 3.0 benchmarks.", "venue": "ArXiv", "authors": ["Kartik  Ramkrishnan", "Antonia  Zhai", "Stephen  McCamant", "Pen Chung Yew"], "year": 2019, "n_citations": 2}
{"id": 6593369, "s2_id": "f7ef2ac06ffa5d394ac2d8de0df4916fde50e21b", "title": "Optimal Scheduling for Exposed Datapath Architectures with Buffered Processing Units by ASP", "abstract": "Conventional processor architectures are restricted in exploiting instruction level parallelism (ILP) due to the relatively low number of programmer-visible registers. Therefore, more recent processor architectures expose their datapaths so that the compiler (1) can schedule parallel instructions to different processing units and (2) can make effective use of local storage of the processing units. Among these architectures, the Synchronous Control Asynchronous Dataflow (SCAD) architecture is a new exposed datapath architecture whose processing units are equipped with first-in first-out (FIFO) buffers at their input and output ports.In contrast to register-based machines, the optimal code generation for SCAD is still a matter of research. In particular, SAT and SMT solvers were used to generate optimal resource constrained and optimal time constrained schedules for SCAD, respectively. As Answer Set Programming (ASP) offers better flexibility in handling such scheduling problems, we focus in this paper on using an answer set solver for both resource and time constrained optimal SCAD code generation. As a major benefit of using ASP, we are able to generate \\emph{all} optimal schedules for a given program which allows one to study their properties. Furthermore, the experimental results of this paper demonstrate that the answer set solver can compete with SAT solvers and outperforms SMT solvers.", "venue": "EasyChair Preprints", "authors": ["Marc  Dahlem", "Anoop  Bhagyanath", "Klaus  Schneider"], "year": 2018, "n_citations": 1}
{"id": 6593698, "s2_id": "7fe6968a25953a1da808c318c5ad31cc42cea321", "title": "An In-Memory Analog Computing Co-Processor for Energy-Efficient CNN Inference on Mobile Devices", "abstract": "In this paper, we develop an in-memory analog computing (IMAC) architecture realizing both synaptic behavior and activation functions within non-volatile memory arrays. Spin-orbit torque magnetoresistive random-access memory (SOT-MRAM) devices are leveraged to realize sigmoidal neurons as well as binarized synapses. First, it is shown the proposed IMAC architecture can be utilized to realize a multilayer perceptron (MLP) classifier achieving orders of magnitude performance improvement compared to previous mixed-signal and digital implementations. Next, a heterogeneous mixed-signal and mixed-precision CPU-IMAC architecture is proposed for convolutional neural networks (CNNs) inference on mobile processors, in which IMAC is designed as a co-processor to realize fully-connected (FC) layers whereas convolution layers are executed in CPU. Architecture-level analytical models are developed to evaluate the performance and energy consumption of the CPU-IMAC architecture. Simulation results exhibit 6.5% and 10% energy savings for CPU-IMAC based realizations of LeNet and VGG CNN models, for MNIST and CIFAR-10 pattern recognition tasks, respectively.", "venue": "2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Mohammed  Elbtity", "Abhishek  Singh", "Brendan  Reidy", "Xiaochen  Guo", "Ramtin  Zand"], "year": 2021, "n_citations": 0}
{"id": 6595180, "s2_id": "6eb05e19f73112b705ff9210ed2a405eb4de670a", "title": "The Multi-Dataflow Composer tool: An open-source tool suite for optimized coarse-grain reconfigurable hardware accelerators and platform design", "abstract": "Abstract Modern embedded and cyber-physical systems require every day more performance, power efficiency and flexibility, to execute several profiles and functionalities targeting the ever growing adaptivity needs and preserving execution efficiency. Such requirements pushed designers towards the adoption of heterogeneous and reconfigurable substrates, which development and management is not that straightforward. Despite acceleration and flexibility are desirable in many domains, the barrier of hardware deployment and operation is still there since specific advanced expertise and skills are needed. Related challenges are effectively tackled by leveraging on automation strategies that in some cases, as in the proposed work, exploit model-based approaches. This paper is focused on the Multi-Dataflow Composer (MDC) tool, that intends to solve issues related to design, optimization and operation of coarse-grain reconfigurable hardware accelerators and their easy adoption in modern heterogeneous substrates. MDC latest features and improvements are introduced in detail and have been assessed on the so far unexplored robotics application field. A multi-profile trajectory generator for a robotic arm is implemented over a Xilinx FPGA board to show in which cases coarse-grain reconfiguration can be applied and which can be the parameters and trade-offs MDC will allow users to play with.", "venue": "Microprocess. Microsystems", "authors": ["Carlo  Sau", "Tiziana  Fanni", "Claudio  Rubattu", "Luigi  Raffo", "Francesca  Palumbo"], "year": 2021, "n_citations": 3}
{"id": 6597311, "s2_id": "5eb9544e846590d799f4f39e69cfbea1d561f44e", "title": "Quantum computers", "abstract": "Over the past several decades, quantum information science has emerged to seek answers to the question: can we gain some advantage by storing, transmitting and processing information encoded in systems that exhibit unique quantum properties? Today it is understood that the answer is yes, and many research groups around the world are working towards the highly ambitious technological goal of building a quantum computer, which would dramatically improve computational power for particular tasks. A number of physical systems, spanning much of modern physics, are being developed for quantum computation. However, it remains unclear which technology, if any, will ultimately prove successful. Here we describe the latest developments for each of the leading approaches and explain the major challenges for the future.", "venue": "Nature", "authors": ["Thaddeus D. Ladd", "Fedor  Jelezko", "Raymond  Laflamme", "Yasunobu  Nakamura", "Christopher R. Monroe", "Jeremy Lloyd O'Brien"], "year": 2010, "n_citations": 748}
{"id": 6600038, "s2_id": "ae0efaf5fb26b0e517dd4d9cbb7400649ce8fa2e", "title": "CRAM: Efficient Hardware-Based Memory Compression for Bandwidth Enhancement", "abstract": "This paper investigates hardware-based memory compression designs to increase the memory bandwidth. When lines are compressible, the hardware can store multiple lines in a single memory location, and retrieve all these lines in a single access, thereby increasing the effective memory bandwidth. However, relocating and packing multiple lines together depending on the compressibility causes a line to have multiple possible locations. Therefore, memory compression designs typically require metadata to specify the compressibility of the line. Unfortunately, even in the presence of dedicated metadata caches, maintaining and accessing this metadata incurs significant bandwidth overheads and can degrade performance by as much as 40%. Ideally, we want to implement memory compression while eliminating the bandwidth overheads of metadata accesses. \nThis paper proposes CRAM, a bandwidth-efficient design for memory compression that is entirely hardware based and does not require any OS support or changes to the memory modules or interfaces. CRAM uses a novel implicit-metadata mechanism, whereby the compressibility of the line can be determined by scanning the line for a special marker word, eliminating the overheads of metadata access. CRAM is equipped with a low-cost Line Location Predictor (LLP) that can determine the location of the line with 98% accuracy. Furthermore, we also develop a scheme that can dynamically enable or disable compression based on the bandwidth cost of storing compressed lines and the bandwidth benefits of obtaining compressed lines, ensuring no degradation for workloads that do not benefit from compression. Our evaluations, over a diverse set of 27 workloads, show that CRAM provides a speedup of up to 73% (average 6%) without causing slowdown for any of the workloads, and consuming a storage overhead of less than 300 bytes at the memory controller.", "venue": "ArXiv", "authors": ["Vinson  Young", "Sanjay  Kariyappa", "Moinuddin K. Qureshi"], "year": 2018, "n_citations": 4}
{"id": 6600161, "s2_id": "30c53af217278515129531941a769c6251557739", "title": "Address Translation Design Tradeoffs for Heterogeneous Systems", "abstract": "This paper presents a broad, pathfinding design space exploration of memory management units (MMUs) for heterogeneous systems. We consider a variety of designs, ranging from accelerators tightly coupled with CPUs (and using their MMUs) to fully independent accelerators that have their own MMUs. We find that regardless of the CPU-accelerator communication, accelerators should not rely on the CPU MMU for any aspect of address translation, and instead must have its own, local, fully-fledged MMU. That MMU, however, can and should be as application-specific as the accelerator itself, as our data indicates that even a 100% hit rate in a small, standard L1 Translation Lookaside Buffer (TLB) presents a substantial accelerator performance overhead. Furthermore, we isolate the benefits of individual MMU components (e.g., TLBs versus page table walkers) and discover that their relative performance, area, and energy are workload dependent, with their interplay resulting in different area-optimal and energy-optimal configurations.", "venue": "ArXiv", "authors": ["Yunsung  Kim", "Guilherme  Cox", "Martha A. Kim", "Abhishek  Bhattacharjee"], "year": 2017, "n_citations": 0}
{"id": 6604987, "s2_id": "0865375788556cc8f8f20648a42ac7048194e243", "title": "NimbRo-OP2X: Affordable Adult-sized 3D-printed Open-Source Humanoid Robot for Research", "abstract": "For several years, high development and production costs of humanoid robots restricted researchers interested in working in the field. To overcome this problem, several research groups have opted to work with simulated or smaller robots, whose acquisition costs are significantly lower. However, due to scale differences and imperfect simulation replicability, results may not be directly reproducible on real, adult-sized robots. In this paper, we present the NimbRo-OP2X, a capable and affordable adult-sized humanoid platform aiming to significantly lower the entry barrier for humanoid robot research. With a height of 135 cm and weight of only 19 kg, the robot can interact in an unmodified, human environment without special safety equipment. Modularity in hardware and software allow this platform enough flexibility to operate in different scenarios and applications with minimal effort. The robot is equipped with an on-board computer with GPU, which enables the implementation of state-of-the-art approaches for object detection and human perception demanded by areas such as manipulation and human-robot interaction. Finally, the capabilities of the NimbRo-OP2X, especially in terms of locomotion stability and visual perception, are evaluated. This includes the performance at RoboCup 2018, where NimbRo-OP2X won all possible awards in the AdultSize class.", "venue": "Int. J. Humanoid Robotics", "authors": ["Grzegorz  Ficht", "Hafez  Farazi", "Diego  Rodriguez", "Dmytro  Pavlichenko", "Philipp  Allgeuer", "Andre  Brandenburger", "Sven  Behnke"], "year": 2020, "n_citations": 1}
{"id": 6606672, "s2_id": "d125e31f35fef2610ac4c407d4711dfd22929748", "title": "Low-Cost Stochastic Number Generators for Stochastic Computing", "abstract": "Stochastic unary computing provides low-area circuits. However, the required area consuming stochastic number generators (SNGs) in these circuits can diminish their overall gain in area, particularly if several SNGs are required. We propose area-efficient SNGs by sharing the permuted output of one linear feedback shift register (LFSR) among several SNGs. With no hardware overhead, the proposed architecture generates stochastic bit streams with minimum stochastic computing correlation (SCC). Compared to the circular shifting approach presented in prior work, our approach produces stochastic bit streams with 67% less average SCC when a 10-bit LFSR is shared between two SNGs. To generalize our approach, we propose an algorithm to find a set of <inline-formula> <tex-math notation=\"LaTeX\">$m$ </tex-math></inline-formula> permutations (<inline-formula> <tex-math notation=\"LaTeX\">$n > m > 2$ </tex-math></inline-formula>) with a minimum pairwise SCC, for an <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>-bit LFSR. The search space for finding permutations with an exact minimum SCC grows rapidly when <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> increases and it is intractable to perform a search algorithm using accurately calculated pairwise SCC values, for <inline-formula> <tex-math notation=\"LaTeX\">$n > 9$ </tex-math></inline-formula>. We propose a similarity function that can be used in the proposed search algorithm to quickly find a set of permutations with SCC values close to the minimum one. We evaluate our approach for several applications. The results show that, compared to prior work, it achieves lower mean-squared error (MSE) with the same (or even lower) area. Additionally, based on simulation results, we show that replacing the comparator component of an SNG circuit with a weighted binary generator can reduce SCC.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Sayed Ahmad Salehi"], "year": 2020, "n_citations": 5}
{"id": 6607143, "s2_id": "51c12875f5e32469727c8369481c17ae1bf66300", "title": "Analyzing crosstalk error in the NISQ era", "abstract": "Noisy Intermediate-Scale Quantum (NISQ) hardware has unavoidable noises, and crosstalk error is a significant error source. When multiple quantum operations are executed simultaneously, the quantum state can be corrupted due to the crosstalk between gates during simultaneous operations, decreasing the circuit fidelity. In this work, we first report on several protocols for characterizing crosstalk. Then, we discuss different crosstalk mitigation methods from the hardware and software perspectives. Finally, we perform crosstalk injection experiments on the IBM quantum device and demonstrate the fidelity improvement with the crosstalk mitigation method.", "venue": "2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)", "authors": ["Siyuan  Niu", "Aida  Todri"], "year": 2021, "n_citations": 0}
{"id": 6607561, "s2_id": "5319040656bc9671fc46012f6f7d1937a1e93e1a", "title": "MLPerf Tiny Benchmark", "abstract": "Advancements in ultra-low-power tiny machine learning (TinyML) systems promise to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted and easily reproducible benchmark for these systems. To meet this need, we present MLPerf Tiny, the first industry-standard benchmark suite for ultra-low-power tiny machine learning systems. The benchmark suite is the collaborative effort of more than 50 organizations from industry and academia and reflects the needs of the community. MLPerf Tiny measures the accuracy, latency, and energy of machine learning inference to properly evaluate the tradeoffs between systems. Additionally, MLPerf Tiny implements a modular design that enables benchmark submitters to show the benefits of their product, regardless of where it falls on the ML deployment stack, in a fair and reproducible manner. The suite features four benchmarks: keyword spotting, visual wake words, image classification, and anomaly detection.", "venue": "ArXiv", "authors": ["Colby  Banbury", "Vijay Janapa Reddi", "Peter  Torelli", "Jeremy  Holleman", "Nat  Jeffries", "Csaba  Kiraly", "Pietro  Montino", "David  Kanter", "Sebastian  Ahmed", "Danilo  Pau", "Urmish  Thakker", "Antonio  Torrini", "Peter  Warden", "Jay  Cordaro", "Giuseppe Di Guglielmo", "Javier  Duarte", "Stephen  Gibellini", "Videet  Parekh", "Honson  Tran", "Nhan  Tran", "Niu  Wenxu", "Xu  Xuesong"], "year": 2021, "n_citations": 3}
{"id": 6611437, "s2_id": "e716867e587f0247360abbc9b532242a7e0eb318", "title": "Data-Centric and Data-Aware Frameworks for Fundamentally Efficient Data Handling in Modern Computing Systems", "abstract": "There is an explosive growth in the size of the input and/or intermediate data used and generated by modern and emerging applications. Unfortunately, modern computing systems are not capable of handling large amounts of data efficiently. Major concepts and components (e.g., the virtual memory system) and predominant execution models (e.g., the processor-centric execution model) used in almost all computing systems are designed without having modern applications' overwhelming data demand in mind. As a result, accessing, moving, and processing large amounts of data faces important challenges in today's systems, making data a first-class concern and a prime performance and energy bottleneck in such systems. This thesis studies the root cause of inefficiency in modern computing systems when handling modern applications' data demand, and aims to fundamentally address such inefficiencies, with a focus on two directions. First, we design SIMDRAM, an end-to-end processing-using-DRAM framework that aids the widespread adoption of processing-using-DRAM, a data-centric computation paradigm that improves the overall performance and efficiency of the system when computing large amounts of data by minimizing the cost of data movement and enabling computation where the data resides. Second, we introduce the Virtual Block Interface (VBI), a novel virtual memory framework that 1) eliminates the inefficiencies of the conventional virtual memory frameworks when handling the high memory demand in modern applications, and 2) is built from the ground up to understand, convey, and exploit data properties, to create opportunities for performance and efficiency improvements.", "venue": "ArXiv", "authors": ["Nastaran  Hajinazar"], "year": 2021, "n_citations": 1}
{"id": 6614176, "s2_id": "89e2f1d1970db3f993130c245c5bb56d3d0e54f2", "title": "Application Specific Cache Simulation Analysis for Application Specific Instruction set Processor", "abstract": "An Efficient Simulation of application specific instruction-set processors (ASIP) is a challenging onus in the area of VLSI design. This paper reconnoiters the possibility of use of ASIP simulators for ASIP Simulation. This proposed study allow as the simulation of the cache memory design with various ASIP simulators like Simple scalar and VEX. In this paper we have implemented the memory configuration according to desire application. These simulators performs the cache related results such as cache name, sets, cache associativity, cache block size, cache replacement policy according to specific application.", "venue": "ArXiv", "authors": ["Ravi  Khatwal", "Manoj Kumar Jain"], "year": 2014, "n_citations": 1}
{"id": 6614488, "s2_id": "cca13566d0795736079db5c1a835cfa9edc7d467", "title": "Threshold Logic in a Flash", "abstract": "This paper describes a novel design of a threshold logic gate (a binary perceptron) and its implementation as a standard cell. This new cell structure, referred to as flash threshold logic (FTL), uses floating gate (flash) transistors to realize the weights associated with a threshold function. The threshold voltages of the flash transistors serve as proxy for the weights. An FTL cell can be equivalently viewed as a multi-input, edge-triggered flipflop which computes a threshold function on a clock edge. Consequently it can used in automatic synthesis of ASICs. The use of flash transistors in the FTL cell allows programming of the weights after fabrication, thereby preventing discovery of its function by a foundry or by reverse engineering. This paper focuses on the design and characteristics of the FTL cell. We present a novel method for programming the weights of an FTL cell for a specified threshold function using a modified perceptron learning algorithm. The algorithm is further extended to select weights to maximize the robustness of the design in the presence of process variations. The FTL circuit was designed in 40nm technology and simulations with layout-extracted parasitics included, demonstrate significant improvements in area (79.7%), power (61.1%), and performance (42.5%) when compared to the equivalent implementations of the same function in conventional static CMOS design. Weight selection targeting robustness is demonstrated using Monte Carlo simulations. The paper also shows how FTL cells can be used for fixing timing errors after fabrication.", "venue": "2019 IEEE 37th International Conference on Computer Design (ICCD)", "authors": ["Ankit  Wagle", "Gian  Singh", "Jinghua  Yang", "Sunil  Khatri", "Sarma  Vrudhula"], "year": 2019, "n_citations": 4}
{"id": 6618639, "s2_id": "41d53ddd71dc728fb548f6503aa2e12c6cadb845", "title": "FPGA design of a cdma2000 turbo decoder", "abstract": "This paper presents the FPGA hardware design of a turbo decoder for the cdma2000 standard. The work includes a study and mathematical analysis of the turbo decoding process, based on the MAX-Log-MAP algorithm. Results of decoding for a packet size of two hundred fifty bits are presented, as well as an analysis of area versus performance, and the key variables for hardware design in turbo decoding.", "venue": "ArXiv", "authors": ["Maribell Sacanamboy Franco", "Fabio G. Guerrero"], "year": 2014, "n_citations": 0}
{"id": 6619539, "s2_id": "da522b2f20abc60310ac09c5b18a2aad30f7715b", "title": "Attacks on Wireless Coexistence: Exploiting Cross-Technology Performance Features for Inter-Chip Privilege Escalation", "abstract": "Modern mobile devices feature multiple wireless technologies, such as Bluetooth, Wi-Fi, and LTE. Each of them is implemented within a separate wireless chip, sometimes packaged as combo chips. However, these chips share components and resources, such as the same antenna or wireless spectrum. Wireless coexistence interfaces enable them to schedule packets without collisions despite shared resources, essential to maximizing networking performance. Today\u2019s hardwired coexistence interfaces hinder clear security boundaries and separation between chips and chip components. This paper shows practical coexistence attacks on Broadcom, Cypress, and Silicon Labs chips deployed in billions of devices. For example, we demonstrate that a Bluetooth chip can directly extract network passwords and manipulate traffic on a Wi-Fi chip. Coexistence attacks enable a novel type of lateral privilege escalation across chip boundaries. We responsibly disclosed the vulnerabilities to the vendors. Yet, only partial fixes were released for existing hardware since wireless chips would need to be redesigned from the ground up to prevent the presented attacks on coexistence.", "venue": "ArXiv", "authors": ["Jiska  Classen", "Francesco  Gringoli", "Michael  Hermann", "Matthias  Hollick"], "year": 2021, "n_citations": 0}
{"id": 6623385, "s2_id": "3e34241d69cfe16b9e337e1dbe333289e7adf7b3", "title": "Fast FPGA Emulation of Analog Dynamics in Digitally-Driven Systems", "abstract": "In this paper, we propose an architecture for FPGA emulation of mixed-signal systems that achieves high accuracy at a high throughput. We represent the analog output of a block as a superposition of step responses to changes in its analog input, and the output is evaluated only when needed by the digital subsystem. Our architecture is therefore intended for digitally-driven systems; that is, those in which the inputs of analog dynamical blocks change only on digital clock edges. We implemented a high-speed link transceiver design using the proposed architecture on a Xilinx FPGA. This design demonstrates how our approach breaks the link between simulation rate and time resolution that is characteristic of prior approaches. The emulator is flexible, allowing for the real-time adjustment of analog dynamics, clock jitter, and various design parameters. We demonstrate that our architecture achieves 1% accuracy while running 3 orders of magnitude faster than a comparable high-performance CPU simulation.", "venue": "2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Steven  Herbst", "ByongChan  Lim", "Mark  Horowitz"], "year": 2018, "n_citations": 3}
{"id": 6625667, "s2_id": "c97ed6b64aeb84c36f0d98aee92fd4745d3f78f7", "title": "Closed-Loop Neural Prostheses With On-Chip Intelligence: A Review and a Low-Latency Machine Learning Model for Brain State Detection", "abstract": "The application of closed-loop approaches in systems neuroscience and therapeutic stimulation holds great promise for revolutionizing our understanding of the brain and for developing novel neuromodulation therapies to restore lost functions. Neural prostheses capable of multi-channel neural recording, on-site signal processing, rapid symptom detection, and closed-loop stimulation are critical to enabling such novel treatments. However, the existing closed-loop neuromodulation devices are too simplistic and lack sufficient on-chip processing and intelligence. In this paper, we first discuss both commercial and investigational closed-loop neuromodulation devices for brain disorders. Next, we review state-of-the-art neural prostheses with on-chip machine learning, focusing on application-specific integrated circuits (ASIC). System requirements, performance and hardware comparisons, design trade-offs, and hardware optimization techniques are discussed. To facilitate a fair comparison and guide design choices among various on-chip classifiers, we propose a new energy-area (E-A) efficiency figure of merit that evaluates hardware efficiency and multi-channel scalability. Finally, we present several techniques to improve the key design metrics of tree-based on-chip classifiers, both in the context of ensemble methods and oblique structures. A novel Depth-Variant Tree Ensemble (DVTE) is proposed to reduce processing latency (e.g., by 2.5\u00d7 on seizure detection task). We further develop a cost-aware learning approach to jointly optimize the power and latency metrics. We show that algorithm-hardware co-design enables the energy- and memory-optimized design of tree-based models, while preserving a high accuracy and low latency. Furthermore, we show that our proposed tree-based models feature a highly interpretable decision process that is essential for safety-critical applications such as closed-loop stimulation.", "venue": "IEEE Transactions on Biomedical Circuits and Systems", "authors": ["Bingzhao  Zhu", "Uisub  Shin", "Mahsa  Shoaran"], "year": 2021, "n_citations": 1}
{"id": 6625822, "s2_id": "46e7dbcc5db8f46db4c670c342b6dd8d91ee0bee", "title": "HMTT: A Hybrid Hardware/Software Tracing System for Bridging Memory Trace's Semantic Gap", "abstract": "Memory trace analysis is an important technology for architecture research, system software (i.e., OS, compiler) optimization, and application performance improvements. Hardware-snooping is an effective and efficient approach to monitor and collect memory traces. Compared with software-based approaches, memory traces collected by hardware-based approaches are usually lack of semantic information, such as process/function/loop identifiers, virtual address and I/O access. In this paper we propose a hybrid hardware/software mechanism which is able to collect memory reference trace as well as semantic information. Based on this mechanism, we designed and implemented a prototype system called HMTT (Hybrid Memory Trace Tool) which adopts a DIMMsnooping mechanism to snoop on memory bus and a software-controlled tracing mechanism to inject semantic information into normal memory trace. To the best of our knowledge, the HMTT system is the first hardware tracing system capable of correlating memory trace with high-level events. Comprehensive validations and evaluations show that the HMTT system has both hardware's (e.g., no distortion or pollution) and software's advantages (e.g., flexibility and more information).", "venue": "ArXiv", "authors": ["Yungang  Bao", "Jinyong  Zhang", "Yan  Zhu", "Dan  Tang", "Yuan  Ruan", "Mingyu  Chen", "Jianping  Fan"], "year": 2011, "n_citations": 3}
{"id": 6626350, "s2_id": "f609696914d443b3810f3bf93dab4b88f1fa7efd", "title": "On the Complexity of Cache Analysis for Different Replacement Policies", "abstract": "Modern processors use cache memory, a memory access that \u201chits\u201d the cache returns early, while a \u201cmiss\u201d takes more time. Given a memory access in a program, cache analysis consists in deciding whether this access is always a hit, always a miss, or is a hit or a miss depending on execution. Such an analysis is of high importance for bounding the worst-case execution time of safety-critical real-time programs. There exist multiple possible policies for evicting old data from the cache when new data are brought in, and different policies, though apparently similar in goals and performance, may be very different from the analysis point of view. In this article, we explore these differences from a complexity-theoretical point of view. Specifically, we show that, among the common replacement policies, Least Recently Used is the only one whose analysis is NP-complete, whereas the analysis problems for the other policies are PSPACE-complete.", "venue": "J. ACM", "authors": ["David  Monniaux", "Valentin  Touzeau"], "year": 2019, "n_citations": 0}
{"id": 6628376, "s2_id": "952ec800bb69063afb0abba7c1f446d21a9475d1", "title": "Modular approach to data preprocessing in ALOHA and application to a smart industry use case", "abstract": "Applications in the smart industry domain, such as interaction with collaborative robots using vocal commands or machine vision systems often requires the deployment of deep learning algorithms on heterogeneous low power computing platforms. The availability of software tools and frameworks to automatize different design steps can support the effective implementation of DL algorithms on embedded systems, reducing related effort and costs. One very important aspect for the acceptance of the framework, is its extensibility, i.e. the capability to accommodate different datasets and define customized preprocessing, without requiring advanced skills. The paper addresses a modular approach, integrated into the ALOHA 1 tool flow, to support the data preprocessing and transformation pipeline. This is realized through customizable plugins and allows the easy extension of the tool flow to encompass new use cases. To demonstrate the effectiveness of the approach, we present some experimental results related to a keyword spotting use case and we outline possible extensions to different use cases. Keywords\u2014 Deep Learning, flows and tools, computer-aided design, edge computing.", "venue": "ArXiv", "authors": ["Cristina  Chesta", "Luca  Rinelli"], "year": 2021, "n_citations": 0}
{"id": 6630123, "s2_id": "b7b33acae9dab188e21a744186160ca39130b0ec", "title": "Ohm-GPU: Integrating New Optical Network and Heterogeneous Memory into GPU Multi-Processors", "abstract": "Traditional graphics processing units (GPUs) suffer from the low memory capacity and demand for high memory bandwidth. To address these challenges, we propose Ohm-GPU, a new optical network based heterogeneous memory design for GPUs. Specifically, Ohm-GPU can expand the memory capacity by combing a set of high-density 3D XPoint and DRAM modules as heterogeneous memory. To prevent memory channels from throttling throughput of GPU memory system, Ohm-GPU replaces the electrical lanes in the traditional memory channel with a high-performance optical network. However, the hybrid memory can introduce frequent data migrations between DRAM and 3D XPoint, which can unfortunately occupy the memory channel and increase the optical network traffic. To prevent the intensive data migrations from blocking normal memory services, Ohm-GPU revises the existing memory controller and designs a new optical network infrastructure, which enables the memory channel to serve the data migrations and memory requests, in parallel. Our evaluation results reveal that Ohm-GPU can improve the performance by 181% and 27%, compared to a DRAM-based GPU memory system and the baseline optical network based heterogeneous memory system, respectively.", "venue": "MICRO", "authors": ["Jie  Zhang", "Myoungsoo  Jung"], "year": 2021, "n_citations": 0}
{"id": 6630650, "s2_id": "c238f2ba05228ebb44e22d2914646617ec2427d9", "title": "A unified polar decoder platform for low-power and low-cost devices", "abstract": "In this paper, we design a polar decoding platform for diverse application scenarios that require low-cost and lowpower communications. Specifically, prevalent polar decoders such as successive cancellation (SC), SC-list (SCL) and Fano decoders are all supported under the same architecture. Unlike high-throughput or low-latency decoders that promote parallelism, this architecture promotes serialization by repeatedly calling a \u201csub-process\u201d that is executed by a core module. The resulting serial SCL-8 decoder is only 3 times as big as an SC decoder. Cost and power are minimized through resource sharing and adaptive decoding techniques, etc. We carried out performance simulation and hardware implementation to evaluate the actual chip area and energy consumption.", "venue": "ArXiv", "authors": ["Jiajie  Tong", "Qifan  Zhang", "Huazi  Zhang", "Rong  Li", "Jun  Wang", "Wen  Tong"], "year": 2021, "n_citations": 0}
{"id": 6631277, "s2_id": "47cb601f5f3903614b96f1131f7f9d548207ae60", "title": "FATE: Fast and Accurate Timing Error Prediction Framework for Low Power DNN Accelerator Design", "abstract": "Deep neural networks (DNN) are increasingly being accelerated on application-specific hardware such as the Google TPU designed especially for deep learning. Timing speculation is a promising approach to further increase the energy efficiency of DNN accelerators. Architectural exploration for timing speculation requires detailed gate-level timing simulations that can be time-consuming for large DNNs which execute millions of multiply-and-accumulate (MAC) operations. In this paper we propose FATE, a new methodology for fast and accurate timing simulations of DNN accelerators like the Google TPU. FATE proposes two novel ideas: (i) DelayNet, a DNN based timing model for MAC units; and (ii) a statistical sampling methodology that reduces the number of MAC operations for which timing simulations are performed. We show that FATE results in between 8 \u00d7 \u221258\u00d7 speed-up in timing simulations, while introducing less than 2% error in classification accuracy estimates. We demonstrate the use of FATE by comparing a conventional DNN accelerator that uses 2's complement (2C) arithmetic with one that uses signed magnitude representation (SMR). We show that that the SMR implementation provides 18% more energy savings for the same classification accuracy than 2C, a result that might be of independent interest.", "venue": "2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)", "authors": ["Jeff Jun Zhang", "Siddharth  Garg"], "year": 2018, "n_citations": 13}
{"id": 6635051, "s2_id": "3a4ffcfe21ff4f95bbb15883a608da8460ba0901", "title": "Efficient Sparse Code Multiple Access Decoder Based on Deterministic Message Passing Algorithm", "abstract": "Being an effective non-orthogonal multiple access (NOMA) technique, sparse code multiple access (SCMA) is promising for future wireless communication. Compared with orthogonal techniques, SCMA enjoys higher overloading tolerance and lower complexity because of its sparsity. In this paper, based on deterministic message passing algorithm (DMPA), algorithmic simplifications such as domain changing and probability approximation are applied for SCMA decoding. Early termination, adaptive decoding, and initial noise reduction are also employed for faster convergence and better performance. Numerical results show that the proposed optimizations benefit both decoding complexity and speed. Furthermore, efficient hardware architectures based on folding and retiming are proposed. VLSI implementation is also given in this paper. Comparison with the state-of-the-art have shown the proposed decoder's advantages in both latency and throughput (multi-Gbps).", "venue": "IEEE Transactions on Vehicular Technology", "authors": ["Chuan  Zhang", "Chao  Yang", "Xu  Pang", "Wenqing  Song", "Wei  Xu", "Shunqing  Zhang", "Zaichen  Zhang", "Xiaohu  You"], "year": 2020, "n_citations": 7}
{"id": 6637041, "s2_id": "940d956a6858eabaaf346962f383cdc2746b89fc", "title": "A Learning-based Approach Towards Automated Tuning of SSD Configurations", "abstract": "Thanks to the mature manufacturing techniques, solid-state drives (SSDs) are highly customizable for applications today, which brings opportunities to further improve their storage performance and resource utilization. However, the SSD efficiency is usually determined by many hardware parameters, making it hard for developers to manually tune them and determine the optimal SSD configurations. In this paper, we present an automated learning-based framework, named LearnedSSD, that utilizes both supervised and unsupervised machine learning (ML) techniques to drive the tuning of hardware configurations for SSDs. LearnedSSD automatically extracts the unique access patterns of a new workload using its block I/O traces, maps the workload to previously workloads for utilizing the learned experiences, and recommends an optimal SSD configuration based on the validated storage performance. LearnedSSD accelerates the development of new SSD devices by automating the hardware parameter configurations and reducing the manual efforts. We develop LearnedSSD with simple yet effective learning algorithms that can run efficiently on multi-core CPUs. Given a target storage workload, our evaluation shows that LearnedSSD can always deliver an optimal SSD configuration for the target workload, and this configuration will not hurt the performance of non-target workloads.", "venue": "ArXiv", "authors": ["Daixuan  Li", "Jian  Huang"], "year": 2021, "n_citations": 0}
{"id": 6638664, "s2_id": "425985d716d7a133bd99457a01683f51dd367c78", "title": "An Efficient List Decoder Architecture for Polar Codes", "abstract": "Long polar codes can achieve the symmetric capacity of arbitrary binary-input discrete memoryless channels under a low-complexity successive cancelation (SC) decoding algorithm. However, for polar codes with short and moderate code lengths, the decoding performance of the SC algorithm is inferior. The cyclic-redundancy-check (CRC)-aided SC-list (SCL)-decoding algorithm has better error performance than the SC algorithm for short or moderate polar codes. In this paper, we propose an efficient list decoder architecture for the CRC-aided SCL algorithm, based on both algorithmic reformulations and architectural techniques. In particular, an area efficient message memory architecture is proposed to reduce the area of the proposed decoder architecture. An efficient path pruning unit suitable for large list size is also proposed. For a polar code of length 1024 and rate 1/2, when list size L=2 and 4, the proposed list decoder architecture is implemented under a Taiwan Semiconductor Manufacturing Company (TSMC) 90-nm CMOS technology. Compared with the list decoders in the literature, our decoder achieves 1.24-1.83 times the area efficiency.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "authors": ["Jun  Lin", "Zhiyuan  Yan"], "year": 2015, "n_citations": 59}
{"id": 6642218, "s2_id": "ead16f1d6238c0cc7805407255d3ed3ac54dfef7", "title": "Enabling Large-Reach TLBs for High-Throughput Processors by Exploiting Memory Subregion Contiguity", "abstract": "Accelerators, like GPUs, have become a trend to deliver future performance desire, and sharing the same virtual memory space between CPUs and GPUs is increasingly adopted to simplify programming. However, address translation, which is the key factor of virtual memory, is becoming the bottleneck of performance for GPUs. In GPUs, a single TLB miss can stall hundreds of threads due to the SIMT execute model, degrading performance dramatically. Through real system analysis, we observe that the OS shows an advanced contiguity (e.g., hundreds of contiguous pages), and more large memory regions with advanced contiguity tend to be allocated with the increase of working sets. Leveraging the observation, we propose MESC to improve the translation efficiency for GPUs. The key idea of MESC is to divide each large page frame (2MB size) in virtual memory space into memory subregions with fixed size (i.e., 64 4KB pages), and store the contiguity information of subregions and large page frames in L2PTEs. With MESC, address translations of up to 512 pages can be coalesced into single TLB entry, without the needs of changing memory allocation policy (i.e., demand paging) and the support of large pages. In the experimental results, MESC achieves 77.2% performance improvement and 76.4% reduction in dynamic translation energy for translation-sensitive workloads.", "venue": "ArXiv", "authors": ["Chao  Yu", "Yuebin  Bai", "Rui  Wang"], "year": 2021, "n_citations": 0}
{"id": 6644083, "s2_id": "3c7860cedfc6d4ed1368bfd9ade43157d2d8d555", "title": "New categories of Safe Faults in a processor-based Embedded System", "abstract": "The identification of safe faults (i.e., faults which are guaranteed not to produce any failure) in an electronic system is a crucial step when analyzing its dependability and its test plan development. Unfortunately, safe fault identification is poorly supported by available EDA tools, and thus remains an open problem. The complexity growth of modern systems used in safety-critical applications further complicates their identification. In this article, we identify some classes of safe faults within an embedded system based on a pipelined processor. A new method for automating the safe fault identification is also proposed. The safe faults belonging to each class are identified resorting to Automatic Test Pattern Generation (ATPG) techniques. The proposed methodology is applied to a sample system built around the OpenRisc1200 open source processor.", "venue": "2019 IEEE 22nd International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)", "authors": ["Cemil Cem G\u00fcrsoy", "Maksim  Jenihhin", "Adeboye Stephen Oyeniran", "Davide  Piumatti", "Jaan  Raik", "Matteo Sonza Reorda", "Raimund  Ubar"], "year": 2019, "n_citations": 2}
{"id": 6646554, "s2_id": "2ce0905c76e0f00a519494f4da46fee519af0489", "title": "A 237 Gbps Unrolled Hardware Polar Decoder", "abstract": "In this letter we present a new architecture for a polar decoder using a reduced complexity successive cancellation decoding algorithm. This novel fully-unrolled, deeply-pipelined architecture is capable of achieving a coded throughput of over 237 Gbps for a (1024,512) polar code implemented using an FPGA. This decoder is two orders of magnitude faster than state-of-the-art polar decoders.", "venue": "ArXiv", "authors": ["Pascal  Giard", "Gabi  Sarkis", "Claude  Thibeault", "Warren J. Gross"], "year": 2014, "n_citations": 37}
{"id": 6651621, "s2_id": "c322f24654eb018ec179f911b41bbbaca34e4018", "title": "Representing digital assets usingMPEG-21 Digital Item Declaration", "abstract": "Various XML-based approaches aimed at representing compound digital assets have emerged over the last several years. Approaches that are of specific relevance to the digital library community include the Metadata Encoding and Transmission Standard (METS), the IMS Content Packaging XML Binding, and the XML Formatted Data Units (XFDU) developed by CCSDS Panel 2. The MPEG-21 Digital Item Declaration (MPEG-21 DID) is another standard specifying the representation of digital assets in XML that, so far, has received little attention in the digital library community. This article gives a brief insight into the MPEG-21 standardization effort, highlights the major characteristics of the MPEG-21 DID Abstract Model, and describes the MPEG-21 Digital Item Declaration Language (MPEG-21 DIDL), an XML syntax for the representation of digital assets based on the MPEG-21 DID Abstract Model. Also, it briefly demonstrates the potential relevance of MPEG-21 DID to the digital library community by describing its use in the aDORe repository environment at the Research Library of the Los Alamos National Laboratory (LANL) for the representation of digital assets.", "venue": "International Journal on Digital Libraries", "authors": ["Jeroen  Bekaert", "Emiel De Kooning", "Herbert Van de Sompel"], "year": 2005, "n_citations": 31}
{"id": 6655242, "s2_id": "f40f8dfc4aa8761cd4817b564f75961447d3f3e4", "title": "Scalable and Efficient Virtual Memory Sharing in Heterogeneous SoCs with TLB Prefetching and MMU-Aware DMA Engine", "abstract": "Shared virtual memory (SVM) is key in heterogeneous systems on chip (SoCs), which combine a general-purpose host processor with a many-core accelerator, both for programmability and to avoid data duplication. However, SVM can bring a significant run time overhead when translation lookaside buffer (TLB) entries are missing. Moreover, allowing DMA burst transfers to write SVM traditionally requires buffers to absorb transfers that miss in the TLB. These buffers have to be overprovisioned for the maximum burst size, wasting precious on-chip memory, and stall all SVM accesses once they are full, hampering the scalability of parallel accelerators. In this work, we present our SVM solution that avoids the majority of TLB misses with prefetching, supports parallel burst DMA transfers without additional buffers, and can be scaled with the workload and number of parallel processors. Our solution is based on three novel concepts: To minimize the rate of TLB misses, the TLB is proactively filled by compiler-generated Prefetching Helper Threads, which use run-time information to issue timely prefetches. To reduce the latency of TLB misses, misses are handled by a variable number of parallel Miss Handling Helper Threads. To support parallel burst DMA transfers to SVM without additional buffers, we add lightweight hardware to a standard DMA engine to detect and react to TLB misses. Compared to the state of the art, our work improves accelerator performance for memory-intensive kernels by up to 4~ and by up to 60% for irregular and regular memory access patterns, respectively.", "venue": "2018 IEEE 36th International Conference on Computer Design (ICCD)", "authors": ["Andreas  Kurth", "Pirmin  Vogel", "Andrea  Marongiu", "Luca  Benini"], "year": 2018, "n_citations": 7}
{"id": 6655635, "s2_id": "d8df49acb537a2a0709172e81f211a2f95f5500b", "title": "A RISC-V Simulator and Benchmark Suite for Designing and Evaluating Vector Architectures", "abstract": "Vector architectures lack tools for research. Consider the gem5 simulator, which is possibly the leading platform for computer-system architecture research. Unfortunately, gem5 does not have an available distribution that includes a flexible and customizable vector architecture model. In consequence, researchers have to develop their own simulation platform to test their ideas, which consume much research time. However, once the base simulator platform is developed, another question is the following: Which applications should be tested to perform the experiments? The lack of Vectorized Benchmark Suites is another limitation. To face these problems, this work presents a set of tools for designing and evaluating vector architectures. First, the gem5 simulator was extended to support the execution of RISC-V Vector instructions by adding a parameterizable Vector Architecture model for designers to evaluate different approaches according to the target they pursue. Second, a novel Vectorized Benchmark Suite is presented: a collection composed of seven data-parallel applications from different domains that can be classified according to the modules that are stressed in the vector architecture. Finally, a study of the Vectorized Benchmark Suite executing on the gem5-based Vector Architecture model is highlighted. This suite is the first in its category that covers the different possible usage scenarios that may occur within different vector architecture designs such as embedded systems, mainly focused on short vectors, or High-Performance-Computing (HPC), usually designed for large vectors.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Crist\u00f3bal  Ram\u00edrez", "C\u00e9sar-Alejandro  Hern\u00e1ndez-Calder\u00f3n", "Oscar  Palomar", "Osman S. Unsal", "Marco A. Ram\u00edrez", "Adri\u00e1n  Cristal"], "year": 2020, "n_citations": 2}
{"id": 6657520, "s2_id": "5dda41cc95a5eeb0947de258f64420424da7a152", "title": "Energy Efficient Full Adder Cell Design With Using Carbon Nanotube Field Effect Transistors In 32 Nanometer Technology", "abstract": "Full Adder is one of the critical parts of logical and arithmetic units. So, presenting a low power full adder cell reduces the power consumption of the entire circuit. Also, using Nano-scale transistors, because of their unique characteristics will save energy consumption and decrease the chip area. In this paper we presented a low power full adder cell by using carbon nanotube field effect transistors (CNTFETs). Simulation results were carried out using HSPICE based on the CNTFET model in 32 nanometer technology in Different values of temperature and VDD.", "venue": "ArXiv", "authors": ["Ali  Ghorbani", "Ghazaleh  Ghorbani"], "year": 2014, "n_citations": 5}
{"id": 6659248, "s2_id": "b90c619809df340a401053b05266c2297d6d2c92", "title": "Revamping Storage Class Memory With Hardware Automated Memory-Over-Storage Solution", "abstract": "Large persistent memories such as NVDIMM have been perceived as a disruptive memory technology, because they can maintain the state of a system even after a power failure and allow the system to recover quickly. However, overheads incurred by a heavy software-stack intervention seriously negate the benefits of such memories. First, to significantly reduce the software stack overheads, we propose HAMS, a hardware auto-mated Memory-over-Storage (MoS) solution. Specifically, HAMS aggregates the capacity of NVDIMM and ultra-low latency flash archives (ULL-Flash) into a single large memory space, which can be used as a working memory expansion or persistent memory expansion, in an OS-transparent manner. HAMS resides in the memory controller hub and manages its MoS address pool over conventional DDR and NVMe interfaces; it employs a simple hardware cache to serve all the memory requests from the host MMU after mapping the storage space of ULL-Flash to the memory space of NVDIMM. Second, to make HAMS more energy-efficient and reliable, we propose an \"advanced HAMS\" which removes unnecessary data transfers between NVDIMM and ULL-Flash after optimizing the datapath and hardware modules of HAMS. This approach unleashes the ULL-Flash and its NVMe controller from the storage box and directly connects the HAMS datapath to NVDIMM over the conventional DDR4 interface. Our evaluations show that HAMS and advanced HAMS can offer 97% and 119% higher system performance than a software-based NVDIMM design, while costing 41% and 45% lower energy, respectively.", "venue": "2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)", "authors": ["Jie  Zhang", "Miryeong  Kwon", "Donghyun  Gouk", "Sungjoon  Koh", "Nam Sung Kim", "Mahmut Taylan Kandemir", "Myoungsoo  Jung"], "year": 2021, "n_citations": 0}
{"id": 6660419, "s2_id": "04621c422475a040e7a661a79559c64531dacc25", "title": "Intelligent Architectures for Intelligent Machines", "abstract": "Computing is bottlenecked by data. Large amounts of application data overwhelm storage capability, communication capability, and computation capability of the modern machines we design today. As a result, many key applications\u2019 performance, efficiency and scalability are bottlenecked by data movement. In this keynote talk, we describe three major shortcomings of modern architectures in terms of 1) dealing with data, 2) taking advantage of the vast amounts of data, and 3) exploiting different semantic properties of application data. We argue that an intelligent architecture should be designed to handle data well. We show that handling data well requires designing architectures based on three key principles: 1) data-centric, 2) data-driven, 3) data-aware. We give several examples for how to exploit each of these principles to design a much more efficient and high performance computing system. We especially discuss recent research that aims to fundamentally reduce memory latency and energy, and practically enable computation close to data, with at least two promising novel directions: 1) performing massively-parallel bulk operations in memory by exploiting the analog operational properties of memory, with low-cost changes, 2) exploiting the logic layer in 3D-stacked memory technology in various ways to accelerate important data-intensive applications. We discuss how to enable adoption of such fundamentally more intelligent architectures, which we believe are key to efficiency, performance, and sustainability. We conclude with some guiding principles for future computing architecture and system designs.", "venue": "2020 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)", "authors": ["Onur  Mutlu"], "year": 2020, "n_citations": 6}
{"id": 6660939, "s2_id": "b5e315ea210b63cd1e472c59d67339733f3f3a5d", "title": "Improving Inference Lifetime of Neuromorphic Systems via Intelligent Synapse Mapping", "abstract": "Non-Volatile Memories (NVMs) such as Resistive RAM (RRAM) are used In neuromorphic systems to Implement high-density and low-power analog synaptic weights. Unfortunately, an RRAM cell can switch its state after reading its content a certain number of times. Such behavior challenges the integrity and program-onee-read-many-times philosophy of implementing machine learning inference on neuromorphic systems, impacting the Quality-of-Serviee (QoS). Elevated temperatures and frequent usage can significantly shorten the number of times an RRAM cell can be reliably read before it becomes absolutely necessary to reprogram. We propose an architectural solution to extend the read endurance of RRAM-based neuromorphic systems. We make two key contributions. First, we formulate the read endurance of an RRAM cell as a function of the programmed synaptic weight and its activation within a machine learning workload. Second, we propose an intelligent workload mapping strategy incorporating the endurance formulation to place the synapses of a machine learning model onto the RRAM cells of the hardware. The objective is to extend the inference lifetime, defined as the number of times the model can be used to generate output (inference) before the trained weights need to be reprogrammed on the RRAM cells of the system. We evaluate our architectural solution with machine learning workloads on a eyele-aeeurate simulator of an RRAM-based neuromorphic system. Our results demonstrate a significant increase in inference lifetime with only a minimal performance impact.", "venue": "2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP)", "authors": ["Shihao  Song", "Twisha  Titirsha", "Anup  Das"], "year": 2021, "n_citations": 4}
{"id": 6676735, "s2_id": "c1138d3e44f9cdd4568271e4049b72aa2eef2f25", "title": "Hardware architectures for successive cancellation decoding of polar codes", "abstract": "The recently-discovered polar codes are widely seen as a major breakthrough in coding theory. These codes achieve the capacity of many important channels under successive cancellation decoding. Motivated by the rapid progress in the theory of polar codes, we propose a family of architectures for efficient hardware implementation of successive cancellation decoders. We show that such decoders can be implemented with O(n) processing elements and O(n) memory elements, while providing constant throughput. We also propose a technique for overlapping the decoding of several consecutive codewords, thereby achieving a significant speed-up factor. We furthermore show that successive cancellation decoding can be implemented in the logarithmic domain, thereby eliminating the multiplication and division operations and greatly reducing the complexity of each processing element.", "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "authors": ["Camille  Leroux", "Ido  Tal", "Alexander  Vardy", "Warren J. Gross"], "year": 2011, "n_citations": 193}
{"id": 6678328, "s2_id": "b69ca309ae5ca99ed2481b0ba187da71ac3bb635", "title": "A Case for Fine-grain Coherence Specialization in Heterogeneous Systems", "abstract": "Hardware specialization is becoming a key enabler of energyefficient performance. Future systems will be increasingly heterogeneous, integrating multiple specialized and programmable accelerators, each with different memory demands. Traditionally, communication between accelerators has been inefficient, typically orchestrated through explicit DMA transfers between different address spaces. More recently, industry has proposed unified coherent memory which enables implicit data movement and more data reuse, but often these interfaces limit the coherence flexibility available to heterogeneous systems. This paper demonstrates the benefits of fine-grained coherence specialization for heterogeneous systems. We propose an architecture that enables low-complexity independent specialization of each individual coherence request in heterogeneous workloads by building upon a simple and flexible baseline coherence interface, Spandex. We then describe how to optimize individual memory requests to improve cache reuse and performance-critical memory latency in emerging heterogeneous workloads. Collectively, our techniques enable significant gains, reducing execution time by up to 61% or network traffic by up to 99% while adding minimal complexity to the Spandex protocol.", "venue": "ArXiv", "authors": ["Johnathan  Alsop", "Weon Taek Na", "Matthew D. Sinclair", "Samuel  Grayson", "Sarita V. Adve"], "year": 2021, "n_citations": 0}
{"id": 6679448, "s2_id": "6f1498905f169bceb7f01252068a3fdff0b7bb13", "title": "At-speed logic BIST for IP cores", "abstract": "This paper describes a flexible logic BIST scheme that features high fault coverage achieved by fault-simulation guided test point insertion, real at-speed test capability for multi-clock designs without clock frequency manipulation, and easy physical implementation due to the use of a low-speed SE signal. Application results of this scheme to two widely used IP cores are also reported.", "venue": "Design, Automation and Test in Europe", "authors": ["B.  Cheon", "E.  Lee", "Laung-Terng  Wang", "Xiaoqing  Wen", "P.  Hsu", "J.  Cho", "J.  Park", "H.  Chao", "Shianling  Wu"], "year": 2005, "n_citations": 8}
{"id": 6679908, "s2_id": "e16338c32bc233b35e56c9b786fd9150d3c92cc8", "title": "CARAM: A Content-Aware Hybrid PCM/DRAM Main Memory System Framework", "abstract": "The emergence of Phase-Change Memory (PCM) provides opportunities for directly connecting persistent memory to main memory bus. While PCM achieves high read throughput and low standby power, the critical concerns are its poor write performance and limited durability, especially when compared to DRAM. A naturally inspired design is the hybrid memory architecture that fuses DRAM and PCM, so as to exploit the positive aspects of both types of memory. Unfortunately, existing solutions are seriously challenged by the limited main memory size, which is the primary bottleneck of in-memory computing. In this paper, we introduce a novel Content Aware hybrid PCM/DRAM main memory system framework - CARAM, which exploits deduplication to improve line sharing with high memory efficiency. CARAM effectively reduces write traffic to hybrid memory by removing unnecessary duplicate line writes. It also substantially extends available free memory space by coalescing redundant lines in hybrid memory, thereby further improving the wear-leveling efficiency of PCM. To obtain high data access performance, we also design a set of acceleration techniques to minimize the overhead caused by extra computation costs. Our experiment results show that CARAM effectively reduces 15%~42% of memory usage and improves I/O bandwidth by 13%~116%, while saving 31%~38% energy consumption, compared to the state-of-the-art of hybrid systems.", "venue": "NPC", "authors": ["Yinjin  Fu"], "year": 2020, "n_citations": 0}
{"id": 6683289, "s2_id": "f23c79c6908e8ae470fab8a2d8e71267cbeb450e", "title": "Uber: Utilizing Buffers to Simplify NoCs for Hundreds-Cores", "abstract": "Approaching ideal wire latency using a network-on-chip (NoC) is an important practical problem for many-core systems, particularly hundreds-cores. Although other researchers have focused on optimizing large meshes, bypassing or speculating router pipelines, or creating more intricate logarithmic topologies, this paper proposes a balanced combination that trades queue buffers for simplicity. Preliminary analysis of nine benchmarks from PARSEC and SPLASH using execution-driven simulation shows that utilization rises from 2% when connecting a single core per mesh port to at least 50%, as slack for delay in concentrator and router queues is around 6x higher compared to the ideal latency of just 20 cycles. That is, a 16-port mesh suffices because queueing is the uncommon case for system performance. In this way, the mesh hop count is bounded to three, as load becomes uniform via extended concentration, and ideal latency is approached using conventional four-stage pipelines for the mesh routers together with minor logarithmic edges. A realistic Uber is also detailed, featuring the same performance as a 64-port mesh that employs optimized router pipelines, improving the baseline by 12%. Ongoing work develops techniques to better balance load by tuning the placement of cache blocks, and compares Uber with bufferless routing.", "venue": "ArXiv", "authors": ["Giorgos  Passas"], "year": 2016, "n_citations": 0}
{"id": 6687180, "s2_id": "b17bd08744886622c8a2fbae6ea1acc9a43b7747", "title": "IRO: Integrity and Reliability Enhanced Ring ORAM", "abstract": "Memory security and reliability are two of the major design concerns in cloud computing systems. State-of-theart memory security-reliability co-designs (e.g. Synergy) have achieved a good balance on performance, confidentiality, integrity, and reliability. However, these works merely rely on encryption to ensure data confidentiality, which has been proven unable to prevent information leakage from memory access patterns. Ring ORAM is an attractive confidential protection protocol to hide memory access patterns to the untrusted storage system. Unfortunately, it does not compatible with the securityreliability co-designs. A forced combination would result in more severe performance loss. In this paper, we propose IRO, an Integrity and Reliability enhanced Ring ORAM design. To reduce the overhead of integrity verification, we propose a low overhead integrity tree RIT and use a Minimum Update Subtree Tree (MUST) to reduce metadata update overhead. To improve memory reliability, we present Secure Replication to provide channel-level error resilience for the ORAM tree and use the mirrored channel technique to guarantee the reliability of the MUST. Last, we use the error correction pointer (ECP) to repair permanent memory cell fault to further improve device reliability and lifetime. A compact metadata design is used to reduce the storage and consulting overhead of the ECP. IRO provides strong security and reliability guarantees, while the resulting storage and performance overhead is very small. Our evaluation shows that IRO only increases 7.54% execution time on average over the Baseline under two channels four AESGCM units setting. With enough AES-GCM units to perform concurrent MAC computing, IRO can reduce 2.14% execution time of the Baseline.", "venue": "ArXiv", "authors": ["Wenpeng  He", "Dan  Feng", "Fang  Wang"], "year": 2020, "n_citations": 0}
{"id": 6688577, "s2_id": "6729ccc7a40a8d3697c04067eef43d6f8c85ff20", "title": "Chaos in computer performance", "abstract": "Modern computer microprocessors are composed of hundreds of millions of transistors that interact through intricate protocols. Their performance during program execution may be highly variable and present aperiodic oscillations. In this paper, we apply current nonlinear time series analysis techniques to the performances of modern microprocessors during the execution of prototypical programs. Our results present pieces of evidence strongly supporting that the high variability of the performance dynamics during the execution of several programs display low-dimensional deterministic chaos, with sensitivity to initial conditions comparable to textbook models. Taken together, these results show that the instantaneous performances of modern microprocessors constitute a complex (or at least complicated) system and would benefit from analysis with modern tools of nonlinear and complexity science.", "venue": "Chaos", "authors": ["Hugues  Berry", "Daniel Gracia P\u00e9rez", "Olivier  Temam"], "year": 2006, "n_citations": 26}
{"id": 6688906, "s2_id": "03544291f71a70323998e8f70d0d97b41354fc4f", "title": "HourGlass: Predictable Time-based Cache Coherence Protocol for Dual-Critical Multi-Core Systems", "abstract": "We present a hardware mechanism called HourGlass to predictably share data in a multi-core system where cores are explicitly designated as critical or non-critical. HourGlass is a time-based cache coherence protocol for dual-critical multi-core systems that ensures worst-case latency (WCL) bounds for memory requests originating from critical cores. Although HourGlass does not provide either WCL or bandwidth guarantees for memory requests from non-critical cores, it promotes the use of timers to improve its bandwidth utilization while still maintaining WCL bounds for critical cores. This encourages a trade-off between the WCL bounds for critical cores, and the improved memory bandwidth for non-critical cores via timer configurations. We evaluate HourGlass using gem5, and with multithreaded benchmark suites including SPLASH-2, and synthetic workloads. Our results show that the WCL for critical cores with HourGlass is always within the analytical WCL bounds, and provides a tighter WCL bound on critical cores compared to the state-of-the-art real-time cache coherence protocol. Further, we show that HourGlass enables a trade-off between provable WCL bounds for critical cores, and improved bandwidth utilization for non-critical cores. The average-case performance of HourGlass is comparable to the state-of-the-art real-time cache coherence protocol, and suffers a slowdown of 1.43x and 1.46x compared to the conventional MSI and MESI protocols.", "venue": "ArXiv", "authors": ["Nivedita  Sritharan", "Anirudh  Kaushik", "Mohamed  Hassan", "Hiren D. Patel"], "year": 2017, "n_citations": 8}
{"id": 6690891, "s2_id": "cca4076507cc95e24d44e9b26bccb057740bcbb2", "title": "Firmware Insider: Bluetooth Randomness is Mostly Random", "abstract": "Bluetooth chips must include a Random Number Generator (RNG). This RNG is used internally within cryptographic primitives but also exposed to the operating system for chip-external applications. In general, it is a black box with security-critical authentication and encryption mechanisms depending on it. In this paper, we evaluate the quality of RNGs in various Broadcom and Cypress Bluetooth chips. We find that the RNG implementation significantly changed over the last decade. Moreover, most devices implement an insecure Pseudo-Random Number Generator (PRNG) fallback. Multiple popular devices, such as the Samsung Galaxy S8 and its variants as well as an iPhone, rely on the weak fallback due to missing a Hardware Random Number Generator (HRNG). We statistically evaluate the output of various HRNGs in chips used by hundreds of millions of devices. While the Broadcom and Cypress HRNGs pass advanced tests, it remains indistinguishable for users if a Bluetooth chip implements a secure RNG without an extensive analysis as in this paper. We describe our measurement methods and publish our tools to enable further public testing.", "venue": "WOOT @ USENIX Security Symposium", "authors": ["Jorn  Tillmanns", "Jiska  Classen", "Felix  Rohrbach", "Matthias  Hollick"], "year": 2020, "n_citations": 4}
{"id": 6693322, "s2_id": "9ab691f687ee89d256e32614b8dc613cfa0ff993", "title": "A Communication-Centric Observability Selection for Post-Silicon System-on-Chip Integration Debug", "abstract": "Reconstruction of how components communicate with each other during system execution is crucial for debugging system-on-chip designs. However, limited observability is the major obstacle to the efficient and accurate reconstruction in the post-silicon validation stage. This paper addresses that problem by proposing several communication event selection methods guided by system-level communication protocols. Such methods are optimized for on-chip communication event tracing infrastructure to enhance observability. The effectiveness of these methods are demonstrated with experiments on a non-trivial multicore SoC prototype. The results show that with the proposed method, more comprehensive information on system internal execution can be inferred from traces under limited observability.", "venue": "20th International Symposium on Quality Electronic Design (ISQED)", "authors": ["Yuting  Cao", "Hao  Zheng", "Sandip  Ray"], "year": 2019, "n_citations": 0}
{"id": 6694487, "s2_id": "53586238026afa4264d079dff84a342fc04f5c28", "title": "Mini-batch Serialization: CNN Training with Inter-layer Data Reuse", "abstract": "Training convolutional neural networks (CNNs) requires intense computations and high memory bandwidth. We find that bandwidth today is over-provisioned because most memory accesses in CNN training can be eliminated by rearranging computation to better utilize on-chip buffers and avoid traffic resulting from large per-layer memory footprints. We introduce the MBS CNN training approach that significantly reduces memory traffic by partially serializing mini-batch processing across groups of layers. This optimizes reuse within on-chip buffers and balances both intra-layer and inter-layer reuse. We also introduce the WaveCore CNN training accelerator that effectively trains CNNs in the MBS approach with high functional-unit utilization. Combined, WaveCore and MBS reduce DRAM traffic by 75%, improve performance by 53%, and save 26% system energy for modern deep CNN training compared to conventional training mechanisms and accelerators.", "venue": "MLSys", "authors": ["Sangkug  Lym", "Armand  Behroozi", "Wei  Wen", "Ge  Li", "Yongkee  Kwon", "Mattan  Erez"], "year": 2019, "n_citations": 13}
{"id": 6694864, "s2_id": "c84f4b8451c8382b1fba95a45c5cda9fcabfe60c", "title": "E-PUR: an energy-efficient processing unit for recurrent neural networks", "abstract": "Recurrent Neural Networks (RNNs) are a key technology for emerging applications such as automatic speech recognition, machine translation or image description. Long Short Term Memory (LSTM) networks are the most successful RNN implementation, as they can learn long term dependencies to achieve high accuracy. Unfortunately, the recurrent nature of LSTM networks significantly constrains the amount of parallelism and, hence, multicore CPUs and many-core GPUs exhibit poor efficiency for RNN inference. In this paper, we present E-PUR, an energy-efficient processing unit tailored to the requirements of LSTM computation. The main goal of E-PUR is to support large recurrent neural networks for low-power mobile devices. E-PUR provides an efficient hardware implementation of LSTM networks that is flexible to support diverse applications. One of its main novelties is a technique that we call Maximizing Weight Locality (MWL), which improves the temporal locality of the memory accesses for fetching the synaptic weights, reducing the memory requirements by a large extent. Our experimental results show that E-PUR achieves real-time performance for different LSTM networks, while reducing energy consumption by orders of magnitude with respect to general-purpose processors and GPUs, and it requires a very small chip area. Compared to a modern mobile SoC, an NVIDIA Tegra X1, E-PUR provides an average energy reduction of 88x.", "venue": "PACT", "authors": ["Franyell  Silfa", "Gem  Dot", "Jose-Maria  Arnau", "Antonio  Gonz\u00e1lez"], "year": 2018, "n_citations": 18}