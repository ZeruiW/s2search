{"id": 246, "s2_id": "dc40ea85970f40ef0dbb7340c96064543e1ab5f2", "title": "GenASiS Basics: Object-oriented utilitarian functionality for large-scale physics simulations (Version 2)", "abstract": "GenASiS Basics provides Fortran 2003 classes furnishing extensible objectoriented utilitarian functionality for large-scale physics simulations on distributed memory supercomputers. This functionality includes physical units and constants; display to the screen or standard output device; message passing; I/O to disk; and runtime parameter management and usage statistics. This revision\u2014Version 3 of Basics\u2014includes a significant name change, some minor additions to functionality, and a major addition to functionality: infrastructure facilitating the offloading of computational kernels to devices such as GPUs.", "venue": "Comput. Phys. Commun.", "authors": ["Christian Y. Cardall", "Reuben D. Budiardja"], "year": 2017, "n_citations": 4}
{"id": 6814, "s2_id": "61a6302f9b18b962f6e92839eaaa04e52a1a7187", "title": "FIESTA4: Optimized Feynman integral calculations with GPU support", "abstract": "Abstract This paper presents a new major release of the program FIESTA (Feynman Integral Evaluation by a Sector decomposiTion Approach). The new release is mainly aimed at optimal performance at large scales when one is increasing the number of sampling points in order to reduce the uncertainty estimates. The release now supports graphical processor units (GPUs) for the numerical integration, methods to optimize cluster-usage, as well as other speed, memory, and stability improvements. Program summary Program title: FIESTA4 Catalogue identifier: AECP_v4_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AECP_v4_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 575847 No. of bytes in distributed program, including test data, etc.: 43008976 Distribution format: tar.gz Programming language: Wolfram Mathematica 7.0 or higher, c++. Computer: From a desktop PC to a supercomputer. Operating system: Unix, Linux, Mac OS X. Has the code been vectorized or parallelized?: Yes RAM: depends on the complexity of the problem Catalogue identifier of previous version: AECP_v3_0 Journal reference of previous version: Comput. Phys. Comm. 185(2014)2090 Classification: 4.4, 4.12, 5, 6.5. External routines: Wolfram Mathematica [1], KyotoCabinet [2], Cuba [3], QHull [4] Does the new version supersede the previous version?: Yes Nature of problem: The sector decomposition approach to evaluating Feynman integrals falls apart into the sector decomposition itself, where one has to minimize the number of sectors; the pole resolution and epsilon expansion; and the numerical integration of the resulting expression. Moreover, in cases where the integrand is complex, one has to perform a contour deformation. Solution method: The program has a number of sector decomposition strategies. Everything except the integration is performed in Wolfram Mathematica [1] (required version is 7.0 or higher). This part of the calculation is parallelized with the use of shared memory. The database is stored on hard disk with the use of the KyotoCabinet [2] database engine. The integration part of the algorithm can be performed on a cluster. It is written in c++ and does not need Wolfram Mathematica. For integration we use the Cuba library package [3]. The sampling point evaluation has been vectorized. It can also use graphical processor units for the parallelization of sampling point evaluation. Reasons for new version: The main reason for the new version is the possibility to use GPUs for faster calculations. Summary of revisions: The main improvements are performance-related. The integration in the new version works 2\u20134 times faster, and another 2\u20134 times faster if one uses a GPU. Also there are multiple stability improvements. Restrictions: The complexity of the problem is mostly restricted by CPU time required to perform the integration and obtain a proper precision. Additional comments: For additional information see, http://science.sander.su , https://bitbucket.org/fiestaIntegrator/fiesta/overview Running time: Depends on the complexity of the problem. References: [1] http://www.wolfram.com/mathematica/ , commercial algebraic software; [2] http://fallabs.com/kyotocabinet/ , open source; [3] http://www.feynarts.de/cuba/ , open source; [4] http://www.qhull.org , open source.", "venue": "Comput. Phys. Commun.", "authors": ["A. V. Smirnov"], "year": 2016, "n_citations": 173}
{"id": 16265, "s2_id": "75e5df89fa0b9741a6f2b71f476f26d8aba2e24e", "title": "Stop talking to me - a communication-avoiding ADER-DG realisation", "abstract": "We present a communication- and data-sensitive formulation of ADER-DG for hyperbolic differential equation systems. Sensitive here has multiple flavours: First, the formulation reduces the persistent memory footprint. This reduces pressure on the memory subsystem. Second, the formulation realises the underlying predictor-corrector scheme with single-touch semantics, i.e., each degree of freedom is read on average only once per time step from the main memory. This reduces communication through the memory controllers. Third, the formulation breaks up the tight coupling of the explicit time stepping's algorithmic steps to mesh traversals. This averages out data access peaks. Different operations and algorithmic steps are ran on different grid entities. Finally, the formulation hides distributed memory data transfer behind the computation aligned with the mesh traversal. This reduces pressure on the machine interconnects. All techniques applied by our formulation are elaborated by means of a rigorous task formalism. They break up ADER-DG's tight causal coupling of compute steps and can be generalised to other predictor-corrector schemes.", "venue": "ArXiv", "authors": ["Dominic E. Charrier", "Tobias  Weinzierl"], "year": 2018, "n_citations": 14}
{"id": 18708, "s2_id": "fda71154149f192f15e633819d4408d95cc2385a", "title": "Automatic Mathematical Information Retrieval to Perform Translations up to Computer Algebra Systems", "abstract": "In mathematics, LaTeX is the de facto standard to prepare documents, e.g., scientific publications. While some formulae are still developed using pen and paper, more complicated mathematical expressions used more and more often with computer algebra systems. Mathematical expressions are often manually transcribed to computer algebra systems. The goal of my doctoral thesis is to improve the efficiency of this workflow. My envisioned method will automatically semantically enrich mathematical expressions so that they can be imported to computer algebra systems and other systems that can take advantage of the semantics, such as search engines or automatic plagiarism detection systems. These imports should preserve the essential semantic features of the expression.", "venue": "CICM Workshops", "authors": ["Andr\u00e9  Greiner-Petter"], "year": 2018, "n_citations": 0}
{"id": 23412, "s2_id": "40714f82d4df5f1bdc340c86ca064cd5738a876d", "title": "Optimized M2L Kernels for the Chebyshev Interpolation based Fast Multipole Method", "abstract": "A fast multipole method (FMM) for asymptotically smooth kernel functions (1/r, 1/r^4, Gauss and Stokes kernels, radial basis functions, etc.) based on a Chebyshev interpolation scheme has been introduced in [Fong et al., 2009]. The method has been extended to oscillatory kernels (e.g., Helmholtz kernel) in [Messner et al., 2012]. Beside its generality this FMM turns out to be favorable due to its easy implementation and its high performance based on intensive use of highly optimized BLAS libraries. However, one of its bottlenecks is the precomputation of the multiple-to-local (M2L) operator, and its higher number of floating point operations (flops) compared to other FMM formulations. Here, we present several optimizations for that operator, which is known to be the costliest FMM operator. The most efficient ones do not only reduce the precomputation time by a factor up to 340 but they also speed up the matrix-vector product. We conclude with comparisons and numerical validations of all presented optimizations.", "venue": "ArXiv", "authors": ["Matthias  Messner", "B\u00e9renger  Bramas", "Olivier  Coulaud", "Eric  Darve"], "year": 2012, "n_citations": 23}
{"id": 24355, "s2_id": "ee7662547725895ee7fd3a0b7b1d0f29a9ebfe50", "title": "Finite Element Integration with Quadrature on the GPU", "abstract": "We present a novel, quadrature-based finite element integration method for low-order elements on GPUs, using a pattern we call \\textit{thread transposition} to avoid reductions while vectorizing aggressively. On the NVIDIA GTX580, which has a nominal single precision peak flop rate of 1.5 TF/s and a memory bandwidth of 192 GB/s, we achieve close to 300 GF/s for element integration on first-order discretization of the Laplacian operator with variable coefficients in two dimensions, and over 400 GF/s in three dimensions. From our performance model we find that this corresponds to 90\\% of our measured achievable bandwidth peak of 310 GF/s. Further experimental results also match the predicted performance when used with double precision (120 GF/s in two dimensions, 150 GF/s in three dimensions). Results obtained for the linear elasticity equations (220 GF/s and 70 GF/s in two dimensions, 180 GF/s and 60 GF/s in three dimensions) also demonstrate the applicability of our method to vector-valued partial differential equations.", "venue": "ArXiv", "authors": ["Matthew G. Knepley", "Karl  Rupp", "Andy R. Terrel"], "year": 2016, "n_citations": 4}
{"id": 29176, "s2_id": "4b4797b727348825fe6c77283d54c4e047a2988d", "title": "Implementing Strassen's Algorithm with CUTLASS on NVIDIA Volta GPUs", "abstract": "Conventional GPU implementations of Strassen's algorithm (Strassen) typically rely on the existing high-performance matrix multiplication (GEMM), trading space for time. As a result, such approaches can only achieve practical speedup for relatively large, \"squarish\" matrices due to the extra memory overhead, and their usages are limited due to the considerable workspace. We present novel Strassen primitives for GPUs that can be composed to generate a family of Strassen algorithms. Our algorithms utilize both the memory and thread hierarchies on GPUs, reusing shared memory and register files inherited from GEMM, fusing additional operations, and avoiding extra workspace. We further exploit intra- and inter-kernel parallelism by batching, streaming, and employing atomic operations. We also develop a performance model for NVIDIA Volta GPUs to select the appropriate blocking parameters and predict the performance for GEMM and Strassen. Overall, our 1-level Strassen can achieve up to 1.11x speedup with a crossover point as small as 1,536 compared to cublasSgemm on a NVIDIA Tesla V100 GPU. With additional workspace, our 2-level Strassen can achieve 1.19x speedup with a crossover point at 7,680.", "venue": "ArXiv", "authors": ["Jianyu  Huang", "Chenhan D. Yu", "Robert A. van de Geijn"], "year": 2018, "n_citations": 6}
{"id": 31718, "s2_id": "44c91c8c7ba418a5d5c574f1a67e458ab43660c2", "title": "Performance Analysis of Effective Symbolic Methods for Solving Band Matrix SLAEs", "abstract": "This paper presents an experimental performance study of implementations of three symbolic algorithms for solving band matrix systems of linear algebraic equations with heptadiagonal, pentadiagonal, and tridiagonal coefficient matrices. The only assumption on the coefficient matrix in order for the algorithms to be stable is nonsingularity. These algorithms are implemented using the GiNaC library of C++ and the SymPy library of Python, considering five different data storing classes. Performance analysis of the implementations is done using the high-performance computing (HPC) platforms \u201cHybriLIT\u201d and \u201cAvitohol\u201d. The experimental setup and the results from the conducted computations on the individual computer systems are presented and discussed. An analysis of the three algorithms is performed.", "venue": "EPJ Web of Conferences", "authors": ["Milena  Veneva", "Alexander  Ayriyan"], "year": 2019, "n_citations": 2}
{"id": 41121, "s2_id": "8e6ebc553eb7489349ac983b671ee3b2f5fbc778", "title": "Misfortunes of a mathematicians' trio using Computer Algebra Systems: Can we trust?", "abstract": "Computer algebra systems are a great help for mathematical research but sometimes unexpected errors in the software can also badly affect it. As an example, we show how we have detected an error of Mathematica computing determinants of matrices of integer numbers: not only it computes the determinants wrongly, but also it produces different results if one evaluates the same determinant twice.", "venue": "ArXiv", "authors": ["Antonio J. Dur\u00e1n Guarde\u00f1o", "Mario  P\u00e9rez", "Juan Luis Varona"], "year": 2013, "n_citations": 15}
{"id": 41413, "s2_id": "a4901727c359ab19ef69b393b4fb08173d6eefdc", "title": "High-Performance Tensor Contraction without Transposition", "abstract": "Tensor computations---in particular tensor contraction (TC)---are important kernels in many scientific computing applications. Due to the fundamental similarity of TC to matrix multiplication and to the availability of optimized implementations such as the BLAS, tensor operations have traditionally been implemented in terms of BLAS operations, incurring both a performance and a storage overhead. Instead, we implement TC using the flexible BLAS-like Instantiation Software (BLIS) framework, which allows for transposition (reshaping) of the tensor to be fused with internal partitioning and packing operations, requiring no explicit transposition operations or additional workspace. This implementation, TBLIS, achieves performance approaching that of matrix multiplication, and in some cases considerably higher than that of traditional TC. Our implementation supports multithreading using an approach identical to that used for matrix multiplication in BLIS, with similar performance characteristics. The complexity...", "venue": "SIAM J. Sci. Comput.", "authors": ["Devin A. Matthews"], "year": 2018, "n_citations": 40}
{"id": 41779, "s2_id": "8fbd1717eedee2c589d81dfcb4f418292fb9eb02", "title": "A Formally Verified Proof of the Central Limit Theorem", "abstract": "We describe a proof of the Central Limit Theorem that has been formally verified in the Isabelle proof assistant. Our formalization builds upon and extends Isabelle\u2019s libraries for analysis and measure-theoretic probability. The proof of the theorem uses characteristic functions, which are a kind of Fourier transform, to demonstrate that, under suitable hypotheses, sums of random variables converge weakly to the standard normal distribution. We also discuss the libraries and infrastructure that supported the formalization, and reflect on some of the lessons we have learned from the effort.", "venue": "Journal of Automated Reasoning", "authors": ["Jeremy  Avigad", "Johannes  H\u00f6lzl", "Luke  Serafin"], "year": 2017, "n_citations": 29}
{"id": 43743, "s2_id": "a119b783940e198e51c622ab0159afda53314d60", "title": "TTC: a tensor transposition compiler for multiple architectures", "abstract": "We consider the problem of transposing tensors of arbitrary dimension and describe TTC, an open source domain-specific parallel compiler. TTC generates optimized parallel C++/CUDA C code that achieves a significant fraction of the system's peak memory bandwidth. TTC exhibits high performance across multiple architectures, including modern AVX-based systems (e.g., Intel Haswell, AMD Steamroller), Intel's Knights Corner as well as different CUDA-based GPUs such as NVIDIA's Kepler and Maxwell architectures. We report speedups of TTC over a meaningful baseline implementation generated by external C++ compilers; the results suggest that a domain-specific compiler can outperform its general purpose counterpart significantly: For instance, comparing with Intel's latest C++ compiler on the Haswell and Knights Corner architecture, TTC yields speedups of up to 8x and 32x, respectively. We also showcase TTC's support for multiple leading dimensions, making it a suitable candidate for the generation of performance-critical packing functions that are at the core of the ubiquitous BLAS 3 routines.", "venue": "ARRAY@PLDI", "authors": ["Paul  Springer", "Aravind  Sankaran", "Paolo  Bientinesi"], "year": 2016, "n_citations": 16}
{"id": 47815, "s2_id": "5a4c1c835d91d08d06f59119fb55121fe38acfe1", "title": "A Bird's Eye View of Matrix Distributed Processing", "abstract": "We present Matrix Distributed Processing, a C++ library for fast development of efficient parallel algorithms. MDP is based on MPI and consists of a collection of C++ classes and functions such as lattice, site and field. Once an algorithm is written using these components the algorithm is automatically parallel and no explicit call to communication functions is required. MDP is particularly suitable for implementing parallel solvers for multi-dimensional differential equations and mesh-like problems.", "venue": "ICCSA", "authors": ["Massimo Di Pierro"], "year": 2003, "n_citations": 0}
{"id": 56194, "s2_id": "64b1ef566b6152fbefc43bced71b725d89d141e3", "title": "Composing Scalable Nonlinear Algebraic Solvers", "abstract": "Most efficient linear solvers use composable algorithmic components, with the most common model being the combination of a Krylov accelerator and one or more preconditioners. A similar set of concepts may be used for nonlinear algebraic systems, where nonlinear composition of different nonlinear solvers may significantly improve the time to solution. We describe the basic concepts of nonlinear composition and preconditioning and present a number of solvers applicable to nonlinear partial differential equations. We have developed a software framework in order to easily explore the possible combinations of solvers. We show that the performance gains from using composed solvers can be substantial compared with gains from standard Newton-Krylov methods.", "venue": "SIAM Rev.", "authors": ["Peter R. Brune", "Matthew G. Knepley", "Barry  Smith", "Xuemin  Tu"], "year": 2015, "n_citations": 88}
{"id": 57756, "s2_id": "2de12da60acd4ef2eee8408ec608f6f00f468345", "title": "Bayes Blocks: An Implementation of the Variational Bayesian Building Blocks Framework", "abstract": "A software library for constructing and learning probabilistic models is presented. The library offers a set of building blocks from which a large variety of static and dynamic models can be built. These include hierarchical models for variances of other variables and many nonlinear models. The underlying variational Bayesian machinery, providing for fast and robust estimation but being mathematically rather involved, is almost completely hidden from the user thus making it very easy to use the library. The building blocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variables and computational nodes which can be combined rather freely.", "venue": "UAI", "authors": ["Markus  Harva", "Tapani  Raiko", "Antti  Honkela", "Harri  Valpola", "Juha  Karhunen"], "year": 2005, "n_citations": 10}
{"id": 58697, "s2_id": "5889b156cdd7b1f0ac4e0d2f9ef100edd7166cb2", "title": "Optimizing Block-Sparse Matrix Multiplications on CUDA with TVM", "abstract": "We implemented and optimized matrix multiplications between dense and block-sparse matrices on CUDA. We leveraged TVM, a deep learning compiler, to explore the schedule space of the operation and generate efficient CUDA code. With the automatic parameter tuning in TVM, our cross-thread reduction based implementation achieved competitive or better performance compared with other state-of-the-art frameworks.", "venue": "ArXiv", "authors": ["Zijing  Gu"], "year": 2020, "n_citations": 0}
{"id": 61369, "s2_id": "dcc7a663b45cf2836935c014538713bcd2f2abd4", "title": "Distributed-memory hierarchical interpolative factorization", "abstract": "The hierarchical interpolative factorization (HIF) offers an efficient way for solving or preconditioning elliptic partial differential equations. By exploiting locality and low-rank properties of the operators, the HIF achieves quasi-linear complexity for factorizing the discrete positive definite elliptic operator and linear complexity for solving the associated linear system. In this paper, the distributed-memory HIF (DHIF) is introduced as a parallel and distributed-memory implementation of the HIF. The DHIF organizes the processes in a hierarchical structure and keeps the communication as local as possible. The computation complexity is $$O(\\frac{N\\log N}{P})$$O(NlogNP) and $$O(\\frac{N}{P})$$O(NP) for constructing and applying the DHIF, respectively, where N is the size of the problem and P is the number of processes. The communication complexity is $$O(\\sqrt{P}\\log ^3 P)\\alpha + O(\\frac{N^{2/3}}{\\sqrt{P}})\\beta $$O(Plog3P)\u03b1+O(N2/3P)\u03b2 where $$\\alpha $$\u03b1 is the latency and $$\\beta $$\u03b2 is the inverse bandwidth. Extensive numerical examples are performed on the NERSC Edison system with up to 8192 processes. The numerical results agree with the complexity analysis and demonstrate the efficiency and scalability of the DHIF.", "venue": "ArXiv", "authors": ["Yingzhou  Li", "Lexing  Ying"], "year": 2016, "n_citations": 19}
{"id": 62135, "s2_id": "e7f7aba78446ce36f4fb5c5c543652127538dd86", "title": "A Mathematica package to cope with partially ordered sets", "abstract": "Mathematica offers, by way of the package Combinatorics, many useful functions to work on graphs and ordered structures, but none of these functions was specific enough to meet the needs of our research group. Moreover, the existing functions are not always helpful when one has to work on new concepts. \nIn this paper we present a package of features developed in Mathematica which we consider particularly useful for the study of certain categories of partially ordered sets. Among the features offered, the package includes: (1) some basic features to treat partially ordered sets; (2) the ability to enumerate, create, and display monotone and regular partitions of partially ordered sets; (3) the capability of constructing the lattices of partitions of a poset, and of doing some useful computations on these structures; (4) the possibility of computing products and coproducts in the category of partially ordered sets and monotone maps; (5) the possibility of computing products and coproducts in the category of forests (disjoint union of trees) and open maps (cf. [DM06] for the product between forests).", "venue": "ArXiv", "authors": ["Pietro  Codara"], "year": 2013, "n_citations": 1}
{"id": 74477, "s2_id": "bf60331a0cfc09c93859ba3eacf10acc266c5dfd", "title": "A Hybrid MPI-CUDA Approach for Nonequispaced Discrete Fourier Transformation", "abstract": "Nonequispaced discrete Fourier transformation (NDFT) is widely applied in all aspects of computational science and engineering. The computational efficiency and accuracy of NDFT has always been a critical issue in hindering its comprehensive applications both in intensive and in extensive aspects of scientific computing. In our previous work (2018, S.-C. Yang et al., Appl. Comput. Harmon. Anal. 44, 273), a CUNFFT method was proposed and it shown outstanding performance in handling NDFT at intermediate scale based on CUDA (Compute Unified Device Architecture) technology. In the current work, we further improved the computational efficiency of the CUNTTF method using an efficient MPI-CUDA hybrid parallelization (HP) scheme of NFFT to achieve a cutting-edge treatment of NDFT at super extended scale. Within this HP-NFFT method, the spatial domain of NDFT is decomposed into several parts according to the accumulative feature of NDFT and the detailed number of CPU and GPU nodes. These decomposed NDFT subcells are independently calculated on different CPU nodes using a MPI process-level parallelization mode, and on different GPU nodes using a CUDA threadlevel parallelization mode and CUNFFT algorithm. A massive benchmarking of the HP-NFFT method indicates that this method exhibit a dramatic improvement in computational efficiency for handling NDFT at super extended scale without loss of computational precision. Furthermore, the HP-NFFT method is validated via the calculation of Madelung constant of fluorite crystal structure, and thereafter verified that this method is robust for the calculation of electrostatic interactions between charged ions in molecular dynamics simulation systems.", "venue": "Comput. Phys. Commun.", "authors": ["Sheng-Chun  Yang", "Yong-Lei  Wang"], "year": 2021, "n_citations": 0}
{"id": 76749, "s2_id": "b3b3482db21ac83790d93043a69d08f10053c097", "title": "Optimizing Geometric Multigrid Methods with Evolutionary Computation", "abstract": "For many linear and nonlinear systems that arise from the discretization of partial differential equations the construction of an efficient multigrid solver is a challenging task. Here we present a novel approach for the optimization of geometric multigrid methods that is based on evolutionary computation, a generic program optimization technique inspired by the principle of natural evolution. A multigrid solver is represented as a tree of mathematical expressions which we generate based on a tailored grammar. The quality of each solver is evaluated in terms of convergence and compute performance using automated local Fourier analysis (LFA) and roofline performance modeling, respectively. Based on these objectives a multi-objective optimization is performed using strongly typed genetic programming with a non-dominated sorting based selection. To evaluate the model-based prediction and to target concrete applications, scalable implementations of an evolved solver can be automatically generated with the ExaStencils framework. We demonstrate our approach by constructing multigrid solvers for the steady-state heat equation with constant and variable coefficients that consistently perform better than common V- and W-cycles.", "venue": "ArXiv", "authors": ["Jonas  Schmitt", "Sebastian  Kuckuk", "Harald  K\u00f6stler"], "year": 2019, "n_citations": 7}
{"id": 83043, "s2_id": "1756e9ad7115cb6dfc7aca417a2230a5950d191b", "title": "Mathematics of Digital Hyperspace", "abstract": "Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of key based indices (such as pointers to strings) and semilinks, GraphBLAS can become a richer associative array algebra and be a plug-in replacement for spreadsheets, database tables, and data centric operating systems, enhancing the navigation of unstructured data found in digital hyperspace.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Jeremy  Kepner", "Timothy  Davis", "Vijay  Gadepally", "Hayden  Jananthan", "Lauren  Milechin"], "year": 2021, "n_citations": 0}
{"id": 88634, "s2_id": "327589c86de90770c2e38609ef160e3d1c6e19ae", "title": "Moore: Interval Arithmetic in Modern C++", "abstract": "We present the library Moore, which implements Interval Arithmetic in modern C++. This library is based on a new feature in the C++ language called concepts, which reduces the problems caused by template meta programming, and leads to a new approach for implementing interval arithmetic libraries in C++.", "venue": "ArXiv", "authors": ["Walter F. Mascarenhas"], "year": 2016, "n_citations": 3}
{"id": 89249, "s2_id": "4240139b285466ad024c77a064947cd91cdaed7c", "title": "A Survey of Singular Value Decomposition Methods for Distributed Tall/Skinny Data", "abstract": "The Singular Value Decomposition (SVD) is one of the most important matrix factorizations, enjoying a wide variety of applications across numerous application domains. In statistics and data analysis, the common applications of SVD inclue Principal Components Analysis (PCA) and regression. Usually these applications arise on data that has far more rows than columns, so-called \"tall/skinny\" matrices. In the big data analytics context, this may take the form of hundreds of millions to billions of rows with only a few hundred columns. There is a need, therefore, for fast, accurate, and scalable tall/skinny SVD implementations which can fully utilize modern computing resources. To that end, we present a survey of three different algorithms for computing the SVD for these kinds of tall/skinny data layouts using MPI for communication. We contextualize these with common big data analytics techniques. Finally, we present both CPU and GPU timing results from the Summit supercomputer, and discuss possible alternative approaches.", "venue": "2020 IEEE/ACM 11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA)", "authors": ["Drew  Schmidt"], "year": 2020, "n_citations": 1}
{"id": 92311, "s2_id": "6cbc32b9a27e8b89e69819f1d8b7f636b48b668b", "title": "multivar_horner: a python package for computing Horner factorisations of multivariate polynomials", "abstract": "Many applications in the sciences require numerically stable and computationally efficient evaluation of multivariate polynomials. Finding beneficial representations of polynomials, such as Horner factorisations, is therefore crucial. multivar_horner, the python package presented here, is the first open source software for computing multivariate Horner factorisations. This work briefly outlines the functionality of the package and puts it into reference to previous work in the field. Benchmarks additionally prove the advantages of the implementation and Horner factorisations in general.", "venue": "J. Open Source Softw.", "authors": ["Jannik  Michelfeit"], "year": 2020, "n_citations": 1}
{"id": 92460, "s2_id": "cbab0b1346e790e6eff61c01c765e27df3fcc5e2", "title": "Constructive Arithmetics in Ore Localizations of Domains", "abstract": "Abstract For a non-commutative domain R and a multiplicatively closed set S the (left) Ore localization of R at S exists if and only if S satisfies the (left) Ore property. Since the concept has been introduced by Ore back in the 1930's, Ore localizations have been widely used in theory and in applications. We investigate the arithmetics of the localized ring S \u2212 1 R from both theoretical and practical points of view. We show that the key component of the arithmetics is the computation of the intersection of a left ideal with a submonoid S of R. It is not known yet whether there exists an algorithmic solution of this problem in general. Still, we provide such solutions for cases where S is equipped with additional structure by distilling three most frequently occurring types of Ore sets. We introduce the notion of the (left) saturation closure and prove that it is a canonical form for (left) Ore sets in R. We provide an implementation of arithmetics over the ubiquitous G-algebras in Singular:Plural and discuss questions arising in this context. Numerous examples illustrate the effectiveness of the proposed approach.", "venue": "J. Symb. Comput.", "authors": ["Johannes  Hoffmann", "Viktor  Levandovskyy"], "year": 2020, "n_citations": 2}
{"id": 98156, "s2_id": "a668f2b75193d318eb2f0a8e6d22005f22be2def", "title": "Exact diagonalization of quantum lattice models on coprocessors", "abstract": "Abstract We implement the Lanczos algorithm on an Intel Xeon Phi coprocessor and compare its performance to a multi-core Intel Xeon CPU and an NVIDIA graphics processor. The Xeon and the Xeon Phi are parallelized with OpenMP and the graphics processor is programmed with CUDA. The performance is evaluated by measuring the execution time of a single step in the Lanczos algorithm. We study two quantum lattice models with different particle numbers, and conclude that for small systems, the multi-core CPU is the fastest platform, while for large systems, the graphics processor is the clear winner, reaching speedups of up to 7.6 compared to the CPU. The Xeon Phi outperforms the CPU with sufficiently large particle number, reaching a speedup of 2.5.", "venue": "Comput. Phys. Commun.", "authors": ["Topi  Siro", "Ari  Harju"], "year": 2016, "n_citations": 1}
{"id": 105202, "s2_id": "8b46950e6ffacbfa2ee0f5bbec49c1d849a1ca80", "title": "Verificarlo: Checking Floating Point Accuracy through Monte Carlo Arithmetic", "abstract": "Numerical accuracy of floating point computation is a well studied topic which has not made its way to the end-user in scientific computing. Yet, it has become a critical issue with the recent requirements for code modernization to harness new highly parallel hardware and perform higher resolution computation. To democratize numerical accuracy analysis, it is important to propose tools and methodologies to study large use cases in a reliable and automatic way. In this paper, we propose verificarlo, an extension to the LLVM compiler to automatically use Monte Carlo Arithmetic in a transparent way for the end-user. It supports all the major languages including C, C++, and Fortran. Unlike source-to-source approaches, our implementation captures the influence of compiler optimizations on the numerical accuracy. We illustrate how Monte Carlo Arithmetic using the verificarlo tool outperforms the existing approaches on various use cases and is a step toward automatic numerical analysis.", "venue": "2016 IEEE 23nd Symposium on Computer Arithmetic (ARITH)", "authors": ["Christophe  Denis", "Pablo de Oliveira Castro", "Eric  Petit"], "year": 2016, "n_citations": 38}
{"id": 110707, "s2_id": "4e2f80270e278b548966ffab31c4ce28c2914042", "title": "Simplifying products of fractional powers of powers", "abstract": "Most computer algebra systems incorrectly simplify <i>z</i> -- <i>z</i>/\u221a<i>w</i><sup>2</sup> 1/<i>w</i><sup>3</sup> -- <i>w</i>\u221a<i>w</i><sup>2</sup> to 0 rather than to 0/0. The reasons for this are: <ol><li>The default simplification does not succeed in simplifying the denominator to 0.</li> <li>There is a rule that 0 is the result of 0 divided by anything that does not simplify to either 0 or 0/0.</li></ol>.\n Many of these systems have more powerful optional transformation and general purpose simplification functions. However that is unlikely to help this example even if one of those functions can simplify the denominator to 0, because the input to those functions is the result of <i>default</i> simplification, which has already incorrectly simplified the overall ratio to 0. Try it on your computer algebra systems!\n This article describes how to simplify products of the form <i>w</i><sup>\u03b1</sup> (<i>w</i><sup>\u03b21</sup>) <sup>\u03bb1</sup> ... (<i>w</i><sup>\u03b2<i>n</i></sup>) <sup>\u03bb<i>n</i></sup> correctly and well, where <i>w</i> is any real or complex expression and the exponents are rational numbers.\n It might seem that correct good simplification of such a restrictive expression class must already be published and/or built into at least one widely used computer-algebra system, but apparently this issue has been overlooked. Default and relevant optional simplification was tested with 86 examples on 5 systems with <i>n</i> = 1. Using a spectrum from the most serious flaw being a result that is not equivalent to the input somewhere to the least serious being not rationalizing a denominator when that does not cause a more serious flaw, the overall percentage of most flaw types is alarming.", "venue": "ACCA", "authors": ["David R. Stoutemyer"], "year": 2013, "n_citations": 1}
{"id": 113769, "s2_id": "4111a6e3e4694b1416086935cc89ac7e809e5b1b", "title": "Efficient Generation of Correctness Certificates for the Abstract Domain of Polyhedra", "abstract": "Polyhedra form an established abstract domain for inferring runtime properties of programs using abstract interpretation. Computations on them need to be certified for the whole static analysis results to be trusted. In this work, we look at how far we can get down the road of a posteriori verification to lower the overhead of certification of the abstract domain of polyhedra. We demonstrate methods for making the cost of inclusion certificate generation negligible. From a performance point of view, our single-representation, constraints-based implementation compares with state-of-the-art implementations.", "venue": "SAS", "authors": ["Alexis  Fouilh\u00e9", "David  Monniaux", "Micha\u00ebl  P\u00e9rin"], "year": 2013, "n_citations": 38}
{"id": 117342, "s2_id": "6a503b72253ff7e7408a567efaf209a3afbd51b2", "title": "HOPE: A Python Just-In-Time compiler for astrophysical computations", "abstract": "The Python programming language is becoming increasingly popular for scientific applications due to its simplicity, versatility, and the broad range of its libraries. A drawback of this dynamic language, however, is its low runtime performance which limits its applicability for large simulations and for the analysis of large data sets, as is common in astrophysics and cosmology. While various frameworks have been developed to address this limitation, most focus on covering the complete language set, and either force the user to alter the code or are not able to reach the full speed of an optimised native compiled language. In order to combine the ease of Python and the speed of C++, we developed HOPE, a specialised Python just-in-time (JIT) compiler designed for numerical astrophysical applications. HOPE focuses on a subset of the language and is able to translate Python code into C++ while performing numerical optimisation on mathematical expressions at runtime. To enable the JIT compilation, the user only needs to add a decorator to the function definition. We assess the performance of HOPE by performing a series of benchmarks and compare its execution speed with that of plain Python, C++ and the other existing frameworks. We find that HOPE improves the performance compared to plain Python by a factor of 2 to 120, achieves speeds comparable to that of C++, and often exceeds the speed of the existing solutions. We discuss the differences between HOPE and the other frameworks, as well as future extensions of its capabilities. The fully documented HOPE package is available at this http URL and is published under the GPLv3 license on PyPI and GitHub.", "venue": "Astron. Comput.", "authors": ["Joel  Akeret", "Lukas  Gamper", "Adam  Amara", "Alexandre  Refregier"], "year": 2015, "n_citations": 23}
{"id": 117777, "s2_id": "7cb0563644dd4a45ea4e3b814c15d5e54d5316ba", "title": "Frequency extraction for BEM-matrices arising from the 3D scalar Helmholtz equation", "abstract": "The discretisation of boundary integral equations for the scalar Helmholtz equation leads to large dense linear systems. Efficient boundary element methods (BEM), such as the fast multipole method (FMM) and H-matrix based methods, focus on structured low-rank approximations of subblocks in these systems. It is known that the ranks of these subblocks increase linearly with the wavenumber. We explore a data-sparse representation of BEM-matrices valid for a range of frequencies, based on extracting the known phase of the Green\u2019s function. Algebraically, this leads to a Hadamard product of a frequency matrix with an H-matrix. We show that the frequency dependency of this H-matrix can be determined using a small number of frequency samples, even for geometrically complex three-dimensional scattering obstacles. We describe an efficient construction of the representation by combining adaptive cross approximation with adaptive rational approximation in the continuous frequency dimension. We show that our data-sparse representation allows to efficiently sample the full BEM-matrix at any given frequency, and as such it may be useful as part of an efficient sweeping routine.", "venue": "ArXiv", "authors": ["Simon  Dirckx", "Daan  Huybrechs", "Karl  Meerbergen"], "year": 2020, "n_citations": 0}
{"id": 125187, "s2_id": "5d48e282a1848eb167d337a2dae34cf7e9ec2cf6", "title": "PhasePack User Guide", "abstract": "\"Phase retrieval\" refers to the recovery of signals from the magnitudes (and not the phases) of linear measurements. While there has been a recent explosion in development of phase retrieval methods, the lack of a common interface has made it difficult to compare new methods against the current state-of-the-art. PhasePack is a software library that creates a common interface for a wide range of phase retrieval schemes. PhasePack also provides a test bed for phase retrieval methods using both synthetic data and publicly available empirical datasets.", "venue": "ArXiv", "authors": ["Rohan  Chandra", "Ziyuan  Zhong", "Justin  Hontz", "Val  McCulloch", "Christoph  Studer", "Tom  Goldstein"], "year": 2017, "n_citations": 0}
{"id": 126282, "s2_id": "38840313c618f31879dd9d3c4baf8c0018c786f0", "title": "Fast calculation of inverse square root with the use of magic constant - analytical approach", "abstract": "We present a mathematical analysis of transformations used in fast calculation of inverse square root for single-precision floating-point numbers. Optimal values of the so called magic constants are derived in a systematic way, minimizing either relative or absolute errors. We show that the value of the magic constant can depend on the number of NewtonRaphson iterations. We present results for one and two iterations.", "venue": "Appl. Math. Comput.", "authors": ["Leonid V. Moroz", "Cezary J. Walczyk", "Andriy  Hrynchyshyn", "Vijay  Holimath", "Jan L. Cieslinski"], "year": 2018, "n_citations": 10}
{"id": 128882, "s2_id": "a8789276b049a62589df7880622868db6beef3aa", "title": "Linguistic Relativity and Programming Languages", "abstract": "The use of programming languages can wax and wane across the decades. We examine the split-apply- combine pattern that is common in statistical computing, and consider how its invocation or implementation in languages like MATLAB and APL differ from R/dplyr. The differences in spelling illustrate how the concept of linguistic relativity applies to programming languages in ways that are analogous to human languages. Finally, we discuss how Julia, by being a high performance yet general purpose dynamic language, allows its users to express different abstractions to suit individual preferences.", "venue": "ArXiv", "authors": ["Jiahao  Chen"], "year": 2018, "n_citations": 1}
{"id": 131021, "s2_id": "f988bf8a461db9cf8ba1bd8b03a849b647a02401", "title": "Kahler: An Implementation of Discrete Exterior Calculus on Hermitian Manifolds", "abstract": "This paper details the techniques and algorithms implemented in Kahler, a Python library that implements discrete exterior calculus on arbitrary Hermitian manifolds. Borrowing techniques and ideas first implemented in PyDEC, Kahler provides a uniquely general framework for computation using discrete exterior calculus. Manifolds can have arbitrary dimension, topology, bilinear Hermitian metrics, and embedding dimension. Kahler comes equipped with tools for generating triangular meshes in arbitrary dimensions with arbitrary topology. Kahler can also generate discrete sharp operators and implement de Rham maps. Computationally intensive tasks are automatically parallelized over the number of cores detected. The program itself is written in Cython--a superset of the Python language that is translated to C and compiled for extra speed. Kahler is applied to several example problems: normal modes of a vibrating membrane, electromagnetic resonance in a cavity, the quantum harmonic oscillator, and the Dirac-Kahler equation. Convergence is demonstrated on random meshes.", "venue": "ArXiv", "authors": ["Alex  Eftimiades"], "year": 2014, "n_citations": 1}
{"id": 132563, "s2_id": "420446d679e3c221da2848a3a1b87f910eff5b62", "title": "Global communications in multiprocessor simulations of flames", "abstract": "In this paper we investigate performance of global communications in a particular parallel code. The code simulates dynamics of expansion of premixed spherical flames using an asymptotic model of Sivashinsky type and a spectral numerical algorithm. As a result, the code heavily relies on global all-to-all interprocessor communications implementing transposition of the distributed data array in which numerical solution to the problem is stored. This global data interdependence makes interprocessor connectivity of the HPC system as important as the floating-point power of the processors of which the system is built. Our experiments show that efficient numerical simulation of this particular model, with global data interdependence, on modern HPC systems is possible. Prospects of performance of more sophisticated models of flame dynamics are analysed as well.", "venue": "ArXiv", "authors": ["Vladimir  Karlin"], "year": 2009, "n_citations": 0}
{"id": 135637, "s2_id": "6c0712b6eecb41e3791bed2883a3f083e0e602c6", "title": "A Generic Library for Stencil Computations", "abstract": "In this era of diverse and heterogeneous computer architectures, the programmability issues, such as productivity and portable efficiency, are crucial to software development and algorithm design. One way to approach the problem is to step away from traditional sequential programming languages and move toward domain specific programming environments to balance between expressivity and efficiency. In order to demonstrate this principle, we developed a domain specific C++ generic library for stencil computations, like PDE solvers. The library features high level constructs to specify computation and allows the development of parallel stencil computations with very limited effort. The high abstraction constructs (like do_all and do_reduce) make the program shorter and cleaner with increased contextual information for better performance exploitation. The results show good performance from Windows multicores, to HPC clusters and machines with accelerators, like GPUs.", "venue": "ArXiv", "authors": ["Mauro  Bianco", "Ugo  Varetto"], "year": 2012, "n_citations": 10}
{"id": 138175, "s2_id": "5b9d2c1f344ee06095937d796f4949da414e7f80", "title": "Tsnnls: A solver for large sparse least squares problems with non-negative variables", "abstract": "The solution of large, sparse constrained least-squares problems is a staple in scientific and engineering applications. However, currently available codes for such problems are proprietary or based on MATLAB. We announce a freely available C implementation of the fast block pivoting algorithm of Portugal, Judice, and Vicente. Our version is several times faster than Matstoms' MATLAB implementation of the same algorithm. Further, our code matches the accuracy of MATLAB's built-in lsqnonneg function.", "venue": "ArXiv", "authors": ["Jason  Cantarella", "Michael  Piatek"], "year": 2004, "n_citations": 35}
{"id": 141584, "s2_id": "a65a6b692ec34ac8151ab692dfdcc7ebe765a1de", "title": "Optimal Cache-Oblivious Mesh Layouts", "abstract": "A mesh is a graph that divides physical space into regularly-shaped regions. Meshes computations form the basis of many applications, including finite-element methods, image rendering, collision detection, and N-body simulations. In one important mesh primitive, called a mesh update, each mesh vertex stores a value and repeatedly updates this value based on the values stored in all neighboring vertices. The performance of a mesh update depends on the layout of the mesh in memory. Informally, if the mesh layout has good data locality (most edges connect a pair of nodes that are stored near each other in memory), then a mesh update runs quickly.This paper shows how to find a memory layout that guarantees that the mesh update has asymptotically optimal memory performance for any set of memory parameters. Specifically, the cost of the mesh update is roughly the cost of a sequential memory scan. Such a memory layout is called cache-oblivious. Formally, for a d-dimensional mesh G, block size B, and cache size M (where M=\u03a9(Bd)), the mesh update of G uses O(1+|G|/B) memory transfers. The paper also shows how the mesh-update performance degrades for smaller caches, where M=o(Bd).The paper then gives two algorithms for finding cache-oblivious mesh layouts. The first layout algorithm runs in time O(|G|log\u20092|G|) both in expectation and with high probability on a RAM. It uses O(1+|G|log\u20092(|G|/M)/B) memory transfers in expectation and O(1+(|G|/B)(log\u20092(|G|/M)+log\u2009|G|)) memory transfers with high probability in the cache-oblivious and disk-access machine (DAM) models. The layout is obtained by finding a fully balanced decomposition tree of G and then performing an in-order traversal of the leaves of the tree.The second algorithm computes a cache-oblivious layout on a RAM in time O(|G|log\u2009|G|log\u2009log\u2009|G|) both in expectation and with high probability. In the DAM and cache-oblivious models, the second layout algorithm uses O(1+(|G|/B) log\u2009(|G|/M)min\u2009{log\u2009log\u2009|G|,log\u2009(|G|/M)}) memory transfers in expectation and O(1+(|G|/B)(log\u2009(|G|/M)min\u2009{log\u2009log\u2009|G|,log\u2009(|G|/M)}+log\u2009|G|)) memory transfers with high probability. The algorithm is based on a new type of decomposition tree, here called a relax-balanced decomposition tree. Again, the layout is obtained by performing an in-order traversal of the leaves of the decomposition tree.", "venue": "Theory of Computing Systems", "authors": ["Michael A. Bender", "Bradley C. Kuszmaul", "Shang-Hua  Teng", "Kebin  Wang"], "year": 2009, "n_citations": 13}
{"id": 144746, "s2_id": "18a53c9c8ba42a741f98405c857265e450c41c0a", "title": "Tensors and n-d Arrays:A Mathematics of Arrays (MoA), psi-Calculus and the Composition of Tensor and Array Operations", "abstract": "The Kronecker product is a key algorithm and is ubiquitous across the physical, biological, and computation social sciences. Thus considerations of optimal implementation are important. The need to have high performance and computational reproducibility is paramount. Moreover, due to the need to compose multiple Kronecker products, issues related to data structures, layout and indexing algebra require a new look at an old problem. This paper discusses the outer product/tensor product and a special case of the tensor product: the Kronecker product, along with optimal implementation when composed, and mapped to complex processor/memory hierarchies. We discuss how the use of ``A Mathematics of Arrays\" (MoA), and the psi-Calculus, (a calculus of indexing with shapes), provides optimal, verifiable, reproducible, scalable, and portable implementations of both hardware and software.", "venue": "ArXiv", "authors": ["Lenore M. Restifo Mullin", "James E. Raynolds"], "year": 2009, "n_citations": 0}
{"id": 146800, "s2_id": "1e0af4bcc44a37b052bd600742416fe9d596b999", "title": "Univariate Polynomial Real Root Isolation: Continued Fractions Revisited", "abstract": "We present algorithmic, complexity and implementation results concerning real root isolation of integer univariate polynomials using the continued fraction expansion of real numbers. We improve the previously known bound by a factor of d\u03c4, where d is the polynomial degree and \u03c4 bounds the coefficient bitsize, thus matching the current record complexity for real root isolation by exact methods. Namely, the complexity bound is \u03bf~ B (d 4 \u03c4 2 ) using a standard bound on the expected bitsize of the integers in the continued fraction expansion. We show how to compute the multiplicities within the same complexity and extend the algorithm to non square-free polynomials. Finally, we present an efficient open-source C++ implementation in the algebraic library SYNAPS, and illustrate its efficiency as compared to other available software. We use polynomials with coefficient bitsize up to 8000 and degree up to 1000.", "venue": "ESA", "authors": ["Elias P. Tsigaridas", "Ioannis Z. Emiris"], "year": 2006, "n_citations": 44}
{"id": 155703, "s2_id": "5fa7f9064a1d32908c01e93c78ff14fb8ce4785f", "title": "Iterative Krylov Methods for Acoustic Problems on Graphics Processing Unit", "abstract": "This paper deals with linear algebra operations on Graphics Processing Unit (GPU) with complex number arithmetic using double precision. An analysis of their uses within iterative Krylov methods is presented to solve acoustic problems. Numerical experiments performed on a set of acoustic matrices arising from the modelisation of acoustic phenomena inside a car compartment are collected, and outline the performance, robustness and effectiveness of our algorithms, with a speed-up up to 28x for dot product, 9.8x for sparse matrix-vector product and solvers.", "venue": "2014 13th International Symposium on Distributed Computing and Applications to Business, Engineering and Science", "authors": ["Abal-Kassim  Cherik Ahamed", "Frederic  Magoules"], "year": 2014, "n_citations": 14}
{"id": 159128, "s2_id": "42785cbb34d9a9c55df2058e785dea0a9e336365", "title": "Bayesian supervised predictive classification and hypothesis testing toolkit for partition exchangeability", "abstract": "November 22, 2021 Title Partition Exchangeability Toolkit Version 1.0.0.1000 Description Bayesian supervised predictive classifiers, hypothesis testing, and parametric estimation under Partition Exchangeability are implemented. The two classifiers presented are the marginal classifier (that assumes test data is i.i.d.) next to a more computationally costly but accurate simultaneous classifier (that finds a labelling for the entire test dataset at once based on simultanous use of all the test data to predict each label). We also provide the Maximum Likelihood Estimation (MLE) of the only underlying parameter of the partition exchangeability generative model as well as hypothesis testing statistics for equality of this parameter with a single value, alternative, or multiple samples. We present functions to simulate the sequences from Ewens Sampling Formula as the realisation of the Poisson-Dirichlet distribution and their respective probabilities. License MIT + file LICENSE Encoding UTF-8 RoxygenNote 7.1.1.9001 Suggests testthat (>= 3.0.0) Config/testthat/edition 3 Imports stats (>= 4.1.0) NeedsCompilation no Author Ville Kinnula [aut], Jing Tang [ctb] (<https://orcid.org/0000-0001-7480-7710>), Ali Amiryousefi [aut, cre] (<https://orcid.org/0000-0002-6317-3860>) Maintainer Ali Amiryousefi <ali.amiryousefi@helsinki.fi> Repository CRAN Date/Publication 2021-11-22 08:50:02 UTC", "venue": "ArXiv", "authors": ["Ville  Kinnula", "Jing  Tang", "Ali  Amiryousefi"], "year": 2021, "n_citations": 0}
{"id": 159199, "s2_id": "5c5014a850e6b0645d18a001ae39fd1f72b669b9", "title": "Lacunaryx: computing bounded-degree factors of lacunary polynomials", "abstract": "In this paper, we report on an implementation in the free software M<scp>athemagix</scp> [9] of lacunary factorization algorithms, distributed as a library called L<scp>acunaryx</scp>. These algorithms take as input a polynomial in sparse representation, that is as a list of nonzero monomials, and an integer <i>d</i>, and compute its degree-<i>d</i> factors.<sup>2</sup> The complexity of these algorithms is polynomial in the <i>sparse size</i> of the input polynomial and <i>d</i>: The sparse size of a polynomial [EQUATION] logwhere the <i>h</i>(\u00b7) denotes the <i>height</i> of a rational number, that is the maximum of the absolute values of its numerator and denominator.", "venue": "ACCA", "authors": ["Bruno  Grenet"], "year": 2016, "n_citations": 2}
{"id": 167896, "s2_id": "1a094c8d4023335efaca2894074d9e66f81656e2", "title": "OpenFPM: A scalable open framework for particle and particle-mesh codes on parallel computers", "abstract": "Abstract Scalable and efficient numerical simulations continue to gain importance, as computation is firmly established as the third pillar of discovery, alongside theory and experiment. Meanwhile, the performance of computing hardware grows through increasingly heterogeneous parallelism, enabling simulations of ever more complex models. However, efficiently implementing scalable codes on heterogeneous, distributed hardware systems becomes the bottleneck. This bottleneck can be alleviated by intermediate software layers that provide higher-level abstractions closer to the problem domain, reducing development times and allowing computational scientists to focus. Here, we present OpenFPM, an open and scalable framework that provides an abstraction layer for numerical simulations using particles and/or meshes. OpenFPM provides transparent and scalable infrastructure for shared-memory and distributed-memory implementations of particles-only and hybrid particle-mesh simulations of both discrete and continuous models, as well as non-simulation codes. This infrastructure is complemented with frequently used numerical routines, as well as interfaces to third-party libraries. We present the architecture and design of OpenFPM, detail the underlying abstractions, and benchmark the framework in applications ranging from Smoothed-Particle Hydrodynamics (SPH) to Molecular Dynamics (MD), Discrete Element Methods (DEM), Vortex Methods, stencil codes (finite differences), and high-dimensional Monte Carlo sampling (CMA-ES), comparing it to the current state of the art and to existing software frameworks. Program summary Program Title: OpenFPM Program Files doi: http://dx.doi.org/10.17632/4yrp8nbm7c.1 Licensing provisions: GPLv3 Programming language: C++ Nature of problem: Writing numerical simulation programs that use meshes, particles, or any combination of the two typically requires long development times, in particular if the code is to scale efficiently on parallel distributed-memory computers. The long development times incur high financial and project-time costs and often lead to sub-optimal program performance as shortcuts are taken. Yet, a large portion of the functionality is common across programs and could be automated or provided as reusable software components, leading to large savings in project costs and potentially improved software performance. Solution method: OpenFPM provides a scalable, highly efficient software platform for numerical simulations using meshes, particles, or any combination of the two on parallel computers. It is based on a well-known set of abstract data types and operators that suffice to express any such simulation, regardless of the application domain. OpenFPM provides reusable, tested, and internally parallelized software components that reduce development times and make parallel computing accessible to computational scientists without extensive knowledge in parallel programming. Additional comments including restrictions and unusual features: OpenFPM is a software library based on which users can implement their simulation codes at a fraction of the development cost. All parallelization and memory handling is transparently done by the library. As its main innovation, OpenFPM makes use of C++ Template Meta Programming in order to enable simulations in arbitrary-dimensional spaces, distribution of arbitrary user-defined C++ objects, and compile-time code optimization and targeting for specific hardware platforms. OpenFPM-based simulations can directly output VTK files for visualization of results and HDF5 files for data archiving.", "venue": "Comput. Phys. Commun.", "authors": ["Pietro  Incardona", "Antonio  Leo", "Yaroslav  Zaluzhnyi", "Rajesh  Ramaswamy", "Ivo F. Sbalzarini"], "year": 2019, "n_citations": 18}
{"id": 173586, "s2_id": "f056ec204a88a02e8087cc04921e11738d0cc0b7", "title": "Polynomial spline collocation method for solving weakly regular Volterra integral equations of the first kind", "abstract": "The polynomial spline collocation method is proposed for solution of Volterra integral equations of the first kind with special piecewise continuous kernels. The Gausstype quadrature formula is used to approximate integrals during the discretization of the proposed projection method. The estimate of accuracy of approximate solution is obtained. Stochastic arithmetics is also used based on the Contr\u00f4le et Estimation Stochastique des Arrondis de Calculs (CESTAC) method and the Control of Accuracy and Debugging for Numerical Applications (CADNA) library. Applying this approach it is possible to find optimal parameters of the projective method. The numerical examples are included to illustrate the efficiency of proposed novel collocation method.", "venue": "ArXiv", "authors": ["A.  Tynda", "S.  Noeiaghdam", "D.  Sidorov"], "year": 2021, "n_citations": 0}
{"id": 176158, "s2_id": "604f5e99b87e2bf9f942c3e443f03be4b4a29035", "title": "Nonsingular Efficient Modeling of Rotations in 3-space using three components", "abstract": "This article introduces yet another representation of rotations in 3-space. The rotations form a 3-dimensional projective space, which fact has not been exploited in Computer Science. We use the four affine patches of this projective space to parametrize the rotations. This affine patch representation is more compact than quaternions (which require 4 components for calculations), encompasses the entire rotation group without singularities (unlike the Euler angles and rotation vector approaches), and requires only ratios of linear or quadratic polynomials for basic computations (unlike the Euler angles and rotation vector approaches which require transcendental functions). \nAs an example, we derive the differential equation for the integration of angular velocity using this affine patch representation of rotations. We remark that the complexity of this equation is the same as the corresponding quaternion equation, but has advantages over the quaternion approach e.g. renormalization to unit length is not required, and state space has no dead directions.", "venue": "ArXiv", "authors": ["Norman J. Goldstein"], "year": 2010, "n_citations": 1}
{"id": 177227, "s2_id": "76916d2a652582a60b71554ee1ab0e444a8337bc", "title": "A generalized inner and outer product of arbitrary multi-dimensional arrays using A Mathematics of Arrays (MoA)", "abstract": "An algorithm has been devised to compute the inner and outer product between two arbitrary multi-dimensional arrays A and B in a single piece of code. It was derived using A Mathematics of Arrays (MoA) and the $\\psi$-calculus. Extensive tests of the new algorithm are presented for running in sequential as well as OpenMP multiple processor modes.", "venue": "ArXiv", "authors": ["James E. Raynolds", "Lenore M. Restifo Mullin"], "year": 2009, "n_citations": 0}
{"id": 178182, "s2_id": "27788b038b5025246cdd69f71cc3b1b35fac0c36", "title": "CameraTransform: a Scientific Python Package for Perspective Camera Corrections", "abstract": "Scientific applications often require an exact reconstruction of object positions and distances from digital images. Therefore, the images need to be corrected for perspective distortions. We present \\textit{CameraTransform}, a python package that performs a perspective image correction whereby the height, tilt/roll angle and heading of the camera can be automatically obtained from the images if additional information such as GPS coordinates or object sizes are provided. We present examples of images of penguin colonies that are recorded with stationary cameras and from a helicopter.", "venue": "ArXiv", "authors": ["Richard  Gerum", "Sebastian  Richter", "Alexander  Winterl", "Ben  Fabry", "Daniel P. Zitterbart"], "year": 2017, "n_citations": 3}
{"id": 181623, "s2_id": "b5273606aeb87015f335a3a4281a38600012e2b0", "title": "Optimizing polynomials for floating-point implementation", "abstract": "The floating-point implementation of a function on an interval often reduces to polynomial approximation, the polynomial being typically provided by Remez algorithm. However, the floating-point evaluation of a Remez polynomial sometimes leads to catastrophic cancellations. This happens when some of the polynomial coefficients are verysmall in magnitude with respects to others. In this case, it is better to force these coefficients to zero, which also reduces the operation count. This technique, classically used for odd or even functions, may be generalized to a much larger class of functions. An algorithm is presented that forces to zero the smaller coefficients of the initial polynomial thanks to a modified Remez algorithm targeting an incomplete monomial basis. One advantage of this technique is that it is purely numerical, the function being used as a numerical black box. This algorithm is implemented within a larger polynomial implementation tool that is demonstrated on a range of examples, resulting in polynomials with less coefficients than those obtained theusual way.", "venue": "ArXiv", "authors": ["Florent de Dinechin", "Christoph Quirin Lauter"], "year": 2008, "n_citations": 13}
{"id": 186894, "s2_id": "999e19d1632e4fd5950b7a9f8f78a10e21cda421", "title": "SymbolicData: SDEval - Benchmarking for Everyone", "abstract": "In this paper we will present SDeval, a software project that contains tools for creating and running benchmarks with a focus on problems in computer algebra. It is built on top of the Symbolic Data project, able to translate problems in the database into executable code for various computer algebra systems. The included tools are designed to be very flexible to use and to extend, such that they can be utilized even in contexts of other communities. With the presentation of SDEval, we will also address particularities of benchmarking in the field of computer algebra. Furthermore, with SDEval, we provide a feasible and automatizable way of reproducing benchmarks published in current research works, which appears to be a difficult task in general due to the customizability of the available programs. We will simultaneously present the current developments in the Symbolic Data project.", "venue": "ArXiv", "authors": ["Albert  Heinle", "Viktor  Levandovskyy", "Andreas  Nareike"], "year": 2013, "n_citations": 6}
{"id": 189025, "s2_id": "13d587a8db6eb03ec1e43dfe6ff83d7fc7d6fd96", "title": "Vectorization of multibyte floating point data formats", "abstract": "We propose a scheme for reduced-precision representation of floating point data on a continuum between IEEE-754 floating point types. Our scheme enables the use of lower precision formats for a reduction in storage space requirements and data transfer volume. We describe how our scheme can be accelerated using existing hardware vector units on a general-purpose processor (GPP). Exploiting native vector hardware allows us to support reduced precision floating point with low overhead. We demonstrate that supporting reduced precision in the compiler as opposed to using a library approach can yield a low overhead solution for GPPs.", "venue": "2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)", "authors": ["Andrew  Anderson", "David  Gregg"], "year": 2016, "n_citations": 6}
{"id": 189956, "s2_id": "68cc586aa0c3c29a445a33a5284cf89c1302105a", "title": "Finite element numerical integration for first order approximations on multi-core architectures", "abstract": "The paper presents investigations on the implementation and performance of the finite element numerical integration algorithm for first order approximations and three processor architectures, popular in scientific computing, classical CPU, Intel Xeon Phi and NVIDIA Kepler GPU. A unifying programming model and portable OpenCL implementation is considered for all architectures. Variations of the algorithm due to different problems solved and different element types are investigated and several optimizations aimed at proper optimization and mapping of the algorithm to computer architectures are demonstrated. Performance models of execution are developed for different processors and tested in practical experiments. The results show the varying levels of performance for different architectures, but indicate that the algorithm can be effectively ported to all of them. The general conclusion is that the finite element numerical integration can achieve sufficient performance on different multiand many-core architectures and should not become a performance bottleneck for finite element simulation codes.", "venue": "ArXiv", "authors": ["Krzysztof  Banas", "Filip  Kruzel", "Jan  Bielanski"], "year": 2015, "n_citations": 0}
{"id": 191134, "s2_id": "7fe5736bdf607ec40cfa50d5adc046203f94a6a1", "title": "Julia implementation of the Dynamic Distributed Dimensional Data Model", "abstract": "Julia is a new language for writing data analysis programs that are easy to implement and run at high performance. Similarly, the Dynamic Distributed Dimensional Data Model (D4M) aims to clarify data analysis operations while retaining strong performance. D4M accomplishes these goals through a composable, unified data model on associative arrays. In this work, we present an implementation of D4M in Julia and describe how it enables and facilitates data analysis. Several experiments showcase scalable performance in our new Julia version as compared to the original Matlab implementation.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Alexander  Chen", "Alan  Edelman", "Jeremy  Kepner", "Vijay  Gadepally", "Dylan  Hutchison"], "year": 2016, "n_citations": 7}
{"id": 203365, "s2_id": "1ae662a013b7a8d79861c4c00cd21a36106ccb6e", "title": "Mesh algorithms for PDE with Sieve I: Mesh distribution", "abstract": "We have developed a new programming framework, called Sieve, to support parallel numerical PDE algorithms operating over distributed meshes. We have also developed a reference implementation of Sieve in C++ as a library of generic algorithms operating on distributed containers conforming to the Sieve interface. Sieve makes instances of the incidence relation, or \\emph{arrows}, the conceptual first-class objects represented in the containers. Further, generic algorithms acting on this arrow container are systematically used to provide natural geometric operations on the topology and also, through duality, on the data. Finally, coverings and duality are used to encode not only individual meshes, but all types of hierarchies underlying PDE data structures, including multigrid and mesh partitions. \nIn order to demonstrate the usefulness of the framework, we show how the mesh partition data can be represented and manipulated using the same fundamental mechanisms used to represent meshes. We present the complete description of an algorithm to encode a mesh partition and then distribute a mesh, which is independent of the mesh dimension, element shape, or embedding. Moreover, data associated with the mesh can be similarly distributed with exactly the same algorithm. The use of a high level of abstraction within the Sieve leads to several benefits in terms of code reuse, simplicity, and extensibility. We discuss these benefits and compare our approach to other existing mesh libraries.", "venue": "Sci. Program.", "authors": ["Matthew G. Knepley", "Dmitry A. Karpeev"], "year": 2009, "n_citations": 42}
{"id": 203562, "s2_id": "9b71b0bcc166995adae49574176aea94de70c06d", "title": "Awkward Arrays in Python, C++, and Numba", "abstract": "The Awkward Array library has been an important tool for physics analysis in Python since September 2018. However, some interface and implementation issues have been raised in Awkward Array\u2019s first year that argue for a reimplementation in C++ and Numba. We describe those issues, the new architecture, and present some examples of how the new interface will look to users. Of particular importance is the separation of kernel functions from data structure management, which allows a C++ implementation and a Numba implementation to share kernel functions, and the algorithm that transforms recordoriented data into columnar Awkward Arrays.", "venue": "EPJ Web of Conferences", "authors": ["Jim  Pivarski", "Peter  Elmer", "David  Lange"], "year": 2020, "n_citations": 5}
{"id": 205631, "s2_id": "a098de0d103bffca92fb2bde79132a89fca0fafe", "title": "Performance engineering for real and complex tall & skinny matrix multiplication kernels on GPUs", "abstract": "General matrix-matrix multiplications with double-precision real and complex entries (DGEMM and ZGEMM) in vendor-supplied BLAS libraries are best optimized for square matrices but often show bad performance for tall & skinny matrices, which are much taller than wide. NVIDIA\u2019s current CUBLAS implementation delivers only a fraction of the potential performance as indicated by the roofline model in this case. We describe the challenges and key characteristics of an implementation that can achieve close to optimal performance. We further evaluate different strategies of parallelization and thread distribution and devise a flexible, configurable mapping scheme. To ensure flexibility and allow for highly tailored implementations we use code generation combined with autotuning. For a large range of matrix sizes in the domain of interest we achieve at least 2/3 of the roofline performance and often substantially outperform state-of-the art CUBLAS results on an NVIDIA Volta GPGPU.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Dominik  Ernst", "Georg  Hager", "Jonas  Thies", "Gerhard  Wellein"], "year": 2021, "n_citations": 3}
{"id": 206503, "s2_id": "319f0ab928cbaea2a8fc2adb1ebfda77cddc00c8", "title": "Theorema 2.0: A Graphical User Interface for a Mathematical Assistant System", "abstract": "Theorema2.0stands fora re-designincludinga completere-implementationofthe Theoremasystem,which was originallydesigned, developed,and implementedby Bruno Buchbergerand his Theoremagroup at RISC. In this paper, we present the \ufb01rst prototype of a graphical user interface (GUI) forthe new system. It heavily relies on powerful interactive capabilities introduced in recent releasesof the underlying Mathematica system, most importantly the possibility of having dynamic objectsconnected to interface elements like sliders, menus, check-boxes, radio-buttons and the like. Allthese features are fully integrated into the Mathematica programming environment and allow theimplementation of a modern user interface.", "venue": "UITP", "authors": ["Wolfgang  Windsteiger"], "year": 2012, "n_citations": 21}
{"id": 208771, "s2_id": "38ddc25e8a286ec7795afc1b9ab5ec200d75ae36", "title": "A Massively Parallel Implementation of Multilevel Monte Carlo for Finite Element Models", "abstract": "The Multilevel Monte Carlo (MLMC) method has proven to be an effective variance-reduction statistical method for Uncertainty Quantification (UQ) in Partial Differential Equation (PDE) models, combining model computations at different levels to create an accurate estimate. Still, the computational complexity of the resulting method is extremely high, particularly for 3D models, which requires advanced algorithms for the efficient exploitation of High Performance Computing (HPC). In this article we present a new implementation of the MLMC in massively parallel computer architectures, exploiting parallelism within and between each level of the hierarchy. The numerical approximation of the PDE is performed using the finite element method but the algorithm is quite general and could be applied to other discretization methods as well, although the focus is on parallel sampling. The two key ingredients of an efficient parallel implementation are a good processor partition scheme together with a good scheduling algorithm to assign work to different processors. We introduce a multiple partition of the set of processors that permits the simultaneous execution of different levels and we develop a dynamic scheduling algorithm to exploit it. The problem of finding the optimal scheduling of distributed tasks in a parallel computer is an NP-complete problem. We propose and analyze a new greedy scheduling algorithm to assign samples and we show that it is a 2-approximation, which is the best that may be expected under general assumptions. On top of this result we design a distributed memory implementation using the Message Passing Interface (MPI) standard. Finally we present a set of numerical experiments illustrating its scalability properties.", "venue": "ArXiv", "authors": ["Santiago  Badia", "Jerrad  Hampton", "Javier  Principe"], "year": 2021, "n_citations": 0}
{"id": 210820, "s2_id": "89b3f01561fa225a88ac89ce01e0d4350ae5a75b", "title": "How to speed up R code: an introduction", "abstract": "Most calculations performed by the average R user are unremarkable in the sense that nowadays, any computer can crush the related code in a matter of seconds. But more and more often, heavy calculations are also performed using R, something especially true in some fields such as statistics. The user then faces total execution times of his codes that are hard to work with: hours, days, even weeks. In this paper, how to reduce the total execution time of various codes will be shown and typical bottlenecks will be discussed. As a last resort, how to run your code on a cluster of computers (most workplaces have one) in order to make use of a larger processing power than the one available on an average computer will also be discussed through two examples.", "venue": "ArXiv", "authors": ["Nathan  Uyttendaele"], "year": 2015, "n_citations": 1}
{"id": 212090, "s2_id": "442777b51ba2a0b7567cb10a63c4fac135402274", "title": "RandFile package for Mathematica for accessing file-based sources of randomness", "abstract": "We present a package for Mathematica computer algebra system which allows the exploitation of local files as sources of random data. We provide the description of the package and illustrate its usage by showing some examples. We also compare the provided functionality with alternative sources of randomness, namely a built-in pseudo-random generator and the package for accessing hardware true random number generators.", "venue": "ArXiv", "authors": ["Jaroslaw Adam Miszczak", "M.  Wahl"], "year": 2013, "n_citations": 0}
{"id": 216772, "s2_id": "d3e6ec48c1934f5176a2ef2bf4b6c7f2333fd901", "title": "An Optimizing Multi-platform Source-to-source Compiler Framework for the NEURON MODeling Language", "abstract": "Domain-specific languages (DSLs) play an increasingly important role in the generation of high performing software. They allow the user to exploit domain knowledge for the generation of more efficient code on target architectures. Here, we describe a new code generation framework (NMODL) for an existing DSL in the NEURON framework, a widely used software for massively parallel simulation of biophysically detailed brain tissue models. Existing NMODL DSL transpilers lack either essential features to generate optimized code or capability to parse the diversity of existing models in the user community. Our NMODL framework has been tested against a large number of previously published user models and offers high-level domain-specific optimizations and symbolic algebraic simplifications before target code generation. NMODL implements multiple SIMD and SPMD targets optimized for modern hardware. When comparing NMODL-generated kernels with NEURON we observe a speedup of up to 20\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times $$\\end{document}, resulting in overall speedups of two different production simulations by \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\sim }{7}{\\times }$$\\end{document}. When compared to SIMD optimized kernels that heavily relied on auto-vectorization by the compiler still a speedup of up to \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\sim }{2}{\\times }$$\\end{document} is observed.", "venue": "ICCS", "authors": ["Pramod S. Kumbhar", "Omar  Awile", "Liam  Keegan", "Jorge Blanco Alonso", "James G. King", "Michael L. Hines", "Felix  Sch\u00fcrmann"], "year": 2020, "n_citations": 3}
{"id": 219107, "s2_id": "97fc65fca91c4e1442e14181a12f76b984a9505a", "title": "Bringing Together Dynamic Geometry Software and the Graphics Processing Unit", "abstract": "We equip dynamic geometry software (DGS) with a user-friendly method that enables massively parallel calculations on the graphics processing unit (GPU). This interplay of DGS and GPU opens up various applications in education and mathematical research. The GPU-aided discovery of mathematical properties, interactive visualizations of algebraic surfaces (raycasting), the mathematical deformation of images and footage in real-time, and computationally demanding numerical simulations of PDEs are examples from the long and versatile list of new domains that our approach makes accessible within a DGS. We ease the development of complex (mathematical) visualizations and provide a rapid-prototyping scheme for general-purpose computations (GPGPU). \nThe possibility to program both CPU and GPU with the use of only one high-level (scripting) programming language is a crucial aspect of our concept. We embed shader programming seamlessly within a high-level (scripting) programming environment. The aforementioned requires the symbolic process of the transcompilation of a high-level programming language into shader programming language for GPU and, in this article, we address the challenge of the automatic translation of a high-level programming language to a shader language of the GPU. To maintain platform independence and the possibility to use our technology on modern devices, we focus on a realization through WebGL.", "venue": "ArXiv", "authors": ["Aaron  Montag", "J\u00fcrgen  Richter-Gebert"], "year": 2018, "n_citations": 5}
{"id": 220044, "s2_id": "90e950e1c4973bf53c31e6aae4d4a4b3d6043b05", "title": "Geometry-oblivious FMM for compressing dense SPD matrices", "abstract": "We present GOFMM (geometry-oblivious FMM), a novel method that creates a hierarchical low-rank approximation, or \"compression,\" of an arbitrary dense symmetric positive definite (SPD) matrix. For many applications, GOFMM enables an approximate matrix-vector multiplication in N log N or even N time, where N is the matrix size. Compression requires N log N storage and work. In general, our scheme belongs to the family of hierarchical matrix approximation methods. In particular, it generalizes the fast multipole method (FMM) to a purely algebraic setting by only requiring the ability to sample matrix entries. Neither geometric information (i.e., point coordinates) nor knowledge of how the matrix entries have been generated is required, thus the term \"geometry-oblivious.\" Also, we introduce a shared-memory parallel scheme for hierarchical matrix computations that reduces synchronization barriers. We present results on the Intel Knights Landing and Haswell architectures, and on the NVIDIA Pascal architecture for a variety of matrices.", "venue": "SC", "authors": ["Chenhan D. Yu", "James  Levitt", "Severin  Reiz", "George  Biros"], "year": 2017, "n_citations": 19}
{"id": 233383, "s2_id": "87b861f027a7b31c4e583d8ce321904e50183d58", "title": "An Introduction to Using Software Tools for Automatic Differentiation", "abstract": "The authors give a gentle introduction to using various software tools for Automatic Differentiation (AD). Ready-to-use examples are discussed and links to further information are presented. The target audience includes all those who are looking for a straight-forward way to get started using the available AD technology. The document is supposed to be dynamic in the sense that its content will be kept up-to-date as the AD software covered is evolving.", "venue": "ArXiv", "authors": ["Uwe  Naumann", "Andrea  Walther"], "year": 2003, "n_citations": 1}
{"id": 233787, "s2_id": "4893d49e2dcd87a2c9ac43463f61e6816f819c65", "title": "On Newton\u2013Raphson Iteration for Multiplicative Inverses Modulo Prime Powers", "abstract": "We study algorithms for the fast computation of modular inverses. Newton-Raphson iteration over p-adic numbers gives a recurrence relation computing modular inverse modulo pm, that is logarithmic in m. We solve the recurrence to obtain an explicit formula for the inverse. Then, we study different implementation variants of this iteration and show that our explicit formula is interesting for small exponent values but slower or large exponent, say of more than 700 bits. Overall, we thus propose a hybrid combination of our explicit formula and the best asymptotic variants. This hybrid combination yields then a constant factor improvement, also for large exponents.", "venue": "IEEE Transactions on Computers", "authors": ["Jean-Guillaume  Dumas"], "year": 2014, "n_citations": 12}
{"id": 237525, "s2_id": "3eced31943612aa97f44fc216e0cc516d1578d8b", "title": "Scrambled Linear Pseudorandom Number Generators", "abstract": "F2-linear pseudorandom number generators are very popular due to their high speed, to the ease with which generators with a sizable state space can be created, and to their provable theoretical properties. However, they suffer from linear artifacts that show as failures in linearity-related statistical tests such as the binary-rank and the linear-complexity test. In this article, we give two new contributions. First, we introduce two new F2-linear transformations that have been handcrafted to have good statistical properties and at the same time to be programmable very efficiently on superscalar processors, or even directly in hardware. Then, we describe some scramblers, that is, nonlinear functions applied to the state array that reduce or delete the linear artifacts, and propose combinations of linear transformations and scramblers that give extremely fast pseudorandom number generators of high quality. A novelty in our approach is that we use ideas from the theory of filtered linear-feedback shift registers to prove some properties of our scramblers, rather than relying purely on heuristics. In the end, we provide simple, extremely fast generators that use a few hundred bits of memory, have provable properties, and pass strong statistical tests.", "venue": "ACM Trans. Math. Softw.", "authors": ["David  Blackman", "Sebastiano  Vigna"], "year": 2021, "n_citations": 37}
{"id": 239137, "s2_id": "45bba787fc4ba8d3412b2e02b18020e01194e15e", "title": "GPU Accelerated Finite Element Assembly with Runtime Compilation", "abstract": "In recent years, high performance scientific computing on graphics processing units (GPUs) have gained widespread acceptance. These devices are designed to offer massively parallel threads for running code with general purpose. There are many researches focus on finite element method with GPUs. However, most of the works are specific to certain problems and applications. Some works propose methods for finite element assembly that is general for a wide range of finite element models. But the development of finite element code is dependent on the hardware architectures. It is usually complicated and error prone using the libraries provided by the hardware vendors. In this paper, we present architecture and implementation of finite element assembly for partial differential equations (PDEs) based on symbolic computation and runtime compilation technique on GPU. User friendly programming interface with symbolic computation is provided. At the same time, high computational efficiency is achieved by using runtime compilation technique. As far as we know, it is the first work using this technique to accelerate finite element assembly for solving PDEs. Experiments show that a one to two orders of speedup is achieved for the problems studied in the paper.", "venue": "ArXiv", "authors": ["Tao  Cui", "Xiaohu  Guo", "Hui  Liu"], "year": 2018, "n_citations": 0}
{"id": 250643, "s2_id": "dcb083b4598082fd59e0fc3dfbd5c5d133087e37", "title": "The Reverse Cuthill-McKee Algorithm in Distributed-Memory", "abstract": "Ordering vertices of a graph is key to minimize fill-in and data structure size in sparse direct solvers, maximize locality in iterative solvers, and improve performance in graph algorithms. Except for naturally parallelizable ordering methods such as nested dissection, many important ordering methods have not been efficiently mapped to distributed-memory architectures. In this paper, we present the first-ever distributed-memory implementation of the reverse Cuthill-McKee (RCM) algorithm for reducing the profile of a sparse matrix. Our parallelization uses a two-dimensional sparse matrix decomposition. We achieve high performance by decomposing the problem into a small number of primitives and utilizing optimized implementations of these primitives. Our implementation attains up to 38x speedup on matrices from various applications on 1024 cores of a Cray XC30 supercomputer and shows strong scaling up to 4096 cores for larger matrices.", "venue": "2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Ariful  Azad", "Mathias  Jacquelin", "Aydin  Bulu\u00e7", "Esmond G. Ng"], "year": 2017, "n_citations": 15}
{"id": 251501, "s2_id": "b2a99651e50e7de46c230baad8546beab14ae39a", "title": "GammaCHI: A package for the inversion and computation of the gamma and chi-square cumulative distribution functions (central and noncentral)", "abstract": "A Fortran 90 module GammaCHI for computing and inverting the gamma and chi-square cumulative distribution functions (central and noncentral) is presented. The main novelty of this package is the reliable and accurate inversion routines for the noncentral cumulative distribution functions. Additionally, the package also provides routines for computing the gamma function, the error function and other functions related to the gamma function. The module includes the routines cdfgamC, invcdfgamC, cdfgamNC, invcdfgamNC, errorfunction, inverfc, gamma, loggam, gamstar and quotgamm for the computation of the central gamma distribution function (and its complementary function), the inversion of the central gamma distribution function, the computation of the noncentral gamma distribution function (and its complementary function), the inversion of the noncentral gamma distribution function, the computation of the error function and its complementary function, the inversion of the complementary error function, the computation of: the gamma function, the logarithm of the gamma function, the regulated gamma function and the ratio of two gamma functions, respectively.", "venue": "Comput. Phys. Commun.", "authors": ["Amparo  Gil", "Javier  Segura", "Nico M. Temme"], "year": 2015, "n_citations": 11}
{"id": 255280, "s2_id": "b500aef6a51f6720ee14c54dc1c09282e2cc488f", "title": "A Fast, Vectorizable Algorithm for Producing Single-Precision Sine-Cosine Pairs", "abstract": "This paper presents an algorithm for computing Sine-Cosine pairs to modest accuracy, but in a manner which contains no conditional tests or branching, making it highly amenable to vectorization. An exemplary implementation for PowerPC AltiVec processors is included, but the algorithm should be easily portable to other achitectures, such as Intel SSE.", "venue": "ArXiv", "authors": ["Marcus H. Mendenhall"], "year": 2004, "n_citations": 0}
{"id": 256278, "s2_id": "a8b08011dfe9cecba3640be173dba9b75b978428", "title": "GraphStream: A Tool for bridging the gap between Complex Systems and Dynamic Graphs", "abstract": "The notion of complex systems is common to many domains, from Biology to Economy, Computer Science, Physics, etc. Often, these systems are made of sets of entities moving in an evolving environment. One of their major characteristics is the emergence of some global properties stemmed from local interactions between the entities themselves and between the entities and the environment. The structure of these systems as sets of interacting entities leads researchers to model them as graphs. However, their understanding requires most often to consider the dynamics of their evolution. It is indeed not relevant to study some properties out of any temporal consideration. Thus, dynamic graphs seem to be a very suitable model for investigating the emergence and the conservation of some properties. GraphStream is a Java-based library whose main purpose is to help researchers and developers in their daily tasks of dynamic problem modeling and of classical graph management tasks: creation, processing, display, etc. It may also be used, and is indeed already used, for teaching purpose. GraphStream relies on an event-based engine allowing several event sources. Events may be included in the core of the application, read from a file or received from an event handler.", "venue": "ArXiv", "authors": ["Yoann  Pign\u00e9", "Antoine  Dutot", "Fr\u00e9d\u00e9ric  Guinand", "Damien  Olivier"], "year": 2008, "n_citations": 140}
{"id": 260175, "s2_id": "eb4840531478e69a2e9e619ce44fd80cb5a50938", "title": "Clustering and feature selection using sparse principal component analysis", "abstract": "In this paper, we study the application of sparse principal component analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks sparse factors, or linear combinations of the data variables, explaining a maximum amount of variance in the data while having only a limited number of nonzero coefficients. PCA is often used as a simple clustering technique and sparse factors allow us here to interpret the clusters in terms of a reduced set of variables. We begin with a brief introduction and motivation on sparse PCA and detail our implementation of the algorithm in d\u2019Aspremont et\u00a0al. (SIAM Rev. 49(3):434\u2013448, 2007). We then apply these results to some classic clustering and feature selection problems arising in biology.", "venue": "ArXiv", "authors": ["Ronny  Luss", "Alexandre  d'Aspremont"], "year": 2007, "n_citations": 41}
{"id": 262497, "s2_id": "f2fb7e109733c16aa68b2b1acce4591c4d955ebf", "title": "differint: A Python Package for Numerical Fractional Calculus", "abstract": "Fractional calculus has become widely studied and applied to physical problems in recent years. As a result, many methods for the numerical computation of fractional derivatives and integrals have been defined. However, these algorithms are often programmed in an ad hoc manner, requiring researchers to implement and debug their own code. This work introduces the \\textit{differint} software package, which offers a single repository for multiple numerical algorithms for the computation of fractional derivatives and integrals. This package is coded in the open-source Python programming language. The Grunwald-Letnikov, improved Grunwald-Letnikov, and Riemann-Liouville algorithms from the fractional calculus are included in this package. The algorithms presented are computed from their descriptions found in [2]. This work concludes with suggestions for the application of the \\textit{differint} software package.", "venue": "ArXiv", "authors": ["Matthew  Adams"], "year": 2019, "n_citations": 1}
{"id": 264092, "s2_id": "1cb644fe40f8ba73a0e2b6e2ced0a3182f637555", "title": "Parallel Nonnegative CP Decomposition of Dense Tensors", "abstract": "The CP tensor decomposition is a low-rank approximation of a tensor. We present a distributed-memory parallel algorithm and implementation of an alternating optimization method for computing a CP decomposition of dense tensors that can enforce nonnegativity of the computed low-rank factors. The principal task is to parallelize the Matricized-Tensor Times Khatri-Rao Product (MTTKRP) bottleneck subcomputation. The algorithm is computation efficient, using dimension trees to avoid redundant computation across MTTKRPs within the alternating method. Our approach is also communication efficient, using a data distribution and parallel algorithm across a multidimensional processor grid that can be tuned to minimize communication. We benchmark our software on synthetic as well as hyperspectral image and neuroscience dynamic functional connectivity data, demonstrating that our algorithm scales well to 100s of nodes (up to 4096 cores) and is faster and more general than the currently available parallel software.", "venue": "2018 IEEE 25th International Conference on High Performance Computing (HiPC)", "authors": ["Grey  Ballard", "Koby  Hayashi", "Ramakrishnan  Kannan"], "year": 2018, "n_citations": 16}
{"id": 265164, "s2_id": "e3a88869c61218d8190d9e082d166afcfed6cb8f", "title": "GPU Acceleration of Hermite Methods for the Simulation of Wave Propagation", "abstract": "The Hermite methods of Goodrich, Hagstrom, and Lorenz (2006) use Hermite interpolation to construct high order numerical methods for hyperbolic initial value problems. The structure of the method has several favorable features for parallel computing. In this work, we propose algorithms that take advantage of the many-core architecture of Graphics Processing Units. The algorithm exploits the compact stencil of Hermite methods and uses data structures that allow for efficient data load and stores. Additionally the highly localized evolution operator of Hermite methods allows us to combine multi-stage time-stepping methods within the new algorithms incurring minimal accesses of global memory. Using a scalar linear wave equation, we study the algorithm by considering Hermite interpolation and evolution as individual kernels and alternatively combined them into a monolithic kernel. For both approaches we demonstrate strategies to increase performance. Our numerical experiments show that although a two kernel approach allows for better performance on the hardware, a monolithic kernel can offer a comparable time to solution with less global memory usage.", "venue": "ArXiv", "authors": ["Arturo  Vargas", "Jesse  Chan", "Thomas  Hagstrom", "Timothy C. Warburton"], "year": 2016, "n_citations": 4}
{"id": 265895, "s2_id": "d632e7ffadb4d2eb32516176e1d1e42d353f559e", "title": "OpenMath and SMT-LIB", "abstract": "OpenMath and SMT-LIB are languages with very different origins, but both \"represent mathematics\". We describe SMT-LIB for the OpenMath community and consider adaptations for both languages to support the growing SC-Square initiative.", "venue": "ArXiv", "authors": ["James H. Davenport", "Matthew  England", "Roberto  Sebastiani", "Patrick  Trentin"], "year": 2018, "n_citations": 0}
{"id": 277371, "s2_id": "9052084265f2d23e584b1a9a59606cccfea047db", "title": "A GPU-enabled finite volume solver for large shallow water simulations", "abstract": "This paper presents the implementation of a HLLC finite volume solver using GPU technology for the solution of shallow water problems in two dimensions. It compares both CPU and GPU approaches for implementing all the solver's steps. The technology of graphics and central processors is highlighted with a particular emphasis on the CUDA architecture of NVIDIA. The simple and well-documented Application Programming Interface (CUDA API) facilitates the use of the display card workstation as an additional computer unit to the central processor. Four professional solutions of the NVIDIA Quadro line are tested. Comparison tests between CPU and GPU are carried out on unstructured grids of small sizes (up to 10,000 elements), medium and large sizes (up to 10,000,000 elements). For all test cases, the accuracy of results is of the same order of magnitude for both approaches. Furthermore, the obtained speed gains with the GPU strongly depend on the model of the graphics card, the size of the problem and the simulation time.", "venue": "ArXiv", "authors": ["Fabrice  Zaoui"], "year": 2018, "n_citations": 1}
{"id": 299530, "s2_id": "11bd8ba186e461298e51bab9f2f70621dd1d560b", "title": "Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in hypre and PETSc", "abstract": "We describe our software package Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) recently publicly released. BLOPEX is available as a stand-alone serial library, as an external package to PETSc (Portable, Extensible Toolkit for Scientific Computation, a general purpose suite of tools developed by Argonne National Laboratory for the scalable solution of partial differential equations and related problems), and is also built into hypre (High Performance Preconditioners, a scalable linear solvers package developed by Lawrence Livermore National Laboratory). The present BLOPEX release includes only one solver\u2014the Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. hypre provides users with advanced high-quality parallel multigrid preconditioners for linear systems. With BLOPEX, the same preconditioners can now be efficiently used for symmetric eigenvalue problems. PETSc facilitates the integration of independently developed application modules, with strict attention to component interoperability, and makes BLOPEX extremely easy to compile and use with preconditioners that are available via PETSc. We present the LOBPCG algorithm in BLOPEX for hypre and PETSc. We demonstrate numerically the scalability of BLOPEX by testing it on a number of distributed and shared memory parallel systems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron workstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition and hypre multigrid preconditioning. We test BLOPEX on a model problem, the standard 7-point finite-difference approximation of the 3-D Laplacian, with the problem size in the range of $10^5$-$10^8$.", "venue": "SIAM J. Sci. Comput.", "authors": ["Andrew V. Knyazev", "Merico E. Argentati", "Ilya  Lashuk", "Evgueni E. Ovtchinnikov"], "year": 2007, "n_citations": 81}
{"id": 300992, "s2_id": "ba418d71709dab9ec90416c96bb260ba54cc4e8f", "title": "The Dune FoamGrid implementation for surface and network grids", "abstract": "We present FoamGrid, a new implementation of the DUNE grid interface. FoamGrid implements one- and two-dimensional grids in a physical space of arbitrary dimension, which allows for grids for curved domains. Even more, the grids are not expected to have a manifold structure, i.e., more than two elements can share a common facet. This makes FoamGrid the grid data structure of choice for simulating structures such as foams, discrete fracture networks, or network flow problems. FoamGrid implements adaptive non-conforming refinement with element parametrizations. As an additional feature it allows removal and addition of elements in an existing grid, which makes FoamGrid suitable for network growth problems. We show how to use FoamGrid, with particular attention to the extensions of the grid interface needed to handle non-manifold topology and grid growth. Three numerical examples demonstrate the possibilities offered by FoamGrid.", "venue": "ArXiv", "authors": ["Oliver  Sander", "Timo  Koch", "Natalie  Schr\u00f6der", "Bernd  Flemisch"], "year": 2015, "n_citations": 18}
{"id": 302505, "s2_id": "12145acfb5692ac27e8dfe68a0cfce1ec6ad6274", "title": "Automatic generation of non-uniform random variates for arbitrary pointwise computable probability densities by tiling", "abstract": "We present a rejection method based on recursive covering of the probability density function with equal tiles. The concept works for any probability density function that is pointwise computable or representable by tabular data. By the implicit construction of piecewise constant majorizing and minorizing functions that are arbitrarily close to the density function the production of random variates is arbitrarily independent of the computation of the density function and extremely fast. The method works unattended for probability densities with discontinuities (jumps and poles). The setup time is short, marginally independent of the shape of the probability density and linear in table size. Recently formulated requirements to a general and automatic non-uniform random number generator are topped. We give benchmarks together with a similar rejection method and with a transformation method.", "venue": "ArXiv", "authors": ["Daniel  Fulger", "Guido  Germano"], "year": 2009, "n_citations": 1}
{"id": 306805, "s2_id": "f0104ba18ff239511cf0e52a7f4586234937b8ae", "title": "A Generic Numbering System based on Catalan Families of Combinatorial Objects", "abstract": "We study novel arithmetic algorithms on a canonical number representation based on the Catalan family of combinatorial objects. Our algorithms work on a generic representation that we illustrate on instances like ordered binary and multiway trees, balanced parentheses languages as well as the usual bitstring-based natural numbers seen through the same generic interface as members of the Catalan family. For numbers corresponding to Catalan objects of low representation complexity, our algorithms provide super-exponential gains while their average and worst case complexity is within constant factors of their traditional counterparts.", "venue": "ArXiv", "authors": ["Paul  Tarau"], "year": 2014, "n_citations": 5}
{"id": 318078, "s2_id": "f6273f4d994d262c9ce90ac3874e4fc36643d93c", "title": "Algorithmic Differentiation for Domain Specific Languages", "abstract": "Algorithmic Differentiation (AD) can be used to automate the generation of derivatives in arbitrary software projects. This will generate maintainable derivatives, that are always consistent with the computation of the software. If a domain specific language (DSL) is used in a software the state of the art approach is to differentiate the DSL library with the same AD tool. The drawback of this solution is the reduced performance since the compiler is no longer able to optimize the e.g. SIMD operations. The new approach in this paper integrates the types and operations of the DSL into the AD tool. It will be an operator overloading tool that is generated from an abstract definition of a DSL. This approach enables the compiler to optimize again e.g. for SIMD operation since all calculations are still performed with the original data types. This will also reduce the required memory for AD since the statements inside the DLS implementation are no longer seen by the AD tool. The implementation is presented in the paper and first results for the performance of the solution are presented.", "venue": "ArXiv", "authors": ["Max  Sagebaum", "Nicolas R. Gauger"], "year": 2018, "n_citations": 1}
{"id": 327788, "s2_id": "569fbc26a448b2944b0d4d6d856f3bdfb82c1c74", "title": "Tactics for Reasoning Modulo AC in Coq", "abstract": "We present a set of tools for rewriting modulo associativity and commutativity (AC) in Coq, solving a long-standing practical problem. We use two building blocks: first, an extensible reflexive decision procedure for equality modulo AC; second, an OCaml plug-in for pattern matching modulo AC. We handle associative only operations, neutral elements, uninterpreted function symbols, and user-defined equivalence relations. By relying on type-classes for the reification phase, we can infer these properties automatically, so that end-users do not need to specify which operation is A or AC, or which constant is a neutral element.", "venue": "CPP", "authors": ["Thomas  Braibant", "Damien  Pous"], "year": 2011, "n_citations": 22}
{"id": 337920, "s2_id": "e7b95abe2af97951efb75aa007d3715edd078679", "title": "pylustrator: Code generation for reproducible figures for publication", "abstract": "One major challenge in science is to make all results potentially reproducible. Thus, along with the raw data, every step from basic processing of the data, evaluation, to the generation of the figures, has to be documented as clearly as possible. While there are many programming libraries that cover the basic processing and plotting steps (e.g. Matplotlib in Python), no library yet addresses the reproducible composing of single plots into meaningful figures for publication. Thus, up to now it is still state-of-the-art to generate publishable figures using image-processing or vector-drawing software leading to unwanted alterations of the presented data in the worst case and to figure quality reduction in the best case. Pylustrator a open source library based on the Matplotlib aims to fill this gap and provides a tool to easily generate the code necessary to compose publication figures from single plots. It provides a graphical user interface where the user can interactively compose the figures. All changes are tracked and converted to code that is automatically integrated into the calling script file. Thus, this software provides the missing link from raw data to the complete plot published in scientific journals and thus contributes to the transparency of the complete evaluation procedure.", "venue": "J. Open Source Softw.", "authors": ["Richard  Gerum"], "year": 2020, "n_citations": 7}
{"id": 341030, "s2_id": "9a57d47edc4f8ab4920af7bd1283c1b488de1654", "title": "Bertini for Macaulay2", "abstract": "Numerical algebraic geometry is the field of computational mathematics concerning the numerical solution of polynomial systems of equations. Bertini, a popular software package for computational applications of this field, includes implementations of a variety of algorithms based on polynomial homotopy continuation. The Macaulay2 package Bertini.m2 provides an interface to Bertini, making it possible to access the core run modes of Bertini in Macaulay2. With these run modes, users can find approximate solutions to zero-dimensional systems and positive-dimensional systems, test numerically whether a point lies on a variety, sample numerically from a variety, and perform parameter homotopy runs.", "venue": "ArXiv", "authors": ["Daniel J. Bates", "Elizabeth  Gross", "Anton  Leykin", "Jose Israel Rodriguez"], "year": 2013, "n_citations": 20}
{"id": 347463, "s2_id": "4e8ad357fa789f6db88ea33e27710abf531c2fe3", "title": "On the Efficacy and High-Performance Implementation of Quaternion Matrix Multiplication", "abstract": "Author(s): Williams-Young, David; Li, Xiaosong | Abstract: Quaternion symmetry is ubiquitous in the physical sciences. As such, much work has been afforded over the years to the development of efficient schemes to exploit this symmetry using real and complex linear algebra. Recent years have also seen many advances in the formal theoretical development of explicitly quaternion linear algebra with promising applications in image processing and machine learning. Despite these advances, there do not currently exist optimized software implementations of quaternion linear algebra. The leverage of optimized linear algebra software is crucial in the achievement of high levels of performance on modern computing architectures, and thus provides a central tool in the development of high-performance scientific software. In this work, a case will be made for the efficacy of high-performance quaternion linear algebra software for appropriate problems. In this pursuit, an optimized software implementation of quaternion matrix multiplication will be presented and will be shown to outperform a vendor tuned implementation for the analogous complex matrix operation. The results of this work pave the path for further development of high-performance quaternion linear algebra software which will improve the performance of the next generation of applicable scientific applications.", "venue": "ArXiv", "authors": ["David B. Williams-Young", "Xiaosong  Li"], "year": 2019, "n_citations": 1}
{"id": 351605, "s2_id": "3755ca2c3674a814e62f09bd8a67f9d7aefa2638", "title": "HPTT: a high-performance tensor transposition C++ library", "abstract": "Recently we presented TTC, a domain-specific compiler for tensor transpositions. Despite the fact that the performance of the generated code is nearly optimal, due to its offline nature, TTC cannot be utilized in all the application codes in which the tensor sizes and the necessary tensor permutations are determined at runtime. To overcome this limitation, we introduce the open-source C++ library High-Performance Tensor Transposition (HPTT). Similar to TTC, HPTT incorporates optimizations such as blocking, multi-threading, and explicit vectorization; furthermore it decomposes any transposition into multiple loops around a so called micro-kernel. This modular design-inspired by BLIS-makes HPTT easy to port to different architectures, by only replacing the hand-vectorized micro-kernel (e.g.,a 4 x 4 transpose). HPTT also offers an optional autotuning framework-guided by performance heuristics-that explores a vast search space of implementations at runtime (similar to FFTW). Across a wide range of different tensor transpositions and architectures (e.g., Intel Ivy Bridge, ARMv7, IBM Power7), HPTT attains a bandwidth comparable to that of SAXPY, and yields remarkable speedups over Eigen's tensor transposition implementation. Most importantly, the integration of HPTT into the Cyclops Tensor Framework (CTF) improves the overall performance of tensor contractions by up to 3.1x.", "venue": "ARRAY@PLDI", "authors": ["Paul  Springer", "Tong  Su", "Paolo  Bientinesi"], "year": 2017, "n_citations": 32}
{"id": 351758, "s2_id": "1e24843f72e628e48dcbd3571aaef0f3cfe3ab01", "title": "RWebData: A High-Level Interface to the Programmable Web", "abstract": "The rise of the programmable web offers new opportunities for the empirically driven social sciences. The access, compilation and preparation of data from the programmable web for statistical analysis can, however, involve substantial up-front costs for the practical researcher. The R-package RWebData provides a high-level framework that allows data to be easily collected from the programmable web in a format that can directly be used for statistical analysis in R (R Core Team 2013) without bothering about the data's initial format and nesting structure. It was developed specifically for users who have no experience with web technologies and merely use R as a statistical software. The core idea and methodological contribution of the package are the disentangling of parsing web data and mapping them with a generic algorithm (independent of the initial data structure) to a flat table-like representation. This paper provides an overview of the high-level functions for R-users, explains the basic architecture of the package, and illustrates the implemented data mapping algorithm.", "venue": "ArXiv", "authors": ["Ulrich  Matter"], "year": 2016, "n_citations": 3}
{"id": 356648, "s2_id": "e2c409ec4fb70671931387b0c720a2ca72ad0b78", "title": "On Quality of Implementation of Fortran 2008 Complex Intrinsic Functions on Branch Cuts", "abstract": "Branch cuts in complex functions have important uses in fracture mechanics, jet flow, and aerofoil analysis. This article introduces tests for validating Fortran 2008 complex functions\u2014LOG, SQRT, ASIN, ACOS, ATAN, ASINH, ACOSH, and ATANH\u2014on branch cuts with arguments of all 3 IEEE floating-point binary formats: binary32, binary64, and binary128, including signed zero and signed infinity. Multiple test failures were revealed, such as wrong signs of results or unexpected overflow, underflow, or NaN. We conclude that the quality of implementation of these Fortran 2008 intrinsics in many compilers is not yet sufficient to remove the need for special code for branch cuts. The electronic appendix contains the full test results with 8 Fortran 2008 compilers: GCC, Flang, Cray, Oracle, PGI, Intel, NAG, and IBM, detailed derivations of the values of these functions on branch cuts and conformal maps of the branch cuts, to be used as a reference. The tests and the results are freely available from https://cmplx.sourceforge.io. This work will be of interest to engineers who use complex functions, as well as to compiler and math library developers.", "venue": "ACM Trans. Math. Softw.", "authors": ["Anton  Shterenlikht"], "year": 2019, "n_citations": 0}
{"id": 365501, "s2_id": "eba2e3cb226ead8c990ccd75932a261b03404ba6", "title": "Composing and Factoring Generalized Green's Operators and Ordinary Boundary Problems", "abstract": "We consider solution operators of linear ordinary boundary problems with \u201ctoo many\u201d boundary conditions, which are not always solvable. These generalized Green\u2019s operators are a certain kind of generalized inverses of differential operators. We answer the question when the product of two generalized Green\u2019s operators is again a generalized Green\u2019s operator for the product of the corresponding differential operators and which boundary problem it solves. Moreover, we show that\u2014provided a factorization of the underlying differential operator\u2014a generalized boundary problem can be factored into lower order problems corresponding to a factorization of the respective Green\u2019s operators. We illustrate our results by examples using the Maple package IntDiffOp, where the presented algorithms are implemented.", "venue": "AADIOS", "authors": ["Anja  Korporal", "Georg  Regensburger"], "year": 2012, "n_citations": 4}
{"id": 365551, "s2_id": "0cca47c2e42d8da2fcfdf97c2590abee123783dc", "title": "One approach to the digital visualization of hedgehogs in holomorphic dynamics", "abstract": "In the field of holomorphic dynamics in one complex variable, hedgehog is the local invariant set arising about a Cremer point and endowed with a very complicate shape as well as relating to very weak numerical conditions. We give a solution to the open problem of its digital visualization, featuring either a time saving approach and a far-reaching insight.", "venue": "ArXiv", "authors": ["Alessandro  Rosa"], "year": 2006, "n_citations": 1}
{"id": 384555, "s2_id": "5d084bc7ef22e3fecf48f2c838b1e36922975962", "title": "High-Performance Solvers for Dense Hermitian Eigenproblems", "abstract": "We introduce a new collection of solvers - subsequently called EleMRRR - for large-scale dense Hermitian eigenproblems. EleMRRR solves various types of problems: generalized, standard, and tridiagonal eigenproblems. Among these, the last is of particular importance as it is a solver on its own right, as well as the computational kernel for the first two; we present a fast and scalable tridiagonal solver based on the Algorithm of Multiple Relatively Robust Representations - referred to as PMRRR. Like the other EleMRRR solvers, PMRRR is part of the freely available Elemental library, and is designed to fully support both message-passing (MPI) and multithreading parallelism (SMP). As a result, the solvers can equally be used in pure MPI or in hybrid MPI-SMP fashion. We conducted a thorough performance study of EleMRRR and ScaLAPACK's solvers on two supercomputers. Such a study, performed with up to 8,192 cores, provides precise guidelines to assemble the fastest solver within the ScaLAPACK framework; it also indicates that EleMRRR outperforms even the fastest solvers built from ScaLAPACK's components.", "venue": "SIAM J. Sci. Comput.", "authors": ["Matthias  Petschow", "Elmar  Peise", "Paolo  Bientinesi"], "year": 2013, "n_citations": 26}
{"id": 385118, "s2_id": "9abf6389f3dda53f467cb33cb451e5bc95987111", "title": "Decoupled Block-Wise ILU(k) Preconditioner on GPU", "abstract": "This research investigates the implementation mechanism of block-wise ILU(k) preconditioner on GPU. The block-wise ILU(k) algorithm requires both the level k and the block size to be designed as variables. A decoupled ILU(k) algorithm consists of a symbolic phase and a factorization phase. In the symbolic phase, a ILU(k) nonzero pattern is established from the point-wise structure extracted from a block-wise matrix. In the factorization phase, the block-wise matrix with a variable block size is factorized into a block lower triangular matrix and a block upper triangular matrix. And a further diagonal factorization is required to perform on the block upper triangular matrix for adapting a parallel triangular solver on GPU.We also present the numerical experiments to study the preconditioner actions on different k levels and block sizes.", "venue": "ArXiv", "authors": ["Bo  Yang", "Hui  Liu", "He  Zhong", "Zhangxin  Chen"], "year": 2017, "n_citations": 1}
{"id": 391270, "s2_id": "1379b9cfa47c7221262b6ae9ef7cce5aa51add83", "title": "Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds", "abstract": "For data given on a nonlinear space, like angles, symmetric positive matrices, the sphere, or the hyperbolic space, there is often enough structure to form a Riemannian manifold. We present the Julia package Manifolds.jl, providing a fast and easy to use library of Riemannian manifolds and Lie groups. We introduce a common interface, available in ManifoldsBase.jl, with which new manifolds, applications, and algorithms can be implemented. We demonstrate the utility of Manifolds.jl using B\u00e9zier splines, an optimization task on manifolds, and a principal component analysis on nonlinear data. In a benchmark, Manifolds.jl outperforms existing packages in Matlab or Python by several orders of magnitude and is about twice as fast as a comparable package implemented in C++.", "venue": "ArXiv", "authors": ["Seth D. Axen", "Mateusz  Baran", "Ronny  Bergmann", "Krzysztof  Rzecki"], "year": 2021, "n_citations": 0}
{"id": 398442, "s2_id": "d0c2a12bd72b19bd3b7ea71246073cd0d5f55604", "title": "Computing with B-series", "abstract": "We present BSeries.jl, a Julia package for the computation and manipulation of B-series, which are a versatile theoretical tool for understanding and designing discretizations of differential equations. We give a short introduction to the theory of B-series and associated concepts and provide examples of their use, including method composition and backward error analysis. The associated software is highly performant and makes it possible to work with B-series of high order.", "venue": "ArXiv", "authors": ["David I. Ketcheson", "Hendrik  Ranocha"], "year": 2021, "n_citations": 0}
{"id": 400250, "s2_id": "2c2234548de4694b6455a19cd0d85a9d6c473456", "title": "Non-Metric Space Library Manual", "abstract": "This document describes a library for similarity searching. Even though the library contains a variety of metric-space access methods, our main focus is on search methods for non-metric spaces. Because there are fewer exact solutions for non-metric spaces, many of our methods give only approximate answers. Thus, the methods are evaluated in terms of efficiency-effectiveness trade-offs rather than merely in terms of their efficiency. Our goal is, therefore, to provide not only state-of-the-art approximate search methods for both non-metric and metric spaces, but also the tools to measure search quality. We concentrate on technical details, i.e., how to compile the code, run the benchmarks, evaluate results, and use our code in other applications. Additionally, we explain how to extend the code by adding new search methods and spaces.", "venue": "ArXiv", "authors": ["Bilegsaikhan  Naidan", "Leonid  Boytsov"], "year": 2015, "n_citations": 11}
{"id": 414596, "s2_id": "902e0652216d336be9aea70daf9a8bb710465c91", "title": "Accelerating R with high performance linear algebra libraries", "abstract": "Linear algebra routines are basic building blocks for the statistical software. In this paper we analyzed how can we can improve R performance for matrix computations. We benchmarked few matrix operations using the standard linear algebra libraries included in the R distribution and high performance libraries like OpenBLAS, GotoBLAS and MKL. Our tests showed the the best results are obtained with the MKL library, the other two libraries having similar performances, but lower than MKL", "venue": "ArXiv", "authors": ["Bogdan  Oancea", "Tudorel  Andrei", "Raluca Mariana Dragoescu"], "year": 2015, "n_citations": 4}
{"id": 415119, "s2_id": "e43a69e332a9bf8d63f807e4e671a8e3b165bf84", "title": "cesium: Open-Source Platform for Time-Series Inference", "abstract": "Inference on time series data is a common requirement in many scientific disciplines and internet of things (IoT) applications, yet there are few resources available to domain scientists to easily, robustly, and repeatably build such complex inference workflows: traditional statistical models of time series are often too rigid to explain complex time domain behavior, while popular machine learning packages require already-featurized dataset inputs. Moreover, the software engineering tasks required to instantiate the computational platform are daunting. cesium is an end-to-end time series analysis framework, consisting of a Python library as well as a web front-end interface, that allows researchers to featurize raw data and apply modern machine learning techniques in a simple, reproducible, and extensible way. Users can apply out-of-the-box feature engineering workflows as well as save and replay their own analyses. Any steps taken in the front end can also be exported to a Jupyter notebook, so users can iterate between possible models within the front end and then fine-tune their analysis using the additional capabilities of the back-end library. The open-source packages make us of many use modern Python toolkits, including xarray, dask, Celery, Flask, and scikit-learn.", "venue": "ArXiv", "authors": ["Brett  Naul", "St\u00e9fan van der Walt", "Arien  Crellin-Quick", "Joshua S. Bloom", "Fernando  P\u00e9rez"], "year": 2016, "n_citations": 11}
{"id": 415389, "s2_id": "688a5bd3cc287cac5ebf10f2a52adf2ec25c7dd8", "title": "An Adaptive Solver for Systems of Linear Equations", "abstract": "Computational implementations for solving systems of linear equations often rely on a one-size-fits-all approach based on LU decomposition of dense matrices stored in column-major format. Such solvers are typically implemented with the aid of the xGESV set of functions available in the low-level LAPACK software, with the aim of reducing development time by taking advantage of well-tested routines. However, this straight-forward approach does not take into account various matrix properties which can be exploited to reduce the computational effort and/or to increase numerical stability. Furthermore, direct use of LAPACK functions can be error-prone for non-expert users and results in source code that has little resemblance to originating mathematical expressions. We describe an adaptive solver that we have implemented inside recent versions of the high-level Armadillo C++ library for linear algebra. The solver automatically detects several common properties of a given system (banded, triangular, symmetric positive definite), followed by solving the system via mapping to a set of suitable LAPACK functions best matched to each property. The solver also detects poorly conditioned systems and automatically seeks a solution via singular value decomposition as a fallback. We show that the adaptive solver leads to notable speedups, while also freeing the user from using direct calls to cumbersome LAPACK functions.", "venue": "2020 14th International Conference on Signal Processing and Communication Systems (ICSPCS)", "authors": ["Conrad  Sanderson", "Ryan  Curtin"], "year": 2020, "n_citations": 2}
{"id": 418501, "s2_id": "8888d41ed3a61d2c9b2434ba3062c621c66f2678", "title": "Modular Arithmetic Expressions and Primality Testing via DNA Self-Assembly", "abstract": "Self-assembly is a fundamental process by which supramolecular species form spon- taneously from their components. This process is ubiquitous throughout the life chemistry and is central to biological information processing. Algorithms for solving many mathematical and com- putational problems via tile self assembly has been proposed by many researchers in the last decade. In particular tile set for doing basic arithmetic of two inputs have been given. In this work we give tile set for doing basic arithmetic (addition, subtraction, multiplication) of n inputs and subsequently computing its modulo. We also present a tile set for primality testing. Finally we present a software 'xtilemod' for doing modular arithmetic. This simplifies the task of creating the input files to xgrow simulator for doing basic (addition, subtraction, multiplication and division) as well as modular arithmetic of n inputs. Similar software for creating tile set for primality testing is also given.", "venue": "ArXiv", "authors": ["Abhishek  Chhajer", "Manish K. Gupta", "Sandeep  Vasani", "Jaley  Dholakiya"], "year": 2012, "n_citations": 2}
{"id": 419316, "s2_id": "5ca5240ce1787d67061085d8a647a9924ef2e196", "title": "Task inefficiency patterns for a wave equation solver", "abstract": "The orchestration of complex algorithms demands high levels of automation to use modern hardware efficiently. Task-based programming with OpenMP is a prominent candidate to accomplish this goal. We study OpenMP5\u2019s tasking in the context of a wave equation solver (ExaHyPE) using three different architectures and runtimes. We describe several task-scheduling flaws present in currently available runtimes, demonstrate how they impact performance and show how to work around them. Finally, we propose extensions to the OpenMP standard.", "venue": "IWOMP", "authors": ["Holger  Schulz", "Gonzalo Brito Gadeschi", "Oleksandr  Rudyy", "Tobias  Weinzierl"], "year": 2021, "n_citations": 0}
{"id": 421234, "s2_id": "a0a9e31fa29cbe237d8970c4da45b0dc7c953160", "title": "Fast Graph Learning with Unique Optimal Solutions", "abstract": "Graph Representation Learning (GRL) has been advancing at an unprecedented rate. However, many results rely on careful design and tuning of architectures, objectives, and training schemes. We propose efficient GRL methods that optimize convexified objectives with known closed form solutions. Guaranteed convergence to a global optimum releases practitioners from hyper-parameter and architecture tuning. Nevertheless, our proposed method achieves competitive or state-ofthe-art performance on popular GRL tasks while providing orders of magnitude speedup. Although the design matrix (M) of our objective is expensive to compute, we exploit results from random matrix theory to approximate solutions in linear time while avoiding an explicit calculation of M. Our code is online: http://github.com/ samihaija/tf-fsvd", "venue": "ArXiv", "authors": ["Sami  Abu-El-Haija", "Valentino  Crespi", "Greg Ver Steeg", "Aram  Galstyan"], "year": 2021, "n_citations": 0}
{"id": 426762, "s2_id": "93869739b8a86d7243a270837ac6d9d6ca758307", "title": "On the very accurate evaluation of the Voigt/complex error function with small imaginary argument", "abstract": "A rapidly convergent series, based on Taylor expansion of the imaginary part of the complex error function, is presented for highly accurate approximation of the Voigt/complex error function with small imaginary argument y \u2264 0.1. Error analysis and run-time tests in double-precision computing platform reveals that in the real and imaginary parts the proposed algorithm provides average accuracy exceeding 10 and 10, respectively, and the calculation speed is as fast as that of reported in recent publications. An optimized MATLAB code providing rapid computation with high accuracy is presented.", "venue": "ArXiv", "authors": ["Yihong  Wang"], "year": 2021, "n_citations": 0}
{"id": 426988, "s2_id": "a6dcfa54194ce2e589b971e2471e5adbefcd4d90", "title": "Supporting 64-bit global indices in Epetra and other Trilinos packages - Techniques used and lessons learned", "abstract": "The Trilinos Project is an effort to facilitate the design, development, integration and ongoing support of mathematical software libraries within an object-oriented framework. It is intended for large-scale, complex multiphysics engineering and scientific applications [2, 4, 3]. Epetra is one of its basic packages. It provides serial and parallel linear algebra capabilities. Before Trilinos version 11.0, released in 2012, Epetra used the C++ int data-type for storing global and local indices for degrees of freedom (DOFs). Since int is typically 32-bit, this limited the largest problem size to be smaller than approximately two billion DOFs. This was true even if a distributed memory machine could handle larger problems. We have added optional support for C++ long long data-type, which is at least 64-bit wide, for global indices. To save memory, maintain the speed of memory-bound operations, and reduce further changes to the code, the local indices are still 32-bit. We document the changes required to achieve this feature and how the new functionality can be used. We also report on the lessons learned in modifying a mature and popular package from various perspectives - design goals, backward compatibility, engineering decisions, C++ language features, effects on existing users and other packages,more\u00a0\u00bb and build integration.\u00ab\u00a0less", "venue": "ArXiv", "authors": ["Chetan  Jhurani", "Travis M. Austin", "Michael A. Heroux", "James M. Willenbring"], "year": 2013, "n_citations": 3}
{"id": 429515, "s2_id": "2633b473ab598a3de08030d846155b5e3c418c73", "title": "Implementation of a Near-Optimal Complex Root Clustering Algorithm", "abstract": "We describe Ccluster, a software for computing natural \\(\\varepsilon \\)-clusters of complex roots in a given box of the complex plane. This algorithm from Becker et al. (2016) is near-optimal when applied to the benchmark problem of isolating all complex roots of an integer polynomial. It is one of the first implementations of a near-optimal algorithm for complex roots. We describe some low level techniques for speeding up the algorithm. Its performance is compared with the well-known MPSolve library and Maple.", "venue": "ICMS", "authors": ["R\u00e9mi  Imbach", "Victor Y. Pan", "Chee-Keng  Yap"], "year": 2018, "n_citations": 24}
{"id": 439042, "s2_id": "be210ef642a49f09ccc89ef91657c06b5e1d383e", "title": "Efficient Spherical Harmonic Transforms aimed at pseudo-spectral numerical simulations", "abstract": "In this paper, we report on very efficient algorithms for the spherical harmonic transform (SHT). Explicitly vectorized variations of the algorithm based on the Gauss-Legendre quadrature are discussed and implemented in the SHTns library which includes scalar and vector transforms. The main breakthrough is to achieve very efficient on-the-fly computations of the Legendre associated functions, even for very high resolutions, by taking advantage of the specific properties of the SHT and the advanced capabilities of current and future computers. This allows us to simultaneously and significantly reduce memory usage and computation time of the SHT. We measure the performance and accuracy of our algorithms. Even though the complexity of the algorithms implemented in SHTns are in $O(N^3)$ (where N is the maximum harmonic degree of the transform), they perform much better than any third party implementation, including lower complexity algorithms, even for truncations as high as N=1023. SHTns is available at this https URL as open source software.", "venue": "ArXiv", "authors": ["Nathana\u00ebl  Schaeffer"], "year": 2012, "n_citations": 157}
{"id": 441420, "s2_id": "90dc2ca6fd625e3244d1e69dd3e6195e14657dd2", "title": "Algorithm for generating orthogonal matrices with rational elements", "abstract": "Special orthogonal matrices with rational elements form the group SO(n,Q), where Q is the field of rational numbers. A theorem describing the structure of an arbitrary matrix from this group is proved. This theorem yields an algorithm for generating such matrices by means of random number routines.", "venue": "ArXiv", "authors": ["Ruslan A. Sharipov"], "year": 2002, "n_citations": 3}
{"id": 457676, "s2_id": "6af5164a91119219db01a9e184dbe9c442d9ada0", "title": "Issues with rounding in the GCC implementation of the ISO 18037:2008 standard fixed-point arithmetic", "abstract": "We describe various issues caused by the lack of round-to-nearest mode in the gcc compiler implementation of the fixed-point arithmetic data types and operations. We demonstrate that round-to-nearest is not performed in the conversion of constants, conversion from one numerical type to a less precise type and results of multiplications. Furthermore, we show that mixed-precision operations in fixed-point arithmetic lose precision on arguments, even before carrying out arithmetic operations. The ISO 18037:2008 standard was created to standardize C language extensions, including fixed-point arithmetic, for embedded systems. Embedded systems are usually based on ARM processors, of which approximately 100 billion have been manufactured by now. Therefore, the observations about numerical issues that we discuss in this paper can be rather dangerous and are important to address, given the wide ranging type of applications that these embedded systems are running.", "venue": "2020 IEEE 27th Symposium on Computer Arithmetic (ARITH)", "authors": ["Mantas  Mikaitis"], "year": 2020, "n_citations": 0}
{"id": 465403, "s2_id": "3f0a4f3e3f97c7e4251da7623b2fbf478d204f61", "title": "Panphasia: a user guide", "abstract": "We make a very large realisation of a Gaussian white noise field, called PANPHASIA, public by releasing software that computes this field. Panphasia is designed specifically for setting up Gaussian initial conditions for cosmological simulations and resimulations of structure formation. We make available both software to compute the field itself and codes to illustrate applications including a modified version of a public serial initial conditions generator. We document the software and present the results of a few basic tests of the field. The properties and method of construction of Panphasia are described in full in a companion paper Jenkins 2013.", "venue": "ArXiv", "authors": ["Adrian  Jenkins", "Stephen  Booth"], "year": 2013, "n_citations": 9}
{"id": 468032, "s2_id": "80fef083581c1fd6093a49bc1c82b5253ddef5e4", "title": "Automatic Generation of Interpolants for Lattice Samplings: Part I - Theory and Analysis", "abstract": "Interpolation is a fundamental technique in scientific computing and is at the heart of many scientific visualization techniques. There is usually a trade-off between the approximation capabilities of an interpolation scheme and its evaluation efficiency. For many applications, it is important for a user to be able to navigate their data in real time. In practice, the evaluation efficiency (or speed) outweighs any incremental improvements in reconstruction fidelity. In this two-part work, we first analyze from a general standpoint the use of compact piece-wise polynomial basis functions to efficiently interpolate data that is sampled on a lattice. In the sequel, we detail how we generate efficient implementations via automatic code generation on both CPU and GPU architectures. Specifically, in this paper, we propose a general framework that can produce a fast evaluation scheme by analyzing the algebro-geometric structure of the convolution sum for a given lattice and basis function combination. We demonstrate the utility and generality of our framework by providing fast implementations of various box splines on the Body Centered and Face Centered Cubic lattices, as well as some non-separable box splines on the Cartesian lattice. We also provide fast implementations for certain Voronoi splines that have not yet appeared in the literature. Finally, we demonstrate that this framework may also be used for non-Cartesian lattices in 4D.", "venue": "ArXiv", "authors": ["Joshua  Horacsek", "Usman  Alim"], "year": 2021, "n_citations": 0}
{"id": 470447, "s2_id": "9d9a9c989039ed00d96455cb0897beae57a43240", "title": "Computer validated proofs of a toolset for adaptable arithmetic", "abstract": "Most existing implementations of multiple precision arithmetic demand that the user sets the precision a priori. A solution is to use the largest precision necessary to reach target accuracy. Some libraries are said adaptable in the sense that they dynamically change the precision of each intermediate operation individually to deliver the target accuracy according to the actual inputs. We present in this text a new adaptable numeric core inspired both from floating point expansions and from on-line arithmetic. The numeric core is cut down to five tools. The first tool that contains many arithmetic operations is proved to be correct. The proofs have been formally checked by the Coq assistant. Developing the proofs, we have formally proved many result published in the literature and we have extended a few of them. This work may let users (i) develop application specific adaptable libraries based on the toolset and / or (ii) write new formal proofs based on the set of validated facts.", "venue": "ArXiv", "authors": ["Sylvie  Boldo", "Marc  Daumas", "Claire  Moreau-Finot", "Laurent  Th\u00e9ry"], "year": 2001, "n_citations": 8}
{"id": 470659, "s2_id": "83ccc4324c690c201c88f11e7373104e8e5f4dd4", "title": "An implementation of a randomized algorithm for principal component analysis", "abstract": "Recent years have witnessed intense development of randomized methods for low-rank approximation. These methods target principal component analysis (PCA) and the calculation of truncated singular value decompositions (SVD). The present paper presents an essentially black-box, fool-proof implementation for Mathworks' MATLAB, a popular software platform for numerical computation. As illustrated via several tests, the randomized algorithms for low-rank approximation outperform or at least match the classical techniques (such as Lanczos iterations) in basically all respects: accuracy, computational efficiency (both speed and memory usage), ease-of-use, parallelizability, and reliability. However, the classical procedures remain the methods of choice for estimating spectral norms, and are far superior for calculating the least singular values and corresponding singular vectors (or singular subspaces).", "venue": "ArXiv", "authors": ["Arthur  Szlam", "Yuval  Kluger", "Mark  Tygert"], "year": 2014, "n_citations": 28}
{"id": 472205, "s2_id": "ff0fe45a7d476d92ca98f742657a70e3b1de7bfa", "title": "Automated Generation and Symbolic Manipulation of Tensor Product Finite Elements", "abstract": "We describe and implement a symbolic algebra for scalar and vector-valued finite elements, enabling the computer generation of elements with tensor product structure on quadrilateral, hexahedral and triangular prismatic cells. The algebra is implemented as an extension to the domain-specific language UFL, the Unified Form Language. This allows users to construct many finite element spaces beyond those supported by existing software packages. We have made corresponding extensions to FIAT, the FInite element Automatic Tabulator, to enable numerical tabulation of such spaces. This tabulation is consequently used during the automatic generation of low-level code that carries out local assembly operations, within the wider context of solving finite element problems posed over such function spaces. We have done this work within the code-generation pipeline of the software package Firedrake; we make use of the full Firedrake package to present numerical examples.", "venue": "SIAM J. Sci. Comput.", "authors": ["Andrew T. T. McRae", "Gheorghe-Teodor  Bercea", "Lawrence  Mitchell", "David A. Ham", "Colin J. Cotter"], "year": 2016, "n_citations": 48}
{"id": 481182, "s2_id": "0ab5be4357af6f8184048c1da934142d3933fc91", "title": "Implementing evaluation strategies for continuous real functions", "abstract": "We give a technical overview of our exact-real implementation of various representations of the space of continuous unary real functions over the unit domain and a family of associated (partial) operations, including integration, range computation, as well as pointwise addition, multiplication, division, sine, cosine, square root and maximisation. \nWe use several representations close to the usual theoretical model, based on an oracle that evaluates the function at a point or over an interval. We also include several representations based on an oracle that computes a converging sequence of rigorous (piecewise or one-piece) polynomial and rational approximations over the whole unit domain. Finally, we describe \"local\" representations that combine both approaches, i.e. oracle-like representations that return a rigorous symbolic approximation of the function over a requested interval sub-domain with a requested effort. \nSee also our paper \"Representations and evaluation strategies for feasibly approximable functions\" which compares the efficiency of these representations and algorithms and also formally describes and analyses one of the key algorithms, namely a polynomial-time division of functions in a piecewise-polynomial representation. We do not reproduce this division algorithm here.", "venue": "ArXiv", "authors": ["Michal  Konecn\u00fd", "Eike  Neumann"], "year": 2019, "n_citations": 0}
{"id": 481310, "s2_id": "2cabb8a7fa81dd65a234076e21b3acb98e211763", "title": "Arb: Efficient Arbitrary-Precision Midpoint-Radius Interval Arithmetic", "abstract": "Arb is a C library for arbitrary-precision interval arithmetic using the midpoint-radius representation, also known as ball arithmetic. It supports real and complex numbers, polynomials, power series, matrices, and evaluation of many special functions. The core number types are designed for versatility and speed in a range of scenarios, allowing performance that is competitive with non-interval arbitrary-precision types such as MPFR and MPC floating-point numbers. We discuss the low-level number representation, strategies for precision and error bounds, and the implementation of efficient polynomial arithmetic with interval coefficients.", "venue": "IEEE Transactions on Computers", "authors": ["Fredrik  Johansson"], "year": 2017, "n_citations": 94}
{"id": 486883, "s2_id": "31c36d445367ba204244bb74893c5654e31c3869", "title": "cuDNN: Efficient Primitives for Deep Learning", "abstract": "We present a library that provides optimized implementations for deep learning primitives. Deep learning workloads are computationally intensive, and optimizing the kernels of deep learning workloads is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized for new processors, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS) [2]. However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, and similarly to the BLAS library, could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.", "venue": "ArXiv", "authors": ["Sharan  Chetlur", "Cliff  Woolley", "Philippe  Vandermersch", "Jonathan  Cohen", "John  Tran", "Bryan  Catanzaro", "Evan  Shelhamer"], "year": 2014, "n_citations": 1328}
{"id": 498197, "s2_id": "15d9c912f256161ce71f50f874cefc6b56d5ee30", "title": "Certified Roundoff Error Bounds Using Bernstein Expansions and Sparse Krivine-Stengle Representations", "abstract": "Floating point error is a drawback of embedded systems implementation that is difficult to avoid. Computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. This problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. In this paper, we propose and compare two new algorithms based on Bernstein expansions and sparse Krivine-Stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial and rational functions. We also provide the convergence rate of these two algorithms. We release two related software package $\\mathtt {FPBern}$FPBern and $\\mathtt {FPKriSten}$FPKriSten, and compare them with the state-of-the-art tools. We show that these two methods achieve competitive performance, while providing accurate upper bounds by comparison with the other tools.", "venue": "IEEE Transactions on Computers", "authors": ["Victor  Magron", "Alexandre  Rocca", "Thao  Dang"], "year": 2019, "n_citations": 2}
{"id": 499054, "s2_id": "71afcc1400f473caa194b72081ceca3c3e17914a", "title": "DBCSR: A Library for Dense Matrix Multiplications on Distributed GPU-Accelerated Systems", "abstract": "Most, if not all the modern scientific simulation packages utilize matrix algebra operations. Among the operation of the linear algebra, one of the most important kernels is the multiplication of matrices, dense and sparse. Examples of application of such a kernel are in electronic structure calculations, machine learning, data mining, graph processing, and digital signal processing. Several optimized libraries exist that can achieve high-performance on distributed systems. Only a few of them target distributed GPU-accelerated systems. In most of the cases, these libraries are provided and optimized by system vendors for their specific computer systems. In this paper we present the DBCSR library (Distributed Block Compressed Sparse Row) for the distributed dense matrix-matrix multiplications. Although the library is specifically designed for block-sparse matrix-matrix multiplications, we optimized it for the dense case on GPU-accelerated systems. We show that the DBCSR outperforms the multiplication of matrices of different sizes and shapes provided by a vendor optimized GPU version of the ScaLAPACK library up to 2.5x (1.4x on average).", "venue": "2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)", "authors": ["Ilia  Sivkov", "Alfio  Lazzaro", "Juerg  Hutter"], "year": 2019, "n_citations": 0}
{"id": 500449, "s2_id": "9bc37d8dfed8313ae7689a7a9d68058e23b7d3cb", "title": "Further scramblings of Marsaglia's xorshift generators", "abstract": "xorshift* generators are a variant of Marsaglia's xorshift generators that eliminate linear artifacts typical of generators based on $\\mathbf Z/2\\mathbf Z$-linear operations using multiplication by a suitable constant. Shortly after high-dimensional xorshift* generators were introduced, Saito and Matsumoto suggested a different way to eliminate linear artifacts based on addition in $\\mathbf Z/2^{32}\\mathbf Z$, leading to the XSadd generator. Starting from the observation that the lower bits of XSadd are very weak, as its reverse fails systematically several statistical tests, we explore xorshift+, a variant of XSadd using 64-bit operations, which leads, in small dimension, to extremely fast high-quality generators.", "venue": "J. Comput. Appl. Math.", "authors": ["Sebastiano  Vigna"], "year": 2017, "n_citations": 52}
{"id": 509159, "s2_id": "73f4da8b1b17653e203030ee8304ccfa23aab92d", "title": "Parallelizing Gaussian Process Calculations in R", "abstract": "We consider parallel computation for Gaussian process calculations to overcome computational and memory constraints on the size of datasets that can be analyzed. Using a hybrid parallelization approach that uses both threading (shared memory) and message-passing (distributed memory), we implement the core linear algebra operations used in spatial statistics and Gaussian process regression in an R package called bigGP that relies on C and MPI. The approach divides the covariance matrix into blocks such that the computational load is balanced across processes while communication between processes is limited. The package provides an API enabling R programmers to implement Gaussian process-based methods by using the distributed linear algebra operations without any C or MPI coding. We illustrate the approach and software by analyzing an astrophysics dataset with n = 67, 275 observations.", "venue": "ArXiv", "authors": ["Christopher J. Paciorek", "Benjamin  Lipshitz", "Wei  Zhuo", "Prabhat", "Cari  Kaufman", "Rollin C. Thomas"], "year": 2013, "n_citations": 27}
{"id": 516936, "s2_id": "cda80e7147990f81444d830348f4d8dd3238a88a", "title": "gearshifft - The FFT Benchmark Suite for Heterogeneous Platforms", "abstract": "Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields ranging from computer science to natural sciences and engineering. With the rising data production bandwidths of modern FFT applications, judging best which algorithmic tool to apply, can be vital to any scientific endeavor. As tailored FFT implementations exist for an ever increasing variety of high performance computer hardware, choosing the best performing FFT implementation has strong implications for future hardware purchase decisions, for resources FFTs consume and for possibly decisive financial and time savings ahead of the competition. This paper therefor presents gearshifft, which is an open-source and vendor agnostic benchmark suite to process a wide variety of problem sizes and types with state-of-the-art FFT implementations (fftw, clFFT and cuFFT). gearshifft provides a reproducible, unbiased and fair comparison on a wide variety of hardware to explore which FFT variant is best for a given problem size.", "venue": "ISC", "authors": ["Peter  Steinbach", "Matthias  Werner"], "year": 2017, "n_citations": 14}
{"id": 523848, "s2_id": "d45733f011e8221a2fe51c6b3e21ed64cf657475", "title": "LinBox Founding Scope Allocation, Parallel Building Blocks, and Separate Compilation", "abstract": "As a building block for a wide range of applications, computational exact linear algebra has to conciliate efficiency and genericity. The goal of the LinBox project is to address this problem in the design of an efficient general-purpose C++ opensource library for exact linear algebra over the integers, the rationals, and finite fields. Matrices can be either dense, sparse or black box (i.e. viewed as a linear operator, acting on vectors only). The library proposes a set of high level linear algebra solutions, such as the rank, the determinant, the solution of a linear system, the Smith normal form, the echelon form, the characteristic polynomial, etc.", "venue": "ICMS", "authors": ["Jean-Guillaume  Dumas", "Thierry  Gautier", "Cl\u00e9ment  Pernet", "B. David Saunders"], "year": 2010, "n_citations": 7}
{"id": 527751, "s2_id": "cf1c04033b4fec0563340a8e4b3fcec58e758ad5", "title": "COREclust: a new package for a robust and scalable analysis of complex data", "abstract": "In this paper, we present a new R package COREclust dedicated to the detection of representative variables in high dimensional spaces with a potentially limited number of observations. Variable sets detection is based on an original graph clustering strategy denoted CORE-clustering algorithm that detects CORE-clusters, i.e. variable sets having a user defined size range and in which each variable is very similar to at least another variable. Representative variables are then robustely estimate as the CORE-cluster centers. This strategy is entirely coded in C++ and wrapped by R using the Rcpp package. A particular effort has been dedicated to keep its algorithmic cost reasonable so that it can be used on large datasets. After motivating our work, we will explain the CORE-clustering algorithm as well as a greedy extension of this algorithm. We will then present how to use it and results obtained on synthetic and real data.", "venue": "ArXiv", "authors": ["Camille  Champion", "Anne-Claire  Brunet", "Jean-Michel  Loubes", "Laurent  Risser"], "year": 2018, "n_citations": 0}
{"id": 528378, "s2_id": "26d90188e6079208b44d698b16d813a00c38c12f", "title": "Nauticle: A general-purpose particle-based simulation tool", "abstract": "Abstract Nauticle is a general-purpose simulation tool for the flexible and highly configurable application of particle-based methods of either discrete or continuum phenomena. The paper introduces a novel approach to the implementation which handles a general formulation composed of user-defined expressions and interaction-laws covering an extensive range of particle-based methods. As a result, Nauticle has three distinct levels for users and developers. At the top level, the Symbolic Form Language (SFL) of Nauticle facilitates the formulation of user-defined numerical models in text-based configuration files. The SFL can be intuitively extended at the intermediate level with new particle methods without tedious recoding or even the knowledge of the bottom level. The paper presents the structure of the underlying general algorithm; then the top two levels are discussed in detail and illustrated by simple application examples. Finally, the efficiency of the code is also tested through a performance benchmark. Program summary Program Title: Nauticle Program Files doi: http://dx.doi.org/10.17632/9kxrzr96ww.1 Licensing provisions: GNU Lesser General Public License v3 Programming language: C++ Nature of problem: Construction of a flexible simulation tool for particle methods by multilevel user and developer interface for building almost arbitrary mathematical models \u2013 in one, two or three dimensions \u2013 through user-defined algebraic and partial differential equations. Solution method: A general formulation of particle methods (schemes) or mathematical models interpretable as a description of relationships between particles by considering them as a set of interaction laws is solved at the bottom level. The simulation case including the definition of the exact form of the equations can be constructed using the proposed Symbolic Form Language at the top level by YAML-documents; hence the solver does not require any programming knowledge or experience. Besides that, at the second level, the Nauticle environment provides an efficient, flexible interface in C++ for the adoption of truly arbitrary new schemes interpreted as the simple extension of the Symbolic Form Language. The collection of particle methods and mathematical models already implemented is: \u2022 Gravitational interaction (for n-body problems) \u2022 Smoothed Particle Hydrodynamics (SPH) \u2022 Discrete Element Method (DEM) \u2022 Discrete Vortex Method (DVM) \u2022 micro-scale social force model \u2022 molecular dynamics based on the Lennard-Jones potential (MD) \u2022 Kuramoto synchronization Restrictions: The present version of Nauticle does not involve implicit schemes. Additional comments: The automated installation of the current Nauticle release requires Advanced Packaging Tool (APT) or Homebrew. The code has been tested under Ubuntu 16.04. and MacOS 10.12. and 10.14. The Nauticle source code is available at www.bitbucket.org/nauticleproject", "venue": "Comput. Phys. Commun.", "authors": ["Bal\u00e1zs  Havasi-T\u00f3th"], "year": 2020, "n_citations": 3}
{"id": 530338, "s2_id": "9c5e31da06537ba884618515815d4db9e9fea771", "title": "Limits of Educational Soft \"GeoGebra\" in a Critical Constructive Review", "abstract": "Mathematical educational soft explore, investigating in a dynamical way, some algebraically, geometrically problems, the expected results being used to involve a lot of mathematical results. One such software soft is GeoGebra. The software is free and multi-platform dynamic mathematics software for learning and teaching, awards in Europe and the USA. This paper describes some critical but constructive investigation using the platform for graph functions and dynamic geometry.", "venue": "ArXiv", "authors": ["Valerian  Antohe"], "year": 2009, "n_citations": 26}
{"id": 533117, "s2_id": "97d2ae057ba44add95e1efea04174945220ee7ed", "title": "Parareal Algorithm Implementation and Simulation in Julia", "abstract": "We present a full implementation of the parareal algorithm---an integration technique to solve differential equations in parallel---in the Julia programming language for a fully general, first-order, initial-value problem. Our implementation accepts both coarse and fine integrators as functional arguments. We use Euler's method and another Runge-Kutta integration technique as the integrators in our experiments. We also present a simulation of the algorithm for purposes of pedagogy.", "venue": "ArXiv", "authors": ["Tyler M. Masthay", "Saverio  Perugini"], "year": 2017, "n_citations": 0}
{"id": 537365, "s2_id": "7fcddb75fa43c4f35695fab058b3609ff7e298c3", "title": "High-Order Discontinuous Galerkin Methods by GPU Metaprogramming", "abstract": "Discontinuous Galerkin (DG) methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust: They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. In a recent publication, we have shown that DG methods also adapt readily to execution on modern, massively parallel graphics processors (GPUs). A number of qualities of the method contribute to this suitability, reaching from locality of reference, through regularity of access patterns, to high arithmetic intensity. In this article, we illuminate a few of the more practical aspects of bringing DG onto a GPU, including the use of a Python-based metaprogramming infrastructure that was created specifically to support DG, but has found many uses across all disciplines of computational science.", "venue": "ArXiv", "authors": ["Andreas  Kl\u00f6ckner", "Timothy C. Warburton", "Jan S. Hesthaven"], "year": 2012, "n_citations": 19}
{"id": 548017, "s2_id": "5bb5fbb2a7c6e57b333c0793a1b173cb2c41f60d", "title": "Lanczos $\\tau$-method optimal algorithm in APS for approximating the mathematical functions", "abstract": "A new procedure is constructed by means of APS in APLAN language. The procedure solves the initial-value problem for linear differential equations of order $k$ with polynomial coefficients and regular singularity in the initialization point in the interval $[a, b]$ and computes the algebraic polynomial $y_n$ of given order $n$. A new algorithm of Lanczos $\\tau$-method is built for this procedure, the solution existence $y_n$ of the initial-value problem proved on this algorithm and also is proved the optimality by precision of order $k$ derivative of the initial-value problem solution.", "venue": "ArXiv", "authors": ["P. N. Denisenko"], "year": 2006, "n_citations": 0}
{"id": 548849, "s2_id": "861cf8a9829b36c875606777d7c64202ddac2463", "title": "A Fixed-Point Type for Octave", "abstract": "This paper announces the availability of a fixed point toolbox for the Matlab compatible software package Octave. This toolbox is released under the GNU Public License, and can be used to model the losses in algorithms implemented in hardware. Furthermore, this paper presents as an example of the use of this toolbox, the effects of a fixed point implementation on the precision of an OFDM modulator.", "venue": "ArXiv", "authors": ["David  Bateman", "Laurent  Mazet", "V\u00e9ronique  Buzenac-Settineri", "Markus  Muck"], "year": 2006, "n_citations": 0}
{"id": 550705, "s2_id": "582cd6ada6adc74d8f11a69b477bad506dbac228", "title": "Architecture and Performance of Devito, a System for Automated Stencil Computation", "abstract": "Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process\u2014from mathematical equations down to C++ code\u2014is performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling, and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the backend of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications.", "venue": "ACM Trans. Math. Softw.", "authors": ["Fabio  Luporini", "Michael  Lange", "Mathias  Louboutin", "Navjot  Kukreja", "Jan  H\u00fcckelheim", "Charles  Yount", "Philipp A. Witte", "Paul H. J. Kelly", "Gerard  Gorman", "Felix J. Herrmann"], "year": 2020, "n_citations": 47}
{"id": 552317, "s2_id": "2ea66d45207255cd05434f92562552b37a9e5f78", "title": "Manopt, a matlab toolbox for optimization on manifolds", "abstract": "Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is well-suited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. \n \nThe Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. By dealing internally with most of the differential geometry, the package aims particularly at lowering the entrance barrier.", "venue": "J. Mach. Learn. Res.", "authors": ["Nicolas  Boumal", "Bamdev  Mishra", "Pierre-Antoine  Absil", "Rodolphe  Sepulchre"], "year": 2014, "n_citations": 661}
{"id": 554293, "s2_id": "53e7e01482932c3a45b5f67962faec1799d285bf", "title": "A GEMM interface and implementation on NVIDIA GPUs for multiple small matrices", "abstract": "We present an interface and an implementation of the General Matrix Multiply (GEMM) routine for multiple small matrices processed simultaneously on NVIDIA graphics processing units (GPUs). We focus on matrix sizes under 16. The implementation can be easily extended to larger sizes. For single precision matrices, our implementation is 30% to 600% faster than the batched cuBLAS implementation distributed in the CUDA Toolkit 5.0 on NVIDIA Tesla K20c. For example, we obtain 104 GFlop/s and 216 GFlop/s when multiplying 100,000 independent matrix pairs of size 10 and 16, respectively. Similar improvement in performance is obtained for other sizes, in single and double precisions for real and complex types, and when the number of matrices is smaller. Apart from our implementation, our different function interface also plays an important role in the improved performance. Applications of this software include finite element computation on GPUs. A second leading dimension-based batched GEMM interface for CUDA.Implementation of GEMM routine for multiple small matrices.30% to 600% faster than the batched cuBLAS in CUDA Toolkit 5.0.Specialized for matrix sizes under 16 on NVIDIA Tesla K20c.", "venue": "J. Parallel Distributed Comput.", "authors": ["Chetan  Jhurani", "Paul  Mullowney"], "year": 2015, "n_citations": 29}
{"id": 561914, "s2_id": "d3fca739a960cfecd2d50e0e4d6dbce13572b194", "title": "d2o: a distributed data object for parallel high-performance computing in Python", "abstract": "We introduce d2o, a Python module for cluster-distributed multi-dimensional numerical arrays. It acts as a layer of abstraction between the algorithm code and the data-distribution logic. The main goal is to achieve usability without losing numerical performance and scalability. d2o\u2019s global interface is similar to the one of a numpy.ndarray, whereas the cluster node\u2019s local data is directly accessible for use in customized high-performance modules. d2o is written in pure Python which makes it portable and easy to use and modify. Expensive operations are carried out by dedicated external libraries like numpy and mpi4py. The performance of d2o is on a par with numpy for serial applications and scales well when moving to an MPI cluster. d2o is open-source software available under the GNU General Public License v3 (GPL-3) at https://gitlab.mpcdf.mpg.de/ift/D2O.", "venue": "Journal of Big Data", "authors": ["Theo  Steininger", "Maksim  Greiner", "Frederik  Beaujean", "Torsten A. En\u00dflin"], "year": 2016, "n_citations": 6}
{"id": 566211, "s2_id": "137038e8ecf8b060414d1c2833989364ec013eb2", "title": "A framework for general sparse matrix-matrix multiplication on GPUs and heterogeneous processors", "abstract": "General sparse matrix-matrix multiplication (SpGEMM) is a fundamental building block for numerous applications such as algebraic multigrid method (AMG), breadth first search and shortest path problem. Compared to other sparse BLAS routines, an efficient parallel SpGEMM implementation has to handle extra irregularity from three aspects: (1) the number of nonzero entries in the resulting sparse matrix is unknown in advance, (2) very expensive parallel insert operations at random positions in the resulting sparse matrix dominate the execution time, and (3) load balancing must account for sparse data in both input matrices.In this work we propose a framework for SpGEMM on GPUs and emerging CPU-GPU heterogeneous processors. This framework particularly focuses on the above three problems. Memory pre-allocation for the resulting matrix is organized by a hybrid method that saves a large amount of global memory space and efficiently utilizes the very limited on-chip scratchpad memory. Parallel insert operations of the nonzero entries are implemented through the GPU merge path algorithm that is experimentally found to be the fastest GPU merge approach. Load balancing builds on the number of necessary arithmetic operations on the nonzero entries and is guaranteed in all stages.Compared with the state-of-the-art CPU and GPU SpGEMM methods, our approach delivers excellent absolute performance and relative speedups on various benchmarks multiplying matrices with diverse sparsity structures. Furthermore, on heterogeneous processors, our SpGEMM approach achieves higher throughput by using re-allocatable shared virtual memory. We design a framework for SpGEMM on modern manycore processors using the CSR format.We present a hybrid method for pre-allocating the resulting sparse matrix.We propose an efficient parallel insert method for long rows of the resulting matrix.We develop a heuristic-based load balancing strategy.Our approach significantly outperforms other known CPU and GPU SpGEMM methods.", "venue": "J. Parallel Distributed Comput.", "authors": ["Weifeng  Liu", "Brian  Vinter"], "year": 2015, "n_citations": 67}
{"id": 569761, "s2_id": "09b11dd581fd9d00c3a55d4a49f83660bd7c3d9a", "title": "Mathematical foundations of the GraphBLAS", "abstract": "The GraphBLAS standard (GraphBlas.org) is being developed to bring the potential of matrix-based graph algorithms to the broadest possible audience. Mathematically, the GraphBLAS defines a core set of matrix-based graph operations that can be used to implement a wide class of graph algorithms in a wide range of programming environments. This paper provides an introduction to the mathematics of the GraphBLAS. Graphs represent connections between vertices with edges. Matrices can represent a wide range of graphs using adjacency matrices or incidence matrices. Adjacency matrices are often easier to analyze while incidence matrices are often better for representing data. Fortunately, the two are easily connected by matrix multiplication. A key feature of matrix mathematics is that a very small number of matrix operations can be used to manipulate a very wide range of graphs. This composability of a small number of operations is the foundation of the GraphBLAS. A standard such as the GraphBLAS can only be effective if it has low performance overhead. Performance measurements of prototype GraphBLAS implementations indicate that the overhead is low.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "Peter  Aaltonen", "David A. Bader", "Aydin  Bulu\u00e7", "Franz  Franchetti", "John R. Gilbert", "Dylan  Hutchison", "Manoj  Kumar", "Andrew  Lumsdaine", "Henning  Meyerhenke", "Scott  McMillan", "Carl  Yang", "John D. Owens", "Marcin  Zalewski", "Timothy G. Mattson", "Jos\u00e9 E. Moreira"], "year": 2016, "n_citations": 147}
{"id": 571422, "s2_id": "626ce40a0cf7870e46b1a17d62b89bd2c31dd010", "title": "The deal.II library, Version 9.0", "abstract": "Abstract This paper provides an overview of the new features of the finite element library deal.II version 9.0.", "venue": "J. Num. Math.", "authors": ["Giovanni  Alzetta", "Daniel  Arndt", "Wolfgang  Bangerth", "Vishal  Boddu", "Benjamin  Brands", "Denis  Davydov", "Rene  Gassm\u00f6ller", "Timo  Heister", "Luca  Heltai", "Katharina  Kormann", "Martin  Kronbichler", "Matthias  Maier", "Jean-Paul  Pelteret", "Bruno  Turcksin", "David  Wells"], "year": 2018, "n_citations": 134}
{"id": 571444, "s2_id": "122dceedc49aab0c0755620edcc3a955d213c4cd", "title": "T3PS: Tool for Parallel Processing in Parameter Scans", "abstract": "T3PS is a program that can be used to quickly design and perform parameter scans while easily taking advantage of the multi-core architecture of current processors. It takes an easy to read and write parameter scan definition file format as input. Based on the parameter ranges and other options contained therein, it distributes the calculation of the parameter space over multiple processes and possibly computers. The derived data is saved in a plain text file format readable by most plotting software. The supported scanning strategies include: grid scan, random scan, Markov Chain Monte Carlo, numerical optimization. Several example parameter scans are shown and compared with results in the literature.", "venue": "ArXiv", "authors": ["Vinzenz  Maurer"], "year": 2015, "n_citations": 1}
{"id": 572182, "s2_id": "086b9f63e508e3334e84427d2243e0cf3bc5d8f7", "title": "cuTT: A High-Performance Tensor Transpose Library for CUDA Compatible GPUs", "abstract": "We introduce the CUDA Tensor Transpose (cuTT) library that implements high-performance tensor transposes for NVIDIA GPUs with Kepler and above architectures. cuTT achieves high performance by (a) utilizing two GPU-optimized transpose algorithms that both use a shared memory buffer in order to reduce global memory access scatter, and by (b) computing memory positions of tensor elements using a thread-parallel algorithm. We evaluate the performance of cuTT on a variety of benchmarks with tensor ranks ranging from 2 to 12 and show that cuTT performance is independent of the tensor rank and that it performs no worse than an approach based on code generation. We develop a heuristic scheme for choosing the optimal parameters for tensor transpose algorithms by implementing an analytical GPU performance model that can be used at runtime without need for performance measurements or profiling. Finally, by integrating cuTT into the tensor algebra library TAL-SH, we significantly reduce the tensor transpose overhead in tensor contractions, achieving as low as just one percent overhead for arithmetically intensive tensor contractions.", "venue": "ArXiv", "authors": ["Antti-Pekka  Hynninen", "Dmitry I. Lyakh"], "year": 2017, "n_citations": 14}
{"id": 574559, "s2_id": "7fbc6d7583d7c8ddf2dcee3dc194cc535c0b92dd", "title": "A Tetrahedral Space-Filling Curve for Nonconforming Adaptive Meshes", "abstract": "We introduce a space-filling curve for triangular and tetrahedral red-refinement that can be computed using bitwise interleaving operations similar to the well-known Z-order or Morton curve for cub...", "venue": "SIAM J. Sci. Comput.", "authors": ["Carsten  Burstedde", "Johannes  Holke"], "year": 2016, "n_citations": 14}
{"id": 575524, "s2_id": "085877b06de0ff260216be516d2d2bf00fde7ff8", "title": "Fast parallel multidimensional FFT using advanced MPI", "abstract": "We present a new method for performing global redistributions of multidimensional arrays essential to parallel fast Fourier (or similar) transforms. Traditional methods use standard all-to-all collective communication of contiguous memory buffers, thus necessary requiring local data realignment steps intermixed in-between redistribution and transform steps. Instead, our method takes advantage of subarray datatypes and generalized all-to-all scatter/gather from the MPI-2 standard to communicate discontiguous memory buffers, effectively eliminating the need for local data realignments. Despite generalized all-to-all communication of discontiguous data being generally slower, our proposal economizes in local work. For a range of strong and weak scaling tests, we found the overall performance of our method to be on par and often better than well-established libraries like MPI-FFTW, P3DFFT, and 2DECOMP&FFT. We provide compact routines implemented at the highest possible level using the MPI bindings for the C programming language. These routines apply to any global redistribution, over any two directions of a multidimensional array, decomposed on arbitrary Cartesian processor grids (1D slabs, 2D pencils, or even higher-dimensional decompositions). The high level implementation makes the code easy to read, maintain, and eventually extend. Our approach enables for future speedups from optimizations in the internal datatype handling engines within MPI implementations.", "venue": "J. Parallel Distributed Comput.", "authors": ["Lisandro  Dalc\u00edn", "Mikael  Mortensen", "David E. Keyes"], "year": 2019, "n_citations": 20}
{"id": 575824, "s2_id": "b4117691962f17fd6b57f05e780135c46fb2c1cf", "title": "Abaqus2Matlab: A suitable tool for finite element post-processing", "abstract": "Abstract A suitable piece of software is presented to connect Abaqus, a sophisticated finite element package, with Matlab, the most comprehensive program for mathematical analysis. This interface between these well-known codes not only benefits from the image processing and the integrated graph-plotting features of Matlab but also opens up new opportunities in results post-processing, statistical analysis and mathematical optimization, among many other possibilities. The software architecture and usage are appropriately described and two problems of particular engineering significance are addressed to demonstrate its capabilities. Firstly, the software is employed to assess cleavage fracture through a novel 3-parameter Weibull probabilistic framework. Then, its potential to create and train neural networks is used to identify damage parameters through a hybrid experimental\u2013numerical scheme, and model crack propagation in structural materials by means of a cohesive zone approach. The source code, detailed documentation and a large number of tutorials can be freely downloaded from www.abaqus2matlab.com .", "venue": "Adv. Eng. Softw.", "authors": ["George  Papazafeiropoulos", "Miguel  Mu\u00f1iz-Calvente", "Emilio  Mart\u00ednez-Pa\u00f1eda"], "year": 2017, "n_citations": 98}
{"id": 575858, "s2_id": "953090d7c53a918359b00f635b31e9adc36a3cc4", "title": "Escaping the abstraction: a foreign function interface for the Unified Form Language [UFL]", "abstract": "High level domain specific languages for the finite element method underpin high productivity programming environments for simulations based on partial differential equations (PDE) while employing automatic code generation to achieve high performance. However, a limitation of this approach is that it does not support operators that are not directly expressible in the vector calculus. This is critical in applications where PDEs are not enough to accurately describe the physical problem of interest. The use of deep learning techniques have become increasingly popular in filling this knowledge gap, for example to include features not represented in the differential equations, or closures for unresolved spatiotemporal scales. We introduce an interface within the Firedrake finite element system that enables a seamless interface with deep learning models. This new feature composes with the automatic differentiation capabilities of Firedrake, enabling the automated solution of inverse problems. Our implementation interfaces with PyTorch and can be extended to other machine learning libraries. The resulting framework supports complex models coupling PDEs and deep learning whilst maintaining separation of concerns between application scientists and software experts.", "venue": "ArXiv", "authors": ["Nacime  Bouziani", "David A. Ham"], "year": 2021, "n_citations": 0}
{"id": 579439, "s2_id": "b5a3c5809ad932f7f141f88979fc0b7b30181877", "title": "Associative Arrays: Unified Mathematics for Spreadsheets, Databases, Matrices, and Graphs", "abstract": "Data processing systems impose multiple views on data as it is processed by the system. These views include spreadsheets, databases, matrices, and graphs. The common theme amongst these views is the need to store and operate on data as whole sets instead of as individual data elements. This work describes a common mathematical representation of these data sets (associative arrays) that applies across a wide range of applications and technologies. Associative arrays unify and simplify these different approaches for representing and manipulating data into common two-dimensional view of data. Specifically, associative arrays (1) reduce the effort required to pass data between steps in a data processing system, (2) allow steps to be interchanged with full confidence that the results will be unchanged, and (3) make it possible to recognize when steps can be simplified or eliminated. Most database system naturally support associative arrays via their tabular interfaces. The D4M implementation of associative arrays uses this feature to provide a common interface across SQL, NoSQL, and NewSQL databases.", "venue": "ArXiv", "authors": ["Jeremy  Kepner", "Julian  Chaidez", "Vijay  Gadepally", "Hayden  Jansen"], "year": 2015, "n_citations": 12}
{"id": 582958, "s2_id": "67b8b765b71c5eab868fae81c1605d541b1b9804", "title": "ProbNum: Probabilistic Numerics in Python", "abstract": "Probabilistic numerical methods (PNMs) solve numerical problems via probabilistic inference. They have been developed for linear algebra, optimization, integration and differential equation simulation. PNMs naturally incorporate prior information about a problem and quantify uncertainty due to finite computational resources as well as stochastic input. In this paper, we present ProbNum: a Python library providing state-of-the-art probabilistic numerical solvers. ProbNum enables custom composition of PNMs for specific problem classes via a modular design as well as wrappers for off-the-shelf use. Tutorials, documentation, developer guides and benchmarks are available online at www.probnum.org.", "venue": "ArXiv", "authors": ["Jonathan  Wenger", "Nicholas  Kramer", "Marvin  Pfortner", "Jonathan  Schmidt", "Nathanael  Bosch", "Nina  Effenberger", "Johannes  Zenn", "Alexandra  Gessner", "Toni  Karvonen", "Franccois-Xavier  Briol", "Maren  Mahsereci", "Philipp  Hennig"], "year": 2021, "n_citations": 0}
{"id": 586759, "s2_id": "927d57b7b4126c7ba013d558277ac7d6ee0b9b44", "title": "Numerical methods for the computation of the confluent and Gauss hypergeometric functions", "abstract": "The two most commonly used hypergeometric functions are the confluent hypergeometric function and the Gauss hypergeometric function. We review the available techniques for accurate, fast, and reliable computation of these two hypergeometric functions in different parameter and variable regimes. The methods that we investigate include Taylor and asymptotic series computations, Gauss\u2013Jacobi quadrature, numerical solution of differential equations, recurrence relations, and others. We discuss the results of numerical experiments used to determine the best methods, in practice, for each parameter and variable regime considered. We provide \u201croadmaps\u201d with our recommendation for which methods should be used in each situation.", "venue": "Numerical Algorithms", "authors": ["John W. Pearson", "Sheehan  Olver", "Mason A. Porter"], "year": 2016, "n_citations": 48}
{"id": 587144, "s2_id": "7559c5dc5a0cd650dce312d8d437dcb40917ec4a", "title": "HILUCSI: Simple, robust, and fast multilevel ILU for large-scale saddle-point problems from PDEs", "abstract": "Incomplete factorization is a widely used preconditioning technique for Krylov subspace methods for solving large-scale sparse linear systems. Its multilevel variants, such as those in ILUPACK and ARMS, are more robust for many symmetric or unsymmetric linear systems than the traditional, single-level incomplete LU (or ILU) techniques. However, multilevel ILU still lacked robustness and efficiency for some large-scale saddle-point problems, which often arise from systems of partial differential equations (PDEs). In this work, we introduce HILUCSI, or Hierarchical Incomplete LU-Crout with Scalability-oriented and Inverse-based dropping, which is specifically designed to take advantage of some special features of such systems. HILUCSI differs from the state-of-the-art ILU techniques in two main aspects. First, HILUCSI leverages the near or partial symmetry of the underlying problems and the inherent block structures of multilevel ILU to improve robustness and to simplify the treatment of indefinite systems. Second, HILUCSI introduces a scalability-oriented dropping in conjunction with a variant of inverse-based dropping to improve the efficiency for large-scale problems from PDEs. We demonstrate the effectiveness of HILUCSI for a number of benchmark problems, including those from mixed formulation of the Poisson equation, Stokes equations, and Navier-Stokes equations. We also compare its performance with ILUPACK, the supernodal ILUTP in SuperLU, and multithreaded direct solvers in PARDISO and MUMPS.", "venue": "Numer. Linear Algebra Appl.", "authors": ["Qiao  Chen", "Aditi  Ghai", "Xiangmin  Jiao"], "year": 2021, "n_citations": 5}
{"id": 589000, "s2_id": "8e7ab565d5ef36cdaf686b1866cc8db39510473f", "title": "Unified form language: A domain-specific language for weak formulations of partial differential equations", "abstract": "We present the Unified Form Language (UFL), which is a domain-specific language for representing weak formulations of partial differential equations with a view to numerical approximation. Features of UFL include support for variational forms and functionals, automatic differentiation of forms and expressions, arbitrary function space hierarchies for multifield problems, general differential operators and flexible tensor algebra. With these features, UFL has been used to effortlessly express finite element methods for complex systems of partial differential equations in near-mathematical notation, resulting in compact, intuitive and readable programs. We present in this work the language and its construction. An implementation of UFL is freely available as an open-source software library. The library generates abstract syntax tree representations of variational problems, which are used by other software libraries to generate concrete low-level implementations. Some application examples are presented and libraries that support UFL are highlighted.", "venue": "TOMS", "authors": ["Martin Sandve Aln\u00e6s", "Anders  Logg", "Kristian B. \u00d8lgaard", "Marie E. Rognes", "Garth N. Wells"], "year": 2014, "n_citations": 290}
{"id": 602034, "s2_id": "3352fa93f289fc0eb9117baaf1cf0711e8f8d59f", "title": "tsrobprep - an R package for robust preprocessing of time series data", "abstract": "Data cleaning is a crucial part of every data analysis exercise. Yet, the currently available R packages do not provide fast and robust methods for cleaning and preparation of time series data. The open source package tsrobprep introduces efficient methods for handling missing values and outliers using model based approaches. For data imputation a probabilistic replacement model is proposed, which may consist of autoregressive components and external inputs. For outlier detection a clustering algorithm based on finite mixture modelling is introduced, which considers time series properties in terms of the gradient and the underlying seasonality as features. The procedure allows to return a probability for each observation being outlying data as well as a specific cause for an outlier assignment in terms of the provided feature space. The methods work robust and are fully tunable. Moreover, by providing the auto data cleaning function the data preprocessing can be carried out in one cast, without comprehensive tuning and providing suitable results. The primary motivation of the package is the preprocessing of energy system data. We present application for electricity load, wind and solar power data.", "venue": "SoftwareX", "authors": ["Michal  Narajewski", "Jens  Kley-Holsteg", "Florian  Ziel"], "year": 2021, "n_citations": 1}
{"id": 606147, "s2_id": "9dd8b19a8c2b946f5298be554549bf31b635a8bb", "title": "OPESCI-FD: Automatic Code Generation Package for Finite Difference Models", "abstract": "In this project, we introduce OPESCI-FD, a Python package built on symbolic mathematics to automatically generate Finite Difference models from a high-level description of the model equations. We investigate applying this framework to generate the propagator program used in seismic imaging. We implement the 3D velocity-stress FD scheme as an example and demonstrate the advantages of usability, flexibility and accuracy of the framework. The design of OPESCI-FD aims to allow rapid development, analysis and optimisation of Finite Difference programs. OPESCI-FD is the foundation for continuing development by the OPESCI project team, building on the research presented in this report. This report concludes by reviewing the further developments that are already under way, as well as the scope for extension to cater for other equations and numerical schemes.", "venue": "ArXiv", "authors": ["Tianjiao  Sun"], "year": 2016, "n_citations": 2}
{"id": 619178, "s2_id": "03a6b3cb002403928567130b310be3910804eb5e", "title": "BEANS - a software package for distributed Big Data analysis", "abstract": "BEANS software is a web based, easy to install and maintain, new tool to store and analyse data in a distributed way for a massive amount of data. It provides a clear interface for querying, filtering, aggregating, and plotting data from an arbitrary number of datasets. Its main purpose is to simplify the process of storing, examining and finding new relations in the so-called Big Data. \nCreation of BEANS software is an answer to the growing needs of the astronomical community to have a versatile tool to store, analyse and compare the complex astrophysical numerical simulations with observations (e.g. simulations of the Galaxy or star clusters with the Gaia archive). However, this software was built in a general form and it is ready to use in any other research field or open source software.", "venue": "ArXiv", "authors": ["Arkadiusz  Hypki"], "year": 2016, "n_citations": 1}
{"id": 623423, "s2_id": "eece4c06b7ed8ddf227abb8257c74d5b982b7f3a", "title": "Genbit Compress Tool(GBC): A Java-Based Tool to Compress DNA Sequences and Compute Compression Ratio(bits/base) of Genomes", "abstract": "We present a Compression Tool , GenBit Compress\u201d, for genetic sequences based on our new proposed \u201cGenBit Compress Algorithm\u201d. Our Tool achieves the best compression ratios for Entire Genome (DNA sequences) . Significantly better compression results show that GenBit compress algorithm is the best among the remaining Genome compression algorithms for non-repetitive DNA sequences in Genomes. The standard Compression algorithms such as gzip or compress cannot compress DNA sequences but only expand them in size. In this paper we consider the problem of DNA compression. It is well known that one of the main features of DNA Sequences is that they contain substrings which are duplicated except for a few random Mutations. For this reason most DNA compressors work by searching and encoding approximate repeats. We depart from this strategy by searching and encoding only exact repeats. our proposed algorithm achieves the best compression ratio for DNA sequences for larger genome. As long as 8 lakh characters can be given as input While achieving the best compression ratios for DNA sequences, our new GenBit Compress program significantly improves the running time of all previous DNA compressors. Assigning binary bits for fragments of DNA sequence is also a unique concept introduced in this program for the first time in DNA compression.", "venue": "ArXiv", "authors": ["P. Raja Rajeswari", "Allam  Apparo", "V. K. Kumar"], "year": 2010, "n_citations": 18}
{"id": 634487, "s2_id": "924183bc40d606864acd73875b4b4d1b93c5bc3a", "title": "FDTD: Solving 1+1D delay PDE in parallel", "abstract": "Abstract We present a proof of concept for solving a 1+1D complex-valued, delay partial differential equation (PDE) that emerges in the study of waveguide quantum electrodynamics (QED) by adapting the finite-difference time-domain (FDTD) method. The delay term is spatially non-local, rendering conventional approaches such as the method of lines inapplicable. We show that by properly designing the grid and by supplying the (partial) exact solution as the boundary condition, the delay PDE can be numerically solved. In addition, we demonstrate that while the delay imposes strong data dependency, multi-thread parallelization can nevertheless be applied to such a problem. Our code provides a numerically exact solution to the time-dependent multi-photon scattering problem in waveguide QED. Program summary Program Title: FDTD: solving 1+1D delay PDE Program Files doi: http://dx.doi.org/10.17632/mmyw3fgjxh.1 Licensing provisions: MIT Programming language: C (C99) Nature of problem: This program solves an unconventional 1+1D delay PDE that emerges in the study of waveguide quantum electrodynamics. The delay PDE is complex-valued and has a non-local delay term, and the solution to it provides the full dynamics of the system consisting of a few 1D photons and a two-level system in front of a mirror. Solution method: The finite-difference time-domain (FDTD) method is adapted. Given the initial condition of the system, the corresponding boundary condition is generated, and then the FDTD solver marches through the entire spacetime grid. Multiple solvers are supported using either OpenMP (wavefront) or pthreads (swarm). Additional comments including restrictions and unusual features: 1. Depending on the input parameters the memory and disk usages of the program can be excessive, so the users should choose the parameters wisely (see main text). 2. The multi-thread support using OpenMP is turned on by default. See README for how to turn it off and switch to pthreads instead. 3. As a by-product, a numerical routine is provided for evaluating the incomplete Gamma function \u03b3 ( n , z ) with nonzero positive integers n \u2265 1 and complex-valued z .", "venue": "Comput. Phys. Commun.", "authors": ["Yao-Lung L. Fang"], "year": 2019, "n_citations": 5}
{"id": 652475, "s2_id": "75b8d28b3cecd21dc59027617a7ecf5f1df16962", "title": "Dynamic Computation of Runge Kutta Fourth Order Algorithm for First and Second Order Ordinary Differential Equation Using Java", "abstract": "Differential equations arise in mathematics, physics,medicine, pharmacology, communications, image processing and animation, etc. An Ordinary Differential Equation (ODE) is a differential equation if it involves derivatives with respect to only one independent variable which can be studied from different perspectives, such as: analytical methods, graphical methods and numerical methods. This research paper therefore revises the standard Runge - Kutta fourth order algorithm by using compiler techniques to dynamically evaluate the inputs and implement the algorithm for both first and second order derivatives of the ODE. We have been able to develop and implement the software that can be used to evaluate inputs and compute solutions (approximately and analytically) for the ODE function at a more efficient rate than the traditional method.", "venue": "ArXiv", "authors": ["A. O. Anidu", "S. A. Arekete", "A. O. Adedayo", "A. O. Adekoya"], "year": 2015, "n_citations": 4}
{"id": 658966, "s2_id": "9d012e976bacafd8377152e06aae0bf6605646be", "title": "Exploiting Asynchronous Priority Scheduling in Parallel Eikonal Solvers", "abstract": "Numerical solutions to the Eikonal equation are computed using variants of the fast marching method, the fast sweeping method, and the fast iterative method. In this paper, we provide a unified view of these algorithms that highlights their similarities and suggests a wider class of Eikonal solvers. We then use this framework to justify applying concurrent priority scheduling techniques to Eikonal solvers. We demonstrate that doing so results in good parallel performance for a problem from seismology. We explain why existing Eikonal solvers may produce different results despite using the same differencing scheme and demonstrate techniques to address these discrepancies. These techniques allow us to obtain deterministic output from our asynchronous fine-grained parallel Eikonal solver.", "venue": "ArXiv", "authors": ["Ian  Henriksen", "Bozhi  You", "Keshav  Pingali"], "year": 2021, "n_citations": 0}
{"id": 663036, "s2_id": "35237cd992ecfefecd6006966c8b9a0051343135", "title": "Minkowski sum of HV-polytopes in Rn", "abstract": "Minkowski sums cover a wide range of applications in many different fields like algebra, morphing, robotics, mechanical CAD/CAM systems ... This paper deals with sums of polytopes in a n dimensional space provided that both H-representation and V-representation are available i.e. the polytopes are described by both their half-spaces and vertices. The first method uses the polytope normal fans and relies on the ability to intersect dual polyhedral cones. Then we introduce another way of considering Minkowski sums of polytopes based on the primal polyhedral cones attached to each vertex.", "venue": "ArXiv", "authors": ["Vincent  Delos", "Denis  Teissandier"], "year": 2014, "n_citations": 8}
{"id": 667751, "s2_id": "b1d76f850a5d78fde852b01a5c3949cc0e50a138", "title": "A Universal Machine for Biform Theory Graphs", "abstract": "Broadly speaking, there are two kinds of semantics-aware assistant systems for mathematics: proof assistants express the semantic in logic and emphasize deduction, and computer algebra systems express the semantics in programming languages and emphasize computation. Combining the complementary strengths of both approaches while mending their complementary weaknesses has been an important goal of the mechanized mathematics community for some time. \n \nWe pick up on the idea of biform theories and interpret it in the MMT/OMDoc framework which introduced the foundations-as-theories approach, and can thus represent both logics and programming languages as theories. This yields a formal, modular framework of biform theory graphs which mixes specifications and implementations sharing the module system and typing information. \n \nWe present automated knowledge management work flows that interface to existing specification/programming tools and enable an OPENMATH Machine, that operationalizes biform theories, evaluating expressions by exhaustively applying the implementations of the respective operators. We evaluate the new biform framework by adding implementations to the OPENMATH standard content dictionaries.", "venue": "MKM/Calculemus/DML", "authors": ["Michael  Kohlhase", "Felix  Mance", "Florian  Rabe"], "year": 2013, "n_citations": 7}
{"id": 675168, "s2_id": "1367bcf138c23bf58d5b66f1896320177b39749b", "title": "Can the Eureqa symbolic regression program, computer algebra and numerical analysis help each other?", "abstract": "The Eureqa symbolic regression program has recently received extensive press praise. A representative quote is \n\"There are very clever 'thinking machines' in existence today, such as Watson, the IBM computer that conquered Jeopardy! last year. But next to Eureqa, Watson is merely a glorified search engine.\" \nThe program was designed to work with noisy experimental data. However, if the data is generated from an expression for which there exists more concise equivalent expressions, sometimes some of the Eureqa results are one or more of those more concise equivalents. If not, perhaps one or more of the returned Eureqa results might be a sufficiently accurate approximation that is more concise than the given expression. Moreover, when there is no known closed form expression, the data points can be generated by numerical methods, enabling Eureqa to find expressions that concisely fit those data points with sufficient accuracy. In contrast to typical regression software, the user does not have to explicitly or implicitly provide a specific expression or class of expressions containiing unknown constants for the software to determine. \nIs Eureqa useful enough in these regards to provide an additional tool for experimental mathematics, computer algebra users and numerical analysis? Yes if used carefully. Can computer algebra and numerical methods help Eureqa? Definitely.", "venue": "ArXiv", "authors": ["David R. Stoutemyer"], "year": 2012, "n_citations": 12}
{"id": 675710, "s2_id": "925fa819231a3c4cde4a81076905ae03f6fe736c", "title": "Tea: A High-level Language and Runtime System for Automating Statistical Analysis", "abstract": "Though statistical analyses are centered on research questions and hypotheses, current statistical analysis tools are not. Users must first translate their hypotheses into specific statistical tests and then perform API calls with functions and parameters. To do so accurately requires that users have statistical expertise. To lower this barrier to valid, replicable statistical analysis, we introduce Tea, a high-level declarative language and runtime system. In Tea, users express their study design, any parametric assumptions, and their hypotheses. Tea compiles these high-level specifications into a constraint satisfaction problem that determines the set of valid statistical tests and then executes them to test the hypothesis. We evaluate Tea using a suite of statistical analyses drawn from popular tutorials. We show that Tea generally matches the choices of experts while automatically switching to non-parametric tests when parametric assumptions are not met. We simulate the effect of mistakes made by non-expert users and show that Tea automatically avoids both false negatives and false positives that could be produced by the application of incorrect statistical tests.", "venue": "UIST", "authors": ["Eunice  Jun", "Maureen  Daum", "Jared  Roesch", "Sarah E. Chasins", "Emery D. Berger", "Ren\u00e9  Just", "Katharina  Reinecke"], "year": 2019, "n_citations": 7}
{"id": 680466, "s2_id": "7eee285127586136e77adaf6c71a6e6ef1def108", "title": "Parallel GPU Implementation of Iterative PCA Algorithms", "abstract": "Principal component analysis (PCA) is a key statistical technique for multivariate data analysis. For large data sets, the common approach to PCA computation is based on the standard NIPALS-PCA algorithm, which unfortunately suffers from loss of orthogonality, and therefore its applicability is usually limited to the estimation of the first few components. Here we present an algorithm based on Gram-Schmidt orthogonalization (called GS-PCA), which eliminates this shortcoming of NIPALS-PCA. Also, we discuss the GPU (Graphics Processing Unit) parallel implementation of both NIPALS-PCA and GS-PCA algorithms. The numerical results show that the GPU parallel optimized versions, based on CUBLAS (NVIDIA), are substantially faster (up to 12 times) than the CPU optimized versions based on CBLAS (GNU Scientific Library).", "venue": "J. Comput. Biol.", "authors": ["Mircea  Andrecut"], "year": 2009, "n_citations": 108}
{"id": 682327, "s2_id": "bd9ba553a21799b868ce7448c4c0b15cddc29a1e", "title": "Implementation and Evaluation of Data-Compression Algorithms for Irregular-Grid Iterative Methods on the PEZY-SC Processor", "abstract": "Iterative methods on irregular grids have been used widely in all areas of comptational science and engineering for solving partial differential equations with complex geometry. They provide the flexibility to express complex shapes with relatively low computational cost. However, the direction of the evolution of high-performance processors in the last two decades have caused serious degradation of the computational efficiency of iterative methods on irregular grids, because of relatively low memory bandwidth. Data compression can in principle reduce the necessary memory memory bandwidth of iterative methods and thus improve the efficiency. We have implemented several data compression algorithms on the PEZY-SC processor, using the matrix generated for the HPCG benchmark as an example. For the SpMV (Sparse Matrix-Vector multiplication) part of the HPCG benchmark, the best implementation without data compression achieved 11.6Gflops/chip, close to the theoretical limit due to the memory bandwidth. Our implementation with data compression has achieved 32.4Gflops. This is of course rather extreme case, since the grid used in HPCG is geometrically regular and thus its compression efficiency is very high. However, in real applications, it is in many cases possible to make a large part of the grid to have regular geometry, in particular when the resolution is high. Note that we do not need to change the structure of the program, except for the addition of the data compression/decompression subroutines. Thus, we believe the data compression will be very useful way to improve the performance of many applications which rely on the use of irregular grids.", "venue": "2016 6th Workshop on Irregular Applications: Architecture and Algorithms (IA3)", "authors": ["Naoki  Yoshifuji", "Ryo  Sakamoto", "Keigo  Nitadori", "Jun  Makino"], "year": 2016, "n_citations": 3}
{"id": 685759, "s2_id": "bfd536bb7fec596554f95ee8d1a5a92d0c8b10ff", "title": "Automatic differentiation for Riemannian optimization on low-rank matrix and tensor-train manifolds", "abstract": "In scientific computing and machine learning applications, matrices and more general multidimensional arrays (tensors) can often be approximated with the help of low-rank decompositions. Since matrices and tensors of fixed rank form smooth Riemannian manifolds, one of the popular tools for finding low-rank approximations is to use Riemannian optimization. Nevertheless, efficient implementation of Riemannian gradients and Hessians, required in Riemannian optimization algorithms, can be a nontrivial task in practice. Moreover, in some cases, analytic formulas are not even available. In this paper, we build upon automatic differentiation and propose a method that, given an implementation of the function to be minimized, efficiently computes Riemannian gradients and matrix-by-vector products between an approximate Riemannian Hessian and a given vector.", "venue": "ArXiv", "authors": ["Alexander  Novikov", "Maxim  Rakhuba", "Ivan  Oseledets"], "year": 2021, "n_citations": 0}
{"id": 699992, "s2_id": "97f224e3db46fc8f4da275769a7979a1941e3610", "title": "A Heuristic Prover for Real Inequalities", "abstract": "We describe a general method for verifying inequalities between real-valued expressions, especially the kinds of straightforward inferences that arise in interactive theorem proving. In contrast to approaches that aim to be complete with respect to a particular language or class of formulas, our method establishes claims that require heterogeneous forms of reasoning, relying on a Nelson-Oppen-style architecture in which special-purpose modules collaborate and share information. The framework is thus modular and extensible. A prototype implementation shows that the method is promising, complementing techniques that are used by contemporary interactive provers.", "venue": "ITP", "authors": ["Jeremy  Avigad", "Robert Y. Lewis", "Cody  Roux"], "year": 2014, "n_citations": 3}
{"id": 702624, "s2_id": "c711bbd5dff800bde9eeaf095885d6eb3b7a6af8", "title": "GAPS: Generator for Automatic Polynomial Solvers", "abstract": "Minimal problems in computer vision raise the demand of generating efficient automatic solvers for polynomial equation systems. Given a polynomial system repeated with different coefficient instances, the traditional Grobner basis or normal form based solution is very inefficient. Fortunately the Grobner basis of a same polynomial system with different coefficients is found to share consistent inner structure. By precomputing such structures offline, Grobner basis as well as the polynomial system solutions can be solved automatically and efficiently online. In the past decade, several tools have been released to generate automatic solvers for a general minimal problems. The most recent tool autogen from Larsson et al. is a representative of these tools with state-of-the-art performance in solver efficiency. GAPS wraps and improves autogen with more user-friendly interface, more functionality and better stability. We demonstrate in this report the main approach and enhancement features of GAPS. A short tutorial of the software is also included.", "venue": "ArXiv", "authors": ["Bo  Li", "Viktor  Larsson"], "year": 2020, "n_citations": 3}
{"id": 703585, "s2_id": "6b63dafbcd0624bb4985a2dbc6d572997fd86151", "title": "FlexDM: Enabling robust and reliable parallel data mining using WEKA", "abstract": "Performing massive data mining experiments with multiple datasets and methods is a common task faced by most bioinformatics and computational biology laboratories. WEKA is a machine learning package designed to facilitate this task by providing tools that allow researchers to select from several classification methods and specific test strategies. Despite its popularity, the current WEKA environment for batch experiments, namely Experimenter, has four limitations that impact its usability: the selection of value ranges for methods options lacks flexibility and is not intuitive; there is no support for parallelisation when running large-scale data mining tasks; the XML schema is difficult to read, necessitating the use of the Experimenter's graphical user interface for generation and modification; and robustness is limited by the fact that results are not saved until the last test has concluded. \nFlexDM implements an interface to WEKA to run batch processing tasks in a simple and intuitive way. In a short and easy-to-understand XML file, one can define hundreds of tests to be performed on several datasets. FlexDM also allows those tests to be executed asynchronously in parallel to take advantage of multi-core processors, significantly increasing usability and productivity. Results are saved incrementally for better robustness and reliability. \nFlexDM is implemented in Java and runs on Windows, Linux and OSX. As we encourage other researchers to explore and adopt our software, FlexDM is made available as a pre-configured bootable reference environment. All code, supporting documentation and usage examples are also available for download at this http URL", "venue": "ArXiv", "authors": ["Madison  Flannery", "David  Budden", "Alexandre  Mendes"], "year": 2014, "n_citations": 0}
{"id": 704002, "s2_id": "513027eda84e42a8c43e062b4680c8ee3bd04990", "title": "Programming CUDA and OpenCL: A Case Study Using Modern C++ Libraries", "abstract": "We present a comparison of several modern C++ libraries providing high-level interfaces for programming multi- and many-core architectures on top of CUDA or OpenCL. The comparison focuses on the solution of ordinary differential equations (ODEs) and is based on odeint, a framework for the solution of systems of ODEs. Odeint is designed in a very flexible way and may be easily adapted for effective use of libraries such as MTL4, VexCL, or ViennaCL, using CUDA or OpenCL technologies. We found that CUDA and OpenCL work equally well for problems of large sizes, while OpenCL has higher overhead for smaller problems. Furthermore, we show that modern high-level libraries allow us to effectively use the computational resources of many-core GPUs or multicore CPUs without much knowledge of the underlying technologies.", "venue": "SIAM J. Sci. Comput.", "authors": ["Denis  Demidov", "Karsten  Ahnert", "Karl  Rupp", "Peter  Gottschling"], "year": 2013, "n_citations": 56}
{"id": 708582, "s2_id": "5ef50c6685154afd25e50a9c1299d62c89f9a694", "title": "Set Reduction In Nonlinear Equations", "abstract": "In this paper, an idea to solve nonlinear equations is presented. During the solution of any problem with Newton's Method, it might happen that some of the unknowns satisfy the convergence criteria where the others fail. The convergence happens only when all variables reach to the convergence limit. A method to reduce the dimension of the overall system by excluding some of the unknowns that satisfy an intermediate tolerance is introduced. In this approach, a smaller system is solved in less amount of time and already established local solutions are preserved and kept as constants while the other variables that belong to the \"set\" will be relaxed. To realize the idea, an algorithm is given that utilizes applications of pointers to reduce and evaluate the sets. Matrix-free Newton-Krylov Techniques are used on a test problem and it is shown that proposed idea improves the overall convergence.", "venue": "ArXiv", "authors": ["Erhan  Turan", "Ali  Ecder"], "year": 2012, "n_citations": 1}
{"id": 712863, "s2_id": "0ef1d8a8aa752085cf126abbfaab3127d8ba97aa", "title": "GPU-accelerating ImageJ Macro image processing workflows using CLIJ", "abstract": "This chapter introduces GPU-accelerated image processing in ImageJ/FIJI. The reader is expected to have some pre-existing knowledge of ImageJ Macro programming. Core concepts such as variables, for-loops, and functions are essential. The chapter provides basic guidelines for improved performance in typical image processing workflows. We present in a step-by-step tutorial how to translate a pre-existing ImageJ macro into a GPU-accelerated macro.", "venue": "ArXiv", "authors": ["Daniela  Vorkel", "Robert  Haase"], "year": 2020, "n_citations": 0}
{"id": 713936, "s2_id": "54b7d133c62dc8ac38110f9e2b94499d374150ac", "title": "The MIXMAX random number generator", "abstract": "Abstract In this paper, we study the randomness properties of unimodular matrix random number generators. Under well-known conditions, these discrete-time dynamical systems have the highly desirable K-mixing properties which guarantee high quality random numbers. It is found that some widely used random number generators have poor Kolmogorov entropy and consequently fail in empirical tests of randomness. These tests show that the lowest acceptable value of the Kolmogorov entropy is around 50. Next, we provide a solution to the problem of determining the maximal period of unimodular matrix generators of pseudo-random numbers. We formulate the necessary and sufficient condition to attain the maximum period and present a family of specific generators in the MIXMAX family with superior performance and excellent statistical properties. Finally, we construct three efficient algorithms for operations with the MIXMAX matrix which is a multi-dimensional generalization of the famous cat-map. First, allowing to compute the multiplication by the MIXMAX matrix with O( N ) operations. Second, to recursively compute its characteristic polynomial with O( N 2 ) operations, and third, to apply skips of large number of steps S to the sequence in O( N 2 log( S )) operations.", "venue": "Comput. Phys. Commun.", "authors": ["Konstantin G. Savvidy"], "year": 2015, "n_citations": 35}
{"id": 715197, "s2_id": "cb3f4f2248bdd753743f679bed9e22153412fb71", "title": "Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs", "abstract": "We present a novel, practical, and provable approach for solving diagonally constrained semi-definite programming (SDP) problems at scale using accelerated non-convex programming. Our algorithm non-trivially combines acceleration motions from convex optimization with coordinate power iteration and matrix factorization techniques. The algorithm is extremely simple to implement, and adds only a single extra hyperparameter \u2013 momentum. We prove that our method admits local linear convergence in the neighborhood of the optimum and always converges to a first-order critical point. Experimentally, we showcase the merits of our method on three major application domains: MaxCut, MaxSAT, and MIMO signal detection. In all cases, our methodology provides significant speedups over non-convex and convex SDP solvers \u2013 5\u00d7 faster than state-ofthe-art non-convex solvers, and 9 to 103\u00d7 faster than convex SDP solvers \u2013 with comparable or improved solution quality. CCS CONCEPTS \u2022 Theory of Computation \u2192 Design and Analysis of Algorithms; Theory and Algorithms for Application Domains; \u2022 Mathematics of Computing\u2192Mathematical Software.", "venue": "ArXiv", "authors": ["Junhyung Lyle Kim", "Jose Antonio Lara Benitez", "Mohammad Taha Toghani", "Cameron  Wolfe", "Zhiwei  Zhang", "Anastasios  Kyrillidis"], "year": 2021, "n_citations": 0}
{"id": 719392, "s2_id": "7e24368786a99a9b3d4b835b717def8be0e53ce2", "title": "HDGlab: An Open-Source Implementation of the Hybridisable Discontinuous Galerkin Method in MATLAB", "abstract": "This paper presents HDGlab , an open source MATLAB implementation of the hybridisable discontinuous Galerkin (HDG) method. The main goal is to provide a detailed description of both the HDG method for elliptic problems and its implementation available in HDGlab . Ultimately, this is expected to make this relatively new advanced discretisation method more accessible to the computational engineering community. HDGlab presents some features not available in other implementations of the HDG method that can be found in the free domain. First, it implements high-order polynomial shape functions up to degree nine, with both equally-spaced and Fekete nodal distributions. Second, it supports curved isoparametric simplicial elements in two and three dimensions. Third, it supports non-uniform degree polynomial approximations and it provides a flexible structure to devise degree adaptivity strategies. Finally, an interface with the open-source high-order mesh generator Gmsh is provided to facilitate its application to practical engineering problems.", "venue": "ArXiv", "authors": ["Matteo  Giacomini", "Ruben  Sevilla", "Antonio  Huerta"], "year": 2020, "n_citations": 5}
{"id": 727510, "s2_id": "4a9587ad4ed8f755e74524499452ea12bd6c7f3b", "title": "Optimizing Inference Performance of Transformers on CPUs", "abstract": "The Transformer architecture revolutionized the field of natural language processing (NLP). Transformers-based models (e.g., BERT) power many important Web services, such as search, translation, question-answering, etc. While enormous research attention is paid to the training of those models, relatively little efforts are made to improve their inference performance. This paper comes to address this gap by presenting an empirical analysis of scalability and performance of inferencing a Transformer-based model on CPUs. Focusing on the highly popular BERT model, we identify key components of the Transformer architecture where the bulk of the computation happens, and propose an Adaptive Linear Module Optimization (ALMO) to speed them up. The optimization is evaluated using the inference benchmark from HuggingFace, and is shown to achieve the speedup of up to x1.71. Notably, ALMO does not require any changes to the implementation of the models nor affects their accuracy.", "venue": "ArXiv", "authors": ["Dave  Dice", "Alex  Kogan"], "year": 2021, "n_citations": 0}
{"id": 727799, "s2_id": "ee41ed1793af61ecb112d59c013e2a46a8897792", "title": "Nektar++: enhancing the capability and application of high-fidelity spectral/hp element methods", "abstract": "Nektar++ is an open-source framework that provides a flexible, performant and scalable platform for the development of solvers for partial differential equations using the high-order spectral/$hp$ element method. In particular, Nektar++ aims to overcome the complex implementation challenges that are often associated with high-order methods, thereby allowing them to be more readily used in a wide range of application areas. In this paper, we present the algorithmic, implementation and application developments associated with our Nektar++ version 5.0 release. We describe some of the key software and performance developments, including our strategies on parallel I/O, on in-situ processing, the use of collective operations for exploiting current and emerging hardware, and interfaces to enable multi-solver coupling. Furthermore, we provide details on a newly developed Python interface that enable more rapid on-boarding of new users unfamiliar with spectral/$hp$ element methods, C++ and/or Nektar++. This release also incorporates a number of numerical method developments - in particular: the method of moving frames, which provides an additional approach for the simulation of equations on embedded curvilinear manifolds and domains; a means of handling spatially variable polynomial order; and a novel technique for quasi-3D simulations to permit spatially-varying perturbations to the geometry in the homogeneous direction. Finally, we demonstrate the new application-level features provided in this release, namely: a facility for generating high-order curvilinear meshes called NekMesh; a novel new AcousticSolver for aeroacoustic problems; our development of a 'thick' strip model for the modelling of fluid-structure interaction problems in the context of vortex-induced vibrations. We conclude by commenting some directions for future code development and expansion.", "venue": "Comput. Phys. Commun.", "authors": ["David  Moxey", "Chris D. Cantwell", "Yan  Bao", "Andrea  Cassinelli", "Giacomo  Castiglioni", "Sehun  Chun", "Emilia  Juda", "Ehsan  Kazemi", "Kilian  Lackhove", "Julian  Marcon", "Gianmarco  Mengaldo", "Douglas  Serson", "Michael  Turner", "Hui  Xu", "Joaquim  Peir\u00f3", "Robert Michael Kirby", "Spencer J. Sherwin"], "year": 2020, "n_citations": 39}
{"id": 728509, "s2_id": "4bf9f14a8bea339604800472e22e9fa435c40d0f", "title": "Locality optimized unstructured mesh algorithms on GPUs", "abstract": "Unstructured-mesh based numerical algorithms such as finite volume and finite element algorithms form an important class of applications for many scientific and engineering domains. The key difficulty in achieving higher performance from these applications is the indirect accesses that lead to data-races when parallelized. Current methods for handling such data-races lead to reduced parallelism and suboptimal performance. Particularly on modern many-core architectures, such as GPUs, that has increasing core/thread counts, reducing data movement and exploiting memory locality is vital for gaining good performance. \nIn this work we present novel locality-exploiting optimizations for the efficient execution of unstructured-mesh algorithms on GPUs. Building on a two-layered coloring strategy for handling data races, we introduce novel reordering and partitioning techniques to further improve efficient execution. The new optimizations are then applied to several well established unstructured-mesh applications, investigating their performance on NVIDIA's latest P100 and V100 GPUs. We demonstrate significant speedups ($1.1\\text{--}1.75\\times$) compared to the state-of-the-art. A range of performance metrics are benchmarked including runtime, memory transactions, achieved bandwidth performance, GPU occupancy and data reuse factors and are used to understand and explain the key factors impacting performance. The optimized algorithms are implemented as an open-source software library and we illustrate its use for improving performance of existing or new unstructured-mesh applications.", "venue": "J. Parallel Distributed Comput.", "authors": ["Andr\u00e1s Attila Sulyok", "G\u00e1bor D\u00e1niel Balogh", "Istvan Z. Reguly", "Gihan R. Mudalige"], "year": 2019, "n_citations": 6}
{"id": 736041, "s2_id": "993f7e6b6c61fb85c7c4ef355e140a37f7f1f766", "title": "A User-Friendly Hybrid Sparse Matrix Class in C++", "abstract": "When implementing functionality which requires sparse matrices, there are numerous storage formats to choose from, each with advantages and disadvantages. To achieve good performance, several formats may need to be used in one program, requiring explicit selection and conversion between the formats. This can be both tedious and error-prone, especially for non-expert users. Motivated by this issue, we present a user-friendly sparse matrix class for the C++ language, with a high-level application programming interface deliberately similar to the widely used MATLAB language. The class internally uses two main approaches to achieve efficient execution: (i) a hybrid storage framework, which automatically and seamlessly switches between three underlying storage formats (compressed sparse column, coordinate list, Red-Black tree) depending on which format is best suited for specific operations, and (ii) template-based meta-programming to automatically detect and optimise execution of common expression patterns. To facilitate relatively quick conversion of research code into production environments, the class and its associated functions provide a suite of essential sparse linear algebra functionality (eg., arithmetic operations, submatrix manipulation) as well as high-level functions for sparse eigendecompositions and linear equation solvers. The latter are achieved by providing easy-to-use abstractions of the low-level ARPACK and SuperLU libraries. The source code is open and provided under the permissive Apache 2.0 license, allowing unencumbered use in commercial products.", "venue": "ICMS", "authors": ["Conrad  Sanderson", "Ryan R. Curtin"], "year": 2018, "n_citations": 61}
{"id": 738794, "s2_id": "64ac6c1d6f9d2e059f7fef1356259f0a69e66266", "title": "Effect of Mixed Precision Computing on H-Matrix Vector Multiplication in BEM Analysis", "abstract": "Hierarchical Matrix (H-matrix) is an approximation technique which splits a target dense matrix into multiple submatrices, and where a selected portion of submatrices are low-rank approximated. The technique substantially reduces both time and space complexity of dense matrix vector multiplication, and hence has been applied to numerous practical problems. In this paper, we aim to accelerate the H-matrix vector multiplication by introducing mixed precision computing, where we employ both binary64 (FP64) and binary32 (FP32) arithmetic operations. We propose three methods to introduce mixed precision computing to H-matrix vector multiplication, and then evaluate them in a boundary element method (BEM) analysis. The numerical tests examine the effects of mixed precision computing, particularly on the required simulation time and rate of convergence of the iterative (BiCG-STAB) linear solver. We confirm the effectiveness of the proposed methods.", "venue": "HPC Asia", "authors": ["Rise  Ooi", "Takeshi  Iwashita", "Takeshi  Fukaya", "Akihiro  Ida", "Rio  Yokota"], "year": 2020, "n_citations": 1}
{"id": 742471, "s2_id": "261bd7a0ea230e8f5311e8445cb40934e6af0a2b", "title": "The surrogate matrix methodology: A reference implementation for low-cost assembly in isogeometric analysis", "abstract": "Graphical abstract", "venue": "MethodsX", "authors": ["Daniel  Drzisga", "Brendan  Keith", "Barbara  Wohlmuth"], "year": 2020, "n_citations": 2}
{"id": 742654, "s2_id": "7b56206262a385f63977f59b263048d8989bebf1", "title": "EURETILE D7.3 - Dynamic DAL benchmark coding, measurements on MPI version of DPSNN-STDP (distributed plastic spiking neural net) and improvements to other DAL codes", "abstract": "The EURETILE project required the selection and coding of a set of dedicated benchmarks. The project is about the software and hardware architecture of future many-tile distributed fault-tolerant systems. We focus on dynamic workloads characterised by heavy numerical processing requirements. The ambition is to identify common techniques that could be applied to both the Embedded Systems and HPC domains. This document is the first public deliverable of Work Package 7: Challenging Tiled Applications.", "venue": "ArXiv", "authors": ["Pier Stanislao Paolucci", "Iuliana  Bacivarov", "Devendra  Rai", "Lars  Schor", "Lothar  Thiele", "Hoeseok  Yang", "Elena  Pastorelli", "Roberto  Ammendola", "Andrea  Biagioni", "Ottorino  Frezza", "Francesca Lo Cicero", "Alessandro  Lonardo", "Francesco  Simula", "Laura  Tosoratto", "Piero  Vicini"], "year": 2014, "n_citations": 2}
{"id": 743033, "s2_id": "f7c09a8e5f59ac5d271d02565cfd82828a686510", "title": "Comparative computational results for some vertex and facet enumeration codes", "abstract": "We report some computational results comparing parallel and sequential codes for vertex/facet enumeration problems for convex polyhedra. The problems chosen span the range from simple to highly degenerate polytopes. We tested one code (lrs) based on pivoting and four codes (cddr+, ppl, normaliz, PORTA) based on the double description method. normaliz employs parallelization as do the codes plrs and mplrs which are based on lrs. We tested these codes using various hardware configurations with up to 1200 cores. Major speedups were obtained by parallelization, particularly by the code mplrs which uses MPI and can operate on clusters of machines.", "venue": "ArXiv", "authors": ["David  Avis", "Charles  Jordan"], "year": 2015, "n_citations": 5}
{"id": 744239, "s2_id": "e6dd9644b2f35662140e08270224f2f928f8921b", "title": "NEP: a module for the parallel solution of nonlinear eigenvalue problems in SLEPc", "abstract": "SLEPc is a parallel library for the solution of various types of large-scale eigenvalue problems. In the last years we have been developing a module within SLEPc, called NEP, that is intended for solving nonlinear eigenvalue problems. These problems can be defined by means of a matrix-valued function that depends nonlinearly on a single scalar parameter. We do not consider the particular case of polynomial eigenvalue problems (which are implemented in a different module in SLEPc) and focus here on rational eigenvalue problems and other general nonlinear eigenproblems involving square roots or any other nonlinear function. The paper discusses how the NEP module has been designed to fit the needs of applications and provides a description of the available solvers, including some implementation details such as parallelization. Several test problems coming from real applications are used to evaluate the performance and reliability of the solvers.", "venue": "ACM Trans. Math. Softw.", "authors": ["Carmen  Campos", "Jose E. Roman"], "year": 2021, "n_citations": 4}
{"id": 746278, "s2_id": "a3ae78a749a86b1335762829b3082605e998d1fa", "title": "ForestClaw: A parallel algorithm for patch-based adaptive mesh refinement on a forest of quadtrees", "abstract": "We describe a parallel, adaptive, multi-block algorithm for explicit integration of time dependent partial differential equations on two-dimensional Cartesian grids. The grid layout we consider consists of a nested hierarchy of fixed size, non-overlapping, logically Cartesian grids stored as leaves in a quadtree. Dynamic grid refinement and parallel partitioning of the grids is done through the use of the highly scalable quadtree/octree library p4est. Because our concept is multi-block, we are able to easily solve on a variety of geometries including the cubed sphere. In this paper, we pay special attention to providing details of the parallel ghost-filling algorithm needed to ensure that both corner and edge ghost regions around each grid hold valid values. \nWe have implemented this algorithm in the ForestClaw code using single-grid solvers from ClawPack, a software package for solving hyperbolic PDEs using finite volumes methods. We show weak and strong scalability results for scalar advection problems on two-dimensional manifold domains on 1 to 64Ki MPI processes, demonstrating neglible regridding overhead.", "venue": "ArXiv", "authors": ["Donna A. Calhoun", "Carsten  Burstedde"], "year": 2017, "n_citations": 8}
{"id": 748729, "s2_id": "cc278f9dae6919d58098e1011578d2aefc256ca3", "title": "COMODI: on the graphical user interface", "abstract": "We propose a series of features for the graphical user interface (GUI) of the COmputational MODule Integrator (COMODI). In view of the special requirements that COMODI type of framework for scientific computing imposes and inspiring from existing solutions that provide advanced graphical visual programming environments, we identify those elements and associated behaviors that have to find their way into the first release of COMODI.", "venue": "Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)", "authors": ["Zsolt I. L\u00e1z\u00e1r", "Andreea  Fanea", "Dragos  Petrascu", "Vladiela  Ciobotariu-Boer", "Bazil  P\u00e2rv"], "year": 2005, "n_citations": 1}
{"id": 749646, "s2_id": "098954dbf6b4e44b6040413f721f5f7acfae0c50", "title": "AVaN Pack: An Analytical/Numerical Solution for Variance-Based Sensitivity Analysis", "abstract": "Sensitivity analysis is an important concept to analyze the influences of parameters in a system, an equation or a collection of data. The methods used for sensitivity analysis are divided into deterministic and statistical techniques. Generally, deterministic techniques analyze fixed points of a model whilst stochastic techniques analyze a range of values. Deterministic methods fail in analyze the entire range of input values and stochastic methods generate outcomes with random errors. In this manuscript, we are interested in stochastic methods, mainly in variance-based techniques such as Variance and Sobol indices, since this class of techniques is largely used on literature. The objective of this manuscript is to present an analytical solution for variance based sensitive analysis. As a result of this research, two small programs were developed in Javascript named as AVaN Pack (Analysis of Variance through Numerical solution). These programs allow users to find the contribution of each individual parameter in any function by means of a mathematical solution, instead of sampling-based ones.", "venue": "ArXiv", "authors": ["Eduardo  Vasconcelos", "Adriano  Souza", "Kelvin  Dias"], "year": 2019, "n_citations": 0}
{"id": 751368, "s2_id": "603b696cd558940bf6f92405a3cd47e6e4714a8e", "title": "Mace4 Reference Manual and Guide", "abstract": "Mace4 is a program that searches for finite models of first-order formulas. For a given domain size, all instances of the formulas over the domain are constructed. The result is a set of ground clauses with equality. Then, a decision procedure based on ground equational rewriting is applied. If satisfiability is detected, one or more models are printed. Mace4 is a useful complement to first-order theorem provers, with the prover searching for proofs and Mace4 looking for countermodels, and it is useful for work on finite algebras. Mace4 performs better on equational problems than did our previous model-searching program Mace2.", "venue": "ArXiv", "authors": ["William  McCune"], "year": 2003, "n_citations": 133}
{"id": 751674, "s2_id": "9d7d4b278c7dfa3cd4b8ea4625c0873a31c68b1e", "title": "A Heterogeneous Accelerated Matrix Multiplication: OpenCL + APU + GPU+ Fast Matrix Multiply", "abstract": "As users and developers, we are witnessing the opening of a new computing scenario: the introduction of hybrid processors into a single die, such as an accelerated processing unit (APU) processor, and the plug-and-play of additional graphics processing units (GPUs) onto a single motherboard. These APU processors provide multiple symmetric cores with their memory hierarchies and an integrated GPU. Moreover, these processors are designed to work with external GPUs that can push the peak performance towards the TeraFLOPS boundary. We present a case study for the development of dense Matrix Multiplication (MM) codes for matrix sizes up to 19K 19K, thus using all of the above computational engines, and an achievable peak performance of 200 GFLOPS for, literally, a madeat-home built. We present the results of our experience, the quirks, the pitfalls, the achieved performance, and the achievable peak performance.", "venue": "ArXiv", "authors": ["Paolo  D'Alberto"], "year": 2012, "n_citations": 0}
{"id": 752874, "s2_id": "666a824f015c754f342ffde711a2de296d380f6b", "title": "Asymptotic Methods of ODEs: Exploring Singularities of the Second Kind", "abstract": "We develop symbolic methods of asymptotic approximations for solutions of linear ordinary differential equations and use to them stabilize numerical calculations. Our method follows classical analysis for first-order systems and higher-order scalar equations where growth behavior is expressed in terms of elementary functions. We then recast our equations in mollified form - thereby obtaining stability.", "venue": "ArXiv", "authors": ["Christopher J. Winfield"], "year": 2011, "n_citations": 1}
{"id": 758033, "s2_id": "ab19447a0cfcf692136f09cd91595c7c20fe351c", "title": "cuFINUFFT: a load-balanced GPU library for general-purpose nonuniform FFTs", "abstract": "Nonuniform fast Fourier transforms dominate the computational cost in many applications including image reconstruction and signal processing. We thus present a general-purpose GPU-based CUDA library for type 1 (nonuniform to uniform) and type 2 (uniform to nonuniform) transforms in dimensions 2 and 3, in single or double precision. It achieves high performance for a given user-requested accuracy, regardless of the distribution of nonuniform points, via cache-aware point reordering, and load-balanced blocked spreading in shared memory. At low accuracies, this gives on-GPU throughputs around 109 nonuniform points per second, and (even including host-device transfer) is typically 4\u201310\u00d7 faster than the latest parallel CPU code FINUFFT (at 28 threads). It is competitive with two established GPU codes, being up to 90\u00d7 faster at high accuracy and/or type 1 clustered point distributions. Finally we demonstrate a 5\u201312\u00d7 speedup versus CPU in an X-ray diffraction 3D iterative reconstruction task at 10\u221212 accuracy, observing excellent multi-GPU weak scaling up to one rank per GPU.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Yu-hsuan  Shih", "Garrett  Wright", "Joakim  and'en", "Johannes  Blaschke", "Alex H. Barnett"], "year": 2021, "n_citations": 1}
{"id": 762923, "s2_id": "eba286d98381b2a0dfdbaa0c0a97fd981befc275", "title": "Stochastic Formal Methods for Hybrid Systems", "abstract": "We provide a framework to bound the probability that accumulated errors were never above a given threshold on hybrid systems. Such systems are used for example to model an aircraft or a nuclear power plant on one side and its software on the other side. This report contains a simple formula based on Levy's and Markov's inequalities and it continues a formal theory of random variables with a special focus on producing concrete results. About a fourth of the bits of all the results of our archetype application remain continuously significant with a probability of failure of one against almost a billion, where worst case analysis considers that no significant bit remains. We are using PVS as such formal tools force explicit statement of all hypotheses and prevent incorrect uses of theorems. As our theorem contains hypotheses on the individual errors, we introduce Hoeffding's inequality and Kolmogorov-Smirnov's test to check that the hypotheses are almost certainly satisfied. The test can also be used to outline sources of errors that need to be analyzed in more details.", "venue": "NASA Formal Methods", "authors": ["Marc  Daumas", "\u00c9rik  Martin-Dorel", "David R. Lester", "Annick  Truffert"], "year": 2009, "n_citations": 2}
{"id": 765189, "s2_id": "7a2abdda652988fbaf5b8af83ad8a6ccd44bd382", "title": "A robust and scalable unfitted adaptive finite element framework for nonlinear solid mechanics", "abstract": "We extend the unfitted $h$-adaptive Finite Element Method ($h$-AgFEM) on parallel tree-based adaptive meshes, recently developed for linear scalar elliptic problems, to handle nonlinear problems in solid mechanics. Leveraging $h$-AgFEM on locally-adapted, non-conforming, tree-based meshes, and its parallel distributed-memory implementation, we can tackle large-, multi-scale problems posed on complex geometries. On top of that, in order to accurately and efficiently capture localized phenomena that frequently occur in nonlinear solid mechanics problems, we propose an algorithm to perform pseudo time-stepping in combination with $h$-adaptive dynamic mesh refinement and re-balancing driven by a-posteriori error estimators. The method is implemented considering both irreducible and mixed (u/p) formulations and thus it is able to robustly face problems involving incompressible materials. In the numerical experiments, both formulations are used to model the inelastic behavior of a wide range of compressible and incompressible materials. First, a selected set of state-of-the-art benchmarks are reproduced as a verification step. Second, a set of experiments is presented with problems involving complex geometries. Among them, we model a cantilever beam problem with spherical voids whose distribution is based on a Cube Closest Packing (CCP). This test involves a discrete domain with up to 11.7M Degrees Of Freedom (DOFs) solved in less than two hours on 3072 cores of a parallel supercomputer.", "venue": "ArXiv", "authors": ["Santiago  Badia", "Manuel  Caicedo", "Alberto F. Mart'in", "Javier  Principe"], "year": 2020, "n_citations": 1}
{"id": 766140, "s2_id": "d2e82673d660a1afbf8ac5dca95fe5764eb89296", "title": "Automatic Generation of Interpolants for Lattice Samplings: Part II - Implementation and Code Generation", "abstract": "In the prequel to this paper, we presented a systematic framework for processing spline spaces. In this paper, we take the results of that framework and provide a code generation pipeline that automatically generates efficient implementations of spline spaces. We decompose the final algorithm from Part I and translate the resulting components into LLVM-IR (a low level language that can be compiled to various targets/architectures). Our design provides a handful of parameters for a practitioner to tune \u2014 this is one of the avenues that provides us with the flexibility to target many different computational architectures and tune performance on those architectures. We also provide an evaluation of the effect of the different parameters on performance.", "venue": "ArXiv", "authors": ["Joshua  Horacsek", "Usman  Alim"], "year": 2021, "n_citations": 0}
{"id": 769717, "s2_id": "dae0386c39878463692763abb74ae9f01a65b357", "title": "An SSD-based eigensolver for spectral analysis on billion-node graphs", "abstract": "Many eigensolvers such as ARPACK and Anasazi have been developed to compute eigenvalues of a large sparse matrix. These eigensolvers are limited by the capacity of RAM. They run in memory of a single machine for smaller eigenvalue problems and require the distributed memory for larger problems. \nIn contrast, we develop an SSD-based eigensolver framework called FlashEigen, which extends Anasazi eigensolvers to SSDs, to compute eigenvalues of a graph with hundreds of millions or even billions of vertices in a single machine. FlashEigen performs sparse matrix multiplication in a semi-external memory fashion, i.e., we keep the sparse matrix on SSDs and the dense matrix in memory. We store the entire vector subspace on SSDs and reduce I/O to improve performance through caching the most recent dense matrix. Our result shows that FlashEigen is able to achieve 40%-60% performance of its in-memory implementation and has performance comparable to the Anasazi eigensolvers on a machine with 48 CPU cores. Furthermore, it is capable of scaling to a graph with 3.4 billion vertices and 129 billion edges. It takes about four hours to compute eight eigenvalues of the billion-node graph using 120 GB memory.", "venue": "ArXiv", "authors": ["Da  Zheng", "Randal C. Burns", "Joshua T. Vogelstein", "Carey E. Priebe", "Alexander S. Szalay"], "year": 2016, "n_citations": 8}
{"id": 770377, "s2_id": "f9f91023daa62ef83ba80e77419c4fddcea8fc4c", "title": "Language-based Abstractions for Dynamical Systems", "abstract": "Ordinary differential equations (ODEs) are the primary means to modelling dynamical systems in many natural and engineering sciences. The number of equations required to describe a system with high heterogeneity limits our capability of effectively performing analyses. This has motivated a large body of research, across many disciplines, into abstraction techniques that provide smaller ODE systems while preserving the original dynamics in some appropriate sense. In this paper we give an overview of a recently proposed computer-science perspective to this problem, where ODE reduction is recast to finding an appropriate equivalence relation over ODE variables, akin to classical models of computation based on labelled transition systems.", "venue": "QAPL@ETAPS", "authors": ["Andrea  Vandin"], "year": 2017, "n_citations": 1}
{"id": 775385, "s2_id": "3784b73a1f392160523400ec0309191c0a96d86f", "title": "MLlib: Machine Learning in Apache Spark", "abstract": "Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.", "venue": "J. Mach. Learn. Res.", "authors": ["Xiangrui  Meng", "Joseph K. Bradley", "Burak  Yavuz", "Evan R. Sparks", "Shivaram  Venkataraman", "Davies  Liu", "Jeremy  Freeman", "D. B. Tsai", "Manish  Amde", "Sean  Owen", "Doris  Xin", "Reynold  Xin", "Michael J. Franklin", "Reza Bosagh Zadeh", "Matei  Zaharia", "Ameet  Talwalkar"], "year": 2016, "n_citations": 1430}
{"id": 776727, "s2_id": "5b805319a85a652ca8df1a6ec317863e34bcf50a", "title": "ParaMonte: A high-performance serial/parallel Monte Carlo simulation library for C, C++, Fortran", "abstract": "ParaMonte (standing for Parallel Monte Carlo) is a serial and MPI/Coarray-parallelized library of Monte Carlo routines for sampling mathematical objective functions of arbitrary-dimensions, in particular, the posterior distributions of Bayesian models in data science, Machine Learning, and scientific inference. The ParaMonte library has been developed with the design goal of unifying the **automation**, **accessibility**, **high-performance**, **scalability**, and **reproducibility** of Monte Carlo simulations. The current implementation of the library includes **ParaDRAM**, a **Para**llel **D**elyaed-**R**ejection **A**daptive **M**etropolis Markov Chain Monte Carlo sampler, accessible from a wide range of programming languages including C, C++, Fortran, with a unified Application Programming Interface and simulation environment across all supported programming languages. The ParaMonte library is MIT-licensed and is permanently located and maintained at [this https URL](this https URL).", "venue": "J. Open Source Softw.", "authors": ["Amir  Shahmoradi", "Fatemeh  Bagheri"], "year": 2021, "n_citations": 5}
{"id": 780510, "s2_id": "031f91c437deeccc376a72368278696d69f43ee5", "title": "A C++11 implementation of arbitrary-rank tensors for high-performance computing", "abstract": "This article discusses an efficient implementation of tensors of arbitrary rank by using some of the idioms introduced by the recently published C++ ISO Standard (C++11). With the aims at providing a basic building block for high-performance computing, a single Array class template is carefully crafted, from which vectors, matrices, and even higher-order tensors can be created. An expression template facility is also built around the array class template to provide convenient mathematical syntax. As a result, by using templates, an extra high-level layer is added to the C++ language when dealing with algebraic objects and their operations, without compromising performance. The implementation is tested running on both CPU and GPU. New version program summary Program title: cpp-array Catalogue identifier: AESA_v1_1 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AESA_v1_1.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU Lesser General Public License, version 3 No. of lines in distributed program, including test data, etc.: 12 376 No. of bytes in distributed program, including test data, etc.: 81 669 Distribution format: tar.gz Programming language: C++. Computer: All modern architectures. Operating system: Linux/Unix/Mac OS. RAM: Problem dependent Classification: 5. External routines: GNU CMake build system and BIAS implementation. NVIDIA CUBLAS for GPU computing. Does the new version supersede the previous version?: Yes Catalogue identifier of previous version: AESA_v1_0 Journal reference of previous version: Comput. Phys. Comm. 185 (2014) 1681 Nature of problem: Tensors are a basic building block for any program in scientific computing. Yet, tensors are not a built-in component of the C++ programming language. Solution method: An arbitrary-rank tensor class template is crafted by using the new features introduced by the C++11 set of requirements. In addition, an entire expression template facility is built on top, to provide mathematical straightforward notation without damaging performance. Reasons for new version: The reason for this version is to make the library more portable. Summary of revisions: The first version of the library relied on the presence of a C interface to the BIAS library (CBLAS). This new version gives priority to a Fortran BIAS implementation when a Fortran compiler is provided during the configuration process. Some minor changes have also been made, including an improved Doxygen documentation, fixed installation when using CUDA, and fixed other minor bugs. Running time: Problem dependent. The tests provided take only seconds. The examples take approximately 15 min. (C) 2014 Elsevier B.V. All rights reserved.", "venue": "Comput. Phys. Commun.", "authors": ["Alejandro M. Arag\u00f3n"], "year": 2014, "n_citations": 1}
{"id": 783823, "s2_id": "74b190305ba9683cca77606725ea52c0eecac8b9", "title": "BayesOpt: A Library for Bayesian optimization with Robotics Applications", "abstract": "The purpose of this paper is twofold. On one side, we present a general framework for Bayesian optimization and we compare it with some related fields in active learning and Bayesian numerical analysis. On the other hand, Bayesian optimization and related problems (bandits, sequential experimental design) are highly dependent on the surrogate model that is selected. However, there is no clear standard in the literature. Thus, we present a fast and flexible toolbox that allows to test and combine different models and criteria with little effort. It includes most of the state-of-the-art contributions, algorithms and models. Its speed also removes part of the stigma that Bayesian optimization methods are only good for \"expensive functions\". The software is free and it can be used in many operating systems and computer languages.", "venue": "ArXiv", "authors": ["Ruben  Martinez-Cantin"], "year": 2013, "n_citations": 0}
{"id": 787358, "s2_id": "b7748e55ba9bf0001f00e87c7614a3b6c1582e36", "title": "COMPLEX-IT: A Case-Based Modeling and Scenario Simulation Platform for Social Inquiry", "abstract": "COMPLEX-IT is a case-based, mixed-methods platform for social inquiry into complex data/systems, designed to increase non-expert access to the tools of computational social science (i.e., cluster analysis, artificial intelligence, data visualization, data forecasting, and scenario simulation). In particular, COMPLEX-IT aids social inquiry though a heavy emphasis on learning about the complex data/system under study, which it does by (a) identifying and forecasting major and minor clusters/trends; (b) visualizing their complex causality; and (c) simulating scenarios for potential interventions. COMPLEX-IT is accessible through the web or can be run locally and is powered by R and the Shiny web framework.", "venue": "ArXiv", "authors": ["Corey  Schimpf", "Brian  Castellani"], "year": 2020, "n_citations": 1}
{"id": 788564, "s2_id": "0a1b97f29ac1864e020e02cae269bc54cbe19e72", "title": "Textbook efficiency: massively parallel matrix-free multigrid for the Stokes system", "abstract": "We employ textbook multigrid efficiency (TME), as introduced by Achi Brandt, to construct an asymptotically optimal monolithic multigrid solver for the Stokes system. The geometric multigrid solver builds upon the concept of hierarchical hybrid grids (HHG), which is extended to higher-order finite-element discretizations, and a corresponding matrix-free implementation. The computational cost of the full multigrid (FMG) iteration is quantified, and the solver is applied to multiple benchmark problems. Through a parameter study, we suggest configurations that achieve TME for both, stabilized equal-order, and Taylor-Hood discretizations. The excellent node-level performance of the relevant compute kernels is presented via a roofline analysis. Finally, we demonstrate the weak and strong scalability to up to $147,456$ parallel processes and solve Stokes systems with more than $3.6 \\times 10^{12}$ (trillion) unknowns.", "venue": "ArXiv", "authors": ["Nils  Kohl", "Ulrich  R\u00fcde"], "year": 2020, "n_citations": 1}
{"id": 792018, "s2_id": "131f7847a77e7baafab279a55a87dc517fa0cd51", "title": "Comments on the Reliability of Lawson and Hanson's Linear Distance Programming Algorithm: Subroutine LDP", "abstract": "This brief paper: (1) Discusses strategies to generate random test cases that can be used to extensively test any Linear Distance Program (LDP) software. (2) Gives three numerical examples of input cases generated by this strategy that cause problems in the Lawson and Hanson LDP module. (3) Proposes, as a standard matter of acceptable implementation procedures, that (unless it is done internally in the software itself, but, in general, this seems to be much rarer than one would expect) all users should test the returned output from any LDP module for self-consistency since it incurs only a small amount of added computational overhead and it is not hard to do.", "venue": "ArXiv", "authors": ["Alan  Rufty"], "year": 2007, "n_citations": 1}
{"id": 800231, "s2_id": "01c414a8b4bdc28a918af25ea521e3cef80c994c", "title": "Physical-type correctness in scientific Python", "abstract": "The representation of units and dimensions in informatics systems is barely codified and often ignored. For instance, the major languages used in scientific computing (Fortran, C and Python), have no type for dimension or unit, and so physical quantities are represented in a program by variables of type real, resulting in the possibility of unit or dimensional errors. In view of this danger, many authors have proposed language schemes for unit-checking and conversion. However, since many physical quantities have the same units, it is possible for a block of code to be unit-compatible, but still physically meaningless. We demonstrate the limitations of three Python unit-libraries and present a justification and method for checking kind-of-quantity.", "venue": "ArXiv", "authors": ["Marcus  Foster", "Sean  Tregeagle"], "year": 2018, "n_citations": 0}
{"id": 803862, "s2_id": "a8251e60c78ef2a44a0a97d1d09c2ec1061f08b2", "title": "Numerical Evaluation of Elliptic Functions, Elliptic Integrals and Modular Forms", "abstract": "We describe algorithms to compute elliptic functions and their relatives (Jacobi theta functions, modular forms, elliptic integrals, and the arithmetic-geometric mean) numerically to arbitrary precision with rigorous error bounds for arbitrary complex variables. Implementations in ball arithmetic are available in the open source Arb library. We discuss the algorithms from a concrete implementation point of view, with focus on performance at tens to thousands of digits of precision.", "venue": "Texts & Monographs in Symbolic Computation", "authors": ["Fredrik  Johansson"], "year": 2019, "n_citations": 1}
{"id": 807445, "s2_id": "a0d6bd1735e46f84b890081d36f7f4d878421429", "title": "CGAlgebra: a Mathematica package for conformal geometric algebra", "abstract": "A tutorial of the Mathematica package CGAlgebra, for conformal geometric algebra calculations is presented. Using rule-based programming, the 5-dimensional conformal geometric algebra is implemented and defined functions simplify the calculations of geometric, outer and inner products, as well as many other calculations related with geometric transformations. CGAlgebra is available from this https URL", "venue": "ArXiv", "authors": ["Jose L. Aragon"], "year": 2017, "n_citations": 0}
{"id": 809411, "s2_id": "89374f0ac7157f1d5c1ba4d278776c3ee261c35e", "title": "A Modular Extension for a Computer Algebra System", "abstract": "Abstract Computer algebra systems are complex software systems that cover a wide range of scientific and practical problems. However, the absolute coverage cannot be achieved. Often, it is required to create a user extension for an existing computer algebra system. In this case, the extensibility of the system should be taken into account. In this paper, we consider a technology for extending the SymPy computer algebra system with a low-level module that implements a random number generator.", "venue": "Programming and Computer Software", "authors": ["Migran N. Gevorkyan", "Anna V. Korolkova", "Dmitry S. Kulyabov", "Leonid A. Sevast'yanov"], "year": 2020, "n_citations": 1}
{"id": 810573, "s2_id": "c72e73bf69d42b923397afbc844a30f37f115b55", "title": "Constructing Performance Models for Dense Linear Algebra Algorithms on Cray XE Systems", "abstract": "Hiding or minimizing the communication cost is key in order to obtain good performance on large-scale systems. While communication overlapping attempts to hide communications cost, 2.5D communication avoiding algorithms improve performance scalability by reducing the volume of data transfers at the cost of extra memory usage. Both approaches can be used together or separately and the best choice depends on the machine, the algorithm and the problem size. Thus, the development of performance models is crucial to determine the best option for each scenario. In this paper, we present a methodology for constructing performance models for parallel numerical routines on Cray XE systems. Our models use portable benchmarks that measure computational cost and network characteristics, as well as performance degradation caused by simultaneous accesses to the network. We validate our methodology by constructing the performance models for the 2D and 2.5D approaches, with and without overlapping, of two matrix multiplication algorithms (Cannon's and SUMMA), triangular solve (TRSM) and Cholesky. We compare the estimations provided by these models with the experimental results using up to 24,576 cores of a Cray XE6 system and predict the performance of the algorithms on larger systems. Results prove that the estimations significantly improve when taking into account network contention.", "venue": "ArXiv", "authors": ["Jorge  Gonz\u00e1lez-Dom\u00ednguez", "Evangelos  Georganas", "Yili  Zheng", "Mar\u00eda J. Mart\u00edn"], "year": 2014, "n_citations": 0}
{"id": 815105, "s2_id": "79aa02c4a33ffffe90df6100229ae2bd8662d28b", "title": "High Accuracy Low Precision QR Factorization and Least Square Solver on GPU with TensorCore", "abstract": "Driven by the insatiable needs to process ever larger amount of data with more complex models, modern computer processors and accelerators are beginning to offer half precision floating point arithmetic support, and extremely optimized special units such as NVIDIA TensorCore on GPU and Google Tensor Processing Unit (TPU) that does half precision matrix-matrix multiplication exceptionally efficiently. In this paper we present a large scale mixed precision linear least square solver that achieves high accuracy using the low precision TensorCore GPU. The mixed precision system consists of both innovative algorithms and implementations, and is shown to be up to 14x faster than single precision cuSOLVER at QR matrix factorization at large scale with slightly lower accuracy, and up to 10x faster than double precision direct QR least square solver with comparable accuracy.", "venue": "ArXiv", "authors": ["Shaoshuai  Zhang", "Panruo  Wu"], "year": 2019, "n_citations": 1}
{"id": 818859, "s2_id": "523a62108fe3ff721fc3eed506c41e09cbbe93dc", "title": "Fast solving of weighted pairing least-squares systems", "abstract": "This paper presents a generalization of the ''weighted least-squares'' (WLS), named ''weighted pairing least-squares'' (WPLS), which uses a rectangular weight matrix and is suitable for data alignment problems. Two fast solving methods, suitable for solving full rank systems as well as rank deficient systems, are studied. Computational experiments clearly show that the best method, in terms of speed, accuracy, and numerical stability, is based on a special {1, 2, 3}-inverse, whose computation reduces to a very simple generalization of the usual ''Cholesky factorization-backward substitution'' method for solving linear systems.", "venue": "J. Comput. Appl. Math.", "authors": ["Pierre  Courrieu"], "year": 2009, "n_citations": 10}
{"id": 821918, "s2_id": "b30e8c414f1585d20f853e7e39c5f5133d80b3d8", "title": "BOML: A Modularized Bilevel Optimization Library in Python for Meta Learning", "abstract": "Meta-learning (a.k.a. learning to learn) has recently emerged as a promising paradigm for a variety of applications. There are now many meta-learning methods, each focusing on different modeling aspects of base and meta learners, but all can be (re)formulated as specific bilevel optimization problems. This work presents BOML, a modularized optimization library that unifies several meta-learning algorithms into a common bilevel optimization framework. It provides a hierarchical optimization pipeline together with a variety of iteration modules, which can be used to solve the mainstream categories of meta-learning methods, such as meta-feature-based and meta-initialization-based formulations. The library is written in Python and is available at this https URL.", "venue": "2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)", "authors": ["Yaohua  Liu", "Risheng  Liu"], "year": 2021, "n_citations": 2}
{"id": 829465, "s2_id": "5bc3f2713de3309a0172ea3ca7e60ca9bb266d89", "title": "A Parallel Structured Divide-and-Conquer Algorithm for Symmetric Tridiagonal Eigenvalue Problems", "abstract": "In this article, a parallel structured divide-and-conquer (PSDC) eigensolver is proposed for symmetric tridiagonal matrices based on ScaLAPACK and a parallel structured matrix multiplication algorithm, called PSMMA. Computing the eigenvectors via matrix-matrix multiplications is the most computationally expensive part of the divide-and-conquer algorithm, and one of the matrices involved in such multiplications is a rank-structured Cauchy-like matrix. By exploiting this particular property, PSMMA constructs the local matrices by using generators of Cauchy-like matrices without any communication, and further reduces the computation costs by using a structured low-rank approximation algorithm. Thus, both the communication and computation costs are reduced. Experimental results show that both PSMMA and PSDC are highly scalable and scale to 4096 processes at least. PSDC has better scalability than PHDC that was proposed in [16] and only scaled to 300 processes for the same matrices. Comparing with PDSTEDC in ScaLAPACK, PSDC is always faster and achieves 1.4x\u20131.6x speedup for some matrices with few deflations. PSDC is also comparable with ELPA, with PSDC being faster than ELPA when using few processes and a little slower when using many processes.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Xia  Liao", "Shengguo  Li", "Yutong  Lu", "Jose E. Roman"], "year": 2021, "n_citations": 2}
{"id": 832165, "s2_id": "7de8c96ccf390b00f0c46eedd2ffc1b8ba6deb68", "title": "Parallel breadth-first search on distributed memory systems", "abstract": "Data-intensive, graph-based computations are pervasive in several scientific applications, and are known to to be quite challenging to implement on distributed memory systems. In this work, we explore the design space of parallel algorithms for Breadth-First Search (BFS), a key subroutine in several graph algorithms. We present two highly-tuned parallel approaches for BFS on large parallel systems: a level-synchronous strategy that relies on a simple vertex-based partitioning of the graph, and a two-dimensional sparse matrix partitioning-based approach that mitigates parallel communication overhead. For both approaches, we also present hybrid versions with intra-node multithreading. Our novel hybrid two-dimensional algorithm reduces communication times by up to a factor of 3.5, relative to a common vertex based approach. Our experimental study identifies execution regimes in which these approaches will be competitive, and we demonstrate extremely high performance on leading distributed-memory parallel systems. For instance, for a 40,000-core parallel execution on Hopper, an AMD MagnyCours based system, we achieve a BFS performance rate of 17.8 billion edge visits per second on an undirected graph of 4.3 billion vertices and 68.7 billion edges with skewed degree distribution.", "venue": "2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC)", "authors": ["Aydin  Bulu\u00e7", "Kamesh  Madduri"], "year": 2011, "n_citations": 206}
{"id": 838708, "s2_id": "1faf0a33ff87c4c8f2b5dfda906b3692a14e7653", "title": "A distributed-memory hierarchical solver for general sparse linear systems", "abstract": "We present a parallel hierarchical solver for general sparse linear systems on distributed-memory machines. For large-scale problems, this fully algebraic algorithm is faster and more memory-efficient than sparse direct solvers because it exploits the low-rank structure of fill-in blocks. Depending on the accuracy of low-rank approximations, the hierarchical solver can be used either as a direct solver or as a preconditioner. The parallel algorithm is based on data decomposition and requires only local communication for updating boundary data on every processor. Moreover, the computation-to-communication ratio of the parallel algorithm is approximately the volume-to-surface-area ratio of the subdomain owned by every processor. We present various numerical results to demonstrate the versatility and scalability of the parallel algorithm.", "venue": "Parallel Comput.", "authors": ["Chao  Chen", "Hadi  Pouransari", "Sivasankaran  Rajamanickam", "Erik G. Boman", "Eric  Darve"], "year": 2018, "n_citations": 16}
{"id": 841343, "s2_id": "8567d6be36c547115dbf981d56237e306ad68658", "title": "High level implementation of geometric multigrid solvers for finite element problems: Applications in atmospheric modelling", "abstract": "The implementation of efficient multigrid preconditioners for elliptic partial differential equations (PDEs) is a challenge due to the complexity of the resulting algorithms and corresponding computer code. For sophisticated (mixed) finite element discretisations on unstructured grids an efficient implementation can be very time consuming and requires the programmer to have in-depth knowledge of the mathematical theory, parallel computing and optimisation techniques on manycore CPUs. In this paper we show how the development of bespoke multigrid preconditioners can be simplified significantly by using a framework which allows the expression of the each component of the algorithm at the correct abstraction level. Our approach (1) allows the expression of the finite element problem in a language which is close to the mathematical formulation of the problem, (2) guarantees the automatic generation and efficient execution of parallel optimised low-level computer code and (3) is flexible enough to support different abstraction levels and give the programmer control over details of the preconditioner. We use the composable abstractions of the Firedrake/PyOP2 package to demonstrate the efficiency of this approach for the solution of strongly anisotropic PDEs in atmospheric modelling. The weak formulation of the PDE is expressed in Unified Form Language (UFL) and the lower PyOP2 abstraction layer allows the manual design of computational kernels for a bespoke geometric multigrid preconditioner. We compare the performance of this preconditioner to a single-level method and hypre's BoomerAMG algorithm. The Firedrake/PyOP2 code is inherently parallel and we present a detailed performance analysis for a single node (24 cores) on the ARCHER supercomputer. Our implementation utilises a significant fraction of the available memory bandwidth and shows very good weak scaling on up to 6,144 compute cores.", "venue": "J. Comput. Phys.", "authors": ["Lawrence  Mitchell", "Eike Hermann M\u00fcller"], "year": 2016, "n_citations": 19}
{"id": 844965, "s2_id": "74778c8c0bd9aaf819350354e695b9d6b7c916f7", "title": "A Domain Decomposition Approach to Implementing Fault Slip in Finite-Element Models of Quasi-static and Dynamic Crustal Deformation", "abstract": "[1]\u00a0We employ a domain decomposition approach with Lagrange multipliers to implement fault slip in a finite-element code, PyLith, for use in both quasi-static and dynamic crustal deformation applications. This integrated approach to solving both quasi-static and dynamic simulations leverages common finite-element data structures and implementations of various boundary conditions, discretization schemes, and bulk and fault rheologies. We have developed a custom preconditioner for the Lagrange multiplier portion of the system of equations that provides excellent scalability with problem size compared to conventional additive Schwarz methods. We demonstrate application of this approach using benchmarks for both quasi-static viscoelastic deformation and dynamic spontaneous rupture propagation that verify the numerical implementation in PyLith.", "venue": "ArXiv", "authors": ["Brad T. Aagaard", "Matthew G. Knepley", "Charles A. Williams"], "year": 2013, "n_citations": 184}
{"id": 846474, "s2_id": "276ae77455e50fef739503c4a584abb8aae8939e", "title": "Knowledge-Based Automatic Generation of Partitioned Matrix Expressions", "abstract": "In a series of papers it has been shown that for many linear algebra operations it is possible to generate families of algorithms by following a systematic procedure. Although powerful, such a methodology involves complex algebraic manipulation, symbolic computations and pattern matching, making the generation a process challenging to be performed by hand. We aim for a fully automated system that from the sole description of a target operation creates multiple algorithms without any human intervention. Our approach consists of three main stages. The first stage yields the core object for the entire process, the Partitioned Matrix Expression (PME), which establishes how the target problem may be decomposed in terms of simpler sub-problems. In the second stage the PME is inspected to identify predicates, the Loop-Invariants, to be used to set up the skeleton of a family of proofs of correctness. In the third and last stage the actual algorithms are constructed so that each of them satisfies its corresponding proof of correctness. In this paper we focus on the first stage of the process, the automatic generation of Partitioned Matrix Expressions. In particular, we discuss the steps leading to a PME and the knowledge necessary for a symbolic system to perform such steps. We also introduce CLICK, a prototype system written in Mathematica that generates PMEs automatically.", "venue": "CASC", "authors": ["Diego  Fabregat-Traver", "Paolo  Bientinesi"], "year": 2011, "n_citations": 25}
{"id": 851030, "s2_id": "019c54a8549f0154ff433adb00df1aaf547adb84", "title": "A Parallel Direct Eigensolver for Sequences of Hermitian Eigenvalue Problems with No Tridiagonalization", "abstract": "In this paper, a Parallel Direct Eigensolver for Sequences of Hermitian Eigenvalue Problems with no tridiagonalization is proposed, denoted by \\texttt{PDESHEP}, and it combines direct methods with iterative methods. \\texttt{PDESHEP} first reduces a Hermitian matrix to its banded form, then applies a spectrum slicing algorithm to the banded matrix, and finally computes the eigenvectors of the original matrix via backtransform. Therefore, compared with conventional direct eigensolvers, \\texttt{PDESHEP} avoids tridiagonalization, which consists of many memory-bounded operations. In this work, the iterative method in \\texttt{PDESHEP} is based on the contour integral method implemented in FEAST. The combination of direct methods with iterative methods for banded matrices requires some efficient data redistribution algorithms both from 2D to 1D and from 1D to 2D data structures. Hence, some two-step data redistribution algorithms are proposed, which can be $10\\times$ faster than ScaLAPACK routine \\texttt{PXGEMR2D}. For the symmetric self-consistent field (SCF) eigenvalue problems, \\texttt{PDESHEP} can be on average $1.25\\times$ faster than the state-of-the-art direct solver in ELPA when using $4096$ processes. Numerical results are obtained for dense Hermitian matrices from real applications and large real sparse matrices from the SuiteSparse collection.", "venue": "ArXiv", "authors": ["Shengguo  Li", "Xinzhe  Wu", "Jose E. Roman", "Ziyang  Yuan", "Lizhi  Cheng"], "year": 2020, "n_citations": 0}
{"id": 851118, "s2_id": "9fd8f6306853eb217d6eb6f3cae44e744747481b", "title": "Abstractions and Automated Algorithms for Mixed Domain Finite Element Methods", "abstract": "Mixed dimensional partial differential equations (PDEs) are equations coupling unknown fields defined over domains of differing topological dimension. Such equations naturally arise in a wide range of scientific fields including geology, physiology, biology, and fracture mechanics. Mixed dimensional PDEs are also commonly encountered when imposing non-standard conditions over a subspace of lower dimension, e.g., through a Lagrange multiplier. In this article, we present general abstractions and algorithms for finite element discretizations of mixed domain and mixed dimensional PDEs of codimension up to one (i.e., nD-mD with |n-m| \u2264 1). We introduce high-level mathematical software abstractions together with lower-level algorithms for expressing and efficiently solving such coupled systems. The concepts introduced here have also been implemented in the context of the FEniCS finite element software. We illustrate the new features through a range of examples, including a constrained Poisson problem, a set of Stokes-type flow models, and a model for ionic electrodiffusion.", "venue": "ACM Trans. Math. Softw.", "authors": ["C'ecile  Daversin-Catty", "Chris N. Richardson", "Ada J. Ellingsrud", "Marie E. Rognes"], "year": 2021, "n_citations": 1}
{"id": 853709, "s2_id": "6aeaeb03000a7ec3b8fb21abab2f6395281140ab", "title": "The search of Type I codes", "abstract": "A self-dual binary linear code is called Type I code if it has singly-even codewords, i.e. it has codewords with weight divisible by 2. The purpose of this paper is to investigate interesting properties of Type I codes of different lengths. Further, we build up a computer-based code-searching program based on our knowledge about Type I codes. Some computation results achieved by this program are given.", "venue": "ArXiv", "authors": ["Carolin  Hannusch", "S. Roland Major"], "year": 2021, "n_citations": 0}
{"id": 860484, "s2_id": "762ad8392626fbc7c76cab91189b8a30569c9303", "title": "Array programming with NumPy", "abstract": "Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.", "venue": "Nat.", "authors": ["Charles R. Harris", "K. Jarrod Millman", "St'efan J. van der Walt", "Ralf  Gommers", "Pauli  Virtanen", "David  Cournapeau", "Eric  Wieser", "Julian  Taylor", "Sebastian  Berg", "Nathaniel J. Smith", "Robert  Kern", "Matti  Picus", "Stephan  Hoyer", "Marten H. van Kerkwijk", "Matthew  Brett", "Allan  Haldane", "Jaime Fern'andez del R'io", "Mark  Wiebe", "Pearu  Peterson", "Pierre  G'erard-Marchant", "Kevin  Sheppard", "Tyler  Reddy", "Warren  Weckesser", "Hameer  Abbasi", "Christoph  Gohlke", "Travis E. Oliphant"], "year": 2020, "n_citations": 2470}
{"id": 866485, "s2_id": "f6ee999393ceacd00fdcc8e30f42002d7eb66c82", "title": "Evaluating the Performance of NVIDIA's A100 Ampere GPU for Sparse Linear Algebra Computations", "abstract": "GPU accelerators have become an important backbone for scientific high performance computing, and the performance advances obtained from adopting new GPU hardware are significant. In this paper we take a first look at NVIDIA's newest server line GPU, the A100 architecture part of the Ampere generation. Specifically, we assess its performance for sparse linear algebra operations that form the backbone of many scientific applications and assess the performance improvements over its predecessor.", "venue": "ArXiv", "authors": ["Yuhsiang Mike Tsai", "Terry  Cojean", "Hartwig  Anzt"], "year": 2020, "n_citations": 4}
{"id": 866625, "s2_id": "80af79d7fb210f54c6049a81b30ef79f650c9ea2", "title": "Minimal Residual Methods for Complex Symmetric, Skew Symmetric, and Skew Hermitian Systems", "abstract": "While there is no lack of efficient Krylov subspace solvers for Hermitian systems, few exist for complex symmetric, skew symmetric, or skew Hermitian systems, which are increasingly important in modern applications including quantum dynamics, electromagnetics, and power systems. For a large, consistent, complex symmetric system, one may apply a non-Hermitian Krylov subspace method disregarding the symmetry of A, or a Hermitian Krylov solver on the equivalent normal equation or an augmented system twice the original dimension. These have the disadvantages of increasing memory, conditioning, or computational costs. An exception is a special version of QMR by Freund (1992), but that may be affected by nonbenign breakdowns unless look-ahead is implemented; furthermore, it is designed for only consistent and nonsingular problems. Greif and Varah (2009) adapted CG for nonsingular skew symmetric linear systems that are necessarily and restrictively of even order. We extend the symmetric and Hermitian algorithms MINRES and MINRES-QLP by Choi, Paige, and Saunders (2011) to complex symmetric, skew symmetric, and skew Hermitian systems. In particular, MINRES-QLP uses a rank-revealing QLP decomposition of the tridiagonal matrix from a three-term recurrent complex symmetric Lanczos process. Whether the systems are real or complex, singular or invertible, compatible or inconsistent, MINRES-QLP computes the unique minimumlength (i.e., pseudoinverse) solutions. It is a significant extension of MINRES by Paige and Saunders (1975) with enhanced stability and capability.", "venue": "ArXiv", "authors": ["Sou-Cheng T. Choi"], "year": 2013, "n_citations": 14}
{"id": 869393, "s2_id": "1e99676f9234be40f0e349e96dc5c4a45db9d961", "title": "A performance spectrum for parallel computational frameworks that solve PDEs", "abstract": "Important computational physics problems are often large\u2010scale in nature, and it is highly desirable to have robust and high performing computational frameworks that can quickly address these problems. However, it is no trivial task to determine whether a computational framework is performing efficiently or is scalable. The aim of this paper is to present various strategies for better understanding the performance of any parallel computational frameworks for solving PDEs. Important performance issues that negatively impact time\u2010to\u2010solution are discussed, and we propose a performance spectrum analysis that can enhance one's understanding of critical aforementioned performance issues. As proof of concept, we examine commonly used finite element simulation packages and software and apply the performance spectrum to quickly analyze the performance and scalability across various hardware platforms, software implementations, and numerical discretizations. It is shown that the proposed performance spectrum is a versatile performance model that is not only extendable to more complex PDEs such as hydrostatic ice sheet flow equations but also useful for understanding hardware performance in a massively parallel computing environment. Potential applications and future extensions of this work are also discussed.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["J.  Chang", "K. B. Nakshatrala", "Matthew G. Knepley", "S. Lennart Johnsson"], "year": 2018, "n_citations": 14}
{"id": 874170, "s2_id": "fad7e4d80e601e3e80abf06cbd06fe9cca62c26f", "title": "HOTTBOX: Higher Order Tensor ToolBOX", "abstract": "HOTTBOX is a Python library for exploratory analysis and visualisation of multi-dimensional arrays of data, also known as tensors. The library includes methods ranging from standard multi-way operations and data manipulation through to multi-linear algebra based tensor decompositions. HOTTBOX also comprises sophisticated algorithms for generalised multi-linear classification and data fusion, such as Support Tensor Machine (STM) and Tensor Ensemble Learning (TEL). For user convenience, HOTTBOX offers a unifying API which establishes a self-sufficient ecosystem for various forms of efficient representation of multi-way data and the corresponding decomposition and association algorithms. Particular emphasis is placed on scalability and interactive visualisation, to support multidisciplinary data analysis communities working on big data and tensors. HOTTBOX also provides means for integration with other popular data science libraries for visualisation and data manipulation. The source code, examples and documentation ca be found at https://github.com/hottbox/hottbox.", "venue": "ArXiv", "authors": ["Ilya  Kisil", "Giuseppe G. Calvi", "Bruno S. Dees", "Danilo P. Mandic"], "year": 2021, "n_citations": 0}
{"id": 880990, "s2_id": "5b7d5fea8e1a52fc6f02c90405ba616463f511a7", "title": "Certifying floating-point implementations using Gappa", "abstract": "High confidence in floating-point programs requires proving numerical properties of final and intermediate values. One may need to guarantee that a value stays within some range, or that the error relative to some ideal value is well bounded. Such work may require several lines of proof for each line of code, and will usually be broken by the smallest change to the code (e.g. for maintenance or optimization purpose). Certifying these programs by hand is therefore very tedious and error-prone. This article discusses the use of the Gappa proof assistant in this context. Gappa has two main advantages over previous approaches: Its input format is very close to the actual C code to validate, and it automates error evaluation and propagation using interval arithmetic. Besides, it can be used to incrementally prove complex mathematical properties pertaining to the C code. Yet it does not require any specific knowledge about automatic theorem proving, and thus is accessible to a wide community. Moreover, Gappa may generate a formal proof of the results that can be checked independently by a lower-level proof assistant like Coq, hence providing an even higher confidence in the certification of the numerical code. The article demonstrates the use of this tool on a real-size example, an elementary function with correctly rounded output.", "venue": "ArXiv", "authors": ["Florent de Dinechin", "Christoph Quirin Lauter", "Guillaume  Melquiond"], "year": 2008, "n_citations": 2}
{"id": 881100, "s2_id": "bffbc0800bd69eb3a3470ae66b893ab6e758d622", "title": "Domain-Specific Acceleration and Auto-Parallelization of Legacy Scientific Code in FORTRAN 77 using Source-to-Source Compilation", "abstract": "Massively parallel accelerators such as GPGPUs, manycores and FPGAs represent a powerful and affordable tool for scientists who look to speed up simulations of complex systems. However, porting code to such devices requires a detailed understanding of heterogeneous programming tools and effective strategies for parallelization. In this paper we present a source to source compilation approach with whole-program analysis to automatically transform single-threaded FORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized kernels. \nThe main contributions of our work are: (1) whole-source refactoring to allow any subroutine in the code to be offloaded to an accelerator. (2) Minimization of the data transfer between the host and the accelerator by eliminating redundant transfers. (3) Pragmatic auto-parallelization of the code to be offloaded to the accelerator by identification of parallelizable maps and reductions. \nWe have validated the code transformation performance of the compiler on the NIST FORTRAN 78 test suite and several real-world codes: the Large Eddy Simulator for Urban Flows, a high-resolution turbulent flow model; the shallow water component of the ocean model Gmodel; the Linear Baroclinic Model, an atmospheric climate model and Flexpart-WRF, a particle dispersion simulator. \nThe automatic parallelization component has been tested on as 2-D Shallow Water model (2DSW) and on the Large Eddy Simulator for Urban Flows (UFLES) and produces a complete OpenCL-enabled code base. The fully OpenCL-accelerated versions of the 2DSW and the UFLES are resp. 9x and 20x faster on GPU than the original code on CPU, in both cases this is the same performance as manually ported code.", "venue": "Computers & Fluids", "authors": ["Wim  Vanderbauwhede", "Gavin  Davidson"], "year": 2018, "n_citations": 6}
{"id": 882623, "s2_id": "dae455a93cd98ee814c0c4f0d3338ec7a94d7781", "title": "Accelerating Sparse Matrix-Matrix Multiplication with GPU Tensor Cores", "abstract": "Abstract Sparse general matrix\u2013matrix multiplication (spGEMM) is an essential component in many scientific and data analytics applications. However, the sparsity pattern of the input matrices and the interaction of their patterns make spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which specialize in dense matrix multiplication. Our aim is to re-purpose TCUs for sparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply sparse rectangular blocks using the mixed precision mode of TCUs. tSparse partitions the input matrices into tiles and operates only on tiles which contain one or more elements. It creates a task list of the tiles, and performs matrix multiplication of these tiles using TCUs. To the best of our knowledge, this is the first time that TCUs are used in the context of spGEMM. We show that spGEMM, with our tiling approach, benefits from TCUs. Our approach significantly improves the performance of spGEMM in comparison to cuSPARSE, CUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.", "venue": "Comput. Electr. Eng.", "authors": ["Orestis  Zachariadis", "Nitin  Satpute", "Juan  G'omez-Luna", "Joaqu'in  Olivares"], "year": 2020, "n_citations": 13}
{"id": 888113, "s2_id": "4bb90ae7b9521d3ce02e2af5b52af09db3b1c2e7", "title": "Differentiable Scripting", "abstract": "In Computational Science, Engineering and Finance (CSEF) scripts typically serve as the \u201cglue\u201d between potentially highly complex and computationally expensive external subprograms. Differentiability of the resulting programs turns out to be essential in the context of derivative-based methods for error analysis, uncertainty quantification, optimization or training of surrogates. We argue that it should be enforced by the scripting language itself through exclusive support of differentiable (smoothed) external subprograms and differentiable intrinsics combined with prohibition of nondifferentiable branches in the data flow. Illustration is provided by a prototype adjoint code compiler for a simple Python-like scripting language.", "venue": "ArXiv", "authors": ["Uwe  Naumann"], "year": 2021, "n_citations": 0}
{"id": 889060, "s2_id": "b3a7659293cbeb118351e10ffd15fdb34300532f", "title": "COMET: A Domain-Specific Compilation of High-Performance Computational Chemistry", "abstract": "The computational power increases over the past decades have greatly enhanced the ability to simulate chemical reactions and understand ever more complex transformations. Tensor contractions are the fundamental computational building block of these simulations. These simulations have often been tied to one platform and restricted in generality by the interface provided to the user. The expanding prevalence of accelerators and researcher demands necessitate a more general approach which is not tied to specific hardware or requires contortion of algorithms to specific hardware platforms. In this paper we present COMET, a domain-specific programming language and compiler infrastructure for tensor contractions targeting heterogeneous accelerators.We present a system of progressive lowering through multiple layers of abstraction and optimization that achieves up to 1.98\u00d7 speedup for 30 tensor contractions commonly used in computational chemistry and beyond.", "venue": "ArXiv", "authors": ["Erdal  Mutlu", "Ruiqin  Tian", "Bin  Ren", "Sriram  Krishnamoorthy", "Roberto  Gioiosa", "Jacques  Pienaar", "Gokcen  Kestor"], "year": 2021, "n_citations": 3}
{"id": 892847, "s2_id": "b3c8909e06feeea8ce6a6468a81e77b5045a2043", "title": "Oasis: A high-level/high-performance open source Navier-Stokes solver", "abstract": "Abstract Oasis is a high-level/high-performance finite element Navier\u2013Stokes solver written from scratch in Python using building blocks from the FEniCS project (fenicsproject.org). The solver is unstructured and targets large-scale applications in complex geometries on massively parallel clusters. Oasis utilizes MPI and interfaces, through FEniCS, to the linear algebra backend PETSc. Oasis advocates a high-level, programmable user interface through the creation of highly flexible Python modules for new problems. Through the high-level Python interface the user is placed in complete control of every aspect of the solver. A version of the solver, that is using piecewise linear elements for both velocity and pressure, is shown to reproduce very well the classical, spectral, turbulent channel simulations of Moser et\u00a0al. (1999). The computational speed is strongly dominated by the iterative solvers provided by the linear algebra backend, which is arguably the best performance any similar implicit solver using PETSc may hope for. Higher order accuracy is also demonstrated and new solvers may be easily added within the same framework. Program summary Program title: Oasis Catalogue identifier: AEUW_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEUW_v1_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: GNU Lesser GPL version 3 or any later version No. of lines in distributed program, including test data, etc.: 3491 No. of bytes in distributed program, including test data, etc.: 266924 Distribution format: tar.gz Programming language: Python/C++. Computer: Any single laptop computer or cluster. Operating system: Any (Linux, OSX, Windows). RAM: a few Megabytes to several hundred Gigabytes Classification: 12. External routines: FEniCS 1.3.0 ( www.fenicsproject.org , that in turn depends on a number of external libraries like MPI, PETSc, Epetra, Boost and ParMetis) Nature of problem: Incompressible, Newtonian fluid flow. Solution method: The finite element method. Unusual features: FEniCS automatically generates and compiles low-level C++ code based on high-level Python code. Running time: The example provided takes a couple of minutes on a single processor.", "venue": "Comput. Phys. Commun.", "authors": ["Mikael  Mortensen", "Kristian  Valen-Sendstad"], "year": 2015, "n_citations": 56}
{"id": 893798, "s2_id": "f43cdf3f0f6af960868803036a7555b59f45699f", "title": "The EPFL Logic Synthesis Libraries", "abstract": "We present a collection of modular open source C++ libraries for the development of logic synthesis applications. The alice library is a lightweight wrapper for shell interfaces, which is the typical user interface for most logic synthesis and design automation applications. It includes a Python interface to support scripting. The lorina library is a parsing library for simple file formats commonly used in logic synthesis. It includes several customizable parsing algorithms and a flexible diagnostic engine. The kitty library is a truth table library for explicit representation and manipulation of Boolean functions. It requires less overhead compared to symbolic counterparts such as binary decision diagrams, but is limited by the number of variables of the Boolean function to represent. Finally, percy is an exact synthesis library with multiple engines to find optimum logic networks. All libraries are well documented and well tested. Furthermore, being header-only, the libraries can be readily used as core components in complex logic synthesis systems.", "venue": "ArXiv", "authors": ["Mathias  Soeken", "Heinz  Riener", "Winston  Haaswijk", "Giovanni De Micheli"], "year": 2018, "n_citations": 44}
{"id": 895605, "s2_id": "6b94ce4858756f86ab2ee3f847633eed12fa9e3f", "title": "PHCpack in Macaulay2", "abstract": "The Macaulay2 package PHCpack provides an interface to PHCpack, a generalpurpose polynomial system solver that uses homotopy continuation. The main method is a numerical blackbox solver which is implemented for all Laurent systems. The package also provides a fast mixed volume computation, the ability to filter solutions, homotopy path tracking, and a numerical irreducible decomposition method. As the size of many problems in applied algebraic geometry often surpasses the capabilities of symbolic software, this package will be of interest to those working on problems involving large polynomial systems.", "venue": "ArXiv", "authors": ["Elizabeth  Gross", "Sonja  Petrovic", "Jan  Verschelde"], "year": 2011, "n_citations": 5}
{"id": 897803, "s2_id": "64b3710d18a470bdb3c51171deba5b41312d1a8d", "title": "Classifying extrema using intervals", "abstract": "We present a straightforward and verified method of deciding whether the n-dimensional point x (n>=1), such that \\nabla f(x)=0, is the local minimizer, maximizer or just a saddle point of a real-valued function f. \nThe method scales linearly with dimensionality of the problem and never produces false results.", "venue": "ArXiv", "authors": ["Marek W. Gutowski"], "year": 2006, "n_citations": 1}
{"id": 903181, "s2_id": "73990ecc54547b5d9f8e8411eb3407eca8c85dfd", "title": "TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems", "abstract": "We introduce iterative methods named TriCG and TriMR for solving symmetric quasi-definite systems based on the orthogonal tridiagonalization process proposed by Saunders, Simon and Yip in 1988. TriCG and TriMR are tantamount to preconditioned Block-Cg and Block-Minres with two right-hand sides in which the two approximate solutions are summed at each iteration, but require less storage and work per iteration. We evaluate the performance of TriCG and TriMR on linear systems generated from the SuiteSparse Matrix Collection and from discretized and stablized Stokes equations. We compare TriCG and TriMR with Symmlq and Minres, the recommended Krylov methods for symmetric and indefinite systems. In all our experiments, TriCG and TriMR terminate earlier than Symmlq and Minres on a residual-based stopping condition with an improvement of up to 50% in terms of number of iterations. They also terminate more reliably than Block-Cg and Block-Minres. Experiments in quadruple and octuple precision suggest that loss of orthogonality in the basis vectors is significantly less pronounced in TriCG and TriMR than in Block-Cg and Block-Minres.", "venue": "SIAM J. Sci. Comput.", "authors": ["Alexis  Montoison", "Dominique  Orban"], "year": 2021, "n_citations": 1}
{"id": 903592, "s2_id": "de992f60bf93b5237f48637f06796c1e4591c2d4", "title": "Quasi-structured quadrilateral meshing in Gmsh - a robust pipeline for complex CAD models", "abstract": "We propose an end-to-end pipeline to robustly generate high-quality quadrilateral meshes for complex CAD models. An initial quad-dominant mesh is generated with frontal point insertion guided by a locally integrable cross field and a scalar size map adapted to the small CAD features. After triangle combination and midpoint-subdivision into an all-quadrilateral mesh, the topology of the mesh is modified to reduce the number of irregular vertices. The idea is to preserve the irregular vertices matching cross-field singularities and to eliminate the others. The topological modifications are either local and based on disk quadrangulations, or more global with the remeshing of patches of quads according to predefined patterns. Validity of the quad mesh is guaranteed by monitoring element quality during all operations and reverting the changes when necessary. Advantages of our approach include robustness, strict respect of the CAD features and support for user-prescribed size constraints. The quad mesher, which is available in Gmsh, is validated and illustrated on two datasets of CAD models.", "venue": "ArXiv", "authors": ["Maxence  Reberol", "Christos  Georgiadis", "Jean-Franccois  Remacle"], "year": 2021, "n_citations": 2}
{"id": 903805, "s2_id": "2728589afc9fa8bec45446a6b4d91189e07237f1", "title": "HomotopyContinuation.jl: A Package for Homotopy Continuation in Julia", "abstract": "We present the Julia package HomotopyContinuation.jl, which provides an algorithmic framework for solving polynomial systems by numerical homotopy continuation. We introduce the basic capabilities of the package and demonstrate the software on an illustrative example. We motivate our choice of Julia and how its features allow us to improve upon existing software packages with respect to usability, modularity and performance. Furthermore, we compare the performance of HomotopyContinuation.jl to the existing packages Bertini and PHCpack.", "venue": "ICMS", "authors": ["Paul  Breiding", "Sascha  Timme"], "year": 2018, "n_citations": 79}
{"id": 909713, "s2_id": "db778808f770c40cfe98a5bce05c102b70c5ef49", "title": "Node aware sparse matrix-vector multiplication", "abstract": "Abstract The sparse matrix\u2013vector multiply (SpMV) operation is a key computational kernel in many simulations and linear solvers. The large communication requirements associated with a reference implementation of a parallel SpMV result in poor parallel scalability. The cost of communication depends on the physical locations of the send and receive processes: messages injected into the network are more costly than messages sent between processes on the same node. In this paper, a node aware parallel SpMV (NAPSpMV) is introduced to exploit knowledge of the system topology, specifically the node-processor layout, to reduce costs associated with communication. The values of the input vector are redistributed to minimize both the number and the size of messages that are injected into the network during a SpMV, leading to a reduction in communication costs. A variety of computational experiments that highlight the efficiency of this approach are presented.", "venue": "J. Parallel Distributed Comput.", "authors": ["Amanda  Bienz", "William D. Gropp", "Luke N. Olson"], "year": 2019, "n_citations": 10}
{"id": 914589, "s2_id": "64a9e6490a48106872a0b305c32d1eeb72d47756", "title": "The Polylogarithm Function in Julia", "abstract": "The polylogarithm function is one of the constellation of important mathematical functions. It has a long history, and many connections to other special functions and series, and many applications, for instance in statistical physics. However, the practical aspects of its numerical evaluation have not received the type of comprehensive treatments lavished on its siblings. Only a handful of formal publications consider the evaluation of the function, and most focus on a specific domain and/or presume arbitrary precision arithmetic will be used. And very little of the literature contains any formal validation of numerical performance. In this paper we present an algorithm for calculating polylogarithms for both complex parameter and argument and evaluate it thoroughly in comparison to the arbitrary precision implementation in Mathematica. The implementation was created in a new scientific computing language Julia, which is ideal for the purpose, but also allows us to write the code in a simple, natural manner so as to make it easy to port the implementation to other such languages.", "venue": "ArXiv", "authors": ["Matthew  Roughan"], "year": 2020, "n_citations": 1}
{"id": 914689, "s2_id": "0be3692bd096e71bbf8d62e4afdb364e02b8f065", "title": "Tinker-HP : Accelerating Molecular Dynamics Simulations of Large Complex Systems with Advanced Point Dipole Polarizable Force Fields using GPUs and Multi-GPUs systems", "abstract": "We present the extension of the Tinker-HP package (Lagard\\`ere et al., Chem. Sci., 2018,9, 956-972) to the use of Graphics Processing Unit (GPU) cards to accelerate molecular dynamics simulations using polarizable many-body force fields. The new high-performance module allows for an efficient use of single- and multi-GPU architectures ranging from research laboratories to modern pre-exascale supercomputer centers. After detailing an analysis of our general scalable strategy that relies on OpenACC and CUDA, we discuss the various capabilities of the package. Among them, the multi-precision possibilities of the code are discussed. If an efficient double precision implementation is provided to preserve the possibility of fast reference computations, we show that a lower precision arithmetic is preferred providing a similar accuracy for molecular dynamics while exhibiting superior performances. As Tinker-HP is mainly dedicated to accelerate simulations using new generation point dipole polarizable force field, we focus our study on the implementation of the AMOEBA model and provide illustrative benchmarks of the code for single- and multi-cards simulations on large biosystems encompassing up to millions of atoms.The new code strongly reduces time to solution and offers the best performances ever obtained using the AMOEBA polarizable force field. Perspectives toward the strong-scaling performance of our multi-node massive parallelization strategy, unsupervised adaptive sampling and large scale applicability of the Tinker-HP code in biophysics are discussed. The present software has been released in phase advance on GitHub in link with the High Performance Computing community COVID-19 research efforts and is free for Academics (see https://github.com/TinkerTools/tinker-hp).", "venue": "ArXiv", "authors": ["Olivier  Adjoua", "Louis  Lagardere", "Luc-Henri  Jolly", "Arnaud  Durocher", "Thibaut  Very", "Isabelle  Dupays", "Zhi  Wang", "Th'eo Jaffrelot Inizan", "Fr'ed'eric  C'elerse", "Pengyu  Ren", "Jay W. Ponder", "Jean-Philip  Piquemal"], "year": 2020, "n_citations": 2}
{"id": 915016, "s2_id": "8781ed9d762091b9d1fb3397afb130cfe67207dd", "title": "Efficient differentiable programming in a functional array-processing language", "abstract": "We present a system for the automatic differentiation (AD) of a higher-order functional array-processing language. The core functional language underlying this system simultaneously supports both source-to-source forward-mode AD and global optimisations such as loop transformations. In combination, gradient computation with forward-mode AD can be as efficient as reverse mode, and that the Jacobian matrices required for numerical algorithms such as Gauss-Newton and Levenberg-Marquardt can be efficiently computed.", "venue": "Proc. ACM Program. Lang.", "authors": ["Amir  Shaikhha", "Andrew  Fitzgibbon", "Dimitrios  Vytiniotis", "Simon L. Peyton Jones", "Christoph  Koch"], "year": 2019, "n_citations": 28}
{"id": 927956, "s2_id": "d353737e5774d156d428c2d147016c5a27d690a1", "title": "Generating and using truly random quantum states in Mathematica", "abstract": "Abstract The problem of generating random quantum states is of a great interest from the quantum information theory point of view. In this paper we present a package for Mathematica computing system harnessing a specific piece of hardware, namely Quantis quantum random number generator (QRNG), for investigating statistical properties of quantum states. The described package implements a number of functions for generating random states, which use Quantis QRNG as a source of randomness. It also provides procedures which can be used in simulations not related directly to quantum information processing. Program summary Program title: TRQS Catalogue identifier: AEKA_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEKA_v1_0.html Program obtainable from: CPC Program Library, Queen\u02bcs University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 7924 No. of bytes in distributed program, including test data, etc.: 88\u2009651 Distribution format: tar.gz Programming language: Mathematica, C Computer: Requires a Quantis quantum random number generator (QRNG, http://www.idquantique.com/true-random-number-generator/products-overview.html ) and supporting a recent version of Mathematica Operating system: Any platform supporting Mathematica; tested with GNU/Linux (32 and 64 bit) RAM: Case dependent Classification: 4.15 Nature of problem: Generation of random density matrices. Solution method: Use of a physical quantum random number generator. Running time: Generating 100 random numbers takes about 1 second, generating 1000 random density matrices takes more than a minute.", "venue": "Comput. Phys. Commun.", "authors": ["Jaroslaw Adam Miszczak"], "year": 2012, "n_citations": 25}
{"id": 929595, "s2_id": "c84b11611ec9c1ac2ddd648faf79ea2759936eab", "title": "Computing all Affine Solution Sets of Binomial Systems", "abstract": "To compute solutions of sparse polynomial systems efficiently we have to exploit the structure of their Newton polytopes. While the application of polyhedral methods naturally excludes solutions with zero components, an irreducible decomposition of a variety is typically understood in affine space, including also those components with zero coordinates. For the problem of computing solution sets in the intersection of some coordinate planes, the direct application of a polyhedral method fails, because the original facial structure of the Newton polytopes may alter completely when selected variables become zero. Our new proposed method enumerates all factors contributing to a generalized permanent and toric solutions as a special case of this enumeration. For benchmark problems such as the adjacent 2-by-2 minors of a general matrix, our methods scale much better than the witness set representations of numerical algebraic geometry.", "venue": "ArXiv", "authors": ["Danko  Adrovic", "Jan  Verschelde"], "year": 2014, "n_citations": 0}
{"id": 939351, "s2_id": "fe4932d7a6f48aadebbaeb167022552c3d15ceb4", "title": "Giotto-ph: A Python Library for High-Performance Computation of Persistent Homology of Vietoris-Rips Filtrations", "abstract": "We introduce giotto-ph, a high-performance, open-source software package for the computation of Vietoris\u2013Rips barcodes. giotto-ph is based on Morozov and Nigmetov\u2019s lockfree (multicore) implementation of Ulrich Bauer\u2019s Ripser package. It also contains a re-working of the GUDHI library\u2019s implementation of Boissonnat and Pritam\u2019s Edge Collapser, which can be used as a pre-processing step to dramatically reduce overall runtimes in certain scenarios. Our contribution is twofold: on the one hand, we integrate existing state-of-the-art ideas coherently in a single library and provide Python bindings to the C++ code. On the other hand, we increase parallelization opportunities and improve overall performance by adopting more efficient data structures. Our persistent homology backend establishes a new state of the art, surpassing even GPU-accelerated implementations such as Ripser++ when using as few as 5\u201310 CPU cores. Furthermore, our implementation of Edge Collapser has fewer software dependencies and improved run-times relative to GUDHI \u2019s original implementation.", "venue": "ArXiv", "authors": ["Juli'an Burella P'erez", "Sydney  Hauke", "Umberto  Lupo", "Matteo  Caorsi", "Alberto  Dassatti"], "year": 2021, "n_citations": 3}
{"id": 945531, "s2_id": "029c8c1557d81869e86ecd91f156337be0b045c1", "title": "SPIKY: a graphical user interface for monitoring spike train synchrony", "abstract": "With the growing availability of multi-unit recordings there is increasing demand for methods which provide the possibility to study similarity patterns of activity across many neurons. Accordingly, a wide variety of approaches to quantify the similarity (or dissimilarity) between two or more spike trains has been suggested. Recently, the ISI- and the SPIKE-distance [1,2] have been proposed as parameter-free and time-scale independent measures of spike train synchrony. The key property of both measures is that they are time-resolved since they rely on instantaneous estimates of spike train dissimilarity. This makes it possible to track changes in instantaneous clustering, i.e., time-localized patterns of (dis)similarity among multiple spike trains. The SPIKE-distance also comes in a causal variant [2] which is defined such that the instantaneous values of dissimilarity are defined from past information only so that time-resolved spike train synchrony can be estimated in real-time. \n \nFor both the regular and the real-time SPIKE-distance, there are several levels of information reduction [3]. The starting point is the most detailed representation in which one instantaneous value is obtained for each pair of spike trains. This results in a matrix of size 'number of sampled time instants' \u00d7 'squared number of spike trains' (i.e. #(tn)N2). By selecting a pair of spike trains one obtains a bivariate dissimilarity profile whereas the selection of a time instant yields an instantaneous matrix of pairwise spike train dissimilarities which can be used to divide the spike trains into instantaneous clusters, i.e., groups of spike trains with low intra-group and high inter-group dissimilarity. Another way to reduce the information is averaging. The spatial average over spike train pairs yields a dissimilarity profile for the selected (sub)population, whereas temporal averaging leads to a bivariate distance matrix for the selected interval or the selected trigger points. Finally, application of the remaining average results in one distance value which describes the overall level of synchrony for a group of spike trains over a given time interval. \n \nThe Matlab source codes for calculating and visualizing both the ISI- and the SPIKE-distance have been made publicly available and have already been widely used in various contexts. However, the use of these codes is not very intuitive and their application requires some basic knowledge of Matlab. Thus it became desirable to provide a more user-friendly and accessible interface. Here we address this need and present the graphical user interface SPIKY [4,5]. This interactive program facilitates the application of the ISI- and the SPIKE-distance to both simulated and real data. SPIKY includes a spike train generator for testing purposes, as well as masks for selecting the analysis window and the neuronal subpopulation of interest. Once given a set of spike train data, it calculates the desired measure and allows visualization of all the different representations mentioned above (such as measure profiles and pairwise dissimilarity matrices). It even includes the possibility to generate movies which are very useful in order to track the varying patterns of (dis)similarity. Finally, we also have increased the high computation speed even further by transferring the most time-consuming parts of the original Matlab code to Matlab executables (MEX) with the new subroutines written in C.", "venue": "BMC Neuroscience", "authors": ["Nebojsa  Bozanic", "Thomas  Kreuz"], "year": 2013, "n_citations": 24}
{"id": 945543, "s2_id": "0eac20ee2ebaede1381f20a276a0d8b6cdf73165", "title": "Julia: A Fresh Approach to Numerical Computing", "abstract": "Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be \u201claws of nature\" by practitioners of numerical computing: \\beginlist \\item High-level dynamic programs have to be slow. \\item One must prototype in one language and then rewrite in another language for speed or deployment. \\item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \\endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after dif...", "venue": "SIAM Rev.", "authors": ["Jeff  Bezanson", "Alan  Edelman", "Stefan  Karpinski", "Viral B. Shah"], "year": 2017, "n_citations": 2496}
{"id": 947319, "s2_id": "37b5cfa3651ccdb7b8b681099f882056b77e04e7", "title": "A computer algebra user interface manifesto", "abstract": "Many computer algebra systems have more than 1000 built-in functions, making expertise difficult. Using mock dialog boxes, this article describes a proposed interactive general-purpose wizard for organizing optional transformations and allowing easy fine grain control over the form of the result -- even by amateurs. This wizard integrates ideas including: flexible subexpression selection; complete control over the ordering of variables and commutative operands, with wellchosen defaults; interleaving the choice of successively less main variables with applicable function choices to provide detailed control without incurring a combinatorial number of applicable alternatives at any one level; quick applicability tests to reduce the listing of inapplicable transformations; using an organizing principle to order the alternatives in a helpful manner; labeling quickly-computed alternatives in dialog boxes with a preview of their results, using ellipsis elisions if necessary or helpful; allowing the user to retreat from a sequence of choices to explore other branches of the tree of alternatives -- or to return quickly to branches already visited; allowing the user to accumulate more than one of the alternative forms; integrating direct manipulation into the wizard; and supporting not only the usual input-result pair mode, but also the useful alternative derivational and in situ replacement modes in a unified window.", "venue": "ACCA", "authors": ["David R. Stoutemyer"], "year": 2014, "n_citations": 0}
{"id": 956629, "s2_id": "0d3f7aa1af354b24ecc5d5a243abac22f68178a6", "title": "copulaedas: An R Package for Estimation of Distribution Algorithms Based on Copulas", "abstract": "The use of copula-based models in EDAs (estimation of distribution algorithms) is currently an active area of research. In this context, the copulaedas package for R provides a platform where EDAs based on copulas can be implemented and studied. The package offers complete implementations of various EDAs based on copulas and vines, a group of well-known optimization problems, and utility functions to study the performance of the algorithms. Newly developed EDAs can be easily integrated into the package by extending an S 4 class with generic functions for their main components. This paper presents copulaedas by providing an overview of EDAs based on copulas, a description of the implementation of the package, and an illustration of its use through examples. The examples include running the EDAs defined in the package, implementing new algorithms, and performing an empirical study to compare the behavior of different algorithms on benchmark functions and a real-world problem.", "venue": "ArXiv", "authors": ["Yasser  Gonz\u00e1lez-Fern\u00e1ndez", "Marta  Soto"], "year": 2012, "n_citations": 23}
{"id": 960705, "s2_id": "449b92ae2b1c7a78c50610d4ca0312746ecf8b1a", "title": "Algorithms for computing triangular decompositions of polynomial systems", "abstract": "We propose new algorithms for computing triangular decompositions of polynomial systems incrementally. With respect to previous works, our improvements are based on a weakened notion of a polynomial GCD modulo a regular chain, which permits to greatly simplify and optimize the sub-algorithms. Extracting common work from similar expensive computations is also a key feature of our algorithms. In our experimental results the implementation of our new algorithms, realized with the RegularChains library in MAPLE, outperforms solvers with similar specifications by several orders of magnitude on sufficiently difficult problems.", "venue": "ISSAC '11", "authors": ["Changbo  Chen", "Marc Moreno Maza"], "year": 2011, "n_citations": 33}
{"id": 965097, "s2_id": "cd2b983a53640dd40a637ddfbb613f70bd78321c", "title": "A Metaprogramming and Autotuning Framework for Deploying Deep Learning Applications", "abstract": "In recent years, deep neural networks (DNNs), have yielded strong results on a wide range of applications. Graphics Processing Units (GPUs) have been one key enabling factor leading to the current popularity of DNNs. However, despite increasing hardware flexibility and software programming toolchain maturity, high efficiency GPU programming remains difficult: it suffers from high complexity, low productivity, and low portability. GPU vendors such as NVIDIA have spent enormous effort to write special-purpose DNN libraries. However, on other hardware targets, especially mobile GPUs, such vendor libraries are not generally available. Thus, the development of portable, open, high-performance, energy-efficient GPU code for DNN operations would enable broader deployment of DNN-based algorithms. Toward this end, this work presents a framework to enable productive, high-efficiency GPU programming for DNN computations across hardware platforms and programming models. In particular, the framework provides specific support for metaprogramming, autotuning, and DNN-tailored data types. Using our framework, we explore implementing DNN operations on three different hardware targets: NVIDIA, AMD, and Qualcomm GPUs. On NVIDIA GPUs, we show both portability between OpenCL and CUDA as well competitive performance compared to the vendor library. On Qualcomm GPUs, we show that our framework enables productive development of target-specific optimizations, and achieves reasonable absolute performance. Finally, On AMD GPUs, we show initial results that indicate our framework can yield reasonable performance on a new platform with minimal effort.", "venue": "ArXiv", "authors": ["Matthew W. Moskewicz", "Ali  Jannesari", "Kurt  Keutzer"], "year": 2016, "n_citations": 4}
{"id": 970445, "s2_id": "4a69cf2338296fd06ec6ac6ba85d5638056d1f7c", "title": "Doubt and Redundancy Kill Soft Errors\u2014Towards Detection and Correction of Silent Data Corruption in Task-based Numerical Software", "abstract": "Resilient algorithms in high-performance computing are subject to rigorous non-functional constraints. Resiliency must not increase the runtime, memory footprint or I/O demands too significantly. We propose a task-based soft error detection scheme that relies on error criteria per task outcome. They formalise how \u201cdubious\u201d an outcome is, i.e. how likely it contains an error. Our whole simulation is replicated once, forming two teams of MPI ranks that share their task results. Thus, ideally each team handles only around half of the workload. If a task yields large error criteria values, i.e. is dubious, we compute the task redundantly and compare the outcomes. Whenever they disagree, the task result with a lower error likeliness is accepted. We obtain a self-healing, resilient algorithm which can compensate silent floating-point errors without a significant performance, I/O or memory footprint penalty. Case studies however suggest that a careful, domain-specific tailoring of the error criteria remains essential.", "venue": "2021 IEEE/ACM 11th Workshop on Fault Tolerance for HPC at eXtreme Scale (FTXS)", "authors": ["Philipp  Samfass", "Tobias  Weinzierl", "Anne  Reinarz", "Michael  Bader"], "year": 2021, "n_citations": 0}
{"id": 974931, "s2_id": "a6a7c3cd00c7bff46dd5c9084f63389e469de943", "title": "Loo.py: from fortran to performance via transformation and substitution rules", "abstract": "A large amount of numerically-oriented code is written and is being written in legacy languages. Much of this code could, in principle, make good use of data-parallel throughput-oriented computer architectures. Loo.py, a transformation-based programming system targeted at GPUs and general data-parallel architectures, provides a mechanism for user-controlled transformation of array programs. This transformation capability is designed to not just apply to programs written specifically for Loo.py, but also those imported from other languages such as Fortran. It eases the trade-off between achieving high performance, portability, and programmability by allowing the user to apply a large and growing family of transformations to an input program. These transformations are expressed in and used from Python and may be applied from a variety of settings, including a pragma-like manner from other languages.", "venue": "ARRAY@PLDI", "authors": ["Andreas  Kl\u00f6ckner"], "year": 2015, "n_citations": 6}
{"id": 975721, "s2_id": "cf51f17e1fdd3cd43e2cc6a36dbc5831e054df6b", "title": "Performance Portability Strategies for Grid C++ Expression Templates", "abstract": "One of the key requirements for the Lattice QCD Application Development as part of the US Exascale Computing Project is performance portability across multiple architectures. Using the Grid C++ expression template as a starting point, we report on the progress made with regards to the Grid GPU offloading strategies. We present both the successes and issues encountered in using CUDA, OpenACC and Just-In-Time compilation. Experimentation and performance on GPUs with a SU(3)\u00d7SU(3) streaming test will be reported. We will also report on the challenges of using current OpenMP 4.x for GPU offloading in the same code.", "venue": "ArXiv", "authors": ["Peter A. Boyle", "Michael A. Clark", "Carleton  DeTar", "Meifeng  Lin", "Verinder S. Rana", "Alejandro Vaquero Avil\u00e9s Casco"], "year": 2017, "n_citations": 8}
{"id": 981627, "s2_id": "cf009e46410c2bd18309f2e7b15f3b549c142e4e", "title": "Sparse Matrix-vector Multiplication on GPGPU Clusters: A New Storage Format and a Scalable Implementation", "abstract": "Sparse matrix-vector multiplication (spMVM) is the dominant operation in many sparse solvers. We investigate performance properties of spMVM with matrices of various sparsity patterns on the nVidia \"Fermi\" class of GPGPUs. A new \"padded jagged diagonals storage\" (pJDS) format is proposed which may substantially reduce the memory overhead intrinsic to the widespread ELLPACK-R scheme while making no assumptions about the matrix structure. In our test scenarios the pJDS format cuts the overall spMVM memory footprint on the GPGPU by up to 70%, and achieves 91% to 130% of the ELLPACK-R performance. Using a suitable performance model we identify performance bottlenecks on the node level that invalidate some types of matrix structures for efficient multi-GPGPU parallelization. For appropriate sparsity patterns we extend previous work on distributed-memory parallel spMVM to demonstrate a scalable hybrid MPI-GPGPU code, achieving efficient overlap of communication and computation.", "venue": "2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum", "authors": ["Moritz  Kreutzer", "Georg  Hager", "Gerhard  Wellein", "Holger  Fehske", "Achim  Basermann", "Alan R. Bishop"], "year": 2012, "n_citations": 56}
{"id": 990807, "s2_id": "7f8e92be9e6113ca3a21d692832de9910d34c2d5", "title": "A Massively Parallel Algebraic Multigrid Preconditioner based on Aggregation for Elliptic Problems with Heterogeneous Coefficients", "abstract": "This paper describes a massively parallel algebraic multigrid method based on non-smoothed aggregation. It is especially suited for solving heterogeneous elliptic problems as it uses a greedy heuristic algorithm for the aggregation that detects changes in the coefficients and prevents aggregation across them. Using decoupled aggregation on each process with data agglomeration onto fewer processes on the coarse level, it weakly scales well in terms of both total time to solution and time per iteration to nearly 300,000 cores. Because of simple piecewise constant interpolation between the levels, its memory consumption is low and allows solving problems with more than 100,000,000,000 degrees of freedom.", "venue": "ArXiv", "authors": ["Markus  Blatt", "Olaf  Ippisch", "Peter  Bastian"], "year": 2012, "n_citations": 11}
{"id": 997952, "s2_id": "26df595c8381838b1a8ea6a7622473ec3dd9a5a6", "title": "GPU Fast Convolution via the Overlap-and-Save Method in Shared Memory", "abstract": "We present an implementation of the overlap-and-save method, a method for the convolution of very long signals with short response functions, which is tailored to GPUs. We have implemented several FFT algorithms (using the CUDA programming language), which exploit GPU shared memory, allowing for GPU accelerated convolution. We compare our implementation with an implementation of the overlap-and-save algorithm utilizing the NVIDIA FFT library (cuFFT). We demonstrate that by using a shared-memory-based FFT, we can achieved significant speed-ups for certain problem sizes and lower the memory requirements of the overlap-and-save method on GPUs.", "venue": "ACM Trans. Archit. Code Optim.", "authors": ["Karel  Ad\u00e1mek", "Sofia  Dimoudi", "Mike  Giles", "Wesley  Armour"], "year": 2020, "n_citations": 4}
{"id": 1008691, "s2_id": "ac462c0af3e3c59cf25905dab8a42bc06ca5c399", "title": "Construction of Single-valued Solutions for Nonintegrable Systems with the Help of the Painleve Test", "abstract": "The Painleve test is very useful to construct not only the Laurent-series solutions but also the elliptic and trigonometric ones. Such single-valued functions are solutions of some polynomial first order differential equations. To find the elliptic solutions we transform an initial nonlinear differential equation in a nonlinear algebraic system in parameters of the Laurent-series solutions of the initial equation. The number of unknowns in the obtained nonlinear system does not depend on number of arbitrary coefficients of the used first order equation. In this paper we describe the corresponding algorithm, which has been realized in REDUCE and Maple.", "venue": "ArXiv", "authors": ["S. Yu. Vernov"], "year": 2004, "n_citations": 10}
{"id": 1010913, "s2_id": "978a572f9266d43f21360e37bce18af867c1398e", "title": "Zeffiro User Interface for Electromagnetic Brain Imaging: a GPU Accelerated FEM Tool for Forward and Inverse Computations in Matlab", "abstract": "This article introduces the Zeffiro interface (ZI) version 2.2 for brain imaging. ZI aims to provide a simple, accessible and multimodal open source platform for finite element method (FEM) based and graphics processing unit (GPU) accelerated forward and inverse computations in the Matlab environment. It allows one to (1) generate a given multi-compartment head model, (2) to evaluate a lead field matrix as well as (3) to invert and analyze a given set of measurements. GPU acceleration is applied in each of the processing stages (1)\u2013(3). In its current configuration, ZI includes forward solvers for electro-/magnetoencephalography (EEG) and linearized electrical impedance tomography (EIT) as well as a set of inverse solvers based on the hierarchical Bayesian model (HBM). We report the results of EEG and EIT inversion tests performed with real and synthetic data, respectively, and demonstrate numerically how the inversion parameters affect the EEG inversion outcome in HBM. The GPU acceleration was found to be essential in the generation of the FE mesh and the LF matrix in order to achieve a reasonable computing time. The code package can be extended in the future based on the directions given in this article.", "venue": "Neuroinformatics", "authors": ["Q.  He", "A.  Rezaei", "S.  Pursiainen"], "year": 2019, "n_citations": 7}
{"id": 1012376, "s2_id": "97c4be65d4667f8a0177e2747f80193ed192c445", "title": "Validation of a PETSc based software implementing a 4DVAR Data Assimilation algorithm: a case study related with an Oceanic Model based on Shallow Water equation", "abstract": "In this work are presented and discussed some results related to the validation process of a software module based on PETSc which implements a Data Assimilation algorithm.", "venue": "ArXiv", "authors": ["Luisa  Carracciuolo", "Emil M. Constantinescu", "Luisa  D'Amore"], "year": 2018, "n_citations": 1}
{"id": 1014138, "s2_id": "c348d9cde4390be510ab6074d829554068236f1c", "title": "Big Math and the One-Brain Barrier A Position Paper and Architecture Proposal", "abstract": "Over the last decades, a class of important mathematical results have required an ever increasing amount of human effort to carry out. For some, the help of computers is now indispensable. We analyze the implications of this trend towards \"big mathematics\", its relation to human cognition, and how machine support for big math can be organized. The central contribution of this position paper is an information model for \"doing mathematics\", which posits that humans very efficiently integrate four aspects: inference, computation, tabulation, and narration around a well-organized core of mathematical knowledge. The challenge for mathematical software systems is that these four aspects need to be integrated as well. We briefly survey the state of the art.", "venue": "ArXiv", "authors": ["Jacques  Carette", "William M. Farmer", "Michael  Kohlhase", "Florian  Rabe"], "year": 2019, "n_citations": 6}
{"id": 1014395, "s2_id": "6888c89bb467179a6e65a73ff1c2447152bcec4e", "title": "Extendible and Efficient Python Framework for Solving Evolution Equations with Stabilized Discontinuous Galerkin Method", "abstract": "This paper discusses a Python interface for the recently published Dune-Fem-DG module which provides highly efficient implementations of the discontinuous Galerkin (DG) method for solving a wide range of nonlinear partial differential equations (PDEs). Although the C++ interfaces of Dune-Fem-DG are highly flexible and customizable, a solid knowledge of C++ is necessary to make use of this powerful tool. With this work, easier user interfaces based on Python and the unified form language are provided to open Dune-Fem-DG for a broader audience. The Python interfaces are demonstrated for both parabolic and first-order hyperbolic PDEs.", "venue": "Communications on Applied Mathematics and Computation", "authors": ["Andreas  Dedner", "Robert  Kl\u00f6fkorn"], "year": 2021, "n_citations": 2}
{"id": 1016389, "s2_id": "93bf437b55d8036fee41f23b6232971a801aa9bc", "title": "Unveiling patterns in xorshift128+ pseudorandom number generators", "abstract": "Xorshift128+ is a newly proposed pseudorandom number generator (PRNG), which is now the standard PRNG on a number of platforms. We demonstrate that three-dimensional plots of the random points generated by the generator have visible structures: they concentrate on particular planes in the cube. We provide a mathematical analysis of this phenomenon.", "venue": "Journal of Computational and Applied Mathematics", "authors": ["Hiroshi  Haramoto", "Makoto  Matsumoto", "Mutsuo  Saito"], "year": 2022, "n_citations": 0}
{"id": 1017801, "s2_id": "49e32aecc426081cbaa7f58ebf71d95adb9fc622", "title": "Large-Scale Discrete Fourier Transform on TPUs", "abstract": "In this work, we present two parallel algorithms for the large-scale discrete Fourier transform (DFT) on Tensor Processing Unit (TPU) clusters. The two parallel algorithms are associated with two DFT formulations: one formulation, denoted as KDFT, is based on the Kronecker product; the other is based on the famous Cooley-Tukey algorithm and phase adjustment, denoted as FFT. Both KDFT and FFT formulations take full advantage of TPU\u2019s strength in matrix multiplications. The KDFT formulation allows direct use of nonuniform inputs without additional step. In the two parallel algorithms, the same strategy of data decomposition is applied to the input data. Through the data decomposition, the dense matrix multiplications in KDFT and FFT are kept local within TPU cores, which can be performed completely in parallel. The communication among TPU cores is achieved through the one-shuffle scheme in both parallel algorithms, with which sending and receiving data takes place simultaneously between two neighboring cores and along the same direction on the interconnect network. The one-shuffle scheme is designed for the interconnect topology of TPU clusters, minimizing the time required by the communication among TPU cores. Both KDFT and FFT are implemented in TensorFlow. The three-dimensional complex DFT is performed on an example of dimension $8192 \\times 8192 \\times 8192$ with a full TPU Pod: the run time of KDFT is 12.66 seconds and that of FFT is 8.3 seconds. Scaling analysis is provided to demonstrate the high parallel efficiency of the two DFT implementations on TPUs.", "venue": "IEEE Access", "authors": ["Tianjian  Lu", "Yi-Fan  Chen", "Blake  Hechtman", "Tao  Wang", "John  Anderson"], "year": 2021, "n_citations": 10}
{"id": 1023690, "s2_id": "43826257cb0c6dbeacf94847c5298a4d12909b02", "title": "PROOFTOOL: a GUI for the GAPT Framework", "abstract": "This paper introduces PROOFTOOL, the graphical user interface for the General Architecture for Proof Theory (GAPT) framework. Its features are described with a focus not only on the visualization but also on the analysis and transformation of proofs and related tree-like structures, and its implementation is explained. Finally, PROOFTOOL is compared with three other graphical interfaces for proofs.", "venue": "UITP", "authors": ["Tsvetan  Dunchev", "Alexander  Leitsch", "Tomer  Libal", "Martin  Riener", "Mikheil  Rukhaia", "Daniel  Weller", "Bruno Woltzenlogel Paleo"], "year": 2012, "n_citations": 15}
{"id": 1024784, "s2_id": "1adc6bf7368a6bf2756a955f5f05249a0b5456cd", "title": "A highly scalable approach to solving linear systems using two-stage multisplitting", "abstract": "Iterative methods for solving large sparse systems of linear equations are widely used in many HPC applications. Extreme scaling of these methods can be difficult, however, since global communication to form dot products is typically required at every iteration. \nTo try to overcome this limitation we propose a hybrid approach, where the matrix is partitioned into blocks. Within each block, we use a highly optimised (parallel) conventional solver, but we then couple the blocks together using block Jacobi or some other multisplitting technique that can be implemented in either a synchronous or an asynchronous fashion. This allows us to limit the block size to the point where the conventional iterative methods no longer scale, and to avoid global communication (and possibly synchronisation) across all processes. \nOur block framework has been built to use PETSc, a popular scientific suite for solving sparse linear systems, as the synchronous intra-block solver, and we demonstrate results on up to 32768 cores of a Cray XE6 system. At this scale, the conventional solvers are still more efficient, though trends suggest that the hybrid approach may be beneficial at higher core counts.", "venue": "ArXiv", "authors": ["Nick  Brown", "J. Mark Bull", "Iain  Bethune"], "year": 2020, "n_citations": 0}
{"id": 1030745, "s2_id": "02085bc753a7a3e3c24dbf8c2bb71e53bcd72fdd", "title": "TRPL+K: Thick-Restart Preconditioned Lanczos+K Method for Large Symmetric Eigenvalue Problems", "abstract": "The Lanczos method is one of the standard approaches for computing a few eigenpairs of a large, sparse, symmetric matrix. It is typically used with restarting to avoid unbounded growth of memory and computational requirements. Thick-restart Lanczos is a popular restarted variant because of its simplicity and numerically robustness. However, convergence can be slow for highly clustered eigenvalues so more effective restarting techniques and the use of preconditioning is needed. In this paper, we present a thick-restart preconditioned Lanczos method, TRPL+K, that combines the power of locally optimal restarting (+K) and preconditioning techniques with the efficiency of the thick-restart Lanczos method. TRPL+K employs an inner-outer scheme where the inner loop applies Lanczos on a preconditioned operator while the outer loop augments the resulting Lanczos subspace with certain vectors from the previous restart cycle to obtain eigenvector approximations with which it thick restarts the outer subspace. We first identify the differences from various relevant methods in the literature. Then, based on an optimization perspective, we show an asymptotic global quasi-optimality of a simplified TRPL+K method compared to an unrestarted global optimal method. Finally, we present extensive experiments showing that TRPL+K either outperforms or matches other state-of-the-art eigenmethods in both matrix-vector multiplications and computational time.", "venue": "SIAM J. Sci. Comput.", "authors": ["Lingfei  Wu", "Fei  Xue", "Andreas  Stathopoulos"], "year": 2019, "n_citations": 3}
{"id": 1042984, "s2_id": "20c2c8eaea171f2b79f5938c33f89b117e5667ae", "title": "Lattice Simulations using OpenACC compilers", "abstract": "OpenACC compilers allow one to use Graphics Processing Units without having to write explicit CUDA codes. Programs can be modified incrementally using OpenMP like directives which causes the compiler to generate CUDA kernels to be run on the GPUs. In this article we look at the performance gain in lattice simulations with dynamical fermions using OpenACC compilers.", "venue": "ArXiv", "authors": ["Pushan  Majumdar"], "year": 2013, "n_citations": 6}
{"id": 1047279, "s2_id": "87e7dbba6d516190db314f02078f19493e83cfe5", "title": "MORLAB - The Model Order Reduction LABoratory", "abstract": "For an easy use of model order reduction techniques in applications, software solutions are needed. In this paper, we describe the MORLAB, Model Order Reduction LABoratory, toolbox as an efficient implementation of model reduction techniques for dense, mediumscale linear time-invariant systems. Giving an introduction to the underlying programming principles of the toolbox, we show the basic idea of spectral splitting and present an overview about implemented model reduction techniques. Two numerical examples are used to illustrate different use cases of the MORLAB toolbox.", "venue": "ArXiv", "authors": ["Peter  Benner", "Steffen W. R. Werner"], "year": 2020, "n_citations": 3}
{"id": 1055354, "s2_id": "004cc813dfe5281dc6a5efd3a4e43966a1ccf16c", "title": "A computer algebra system for the study of commutativity up-to-coherent homotopies", "abstract": "The Python package ComCH is a lightweight specialized computer algebra system that provides models for well known objects, the surjection and Barratt-Eccles operads, parameterizing the product structure of algebras that are commutative in a derived sense. The primary examples of such algebras treated by ComCH are the cochain complexes of spaces, for which it provides effective constructions of Steenrod cohomology operations at all prime.", "venue": "ArXiv", "authors": ["Anibal M. Medina-Mardones"], "year": 2021, "n_citations": 4}
{"id": 1055659, "s2_id": "b15aa62ba806cfa3aecfd8b08182226082dcc4cd", "title": "Efficient Random-Walk Methods for Approximating Polytope Volume", "abstract": "We experimentally study the fundamental problem of computing the volume of a convex polytope given as an intersection of linear inequalities. We implement and evaluate practical randomized algorithms for accurately approximating the polytope's volume in high dimensions (e.g. one hundred). To carry out this efficiently we experimentally correlate the effect of parameters, such as random walk length and number of sample points, on accuracy and runtime. Moreover, we exploit the problem's geometry by implementing an iterative rounding procedure, computing partial generations of random points and designing fast polytope boundary oracles. Our publicly available code is significantly faster than exact computation and more accurate than existing approximation methods. We provide volume approximations for the Birkhoff polytopes B11, \u2026, B15, whereas exact methods have only computed that of B10.", "venue": "SoCG", "authors": ["Ioannis Z. Emiris", "Vissarion  Fisikopoulos"], "year": 2014, "n_citations": 33}
{"id": 1057646, "s2_id": "17759929a34cd0dadd6d705e9815240c8022e7f4", "title": "A Report of a Significant Error On a Frequently Used Pseudo Random Number Generator", "abstract": "Emergence of stochastic simulations as an extensively used computational tool for scientific purposes intensified the need for more accurate ways of generating sufficiently long sequences of uncorrelated random numbers. Even though several different methods have been proposed for this end, deterministic algorithms known as pseudo-random number generators (PRNGs) emerged to be the most widely used tool as a replicable, portable and easy to use method to generate such random number sequences. Here, we introduce a simple Poisson process whose simulation gives systematic errors when the very commonly used random number generator of the GNU C Library (Glibc) is utilised. The PRNG of Glibc is an additive lagged Fibonacci generator, the family of such PRNGs are accepted as relatively safe among other PRNGs. The systematic errors indicate complex correlation relations among random numbers which requires a further explanation.", "venue": "ArXiv", "authors": ["Ayse Ferhan Yesil", "M. Cemal Yalabik"], "year": 2014, "n_citations": 1}
{"id": 1061357, "s2_id": "833ae6fa3223ebd911812542e805bf61a27b2c65", "title": "Machine Learning using Stata/Python", "abstract": "We present two related Stata modules, r ml stata and c ml stata, for fitting popular Machine Learning (ML) methods both in a regression and a classification setting. Using the recent Stata/Python integration platform (sfi) of Stata 16, these commands provide hyper-parameters\u2019 optimal tuning via K-fold cross-validation using greed search. More specifically, they make use of the Python Scikit-learn API to carry out both cross-validation and outcome/label prediction.", "venue": "ArXiv", "authors": ["Giovanni  Cerulli"], "year": 2021, "n_citations": 2}
{"id": 1062411, "s2_id": "104ace84d158d0446c2734e3c2e5bfac2ae48603", "title": "On The Evolution Of User Support Topics in Computational Science and Engineering Software", "abstract": "We investigate ten years of user support emails in the large-scale solver library PETSc in order to identify changes in user requests. For this purpose we assign each email thread to one or several categories describing the type of support request. We find that despite several changes in hardware architecture as well programming models, the relative share of emails for the individual categories does not show a notable change over time. This is particularly remarkable as the total communication volume has increased four-fold in the considered time frame, indicating a considerable growth of the user base. Our data also demonstrates that user support cannot be substituted with what is often referred to as 'better documentation' and that the involvement of core developers in user support is essential.", "venue": "ArXiv", "authors": ["Karl  Rupp", "Satish  Balay", "Jed  Brown", "Matthew G. Knepley", "Lois C. McInnes", "Barry  Smith"], "year": 2015, "n_citations": 0}
{"id": 1068833, "s2_id": "7d2ee50660399fd7c5f921fc91c03365aac9d2d5", "title": "Applying dissipative dynamical systems to pseudorandom number generation: Equidistribution property and statistical independence of bits at distances up to logarithm of mesh size", "abstract": "The behavior of a family of dissipative dynamical systems representing transformations a of a two-dimensional torus is studied on a discrete lattice and compared with that of conservative hyperbolic automorphisms of the torus. Applying dissipative dynamical systems to generation of pseudorandom numbers is shown to be advantageous and equidistribution of probabilities for the sequences of bits can be achieved. A new algorithm for generating uniform pseudorandom numbers is proposed. The theory of the generator, which includes proofs of periodic properties and of statistical independence of bits at distances up to logarithm of mesh size, is presented. Extensive statistical testing using available test packages demonstrates excellent results, while the speed of the generator is comparable to other modern generators.", "venue": "ArXiv", "authors": ["Lev Yu. Barash"], "year": 2010, "n_citations": 5}
{"id": 1076625, "s2_id": "3a6e4b12b93cb9cca7ac39e5693e8cf8bd83d6e2", "title": "A Class of Parallel Tiled Linear Algebra Algorithms for Multicore Architectures", "abstract": "As multicore systems continue to gain ground in the high performance computing world, linear algebra algorithms have to be reformulated or new algorithms have to be developed in order to take advantage of the architectural features on these new processors. Fine grain parallelism becomes a major requirement and introduces the necessity of loose synchronization in the parallel execution of an operation. This paper presents algorithms for the Cholesky, LU and QR factorization where the operations can be represented as a sequence of small tasks that operate on square blocks of data. These tasks can be dynamically scheduled for execution based on the dependencies among them and on the availability of computational resources. This may result in out of order execution of tasks which will completely hide the presence of intrinsically sequential tasks in the factorization. Performance comparisons are presented with LAPACK algorithms where parallelism can only be exploited at the level of the BLAS operations and vendor implementations.", "venue": "Parallel Comput.", "authors": ["Alfredo  Buttari", "Julien  Langou", "Jakub  Kurzak", "Jack J. Dongarra"], "year": 2009, "n_citations": 522}
{"id": 1077151, "s2_id": "b7abc8529e7a3098357f85b1764a5d6bf1004b6a", "title": "Puiseux Series and Algebraic Solutions of First Order Autonomous AODEs - A MAPLE Package", "abstract": "There exist several methods for computing exact solutions of algebraic differential equations. Most of the methods, however, do not ensure existence and uniqueness of the solutions and might fail after several steps, or are restricted to linear equations. The authors have presented in previous works a method to overcome this problem for autonomous first order algebraic ordinary differential equations and formal Puiseux series solutions and algebraic solutions. In the first case, all solutions can uniquely be represented by a sufficiently large truncation and in the latter case by its minimal polynomial. The main contribution of this paper is the implementation, in a MAPLE package named FirstOrderSolve, of the algorithmic ideas presented therein. More precisely, all formal Puiseux series and algebraic solutions, including the generic and singular solutions, are computed and described uniquely. The computation strategy is to reduce the given differential equation to a simpler one by using local parametrizations and the already known degree bounds. keywords Maple, Symbolic computation, Algebraic differential equation, Formal Puiseux series solution, Algebraic solution.", "venue": "MC", "authors": ["Fran\u00e7ois  Boulier", "Jose  Cano", "Sebastian  Falkensteiner", "J. Rafael Sendra"], "year": 2020, "n_citations": 0}
{"id": 1083968, "s2_id": "2ed09ce69ec5e46e55c44e894aed20022bc97772", "title": "Reliable Generation of High-Performance Matrix Algebra", "abstract": "Scientific programmers often turn to vendor-tuned Basic Linear Algebra Subprograms (BLAS) to obtain portable high performance. However, many numerical algorithms require several BLAS calls in sequence, and those successive calls do not achieve optimal performance. The entire sequence needs to be optimized in concert. Instead of vendor-tuned BLAS, a programmer could start with source code in Fortran or C (e.g., based on the Netlib BLAS) and use a state-of-the-art optimizing compiler. However, our experiments show that optimizing compilers often attain only one-quarter of the performance of hand-optimized code. In this article, we present a domain-specific compiler for matrix kernels, the Build to Order BLAS (BTO), that reliably achieves high performance using a scalable search algorithm for choosing the best combination of loop fusion, array contraction, and multithreading for data parallelism. The BTO compiler generates code that is between 16% slower and 39% faster than hand-optimized code.", "venue": "ACM Trans. Math. Softw.", "authors": ["Thomas  Nelson", "Geoffrey  Belter", "Jeremy G. Siek", "Elizabeth R. Jessup", "Boyana  Norris"], "year": 2015, "n_citations": 12}
{"id": 1084801, "s2_id": "9bbe8cc5eb17755d653d58ce8494c7395503ae69", "title": "Efficient Dense Gaussian Elimination over the Finite Field with Two Elements", "abstract": "In this work we describe an efficient implementation of a hierarchy of algorithms for Gaussian elimination upon dense matrices over the field with two elements. We discuss both well-known and new algorithms as well as our implementations in the M4RI library, which has been adopted into Sage. The focus of our discussion is a block iterative algorithm for PLE decomposition which is inspired by the M4RI algorithm. The implementation presented in this work provides considerable performance gains in practice when compared to the previously fastest implementation. We provide performance figures on x86_64 CPUs to demonstrate the alacrity of our approach.", "venue": "ArXiv", "authors": ["Martin R. Albrecht", "Gregory V. Bard", "Cl\u00e9ment  Pernet"], "year": 2011, "n_citations": 13}
{"id": 1085874, "s2_id": "a7dfbe119a30c144e9bc72afec9be6838df6b813", "title": "FEAST Eigenvalue Solver v4.0 User Guide", "abstract": "The FEAST eigensolver package is a free high-performance numerical library for solving the Hermitian and non-Hermitian eigenvalue problems, and obtaining all the eigenvalues and (right/left) eigenvectors within a given search interval or arbitrary contour in the complex plane. Its originality lies with a new transformative numerical approach to the traditional eigenvalue algorithm design - the FEAST algorithm. The FEAST eigensolver combines simplicity and efficiency and it offers many important capabilities for achieving high performance, robustness, accuracy, and scalability on parallel architectures. FEAST is both a comprehensive library package, and an easy to use software. It includes flexible reverse communication interfaces and ready to use predefined interfaces for dense, banded and sparse systems. The current version v3.0 of the FEAST package can address both Hermitian and non-Hermitian eigenvalue problems (real symmetric, real non-symmetric, complex Hermitian, complex symmetric, or complex general systems) on both shared-memory and distributed memory architectures (i.e contains both FEAST-SMP and FEAST-MPI packages). This User's guide provides instructions for installation setup, a detailed description of the FEAST interfaces and a large number of examples.", "venue": "ArXiv", "authors": ["Eric  Polizzi"], "year": 2020, "n_citations": 11}
{"id": 1112676, "s2_id": "b2542afe8f21c513e6b2f3e8f28995f8d09856cd", "title": "Automatic Generation of Vectorized Montgomery Algorithm", "abstract": "Modular arithmetic is widely used in crytography and symbolic computation. This paper presents a vectorized Montgomery algorithm for modular multiplication, the key to fast modular arithmetic, that fully utilizes the SIMD instructions. We further show how the vectorized algorithm can be automatically generated by the {\\SPIRAL} system, as part of the effort for automatic generation of a modular polynomial multiplication library.", "venue": "ArXiv", "authors": ["Lingchuan  Meng"], "year": 2016, "n_citations": 0}
{"id": 1118418, "s2_id": "3bde8fea845dc34e6b2aeb433f7e0dc993c67916", "title": "Performance portability study of linear algebra kernels in OpenCL", "abstract": "The performance portability of OpenCL kernel implementations for common memory bandwidth limited linear algebra operations across different hardware generations of the same vendor as well as across vendors is studied. Certain combinations of kernel implementations and work sizes are found to exhibit good performance across compute kernels, hardware generations, and, to a lesser degree, vendors. As a consequence, it is demonstrated that the optimization of a single kernel is often sufficient to obtain good performance for a large class of more complicated operations.", "venue": "IWOCL '14", "authors": ["Karl  Rupp", "Philippe  Tillet", "Florian  Rudolf", "Josef  Weinbub", "Tibor  Grasser", "Ansgar  J\u00fcngel"], "year": 2014, "n_citations": 8}
{"id": 1127310, "s2_id": "d4fe8d16c484b7ce56649dadd5d8077ad2d9688f", "title": "AutoHOOT: Automatic High-Order Optimization for Tensors", "abstract": "High-order optimization methods, including Newton's method and its variants as well as alternating minimization methods, dominate the optimization algorithms for tensor decompositions and tensor networks. These tensor methods are used for data analysis and simulation of quantum systems. In this work, we introduce AutoHOOT, the first automatic differentiation (AD) framework targeting at high-order optimization for tensor computations. AutoHOOT takes input tensor computation expressions and generates optimized derivative expressions. In particular, AutoHOOT contains a new explicit Jacobian / Hessian expression generation kernel whose outputs maintain the input tensors' granularity and are easy to optimize. The expressions are then optimized by both the traditional compiler optimization techniques and specific tensor algebra transformations. Experimental results show that AutoHOOT achieves competitive CPU and GPU performance for both tensor decomposition and tensor network applications compared to existing AD software and other tensor computation libraries with manually written kernels. The tensor methods generated by AutoHOOT are also well-parallelizable, and we demonstrate good scalability on a distributed memory supercomputer.", "venue": "PACT", "authors": ["Linjian  Ma", "Jiayu  Ye", "Edgar  Solomonik"], "year": 2020, "n_citations": 6}
{"id": 1130336, "s2_id": "526166ec0ddf427584a2ed51941885abdffb7448", "title": "A dedicated greedy pursuit algorithm for sparse spectral representation of music sound.", "abstract": "A dedicated algorithm for sparse spectral representation of music sound is presented. The goal is to enable the representation of a piece of music signal as a linear superposition of as few spectral components as possible, without affecting the quality of the reproduction. A representation of this nature is said to be sparse. In the present context sparsity is accomplished by greedy selection of the spectral components, from an overcomplete set called a dictionary. The proposed algorithm is tailored to be applied with trigonometric dictionaries. Its distinctive feature being that it avoids the need for the actual construction of the whole dictionary, by implementing the required operations via the fast Fourier transform. The achieved sparsity is theoretically equivalent to that rendered by the orthogonal matching pursuit (OMP) method. The contribution of the proposed dedicated implementation is to extend the applicability of the standard OMP algorithm, by reducing its storage and computational demands. The suitability of the approach for producing sparse spectral representation is illustrated by comparison with the traditional method, in the line of the short time Fourier transform, involving only the corresponding orthonormal trigonometric basis.", "venue": "The Journal of the Acoustical Society of America", "authors": ["Laura  Rebollo-Neira", "Gagan  Aggarwal"], "year": 2016, "n_citations": 7}
{"id": 1135898, "s2_id": "a7aab5ed26fd1a30ddc595f9a0fb774cfe11e408", "title": "Improving the Performance of the GMRES Method using Mixed-Precision Techniques", "abstract": "The GMRES method is used to solve sparse, non-symmetric systems of linear equations arising from many scientific applications. The solver performance within a single node is memory bound, due to the low arithmetic intensity of its computational kernels. To reduce the amount of data movement, and thus, to improve performance, we investigated the effect of using a mix of single and double precision while retaining double-precision accuracy. Previous efforts have explored reduced precision in the preconditioner, but the use of reduced precision in the solver itself has received limited attention. We found that GMRES only needs double precision in computing the residual and updating the approximate solution to achieve double-precision accuracy, although it must restart after each improvement of single-precision accuracy. This finding holds for the tested orthogonalization schemes: Modified Gram-Schmidt (MGS) and Classical Gram-Schmidt with Re-orthogonalization (CGSR). Furthermore, our mixed-precision GMRES, when restarted at least once, performed 19% and 24% faster on average than double-precision GMRES for MGS and CGSR, respectively. Our implementation uses generic programming techniques to ease the burden of coding implementations for different data types. Our use of the Kokkos library allowed us to exploit parallelism and optimize data management. Additionally, KokkosKernels was used when producing performance results. In conclusion, using a mix of single and double precision in GMRES can improve performance while retaining double-precision accuracy.", "venue": "SMC", "authors": ["Neil  Lindquist", "Piotr  Luszczek", "Jack  Dongarra"], "year": 2020, "n_citations": 5}
{"id": 1136441, "s2_id": "6caae7f795e816b8b2982628a0b579cf5e2ea435", "title": "The aggregated unfitted finite element method on parallel tree-based adaptive meshes", "abstract": "In this work, we present an adaptive unfitted finite element scheme that combines the aggregated finite element method with parallel adaptive mesh refinement. We introduce a novel scalable distributed-memory implementation of the resulting scheme on locally-adapted Cartesian forest-of-trees meshes. We propose a two-step algorithm to construct the finite element space at hand that carefully mixes aggregation constraints of problematic degrees of freedom, which get rid of the small cut cell problem, and standard hanging degree of freedom constraints, which ensure trace continuity on non-conforming meshes. Following this approach, we derive a finite element space that can be expressed as the original one plus well-defined linear constraints. Moreover, it requires minimum parallelization effort, using standard functionality available in existing large-scale finite element codes. Numerical experiments demonstrate its optimal mesh adaptation capability, robustness to cut location and parallel efficiency, on classical Poisson $hp$-adaptivity benchmarks. Our work opens the path to functional and geometrical error-driven dynamic mesh adaptation with the aggregated finite element method in large-scale realistic scenarios. Likewise, it can offer guidance for bridging other scalable unfitted methods and parallel adaptive mesh refinement.", "venue": "SIAM J. Sci. Comput.", "authors": ["Santiago  Badia", "Alberto F. Mart'in", "Eric  Neiva", "Francesc  Verdugo"], "year": 2021, "n_citations": 5}
{"id": 1137404, "s2_id": "a141045f2548b98fb03b8418b6c4b05a50328287", "title": "Computing Derivatives for PETSc Adjoint Solvers using Algorithmic Differentiation", "abstract": "Most nonlinear partial differential equation (PDE) solvers require the Jacobian matrix associated to the differential operator. In PETSc, this is typically achieved by either an analytic derivation or numerical approximation method such as finite differences. For complex applications, hand-coding the Jacobian can be time-consuming and error-prone, yet computationally efficient. Whilst finite difference approximations are straight-forward to implement, they have high arithmetic complexity and low accuracy. Alternatively, one may compute Jacobians using algorithmic differentiation (AD), yielding the same derivatives as an analytic derivation, with the added benefit that the implementation is problem independent. In this work, the operator overloading AD tool ADOL-C is applied to generate Jacobians for time-dependent, nonlinear PDEs and their adjoints. Various strategies are considered, including compressed and matrix-free approaches. In numerical experiments with a 2D diffusion-reaction model, the performance of these strategies has been studied and compared to the hand-derived version.", "venue": "ArXiv", "authors": ["Joseph G. Wallwork", "Paul D. Hovland", "Hong  Zhang", "Oana  Marin"], "year": 2019, "n_citations": 1}
{"id": 1138941, "s2_id": "dcb309be34da244f2f94f7ab7453e727507757a3", "title": "Computations with one and two real algebraic numbers", "abstract": "We present algorithmic and complexity results concerning computations with one and two real algebraic numbers, as well as real solving of univariate polynomials and bivariate polynomial systems with integer coefficients using Sturm-Habicht sequences. \nOur main results, in the univariate case, concern the problems of real root isolation (Th. 19) and simultaneous inequalities (Cor.26) and in the bivariate, the problems of system real solving (Th.42), sign evaluation (Th. 37) and simultaneous inequalities (Cor. 43).", "venue": "ArXiv", "authors": ["Ioannis Z. Emiris", "Elias P. Tsigaridas"], "year": 2005, "n_citations": 2}
{"id": 1143868, "s2_id": "f99b3e370136f13a9e2138db31bebc8b3b69883d", "title": "BLISlab: A Sandbox for Optimizing GEMM", "abstract": "Matrix-matrix multiplication is a fundamental operation of great importance to scientific computing and, increasingly, machine learning. It is a simple enough concept to be introduced in a typical high school algebra course yet in practice important enough that its implementation on computers continues to be an active research topic. This note describes a set of exercises that use this operation to illustrate how high performance can be attained on modern CPUs with hierarchical memories (multiple caches). It does so by building on the insights that underly the BLAS-like Library Instantiation Software (BLIS) framework by exposing a simplified \"sandbox\" that mimics the implementation in BLIS. As such, it also becomes a vehicle for the \"crowd sourcing\" of the optimization of BLIS. We call this set of exercises BLISlab.", "venue": "ArXiv", "authors": ["Jianyu  Huang", "Robert A. van de Geijn"], "year": 2016, "n_citations": 5}
{"id": 1144540, "s2_id": "ec036acf2fd8aaa4d9038b702e3f7af7470f79d5", "title": "Performance of FORTRAN and C GPU Extensions for a Benchmark Suite of Fourier Pseudospectral Algorithms", "abstract": "A comparison of PGI Open ACC, FORTRAN CUDA, and Nvidia CUDA pseudospectral methods on a single GPU and GCC FORTRAN on single and multiple CPU cores is reported. The GPU implementations use CuFFT and the CPU implementations use FFTW. Porting pre-existing FORTRAN codes to utilize a GPUs is efficient and easy to implement with Open ACC and CUDA FORTRAN. Example programs are provided.", "venue": "2012 Symposium on Application Accelerators in High Performance Computing", "authors": ["Rigge  Cloutier", "Benson K. Muite", "Paul  Rigge"], "year": 2012, "n_citations": 11}
{"id": 1145867, "s2_id": "c44f28c7d01503a87f435364955e1ced8468adc1", "title": "Difference Equations in Massive Higher Order Calculations", "abstract": "The calculation of massive 2\u2010loop operator matrix elements, required for the higher order Wilson coefficients for heavy flavor production in deeply inelas tic scattering, leads to new types of multiple infinite sums over harmonic sums and related functi ons, which depend on the Mellin parameter N. We report on the solution of these sums through higher order difference equations using the summation packageSigma.", "venue": "ArXiv", "authors": ["I.  Bierenbaum", "Johannes  Bl\u00fcmlein", "Sebastian  Klein", "Carsten  Schneider"], "year": 2007, "n_citations": 13}
{"id": 1153946, "s2_id": "4d8be4b52715d7ee89a6b97e33f6a32aed8813e5", "title": "TRUST-TECH based Methods for Optimization and Learning", "abstract": "Many problems that arise in machine learning domain deal with nonlinearity and quite often demand users to obtain global optimal solutions rather than local optimal ones. Optimization problems are inherent in machine learning algorithms and hence many methods in machine learning were inherited from the optimization literature. Popularly known as the initialization problem, the ideal set of parameters required will significantly depend on the given initialization values. The recently developed TRUST-TECH (TRansformation Under STability-reTaining Equilibria CHaracterization) methodology systematically explores the subspace of the parameters to obtain a complete set of local optimal solutions. In this thesis work, we propose TRUST-TECH based methods for solving several optimization and machine learning problems. Two stages namely, the local stage and the neighborhood-search stage, are repeated alternatively in the solution space to achieve improvements in the quality of the solutions. Our methods were tested on both synthetic and real datasets and the advantages of using this novel framework are clearly manifested. This framework not only reduces the sensitivity to initialization, but also allows the flexibility for the practitioners to use various global and local methods that work well for a particular problem of interest. Other hierarchical stochastic algorithms like evolutionary algorithms and smoothing algorithms are also studied and frameworks for combining these methods with TRUST-TECH have been proposed and evaluated on several test systems.", "venue": "ArXiv", "authors": ["Chandan K. Reddy"], "year": 2007, "n_citations": 5}
{"id": 1157412, "s2_id": "dffcd98ff8f270550af02c3a1b45d63dcc4b9f0f", "title": "Building the Tangent and Adjoint codes of the Ocean General Circulation Model OPA with the Automatic Differentiation tool TAPENADE", "abstract": "The ocean general circulation model OPA is developed by the LODYC team at Paris VI university. OPA has recently undergone a major rewriting, migrating to FORTRAN95, and its adjoint code needs to be rebuilt. For earlier versions, the adjoint of OPA was written by hand at a high development cost. We use the Automatic Differentiation tool TAPENADE to build mechanicaly the tangent and adjoint codes of OPA. We validate the differentiated codes by comparison with divided differences, and also with an identical twin experiment. We apply state-of-the-art methods to improve the performance of the adjoint code. In particular we implement the Griewank and Walther's binomial checkpointing algorithm which gives us an optimal trade-off between time and memory consumption. We apply a specific strategy to differentiate the iterative linear solver that comes from the implicit time stepping scheme", "venue": "ArXiv", "authors": ["Moulay Hicham Tber", "Laurent  Hasco\u00ebt", "Arthur  Vidard", "Benjamin  Dauvergne"], "year": 2007, "n_citations": 17}
{"id": 1157951, "s2_id": "44d07be50e3b3845125088028ab4850be745202b", "title": "Disciplined quasiconvex programming", "abstract": "We present a composition rule involving quasiconvex functions that generalizes the classical composition rule for convex functions. This rule complements well-known rules for the curvature of quasiconvex functions under increasing functions and pointwise maximums. We refer to the class of optimization problems generated by these rules, along with a base set of quasiconvex and quasiconcave functions, as disciplined quasiconvex programs . Disciplined quasiconvex programming generalizes disciplined convex programming, the class of optimization problems targeted by most modern domain-specific languages for convex optimization. We describe an implementation of disciplined quasiconvex programming that makes it possible to specify and solve quasiconvex programs in CVXPY 1.0.", "venue": "Optim. Lett.", "authors": ["Akshay  Agrawal", "Stephen  Boyd"], "year": 2020, "n_citations": 14}
{"id": 1160937, "s2_id": "ee82245af5a9b242daa8b03642ff5175b4a1c683", "title": "GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra", "abstract": "We propose GraphMineSuite (GMS): the rst benchmarking suite for graph mining that facilitates evaluating and constructing highperformance graph mining algorithms. First, GMS comes with a benchmark specication based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS oers a carefully designed software platform for seamless testing of dierent ne-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and dierence, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by >2\u21e5), maximal clique listing (by >9\u21e5), k-clique listing (by up to 1.1\u21e5), and subgraph isomorphism (by 2.5\u21e5), also obtaining better theoretical performance bounds. PVLDB Reference Format: M. Besta et al.. GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra. PVLDB, 14(11): 1922 1936, 2021. doi:10.14778/3476249.3476252 PVLDB Artifact Availability: The source code, data, and/or other artifacts have been made available at https://graphminesuite.spcl.inf.ethz.ch/. This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 14, No. 11 ISSN 2150-8097. doi:10.14778/3476249.3476252", "venue": "Proc. VLDB Endow.", "authors": ["Maciej  Besta", "Zur  Vonarburg-Shmaria", "Yannick  Schaffner", "Leonardo  Schwarz", "Grzegorz  Kwasniewski", "Lukas  Gianinazzi", "Jakub  Ber\u00e1nek", "Kacper  Janda", "Tobias  Holenstein", "Sebastian  Leisinger", "Peter  Tatkowski", "Esref  \u00d6zdemir", "Adrian  Balla", "Marcin  Copik", "Philipp  Lindenberger", "Pavel  Kalvoda", "Marek  Konieczny", "Onur  Mutlu", "Torsten  Hoefler"], "year": 2021, "n_citations": 6}
{"id": 1169506, "s2_id": "f2ede4e5c06802f886ae7b804c2896a33cd2c05c", "title": "Fast arithmetics in Artin-Schreier towers over finite fields", "abstract": "An Artin-Schreier tower over the finite field F\"p is a tower of field extensions generated by polynomials of the form X^p-X-@a. Following Cantor and Couveignes, we give algorithms with quasi-linear time complexity for arithmetic operations in such towers. As an application, we present an implementation of Couveignes' algorithm for computing isogenies between elliptic curves using the p-torsion.", "venue": "J. Symb. Comput.", "authors": ["Luca De Feo", "\u00c9ric  Schost"], "year": 2012, "n_citations": 11}
{"id": 1170092, "s2_id": "8444720736a75459908b19a278ef416eca9985b5", "title": "Parameterized Adaptive Multidimensional Integration Routines (PAMIR): Localization by Repeated 2^p Subdivision", "abstract": "This book draft gives the theory of a new method for p dimensional adaptive integration by repeated 2^p subdivision of simplexes and hypercubes. A new method of constructing high order integration routines for these geometries permits adjustable samplings of the integration region controlled by user supplied parameters. An outline of the programs and use instructions are also included in the draft. The fortran programs are not included, but will be published with this draft as a book.", "venue": "ArXiv", "authors": ["Stephen L. Adler"], "year": 2010, "n_citations": 0}
{"id": 1178916, "s2_id": "78f5b00f9a09004d29fea12e1b3042649b958664", "title": "Model Evidence with Fast Tree Based Quadrature", "abstract": "High dimensional integration is essential to many areas of science, ranging from particle physics to Bayesian inference. Approximating these integrals is hard, due in part to the difficulty of locating and sampling from regions of the integration domain that make significant contributions to the overall integral. Here, we present a new algorithm called Tree Quadrature (TQ) that separates this sampling problem from the problem of using those samples to produce an approximation of the integral. TQ places no qualifications on how the samples provided to it are obtained, allowing it to use state-of-the-art sampling algorithms that are largely ignored by existing integration algorithms. Given a set of samples, TQ constructs a surrogate model of the integrand in the form of a regression tree, with a structure optimised to maximise integral precision. The tree divides the integration domain into smaller containers, which are individually integrated and aggregated to estimate the overall integral. Any method can be used to integrate each individual container, so existing integration methods, like Bayesian Monte Carlo, can be combined with TQ to boost their performance. On a set of benchmark problems, we show that TQ provides accurate approximations to integrals in up to 15 dimensions; and in dimensions 4 and above, it outperforms simple Monte Carlo and the popular Vegas method.", "venue": "ArXiv", "authors": ["Thomas  Foster", "Chon Lok Lei", "Martin  Robinson", "David  Gavaghan", "Ben  Lambert"], "year": 2020, "n_citations": 3}
{"id": 1181974, "s2_id": "153685e20a882334b7f5e8117aea875fb165dd5d", "title": "HOL(y)Hammer: Online ATP Service for HOL Light", "abstract": "AbstractHOL(y)Hammer is an online AI/ATP service for formal (computer-understandable) mathematics encoded in the HOL Light\n system. The service allows its users to upload and automatically process an arbitrary formal development (project) based on HOL Light, and to attack arbitrary conjectures that use the concepts defined in some of the uploaded projects. For that, the service uses several automated reasoning systems combined with several premise selection methods trained on all the project proofs. The projects that are readily available on the server for such query answering include the recent versions of the Flyspeck, Multivariate Analysis and Complex Analysis libraries. The service runs on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP combinations and 4 decision procedures that contribute to its overall performance. The system is also available for local installation by interested users, who can customize it for their own proof development. An Emacs interface allowing parallel asynchronous queries to the service is also provided. The overall structure of the service is outlined, problems that arise and their solutions are discussed, and an initial account of using the system is given.", "venue": "Math. Comput. Sci.", "authors": ["Cezary  Kaliszyk", "Josef  Urban"], "year": 2015, "n_citations": 78}
{"id": 1184405, "s2_id": "3437e3a2d0cb37e3bdc4994d90224fa36456e697", "title": "Algorithmic Causal Effect Identification with causaleffect", "abstract": "Our evolution as a species made a huge step forward when we understood the relationships between causes and effects. These associations may be trivial for some events, but they are not in complex scenarios. To rigorously prove that some occurrences are caused by others, causal theory and causal inference were formalized, introducing the do-operator and its associated rules. The main goal of this report is to review and implement in Python some algorithms to compute conditional and non-conditional causal queries from observational data. To this end, we first present some basic background knowledge on probability and graph theory, before introducing important results on causal theory, used in the construction of the algorithms. We then thoroughly study the identification algorithms presented by Shpitser and Pearl in 2006 [SP 2006a, SP 2006b], explaining our implementation in Python alongside. The main identification algorithm can be seen as a repeated application of the rules of do-calculus, and it eventually either returns an expression for the causal query from experimental probabilities or fails to identify the causal effect, in which case the effect is non-identifiable. We introduce our newly developed Python library and give some usage examples.", "venue": "ArXiv", "authors": ["Mart'i  Pedemonte", "Jordi  Vitria", "'Alvaro  Parafita"], "year": 2021, "n_citations": 0}
{"id": 1191919, "s2_id": "1961babf42a66bc66fa723fe5be542bf8f4b9246", "title": "OpenMP, OpenMP/MPI, and CUDA/MPI C programs for solving the time-dependent dipolar Gross-Pitaevskii equation", "abstract": "We present new versions of the previously published C and CUDA programs for solving the dipolar Gross\u2013Pitaevskii equation in one, two, and three spatial dimensions, which calculate stationary and non-stationary solutions by propagation in imaginary or real time. Presented programs are improved and parallelized versions of previous programs, divided into three packages according to the type of parallelization. First package contains improved and threaded version of sequential C programs using OpenMP. Second package additionally parallelizes three-dimensional variants of the OpenMP programs using MPI, allowing them to be run on distributed-memory systems. Finally, previous three-dimensional CUDA-parallelized programs are further parallelized using MPI, similarly as the OpenMP programs. We also present speedup test results obtained using new versions of programs in comparison with the previous sequential C and parallel CUDA programs. The improvements to the sequential version yield a speedup of 1.1\u20131.9, depending on the program. OpenMP parallelization yields further speedup of 2\u201312 on a 16-core workstation, while OpenMP/MPI version demonstrates a speedup of 11.5\u201316.5 on a computer cluster with 32 nodes used. CUDA/MPI version shows a speedup of 9\u201310 on a computer cluster with 32 nodes.", "venue": "Comput. Phys. Commun.", "authors": ["Vladimir  Loncar", "Luis E. Young-S.", "Srdjan  Skrbic", "Paulsamy  Muruganandam", "Sadhan K. Adhikari", "Antun  Balaz"], "year": 2016, "n_citations": 30}
{"id": 1193238, "s2_id": "62ebebbd2fbd92394c8ef007498adef963209718", "title": "Simulation Intelligence: Towards a New Generation of Scientific Methods", "abstract": "The original\"Seven Motifs\"set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the\"Nine Motifs of Simulation Intelligence\", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.", "venue": "ArXiv", "authors": ["Alexander  Lavin", "Hector  Zenil", "Brooks  Paige", "David  Krakauer", "Justin  Gottschlich", "Tim  Mattson", "Anima  Anandkumar", "Sanjay  Choudry", "Kamil  Rocki", "Atilim Gunecs Baydin", "Carina  Prunkl", "Olexandr  Isayev", "Erik  Peterson", "Peter L. McMahon", "Jakob  Macke", "Kyle  Cranmer", "Jiaxin  Zhang", "Haruko  Wainwright", "Adi  Hanuka", "Manuela  Veloso", "Samuel  Assefa", "Stephan  Zheng", "Avi  Pfeffer"], "year": 2021, "n_citations": 0}
{"id": 1197966, "s2_id": "970b3ec9458609c13bf8f1ddfef1ab2fdcc2a2b9", "title": "Optimising finite-difference methods for PDEs through parameterised time-tiling in Devito", "abstract": "Finite-difference methods are widely used in solving partial differential equations. In a large problem set, approximations can take days or weeks to evaluate, yet the bulk of computation may occur within a single loop nest. The modelling process for researchers is not straightforward either, requiring models with differential equations to be translated into stencil kernels, then optimised separately. One tool that seeks to speed up and eliminate mistakes from this tedious procedure is Devito, used to efficiently employ finite-difference methods. \nIn this work, we implement time-tiling, a loop nest optimisation, in Devito yielding a decrease in runtime of up to 45%, and at least 20% across stencils from the acoustic wave equation family, widely used in Devito's target domain of seismic imaging. We present an estimator for arithmetic intensity under time-tiling and a model to predict runtime improvements in stencil computations. We also consider generalisation of time-tiling to imperfect loop nests, a less widely studied problem.", "venue": "ArXiv", "authors": ["Nicholas  Sim"], "year": 2018, "n_citations": 0}
{"id": 1198719, "s2_id": "b1097de4b03d0f38e2e7da58af662366243c1df8", "title": "Subspace-preserving sparsification of matrices with minimal perturbation to the near null-space. Part I: Basics", "abstract": "This is the first of two papers to describe a matrix sparsification algorithm that takes a general real or complex matrix as input and produces a sparse output matrix of the same size. The non-zero entries in the output are chosen to minimize changes to the singular values and singular vectors corresponding to the near null-space of the input. The output matrix is constrained to preserve left and right null-spaces exactly. The sparsity pattern of the output matrix is automatically determined or can be given as input. \nIf the input matrix belongs to a common matrix subspace, we prove that the computed sparse matrix belongs to the same subspace. This works without imposing explicit constraints pertaining to the subspace. This property holds for the subspaces of Hermitian, complex-symmetric, Hamiltonian, circulant, centrosymmetric, and persymmetric matrices, and for each of the skew counterparts. \nApplications of our method include computation of reusable sparse preconditioning matrices for reliable and efficient solution of high-order finite element systems. The second paper in this series describes our open-source implementation, and presents further technical details.", "venue": "ArXiv", "authors": ["Chetan  Jhurani"], "year": 2013, "n_citations": 6}
{"id": 1201303, "s2_id": "afadb6f0d5fe03ea5158df1abb0f9c738d9ae194", "title": "Block-structured adaptive mesh refinement algorithms for Vlasov simulation", "abstract": "Direct discretization of continuum kinetic equations, like the Vlasov equation, are under-utilized because the distribution function generally exists in a high-dimensional (>3D) space and computational cost increases geometrically with dimension. We propose to use high-order finite-volume techniques with block-structured adaptive mesh refinement (AMR) to reduce the computational cost. The primary complication comes from a solution state comprised of variables of different dimensions. We develop the algorithms required to extend standard single-dimension block structured AMR to the multi-dimension case. Specifically, algorithms for reduction and injection operations that transfer data between mesh hierarchies of different dimensions are explained in detail. In addition, modifications to the basic AMR algorithm that enable the use of high-order spatial and temporal discretizations are discussed. Preliminary results for a standard 1D+1V Vlasov-Poisson test problem are presented. Results indicate that there is potential for significant savings for some classes of Vlasov problems.", "venue": "J. Comput. Phys.", "authors": ["Jeffrey A. F. Hittinger", "Jeffrey W. Banks"], "year": 2013, "n_citations": 31}
{"id": 1202657, "s2_id": "145587b0fbd9d063f77a0e15aa9370729084dd01", "title": "A Case for Malleable Thread-Level Linear Algebra Libraries: The LU Factorization With Partial Pivoting", "abstract": "We propose two novel techniques for overcoming load-imbalance encountered when implementing so-called look-ahead mechanisms in relevant dense matrix factorizations for the solution of linear systems. Both techniques target the scenario where two thread teams are created/activated during the factorization, with each team in charge of performing an independent task/branch of execution. The first technique promotes worker sharing (WS) between the two tasks, allowing the threads of the task that completes first to be reallocated for use by the costlier task. The second technique allows a fast task to alert the slower task of completion, enforcing the early termination (ET) of the second task, and a smooth transition of the factorization procedure into the next iteration. The two mechanisms are instantiated via a new malleable thread-level implementation of the basic linear algebra subprograms, and their benefits are illustrated via an implementation of the LU factorization with partial pivoting enhanced with look-ahead. Concretely, our experimental results on an Intel-Xeon system with 12 cores show the benefits of combining WS+ET, reporting competitive performance in comparison with a task-parallel runtime-based solution.", "venue": "IEEE Access", "authors": ["Sandra  Catal\u00e1n", "Jos\u00e9 R. Herrero", "Enrique S. Quintana-Ort\u00ed", "Rafael  Rodr\u00edguez-S\u00e1nchez", "Robert  Van De Geijn"], "year": 2019, "n_citations": 14}
{"id": 1213303, "s2_id": "9a3dabc33c3959aa95159f0d1eefceaefa0ffea9", "title": "FullSWOF_Paral: Comparison of two parallelization strategies (MPI and SKELGIS) on a software designed for hydrology applications", "abstract": "In this paper, we perform a comparison of two approaches for the parallelization of an existing, free software, FullSWOF 2D (http://www. univ-orleans.fr/mapmo/soft/FullSWOF/ that solves shallow water equations for applications in hydrology) based on a domain decomposition strategy. The first approach is based on the classical MPI library while the second approach uses Parallel Algorithmic Skeletons and more precisely a library named SkelGIS (Skeletons for Geographical Information Systems). The first results presented in this article show that the two approaches are similar in terms of performance and scalability. The two implementation strategies are however very different and we discuss the advantages of each one.", "venue": "ArXiv", "authors": ["St\u00e9phane  Cordier", "H\u00e9l\u00e8ne  Coullon", "Olivier  Delestre", "Christian  Laguerre", "Minh-Hoang  Le", "Daniel  Pierre", "Georges  Sadaka"], "year": 2013, "n_citations": 20}
{"id": 1224096, "s2_id": "f05c0ec1f40bb1db980c47aee4ce71fd78953b63", "title": "Adaptive numerical simulations with Trixi.jl: A case study of Julia for scientific computing", "abstract": "We present Trixi.jl, a Julia package for adaptive high-order numerical simulations of hyperbolic partial differential equations. Utilizing Julia\u2019s strengths, Trixi.jl is extensible, easy to use, and fast. We describe the main design choices that enable these features and compare Trixi.jl with a mature open source Fortran code that uses the same numerical methods. We conclude with an assessment of Julia for simulation-focused scientific computing, an area that is still dominated by traditional high-performance computing languages such as C, C++, and Fortran.", "venue": "ArXiv", "authors": ["Hendrik  Ranocha", "Michael  Schlottke-Lakemper", "Andrew R. Winters", "Erik  Faulhaber", "Jesse  Chan", "Gregor J. Gassner"], "year": 2021, "n_citations": 2}
{"id": 1243603, "s2_id": "f49454c61386e759ccd5cdcdddf8d85c61d81e96", "title": "FEMPAR: An Object-Oriented Parallel Finite Element Framework", "abstract": "FEMPAR is an open source object oriented Fortran200X scientific software library for the high-performance scalable simulation of complex multiphysics problems governed by partial differential equations at large scales, by exploiting state-of-the-art supercomputing resources. It is a highly modularized, flexible, and extensible library, that provides a set of modules that can be combined to carry out the different steps of the simulation pipeline. FEMPAR includes a rich set of algorithms for the discretization step, namely (arbitrary-order) grad, div, and curl-conforming finite element methods, discontinuous Galerkin methods, B-splines, and unfitted finite element techniques on cut cells, combined with h-adaptivity. The linear solver module relies on state-of-the-art bulk-asynchronous implementations of multilevel domain decomposition solvers for the different discretization alternatives and block-preconditioning techniques for multiphysics problems. FEMPAR is a framework that provides users with out-of-the-box state-of-the-art discretization techniques and highly scalable solvers for the simulation of complex applications, hiding the dramatic complexity of the underlying algorithms. But it is also a framework for researchers that want to experience with new algorithms and solvers, by providing a highly extensible framework. In this work, the first one in a series of articles about FEMPAR, we provide a detailed introduction to the software abstractions used in the discretization module and the related geometrical module. We also provide some ingredients about the assembly of linear systems arising from finite element discretizations, but the software design of complex scalable multilevel solvers is postponed to a subsequent work.", "venue": "Archives of computational methods in engineering : state of the art reviews", "authors": ["Santiago  Badia", "Alberto F. Mart\u00edn", "Javier  Principe"], "year": 2018, "n_citations": 38}
{"id": 1251335, "s2_id": "58fd3c545426dc7eeb05965b089ac211a689be52", "title": "Auto-Vectorizing TensorFlow Graphs: Jacobians, Auto-Batching And Beyond", "abstract": "We propose a static loop vectorization optimization on top of high level dataflow IR used by frameworks like TensorFlow. A new statically vectorized parallel-for abstraction is provided on top of TensorFlow, and used for applications ranging from auto-batching and per-example gradients, to jacobian computation, optimized map functions and input pipeline optimization. We report huge speedups compared to both loop based implementations, as well as run-time batching adopted by the DyNet framework.", "venue": "ArXiv", "authors": ["Ashish  Agarwal", "Igor  Ganichev"], "year": 2019, "n_citations": 6}
{"id": 1251890, "s2_id": "d88e3ca7a1b58d0347e73d7fa68ce1fe44fef00e", "title": "Enhancing the scalability and load balancing of the parallel selected inversion algorithm via tree-based asynchronous communication", "abstract": "We develop a method for improving the parallel scalability of the recently developed parallel selected inversion algorithm [Jacquelin, Lin and Yang 2014], named PSelInv, on massively parallel distributed memory machines. In the PSelInv method, we compute selected elements of the inverse of a sparse matrix A that can be decomposed as A = LU, where L is lower triangular and U is upper triangular. Updating these selected elements of A 1 requires restricted collective communications among a subset of processors within each column or row communication group created by a block cyclic distribution of L and U. We describe how this type of restricted collective communication can be implemented by using asynchronous point-to-point MPI communication functions combined with a binary tree based data propagation scheme. Because multiple restricted collective communications may take place at the same time in the parallel selected inversion algorithm, we need to use a heuristic to prevent processors participating in multiple collective communications from receiving too many messages. This heuristic allows us to reduce communication load imbalance and improve the overall scalability of the selected inversion algorithm. For instance, when 6; 400 processors are used, we observe over 5x speedup for test matrices. It also mitigates the performance variability introduced by an inhomogeneous network topology.", "venue": "ArXiv", "authors": ["Mathias  Jacquelin", "Lin  Lin", "Nathan  Wichmann", "Chao  Yang"], "year": 2015, "n_citations": 7}
{"id": 1253462, "s2_id": "847bc9bf0e9d8000b2cf441aef331e328186bacb", "title": "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data", "abstract": "Many large datasets exhibit power-law statistics: The web graph, social networks, text data, click through data etc. Their adjacency graphs are termed natural graphs, and are known to be difficult to partition. As a consequence most distributed algorithms on these graphs are communication intensive. Many algorithms on natural graphs involve an Allreduce: a sum or average of partitioned data which is then shared back to the cluster nodes. Examples include PageRank, spectral partitioning, and many machine learning algorithms including regression, factor (topic) models, and clustering. In this paper we describe an efficient and scalable Allreduce primitive for power-law data. We point out scaling problems with existing butterfly and round-robin networks for Sparse Allreduce, and show that a hybrid approach improves on both. Furthermore, we show that Sparse Allreduce stages should be nested instead of cascaded (as in the dense case). And that the optimum throughput Allreduce network should be a butterfly of heterogeneous degree where degree decreases with depth into the network. Finally, a simple replication scheme is introduced to deal with node failures. We present experiments showing significant improvements over existing systems such as PowerGraph and Hadoop.", "venue": "ArXiv", "authors": ["Huasha  Zhao", "John F. Canny"], "year": 2013, "n_citations": 6}
{"id": 1254759, "s2_id": "1c6b9477d83080458b1bad359f7f4330f1d5e1b3", "title": "PyArmadillo: a streamlined linear algebra library for Python", "abstract": "PyArmadillo is a linear algebra library for the Python language, with the aim of closely mirroring the programming interface of the widely used Armadillo C++ library, which in turn is deliberately similar to Matlab. PyArmadillo hence facilitates algorithm prototyping with Matlab-like syntax directly in Python, and relatively straightforward conversion of PyArmadillo-based Python code into performant Armadillo-based C++ code. The converted code can be used for purposes such as speeding up Python-based programs in conjunction with pybind11, or the integration of algorithms originally prototyped in Python into larger C++ codebases. PyArmadillo provides objects for matrices and cubes, as well as over 200 associated functions for manipulating data stored in the objects. Integer, floating point and complex numbers are supported. Various matrix factorisations are provided through integration with LAPACK, or one of its high performance drop-in replacements such as Intel MKL or OpenBLAS. PyArmadillo is open-source software, distributed under the Apache 2.0 license; it can be obtained at https://pyarma.sourceforge.io or via the Python Package Index in precompiled form.", "venue": "Journal of Open Source Software", "authors": ["Jason  Rumengan", "Terry Yue Zhuo", "Conrad  Sanderson"], "year": 2021, "n_citations": 1}
{"id": 1256040, "s2_id": "34223642536965bbb85ba80059baeff1f7abd235", "title": "Best Practices for Scientific Computing", "abstract": "We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.", "venue": "PLoS biology", "authors": ["Dhavide A. Aruliah", "C. Titus Brown", "Neil P. Chue Hong", "Matt  Davis", "Richard T. Guy", "Steven H. D. Haddock", "Katy  Huff", "Ian M. Mitchell", "Mark D. Plumbley", "Ben  Waugh", "Ethan P. White", "Greg  Wilson", "Paul  Wilson"], "year": 2014, "n_citations": 510}
{"id": 1262112, "s2_id": "e93a9e1f9271d9573377185d9f5fa796b6066157", "title": "Rank-profile revealing Gaussian elimination and the CUP matrix decomposition", "abstract": "Transforming a matrix over a field to echelon form, or decomposing the matrix as a product of structured matrices that reveal the rank profile, is a fundamental building block of computational exact linear algebra. This paper surveys the well known variations of such decompositions and transformations that have been proposed in the literature. We present an algorithm to compute the CUP decomposition of a matrix, adapted from the LSP algorithm of Ibarra, Moran and Hui (1982), and show reductions from the other most common Gaussian elimination based matrix transformations and decompositions to the CUP decomposition. We discuss the advantages of the CUP algorithm over other existing algorithms by studying time and space complexities: the asymptotic time complexity is rank sensitive, and comparing the constants of the leading terms, the algorithms for computing matrix invariants based on the CUP decomposition are always at least as good except in one case. We also show that the CUP algorithm, as well as the computation of other invariants such as transformation to reduced column echelon form using the CUP algorithm, all work in place, allowing for example to compute the inverse of a matrix on the same storage as the input matrix.", "venue": "J. Symb. Comput.", "authors": ["Claude-Pierre  Jeannerod", "Cl\u00e9ment  Pernet", "Arne  Storjohann"], "year": 2013, "n_citations": 36}
{"id": 1265479, "s2_id": "beac8832a67b72d68934ac89b8378007c4bba66d", "title": "PIFE-PIC: Parallel Immersed-Finite-Element Particle-In-Cell For 3-D Kinetic Simulations of Plasma-Material Interactions", "abstract": "This paper presents a recently developed particle simulation code package PIFE-PIC, which is a novel three-dimensional (3-D) Parallel Immersed-Finite-Element (IFE) Particle-in-Cell (PIC) simulation model for particle simulations of plasma-material interactions. This framework is based on the recently developed non-homogeneous electrostatic IFE-PIC algorithm, which is designed to handle complex plasma-material interface conditions associated with irregular geometries using a Cartesian-mesh-based PIC. Three-dimensional domain decomposition is utilized for both the electrostatic field solver with IFE and the particle operations in PIC to distribute the computation among multiple processors. A simulation of the orbital-motion-limited (OML) sheath of a dielectric sphere immersed in a stationary plasma is carried out to validate PIFE-PIC and profile the parallel performance of the code package. Furthermore, a large-scale simulation of plasma charging at a lunar crater containing 2 million PIC cells (10 million FE/IFE cells) and about 520 million particles, running for 20,000 PIC steps in about 109 wall-clock hours, is presented to demonstrate the high-performance computing capability of PIFE-PIC.", "venue": "SIAM J. Sci. Comput.", "authors": ["Daoru  Han", "Xiaoming  He", "David  Lund", "Xu  Zhang"], "year": 2021, "n_citations": 0}
{"id": 1272377, "s2_id": "0b0f306dab625b6dc001bbd4387959a82cfe6b66", "title": "Takin: An open-source software for experiment planning, visualisation, and data analysis", "abstract": "Due to the instrument's non-trivial resolution function, measurements on triple-axis spectrometers require extra care from the experimenter in order to obtain optimal results and to avoid unwanted spurious artefacts. We present a free and open-source software system that aims to ease many of the tasks encountered during the planning phase, in the execution and in data treatment of experiments performed on neutron triple-axis spectrometers. The software is currently in use and has been successfully tested at the MLZ, but can be configured to work with other triple-axis instruments and instrument control systems.", "venue": "SoftwareX", "authors": ["T.  Weber", "R.  Georgii", "P.  B\u00f6ni"], "year": 2016, "n_citations": 13}
{"id": 1275274, "s2_id": "6d0a485261d14f0bb6e9a3985d2bcd34c98d48cf", "title": "A New Sparse Matrix Vector Multiplication GPU Algorithm Designed for Finite Element Problems", "abstract": "Recently, graphics processors (GPUs) have been increasingly leveraged in a variety of scientific computing applications. However, architectural differences between CPUs and GPUs necessitate the development of algorithms that take advantage of GPU hardware. As sparse matrix vector multiplication (SPMV) operations are commonly used in finite element analysis, a new SPMV algorithm and several variations are developed for unstructured finite element meshes on GPUs. The effective bandwidth of current GPU algorithms and the newly proposed algorithms are measured and analyzed for 15 sparse matrices of varying sizes and varying sparsity structures. The effects of optimization and differences between the new GPU algorithm and its variants are then subsequently studied. Lastly, both new and current SPMV GPU algorithms are utilized in the GPU CG Solver in GPU finite element simulations of the heart. These results are then compared against parallel PETSc finite element implementation results. The effective bandwidth tests indicate that the new algorithms compare very favorably with current algorithms for a wide variety of sparse matrices and can yield very notable benefits. GPU finite element simulation results demonstrate the benefit of using GPUs for finite element analysis, and also show that the proposed algorithms can yield speedup factors up to 12-fold for real finite element applications.", "venue": "ArXiv", "authors": ["Jonathan  Wong", "Ellen  Kuhl", "Eric  Darve"], "year": 2015, "n_citations": 0}
{"id": 1276977, "s2_id": "12ee6d43e0454d9431d0f1388791a3398a35c981", "title": "Butterfly factorization via randomized matrix-vector multiplications", "abstract": "This paper presents an adaptive randomized algorithm for computing the butterfly factorization of a $m\\times n$ matrix with $m\\approx n$ provided that both the matrix and its transpose can be rapidly applied to arbitrary vectors. The resulting factorization is composed of $O(\\log n)$ sparse factors, each containing $O(n)$ nonzero entries. The factorization can be attained using $O(n^{3/2}\\log n)$ computation and $O(n\\log n)$ memory resources. The proposed algorithm applies to matrices with strong and weak admissibility conditions arising from surface integral equation solvers with a rigorous error bound, and is implemented in parallel.", "venue": "SIAM J. Sci. Comput.", "authors": ["Yang  Liu", "Xin  Xing", "Han  Guo", "Eric  Michielssen", "Pieter  Ghysels", "Xiaoye Sherry Li"], "year": 2021, "n_citations": 8}
{"id": 1278260, "s2_id": "1d407cead445ca888570bfb9eb09fb3f835c1015", "title": "Sparse Matrix Implementation in Octave", "abstract": "There are many classes of mathematical problems which give rise to matrices, where a large number of the elements are zero. In this case it makes sense to have a special matrix type to handle this class of problems where only the non-zero elements of the matrix are stored. Not only does this reduce the amount of memory to store the matrix, but it also means that operations on this type of matrix can take advantage of the a-priori knowledge of the positions of the non-zero elements to accelerate their calculations. A matrix type that stores only the non-zero elements is generally called sparse. \nUntil recently Octave has lacked a full implementation of sparse matrices. This article address the implementation of sparse matrices within Octave, including their storage, creation, fundamental algorithms used, their implementations and the basic operations and functions implemented for sparse matrices. Mathematical issues such as the return types of sparse operations, matrix fill-in and reordering for sparse matrix factorization is discussed in the context of a real example. \nBenchmarking of Octave's implementation of sparse operations compared to their equivalent in Matlab are given and their implications discussed. Results are presented for multiplication and linear algebra operations for various matrix orders and densities. Furthermore, the use of Octave's sparse matrix implementation is demonstrated using a real example of a finite element model (FEM) problem. Finally, the method of using sparse matrices with Octave's oct-files is discussed. The means of creating, using and returning sparse matrices within oct-files is discussed as well as the differences between Octave's Sparse and Array classes.", "venue": "ArXiv", "authors": ["David  Bateman", "Andy  Adler"], "year": 2006, "n_citations": 5}
{"id": 1284534, "s2_id": "b88aff3b7bb19035ed3e420ff3cbc50bf9fe2df5", "title": "Tensor Train decomposition on TensorFlow (T3F)", "abstract": "Tensor Train decomposition is used across many branches of machine learning, but until now it lacked an implementation with GPU support, batch processing, automatic differentiation, and versatile functionality for Riemannian optimization framework, which takes in account the underlying manifold structure in order to construct efficient optimization methods. In this work, we propose a library that aims to fix it and makes machine learning papers that rely on Tensor Train decomposition easier to implement. The library includes 92% test coverage, examples, and API reference documentation.", "venue": "J. Mach. Learn. Res.", "authors": ["Alexander  Novikov", "Pavel  Izmailov", "Valentin  Khrulkov", "Michael  Figurnov", "Ivan V. Oseledets"], "year": 2020, "n_citations": 37}
{"id": 1287514, "s2_id": "21fcd4e765ecd51a1f0ff2cfdae1be4eb9b21e4b", "title": "NeuralSens: Sensitivity Analysis of Neural Networks", "abstract": "Neural networks are important tools for data-intensive analysis and are commonly applied to model non-linear relationships between dependent and independent variables. However, neural networks are usually seen as \"black boxes\" that offer minimal information about how the input variables are used to predict the response in a fitted model. This article describes the \\pkg{NeuralSens} package that can be used to perform sensitivity analysis of neural networks using the partial derivatives method. Functions in the package can be used to obtain the sensitivities of the output with respect to the input variables, evaluate variable importance based on sensitivity measures and characterize relationships between input and output variables. Methods to calculate sensitivities are provided for objects from common neural network packages in \\proglang{R}, including \\pkg{neuralnet}, \\pkg{nnet}, \\pkg{RSNNS}, \\pkg{h2o}, \\pkg{neural}, \\pkg{forecast} and \\pkg{caret}. The article presents an overview of the techniques for obtaining information from neural network models, a theoretical foundation of how are calculated the partial derivatives of the output with respect to the inputs of a multi-layer perceptron model, a description of the package structure and functions, and applied examples to compare \\pkg{NeuralSens} functions with analogous functions from other available \\proglang{R} packages.", "venue": "ArXiv", "authors": ["J.  Pizarroso", "J.  Portela", "A.  Munoz"], "year": 2020, "n_citations": 6}
{"id": 1288844, "s2_id": "5734e10de8a1fb0ced8163b5a148bd473a66b9ff", "title": "GraphMaps: Browsing Large Graphs as Interactive Maps", "abstract": "Algorithms for laying out large graphs have seen significant progress in the past decade. However, browsing large graphs remains a challenge. Rendering thousands of graphical elements at once often results in a cluttered image, and navigating these elements naively can cause disorientation. To address this challenge we propose a method called GraphMaps, mimicking the browsing experience of online geographic maps. \n \nGraphMaps creates a sequence of layers, where each layer refines the previous one. During graph browsing, GraphMaps chooses the layer corresponding to the zoom level, and renders only those entities of the layer that intersect the current viewport. The result is that, regardless of the graph size, the number of entities rendered at each view does not exceed a predefined threshold, yet all graph elements can be explored by the standard zoom and pan operations. \n \nGraphMaps preprocesses a graph in such a way that during browsing, the geometry of the entities is stable, and the viewer is responsive. Our case studies indicate that GraphMaps is useful in gaining an overview of a large graph, and also in exploring a graph on a finer level of detail.", "venue": "Graph Drawing", "authors": ["Lev  Nachmanson", "Roman  Prutkin", "Bongshin  Lee", "Nathalie Henry Riche", "Alexander E. Holroyd", "Xiaoji  Chen"], "year": 2015, "n_citations": 21}
{"id": 1291772, "s2_id": "b352748b5bddd14ad622cf69998f52d8c564fb69", "title": "Regressor: A C program for Combinatorial Regressions", "abstract": "In statistics, researchers use Regression models for data analysis and prediction in many productive sectors (industry, business, academy, etc.). Regression models are mathematical functions representing an approximation of dependent variable $Y$ from n independent variables $X_i \\in X$. The literature presents many regression methods divided into single and multiple regressions. There are several procedures to generate regression models and sets of commercial and academic tools that implement these procedures. This work presents one open-source program called Regressor that makes models from a specific variation of polynomial regression. These models relate the independent variables to generate an approximation of the original output dependent data. In many tests, Regressor was able to build models five times more accurate than commercial tools.", "venue": "ArXiv", "authors": ["Eduardo M. Vasconcelos", "Adriano Gouveia de Souza"], "year": 2020, "n_citations": 0}
{"id": 1292260, "s2_id": "60ccff42597681c083bd70cf5ba8a8c5f5fa83da", "title": "Bloscpack: a compressed lightweight serialization format for numerical data", "abstract": "This paper introduces the Bloscpack file format and the accompanying Python reference implementation. Bloscpack is a lightweight, compressed binary file-format based on the Blosc codec and is designed for lightweight, fast serialization of numerical data. This article presents the features of the file-format and some some API aspects of the reference implementation, in particular the ability to handle Numpy ndarrays. Furthermore, in order to demonstrate its utility, the format is compared both feature- and performance-wise to a few alternative lightweight serialization solutions for Numpy ndarrays. The performance comparisons take the form of some comprehensive benchmarks over a range of different artificial datasets with varying size and complexity, the results of which are presented as the last section of this article.", "venue": "ArXiv", "authors": ["Valentin  Haenel"], "year": 2014, "n_citations": 3}
{"id": 1299110, "s2_id": "d2dae1065266202e9703548dc5d5dce74ebdc3ef", "title": "alphaCertified: certifying solutions to polynomial systems", "abstract": "Smale's \u00ae-theory uses estimates related to the convergence of Newton's method to certify that Newton iterations will converge quadratically to solutions to a square polynomial system. The program alphaCertified implements algorithms based on \u00ae-theory to certify solutions of polynomial systems using both exact rational arith- metic and arbitrary precision floating point arithmetic. It also implements algorithms that certify whether a given point corresponds to a real solution, and algorithms to heuristically validate solutions to overdetermined systems. Examples are presented to demonstrate the algorithms.", "venue": "ArXiv", "authors": ["Jonathan D. Hauenstein", "Frank  Sottile"], "year": 2010, "n_citations": 17}
{"id": 1304240, "s2_id": "4e464372e339dfa0290862fc10b29826a98a9d87", "title": "Building Bricks with Bricks, with Mathematica", "abstract": "In this work we solve a special case of the problem of building an n-dimensional parallelepiped using a given set of n-dimensional parallelepipeds. Consider the identity x^3 = x(x-1)(x-2)+3x(x-1+x). For sufficiently large x, we associate with x^3 a cube with edges of size x, with x(x-1)(x-2) a parallelepiped with edges x, x-1, x-2, with 3x(x-1+x) three parallelepipeds of edges x, x-1, 1, and with x a parallelepiped of edges x, 1, 1. The problem we takle is the actual construction of the cube using the given parallelepipeds. In [DDNP90] it was shown how to solve this specific problem and all similar instances in which a (monic) polynomial is expressed as a linear combination of a persistent basis. That is to say a sequence of polynomials q_0 = 1, and q_k(x) = q_{k-1}(x)(x-r_k) for k > 0. Here, after [Fil10], we deal with a multivariate version of the problem with respect to a basis of polynomials of the same degree (binomial basis). We show that it is possible to build the parallelepiped associated with a multivariate polynomial P(x_1, ..., x_n)=(x_1- s_1)...(x_n-s_n) with integer roots, using the parallelepipeds described by the elements of the basis. We provide an algorithm in Mathematica to solve the problem for each n. Moreover, for n = 2, 3, 4 (in the latter case, only when a projection is possible) we use Mathematica to display a step by step construction of the parallelepiped P(x1,...,x_n).", "venue": "ArXiv", "authors": ["Pietro  Codara", "Ottavio M. D'Antona", "Daniele  Filaretti"], "year": 2013, "n_citations": 0}
{"id": 1309336, "s2_id": "6eaf215f57e370f3eadab7e5f233db2d67466fd5", "title": "Software for Computing the Spheroidal Wave Functions Using Arbitrary Precision Arithmetic", "abstract": "The spheroidal wave functions, which are the solutions to the Helmholtz equation in spheroidal coordinates, are notoriously difficult to compute. Because of this, practically no programming language comes equipped with the means to compute them. This makes problems that require their use hard to tackle. We have developed computational software for calculating these special functions. Our software is called spheroidal and includes several novel features, such as: using arbitrary precision arithmetic; adaptively choosing the number of expansion coefficients to compute and use; and using the Wronskian to choose from several different methods for computing the spheroidal radial functions to improve their accuracy. There are two types of spheroidal wave functions: the prolate kind when prolate spheroidal coordinates are used; and the oblate kind when oblate spheroidal coordinate are used. In this paper, we describe both, methods for computing them, and our software. We have made our software freely available on our webpage.", "venue": "ArXiv", "authors": ["Ross  Adelman", "Nail A. Gumerov", "Ramani  Duraiswami"], "year": 2014, "n_citations": 19}
{"id": 1327108, "s2_id": "8812b22f8c0719f3128fb3b56b9dbcd3435b4660", "title": "Realcertify: a maple package for certifying non-negativity", "abstract": "Let Q (resp. R) be the field of rational (resp. real) numbers and <i>X</i> = (<i>X</i><sub>1</sub>, ... , <i>X<sub>n</sub></i>) be variables. Deciding the non-negativity of polynomials in Q[<i>X</i>] over R<sup><i>n</i></sup> or over semi-algebraic domains defined by polynomial constraints in Q[<i>X</i>] is a classical algorithmic problem for symbolic computation.\n The Maple package R<scp>eal</scp>C<scp>ertify</scp> tackles this decision problem by computing sum of squares certificates of non-negativity for inputs where such certificates hold over the rational numbers. It can be applied to numerous problems coming from engineering sciences, program verification and cyber-physical systems. It is based on hybrid symbolic-numeric algorithms based on semi-definite programming.", "venue": "ACCA", "authors": ["Victor  Magron", "Mohab Safey El Din"], "year": 2018, "n_citations": 12}
{"id": 1339188, "s2_id": "153e3e9a593ea1d075a768cfbfb4c2945e59c492", "title": "Q#, a quantum computation package for the .NET platform", "abstract": "Quantum computing is a promising approach of computation that is based on equations from Quantum Mechanics. A simulator for quantum algorithms must be capable of performing heavy mathematical matrix transforms. The design of the simulator itself takes one of three forms: Quantum Turing Machine, Network Model or circuit model of connected gates or, Quantum Programming Language, yet, some simulators are hybrid. We studied previous simulators and then we adopt features from three simulators of different implementation languages, different paradigms, and for different platforms. They are Quantum Computing Language (QCL), QUASI, and Quantum Optics Toolbox for Matlab 5. Our simulator for quantum algorithms takes the form of a package or a programming library for Quantum computing, with a case study showing the ability of using it in the circuit model. The .NET is a promising platform for computing. VB.NET is an easy, high productive programming language with the full power and functionality provided by the .NET framework. It is highly readable, writeable, and flexible language, compared to another language such as C#.NET in many aspects. We adopted VB.NET although its shortage in built-in mathematical complex and matrix operations, compared to Matlab. For implementation, we first built a mathematical core of matrix operations. Then, we built a quantum core which contains: basic qubits and register operations, basic 1D, 2D, and 3D quantum gates, and multi-view visualization of the quantum state, then a window for demos to show you how to use and get the most of the package.", "venue": "ArXiv", "authors": ["A. S. Tolba", "M. Z. Rashad", "M. A. El-Dosuky"], "year": 2013, "n_citations": 3}
{"id": 1348102, "s2_id": "1a38746e242f46a206d667a03e6ec3c7aefd0cf7", "title": "General Complex Polynomial Root Solver and Its Further Optimization for Binary Microlenses", "abstract": "We present a new algorithm to solve polynomial equations, and publish its code, which is 1.6\u20133 times faster than the ZROOTS subroutine that is commercially available from Numerical Recipes, depending on application. The largest improvement, when compared to naive solvers, comes from a fail-safe procedure that permits us to skip the majority of the calculations in the great majority of cases, without risking catastrophic failure in the few cases that these are actually required. Second, we identify a discriminant that enables a rational choice between Laguerre\u2019s Method and Newton\u2019s Method (or a new intermediate method) on a case-by-case basis. We briefly review the history of root solving and demonstrate that \u201cNewton\u2019s Method\u201d was discovered neither by Newton (1671) nor by Raphson (1690), but only by Simpson (1740). Some of the arguments leading to this conclusion were first given by the British historian of science Nick Kollerstrom in 1992, but these do not appear to have penetrated the astronomical community. Finally, we argue that Numerical Recipes should voluntarily surrender its copyright protection for non-profit applications, despite the fact that, in this particular case, such protection was the major stimulant for developing our improved algorithm.", "venue": "ArXiv", "authors": ["J.  Skowron", "A.  Gould"], "year": 2012, "n_citations": 23}
{"id": 1354647, "s2_id": "1a8f2b7c438c05b78c6169d7cd396243d52fb071", "title": "Using Jupyter for Reproducible Scientific Workflows", "abstract": "Literate computing has emerged as an important tool for computational studies and open science, with growing folklore of best practices. In this work, we report two case studies\u2014one in computational magnetism and another in computational mathematics\u2014where domain-specific software was exposed to the Jupyter environment. This enables high level control of simulations and computation, interactive exploration of computational results, batch processing on HPC resources, and reproducible workflow documentation in Jupyter notebooks. In the first study, Ubermag drives existing computational micromagnetics software through a domain-specific language embedded in Python. In the second study, a dedicated Jupyter kernel interfaces with the GAP system for computational discrete algebra and its dedicated programming language. In light of these case studies, we discuss the benefits of this approach, including progress toward more reproducible and reusable research results and outputs, notably through the use of infrastructure such as JupyterHub and Binder.", "venue": "Computing in Science & Engineering", "authors": ["Marijan  Beg", "Juliette  Taka", "Thomas  Kluyver", "Alexander  Konovalov", "Min  Ragan-Kelley", "Nicolas M. Thi\u00e9ry", "Hans  Fangohr"], "year": 2021, "n_citations": 10}
{"id": 1358735, "s2_id": "4540ad64da31531cbb00ad26ae050bcbf856238a", "title": "Comparative study of finite element methods using the Time-Accuracy-Size (TAS) spectrum analysis", "abstract": "We present a performance analysis appropriate for comparing algorithms using different numerical discretizations. By taking into account the total time-to-solution, numerical accuracy with respect to an error norm, and the computation rate, a cost-benefit analysis can be performed to determine which algorithm and discretization are particularly suited for an application. This work extends the performance spectrum model in Chang et. al. 2017 for interpretation of hardware and algorithmic tradeoffs in numerical PDE simulation. As a proof-of-concept, popular finite element software packages are used to illustrate this analysis for Poisson's equation.", "venue": "SIAM J. Sci. Comput.", "authors": ["Justin  Chang", "Maurice S. Fabien", "Matthew G. Knepley", "Richard T. Mills"], "year": 2018, "n_citations": 7}
{"id": 1364120, "s2_id": "8236047b8de79226f7dc9823d6f880644132f868", "title": "The LAPW method with eigendecomposition based on the Hari-Zimmermann generalized hyperbolic SVD", "abstract": "In this paper we propose an accurate, highly parallel algorithm for the generalized eigendecomposition of a matrix pair $(H, S)$, given in a factored form $(F^{\\ast} J F, G^{\\ast} G)$. Matrices $H$ and $S$ are generally complex and Hermitian, and $S$ is positive definite. These type of matrices emerge from the representation of the Hamiltonian of a quantum mechanical system in terms of an overcomplete set of basis functions. This expansion is part of a class of models within the broad field of Density Functional Theory, which is considered the golden standard in Condensed Matter Physics. The overall algorithm consists of four phases, the second and the fourth being optional, where the two last phases are computation of the generalized hyperbolic SVD of a complex matrix pair $(F,G)$, according to a given matrix $J$ defining the hyperbolic scalar product. If $J = I$, then these two phases compute the GSVD in parallel very accurately and efficiently.", "venue": "SIAM J. Sci. Comput.", "authors": ["Sanja  Singer", "Edoardo Di Napoli", "Vedran  Novakovic", "Gayatri  Caklovic"], "year": 2020, "n_citations": 6}
{"id": 1375942, "s2_id": "e9588bd9120965713b2261b7444b6fd218322dca", "title": "Efficient Decomposition of Dense Matrices over GF(2)", "abstract": "In this work we describe an efficient implementation of a hierarchy of algorithms for the decomposition of dense matrices over the field with two elements (GF(2)). Matrix decomposition is an essential building block for solving dense systems of linear and non-linear equations and thus much research has been devoted to improve the asymptotic complexity of such algorithms. In this work we discuss an implementation of both well-known and improved algorithms in the M4RI library. The focus of our discussion is on a new variant of the M4RI algorithm - denoted MMPF in this work -- which allows for considerable performance gains in practice when compared to the previously fastest implementation. We provide performance figures on x86_64 CPUs to demonstrate the viability of our approach.", "venue": "ArXiv", "authors": ["Martin R. Albrecht", "Cl\u00e9ment  Pernet"], "year": 2010, "n_citations": 7}
{"id": 1381296, "s2_id": "b70d2cd4d6a67d3da35dc4be9ca27202e165e2d8", "title": "Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem", "abstract": "Random variables and their distributions are a central part in many areas of statistical methods. The Distributions.jl package provides Julia users and developers tools for working with probability distributions, leveraging Julia features for their intuitive and flexible manipulation, while remaining highly efficient through zero-cost abstractions.", "venue": "J. Stat. Softw.", "authors": ["Mathieu  Besan\u00e7on", "David  Anthoff", "Alex  Arslan", "Simon  Byrne", "Dahua  Lin", "Theodore  Papamarkou", "John  Pearson"], "year": 2021, "n_citations": 19}
{"id": 1385713, "s2_id": "f74d82dec2b8c6b817ac85c8d5b8ba1be09b0e51", "title": "Investigating independent subsets of graphs, with Mathematica", "abstract": "With this work we aim to show how Mathematica can be a useful tool to investigate properties of combinatorial structures. Specifically, we will face enumeration problems on independent subsets of powers of paths and cycles, trying to highlight the correspondence with other combinatorial objects with the same cardinality. Then we will study the structures obtained by ordering properly independent subsets of paths and cycles. We will approach some enumeration problems on the resulting partially ordered sets, putting in evidence the correspondences with structures known as Fibonacci and Lucas Cubes.", "venue": "ArXiv", "authors": ["Pietro  Codara", "Ottavio M. D'Antona"], "year": 2013, "n_citations": 0}
{"id": 1385906, "s2_id": "d737fb0568cd4288bb5aed250766958204031130", "title": "GTApprox: Surrogate modeling for industrial design", "abstract": "We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems.", "venue": "Adv. Eng. Softw.", "authors": ["Mikhail  Belyaev", "Evgeny  Burnaev", "Ermek  Kapushev", "Maxim  Panov", "Pavel V. Prikhodko", "Dmitry P. Vetrov", "Dmitry  Yarotsky"], "year": 2016, "n_citations": 41}
{"id": 1385920, "s2_id": "dd98edd7c176fc9f301e18e84048c0fccccb8e0d", "title": "Program generation for small-scale linear algebra applications", "abstract": "We present SLinGen, a program generation system for linear algebra. The input to SLinGen is an application expressed mathematically in a linear-algebra-inspired language (LA) that we define. LA provides basic scalar/vector/matrix additions/multiplications and higher level operations including linear systems solvers, Cholesky and LU factorizations. The output of SLinGen is performance-optimized single-source C code, optionally vectorized with intrinsics. The target of SLinGen are small-scale computations on fixed-size operands, for which a straightforward implementation using optimized libraries (e.g., BLAS or LAPACK) is known to yield suboptimal performance (besides increasing code size and introducing dependencies), but which are crucial in control, signal processing, computer vision, and other domains. Internally, SLinGen uses synthesis and DSL-based techniques to optimize at a high level of abstraction. We benchmark our program generator on three prototypical applications: the Kalman filter, Gaussian process regression, and an L1-analysis convex solver, as well as basic routines including Cholesky factorization and solvers for the continuous-time Lyapunov and Sylvester equations. The results show significant speed-ups compared to straightforward C with Intel icc and clang with a polyhedral optimizer, as well as library-based and template-based implementations.", "venue": "CGO", "authors": ["Daniele G. Spampinato", "Diego  Fabregat-Traver", "Paolo  Bientinesi", "Markus  P\u00fcschel"], "year": 2018, "n_citations": 26}
{"id": 1387863, "s2_id": "83d687cc1b541451cbf7554ac4d28519edb7f9d2", "title": "Pressio: Enabling projection-based model reduction for large-scale nonlinear dynamical systems", "abstract": "This work introduces Pressio, an open-source project aimed at enabling leading-edge projection-based reduced order models (ROMs) for large-scale nonlinear dynamical systems in science and engineering. Pressio provides model-reduction methods that can reduce both the number of spatial and temporal degrees of freedom for any dynamical system expressible as a system of parameterized ordinary differential equations (ODEs). We leverage this simple, expressive mathematical framework as a pivotal design choice to enable a minimal application programming interface (API) that is natural to dynamical systems. The core component of Pressio is a C++11 header-only library that leverages generic programming to support applications with arbitrary data types and arbitrarily complex programming models. This is complemented with Python bindings to expose these C++ functionalities to Python users with negligible overhead and no user-required binding code. We discuss the distinguishing characteristics of Pressio relative to existing model-reduction libraries, outline its key design features, describe how the user interacts with it, and present two test cases---including one with over 20 million degrees of freedom---that highlight the performance results of Pressio and illustrate the breath of problems that can be addressed with it.", "venue": "ArXiv", "authors": ["Francesco  Rizzi", "Patrick J. Blonigan", "Kevin T. Carlberg"], "year": 2020, "n_citations": 0}
{"id": 1388786, "s2_id": "966ba7ef865657e19d8b577016e421480634478f", "title": "Adaptive control in roll-forward recovery for extreme scale multigrid", "abstract": "With the increasing number of compute components, failures in future exa-scale computer systems are expected to become more frequent. This motivates the study of novel resilience techniques. Here, we extend a recently proposed algorithm-based recovery method for multigrid iterations by introducing an adaptive control. After a fault, the healthy part of the system continues the iterative solution process, while the solution in the faulty domain is reconstructed by an asynchronous online recovery. The computations in both the faulty and the healthy subdomains must be coordinated in a sensitive way, in particular, both under- and over-solving must be avoided. Both of these waste computational resources and will therefore increase the overall time-to-solution. To control the local recovery and guarantee an optimal recoupling, we introduce a stopping criterion based on a mathematical error estimator. It involves hierarchically weighted sums of residuals within the context of uniformly refined meshes and is well-suited in the context of parallel high-performance computing. The recoupling process is steered by local contributions of the error estimator before the fault. Failure scenarios when solving up to 6.9 \u00d7 1011 unknowns on more than 245,766 parallel processes will be reported on a state-of-the-art peta-scale supercomputer demonstrating the robustness of the method.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Markus  Huber", "Ulrich  R\u00fcde", "Barbara I. Wohlmuth"], "year": 2019, "n_citations": 3}
{"id": 1388840, "s2_id": "3e5cbe7d0a9c7e754091e0d83e821f73d9607735", "title": "A study of vectorization for matrix-free finite element methods", "abstract": "Vectorization is increasingly important to achieve high performance on modern hardware with SIMD instructions. Assembly of matrices and vectors in the finite element method, which is characterized by iterating a local assembly kernel over unstructured meshes, poses difficulties to effective vectorization. Maintaining a user-friendly high-level interface with a suitable degree of abstraction while generating efficient, vectorized code for the finite element method is a challenge for numerical software systems and libraries. In this work, we study cross-element vectorization in the finite element framework Firedrake via code transformation and demonstrate the efficacy of such an approach by evaluating a wide range of matrix-free operators spanning different polynomial degrees and discretizations on two recent CPUs using three mainstream compilers. Our experiments show that our approaches for cross-element vectorization achieve 30% of theoretical peak performance for many examples of practical significance, and exceed 50% for cases with high arithmetic intensities, with consistent speed-up over (intra-element) vectorization restricted to the local assembly kernels.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Tianjiao  Sun", "Lawrence  Mitchell", "Kaushik  Kulkarni", "Andreas  Kl\u00f6ckner", "David A Ham", "Paul HJ Kelly"], "year": 2020, "n_citations": 7}
{"id": 1394805, "s2_id": "cb0a01e3468247b631a5ecc052dcc225cbeeef8c", "title": "Sample programs in C++ for matrix computations in max plus algebra", "abstract": "The main purpose of this paper is to propose five programs in C++ for matrix computations and solving recurrent equations systems with entries in max plus algebra.", "venue": "ArXiv", "authors": ["Mihai  Ivan", "Gheorghe  Ivan"], "year": 2012, "n_citations": 1}
{"id": 1397946, "s2_id": "d5d43d314b2a56d7e446f9cebaae1a5ca94a63b6", "title": "High performance uncertainty quantification with parallelized multilevel Markov chain Monte Carlo", "abstract": "Numerical models of complex real-world phenomena often necessitate High Performance Computing (HPC). Uncertainties increase problem dimensionality further and pose even greater challenges. We present a parallelization strategy for multilevel Markov chain Monte Carlo, a state-of-the-art, algorithmically scalable Uncertainty Quantification (UQ) algorithm for Bayesian inverse problems, and a new software framework allowing for large-scale parallelism across forward model evaluations and the UQ algorithms themselves. The main scalability challenge presents itself in the form of strong data dependencies introduced by the MLMCMC method, prohibiting trivial parallelization. Our software is released as part of the modular and open-source MIT Uncertainty Quantification Library (MUQ), and can easily be coupled with arbitrary user codes. We demonstrate it using the Distributed and Unified Numerics Environment (DUNE) and the ExaHyPE Engine. The latter provides a realistic, large-scale tsunami model in which we identify the source of a tsunami from buoy-elevation data.", "venue": "SC", "authors": ["Linus  Seelinger", "Anne  Reinarz", "Leonhard  Rannabauer", "Michael  Bader", "Peter  Bastian", "Robert  Scheichl"], "year": 2021, "n_citations": 1}
{"id": 1398950, "s2_id": "caad040d4608a99950930840d103e18b3f700ffb", "title": "Binary Tree Arithmetic with Generalized Constructors", "abstract": "We describe arithmetic computations in terms of operations on some well known free algebras (S1S, S2S and ordered rooted binary trees) while emphasizing the common structure present in all them when seen as isomorphic with the set of natural numbers. \nConstructors and deconstructors seen through an initial algebra semantics are generalized to recursively defined functions obeying similar laws. \nImplementation using Scala's apply and unapply are discussed together with an application to a realistic arbitrary size arithmetic package written in Scala, based on the free algebra of rooted ordered binary trees, which also supports rational number operations through an extension to signed rationals of the Calkin-Wilf bijection.", "venue": "ArXiv", "authors": ["Paul  Tarau"], "year": 2013, "n_citations": 0}
{"id": 1399142, "s2_id": "ce3c2b5cec1d2927d6b1ca6ff2d06152ecb76642", "title": "CosmoMC Installation and Running Guidelines", "abstract": "CosmoMC is a Fortran 95 Markov-Chain Monte-Carlo (MCMC) engine to explore the cosmological parameter space, plus a Python suite for plotting and presenting results (see this http URL). This document describes the installation of the CosmoMC on a Linux system (Ubuntu 14.04.1 LTS 64-bit version). It is written for those who want to use it in their scientific research but without much training on Linux and the program. Besides a step-by-step installation guide, we also give a brief introduction of how to run the program on both a desktop and a cluster. We share our way to generate the plots that are commonly used in the references of cosmology. For more information, one can refer to the CosmoCoffee forum (this http URL) or contact the authors of this document. Questions and comments would be much appreciated.", "venue": "ArXiv", "authors": ["Ming-Hua  Li", "Ping  Wang", "Zhe  Chang", "Dong  Zhao"], "year": 2014, "n_citations": 1}
{"id": 1399454, "s2_id": "cd8f2054dc79466f5378fec40370d468a420ff41", "title": "Having Fun with Lambert W(x) Function", "abstract": "This short note presents the Lambert W(x) function and its possible application in the framework of physics related to the Pierre Auger Observatory. The actual numerical implementation in C++ consists of Halley's and Fritsch's iteration with branch-point expansion, asymptotic series and rational fits as initial approximations.", "venue": "ArXiv", "authors": ["Darko  Veberic"], "year": 2010, "n_citations": 48}
{"id": 1400143, "s2_id": "4f6d416003a06e16fb4b3cfef5ec9966170524d6", "title": "Exploiting Symmetry in Tensors for High Performance: Multiplication with Symmetric Tensors", "abstract": "Symmetric tensor operations arise in a wide variety of computations. However, the benefits of exploiting symmetry in order to reduce storage and computation is in conflict with a desire to simplify memory access patterns. In this paper, we propose a blocked data structure (blocked compact symmetric storage) wherein we consider the tensor by blocks and store only the unique blocks of a symmetric tensor. We propose an algorithm by blocks, already shown of benefit for matrix computations, that exploits this storage format by utilizing a series of temporary tensors to avoid redundant computation. Further, partial symmetry within temporaries is exploited to further avoid redundant storage and redundant computation. A detailed analysis shows that, relative to storing and computing with tensors without taking advantage of symmetry and partial symmetry, storage requirements are reduced by a factor of $ O( m! )$ and computational requirements by a factor of $O( (m+1)!/2^m )$, where $ m $ is the order of the tensor...", "venue": "SIAM J. Sci. Comput.", "authors": ["Martin D. Schatz", "Tze Meng Low", "Robert A. van de Geijn", "Tamara G. Kolda"], "year": 2014, "n_citations": 39}
{"id": 1420031, "s2_id": "d33a1ebe45a121cdd5857de59e6535cd56c6986c", "title": "Numerical integration in arbitrary-precision ball arithmetic", "abstract": "We present an implementation of arbitrary-precision numerical integration with rigorous error bounds in the Arb library. Rapid convergence is ensured for piecewise complex analytic integrals by use of the Petras algorithm, which combines adaptive bisection with adaptive Gaussian quadrature where error bounds are determined via complex magnitudes without evaluating derivatives. The code is general, easy to use, and efficient, often outperforming existing non-rigorous software.", "venue": "ICMS", "authors": ["Fredrik  Johansson"], "year": 2018, "n_citations": 14}
{"id": 1428146, "s2_id": "cd61cf150ecaf9a0270baedb1d8cab3cbc54b2ee", "title": "Modernizing PHCpack through phcpy", "abstract": "PHCpack is a large software package for solving systems of polynomial equations. The executable phc is menu driven and file oriented. This paper describes the development of phcpy, a Python interface to PHCpack. Instead of navigating through menus, users of phcpy solve systems in the Python shell or via scripts. Persistent objects replace intermediate files.", "venue": "ArXiv", "authors": ["Jan  Verschelde"], "year": 2013, "n_citations": 16}
{"id": 1429200, "s2_id": "fe3194bd2dcd5551e1b067ef45b2ed9bc68083cd", "title": "Accelerating wave-propagation algorithms with adaptive mesh refinement using the Graphics Processing Unit (GPU)", "abstract": "Clawpack is a library for solving nonlinear hyperbolic partial differential equations using high-resolution finite volume methods based on Riemann solvers and limiters. It supports Adaptive Mesh Refinement (AMR), which is essential in solving multi-scale problems. Recently, we added capabilities to accelerate the code by using the Graphics Process Unit (GPU). Routines that manage CPU and GPU AMR data and facilitate the execution of GPU kernels are added. Customized and CPU thread-safe memory managers are designed to manage GPU and CPU memory pools, which is essential in eliminating the overhead of memory allocation and de-allocation. A global reduction is conducted every time step for dynamically adjusting the time step based on Courant number restrictions. Some small GPU kernels are merged into bigger kernels, which greatly reduces kernel launching overhead. A speed-up between $2$ and $3$ for the total running time is observed in an acoustics benchmark problem.", "venue": "ArXiv", "authors": ["Xinsheng  Qin", "Randall J. LeVeque", "Michael R. Motley"], "year": 2018, "n_citations": 2}
{"id": 1433862, "s2_id": "dd8e00de4f110912287166378edcb891de488026", "title": "An Asymptotic Cost Model for Autoscheduling Sparse Tensor Programs", "abstract": "While loop reordering and fusion can make big impacts on the constant-factor performance of dense tensor programs, the effects on sparse tensor programs are asymptotic, often leading to orders of magnitude performance differences in practice. Sparse tensors also introduce a choice of compressed storage formats that can have asymptotic effects. Research into sparse tensor compilers has led to simplified languages that express these tradeoffs, but the user is expected to provide a schedule that makes the decisions. This is challenging because schedulers must anticipate the interaction between sparse formats, loop structure, potential sparsity patterns, and the compiler itself. Automating this decision making process stands to finally make sparse tensor compilers accessible to end users. We present, to the best of our knowledge, the first automatic asymptotic scheduler for sparse tensor programs. We provide an approach to abstractly represent the asymptotic cost of schedules and to choose between them. We narrow down the search space to a manageably small \u201cPareto frontier\u201d of asymptotically undominated kernels. We test our approach by compiling these kernels with the TACO sparse tensor compiler and comparing them with those generated with the default TACO schedules. Our results show that our approach reduces the scheduling space by orders of magnitude and that the generated kernels perform asymptotically better than those generated using the default schedules.", "venue": "ArXiv", "authors": ["Peter  Ahrens", "Fredrik  Kjolstad", "Saman  Amarasinghe"], "year": 2021, "n_citations": 0}
{"id": 1436166, "s2_id": "1982e3e667540117c8395333c6524107c814ffd8", "title": "Revisiting Matrix Product on Master-Worker Platforms", "abstract": "This paper is aimed at designing efficient parallel matrix-product algorithms for homogeneous master-worker platforms. While matrix-product is well-understood for homogeneous 2D-arrays of processors (e.g., Cannon algorithm and ScaLAPACK outer product algorithm), there are two key hypotheses that render our work original and innovative: 1) centralized data: we assume that all matrix files originate from, and must be returned to, the master. The master distributes both data and computations to the workers (while in ScaLAPACK, input and output matrices are initially distributed among participating resources). Typically, our approach is useful in the context of speeding up MATLAB or SCILAB clients running on a server (which acts as the master and initial repository of files). 2) Limited memory: because we investigate the parallelization of large problems, we cannot assume that full matrix panels can be stored in the worker memories and re-used for subsequent updates (as in ScaLAPACK). The amount of memory available in each worker is expressed as a given number of buffers, where a buffer can store a square block of matrix elements. These square blocks are chosen so as to harness the power of level 3 BIAS routines; they are of size 80 or 100 on most platforms. We have devised efficient algorithms for resource selection (deciding which workers to enroll) and communication ordering (both for input and result messages), and we report a set of MPI experiments conducted on a platform at the University of Tennessee.", "venue": "2007 IEEE International Parallel and Distributed Processing Symposium", "authors": ["Jack J. Dongarra", "Jean-Francois  Pineau", "Yves  Robert", "Zhiao  Shi", "Fr\u00e9d\u00e9ric  Vivien"], "year": 2007, "n_citations": 11}
{"id": 1446560, "s2_id": "0b6c613d118e7093d9a8fab89d4d4760076e7ffd", "title": "Devito: Towards a Generic Finite Difference DSL Using Symbolic Python", "abstract": "Domain specific languages (DSL) have been used in a variety of fields to express complex scientific problems in a concise manner and provide automated performance optimization for a range of computational architectures. As such DSLs provide a powerful mechanism to speed up scientific Python computation that goes beyond traditional vectorization and pre-compilation approaches, while allowing domain scientists to build applications within the comforts of the Python software ecosystem. In this paper we present Devito, a new finite difference DSL that provides optimized stencil computation from high-level problem specifications based on symbolic Python expressions. We demonstrate Devito's symbolic API and performance advantages over traditional Python acceleration methods before highlighting its use in the scientific context of seismic inversion problems.", "venue": "2016 6th Workshop on Python for High-Performance and Scientific Computing (PyHPC)", "authors": ["Michael  Lange", "Navjot  Kukreja", "Mathias  Louboutin", "Fabio  Luporini", "Felippe  Vieira", "Vincenzo  Pandolfo", "Paulius  Velesko", "Paulius  Kazakas", "Gerard  Gorman"], "year": 2016, "n_citations": 23}
{"id": 1450749, "s2_id": "cce76daf5b91002dd245be9680e556162a08c4af", "title": "Using RngStreams for parallel random number generation in C++ and R", "abstract": "The RngStreams software package provides one viable solution to the problem of creating independent random number streams for simulations in parallel processing environments. Techniques are presented for effectively using RngStreams with C++ programs that are parallelized via OpenMP or MPI. Ways to access the backbone generator from RngStreams in R through the parallel and rstream packages are also described. The ideas in the paper are illustrated with both a simple running example and a Monte Carlo integration application.", "venue": "Comput. Stat.", "authors": ["Andrew T. Karl", "Randy  Eubank", "Jelena  Milovanovic", "Mark  Reiser", "Dennis  Young"], "year": 2014, "n_citations": 7}
{"id": 1461471, "s2_id": "4ea5f03dd4b0443993c7551fd9fe9fe6bdf27330", "title": "The DUNE Framework: Basic Concepts and Recent Developments", "abstract": "This paper presents the basic concepts and the module structure of the Distributed and Unified Numerics Environment and reflects on recent developments and general changes that happened since the release of the first Dune version in 2007 and the main papers describing that state [1, 2]. This discussion is accompanied with a description of various advanced features, such as coupling of domains and cut cells, grid modifications such as adaptation and moving domains, high order discretizations and node level performance, non-smooth multigrid methods, and multiscale methods. A brief discussion on current and future development directions of the framework concludes the paper.", "venue": "Comput. Math. Appl.", "authors": ["Peter  Bastian", "Markus  Blatt", "Andreas  Dedner", "Nils-Arne  Dreier", "Christian  Engwer", "Ren\u00e9  Fritze", "Carsten  Gr\u00e4ser", "Dominic  Kempf", "Robert  Kl\u00f6fkorn", "Mario  Ohlberger", "Oliver  Sander"], "year": 2021, "n_citations": 25}
{"id": 1462731, "s2_id": "22de1a3b6687af08956cf7cc4e5c94b9cf1b9ea6", "title": "The myth of equidistribution for high-dimensional simulation", "abstract": "A pseudo-random number generator (RNG) might be used to generate w-bit random samples in d dimensions if the number of state bits is at least dw. Some RNGs perform better than others and the concept of equidistribution has been introduced in the literature in order to rank different RNGs. We define what it means for a RNG to be (d, w)-equidistributed, and then argue that (d, w)-equidistribution is not necessarily a desirable property.", "venue": "ArXiv", "authors": ["Richard P. Brent"], "year": 2010, "n_citations": 3}
{"id": 1463510, "s2_id": "f340d1da09a5c0e60c7bf99d8764c21bd0e8b534", "title": "Chebyshev Filter Diagonalization on Modern Manycore Processors and GPGPUs", "abstract": "Chebyshev filter diagonalization is well established in quantum chemistry and quantum physics to compute bulks of eigenvalues of large sparse matrices. Choosing a block vector implementation, we investigate optimization opportunities on the new class of high-performance compute devices featuring both high-bandwidth and low-bandwidth memory. We focus on the transparent access to the full address space supported by both architectures under consideration: Intel Xeon Phi \"Knights Landing\" and Nvidia \"Pascal.\" \nWe propose two optimizations: (1) Subspace blocking is applied for improved performance and data access efficiency. We also show that it allows transparently handling problems much larger than the high-bandwidth memory without significant performance penalties. (2) Pipelining of communication and computation phases of successive subspaces is implemented to hide communication costs without extra memory traffic. \nAs an application scenario we use filter diagonalization studies on topological insulator materials. Performance numbers on up to 512 nodes of the OakForest-PACS and Piz Daint supercomputers are presented, achieving beyond 100 Tflop/s for computing 100 inner eigenvalues of sparse matrices of dimension one billion.", "venue": "ISC", "authors": ["Moritz  Kreutzer", "Georg  Hager", "Dominik  Ernst", "Holger  Fehske", "Alan R. Bishop", "Gerhard  Wellein"], "year": 2018, "n_citations": 15}
{"id": 1466450, "s2_id": "f3c302bed49ac810b4bd60c2cf83375beae8ea4b", "title": "Semi-analytic integration for a parallel space-time boundary element method modeling the heat equation", "abstract": "The presented paper concentrates on the boundary element method (BEM) for the heat equation in three spatial dimensions. In particular, we deal with tensor product space-time meshes allowing for quadrature schemes analytic in time and numerical in space. The spatial integrals can be treated by standard BEM techniques known from three dimensional stationary problems. The contribution of the paper is twofold. First, we provide temporal antiderivatives of the heat kernel necessary for the assembly of BEM matrices and the evaluation of the representation formula. Secondly, the presented approach has been implemented in a publicly available library besthea allowing researchers to reuse the formulae and BEM routines straightaway. The results are validated by numerical experiments in an HPC environment.", "venue": "Computers & Mathematics with Applications", "authors": ["Jan  Zapletal", "Raphael  Watschinger", "Gunther  Of", "Michal  Merta"], "year": 2021, "n_citations": 3}
{"id": 1479923, "s2_id": "181851db98ce958536913697ae4f427b1e4853ad", "title": "The Ocean Tensor Package", "abstract": "Matrix and tensor operations form the basis of a wide range of fields and applications, and in many cases constitute a substantial part of the overall computational complexity. The ability of general-purpose GPUs to speed up many of these operations and enable others has resulted in a widespread adaptation of these devices. In order for tensor operations to take full advantage of the computational power, specialized software is required, and currently there exist several packages (predominantly in the area of deep learning) that incorporate tensor operations on both CPU and GPU. Nevertheless, a stand-alone framework that supports general tensor operations is still missing. In this paper we fill this gap and propose the Ocean Tensor Library: a modular tensor-support package that is designed to serve as a foundational layer for applications that require dense tensor operations on a variety of device types. The API is carefully designed to be powerful, extensible, and at the same time easy to use. The package is available as open source.", "venue": "ArXiv", "authors": ["Ewout van den Berg"], "year": 2018, "n_citations": 2}
{"id": 1480916, "s2_id": "1bf98735d941c19c227e48ce02c1ec62787ee2d4", "title": "ForestClaw: Hybrid forest-of-octrees AMR for hyperbolic conservation laws", "abstract": "We present a new hybrid paradigm for parallel adaptive mesh refinement (AMR) that combines the scalability and lightweight architecture of tree-based AMR with the computational efficiency of patch-based solvers for hyperbolic conservation laws. The key idea is to interpret each leaf of the AMR hierarchy as one uniform compute patch in $\\sR^d$ with $m^d$ degrees of freedom, where $m$ is customarily between 8 and 32. Thus, computation on each patch can be optimized for speed, while we inherit the flexibility of adaptive meshes. In our work we choose to integrate with the p4est AMR library since it allows us to compose the mesh from multiple mapped octrees and enables the cubed sphere and other nontrivial multiblock geometries. We describe aspects of the parallel implementation and close with scalings for both MPI-only and OpenMP/MPI hybrid runs, where the largest MPI run executes on 16,384 CPU cores.", "venue": "PARCO", "authors": ["Carsten  Burstedde", "Donna A. Calhoun", "Kyle T. Mandli", "Andy R. Terrel"], "year": 2013, "n_citations": 26}
{"id": 1481265, "s2_id": "12bac6f59d7048baa354fdb72c2ebe17d36de2e4", "title": "Rectangular full packed format for cholesky's algorithm: factorization, solution, and inversion", "abstract": "We describe a new data format for storing triangular, symmetric, and Hermitian matrices called Rectangular Full Packed Format (RFPF). The standard two-dimensional arrays of Fortran and C (also known as full format) that are used to represent triangular and symmetric matrices waste nearly half of the storage space but provide high performance via the use of Level 3 BLAS. Standard packed format arrays fully utilize storage (array space) but provide low performance as there is no Level 3 packed BLAS. We combine the good features of packed and full storage using RFPF to obtain high performance via using Level 3 BLAS as RFPF is a standard full-format representation. Also, RFPF requires exactly the same minimal storage as packed the format. Each LAPACK full and/or packed triangular, symmetric, and Hermitian routine becomes a single new RFPF routine based on eight possible data layouts of RFPF. This new RFPF routine usually consists of two calls to the corresponding LAPACK full-format routine and two calls to Level 3 BLAS routines. This means no new software is required. As examples, we present LAPACK routines for Cholesky factorization, Cholesky solution, and Cholesky inverse computation in RFPF to illustrate this new work and to describe its performance on several commonly used computer platforms. Performance of LAPACK full routines using RFPF versus LAPACK full routines using the standard format for both serial and SMP parallel processing is about the same while using half the storage. Performance gains are roughly one to a factor of 43 for serial and one to a factor of 97 for SMP parallel times faster using vendor LAPACK full routines with RFPF than with using vendor and/or reference packed routines.", "venue": "TOMS", "authors": ["Fred G. Gustavson", "Jerzy  Wasniewski", "Jack J. Dongarra", "Julien  Langou"], "year": 2010, "n_citations": 20}
{"id": 1481672, "s2_id": "5e36934ce0d792cb8977d253661ebb9bfebe7200", "title": "Transferring a symbolic polynomial expression from \\emph{Mathematica} to \\emph{Matlab}", "abstract": "A \\emph{Mathematica} Notebook is presented which allows for the transfer or any kind of polynomial expression to \\emph{Matlab}. The output is formatted in such a way that \\emph{Matlab} routines such as \"Root\" can be readily implemented. Once the Notebook has been executed, only one copy-paste operation in necessary.", "venue": "ArXiv", "authors": ["A.  Bret"], "year": 2010, "n_citations": 1}
{"id": 1483634, "s2_id": "f8da563ba32d2f77591228997e51d2497dbc4717", "title": "Triangular decomposition of semi-algebraic systems", "abstract": "Regular chains and triangular decompositions are fundamental and well-developed tools for describing the complex solutions of polynomial systems. This paper proposes adaptations of these tools focusing on solutions of the real analogue: semi-algebraic systems. We show that any such system can be decomposed into finitely many regular semi-algebraic systems. We propose two specifications (full and lazy) of such a decomposition and present corresponding algorithms. Under some simplifying assumptions, the lazy decomposition can be computed in singly exponential time w.r.t. the number of variables. We have implemented our algorithms and present experimental results illustrating their effectiveness.", "venue": "J. Symb. Comput.", "authors": ["Changbo  Chen", "James H. Davenport", "John P. May", "Marc Moreno Maza", "Bican  Xia", "Rong  Xiao"], "year": 2013, "n_citations": 67}
{"id": 1483884, "s2_id": "df1ae2331588a6b232a76e5ff3028e696eb4d86c", "title": "Solving large number of non-stiff, low-dimensional ordinary differential equation systems on GPUs and CPUs: performance comparisons of MPGOS, ODEINT and DifferentialEquations.jl", "abstract": "In this paper, the performance characteristics of different solution techniques and program packages to solve a large number of independent ordinary differential equation systems is examined. The employed hardware are an Intel Core i7-4820K CPU with 30.4 GFLOPS peak double-precision performance per cores and an Nvidia GeForce Titan Black GPU that has a total of 1707 GFLOPS peak double-precision performance. The tested systems (Lorenz equation, Keller--Miksis equation and a pressure relief valve model) are non-stiff and have low dimension. Thus, the performance of the codes are not limited by memory bandwidth, and Runge--Kutta type solvers are efficient and suitable choices. The tested program packages are MPGOS written in C++ and specialised only for GPUs; ODEINT implemented in C++, which supports execution on both CPUs and GPUs; finally, DifferentialEquations.jl written in Julia that also supports execution on both CPUs and GPUs. Using GPUs, the program package MPGOS is superior. For CPU computations, the ODEINT program package has the best performance.", "venue": "ArXiv", "authors": ["D\u00e1niel  Nagy", "Lambert  Plavecz", "Ferenc  Heged\u00fcs"], "year": 2020, "n_citations": 1}
{"id": 1488089, "s2_id": "1aea00b615ba3ca6c0a9c94022b467908aac7399", "title": "Automatic Deduction in Dynamic Geometry using Sage", "abstract": "We present a symbolic tool that provides robust algebraic methods to handle automatic deduction tasks for a dynamic geometry construction. The main prototype has been developed as two different worksheets for the open source computer algebra system Sage, corresponding to two different ways of coding a geometric construction. In one worksheet, diagrams constructed with the open source dynamic geometry system GeoGebra are accepted. In this worksheet, Groebner bases are used to either compute the equation of a geometric locus in the case of a locus construction or to determine the truth of a general geometric statement included in the GeoGebra construction as a boolean variable. In the second worksheet, locus constructions coded using the common file format for dynamic geometry developed by the Intergeo project are accepted for computation. The prototype and several examples are provided for testing. Moreover, a third Sage worksheet is presented in which a novel algorithm to eliminate extraneous parts in symbolically computed loci has been implemented. The algorithm, based on a recent work on the Groebner cover of parametric systems, identifies degenerate components and extraneous adherence points in loci, both natural byproducts of general polynomial algebraic methods. Detailed examples are discussed.", "venue": "ThEdu", "authors": ["Francisco  Botana", "Miguel A. Ab\u00e1nades"], "year": 2011, "n_citations": 5}
{"id": 1488291, "s2_id": "0c6ab1d7631aec1faf4493352293bf19d06ee40f", "title": "A tuned and scalable fast multipole method as a preeminent algorithm for exascale systems", "abstract": "Among the algorithms that are likely to play a major role in future exascale computing, the fast multipole method (fmm) appears as a rising star. Our previous recent work showed scaling of an fmm on gpu clusters, with problem sizes of the order of billions of unknowns. That work led to an extremely parallel fmm, scaling to thousands of gpus or tens of thousands of cpus. This paper reports on a campaign of performance tuning and scalability studies using multi-core cpus, on the Kraken supercomputer. All kernels in the fmm were parallelized using OpenMP, and a test using 107 particles randomly distributed in a cube showed 78% efficiency on 8 threads. Tuning of the particle-to-particle kernel using single instruction multiple data (SIMD) instructions resulted in 4 \u00d7 speed-up of the overall algorithm on single-core tests with 103\u2013107 particles. Parallel scalability was studied in both strong and weak scaling. The strong scaling test used 108 particles and resulted in 93% parallel efficiency on 2048 processes for the non-SIMD code and 54% for the SIMD-optimized code (which was still 2 \u00d7 faster). The weak scaling test used 106 particles per process, and resulted in 72% efficiency on 32,768 processes, with the largest calculation taking about 40 seconds to evaluate more than 32 billion unknowns. This work builds up evidence for our view that fmm is poised to play a leading role in exascale computing, and we end the paper with a discussion of the features that make it a particularly favorable algorithm for the emerging heterogeneous and massively parallel architectural landscape. The code is open for unrestricted use under the MIT license.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Rio  Yokota", "Lorena A. Barba"], "year": 2012, "n_citations": 63}
{"id": 1490840, "s2_id": "69893e4a20106afc6d2e9689af7f3bf0ef4bdf8c", "title": "Optimised finite difference computation from symbolic equations", "abstract": "Domain-specific high-productivity environments are playing an increasingly important role in scientific computing due to the levels of abstraction and automation they provide. In this paper we introduce Devito, an open-source domain-specific framework for solving partial differential equations from symbolic problem definitions by the finite difference method. We highlight the generation and automated execution of highly optimized stencil code from only a few lines of high-level symbolic Python for a set of scientific equations, before exploring the use of Devito operators in seismic inversion problems.", "venue": "ArXiv", "authors": ["Michael  Lange", "Navjot  Kukreja", "Fabio  Luporini", "Mathias  Louboutin", "Charles  Yount", "Jan  H\u00fcckelheim", "Gerard  Gorman"], "year": 2017, "n_citations": 9}
{"id": 1498022, "s2_id": "7fdb7df9e8e5c19f636ac0624a7b3addf1c67cd4", "title": "Eigen-AD: Algorithmic Differentiation of the Eigen Library", "abstract": "In this work we present useful techniques and possible enhancements when applying an Algorithmic Differentiation (AD) tool to the linear algebra library Eigen using our in-house AD by overloading (AD-O) tool as a case study. After outlining performance and feasibility issues when calculating derivatives for the official Eigen release, we propose Eigen-AD, which enables different optimization options for an AD-O tool by providing add-on modules for Eigen. The range of features includes a better handling of expression templates for general performance improvements as well as implementations of symbolically derived expressions for calculating derivatives of certain core operations. The software design allows an AD-O tool to provide specializations to automatically include symbolic operations and thereby keep the look and feel of plain AD by overloading. As a showcase, is provided with such a module and its significant performance improvements are validated by benchmarks.", "venue": "ICCS", "authors": ["Patrick  Peltzer", "Johannes  Lotz", "Uwe  Naumann"], "year": 2020, "n_citations": 2}
{"id": 1507097, "s2_id": "ca24492a46856221086e77ffe29e8a69e1be0595", "title": "CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs", "abstract": "Deep Neural Networks are becoming increasingly popular in always-on IoT edge devices performing data analytics right at the source, reducing latency as well as energy consumption for data communication. This paper presents CMSIS-NN, efficient kernels developed to maximize the performance and minimize the memory footprint of neural network (NN) applications on Arm Cortex-M processors targeted for intelligent IoT edge devices. Neural network inference based on CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X improvement in energy efficiency.", "venue": "ArXiv", "authors": ["Liangzhen  Lai", "Naveen  Suda", "Vikas  Chandra"], "year": 2018, "n_citations": 155}
{"id": 1507625, "s2_id": "6cb1df37cc67426c31f830f8edf68689cd26744a", "title": "Performance evaluation of multiple precision matrix multiplications using parallelized Strassen and Winograd algorithms", "abstract": "It is well known that Strassen and Winograd algorithms can reduce the computational costs associated with dense matrix multiplication. We have already shown that they are also very effective for software-based multiple precision floating-point arithmetic environments such as the MPFR/GMP library. In this paper, we show that we can obtain the same effectiveness for double-double (DD) and quadruple-double (QD) environments supported by the QD library, and that parallelization can increase the speed of these multiple precision matrix multiplications. Finally, we demonstrate that our implemented parallelized Strassen and Winograd algorithms can increase the speed of parallelized LU decomposition.", "venue": "JSIAM Lett.", "authors": ["Tomonori  Kouya"], "year": 2016, "n_citations": 4}
{"id": 1512889, "s2_id": "651943e1d09bf3a5e5691ca1ed595c49babd51a7", "title": "GEMMbench: a framework for reproducible and collaborative benchmarking of matrix multiplication", "abstract": "The generic matrix-matrix multiplication (GEMM) is arguably the most popular computational kernel of the 20th century. Yet, surprisingly, no common methodology for evaluating GEMM performance has been established over the many decades of using GEMM for comparing architectures, compilers and ninja-class programmers. \nWe introduce GEMMbench, a framework and methodology for evaluating performance of GEMM implementations. GEMMbench is implemented on top of Collective Knowledge (CK), a lightweight framework for reproducible and collaborative R&D in computer systems. Using CK allows the R&D community to crowdsource hand-written and compiler-generated GEMM implementations and to study their performance across multiple platforms, data sizes and data types. \nOur initial implementation supports hand-written OpenCL kernels operating on matrices consisting of single- and double-precision floating-point values, and producing single or multiple output elements per work-item (via thread coarsening and vectorization).", "venue": "ArXiv", "authors": ["Anton  Lokhmotov"], "year": 2015, "n_citations": 2}
{"id": 1525904, "s2_id": "ef7b42c0df2e13f42ed9a49c0639277b281ade87", "title": "FastAD: Expression Template-Based C++ Library for Fast and Memory-Efficient Automatic Differentiation", "abstract": "Automatic differentiation is a set of techniques to efficiently and accurately compute the derivative of a function represented by a computer program. Existing C++ libraries for automatic differentiation (e.g. Adept, Stan Math Library), however, exhibit large memory consumptions and runtime performance issues. This paper introduces FastAD, a new C++ template library for automatic differentiation, that overcomes all of these challenges in existing libraries by using vectorization, simpler memory management using a fully expression-templatebased design, and other compile-time optimizations to remove some runtime overhead. Benchmarks show that FastAD performs 2-10 times faster than Adept and 2-19 times faster than Stan across various test cases including a few real-world examples.", "venue": "ArXiv", "authors": ["James  Yang"], "year": 2021, "n_citations": 0}
{"id": 1535399, "s2_id": "0b3567c8309bf8a0cc5e83d118e0ff5fd0e48160", "title": "Open Traffic Models - A framework for hybrid simulation of transportation networks", "abstract": "This paper introduces a new approach to hybrid traffic modeling, along with its implementation in software. The software allows modelers to assign traffic models to individual links in a network. Each model implements a series of methods, refered to as the modeling interface. These methods are used by the program to exchange information between adjacent models. Traffic controllers are implemented in a similar manner. The paper outlines the important components of the method: the network description, the description of demands, and the modeling and control interfaces. We include tests demonstrating the propagation of congestion between pairs of macroscpoic, mesoscopic, and microscopic models. Open Traffic Models is an open source implementation of these concepts, and is available at this https URL.", "venue": "ArXiv", "authors": ["Gabriel  Gomes"], "year": 2019, "n_citations": 2}
{"id": 1539015, "s2_id": "b5f8dce6dd718ebfbaf53bab4548cb7bdc9b833c", "title": "Accelerated Polynomial Evaluation and Differentiation at Power Series in Multiple Double Precision", "abstract": "The problem is to evaluate a polynomial in several variables and its gradient at a power series truncated to some finite degree with multiple double precision arithmetic. To compensate for the cost overhead of multiple double precision and power series arithmetic, data parallel algorithms for general purpose graphics processing units are presented. The reverse mode of algorithmic differentiation is organized into a massively parallel computation of many convolutions and additions of truncated power series. Experimental results demonstrate that teraflop performance is obtained in deca double precision with power series truncated at degree 152. The algorithms scale well for increasing precision and increasing degrees.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Jan  Verschelde"], "year": 2021, "n_citations": 1}
{"id": 1540116, "s2_id": "39d765f8cfcf83e238496bc1e4b809996af06737", "title": "LIBOPT - An environment for testing solvers on heterogeneous collections of problems - Version 1.0", "abstract": "The Libopt environment is both a methodology and a set of tools that can be used for testing, comparing, and profiling solvers on problems belonging to various collections. These collections can be heterogeneous in the sense that their problems can have common features that differ from one collection to the other. Libopt brings a unified view on this composite world by offering, for example, the possibility to run any solver on any problem compatible with it, using the same Unix/Linux command. The environment also provides tools for comparing the results obtained by solvers on a specified set of problems. Most of the scripts going with the Libopt environment have been written in Perl.", "venue": "ArXiv", "authors": ["Jean Charles Gilbert", "Xavier  Jonsson"], "year": 2007, "n_citations": 25}
{"id": 1543059, "s2_id": "c0ba19ccfc1de603710cd8bc476423e00c98acca", "title": "Merlion: A Machine Learning Library for Time Series", "abstract": "We introduce Merlion1, an open-source machine learning library for time series. It features a unified interface for many commonly used models and datasets for anomaly detection and forecasting on both univariate and multivariate time series, along with standard pre/postprocessing layers. It has several modules to improve ease-of-use, including visualization, anomaly score calibration to improve interpetability, AutoML for hyperparameter tuning and model selection, and model ensembling. Merlion also provides a unique evaluation framework that simulates the live deployment and re-training of a model in production. This library aims to provide engineers and researchers a one-stop solution to rapidly develop models for their specific time series needs and benchmark them across multiple time series datasets. In this technical report, we highlight Merlion\u2019s architecture and major functionalities, and we report benchmark numbers across different baseline models and ensembles.", "venue": "ArXiv", "authors": ["Aadyot  Bhatnagar", "Paul  Kassianik", "Chenghao  Liu", "Tian  Lan", "Wenzhuo  Yang", "Rowan  Cassius", "Doyen  Sahoo", "Devansh  Arpit", "Sri  Subramanian", "Gerald  Woo", "Amrita  Saha", "Arun Kumar Jagota", "Gokulakrishnan  Gopalakrishnan", "Manpreet  Singh", "K C Krithika", "Sukumar  Maddineni", "Daeki  Cho", "Bo  Zong", "Yingbo  Zhou", "Caiming  Xiong", "Silvio  Savarese", "Steven  Hoi", "Huan  Wang"], "year": 2021, "n_citations": 4}
{"id": 1544754, "s2_id": "0394388258a0fc224f9b8755bcbf6961ca366f61", "title": "The deal.II library, Version 9.1", "abstract": "Abstract This paper provides an overview of the new features of the finite element library deal.II, version 9.1.", "venue": "J. Num. Math.", "authors": ["Daniel  Arndt", "Wolfgang  Bangerth", "Thomas C. Clevenger", "Denis  Davydov", "Marc  Fehling", "Daniel  Garcia-Sanchez", "Graham  Harper", "Timo  Heister", "Luca  Heltai", "Martin  Kronbichler", "Ross Maguire Kynch", "Matthias  Maier", "Jean-Paul  Pelteret", "Bruno  Turcksin", "David  Wells"], "year": 2019, "n_citations": 135}
{"id": 1550352, "s2_id": "c3c0cb1aca6c18284b5fda1d6eb27183384ac272", "title": "On Parallel Solution of Sparse Triangular Linear Systems in CUDA", "abstract": "The acceleration of sparse matrix computations on modern many-core processors, such as the graphics processing units (GPUs), has been recognized and studied over a decade. Significant performance enhancements have been achieved for many sparse matrix computational kernels such as sparse matrix-vector products and sparse matrix-matrix products. Solving linear systems with sparse triangular structured matrices is another important sparse kernel as demanded by a variety of scientific and engineering applications such as sparse linear solvers. However, the development of efficient parallel algorithms in CUDA for solving sparse triangular linear systems remains a challenging task due to the inherently sequential nature of the computation. In this paper, we will revisit this problem by reviewing the existing level-scheduling methods and proposing algorithms with self-scheduling techniques. Numerical results have indicated that the CUDA implementations of the proposed algorithms can outperform the state-of-the-art solvers in cuSPARSE by a factor of up to $2.6$ for structured model problems and general sparse matrices.", "venue": "ArXiv", "authors": ["Ruipeng  Li"], "year": 2017, "n_citations": 8}
{"id": 1553088, "s2_id": "027c5e87320de38502ad0765cb633858aff2b94c", "title": "Portable high-order finite element kernels I: Streaming Operations", "abstract": "This paper is devoted to the development of highly efficient kernels performing vector operations relevant in linear system solvers. In particular, we focus on the low arithmetic intensity operations (i.e., streaming operations) performed within the conjugate gradient iterative method, using the parameters specified in the CEED benchmark problems for high-order hexahedral finite elements. We propose a suite of new Benchmark Streaming tests to focus on the distinct streaming operations which must be performed. We implemented these new tests using the OCCA abstraction framework to demonstrate portability of these streaming operations on different GPU architectures, and propose a simple performance model for such kernels which can accurately capture data movement rates as well as kernel launch costs.", "venue": "ArXiv", "authors": ["Noel  Chalmers", "Tim  Warburton"], "year": 2020, "n_citations": 1}
{"id": 1557343, "s2_id": "bf3d89e4bbe7a3eed2f9667aa125ebc17c76f7d9", "title": "A Hermite-like basis for faster matrix-free evaluation of interior penalty discontinuous Galerkin operators", "abstract": "This work proposes a basis for improved throughput of matrix-free evaluation of discontinuous Galerkin symmetric interior penalty discretizations on hexahedral elements. The basis relies on ideas of Hermite polynomials. It is used in a fully discontinuous setting not for higher order continuity but to minimize the effective stencil width, namely to limit the neighbor access of an element to one data point for the function value and one for the derivative. The basis is extended to higher orders with nodal contributions derived from roots of Jacobi polynomials and extended to multiple dimensions with tensor products, which enable the use of sum factorization. The beneficial effect of the reduced data access on modern processors is shown. Furthermore, the viability of the basis in the context of multigrid solvers is analyzed. While a plain point-Jacobi approach is less efficient than with the best nodal polynomials, a basis change via sum-factorization techniques enables the combination of the fast matrix-vector products with effective multigrid constituents. The basis change is essentially for free on modern hardware because these computations can be hidden behind the cost of the data access.", "venue": "ArXiv", "authors": ["Martin  Kronbichler", "Katharina  Kormann", "Niklas  Fehn", "Peter  Munch", "Julius  Witte"], "year": 2019, "n_citations": 6}
{"id": 1560320, "s2_id": "040c58928a9ff3d4373f23868313c49b0744643d", "title": "SiRnn: A Math Library for Secure RNN Inference", "abstract": "Complex machine learning (ML) inference algorithms like recurrent neural networks (RNNs) use standard functions from math libraries like exponentiation, sigmoid, tanh, and reciprocal of square root. Although prior work on secure 2-party inference provides specialized protocols for convolutional neural networks (CNNs), existing secure implementations of these math operators rely on generic 2-party computation (2PC) protocols that suffer from high communication. We provide new specialized 2PC protocols for math functions that crucially rely on lookup-tables and mixed-bitwidths to address this performance overhead; our protocols for math functions communicate up to 423\u00d7 less data than prior work. Furthermore, our math implementations are numerically precise, which ensures that the secure implementations preserve model accuracy of cleartext. We build on top of our novel protocols to build SiRnn, a library for end-to-end secure 2-party DNN inference, that provides the first secure implementations of an RNN operating on time series sensor data, an RNN operating on speech data, and a state-of-the-art ML architecture that combines CNNs and RNNs for identifying all heads present in images. Our evaluation shows that SiRnn achieves up to three orders of magnitude of performance improvement when compared to inference of these models using an existing state-of-the-art 2PC framework.", "venue": "2021 IEEE Symposium on Security and Privacy (SP)", "authors": ["Deevashwer  Rathee", "Mayank  Rathee", "G. Rahul Kranti Kiran", "Divya  Gupta", "Rahul  Sharma", "Nishanth  Chandran", "Aseem  Rastogi"], "year": 2021, "n_citations": 3}
{"id": 1563050, "s2_id": "2a9f7033ba7d175a9070c7328549742706bc3558", "title": "Enabling GPU Accelerated Computing in the SUNDIALS Time Integration Library", "abstract": "As part of the Exascale Computing Project (ECP), a recent focus of development efforts for the SUite of Nonlinear and DIfferential/ALgebraic equation Solvers (SUNDIALS) has been to enable GPU-accelerated time integration in scientific applications at extreme scales. This effort has resulted in several new GPU-enabled implementations of core SUNDIALS data structures, support for programming paradigms which are aware of the heterogeneous architectures, and the introduction of utilities to provide new points of flexibility. In this paper, we discuss our considerations, both internal and external, when designing these new features and present the features themselves. We also present performance results for several of the features on the Summit supercomputer and early access hardware for the Frontier supercomputer, which demonstrate negligible performance overhead resulting from the additional infrastructure and significant speedups when using both NVIDIA and AMD GPUs.", "venue": "Parallel Comput.", "authors": ["Cody J. Balos", "David J. Gardner", "Carol S. Woodward", "Daniel R. Reynolds"], "year": 2021, "n_citations": 1}
{"id": 1564511, "s2_id": "56df3ce372f5ffae91777cb716305f1111212625", "title": "An Empirical Analysis of the R Package Ecosystem", "abstract": "In this research, we present a comprehensive, longitudinal empirical summary of the R package ecosystem, including not just CRAN, but also Bioconductor and GitHub. We analyze more than 25,000 packages, 150,000 releases, and 15 million files across two decades, providing comprehensive counts and trends for common metrics across packages, releases, authors, licenses, and other important metadata. We find that the historical growth of the ecosystem has been robust under all measures, with a compound annual growth rate of 29% for active packages, 28% for new releases, and 26% for active maintainers. As with many similar social systems, we find a number of highly right-skewed distributions with practical implications, including the distribution of releases per package, packages and releases per author or maintainer, package and maintainer dependency in-degree, and size per package and release. For example, the top five packages are imported by nearly 25% of all packages, and the top ten maintainers support packages that are imported by over half of all packages. We also highlight the dynamic nature of the ecosystem, recording both dramatic acceleration and notable deceleration in the growth of R. From a licensing perspective, we find a notable majority of packages are distributed under copyleft licensing or omit licensing information entirely. The data, methods, and calculations herein provide an anchor for public discourse and industry decisions related to R and CRAN, serving as a foundation for future research on the R software ecosystem and \"data science\" more broadly.", "venue": "ArXiv", "authors": ["Ethan  Bommarito", "Michael J Bommarito"], "year": 2021, "n_citations": 0}
{"id": 1570138, "s2_id": "6c74ec4a9b1fcee5b987fc07299614750da482b4", "title": "Moore: Interval Arithmetic in C++20", "abstract": "This article presents the Moore library for interval arithmetic in C++20. It gives examples of how the library can be used, and explains the basic principles underlying its design.", "venue": "NAFIPS", "authors": ["Walter F. Mascarenhas"], "year": 2018, "n_citations": 4}
{"id": 1573249, "s2_id": "6fe6e4218037654b9a2a23916f2503444fc70072", "title": "Lectures on Reduce and Maple at UAM I - Mexico", "abstract": "These lectures give a brief introduction to the Computer Algebra systems Reduce and Maple. The aim is to provide a systematic survey of most important commands and concepts. In particular, this includes a discussion of simplification schemes and the handling of simplification and substitution rules (e.g., a Lie Algebra is implemented in Reduce by means of simplification rules). \nAnother emphasis is on the different implementations of tensor calculi and the exterior calculus by Reduce and Maple and their application in Gravitation theory and Differential Geometry. \nI held the lectures at the Universidad Autonoma Metropolitana-Iztapalapa, Departamento de Fisica, Mexico, in November 1999.", "venue": "ArXiv", "authors": ["Marc  Toussaint"], "year": 2001, "n_citations": 2}
{"id": 1575516, "s2_id": "9719a1acdb8f217595fe320570f19d6549157055", "title": "TeXmacs-maxima interface", "abstract": "This tutorial presents features of the new and improved TeXmacs-maxima interface. It is designed for running maxima-5.9.2 from TeXmacs-1.0.5 (or later).", "venue": "ArXiv", "authors": ["A. G. Grozin"], "year": 2005, "n_citations": 2}
{"id": 1578083, "s2_id": "d1cb2a562df8429057040bb2f85487be7536b2a8", "title": "Applying the swept rule for solving explicit partial differential equations on heterogeneous computing systems", "abstract": "Applications that exploit the architectural details of high-performance computing (HPC) systems have become increasingly invaluable in academia and industry over the past two decades. The most important hardware development of the last decade in HPC has been the general purpose graphics processing unit (GPGPU), a class of massively parallel devices that now contributes the majority of computational power in the top 500 supercomputers. As these systems grow, small costs such as latency\u2014due to the fixed cost of memory accesses and communication\u2014accumulate in a large simulation and become a significant barrier to performance. The swept time-space decomposition rule is a communication-avoiding technique for time-stepping stencil update formulas that attempts to reduce latency costs. This work extends the swept rule by targeting heterogeneous, CPU/GPU architectures representing current and future HPC systems. We compare our approach to a naive decomposition scheme with two test equations using an MPI+CUDA pattern on 40 processes over two nodes containing one GPU. The swept rule produces a factor of 1.9 to 23 speedup for the heat equation and a factor of 1.1 to 2.0 speedup for the Euler equations, using the same processors and work distribution, and with the best possible configurations. These results show the potential effectiveness of the swept rule for different equations and numerical schemes on massively parallel compute systems that incur substantial latency costs.", "venue": "The Journal of Supercomputing", "authors": ["Daniel J. Magee", "Anthony S. Walker", "Kyle E. Niemeyer"], "year": 2020, "n_citations": 2}
{"id": 1582167, "s2_id": "fe20d5b27476f6132d6cc59f0fe1e26ac305f38e", "title": "Efficient exascale discretizations: High-order finite element methods", "abstract": "Efficient exploitation of exascale architectures requires rethinking of the numerical algorithms used in many large-scale applications. These architectures favor algorithms that expose ultra fine-grain parallelism and maximize the ratio of floating point operations to energy intensive data movement. One of the few viable approaches to achieve high efficiency in the area of PDE discretizations on unstructured grids is to use matrix-free/partially assembled high-order finite element methods, since these methods can increase the accuracy and/or lower the computational time due to reduced data motion. In this paper we provide an overview of the research and development activities in the Center for Efficient Exascale Discretizations (CEED), a co-design center in the Exascale Computing Project that is focused on the development of next-generation discretization software and algorithms to enable a wide range of finite element applications to run efficiently on future hardware. CEED is a research partnership involving more than 30 computational scientists from two US national labs and five universities, including members of the Nek5000, MFEM, MAGMA and PETSc projects. We discuss the CEED co-design activities based on targeted benchmarks, miniapps and discretization libraries and our work on performance optimizations for large-scale GPU architectures. We also provide a broad overview of research and development activities in areas such as unstructured adaptive mesh refinement algorithms, matrix-free linear solvers, high-order data visualization, and list examples of collaborations with several ECP and external applications.", "venue": "The International Journal of High Performance Computing Applications", "authors": ["Tzanio  Kolev", "Paul  Fischer", "Misun  Min", "Jack  Dongarra", "Jed  Brown", "Veselin  Dobrev", "Tim  Warburton", "Stanimire  Tomov", "Mark S Shephard", "Ahmad  Abdelfattah", "Valeria  Barra", "Natalie  Beams", "Jean-Sylvain  Camier", "Noel  Chalmers", "Yohann  Dudouit", "Ali  Karakus", "Ian  Karlin", "Stefan  Kerkemeier", "Yu-Hsiang  Lan", "David  Medina", "Elia  Merzari", "Aleksandr  Obabko", "Will  Pazner", "Thilina  Rathnayake", "Cameron W Smith", "Lukas  Spies", "Kasia  Swirydowicz", "Jeremy  Thompson", "Ananias  Tomboulides", "Vladimir  Tomov"], "year": 2021, "n_citations": 4}
{"id": 1582694, "s2_id": "1774b0c957c0cc3c20b904279f76287b582d9920", "title": "Math-Aware Search Engines: Physics Applications and Overview", "abstract": "Search engines for equations now exist, which return results matching the query's mathematical meaning or structural presentation. Operating over scientific papers, online encyclopedias, and math discussion forums, their content includes physics, math, and other sciences. They enable physicists to avoid jargon and more easily target mathematical content within and across disciplines. As a natural extension of keyword-based search, they open up a new world for discovering both exact and approximate mathematical solutions; physical systems' analogues and alternative models; and physics' patterns. \nThis review presents the existing math-aware search engines, discusses methods for maximizing their search success, and overviews their math-matching capabilities. Proposed applications to physics are also given, to contribute towards developers' and physicists' exploration of the newly available search horizons.", "venue": "ArXiv", "authors": ["Deanna C. Pineau"], "year": 2016, "n_citations": 6}
{"id": 1582777, "s2_id": "3088233e1c90b238ba040cc1f4957ef140a13cee", "title": "Fully Parallel Mesh I/O using PETSc DMPlex with an Application to Waveform Modeling", "abstract": "Large-scale PDE simulations using high-order finite-element methods on unstructured meshes are an indispensable tool in science and engineering. The widely used open-source PETSc library offers an efficient representation of generic unstructured meshes within its DMPlex module. This paper details our recent implementation of parallel mesh reading and topological interpolation (computation of edges and faces from a cell-vertex mesh) into DMPlex. We apply these developments to seismic wave propagation scenarios on Mars as an example application. The principal motivation is to overcome single-node memory limits and reach mesh sizes which were impossible before. Moreover, we demonstrate that scalability of I/O and topological interpolation goes beyond 12'000 cores, and memory-imposed limits on mesh size vanish.", "venue": "SIAM J. Sci. Comput.", "authors": ["Vaclav  Hapla", "Matthew G. Knepley", "Michael  Afanasiev", "Christian  Boehm", "Martin van Driel", "Lion  Krischer", "Andreas  Fichtner"], "year": 2021, "n_citations": 2}
{"id": 1583394, "s2_id": "70cf3a5c76fdd3f1bf228e8d24e4cc7e5981d6c7", "title": "SIMD Vectorization for the Lennard-Jones Potential with AVX2 and AVX-512 instructions", "abstract": "Abstract This work describes the SIMD vectorization of the force calculation of the Lennard-Jones potential with Intel AVX2 and AVX-512 instruction sets. Since the force-calculation kernel of the molecular dynamics method involves indirect access to memory, the data layout is one of the most important factors in vectorization. We find that the Array of Structures (AoS) with padding exhibits better performance than Structure of Arrays (SoA) with appropriate vectorization and optimizations. In particular, AoS with 512-bit width exhibits the best performance among the architectures. While the difference in performance between AoS and SoA is significant for the vectorization with AVX2, that with AVX-512 is minor. The effect of other optimization techniques, such as software pipelining together with vectorization, is also discussed. We present results for benchmarks on three CPU architectures: Intel Haswell (HSW), Knights Landing (KNL), and Skylake (SKL). The performance gains by vectorization are about 42% on HSW compared with the code optimized without vectorization. On KNL, the hand-vectorized codes exhibit 34% better performance than the codes vectorized automatically by the Intel compiler. On SKL, the code vectorized with AVX2 exhibits slightly better performance than that with vectorized AVX-512.", "venue": "Comput. Phys. Commun.", "authors": ["Hiroshi  Watanabe", "Koh M. Nakagawa"], "year": 2019, "n_citations": 8}
{"id": 1587041, "s2_id": "629618a4188a75e4408aadbbb27c53ae48cb7762", "title": "On a solution to display non-filled-in quaternionic Julia sets", "abstract": "During early 1980s, the so-called `escape time' method, developed to display the Julia sets for complex dynamical systems, was exported to quaternions in order to draw analogous pictures in this wider numerical field. Despite of the fine results in the complex plane, where all topological configurations of Julia sets have been successfully displayed, the `escape time' method fails to render properly the non-filled-in variety of quaternionic Julia sets. So their digital visualisation remained an open problem for several years. Both the solution for extending this old method to non-filled-in quaternionic Julia sets and its implementation into a program are explained here.", "venue": "ArXiv", "authors": ["Alessandro  Rosa"], "year": 2006, "n_citations": 3}
{"id": 1588572, "s2_id": "a99fd224373502953d4c040705a9ab8960d17487", "title": "Open Graphs and Computational Reasoning", "abstract": "We present a form of algebraic reasoning for computational objects which are expressed as graphs. Edges describe the flow of data between primitive operations which are represented by vertices. These graphs have an interface made of half-edges (edges which are drawn with an unconnected end) and enjoy rich compositional principles by connecting graphs along these half-edges. In particular, this allows equations and rewrite rules to be specified between graphs. Particular computational models can then be encoded as an axiomatic set of such rules. Further rules can be derived graphically and rewriting can be used to simulate the dynamics of a computational system, e.g. evaluating a program on an input. Examples of models which can be formalised in this way include traditional electronic circuits as well as recent categorical accounts of quantum information.", "venue": "DCM", "authors": ["Lucas  Dixon", "Ross  Duncan", "Aleks  Kissinger"], "year": 2010, "n_citations": 18}
{"id": 1599822, "s2_id": "fa17c32de5a13e79b5fcd68e8cf719e2dfb8c02a", "title": "A Python extension for the massively parallel multiphysics simulation framework waLBerla", "abstract": "We present a Python extension to the massively parallel HPC simulation toolkit waLBerla. waLBerla is a framework for stencil based algorithms operating on block-structured grids, with the main application field being fluid simulations in complex geometries using the lattice Boltzmann method. Careful performance engineering results in excellent node performance and good scalability to over 400,000 cores. To increase the usability and flexibility of the framework, a Python interface was developed. Python extensions are used at all stages of the simulation pipeline: they simplify and automate scenario setup, evaluation, and plotting. We show how our Python interface outperforms the existing text-file-based configuration mechanism, providing features like automatic nondimensionalization of physical quantities and handling of complex parameter dependencies. Furthermore, Python is used to process and evaluate results while the simulation is running, leading to smaller output files and the possibility to adjust parameters dependent on the current simulation state. C++ data structures are exported such that a seamless interfacing to other numerical Python libraries is possible. The expressive power of Python and the performance of C++ make development of efficient code with low time effort possible.", "venue": "Int. J. Parallel Emergent Distributed Syst.", "authors": ["Martin  Bauer", "Florian  Schornbaum", "Christian  Godenschwager", "Matthias  Markl", "Daniela  Anderl", "Harald  K\u00f6stler", "Ulrich  R\u00fcde"], "year": 2016, "n_citations": 11}
{"id": 1601218, "s2_id": "fc417b3bd41337c5ff08d9c2fb3cc30aae5ef4a3", "title": "OpenSBLI: A framework for the automated derivation and parallel execution of finite difference solvers on a range of computer architectures", "abstract": "Exascale computing will feature novel and potentially disruptive hardware architectures. Exploiting these to their full potential is non-trivial. Numerical modelling frameworks involving finite difference methods are currently limited by the 'static' nature of the hand-coded discretisation schemes and repeatedly may have to be re-written to run efficiently on new hardware. In contrast, OpenSBLI uses code generation to derive the model's code from a high-level specification. Users focus on the equations to solve, whilst not concerning themselves with the detailed implementation. Source-to-source translation is used to tailor the code and enable its execution on a variety of hardware.", "venue": "J. Comput. Sci.", "authors": ["Christian T. Jacobs", "Satya P. Jammy", "Neil D. Sandham"], "year": 2017, "n_citations": 40}
{"id": 1602225, "s2_id": "f41a32130b4d9db7e57fbca691184b894ec36ccb", "title": "TensorDiffEq: Scalable Multi-GPU Forward and Inverse Solvers for Physics Informed Neural Networks", "abstract": "Physics-Informed Neural Networks promise to revolutionize science and engineering practice, by introducing domain-aware deep machine learning models into scientific computation. Several software suites have emerged to make the implementation and usage of these architectures available to the research and industry communities. Here we introduce TensorDiffEq, built on Tensorflow 2.x, which presents an intuitive Keras-like interface for problem domain definition, model definition, and solution of forward and inverse problems using physics-aware deep learning methods. TensorDiffEq takes full advantage of Tensorflow 2.x infrastructure for deployment on multiple GPUs, allowing the implementation of large high-dimensional and complex models. Simultaneously, TensorDiffEq supports the Keras API for custom neural network architecture definitions. In the case of smaller or simpler models, the package allows for rapid deployment on smaller-scale CPU platforms with negligible changes to the implementation scripts. We demonstrate the basic usage and capabilities of TensorDiffEq in solving forward, inverse, and data assimilation problems of varying sizes and levels of complexity. The source code is available at https://github.com/tensordiffeq.", "venue": "ArXiv", "authors": ["Levi D. McClenny", "Mulugeta A. Haile", "Ulisses M. Braga-Neto"], "year": 2021, "n_citations": 2}
{"id": 1605132, "s2_id": "559769cd74cd44cd47eae62eef2be2a7c639463e", "title": "A Feature Complete SPIKE Banded Algorithm and Solver", "abstract": "New features and enhancements for the SPIKE banded solver are presented. Among all the SPIKE algorithm versions, we focus our attention on the recursive SPIKE technique which provides the best trade-off between generality and parallel efficiency, but was known for its lack of flexibility. Its application was essentially limited to power of two number of cores/processors. This limitation is successfully addressed in this paper. In addition, we present a new transpose solve option, a standard feature of most numerical solver libraries which has never been addressed by the SPIKE algorithm so far. A pivoting recursive SPIKE strategy is finally presented as an alternative to non-pivoting scheme for systems with large condition numbers. All these new enhancements participate to create a feature complete SPIKE algorithm and a new black-box SPIKE-OpenMP package that significantly outperforms the performance and scalability obtained with other state-of-the-art banded solvers.", "venue": "ArXiv", "authors": ["Braegan S. Spring", "Eric  Polizzi", "Ahmed H. Sameh"], "year": 2018, "n_citations": 0}
{"id": 1614757, "s2_id": "3127240c7a353f13c67643db4461aa343ea07e10", "title": "Optimizing AIREBO: Navigating the Journey from Complex Legacy Code to High Performance", "abstract": "Despite initiatives to improve the quality of scientific codes, there still is a large presence of legacy code. Such code often needs to implement a lot of functionality under time constrains, sacrificing quality. Additionally, quality is rarely improved by optimizations for new architectures. This development model leads to code that is increasingly difficult to work with. Our suggested solution includes complexity-reducing refactoring and hardware abstraction. We focus on the AIREBO potential from LAMMPS, where the challenge is that any potential kernel is rather large and complex, hindering systematic optimization. This issue is common to codes that model multiple physical phenomena. We present our journey from the C++ port of a previous Fortran code to performance-portable, KNC-hybrid, vectorized, scalable, optimized code supporting full and reduced precision. The journey includes extensive testing that fixed bugs in the original code. Large-scale, full-precision runs sustain speedups of more than 4x (KNL) and 3x (Skylake).", "venue": "ArXiv", "authors": ["Markus  H\u00f6hnerbach", "Paolo  Bientinesi"], "year": 2018, "n_citations": 0}
{"id": 1616944, "s2_id": "de32b00b7460de7bd2907f8328b077816ece4816", "title": "hyper.deal: An Efficient, Matrix-free Finite-element Library for High-dimensional Partial Differential Equations", "abstract": "This work presents the efficient, matrix-free finite-element library hyper.deal for solving partial differential equations in two up to six dimensions with high-order discontinuous Galerkin methods. It builds upon the low-dimensional finite-element library deal.II to create complex low-dimensional meshes and to operate on them individually. These meshes are combined via a tensor product on the fly, and the library provides new special-purpose highly optimized matrix-free functions exploiting domain decomposition as well as shared memory via MPI-3.0 features. Both node-level performance analyses and strong/weak-scaling studies on up to 147,456 CPU cores confirm the efficiency of the implementation. Results obtained with the library hyper.deal are reported for high-dimensional advection problems and for the solution of the Vlasov\u2013Poisson equation in up to six-dimensional phase space.", "venue": "ACM Trans. Math. Softw.", "authors": ["Peter  Munch", "Katharina  Kormann", "Martin  Kronbichler"], "year": 2021, "n_citations": 4}
{"id": 1626895, "s2_id": "e947e9544b1f34ad9bcded88902887d036690dc2", "title": "SUNDIALS Multiphysics+MPIManyVector Performance Testing", "abstract": "In this report we document performance test results on a SUNDIALS-based multiphysics demonstration application. We aim to assess the large-scale parallel performance of new capabilities that have been added to the SUNDIALS suite of time integrators and nonlinear solvers in recent years under funding from both the Exascale Computing Project (ECP) and the Scientific Discovery through Advanced Scientific (SciDAC) program, specifically: (a) SUNDIALS' new MPIManyVector module, that allows extreme flexibility in how a solution \"vector\" is staged on computational resources, (b) ARKode's new multirate integration module, MRIStep, allowing high-order accurate calculations that subcycle \"fast\" processes within \"slow\" ones, (c) SUNDIALS' new flexible linear solver interfaces, that allow streamlined specification of problem-specific linear solvers, and (d) SUNDIALS' new N_Vector additions of \"fused\" vector operations (to increase arithmetic intensity) and separation of reduction operations into \"local\" and \"global\" versions (to reduce latency by combining multiple reductions into a single MPI_Allreduce call). We anticipate that subsequent reports will extend this work to investigate a variety of other new features, including SUNDIALS' generic SUNNonlinearSolver interface and accelerator-enabled N_Vector modules, and upcoming MRIStep extensions to support custom \"fast\" integrators (that leverage problem structure) and IMEX integration of the \"slow\" time scale (to add diffusion).", "venue": "ArXiv", "authors": ["Daniel R. Reynolds", "David J. Gardner", "Cody J. Balos", "Carol S. Woodward"], "year": 2019, "n_citations": 0}
{"id": 1633566, "s2_id": "992f9cc77f50023fa70ad680b6511390024267bc", "title": "Tensor-Tensor Product Toolbox", "abstract": "The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011] is a natural generalization of matrix multiplication. Based on t-product, many operations on matrix can be extended to tensor cases, including tensor SVD, tensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many others. The linear algebraic structure of tensors are similar to the matrix cases. We develop a Matlab toolbox to implement several basic operations on tensors based on t-product. The toolbox is available at this https URL", "venue": "ArXiv", "authors": ["Canyi  Lu"], "year": 2018, "n_citations": 9}
{"id": 1635968, "s2_id": "d25eed556be8d216033133c53bd57d283eb08e15", "title": "Semi-Lagrangian Vlasov simulation on GPUs", "abstract": "Abstract In this paper, our goal is to efficiently solve the Vlasov equation on GPUs. A semi-Lagrangian discontinuous Galerkin scheme is used for the discretization. Such kinetic computations are extremely expensive due to the high-dimensional phase space. The SLDG code, which is publicly available under the MIT license, abstracts the number of dimensions and uses a shared codebase for both GPU and CPU based simulations. We investigate the performance of the implementation on a range of both Tesla (V100, Titan V, K80) and consumer (GTX 1080 Ti) GPUs. Our implementation is typically able to achieve a performance of approximately 470 GB/s on a single GPU and 1600 GB/s on four V100 GPUs connected via NVLink. This results in a speedup of about a factor of ten (comparing a single GPU with a dual socket Intel Xeon Gold node) and approximately a factor of 35 (comparing a single node with and without GPUs). In addition, we investigate the effect of single precision computation on the performance of the SLDG code and demonstrate that a template based dimension independent implementation can achieve good performance regardless of the dimensionality of the problem.", "venue": "Comput. Phys. Commun.", "authors": ["Lukas  Einkemmer"], "year": 2020, "n_citations": 6}
{"id": 1639731, "s2_id": "f1f639c0ca5acbc0d755319c9f7243d5387bd39b", "title": "Solving Polynomial Systems in the Cloud with Polynomial Homotopy Continuation", "abstract": "Polynomial systems occur in many fields of science and engineering. Polynomial homotopy continuation methods apply symbolic-numeric algorithms to solve polynomial systems. We describe the design and implementation of our web interface and reflect on the application of polynomial homotopy continuation methods to solve polynomial systems in the cloud. Via the graph isomorphism problem we organize and classify the polynomial systems we solved. The classification with the canonical form of a graph identifies newly submitted systems with systems that have already been solved.", "venue": "CASC", "authors": ["Nathan  Bliss", "Jeff  Sommars", "Jan  Verschelde", "Xiangcheng  Yu"], "year": 2015, "n_citations": 8}
{"id": 1641516, "s2_id": "da4e6bd9885a8e33dcaff1aca14a325583a0fcec", "title": "A tutorial-driven introduction to the parallel finite element library FEMPAR v1.0.0", "abstract": "This work is a user guide to the FEMPAR scientific software library. FEMPAR is an open-source object-oriented framework for the simulation of partial differential equations (PDEs) using finite element methods on distributed-memory platforms. It provides a rich set of tools for numerical discretization and built-in scalable solvers for the resulting linear systems of equations. An application expert that wants to simulate a PDE-governed problem has to extend the framework with a description of the weak form of the PDE at hand (and additional perturbation terms for non-conforming approximations). We show how to use the library by going through three different tutorials. The first tutorial simulates a linear PDE (Poisson equation) in a serial environment for a structured mesh using both continuous and discontinuous Galerkin finite element methods. The second tutorial extends it with adaptive mesh refinement on octree meshes. The third tutorial is a distributed-memory version of the previous one that combines a scalable octree handler and a scalable domain decomposition solver. The exposition is restricted to linear PDEs and simple geometries to keep it concise. The interested user can dive into more tutorials available in the FEMPAR public repository to learn about further capabilities of the library, e.g., nonlinear PDEs and nonlinear solvers, time integration, multi-field PDEs, block preconditioning, or unstructured mesh handling.", "venue": "Comput. Phys. Commun.", "authors": ["Santiago  Badia", "Alberto F. Mart\u00edn"], "year": 2020, "n_citations": 5}
{"id": 1649258, "s2_id": "268c5c83cc1c9094ad5c56af0e241285b963531a", "title": "Meta-analysis parameters computation: a Python approach to facilitate the crossing of experimental conditions", "abstract": "Meta-analysis is a data aggregation method that establishes an overall and objective level of evidence based on the results of several studies. It is necessary to maintain a high level of homogeneity in the aggregation of data collected from a systematic literature review. However, the current tools do not allow a cross-referencing of the experimental conditions that could explain the heterogeneity observed between studies. This article aims at proposing a Python programming code containing several functions allowing the analysis and rapid visualization of data from many studies, while allowing the possibility of cross-checking the results by experimental condition.", "venue": "ArXiv", "authors": ["Flavien  Quijoux", "Charles  Truong", "Ali'enor  Vienne-Jumeau", "Laurent  Oudre", "Franccois  BERTIN-HUGAULT", "Philippe  ZAWIEJA", "Marie  LEFEVRE", "Pierre-Paul  VIDAL", "Damien  RICARD"], "year": 2020, "n_citations": 1}
{"id": 1657354, "s2_id": "af714722f2dd35e9be7a5f89696b1969f90df762", "title": "Factorization of Z-homogeneous polynomials in the First (q)-Weyl Algebra", "abstract": "Factorization of elements of noncommutative rings is an important problem both in theory and applications. For the class of domains admitting nontrivial grading, we have recently proposed an approach, which utilizes the grading in order to factor general elements. This is heavily based on the factorization of graded elements. In this paper, we present algorithms to factorize weighted homogeneous (graded) elements in the polynomial first q-Weyl and Weyl algebras, which are both viewed as \\({ \\mathbb {Z}}\\)-graded rings. We show that graded polynomials have finite number of factorizations. Moreover, the factorization of such can be almost completely reduced to commutative univariate factorization over the same base field with some additional uncomplicated combinatorial steps. This allows to deduce the complexity of our algorithms in detail, which we prove to be polynomial-time. Furthermore, we show, that for a graded polynomial p, irreducibility of p in the polynomial first Weyl algebra implies its irreducibility in the localized (rational) Weyl algebra, which is not true for general polynomials. We report on our implementation in the computer algebra system Singular. For graded polynomials, it outperforms currently available implementations for factoring in the first Weyl algebra\u2014in speed as well as in elegancy of the results.", "venue": "ArXiv", "authors": ["Albert  Heinle", "Viktor  Levandovskyy"], "year": 2013, "n_citations": 11}
{"id": 1657473, "s2_id": "20384b6d47461c7db8a3e4d08ed7d4afa98f8f6d", "title": "Temporal blocking of finite-difference stencil operators with sparse \u201coff-the-grid\u201d sources", "abstract": "Stencil kernels dominate a range of scientific applications, including seismic and medical imaging, image processing, and neural networks. Temporal blocking is a performance optimization that aims to reduce the required memory bandwidth of stencil computations by re-using data from the cache for multiple time steps. It has already been shown to be beneficial for this class of algorithms. However, applying temporal blocking to practical applications\u2019 stencils remains challenging. These computations often consist of sparsely located operators not aligned with the computational grid (\u201coff-the-grid\u201d). Our work is motivated by modelling problems in which source injections result in wavefields that must then be measured at receivers by interpolation from the grided wavefield. The resulting data dependencies make the adoption of temporal blocking much more challenging. We propose a methodology to inspect these data dependencies and reorder the computation, leading to performance gains in stencil codes where temporal blocking has not been applicable. We implement this novel scheme in the Devito domain-specific compiler toolchain. Devito implements a domain-specific language embedded in Python to generate optimized partial differential equation solvers using the finite-difference method from high-level symbolic problem definitions. We evaluate our scheme using isotropic acoustic, anisotropic acoustic, and isotropic elastic wave propagators of industrial significance. After auto-tuning, performance evaluation shows that this enables substantial performance improvement through temporal blocking over highly-optimized vectorized spatially-blocked code of up to 1.6x.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["George  Bisbas", "Fabio  Luporini", "Mathias  Louboutin", "Rhodri  Nelson", "Gerard  Gorman", "Paul H. J. Kelly"], "year": 2021, "n_citations": 0}
{"id": 1666494, "s2_id": "222f2050be19098f067f5fb227a1c02b697773e3", "title": "Evaluation of the partitioned global address space (PGAS) model for an inviscid Euler solver", "abstract": "In this paper we evaluate the performance of Unified Parallel C (which implements the partitioned global address space programming model) using a numerical method that is widely used in fluid dynamics. In order to evaluate the incremental approach to parallelization (which is possible with UPC) and its performance characteristics, we implement different levels of optimization of the UPC code and compare it with an MPI parallelization on four different clusters of the Austrian HPC infrastructure (LEO3, LEO3E, VSC2, VSC3) and on an Intel Xeon Phi. We find that UPC is significantly easier to develop in compared to MPI and that the performance achieved is comparable to MPI in most situations. The obtained results show worse performance (on VSC2), competitive performance (on LEO3, LEO3E and VSC3), and superior performance (on the Intel Xeon Phi).", "venue": "Parallel Comput.", "authors": ["Martina  Prugger", "Lukas  Einkemmer", "Alexander  Ostermann"], "year": 2016, "n_citations": 6}
{"id": 1666961, "s2_id": "7403f9457892ecd4975999a38d4a5a23831a93fa", "title": "Grassland: A Rapid Algebraic Modeling System for Million-variable Optimization", "abstract": "An algebraic modeling system (AMS) is a type of mathematical software for optimization problems, which allows users to define symbolic mathematical models in a specific language, instantiate them with given source of data, and solve them with the aid of external solver engines. With the bursting scale of business models and increasing need for timeliness, traditional AMSs are not sufficient to meet the following industry needs: 1) million-variable models need to be instantiated from raw data very efficiently; 2) Strictly feasible solution of million-variable models need to be delivered in a rapid manner to make up-to-date decisions against highly dynamic environments. Grassland is a rapid AMS that provides an end-to-end solution to tackle these emerged new challenges. It integrates a parallelized instantiation scheme for large-scale linear constraints, and a sequential decomposition method that accelerates model solving exponentially with an acceptable loss of optimality. Extensive benchmarks on both classical models and real enterprise scenario demonstrate 6-10x speedup of Grassland over state-of-the-art solutions on model instantiation. Our proposed system has been deployed in the large-scale real production planning scenario of Huawei. With the aid of our decomposition method, Grassland successfully accelerated Huawei's million-variable production planning simulation pipeline from hours to 3-5 minutes, supporting near-real-time production plan decision making against highly dynamic supply-demand environment.", "venue": "CIKM", "authors": ["Xihan  Li", "Xiongwei  Han", "Zhishuo  Zhou", "Mingxuan  Yuan", "Jia  Zeng", "Jun  Wang"], "year": 2021, "n_citations": 0}
{"id": 1682930, "s2_id": "ce5930824a2d7799a10ff12efd3b669d5d616d95", "title": "Symmetric matrix inversion using modified Gaussian elimination", "abstract": "In this paper we present two different variants of method for symmetric matrix inversion, based on modified Gaussian elimination. Both methods avoid computation of square roots and have a reduced machine time's spending. Further, both of them can be used efficiently not only for positive (semi-) definite, but for any non-singular symmetric matrix inversion. We use simulation to verify results, which represented in this paper.", "venue": "ArXiv", "authors": ["Anton  Kochnev", "Nicolai  Savelov"], "year": 2015, "n_citations": 1}
{"id": 1687732, "s2_id": "decd6a0c2ddf81bc765dd0bb620c5e48abf293f6", "title": "Adaptive simulated annealing (ASA): Lessons learned", "abstract": "Adaptive s imulated annealing (ASA) is a global optimization algorithm based on an associated proof that the parameter space can be sampled much more ef ficiently than by using other previous simulated annealing algorithms. The author\u2019 sA SA code has been publicly available for ove rt wo y ears. During this time the author has volunteered to help people via e-mail, and the feedback obtained has been used to further de velop the code. Some lessons learned, in particular some which are rele vant to other simulated annealing algorithms, are described.", "venue": "ArXiv", "authors": ["Lester  Ingber"], "year": 2000, "n_citations": 599}
{"id": 1692103, "s2_id": "f3dc57e0e26a5a3841f72a625c6019a3e72ef63a", "title": "Reliability Conditions in Quadrature Algorithms", "abstract": "The detection of insufficiently resolved or ill-conditioned integrand structures is critical for the reliability assessment of the quadrature rule outputs. We discuss a method of analysis of the profile of the integrand at the quadrature knots which allows inferences approaching the theoretical 100% rate of success, under error estimate sharpening. The proposed procedure is of the highest interest for the solution of parametric integrals arising in complex physical models.", "venue": "ArXiv", "authors": ["Gh.  Adam", "S.  Adam", "N. M. Plakida"], "year": 2003, "n_citations": 10}
{"id": 1693810, "s2_id": "9dfcb33ac58c3ef0a2837a4a2f763854ccec9ecf", "title": "Calcium: Computing in Exact Real and Complex Fields", "abstract": "Calcium is a C library for real and complex numbers in a form suitable for exact algebraic and symbolic computation. Numbers are represented as elements of fields Q(a1,...,an) where the extension numbers ak may be algebraic or transcendental. The system combines efficient field operations with automatic discovery and certification of algebraic relations, resulting in a practical computational model of R and C in which equality is rigorously decidable for a large class of numbers.", "venue": "ISSAC", "authors": ["Fredrik  Johansson"], "year": 2021, "n_citations": 0}
{"id": 1694486, "s2_id": "460dc3026624a3160ff5305d734c234964435d06", "title": "Yet Another Tensor Toolbox for Discontinuous Galerkin Methods and Other Applications", "abstract": "The numerical solution of partial differential equations is at the heart of many grand challenges in supercomputing. Solvers based on high-order discontinuous Galerkin (DG) discretisation have been shown to scale on large supercomputers with excellent performance and efficiency if the implementation exploits all levels of parallelism and is tailored to the specific architecture. However, every year new supercomputers emerge and the list of hardware-specific considerations grows simultaneously with the list of desired features in a DG code. Thus, we believe that a sustainable DG code needs an abstraction layer to implement the numerical scheme in a suitable language. We explore the possibility to abstract the numerical scheme as small tensor operations, describe them in a domain-specific language (DSL) resembling the Einstein notation, and to map them to small General Matrix-Matrix Multiplication routines. The compiler for our DSL implements classic optimisations that are used for large tensor contractions, and we present novel optimisation techniques such as equivalent sparsity patterns and optimal index permutations for temporary tensors. Our application examples, which include the earthquake simulation software SeisSol, show that the generated kernels achieve over 50% peak performance of a recent 48-core Skylake system while the DSL considerably simplifies the implementation.", "venue": "ACM Trans. Math. Softw.", "authors": ["Carsten  Uphoff", "Michael  Bader"], "year": 2020, "n_citations": 9}
{"id": 1698034, "s2_id": "fd6a5fd4aa43d416ebd5ff85cfd10467c65cefd2", "title": "Vertical, Temporal, and Horizontal Scaling of Hierarchical Hypersparse GraphBLAS Matrices", "abstract": "Hypersparse matrices are a powerful enabler for a variety of network, health, finance, and social applications. Hierarchical hypersparse GraphBLAS matrices enable rapid streaming updates while preserving algebraic analytic power and convenience. In many contexts, the rate of these updates sets the bounds on performance. This paper explores hierarchical hypersparse update performance on a variety of hardware with identical software configurations. The high-level language bindings of the GraphBLAS readily enable performance experiments on simultaneous diverse hardware. The best single process performance measured was 4,000,000 updates per second. The best single node performance measured was 170,000,000 updates per second. The hardware used spans nearly a decade and allows a direct comparison of hardware improvements for this computation over this time range; showing a 2x increase in single-core performance, a 3x increase in single process performance, and a 5x increase in single node performance. Running on nearly 2,000 MIT SuperCloud nodes simultaneously achieved a sustained update rate of over 200,000,000,000 updates per second. Hierarchical hypersparse GraphBLAS allows the MIT SuperCloud to analyze extremely large streaming network data sets.", "venue": "2021 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "Tim  Davis", "Chansup  Byun", "William  Arcand", "David  Bestor", "William  Bergeron", "Vijay  Gadepally", "Matthew  Hubbell", "Michael  Houle", "Michael  Jones", "Anna  Klein", "Lauren  Milechin", "Julie  Mullen", "Andrew  Prout", "Albert  Reuther", "Antonio  Rosa", "Siddharth  Samsi", "Charles  Yee", "Peter  Michaleas"], "year": 2021, "n_citations": 0}
{"id": 1700379, "s2_id": "24241574eb14565663bec4d0b02d6231968cb7fc", "title": "Deep Learning Framework From Scratch Using Numpy", "abstract": "This work is a rigorous development of a complete and general-purpose deep learning framework from the ground up. The fundamental components of deep learning - automatic differentiation and gradient methods of optimizing multivariable scalar functions - are developed from elementary calculus and implemented in a sensible object-oriented approach using only Python and the Numpy library. Demonstrations of solved problems using the framework, named ArrayFlow, include a computer vision classification task, solving for the shape of a catenary, and a 2nd order differential equation.", "venue": "ArXiv", "authors": ["Andrei  Nicolae"], "year": 2020, "n_citations": 0}
{"id": 1713222, "s2_id": "82d863c3b4750a52117b464bb0c7db1c92e6f387", "title": "Manyopt: An Extensible Tool for Mixed, Non-Linear Optimization Through SMT Solving", "abstract": "Optimization of Mixed-Integer Non-Linear Programming (MINLP) supports important decisions in applications such as Chemical Process Engineering. But current solvers have limited ability for deductive reasoning or the use of domain-specific theories, and the management of integrality constraints does not yet exploit automated reasoning tools such as SMT solvers. This seems to limit both scalability and reach of such tools in practice. We therefore present a tool, ManyOpt, for MINLP optimization that enables experimentation with reduction techniques which transform a MINLP problem to feasibility checking realized by an SMT solver. ManyOpt is similar to the SAT solver ManySAT in that it runs a specified number of such reduction techniques in parallel to get the strongest result on a given MINLP problem. The tool is implemented in layers, which we may see as features and where reduction techniques are feature vectors. Some of these features are inspired by known MINLP techniques whereas others are novel and specific to SMT. Our experimental results on standard benchmarks demonstrate the benefits of this approach. The tool supports a variety of SMT solvers and is easily extensible with new features, courtesy of its layered structure. For example, logical formulas for deductive reasoning are easily added to constrain further the optimization of a MINLP problem of interest.", "venue": "ArXiv", "authors": ["Andrea Callia D'Iddio", "Michael  Huth"], "year": 2017, "n_citations": 4}
{"id": 1723478, "s2_id": "af8deb6fb5d9f889ab3583e7bc44ec99699ad910", "title": "Random problems with R", "abstract": "R (Version 3.5.1 patched) has an issue with its random sampling functionality. R generates random integers between $1$ and $m$ by multiplying random floats by $m$, taking the floor, and adding $1$ to the result. Well-known quantization effects in this approach result in a non-uniform distribution on $\\{ 1, \\ldots, m\\}$. The difference, which depends on $m$, can be substantial. Because the sample function in R relies on generating random integers, random sampling in R is biased. There is an easy fix: construct random integers directly from random bits, rather than multiplying a random float by $m$. That is the strategy taken in Python's numpy.random.randint() function, among others. Example source code in Python is available at this https URL (see functions getrandbits() and randbelow_from_randbits()).", "venue": "ArXiv", "authors": ["Kellie  Ottoboni", "Philip B. Stark"], "year": 2018, "n_citations": 1}
{"id": 1723806, "s2_id": "42091a1495fc439194a7b5bbb76e9c4e9cf20312", "title": "Conical : An extended module for computing a numerically satisfactory pair of solutions of the differential equation for conical functions", "abstract": "Abstract Conical functions appear in a large number of applications in physics and engineering. In this paper we describe an extension of our module Conical \u00a0(Gil et al., 2012) for the computation of conical functions. Specifically, the module includes now a routine for computing the function R \u2212 1 2 + i \u03c4 m ( x ) , a real-valued numerically satisfactory companion of the function P \u2212 1 2 + i \u03c4 m ( x ) for x > 1 . In this way, a natural basis for solving Dirichlet problems bounded by conical domains is provided. The module also improves the performance of our previous algorithm for the conical function P \u2212 1 2 + i \u03c4 m ( x ) and it includes now the computation of the first order derivative of the function. This is also considered for the function R \u2212 1 2 + i \u03c4 m ( x ) in the extended algorithm. Program summary Program Title: Module Conical Program Files doi: http://dx.doi.org/10.17632/rpw5d8gdkg.1 Licensing provisions: CC by 4.0 Programming language: Fortran 90 External routines/libraries: The module Conical uses a Fortran 90 version of the routine dkia (developed by the authors) for computing the modified Bessel functions K i a ( x ) and its derivative. This routine is available at http://toms.calgo.org . Nature of problem: These functions are the natural function basis for solving, for example, the Laplace\u2019s problem in spherical coordinates for two intersecting cones or for regions bounded by two intersecting spheres, or by one or two confocal hyperboloids of revolution when using toroidal coordinates. The conical function P \u2212 1 2 + i \u03c4 m ( x ) is also used in the Mehler\u2013Fock integral transform for problems in potential and heat theory, Solution method: The algorithm uses different methods of computation depending on the function under consideration ( P \u2212 1 2 + i \u03c4 m ( x ) or R \u2212 1 2 + i \u03c4 m ( x ) ) and the values of x , \u03c4 and m : numerical quadrature, asymptotic expansions in terms of elementary functions, asymptotic expansions in terms of Bessel functions, asymptotic expansions for tau large and backward/forward recursion of three-term recurrence relations. Restrictions: In order to avoid underflow/overflow problems in standard IEEE double precision arithmetic, the admissible parameter ranges for computing the conical function P \u2212 1 2 + i \u03c4 m ( x ) in the routine conicp are: \u2212 1 x 1 , 0 \u03c4 = 100 , 0 \u2264 m \u2264 40 1 x \u2264 100 , 0 \u03c4 \u2264 100 , 0 \u2264 m \u2264 100 . When using the routines conicr and conicpr , the admissible parameter ranges for computing the functions P \u2212 1 2 + i \u03c4 m ( x ) and R \u2212 1 2 + i \u03c4 m ( x ) are 1 x \u2264 100 , 0 \u03c4 \u2264 100 , 0 \u2264 m \u2264 100 .", "venue": "Comput. Phys. Commun.", "authors": ["T. Mark Dunster", "Amparo  Gil", "Javier  Segura", "Nico M. Temme"], "year": 2017, "n_citations": 1}
{"id": 1728688, "s2_id": "2dcb551ccfb6d01e6bbfe8ca0440be78f9c83afb", "title": "Computational Understanding and Manipulation of Symmetries", "abstract": "For natural and artificial systems with some symmetry structure, computational understanding and manipulation can be achieved without learning by exploiting the algebraic structure. This algebraic coordinatization is based on a hierarchical (de)composition method. Here we describe this method and apply it to permutation puzzles. Coordinatization yields a structural understanding, not just solutions for the puzzles. In the case of the Rubik\u2019s Cubes, different solving strategies correspond to different decompositions.", "venue": "ACALCI", "authors": ["Attila  Egri-Nagy", "Chrystopher L. Nehaniv"], "year": 2015, "n_citations": 0}
{"id": 1730361, "s2_id": "2843208e57d31907749b7e5c48fef5a40d5f7551", "title": "ensmallen: a flexible C++ library for efficient function optimization", "abstract": "We present ensmallen, a fast and flexible C++ library for mathematical optimization of arbitrary user-supplied functions, which can be applied to many machine learning problems. Several types of optimizations are supported, including differentiable, separable, constrained, and categorical objective functions. The library provides many pre-built optimizers (including numerous variants of SGD and Quasi-Newton optimizers) as well as a flexible framework for implementing new optimizers and objective functions. Implementation of a new optimizer requires only one method and a new objective function requires typically one or two C++ functions. This can aid in the quick implementation and prototyping of new machine learning algorithms. Due to the use of C++ template metaprogramming, ensmallen is able to support compiler optimizations that provide fast runtimes. Empirical comparisons show that ensmallen is able to outperform other optimization frameworks (like Julia and SciPy), sometimes by large margins. The library is distributed under the BSD license and is ready for use in production environments.", "venue": "NIPS 2018", "authors": ["Shikhar  Bhardwaj", "Ryan R. Curtin", "Marcus  Edel", "Yannis  Mentekidis", "Conrad  Sanderson"], "year": 2018, "n_citations": 9}
{"id": 1737617, "s2_id": "01aa2eb2501dc5700d2f57c0fa9e99eceffd5ba0", "title": "Digital Annealer for quadratic unconstrained binary optimization: a comparative performance analysis", "abstract": "Digital Annealer (DA) is a computer architecture designed for tackling combinatorial optimization problems formulated as quadratic unconstrained binary optimization (QUBO) models. In this paper, we present the results of an extensive computational study to evaluate the performance of DA in a systematic way in comparison to multiple state-of-the-art solvers for different problem classes. We examine pure QUBO models, as well as QUBO reformulations of three constrained problems, namely quadratic assignment, quadratic cycle partition, and selective graph coloring, with the last two being new applications for DA. For the selective graph coloring problem, we also present a size reduction heuristic that significantly increases the number of eligible instances for DA. Our experimental results show that despite being in its development stage, DA can provide high-quality solutions quickly and in that regard rivals the state of the art, particularly for large instances. Moreover, as opposed to established solvers, within its limit on the number of decision variables, DA\u2019s solution times are not affected by the increase in instance size. These findings illustrate that DA has the potential to become a successful technology in tackling combinatorial optimization problems.", "venue": "ArXiv", "authors": ["Oylum  Seker", "Neda  Tanoumand", "Merve  Bodur"], "year": 2020, "n_citations": 4}
{"id": 1741204, "s2_id": "b18961c27b186c01eb3f6a31e3504a2a568ca52e", "title": "Batched computation of the singular value decompositions of order two by the AVX-512 vectorization", "abstract": "In this paper a vectorized algorithm for simultaneously computing up to eight singular value decompositions (SVDs, each of the form $A=U\\Sigma V^{\\ast}$) of real or complex matrices of order two is proposed. The algorithm extends to a batch of matrices of an arbitrary length $n$, that arises, for example, in the annihilation part of the parallel Kogbetliantz algorithm for the SVD of a square matrix of order $2n$. The SVD algorithm for a single matrix of order two is derived first. It scales, in most instances error-free, the input matrix $A$ such that its singular values $\\Sigma_{ii}$ cannot overflow whenever its elements are finite, and then computes the URV factorization of the scaled matrix, followed by the SVD of a non-negative upper-triangular middle factor. A vector-friendly data layout for the batch is then introduced, where the same-indexed elements of each of the input and the output matrices form vectors, and the algorithm's steps over such vectors are described. The vectorized approach is then shown to be about three times faster than processing each matrix in isolation, while slightly improving accuracy over the straightforward method for the $2\\times 2$ SVD.", "venue": "Parallel Process. Lett.", "authors": ["Vedran  Novakovi'c"], "year": 2020, "n_citations": 1}
{"id": 1742480, "s2_id": "8cc53dc6d369834b3dc69ca51ac92329b09d634d", "title": "Massively parallel implementation in Python of a pseudo-spectral DNS code for turbulent flows", "abstract": "Direct Numerical Simulations (DNS) of the Navier Stokes equations is a valuable research tool in fluid dynamics, but there are very few publicly available codes and, due to heavy number crunching, codes are usually written in low-level languages. In this work a \\textasciitilde{}100 line standard scientific Python DNS code is described that nearly matches the performance of pure C for thousands of processors and billions of unknowns. With optimization of a few routines in Cython, it is found to match the performance of a more or less identical solver implemented from scratch in C++. Keys to the efficiency of the solver are the mesh decomposition and three dimensional FFT routines, implemented directly in Python using MPI, wrapped through MPI for Python, and a serial FFT module (both numpy.fft or pyFFTW may be used). Two popular decomposition strategies, slab and pencil, have been implemented and tested.", "venue": "ArXiv", "authors": ["Mikael  Mortensen"], "year": 2016, "n_citations": 3}
{"id": 1747491, "s2_id": "fb2da6343958fd468ff3a178c21dec27c5838378", "title": "Approximating Mathematical Semantic Web Services Using Approximation Formulas and Numerical Methods", "abstract": "Mathematical semantic web services are very useful in practice, but only a small number of research results are reported in this area. In this paper we present a method of obtaining an approximation of a mathematical semantic web service, from its semantic description, using existing mathematical semantic web services, approximation formulas, and numerical methods techniques. We also give a method for automatic comparison of two complexity functions. In addition, we present a method for classifying the numerical methods mathematical semantic web services from a library.", "venue": "ArXiv", "authors": ["Andrei-Horia  Mogos", "Mugurel Ionut Andreica"], "year": 2009, "n_citations": 1}
{"id": 1749128, "s2_id": "5a6b2aac784b6d1b7366a8073146a1a7ca0f939e", "title": "Automatically Building Diagrams for Olympiad Geometry Problems", "abstract": "We present a method for automatically building diagrams for olympiad-level geometry problems and implement our approach in a new open-source software tool, the Geometry Model Builder (GMB). Central to our method is a new domain-specific language, the Geometry Model-Building Language (GMBL), for specifying geometry problems along with additional metadata useful for building diagrams. A GMBL program specifies (1) how to parameterize geometric objects (or sets of geometric objects) and initialize these parameterized quantities, (2) which quantities to compute directly from other quantities, and (3) additional constraints to accumulate into a (differentiable) loss function. A GMBL program induces a (usually) tractable numerical optimization problem whose solutions correspond to diagrams of the original problem statement, and that we can solve reliably using gradient descent. Of the 39 geometry problems since 2000 appearing in the International Mathematical Olympiad, 36 can be expressed in our logic and our system can produce diagrams for 94% of them on average. To the best of our knowledge, our method is the first in automated geometry diagram construction to generate models for such complex problems.", "venue": "CADE", "authors": ["Ryan  Krueger", "Jesse Michael Han", "Daniel  Selsam"], "year": 2021, "n_citations": 0}
{"id": 1750289, "s2_id": "8c6f71edde64950bd2e5afd2ce03078b329ebf5a", "title": "High Performance Rearrangement and Multiplication Routines for Sparse Tensor Arithmetic", "abstract": "Researchers are increasingly incorporating numeric high-order data, i.e., numeric tensors, within their practice. Just like the matrix/vector (MV) paradigm, the development of multi-purpose, but high-performance, sparse data structures and algorithms for arithmetic calculations, e.g., those found in Einstein-like notation, is crucial for the continued adoption of tensors. We use the example of high-order differential operators to illustrate this need. As sparse tensor arithmetic is an emerging research topic, with challenges distinct from the MV paradigm, many aspects require further articulation. We focus on three core facets. First, aligning with prominent voices in the field, we emphasise the importance of data structures able to accommodate the operational complexity of tensor arithmetic. However, we describe a linearised coordinate (LCO) data structure that provides faster and more memory-efficient sorting performance. Second, flexible data structures, like the LCO, rely heavily on sorts and permutations. We introduce an innovative permutation algorithm, based on radix sort, that is tailored to rearrange already-sorted sparse data, producing significant performance gains. Third, we introduce a novel poly-algorithm for sparse tensor products, where hyper-sparsity is a possibility. Different manifestations of hyper-sparsity demand their own approach, which our poly-algorithm is the first to provide. These developments are incorporated within our LibNT and NTToolbox software libraries. Benchmarks, frequently drawn from the high-order differential operators example, demonstrate the practical impact of our routines, with speed-ups of 40% or higher compared to alternative high-performance implementations. Comparisons against the MATLAB Tensor Toolbox show over 10 times speed improvements. Thus, these advancements produce significant practical improvements for sparse tensor arithmetic.", "venue": "SIAM J. Sci. Comput.", "authors": ["Adam P. Harrison", "Dileepan  Joseph"], "year": 2018, "n_citations": 5}
{"id": 1756150, "s2_id": "e41e08a8d124ef949c8968b88f1270a730b7ca94", "title": "OTTER 3.3 Reference Manual", "abstract": "OTTER is a resolution-style theorem-proving program for first-order logic with equality. OTTER includes the inference rules binary resolution, hyperresolution, UR-resolution, and binary paramodulation. Some of its other abilities and features are conversion from first-order formulas to clauses, forward and back subsumption, factoring, weighting, answer literals, term ordering, forward and back demodulation, evaluable functions and predicates, Knuth-Bendix completion, and the hints strategy. OTTER is coded in ANSI C, is free, and is portable to many different kinds of computer.", "venue": "ArXiv", "authors": ["William  McCune"], "year": 2003, "n_citations": 164}
{"id": 1764541, "s2_id": "c5d79ee590935e2565e880e214bc82ad28aed238", "title": "GRINS: A Multiphysics Framework Based on the libMesh Finite Element Library", "abstract": "The progression of scientific computing resources has enabled the numerical approximation of mathematical models describing complex physical phenomena. A significant portion of researcher time is typically dedicated to the development of software to compute the numerical solutions. This work describes a flexible C++ software framework, built on the libMesh finite element library, designed to alleviate developer burden and provide easy access to modern computational algorithms, including quantity-of-interest-driven parallel adaptive mesh refinement on unstructured grids and adjoint-based sensitivities. Other software environments are highlighted and the current work motivated; in particular, the present work is an attempt to balance software infrastructure and user flexibility. The applicable class of problems and design of the software components is discussed in detail. Several examples demonstrate the effectiveness of the design, including applications that incorporate uncertainty. Current and planned developments are discussed.", "venue": "SIAM J. Sci. Comput.", "authors": ["Paul T. Bauman", "Roy H. Stogner"], "year": 2016, "n_citations": 15}
{"id": 1768724, "s2_id": "ed425f101252244aff1cca32e5fb0291a2da5abc", "title": "Kleene Algebra with Tests and Coq Tools for while Programs", "abstract": "We present a Coq library about Kleene algebra with tests, including a proof of their completeness over the appropriate notion of languages, a decision procedure for their equational theory, and tools for exploiting hypotheses of a certain kind in such a theory. \n \nKleene algebra with tests make it possible to represent if-then-else statements and while loops in imperative programming languages. They were actually introduced as an alternative to propositional Hoare logic. \n \nWe show how to exploit the corresponding Coq tools in the context of program verification by proving equivalences of while programs, correctness of some standard compiler optimisations, Hoare rules for partial correctness, and a particularly challenging equivalence of flowchart schemes.", "venue": "ITP", "authors": ["Damien  Pous"], "year": 2013, "n_citations": 42}
{"id": 1774160, "s2_id": "79b5bc002efb1795e0e8244b833816abe09870c4", "title": "From MPI to MPI+OpenACC: Conversion of a legacy FORTRAN PCG solver for the spherical Laplace equation", "abstract": "A real-world example of adding OpenACC to a legacy MPI FORTRAN Preconditioned Conjugate Gradient code is described, and timing results for multi-node multi-GPU runs are shown. The code is used to obtain three-dimensional spherical solutions to the Laplace equation. Its application is finding potential field solutions of the solar corona, a useful tool in space weather modeling. We highlight key tips, strategies, and challenges faced when adding OpenACC, including linking FORTRAN code to the cuSparse library, using CUDA-aware MPI, maintaining portability, and dealing with multi-node, multi-GPU run-time environments. Timing results are shown for the code running with MPI-only (up to 1728 CPU cores) and with MPI+OpenACC (up to 64 NVIDIA P100 GPUs). Performance portability is also addressed, including results using MPI+OpenACC for multi-core x86 CPUs.", "venue": "ArXiv", "authors": ["Ronald M. Caplan", "Zoran  Mikic", "Jon A. Linker"], "year": 2017, "n_citations": 0}
{"id": 1776604, "s2_id": "1843d328e32c7437b77f918b8b9af70947f1cfa8", "title": "Linnea: Automatic Generation of Efficient Linear Algebra Programs", "abstract": "The translation of linear algebra computations into efficient sequences of library calls is a non-trivial task that requires expertise in both linear algebra and high-performance computing. Almost all high-level languages and libraries for matrix computations (e.g., Matlab, Eigen) internally use optimized kernels such as those provided by BLAS and LAPACK; however, their translation algorithms are often too simplistic and thus lead to a suboptimal use of said kernels, resulting in significant performance losses. In order to combine the productivity offered by high-level languages, and the performance of low-level kernels, we are developing Linnea, a code generator for linear algebra problems. As input, Linnea takes a high-level description of a linear algebra problem; as output, it returns an efficient sequence of calls to high-performance kernels. Linnea uses a custom best-first search algorithm to find a first solution in less than a second, and increasingly better solutions when given more time. In 125 test problems, the code generated by Linnea almost always outperforms Matlab, Julia, Eigen and Armadillo, with speedups up to and exceeding 10x.", "venue": "ACM Trans. Math. Softw.", "authors": ["Henrik  Barthels", "Christos  Psarras", "Paolo  Bientinesi"], "year": 2021, "n_citations": 4}
{"id": 1780742, "s2_id": "54bc637f34f0b38df7ffbc0e6aef8b55644364e2", "title": "ELFI: Engine for Likelihood Free Inference", "abstract": "Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.", "venue": "J. Mach. Learn. Res.", "authors": ["Jarno  Lintusaari", "Henri  Vuollekoski", "Antti  Kangasr\u00e4\u00e4si\u00f6", "Kusti  Skyt\u00e9n", "Marko  J\u00e4rvenp\u00e4\u00e4", "Michael U. Gutmann", "Aki  Vehtari", "Jukka  Corander", "Samuel  Kaski"], "year": 2018, "n_citations": 43}
{"id": 1781825, "s2_id": "814d8e022702b43204c5943c456c96b093cb0f8a", "title": "Disciplined geometric programming", "abstract": "We introduce log-log convex programs, which are optimization problems with positive variables that become convex when the variables, objective functions, and constraint functions are replaced with their logs, which we refer to as a log-log transformation. This class of problems generalizes traditional geometric programming and generalized geometric programming, and it includes interesting problems involving nonnegative matrices. We give examples of log-log convex functions, some well-known and some less so, and we develop an analog of disciplined convex programming, which we call disciplined geometric programming. Disciplined geometric programming is a subclass of log-log convex programming generated by a composition rule and a set of functions with known curvature under the log-log transformation. Finally, we describe an implementation of disciplined geometric programming as a reduction in CVXPY 1.0.", "venue": "Optim. Lett.", "authors": ["Akshay  Agrawal", "Steven  Diamond", "Stephen P. Boyd"], "year": 2019, "n_citations": 18}
{"id": 1782655, "s2_id": "bd5940720789a9878927a90a514ad17724ac86e1", "title": "Universal Numbers Library: design and implementation of a high-performance reproducible number systems library", "abstract": "With the proliferation of embedded systems requiring intelligent behavior, custom number systems to optimize performance per Watt of the entire system become essential components for successful commercial products. We present the Universal Number Library, a high-performance number systems library that includes arbitrary integer, decimal, fixed-point, floating point, and introduces two tapered floating-point types, posit and valid that support reproducible arithmetic computation in arbitrary concurrency environments. We discuss the design of the Universal library as a run-time for application development, and as a platform for application-driven hardware validation. The library implementation is described, and examples are provided to show educational examples to elucidate the number system properties, and how specialization is used to yield very highperformance emulation on existing x86, ARM, and POWER processors. We will highlight the integration of the library in larger application environments in computational science and engineering to enable multi-precision and adaptive precision algorithms to improve performance and efficiency of large scale and real-time applications. We will demonstrate the integration of the Universal library into a high-performance reproducible linear algebra run-time. We will conclude with the roadmap of additional functionality of the library as we are targeting new application domains, such as Software Defined Radio, instrumentation, sensor fusion, and model-predictive control. (Abstract) Keywords\u2014floating point, posit, reproducible linear algebra, C++, software, number system (key words)", "venue": "ArXiv", "authors": ["E. Theodore L. Omtzigt", "Peter  Gottschling", "Mark  Seligman", "William  Zorn"], "year": 2020, "n_citations": 0}
{"id": 1786976, "s2_id": "c6fbc191429e5370fd16d3988861a1979a1f2d86", "title": "Landau: language for dynamical systems with automatic differentiation", "abstract": "Most numerical solvers used to determine free variables of dynamical systems rely on first-order derivatives of the state of the system w.r.t. the free variables. The number of the free variables can be fairly large. One of the approaches of obtaining those derivatives is the integration of the derivatives simultaneously with the dynamical equations, which is best done with the automatic differentiation technique. Even though there exist many automatic differentiation tools, none have been found to be scalable and usable for practical purposes of dynamic systems modeling. Landau is a Turing incomplete statically typed domain-specific language aimed to fill this gap. The Turing incompleteness provides the ability of sophisticated source code analysis and, as a result, a highly optimized compiled code. Among other things, the language syntax supports functions, compile-time ranged for loops, if/else branching constructions, real variables and arrays, and the ability to manually discard calculation where the automatic derivatives values are expected to be negligibly small. In spite of reasonable restrictions, the language is rich enough to express and differentiate any cumbersome paper-equation with practically no effort.", "venue": "ArXiv", "authors": ["Ivan  Dolgakov", "Dmitry  Pavlov"], "year": 2019, "n_citations": 0}
{"id": 1788210, "s2_id": "dfbbf0abf157b8fe8679819b814859d6b98b2379", "title": "Hydra: a C++11 framework for data analysis in massively parallel platforms", "abstract": "Hydra is a header-only, templated and C++11-compliant framework designed to perform the typical bottleneck calculations found in common HEP data analyses on massively parallel platforms. The framework is implemented on top of the C++11 Standard Library and a variadic version of the Thrust library and is designed to run on Linux systems, using OpenMP, CUDA and TBB enabled devices. This contribution summarizes the main features of Hydra. A basic description of the overall design, functionality and user interface is provided, along with some code examples and measurements of performance.", "venue": "Journal of Physics: Conference Series", "authors": ["A. A. Alves", "Michael D. Sokoloff"], "year": 2018, "n_citations": 2}
{"id": 1795316, "s2_id": "10ba30b0180e255596f622f96b45cfaa868911ce", "title": "Look-ahead in the two-sided reduction to compact band forms for symmetric eigenvalue problems and the SVD", "abstract": "We address the reduction to compact band forms, via unitary similarity transformations, for the solution of symmetric eigenvalue problems and the computation of the singular value decomposition (SVD). Concretely, in the first case, we revisit the reduction to symmetric band form, while, for the second case, we propose a similar alternative, which transforms the original matrix to (unsymmetric) band form, replacing the conventional reduction method that produces a triangular\u2013band output. In both cases, we describe algorithmic variants of the standard Level 3 Basic Linear Algebra Subroutines (BLAS)-based procedures, enhanced with look-ahead, to overcome the performance bottleneck imposed by the panel factorization. Furthermore, our solutions employ an algorithmic block size that differs from the target bandwidth, illustrating the important performance benefits of this decision. Finally, we show that our alternative compact band form for the SVD is key to introduce an effective look-ahead strategy into the corresponding reduction procedure.", "venue": "Numerical Algorithms", "authors": ["Rafael  Rodr\u00edguez-S\u00e1nchez", "Sandra  Catal\u00e1n", "Jos\u00e9 R. Herrero", "Enrique S. Quintana-Ort\u00ed", "Andr\u00e9s E. Tom\u00e1s"], "year": 2018, "n_citations": 1}
{"id": 1805229, "s2_id": "42da0ffb1aef089ecb03ff5077f5a77f8ac6b5e8", "title": "Using Premia and Nsp for constructing a risk management benchmark for testing parallel architecture", "abstract": "Financial institutions have massive computations to carry out overnight, which are very CPU demanding. The challenge is to price many different products on a cluster\u2010like architecture. We have used the Premia software to valuate the financial derivatives. In this work, we explain how Premia can be embedded into Nsp, a scientific software like MATLAB, to provide a powerful tool to valuate a whole portfolio. Finally, we have integrated an Message Passing Interface (MPI) toolbox into Nsp to enable the use of Premia to solve a bunch of pricing problems on a cluster. This unified framework can then be used to test different parallel architectures. Copyright \u00a9 2012 John Wiley & Sons, Ltd.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Jean-Philippe  Chancelier", "Bernard  Lapeyre", "J\u00e9r\u00f4me  Lelong"], "year": 2014, "n_citations": 2}
{"id": 1806729, "s2_id": "364fb0677a5d7083e56c0e38629a78cb94836f53", "title": "API design for machine learning software: experiences from the scikit-learn project", "abstract": "Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.", "venue": "ArXiv", "authors": ["Lars  Buitinck", "Gilles  Louppe", "Mathieu  Blondel", "Fabian  Pedregosa", "Andreas  Mueller", "Olivier  Grisel", "Vlad  Niculae", "Peter  Prettenhofer", "Alexandre  Gramfort", "Jaques  Grobler", "Robert  Layton", "Jacob  VanderPlas", "Arnaud  Joly", "Brian  Holt", "Ga\u00ebl  Varoquaux"], "year": 2013, "n_citations": 1171}
{"id": 1808760, "s2_id": "e8c334df36db33c1c31c14ac8379f138cea16801", "title": "Using performance analysis tools for parallel-in-time integrators - Does my time-parallel code do what I think it does?", "abstract": "While many ideas and proofs of concept for parallel-in-time integration methods exists, the number of large-scale, accessible time-parallel codes is rather small. This is often due to the apparent or subtle complexity of the algorithms and the many pitfalls awaiting developers of parallel numerical software. One example of such a time-parallel code is pySDC, which implements, among others, the parallel full approximation scheme in space and time (PFASST). Inspired by nonlinear multigrid ideas, PFASST allows to integrate multiple time-steps simultaneously using a space-time hierarchy of spectral deferred corrections. In this paper we demonstrate the application of performance analysis tools to the PFASST implementation pySDC. Tracing the path we took for this work, we highlight the obstacles encountered, describe remedies and explain the sometimes surprising findings made possible by the tools. Although focusing only on a single implementation of a particular parallel-in-time integrator, we hope that our results and in particular the way we obtained them are a blueprint for other time-parallel codes.", "venue": "ArXiv", "authors": ["Robert  Speck", "Michael  Knobloch", "Andreas  Gocht", "Sebastian  L\u00fchrs"], "year": 2019, "n_citations": 0}
{"id": 1811506, "s2_id": "9586a71b1c0ebb3f452b01650c8968bb8c7b35b9", "title": "Nanosurveyor: a framework for real-time data processing", "abstract": "BackgroundThe ever improving brightness of accelerator based sources is enabling novel observations and discoveries with faster frame rates, larger fields of view, higher resolution, and higher dimensionality.ResultsHere we present an integrated software/algorithmic framework designed to capitalize on high-throughput experiments through efficient kernels, load-balanced workflows, which are scalable in design. We describe the streamlined processing pipeline of ptychography data analysis.ConclusionsThe pipeline provides throughput, compression, and resolution as well as rapid feedback to the microscope operators.", "venue": "Advanced Structural and Chemical Imaging", "authors": ["Benedikt J. Daurer", "Hari  Krishnan", "Talita  Perciano", "Filipe R. N. C. Maia", "David A. Shapiro", "James A. Sethian", "Stefano  Marchesini"], "year": 2017, "n_citations": 9}
{"id": 1817961, "s2_id": "c838e921d6bcf6b405b961b95c3fc432d8ad2d4c", "title": "A modular framework for randomness extraction based on Trevisan's construction", "abstract": "Informally, an extractor delivers perfect randomness from a source that may be far away from the uniform distribution, yet contains some randomness. This task is a crucial ingredient of any attempt to produce perfectly random numbers---required, for instance, by cryptographic protocols, numerical simulations, or randomised computations. Trevisan's extractor raised considerable theoretical interest not only because of its data parsimony compared to other constructions, but particularly because it is secure against quantum adversaries, making it applicable to quantum key distribution. \nWe discuss a modular, extensible and high-performance implementation of the construction based on various building blocks that can be flexibly combined to satisfy the requirements of a wide range of scenarios. Besides quantitatively analysing the properties of many combinations in practical settings, we improve previous theoretical proofs, and give explicit results for non-asymptotic cases. The self-contained description does not assume familiarity with extractors.", "venue": "ArXiv", "authors": ["Wolfgang  Mauerer", "Christopher  Portmann", "Volkher B. Scholz"], "year": 2012, "n_citations": 24}
{"id": 1823654, "s2_id": "e9ac23aeb0f3cecd0916c0e12a15bb762b997981", "title": "Convex Hull Calculations: a Matlab Implementation and Correctness Proofs for the lrs-Algorithm", "abstract": "This paper provides full \\Matlab-code and informal correctness proofs for the lexicographic reverse search algorithm for convex hull calculations. The implementation was tested on a 1993 486-PC for various small and some larger, partially highly degenerate combinatorial polytopes, one of which (a certain 13-dimensional 24 vertex polyhedron) occurs naturally in the study of a well known problem posed by Professor Graciano de Oliveira: see end of section 1.", "venue": "ArXiv", "authors": ["Alexander  Kovacec", "Bernardete  Ribeiro"], "year": 2016, "n_citations": 1}
{"id": 1824223, "s2_id": "9a76b221e01e1509ba9f02027b5292a15bde68bc", "title": "Solving Poisson's equation on the Microsoft HoloLens", "abstract": "We present a mixed reality application (HoloFEM) for the Microsoft HoloLens. The application lets a user define and solve a physical problem governed by Poisson's equation with the surrounding real world geometry as input data. Holograms are used to visualise both the problem and the solution. The finite element method is used to solve Poisson's equation. Solving and visualising partial differential equations in mixed reality could have potential usage in areas such as building planning and safety engineering.", "venue": "VRST", "authors": ["Anders  Logg", "Carl  Lundholm", "Magne  Nordaas"], "year": 2017, "n_citations": 2}
{"id": 1827339, "s2_id": "bc59fcc7f8b1a0d9106e1ff2dddb22304d8267bb", "title": "Point-and-Write - Documenting Formal Mathematics by Reference", "abstract": "This paper describes the design and implementation of mechanisms for light-weight inclusion of formal mathematics in informal mathematical writings, particularly in a Web-based setting. This is conceptually done in three stages: (i) by choosing a suitable representation layer (based on RDF) for encoding the information about available resources of formal mathematics, (ii) by exporting this information from formal libraries, and (iii) by providing syntax and implementation for including formal mathematics in informal writings. \n \nWe describe the use case of an author referring to formal text from an informal narrative, and discuss design choices entailed by this use case. Furthermore, we describe an implementation of the use case within the Agora prototype: a Wiki for collaborating on formalized mathematics.", "venue": "AISC/MKM/Calculemus", "authors": ["Carst  Tankink", "Christoph  Lange", "Josef  Urban"], "year": 2012, "n_citations": 4}
{"id": 1832846, "s2_id": "d82f427d3c56b879d6792c40741b5541a41244ab", "title": "Bidiagonalization with Parallel Tiled Algorithms", "abstract": "We consider algorithms for going from a ``full'' matrix to a condensed \n``band bidiagonal'' form using orthogonal transformations. \nWe use the framework of ``algorithms by tiles''. \nWithin this framework, we study: (i) the tiled \nbidiagonalization algorithm \\bidiag, which is a tiled version of the \nstandard scalar bidiagonalization algorithm; and \n(ii) the R-bidiagonalization algorithm \\rbidiag, which \nis a tiled version of the algorithm which \nconsists in first performing the QR factorization of \nthe initial matrix, then performing the band-bidiagonalization of the R-factor. \nFor both bidiagonalization algorithms \\bidiag and \\rbidiag, \nwe use four main \ntypes of reduction trees, namely \\FlatTS, \\FlatTT, \\Greedy, \nand a newly introduced auto-adaptive tree, \\Auto. \nWe provide a study of critical path lengths for these tiled algorithms, which shows \nthat (i) \\rbidiag has a shorter critical path length than \n\\bidiag for tall and skinny matrices, and (ii) \\Greedy based schemes are \nmuch better than earlier proposed variants with unbounded resources. \nWe provide experiments on a single multicore node, and on a few multicore nodes of a parallel distributed \nshared-memory system, to show the superiority of the new algorithms on a \nvariety of matrix sizes, matrix shapes and core counts.", "venue": "ArXiv", "authors": ["Mathieu  Faverge", "Julien  Langou", "Yves  Robert", "Jack J. Dongarra"], "year": 2016, "n_citations": 1}
{"id": 1839619, "s2_id": "e426c8ef376b57aad3f5930a77259089f433ffa3", "title": "Leveraging GPU batching for scalable nonlinear programming through massive Lagrangian decomposition", "abstract": "We present the implementation of a trust-region Newton algorithm ExaTron for bound-constrained nonlinear programming problems, fully running on multiple GPUs. Without data transfers between CPU and GPU, our implementation has achieved the elimination of a major performance bottleneck under a memory-bound situation, particularly when solving many small problems in batch. We discuss the design principles and implementation details for our kernel function and core operations. Different design choices are justified by numerical experiments. By using the application of distributed control of alternating current optimal power flow, where a large problem is decomposed into many smaller nonlinear programs using a Lagrangian approach, we demonstrate computational performance of ExaTron on the Summit supercomputer at Oak Ridge National Laboratory. Our numerical results show the linear scaling with respect to the batch size and the number of GPUs and more than 35 times speedup on 6 GPUs than on 40 CPUs available on a single node.", "venue": "ArXiv", "authors": ["Youngdae  Kim", "Franccois  Pacaud", "Kibaek  Kim", "Mihai  Anitescu"], "year": 2021, "n_citations": 4}
{"id": 1840941, "s2_id": "f387d94e488f6a09d08433d2bdee4b6d2f4876dc", "title": "Improving MATLAB's isprime performance without arbitrary-precision arithmetic", "abstract": "MATLAB is a numerical computing platform used by scientists, engineers, mathematicians, and students which contains many mathematical functions, including isprime. MATLAB\u2019s isprime function determines which elements of an input array are prime. This research details modular arithmetic techniques, the Miller\u2014Rabin primality test, vectorized operations, and division-minimizing strategies which harness the power of MATLAB\u2019s capabilities to improve isprime\u2019s performance. The results are typically 5 to 10 times faster for small integers and many hundreds of times faster for large integers and long arrays.", "venue": "ArXiv", "authors": ["Travis  Near"], "year": 2021, "n_citations": 0}
{"id": 1846301, "s2_id": "a03b9ab12263545f99b301bed448f163eb6e5301", "title": "Evaluating Polynomials in Several Variables and their Derivatives on a GPU Computing Processor", "abstract": "In order to obtain more accurate solutions of polynomial systems with numerical continuation methods we use multiprecision arithmetic. Our goal is to offset the overhead of double double arithmetic accelerating the path trackers and in particular Newton's method with a general purpose graphics processing unit. In this paper we describe algorithms for the massively parallel evaluation and differentiation of sparse polynomials in several variables. We report on our implementation of the algorithmic differentiation of products of variables on the NVIDIA Tesla C2050 Computing Processor using the NVIDIA CUDA compiler tools.", "venue": "2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum", "authors": ["Jan  Verschelde", "Genady  Yoffe"], "year": 2012, "n_citations": 16}
{"id": 1850976, "s2_id": "b9d9887c9a676add04a064166763b5f4292cc2a5", "title": "Gravitational Octree Code Performance Evaluation on Volta GPU", "abstract": "In this study, the gravitational octree code originally optimized for the Fermi, Kepler, and Maxwell GPU architectures is adapted to the Volta architecture. The Volta architecture introduces independent thread scheduling requiring either the insertion of the explicit synchronizations at appropriate locations or the enforcement of the same implicit synchronizations as do the Pascal or earlier architectures by specifying -gencode arch=compute_60,code=sm_70. The performance measurements on Tesla V100, the current flagship GPU by NVIDIA, revealed that the N-body simulations of the Andromeda galaxy model with 223 = 8 388 608 particles took 3.8 \u00d7 10-2 s or 3.3 \u00d7 10-2 s per step for cases without or with the implicit synchronizations, respectively. Tesla V100 achieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the flagship GPU in the previous generation. The observed speed-up of 2.2 is greater than 1.5, which is the ratio of the theoretical peak performance of the two GPUs. The independence of the units for integer operations from those for floating-point number operations enables the overlapped execution of integer and floating-point number operations. It hides the execution time of the integer operations leading to the speed-up rate above the theoretical peak performance ratio. Tesla V100 can execute N-body simulation with up to 25 \u00d7 220 = 26 214 400 particles, and it took 2.0 \u00d7 10-1 s per step. It corresponds to 3.5 TFlop/s, which is 22% of the single-precision theoretical peak performance.", "venue": "ICPP", "authors": ["Yohei  Miki"], "year": 2019, "n_citations": 0}
{"id": 1851645, "s2_id": "041bf4b9049696aecb7b24719e9585ef9b75dd5d", "title": "Enhanced Formulation for Guillotine 2D Cutting Problems", "abstract": "We advance the state of the art in Mixed-Integer Linear Programming (MILP) formulations for Guillotine 2D Cutting Problems by (i) adapting a previouslyknown reduction to our preprocessing phase and by (ii) enhancing a previous formulation by cutting down its size and symmetries. Our focus is the Guillotine 2D Knapsack Problem with orthogonal and unrestricted cuts, constrained demand, unlimited stages, and no rotation \u2013 however, the formulation may be adapted to many related problems. The code is available. Concerning the set of 59 instances used to benchmark the original formulation, and summing the statistics for all models generated, the enhanced formulation has only a small fraction of the variables and constraints of the original model (respectively, 3.07% and 8.35%). The enhanced formulation also takes about 4 hours to solve all instances while the original formulation takes 12 hours to solve 53 of them (the other six runs hit a three-hour time limit each). We integrate, to both formulations, a pricing framework proposed for the original formulation; the enhanced formulation keeps a significant advantage in this situation. Finally, in a recently proposed set of 80 harder instances, the enhanced formulation (with and without the pricing framework) found: 22 optimal solutions for the unrestricted problem (5 already known, 17 new); 22 optimal solutions for the restricted problem (all are new and they are not the same 22 of the optimal unrestricted solutions); better lower bounds for 25 instances; better upper bounds for 58 instances.", "venue": "ArXiv", "authors": ["Henrique  Becker", "Olinto  Araujo", "Luciana S. Buriol"], "year": 2021, "n_citations": 0}
{"id": 1856321, "s2_id": "2a0a0aa34797cc7a84c5f374d9f5054251b30e04", "title": "A Computer Algebra Package for Polynomial Sequence Recognition", "abstract": "The software package developed in the thesis research implements functions for the intelligent guessing of polynomial sequence formulas based on user-defined expected sequence factors of the input coefficients. We present a specialized hybrid approach to finding exact representations for polynomial sequences that is motivated by the need for an automated procedures to discover the precise forms of these sums based on user guidance, or intuition, as to special sequence factors present in the formulas. In particular, the package combines the user input on the expected special sequence factors in the polynomial coefficient formulas with calls to the existing functions as subroutines that then process formulas for the remaining sequence terms already recognized by these packages. The factorization\u00a0\u2013 based approach to polynomial sequence recognition is unique to this package and allows the search functions to find expressions for polynomial sums involving Stirling numbers and other special triangular sequences that are not readily handled by other software packages. The thesis contains a number of concrete, working examples of the package that are intended to both demonstrate usage and to document its current sequence recognition capabilities.", "venue": "ArXiv", "authors": ["Maxie D. Schmidt"], "year": 2016, "n_citations": 0}
{"id": 1858778, "s2_id": "fe87b979a097df4c3b1cd3d54b39548afc48473d", "title": "Combined Sieve Algorithm for Prime Gaps", "abstract": "A new Combined Sieve algorithm is presented with cost proportional to the number of enumerated factors over a series of intervals. This algorithm achieves a significant speedup, over a traditional sieve, when handling many ([10^4, 10^7]) intervals concurrently. The speedup comes from a space-time tradeoff and a novel solution to a modular equation. In real world tests, this new algorithm regularly runs 10,000x faster. This faster sieve paired with higher sieving limits eliminates more composites and accelerates the search for large prime gaps by 30-70%. During the development and testing of this new algorithm, two top-10 record merit prime gaps were discovered.", "venue": "ArXiv", "authors": ["Seth  Troisi"], "year": 2020, "n_citations": 0}
{"id": 1867374, "s2_id": "4387258a60ee767a724c036054ba8801ea5de96d", "title": "Robust and scalable h-adaptive aggregated unfitted finite elements for interface elliptic problems", "abstract": "This work introduces a novel, fully robust and highly-scalable, $h$-adaptive aggregated unfitted finite element method for large-scale interface elliptic problems. The new method is based on a recent distributed-memory implementation of the aggregated finite element method atop a highly-scalable Cartesian forest-of-trees mesh engine. It follows the classical approach of weakly coupling nonmatching discretisations at the interface to model internal discontinuities at the interface. We propose a natural extension of a single-domain parallel cell aggregation scheme to problems with a finite number of interfaces; it straightforwardly leads to aggregated finite element spaces that have the structure of a Cartesian product. We demonstrate, through standard numerical analysis and exhaustive numerical experimentation on several complex Poisson and linear elasticity benchmarks, that the new technique enjoys the following properties: well-posedness, robustness to cut location and material contrast, optimal (h-adaptive) approximation properties, high scalability and easy implementation in large-scale finite element codes. As a result, the method offers great potential as a useful finite element solver for large-scale multiphase and multiphysics problems modelled by partial differential equations.", "venue": "Computer Methods in Applied Mechanics and Engineering", "authors": ["Eric  Neiva", "Santiago  Badia"], "year": 2021, "n_citations": 3}
{"id": 1873464, "s2_id": "be2299e123fd3837134fd9689caf297ded703132", "title": "Randomized Projection for Rank-Revealing Matrix Factorizations and Low-Rank Approximations", "abstract": "Rank-revealing matrix decompositions provide an essential tool in spectral analysis of matrices, including the Singular Value Decomposition (SVD) and related low-rank approximation techniques. QR with Column Pivoting (QRCP) is usually suitable for these purposes, but it can be much slower than the unpivoted QR algorithm. For large matrices, the difference in performance is due to increased communication between the processor and slow memory, which QRCP needs in order to choose pivots during decomposition. Our main algorithm, Randomized QR with Column Pivoting (RQRCP), uses randomized projection to make pivot decisions from a much smaller sample matrix, which we can construct to reside in a faster level of memory than the original matrix. This technique may be understood as trading vastly reduced communication for a controlled increase in uncertainty during the decision process. For rank-revealing purposes, the selection mechanism in RQRCP produces results that are the same quality as the standard algorithm, but with performance near that of unpivoted QR (often an order of magnitude faster for large matrices). We also propose two formulas that facilitate further performance improvements. The first efficiently updates sample matrices to avoid computing new randomized projections. The second avoids large trailing updates during the decomposition in truncated low-rank approximations. Our truncated version of RQRCP also provides a key initial step in our truncated SVD approximation, TUXV. These advances open up a new performance domain for large matrix factorizations that will support efficient problem-solving techniques for challenging applications in science, engineering, and data analysis.", "venue": "SIAM Rev.", "authors": ["Jed A. Duersch", "Ming  Gu"], "year": 2020, "n_citations": 5}
{"id": 1877292, "s2_id": "d98484674849ebd461cfd158094a812ff48427b9", "title": "Formal Model-Driven Engineering: Generating Data and Behavioural Components", "abstract": "Model-driven engineering is the automatic production of software artefacts from abstract models of structure and functionality. By targeting a specific class of system, it is possible to automate aspects of the development process, using model transformations and code generators that encode domain knowledge and implementation strategies. Using this approach, questions of correctness for a complex, software system may be answered through analysis of abstract models of lower complexity, under the assumption that the transformations and generators employed are themselves correct. This paper shows how formal techniques can be used to establish the correctness of model transformations used in the generation of software components from precise object models. The source language is based upon existing, formal techniques; the target language is the widely-used SQL notation for database programming. Correctness is established by giving comparable, relational semantics to both languages, and checking that the transformations are semantics-preserving.", "venue": "FTSCS", "authors": ["Chen-Wei  Wang", "Jim  Davies"], "year": 2012, "n_citations": 1}
{"id": 1880123, "s2_id": "f0ea21fd12ab2eddcc39f03a6b957b4f9efb43cd", "title": "RLIBM-ALL: A Novel Polynomial Approximation Method to Produce Correctly Rounded Results for Multiple Representations and Rounding Modes", "abstract": "Mainstream math libraries for floating point (FP) do not produce correctly rounded results for all inputs. In contrast, CR-LIBM and RLIBM provide correctly rounded implementations for a specific FP representation with one rounding mode. Using such libraries for a representation with a new rounding mode or with different precision will result in wrong results due to double rounding. This paper proposes a novel method to generate a single polynomial approximation that produces correctly rounded results for all inputs for multiple rounding modes and multiple precision configurations. To generate a correctly rounded library for $n$-bits, our key idea is to generate such a polynomial approximation for a representation with $n+2$-bits using the \\emph{round-to-odd} mode. We prove that the resulting polynomial approximation will produce correctly rounded results for all five rounding modes in the standard and for multiple representations with $k$-bits such that $|E| +1<k \\leq n$, where $|E|$ is the number of exponent bits in the representation. Building on our prior work in the RLIBM project, we also approximate the correctly rounded result when we generate the library with $n+2$-bits using the round-to-odd mode. We also generate polynomial approximations by structuring it as a linear programming problem but propose enhancements to polynomial generation to handle the round-to-odd mode. Our prototype is the first 32-bit float library that produces correctly rounded results with all rounding modes in the IEEE standard for all inputs with a single polynomial approximation. It also produces correctly rounded results for any FP configuration ranging from 10-bits to 32-bits while also being faster than mainstream libraries.", "venue": "ArXiv", "authors": ["Jay P. Lim", "Santosh  Nagarakatte"], "year": 2021, "n_citations": 4}
{"id": 1880908, "s2_id": "8ee311d0cd038a61e442fc4fe5cca32d26e809f1", "title": "Hierarchical Matrix Operations on GPUs: Matrix-Vector Multiplication and Compression", "abstract": "Hierarchical matrices are space and time efficient representations of dense matrices that exploit the low rank structure of matrix blocks at different levels of granularity. The hierarchically low rank block partitioning produces representations that can be stored and operated on in near-linear complexity instead of the usual polynomial complexity of dense matrices. In this paper, we present high performance implementations of matrix vector multiplication and compression operations for the $\\mathcal{H}^2$ variant of hierarchical matrices on GPUs. This variant exploits, in addition to the hierarchical block partitioning, hierarchical bases for the block representations and results in a scheme that requires only $O(n)$ storage and $O(n)$ complexity for the mat-vec and compression kernels. These two operations are at the core of algebraic operations for hierarchical matrices, the mat-vec being a ubiquitous operation in numerical algorithms while compression/recompression represents a key building block for other algebraic operations, which require periodic recompression during execution. The difficulties in developing efficient GPU algorithms come primarily from the irregular tree data structures that underlie the hierarchical representations, and the key to performance is to recast the computations on flattened trees in ways that allow batched linear algebra operations to be performed. This requires marshaling the irregularly laid out data in a way that allows them to be used by the batched routines. Marshaling operations only involve pointer arithmetic with no data movement and as a result have minimal overhead. Our numerical results on covariance matrices from 2D and 3D problems from spatial statistics show the high efficiency our routines achieve---over 550GB/s for the bandwidth-limited mat-vec and over 850GFLOPS/s in sustained performance for the compression on the P100 Pascal GPU.", "venue": "ArXiv", "authors": ["Wajih Halim Boukaram", "George  Turkiyyah", "David E. Keyes"], "year": 2019, "n_citations": 9}
{"id": 1887786, "s2_id": "53436fed20948b5cbf0d1654f251c9d022c56fc1", "title": "Coloured and task-based stencil codes", "abstract": "Simple stencil codes are and remain an important building block in scientific computing. On shared memory nodes, they are traditionally parallelised through colouring or (recursive) tiling. New OpenMP versions alternatively allow users to specify data dependencies explicitly and to outsource the decision how to distribute the work to the runtime system. We evaluate traditional multithreading strategies on both Broadwell and KNL, study the arising assignment of tasks to threads and, from there, derive two efficient ways to parallelise stencil codes on regular Cartesian grids that fuse colouring and task-based approaches.", "venue": "ArXiv", "authors": ["Benjamin  Hazelwood", "Tobias  Weinzierl"], "year": 2018, "n_citations": 0}
{"id": 1888725, "s2_id": "5badceba951dc5c7a23d5c8c632a640237adada4", "title": "vSMC: Parallel Sequential Monte Carlo in C++", "abstract": "Sequential Monte Carlo is a family of algorithms for sampling from a sequence of distributions. Some of these algorithms, such as particle filters, are widely used in physics and signal processing research. More recent developments have established their application in more general inference problems such as Bayesian modeling. These algorithms have attracted considerable attention in recent years not only be- cause that they have desired statistical properties, but also because they admit natural and scalable parallelization. However, they are perceived to be difficult to implement. In addition, parallel programming is often unfamiliar to many researchers though conceptually appealing. A C++ template library is presented for the purpose of implementing generic sequential Monte Carlo algorithms on parallel hardware. Two examples are presented: a simple particle filter and a classic Bayesian modeling problem.", "venue": "ArXiv", "authors": ["Yan  Zhou"], "year": 2013, "n_citations": 14}
{"id": 1889627, "s2_id": "811baa1277352f37b1cdbaa3aea122ade4ebd5cd", "title": "Early Observations on Performance of Google Compute Engine for Scientific Computing", "abstract": "Although Cloud computing emerged for business applications in industry, public Cloud services have been widely accepted and encouraged for scientific computing in academia. The recently available Google Compute Engine (GCE) is claimed to support high-performance and computationally intensive tasks, while little evaluation studies can be found to reveal GCE's scientific capabilities. Considering that fundamental performance benchmarking is the strategy of early-stage evaluation of new Cloud services, we followed the Cloud Evaluation Experiment Methodology (CEEM) to benchmark GCE and also compare it with Amazon EC2, to help understand the elementary capability of GCE for dealing with scientific problems. The experimental results and analyses show both potential advantages of, and possible threats to applying GCE to scientific computing. For example, compared to Amazon's EC2 service, GCE may better suit applications that require frequent disk operations, while it may not be ready yet for single VM-based parallel computing. Following the same evaluation methodology, different evaluators can replicate and/or supplement this fundamental evaluation of GCE. Based on the fundamental evaluation results, suitable GCE environments can be further established for case studies of solving real science problems.", "venue": "2013 IEEE 5th International Conference on Cloud Computing Technology and Science", "authors": ["Zheng  Li", "Liam  O'Brien", "Rajiv  Ranjan", "Miranda  Zhang"], "year": 2013, "n_citations": 21}
{"id": 1891791, "s2_id": "aac95d44cf39e38d8cf70c9ccc5eec032bc24c54", "title": "Accelerating Geometric Multigrid Preconditioning with Half-Precision Arithmetic on GPUs", "abstract": "With the hardware support for half-precision arithmetic on NVIDIA V100 GPUs, high-performance computing applications can benefit from lower precision at appropriate spots to speed up the overall execution time. In this paper, we investigate a mixed-precision geometric multigrid method to solve large sparse systems of equations stemming from discretization of elliptic PDEs. While the final solution is always computed with high-precision accuracy, an iterative refinement approach with multigrid preconditioning in lower precision and residuum scaling is employed. We compare the FP64 baseline for Poisson's equation to purely FP16 multigrid preconditioning and to the employment of FP16-FP32-FP64 combinations within a mesh hierarchy. While the iteration count is almost not affected by using lower accuracy, the solver runtime is considerably decreased due to the reduced memory transfer and a speedup of up to 2.5x is gained for the overall solver. We investigate the performance of selected kernels with the hierarchical Roofline model.", "venue": "ArXiv", "authors": ["Kyaw L. Oo", "Andreas  Vogel"], "year": 2020, "n_citations": 2}
{"id": 1893626, "s2_id": "69a0313e849117648ed3b0408458de80241308c9", "title": "Trigger Detection for Adaptive Scientific Workflows Using Percentile Sampling", "abstract": "Increasing complexity of scientific simulations and HPC architectures are driving the need for adaptive workflows, where the composition and execution of computational and data manipulation steps dynamically depend on the evolutionary state of the simulation itself. Consider for example, the frequency of data storage. Critical phases of the simulation should be captured with high frequency and with high fidelity for post-analysis, however we cannot afford to retain the same frequency for the full simulation due to the high cost of data movement. We can instead look for triggers, indicators that the simulation will be entering a critical phase and adapt the workflow accordingly. \nWe present a method for detecting triggers and demonstrate its use in direct numerical simulations of turbulent combustion using S3D. We show that chemical explosive mode analysis (CEMA) can be used to devise a noise-tolerant indicator for rapid increase in heat release. However, exhaustive computation of CEMA values dominates the total simulation, thus is prohibitively expensive. To overcome this bottleneck, we propose a quantile-sampling approach. Our algorithm comes with provable error/confidence bounds, as a function of the number of samples. Most importantly, the number of samples is independent of the problem size, thus our proposed algorithm offers perfect scalability. Our experiments on homogeneous charge compression ignition (HCCI) and reactivity controlled compression ignition (RCCI) simulations show that the proposed method can detect rapid increases in heat release, and its computational overhead is negligible. Our results will be used for dynamic workflow decisions about data storage and mesh resolution in future combustion simulations. Proposed framework is generalizable and we detail how it could be applied to a broad class of scientific simulation workflows.", "venue": "SIAM J. Sci. Comput.", "authors": ["Janine  Bennett", "Ankit  Bhagatwala", "Jacqueline  Chen", "Ali  Pinar", "Maher  Salloum", "Seshadhri  Comandur"], "year": 2016, "n_citations": 13}
{"id": 1898616, "s2_id": "4265e8d1d46fd8e7f90d756f91a9b3000658e755", "title": "Towards the Formal Specification and Verification of Maple Programs", "abstract": "In this paper, we present our ongoing work and initial results on the formal specification and verification of MiniMaple (a substantial subset of Maple with slight extensions) programs. The main goal of our work is to find behavioral errors in such programs w.r.t. their specifications by static analysis. This task is more complex for widely used computer algebra languages like Maple as these are fundamentally different from classical languages: they support non-standard types of objects such as symbols, unevaluated expressions and polynomials and require abstract computer algebraic concepts and objects such as rings and orderings etc. As a starting point we have defined and formalized a syntax, semantics, type system and specification language for MiniMaple.", "venue": "AISC/MKM/Calculemus", "authors": ["Muhammad Taimoor Khan", "Wolfgang  Schreiner"], "year": 2012, "n_citations": 8}
{"id": 1909594, "s2_id": "869a84597ca5d9d7ef10e0bcd7f0c0e0b30eee15", "title": "DUNE as an Example of Sustainable Open Source Scientific Software Development", "abstract": "In this paper we describe how DUNE, an open source scientific software framework, is developed. Having a sustainable software framework for the solution of partial differential equations is the main driver of DUNE's development. We take a look how DUNE strives to stay sustainable software.", "venue": "ArXiv", "authors": ["Markus  Blatt"], "year": 2013, "n_citations": 5}
{"id": 1915512, "s2_id": "86f4826f5c8c2f41f2a2e9aa2ad15b8ba8d4c361", "title": "Abstract Compilation for Verification of Numerical Accuracy Properties", "abstract": "Verification of numerical accuracy properties in modern software remains an important and challenging task. This paper describes an original framework combining different solutions for numerical accuracy. First, we extend an existing runtime verification tool called E-ACSL with rational numbers to monitor accuracy properties at runtime. Second, we present an abstract compiler, FLDCompiler, that performs a source-to-source transformation such that the execution of the resulting program, called an abstract execution, is an abstract interpretation of the initial program. Third, we propose an instrumentation library FLDLib that formally propagates accuracy properties along an abstract execution. While each of these solutions has its own interest, we emphasize the benefits of their combination for an industrial setting. Initial experiments show that the proposed technique can efficiently and soundly analyze the accuracy of industrial programs by restricting the analysis on thin numerical scenarios.", "venue": "ArXiv", "authors": ["Maxime  Jacquemin", "Fonenantsoa  Maurica", "Nikolai  Kosmatov", "Julien  Signoles", "Franck  V'edrine"], "year": 2019, "n_citations": 0}
{"id": 1920052, "s2_id": "b193ce68417122a7bd65f01de08ed534d6c4b3c6", "title": "Magnus integrators on multicore CPUs and GPUs", "abstract": "Abstract In the present paper we consider numerical methods to solve the discrete Schrodinger equation with a time dependent Hamiltonian (motivated by problems encountered in the study of spin systems). We will consider both short-range interactions, which lead to evolution equations involving sparse matrices, and long-range interactions, which lead to dense matrices. Both of these settings show very different computational characteristics. We use Magnus integrators for time integration and employ a framework based on Leja interpolation to compute the resulting action of the matrix exponential. We consider both traditional Magnus integrators (which are extensively used for these types of problems in the literature) as well as the recently developed commutator-free Magnus integrators and implement them on modern CPU and GPU (graphics processing unit) based systems. We find that GPUs can yield a significant speed-up (up to a factor of 10 in the dense case) for these types of problems. In the sparse case GPUs are only advantageous for large problem sizes and the achieved speed-ups are more modest. In most cases the commutator-free variant is superior but especially on the GPU this advantage is rather small. In fact, none of the advantage of commutator-free methods on GPUs (and on multi-core CPUs) is due to the elimination of commutators. This has important consequences for the design of more efficient numerical methods.", "venue": "Comput. Phys. Commun.", "authors": ["N.  Auer", "Lukas  Einkemmer", "Peter  Kandolf", "Alexander  Ostermann"], "year": 2018, "n_citations": 13}
{"id": 1921390, "s2_id": "9e7d4d08eee494f88042aa2829bac1cdd8f36910", "title": "NetworkDynamics.jl - Composing and simulating complex networks in Julia", "abstract": "NetworkDynamics.jl is an easy-to-use and computationally efficient package for simulating heterogeneous dynamical systems on complex networks, written in Julia, a high-level, high-performance, dynamic programming language. By combining state-of-the-art solver algorithms from DifferentialEquations.jl with efficient data structures, NetworkDynamics.jl achieves top performance while supporting advanced features such as events, algebraic constraints, time delays, noise terms, and automatic differentiation.", "venue": "Chaos", "authors": ["Michael  Lindner", "Lucas  Lincoln", "Fenja  Drauschke", "Julia Monika Koulen", "Hans  W\u00fcrfel", "Anton  Plietzsch", "Frank  Hellmann"], "year": 2021, "n_citations": 4}
{"id": 1930210, "s2_id": "375f9fb3e9da0c8046fab7274a2b3197d3593e36", "title": "How Can I Do That with ACL2? Recent Enhancements to ACL2", "abstract": "The last several years have seen major enhancements to ACL2 functionality, largely driven by requests from its user community, including utilities now in common use such as 'make-event', 'mbe', and trust tags. In this paper we provide user-level summaries of some ACL2 enhancements introduced after the release of Version 3.5 (in May, 2009, at about the time of the 2009 ACL2 workshop) up through the release of Version 4.3 in July, 2011, roughly a couple of years later. Many of these features are not particularly well known yet, but most ACL2 users could take advantage of at least some of them. Some of the changes could affect existing proof efforts, such as a change that treats pairs of functions such as 'member' and 'member-equal' as the same function.", "venue": "ACL2", "authors": ["Matt  Kaufmann", "J Strother Moore"], "year": 2011, "n_citations": 5}
{"id": 1930940, "s2_id": "5db8f5961e81fd1668446c97faddfdd64aac6f3b", "title": "Computational Tools for Cohomology of Toric Varieties", "abstract": "Novel nonstandard techniques for the computation of cohomology classes on toric varieties are summarized. After an introduction of the basic definitions and properties of toric geometry, we discuss a specific computational algorithm for the determination of the dimension of line-bundle-valued cohomology groups on toric varieties. Applications to the computation of chiral massless matter spectra in string compactifications are discussed, and using the software package cohomCalg, its utility is highlighted on a new target space dual pair of (0,2) heterotic string models.", "venue": "ArXiv", "authors": ["Ralph  Blumenhagen", "Benjamin  Jurke", "Thorsten  Rahn"], "year": 2011, "n_citations": 20}
{"id": 1930945, "s2_id": "d853b15d462f66a0801725c1cb9878c58b893102", "title": "Parallel Element-based Algebraic Multigrid for H(curl) and H(div) Problems Using the ParELAG Library", "abstract": "This paper presents the use of element-based algebraic multigrid (AMGe) hierarchies, implemented in the ParELAG (Parallel Element Agglomeration Algebraic Multigrid Upscaling and Solvers) library, to produce multilevel preconditioners and solvers for H(curl) and H(div) formulations. ParELAG constructs hierarchies of compatible nested spaces, forming an exact de Rham sequence on each level. This allows the application of hybrid smoothers on all levels and AMS (Auxiliary-space Maxwell Solver) or ADS (Auxiliary-space Divergence Solver) on the coarsest levels, obtaining complete multigrid cycles. Numerical results are presented, showing the parallel performance of the proposed methods. As a part of the exposition, this paper demonstrates some of the capabilities of ParELAG and outlines some of the components and procedures within the library.", "venue": "ArXiv", "authors": ["Delyan Z. Kalchev", "Panayot S. Vassilevski", "Umberto  Villa"], "year": 2021, "n_citations": 0}
{"id": 1932335, "s2_id": "e90d23939c7fccad70a430da5693e3b850a7b5b3", "title": "Runtime-Flexible Multi-dimensional Arrays and Views for C++98 and C++0x", "abstract": "Multi-dimensional arrays are among the most fundamental and most useful data structures of all. In C++, excellent template libraries exist for arrays whose dimension is fixed at runtime. Arrays whose dimension can change at runtime have been implemented in C. However, a generic object-oriented C++ implementation of runtime-flexible arrays has so far been missing. In this article, we discuss our new implementation called Marray, a package of class templates that fills this gap. Marray is based on views as an underlying concept. This concept brings some of the flexibility known from script languages such as R and MATLAB to C++. Marray is free both for commercial and non-commercial use and is publicly available from www.andres.sc/marray", "venue": "ArXiv", "authors": ["Bj\u00f6rn  Andres", "Ullrich  K\u00f6the", "Thorben  Kr\u00f6ger", "Fred A. Hamprecht"], "year": 2010, "n_citations": 7}
{"id": 1933532, "s2_id": "a8837fd425a2804a809f5e1f430c2410b70a4492", "title": "User Manual for the Complex Conjugate Gradient Methods Library CCGPAK 2.0", "abstract": "This manual describes the library of conjugate gradients codes CCGPAK, which solves system of complex linear system of equations. The library is written in FORTRAN90 and is highly portable. The codes are general and provide mechanism for matrix times vector multiplication which is separated from the conjugate gradient iterations itself. It is simple to switch between single and double precisions. All codes follow the same naming conventions.", "venue": "ArXiv", "authors": ["Piotr J. Flatau"], "year": 2012, "n_citations": 0}
{"id": 1938717, "s2_id": "490b08dc922e23c5e21a459f0f017d7fe983aeba", "title": "Mathematical Software: Past, Present, and Future", "abstract": "This paper provides some reflections on the field of mathematical software on the occasion of John Rice\u2019s 65th birthday. I describe some of the common themes of research in this field and recall some significant events in its evolution. Finally, I raise a number of issues that are of concern to future developments.", "venue": "ArXiv", "authors": ["Ronald F. Boisvert"], "year": 2000, "n_citations": 10}
{"id": 1944599, "s2_id": "6a0f099fcb5a69acc8895ee052675172b68d403e", "title": "Towards whole program generation of quadrature-free discontinuous Galerkin methods for the shallow water equations", "abstract": "The shallow water equations (SWE) are a commonly used model to study tsunamis, tides, and coastal ocean circulation. However, there exist various approaches to discretize and solve them efficiently. Which of them is best for a certain scenario is often not known and, in addition, depends heavily on the used HPC platform. From a simulation software perspective, this places a premium on the ability to adapt easily to different numerical methods and hardware architectures. One solution to this problem is to apply code generation techniques and to express methods and specific hardware-dependent implementations on different levels of abstraction. This allows for a separation of concerns and makes it possible, e.g., to exchange the discretization scheme without having to rewrite all low-level optimized routines manually. In this paper, we show how code for an advanced quadrature-free discontinuous Galerkin (DG) discretized shallow water equation solver can be generated. Here, we follow the multi-layered approach from the ExaStencils project that starts from the continuous problem formulation, moves to the discrete scheme, spells out the numerical algorithms, and, finally, maps to a representation that can be transformed to a distributed memory parallel implementation by our in-house Scala-based source-to-source compiler. Our contributions include: A new quadrature-free discontinuous Galerkin formulation, an extension of the class of supported computational grids, and an extension of our toolchain allowing to evaluate discrete integrals stemming from the DG discretization implemented in Python. As first results we present the whole toolchain and also demonstrate the convergence of our method for higher order DG discretizations.", "venue": "ArXiv", "authors": ["Sara  Faghih-Naini", "Sebastian  Kuckuk", "Vadym  Aizinger", "Daniel  Zint", "Roberto  Grosso", "Harald  K\u00f6stler"], "year": 2019, "n_citations": 3}
{"id": 1947101, "s2_id": "acf4d116ff8992fead1b4167d8cfd560d63f1361", "title": "XMLlab : multimedia publication of simulations applets using XML and Scilab", "abstract": "We present an XML-based simulation authoring environment. The proposed description language allows to describe mathematical objects such as systems of ordinary differential equations, partial differential equations in two dimensions, or simple curves and surfaces. It also allows to describe the parameters on which these objects depend. This language is independent of the target software and allows to ensure the perennity of author's work, as well as collaborative work and content reuse. The actual implementation of XMLlab allows to run the generated simulations within the open source mathematical software Scilab, either locally when Scilab is installed on the client machines, or on thin clients running a simple web browser, when XMLlab and Scilab are installed on a distant server running a standard HTTP server.", "venue": "ArXiv", "authors": ["St\u00e9phane  Mottelet", "Andr\u00e9  Pauss"], "year": 2011, "n_citations": 1}
{"id": 1954474, "s2_id": "4b66bcd4c4f5edc4aec368fd68f5465a5c78d6b1", "title": "Parallel Triangular Solvers on GPU", "abstract": "In this paper, we investigate GPU based parallel triangular solvers systematically. The parallel triangular solvers are fundamental to incomplete LU factorization family preconditioners and algebraic multigrid solvers. We develop a new matrix format suitable for GPU devices. Parallel lower triangular solvers and upper triangular solvers are developed for this new data structure. With these solvers, ILU preconditioners and domain decomposition preconditioners are developed. Numerical results show that we can speed triangular solvers around seven times faster.", "venue": "ArXiv", "authors": ["Zhangxin  Chen", "Hui  Liu", "Bo  Yang"], "year": 2016, "n_citations": 9}
{"id": 1954987, "s2_id": "127d8d68adbb862721d536467690c418fe17125e", "title": "An Asynchronous Task-based Fan-Both Sparse Cholesky Solver", "abstract": "Author(s): Jacquelin, Mathias; Zheng, Yili; Ng, Esmond; Yelick, Katherine | Abstract: Systems of linear equations arise at the heart of many scientific and engineering applications. Many of these linear systems are sparse; i.e., most of the elements in the coefficient matrix are zero. Direct methods based on matrix factorizations are sometimes needed to ensure accurate solutions. For example, accurate solution of sparse linear systems is needed in shift-invert Lanczos to compute interior eigenvalues. The performance and resource usage of sparse matrix factorizations are critical to time-to-solution and maximum problem size solvable on a given platform. In many applications, the coefficient matrices are symmetric, and exploiting symmetry will reduce both the amount of work and storage cost required for factorization. When the factorization is performed on large-scale distributed memory platforms, communication cost is critical to the performance of the algorithm. At the same time, network topologies have become increasingly complex, so that modern platforms exhibit a high level of performance variability. This makes scheduling of computations an intricate and performance-critical task. In this paper, we investigate the use of an asynchronous task paradigm, one-sided communication and dynamic scheduling in implementing sparse Cholesky factorization (symPACK) on large-scale distributed memory platforms. Our solver symPACK relies on efficient and flexible communication primitives provided by the UPC++ library. Performance evaluation shows good scalability and that symPACK outperforms state-of-the-art parallel distributed memory factorization packages, validating our approach on practical cases.", "venue": "ArXiv", "authors": ["Mathias  Jacquelin", "Yili  Zheng", "Esmond G. Ng", "Katherine A. Yelick"], "year": 2016, "n_citations": 8}
{"id": 1955738, "s2_id": "7d30e29cfeac7640db70903ecff34599e9a83fe6", "title": "Krylov Subspace Recycling for Sequences of Shifted Linear Systems", "abstract": "Abstract We study the use of Krylov subspace recycling for the solution of a sequence of slowly-changing families of linear systems, where each family consists of shifted linear systems that differ in the coefficient matrix only by multiples of the identity. Our aim is to explore the simultaneous solution of each family of shifted systems within the framework of subspace recycling, using one augmented subspace to extract candidate solutions for all the shifted systems. The ideal method would use the same augmented subspace for all systems and have fixed storage requirements, independent of the number of shifted systems per family. We show that a method satisfying both requirements cannot exist in this framework. As an alternative, we introduce two schemes. One constructs a separate deflation space for each shifted system but solves each family of shifted systems simultaneously. The other builds only one recycled subspace and constructs approximate corrections to the solutions of the shifted systems at each cycle of the iterative linear solver while only minimizing the base system residual. At convergence of the base system solution, we apply the method recursively to the remaining unconverged systems. We present numerical examples involving systems arising in lattice quantum chromodynamics.", "venue": "ArXiv", "authors": ["Kirk M. Soodhalter", "Daniel B. Szyld", "Fei  Xue"], "year": 2013, "n_citations": 54}
{"id": 1960954, "s2_id": "54cdb2e1fc88d6e4442b4f0a3f19c84a53bc363c", "title": "Certified Exact Transcendental Real Number Computation in Coq", "abstract": "Reasoning about real number expressions in a proof assistant is challenging. Several problems in theorem proving can be solved by using exactreal number computation. I have implemented a library for reasoning and computing with complete metric spaces in the Coq proof assistant and used this library to build a constructive real number implementation including elementary real number functions and proofs of correctness. Using this library, I have created a tactic that automatically proves strict inequalities over closed elementary real number expressions by computation.", "venue": "TPHOLs", "authors": ["Russell  O'Connor"], "year": 2008, "n_citations": 52}
{"id": 1967337, "s2_id": "3e729f903682d49ba403e595b44c779e4d97ea82", "title": "A Batched GPU Methodology for Numerical Solutions of Partial Differential Equations", "abstract": "In this paper we present a methodology for data accesses when solving batches of Tridiagonal and Pentadiagonal matrices that all share the same left-hand-side (LHS) matrix. The intended application is to the numerical solution of Partial Differential Equations via the finite-difference method, although the methodology is applicable more broadly. By only storing one copy of this matrix, a significant reduction in storage overheads is obtained, together with a corresponding decrease in compute time. Taken together, these two performance enhancements lead to an overall more efficient implementation over the current state of the art algorithms cuThomasBatch and cuPentBatch, allowing for a greater number of systems to be solved on a single GPU. We demonstrate the methodology in the case of the Diffusion Equation, Hyperdiffusion Equation, and the Cahn\u2013Hilliard Equation, all in one spatial dimension. In this last example, we demonstrate how the method can be used to perform 2 independent simulations of phase separation in one dimension. In this way, we build up a robust statistical description of the coarsening phenomenon which is the defining behavior of phase separation. We anticipate that the method will be of further use in other similar contexts requiring statistical simulation of physical systems. Program Summary Program Title: CUDA Batched Tridiagonal and Pentadiagonal Schemes Licensing Provision: Apache License 2.0 Programming Languages: C, C++, CUDA Computer: Variable, equipped with CUDA capable GPU Operating System: Linux, Mac and Windows Nature of Problem: Various implementations of batched Pentadiagonal and Tridiagonal solvers exist for CUDA using an interleaved data layout format. All of the current state of the art implementations require large amounts of memory due to the need for every thread to have its own copy of the A matrix. There are many situations, particularly in the batch solving of PDEs in 1D and 2D, where the A matrix is the same, thus the data overheads are unnecessarily large reducing the number of systems that could be solved on a single GPU. Solution method: In this paper we eliminate the A matrix requirements and allow every thread to access the A matrix, a dramatic saving in memory and also grants an additional speed-up over the existing state of the art. Tridiagonal Functions Source: https://github.com/EndCar808/cuThomasConstantBatch Pentadiagonal Functions Source: https://github.com/munstermonster/cuPentConstantBatch", "venue": "ArXiv", "authors": ["Enda  Carroll", "Andrew  Gloster", "Miguel D. Bustamante", "Lennon 'O' N'araigh"], "year": 2021, "n_citations": 0}
{"id": 1971355, "s2_id": "1820be75d5d9b18e2a33992b227b374a1d75f8f9", "title": "A SageTeX Hypermatrix Algebra Package", "abstract": "We describe here a rudimentary sage implementation of the Bhattacharya-Mesner hypermatrix algebra package.", "venue": "ArXiv", "authors": ["Edinah K. Gnang", "Ori  Parzanchevski", "Yuval  Filmus"], "year": 2014, "n_citations": 0}
{"id": 1972076, "s2_id": "fc746de86732683d23a9f7d8a57e36833de1be29", "title": "hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems Governed by PDEs: Part I: Deterministic Inversion and Linearized Bayesian Inference", "abstract": "We present an extensible so\u0089ware framework, hIPPYlib, for solution of large-scale deterministic and Bayesian inverse problems governed by partial di\u0082erential equations (PDEs) with (possibly) in\u0080nite-dimensional parameter \u0080elds (which are high-dimensional a\u0089er discretization). hIPPYlib overcomes the prohibitively expensive nature of Bayesian inversion for this class of problems by implementing state-of-the-art scalable algorithms for PDE-based inverse problems that exploit the structure of the underlying operators, notably the Hessian of the log-posterior. \u008ce key property of the algorithms implemented in hIPPYlib is that the solution of the inverse problem is computed at a cost, measured in linearized forward PDE solves, that is independent of the parameter dimension. \u008ce mean of the posterior is approximated by the MAP point, which is found by minimizing the negative log-posterior with an inexact matrix-free Newton-CG method. \u008ce posterior covariance is approximated by the inverse of the Hessian of the negative log posterior evaluated at the MAP point. \u008ce construction of the posterior covariance is made tractable by invoking a low-rank approximation of the Hessian of the log-likelihood. Scalable tools for sample generation are also discussed. hIPPYlib makes all of these advanced algorithms easily accessible to domain scientists and provides an environment that expedites the development of new algorithms.", "venue": "ACM Trans. Math. Softw.", "authors": ["Umberto  Villa", "Noemi  Petra", "Omar  Ghattas"], "year": 2021, "n_citations": 19}
{"id": 1975810, "s2_id": "54a20dbd409436be4f188dfa9a78949a1cac230d", "title": "Benchmarking optimization software with performance profiles", "abstract": "Abstract.We propose performance profiles \u2014 distribution functions for a performance metric \u2014 as a tool for benchmarking and comparing optimization software. We show that performance profiles combine the best features of other tools for performance evaluation.", "venue": "Math. Program.", "authors": ["Elizabeth D. Dolan", "Jorge J. Mor\u00e9"], "year": 2002, "n_citations": 3115}
{"id": 1976610, "s2_id": "0bb70d96e5bab999ffa645992ee73365246c176b", "title": "The Unum Number Format: Mathematical Foundations, Implementation and Comparison to IEEE 754 Floating-Point Numbers", "abstract": "This thesis examines a modern concept for machine numbers based on interval arithmetic called 'Unums' and compares it to IEEE 754 floating-point arithmetic, evaluating possible uses of this format where floating-point numbers are inadequate. In the course of this examination, this thesis builds theoretical foundations for IEEE 754 floating-point numbers, interval arithmetic based on the projectively extended real numbers and Unums.", "venue": "ArXiv", "authors": ["Laslo  Hunhold"], "year": 2017, "n_citations": 1}
{"id": 1976767, "s2_id": "9266e328626bc8bdb01bcabedf855c5bea7fb39f", "title": "Out-of-core singular value decomposition", "abstract": "Singular value decomposition (SVD) is a standard matrix factorization technique that produces optimal low-rank approximations of matrices. It has diverse applications, including machine learning, data science and signal processing. However, many common problems involve very large matrices that cannot fit in the main memory of commodity computers, making it impractical to use standard SVD algorithms that assume fast random access or large amounts of space for intermediate calculations. To address this issue, we have implemented an out-of-core (external memory) randomized SVD solution that is fully scalable and efficiently parallelizable. This solution factors both dense and sparse matrices of arbitrarily large size within arbitrarily small memory limits, efficiently using out-of-core storage as needed. It uses an innovative technique for partitioning matrices that lends itself to out-of-core and parallel processing, as well as memory and I/O use planning, automatic load balancing, performance tuning, and makes possible a number of other practical enhancements to the current state-of-the-art. Furthermore, by using persistent external storage (generally HDDs or SSDs), users can resume interrupted operations without having to recalculate previously performed steps, solving a major practical problem in factoring very large matrices.", "venue": "ArXiv", "authors": ["Vadim  Demchik", "Miroslav  Bac\u00e1k", "Stefan  Bordag"], "year": 2019, "n_citations": 4}
{"id": 1983590, "s2_id": "599ff27e9808ee5772db71991c0bd1d9f2c1c90d", "title": "Bringing Trimmed Serendipity Methods to Computational Practice in Firedrake", "abstract": "We present an implementation of the trimmed serendipity finite element family, using the open source finite element package Firedrake. The new elements can be used seamlessly within the software suite for problems requiring H1, H (curl), or H (div)-conforming elements on meshes of squares or cubes. To test how well trimmed serendipity elements perform in comparison to traditional tensor product elements, we perform a sequence of numerical experiments including the primal Poisson, mixed Poisson, and Maxwell cavity eigenvalue problems. Overall, we find that the trimmed serendipity elements converge, as expected, at the same rate as the respective tensor product elements while being able to offer significant savings in the time or memory required to solve certain problems.", "venue": "ArXiv", "authors": ["Justin  Crum", "Cyrus  Cheng", "David A. Ham", "Lawrence  Mitchell", "Robert C. Kirby", "Joshua A. Levine", "Andrew  Gillette"], "year": 2021, "n_citations": 0}
{"id": 1983865, "s2_id": "fb3968cbccc5f601f34acf5a36a3a26a071eb6f0", "title": "Verified Real Number Calculations: A Library for Interval Arithmetic", "abstract": "Real number calculations on elementary functions are remarkably difficult to handle in mechanical proofs. In this paper, we show how these calculations can be performed within a theorem prover or proof assistant in a convenient and highly automated as well as interactive way. First, we formally establish upper and lower bounds for elementary functions. Then, based on these bounds, we develop a rational interval arithmetic where real number calculations take place in an algebraic setting. In order to reduce the dependency effect of interval arithmetic, we integrate two techniques: interval splitting and Taylor series expansions. This pragmatic approach has been developed, and formally verified, in a theorem prover. The formal development also includes a set of customizable strategies to automate proofs involving explicit calculations over real numbers. Our ultimate goal is to provide guaranteed proofs of numerical properties with minimal human theorem-prover interaction.", "venue": "IEEE Transactions on Computers", "authors": ["Marc  Daumas", "David R. Lester", "C\u00e9sar A. Mu\u00f1oz"], "year": 2009, "n_citations": 57}
{"id": 1985713, "s2_id": "0f39b5cc7f38220efab51e72d2cf3ab7eeb4567d", "title": "Proceedings of the 6th European Conference on Python in Science (EuroSciPy 2013)", "abstract": "These are the proceedings of the 6th European Conference on Python in Science, EuroSciPy 2013, that was held in Brussels (21-25 August 2013).", "venue": "ArXiv", "authors": ["Pierre de Buyl", "Nelle  Varoquaux"], "year": 2014, "n_citations": 1}
{"id": 1987902, "s2_id": "534f113dd62358de7bc1713c354e5ee113986b87", "title": "A user-guide to Gridap - grid-based approximation of partial differential equations in Julia", "abstract": "We present Gridap, a new scientific software library for the numerical approximation of partial differential equations (PDEs) using grid-based approximations. Gridap is an open-source software project exclusively written in the Julia programming language. The main motivation behind the development of this library is to provide an easy-to-use framework for the development of complex PDE solvers in a dynamically typed style without sacrificing the performance of statically typed languages. This work is a tutorial-driven user guide to the library. It covers some popular linear and nonlinear PDE systems for scalar and vector fields, single and multi-field problems, conforming and nonconforming finite element discretizations, on structured and unstructured meshes of simplices and hexahedra.", "venue": "ArXiv", "authors": ["Francesc  Verdugo", "Santiago  Badia"], "year": 2019, "n_citations": 2}
{"id": 1990422, "s2_id": "35d64ed69261f11d21549317ced2e8541761dd65", "title": "Fast Multiplication of Large Integers: Implementation and Analysis of the DKSS Algorithm", "abstract": "The Sch\\\"onhage-Strassen algorithm (SSA) is the de-facto standard for multiplication of large integers. For $N$-bit numbers it has a time bound of $O(N \\cdot \\log N \\cdot \\log \\log N)$. De, Kurur, Saha and Saptharishi (DKSS) presented an asymptotically faster algorithm with a better time bound of $N \\cdot \\log N \\cdot 2^{O(\\log^* N)}$. In this diploma thesis, results of an implementation of DKSS multiplication are presented: run-time is about 30 times larger than SSA, while memory requirements are about 3.75 times higher than SSA. A possible crossover point is estimated to be out of reach even if we utilized the whole universe for computer memory.", "venue": "ArXiv", "authors": ["Christoph  L\u00fcders"], "year": 2015, "n_citations": 2}
{"id": 1994230, "s2_id": "fc97417a7227bd0195372c76b4e591ebf79992da", "title": "Implicit SVD for Graph Representation Learning", "abstract": "Recent improvements in the performance of state-of-the-art (SOTA) methods for Graph Representational Learning (GRL) have come at the cost of significant computational resource requirements for training, e.g., for calculating gradients via backprop over many data epochs. Meanwhile, Singular Value Decomposition (SVD) can find closed-form solutions to convex problems, using merely a handful of epochs. In this paper, we make GRL more computationally tractable for those with modest hardware. We design a framework that computes SVD of implicitly defined matrices, and apply this framework to several GRL tasks. For each task, we derive linear approximation of a SOTA model, where we design (expensiveto-store) matrix M and train the model, in closed-form, via SVD of M, without calculating entries of M. By converging to a unique point in one step, and without calculating gradients, our models show competitive empirical test performance over various graphs such as article citation and biological interaction networks. More importantly, SVD can initialize a deeper model, that is architected to be nonlinear almost everywhere, though behaves linearly when its parameters reside on a hyperplane, onto which SVD initializes. The deeper model can then be fine-tuned within only a few epochs. Overall, our procedure trains hundreds of times faster than state-of-the-art methods, while competing on empirical test performance. We open-source our implementation at: https://github.com/samihaija/isvd", "venue": "ArXiv", "authors": ["Sami  Abu-El-Haija", "Hesham  Mostafa", "Marcel  Nassar", "Valentino  Crespi", "Greg Ver Steeg", "Aram  Galstyan"], "year": 2021, "n_citations": 0}
{"id": 1994265, "s2_id": "9b5f9fffd1697817c64c22dfb1549fa0b0395a40", "title": "Non-Conforming Mesh Refinement for High-Order Finite Elements", "abstract": "We propose a general algorithm for non-conforming adaptive mesh refinement (AMR) of unstructured meshes in high-order finite element codes. Our focus is on h-refinement with a fixed polynomial order. The algorithm handles triangular, quadrilateral, hexahedral and prismatic meshes of arbitrarily high order curvature, for any order finite element space in the de Rham sequence. We present a flexible data structure for meshes with hanging nodes and a general procedure to construct the conforming interpolation operator, both in serial and in parallel. The algorithm and data structure allow anisotropic refinement of tensor product elements in 2D and 3D, and support unlimited refinement ratios of adjacent elements. We report numerical experiments verifying the correctness of the algorithms, and perform a parallel scaling study to show that we can adapt meshes containing billions of elements and run efficiently on 393,000 parallel tasks. Finally, we illustrate the integration of dynamic AMR into a high-order Lagrangian hydrodynamics solver.", "venue": "ArXiv", "authors": ["Jakub  Cerven\u00fd", "Veselin  Dobrev", "Tzanio V. Kolev"], "year": 2019, "n_citations": 5}
{"id": 1996415, "s2_id": "dcceb0a70b759ce508f0e3bfc41a02474d57f37e", "title": "Object-oriented implementations of the MPDATA advection equation solver in C++, Python and Fortran", "abstract": "Three object-oriented implementations of a prototype solver of the advection equation are introduced. The presented programs are based on Blitz++ (C++), NumPy (Python), and Fortran\u2019s built-in array containers .T he solvers include an implementation of the Multidimensional Positive-Definite Advective Transport Algorithm (MPDATA). The introduced codes exemplify ho wt he application of object-oriented programming (OOP) techniques allows to reproduce the mathematical notation used in th el iterature within the program code. A discussion on the tradeo! so f the programming language choice is presented. The main a ngles of comparison are code brevity and syntax clarity (and hence maintainability and auditability) as well as performance. In the case of Python, a significant performance gain is observed when switching from the standard interpreter (CPython) to the PyPy implementation of Python. Entire source code of all three implementations is embedded in the text and is licensed under the terms of the GNU GPL license.", "venue": "ArXiv", "authors": ["Sylwester  Arabas", "Dorota  Jarecka", "Anna  Jaruga", "Maciej  Fijalkowski"], "year": 2013, "n_citations": 1}
{"id": 2000845, "s2_id": "159538682b34cd1b9700adb8a08007857463da58", "title": "Algorithms and complexity for Turaev\u2013Viro invariants", "abstract": "The Turaev\u2013Viro invariants are a powerful family of topological invariants for distinguishing between different 3-manifolds. They are invaluable for mathematical software, but current algorithms to compute them require exponential time. The invariants are parameterized by an integer $$r \\ge 3$$r\u22653. We resolve the question of complexity for $$r=3$$r=3 and $$r=4$$r=4, giving simple proofs that the Turaev\u2013Viro invariants for $$r=3$$r=3 can be computed in polynomial time, but computing the invariant for $$r=4$$r=4 is #P-hard. Moreover, we describe an algorithm for arbitrary r, which is fixed-parameter tractable with respect to the treewidth of the dual graph of the input triangulation. We show through concrete implementation and experimentation that this algorithm is practical\u2014and indeed preferable\u2014to the prior state of the art for real computation. The algorithm generalises to every triangulated 3-manifold invariant defined from tensor network contraction.", "venue": "J. Appl. Comput. Topol.", "authors": ["Benjamin A. Burton", "Cl\u00e9ment  Maria", "Jonathan  Spreer"], "year": 2018, "n_citations": 12}
{"id": 2009471, "s2_id": "dd2a7a05e4631350bec31f46452d27f05cafe9ae", "title": "A Hybrid Parallelization of AIM for Multi-Core Clusters: Implementation Details and Benchmark Results on Ranger", "abstract": "This paper presents implementation details and empirical results for a hybrid message passing and shared memory paralleliziation of the adaptive integral method (AIM). AIM is implemented on a (near) petaflop supercomputing cluster of quad-core processors and its accuracy, complexity, and scalability are investigated by solving benchmark scattering problems. The timing and speedup results on up to 1024 processors show that the hybrid MPI/OpenMP parallelization of AIM exhibits better strong scalability (fixed problem size speedup) than pure MPI parallelization of it when multiple cores are used on each processor.", "venue": "ArXiv", "authors": ["Fangzhou  Wei", "Ali E. Yilmaz"], "year": 2010, "n_citations": 4}
{"id": 2011324, "s2_id": "fb98d074dd4a56a359e4179ba80ebadc8211bfcc", "title": "On the bit-complexity of sparse polynomial multiplication", "abstract": "In this paper, we present fast algorithms for the product of two multivariate polynomials in sparse representation. The bit complexity of our algorithms are studied in detail for various types of coefficients, and we derive new complexity results for the power series multiplication in many variables. Our algorithms are implemented and freely available within the Mathemagix software. We show that their theoretical costs are well-reflected in practice.", "venue": "ArXiv", "authors": ["Joris van der Hoeven", "Gr\u00e9goire  Lecerf"], "year": 2009, "n_citations": 13}
{"id": 2016031, "s2_id": "89bd25ef3eaa8b1eaabee0816a70b930720db953", "title": "Cross-Platform Performance Portability Using Highly Parametrized SYCL Kernels", "abstract": "Over recent years heterogeneous systems have become more prevalent across HPC systems, with over 100 supercomputers in the TOP500 incorporating GPUs or other accelerators. These hardware platforms have different performance characteristics and optimization requirements. In order to make the most of multiple accelerators a developer has to provide implementations of their algorithms tuned for each device. Hardware vendors provide libraries targeting their devices specifically, which provide good performance but frequently have different API designs, hampering portability. \nThe SYCL programming model allows users to write heterogeneous programs using completely standard C++, and so developers have access to the power of C++ templates when developing compute kernels. In this paper we show that by writing highly parameterized kernels for matrix multiplies and convolutions we achieve performance competitive with vendor implementations across different architectures. Furthermore, tuning for new devices amounts to choosing the combinations of kernel parameters that perform best on the hardware.", "venue": "ArXiv", "authors": ["John  Lawson", "Mehdi  Goli", "Duncan  McBain", "Daniel  Soutar", "Louis  Sugy"], "year": 2019, "n_citations": 4}
{"id": 2022244, "s2_id": "5f5ffc487175a61ec6626524839a6a79c6ada076", "title": "Scalable Metropolis Monte Carlo for simulation of hard shapes", "abstract": "Abstract We design and implement a scalable hard particle Monte Carlo simulation toolkit (HPMC), and release it open source as part of HOOMD-blue. HPMC runs in parallel on many CPUs and many GPUs using domain decomposition. We employ BVH trees instead of cell lists on the CPU for fast performance, especially with large particle size disparity, and optimize inner loops with SIMD vector intrinsics on the CPU. Our GPU kernel proposes many trial moves in parallel on a checkerboard and uses a block-level queue to redistribute work among threads and avoid divergence. HPMC supports a wide variety of shape classes, including spheres/disks, unions of spheres, convex polygons, convex spheropolygons, concave polygons, ellipsoids/ellipses, convex polyhedra, convex spheropolyhedra, spheres cut by planes, and concave polyhedra. NVT and NPT ensembles can be run in 2D or 3D triclinic boxes. Additional integration schemes permit Frenkel\u2013Ladd free energy computations and implicit depletant simulations. In a benchmark system of a fluid of 4096 pentagons, HPMC performs 10 million sweeps in 10\u00a0min on 96 CPU cores on XSEDE Comet. The same simulation would take 7.6\u00a0h in serial. HPMC also scales to large system sizes, and the same benchmark with 16.8 million particles runs in 1.4\u00a0h on 2048 GPUs on OLCF Titan.", "venue": "Comput. Phys. Commun.", "authors": ["Joshua A. Anderson", "M. Eric Irrgang", "Sharon C. Glotzer"], "year": 2016, "n_citations": 60}
{"id": 2023461, "s2_id": "c0058a86d8c613a9340720ff59db7a4b8793d077", "title": "Highly efficient lattice Boltzmann multiphase simulations of immiscible fluids at high-density ratios on CPUs and GPUs through code generation", "abstract": "A high-performance implementation of a multiphase lattice Boltzmann method based on the conservative Allen-Cahn model supporting high-density ratios and high Reynolds numbers is presented. Meta-programming techniques are used to generate optimized code for CPUs and GPUs automatically. The coupled model is specified in a high-level symbolic description and optimized through automatic transformations. The memory footprint of the resulting algorithm is reduced through the fusion of compute kernels. A roofline analysis demonstrates the excellent efficiency of the generated code on a single GPU. The resulting single GPU code has been integrated into the multiphysics framework waLBerla to run massively parallel simulations on large domains. Communication hiding and GPUDirect-enabled MPI yield near-perfect scaling behavior. Scaling experiments are conducted on the Piz Daint supercomputer with up to 2048 GPUs, simulating several hundred fully resolved bubbles. Further, validation of the implementation is shown in a physically relevant scenario\u2014a three-dimensional rising air bubble in water.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Markus  Holzer", "Martin  Bauer", "Harald  K\u00f6stler", "Ulrich  R\u00fcde"], "year": 2021, "n_citations": 4}
{"id": 2024507, "s2_id": "75480fb8a593edb65363aef8ac9ee322e9ca246a", "title": "Parallel time integration using Batched BLAS (Basic Linear Algebra Subprograms) routines", "abstract": "We present an approach for integrating the time evolution of quantum systems. We leverage the computation power of graphics processing units (GPUs) to perform the integration of all time steps in parallel. The performance boost is especially prominent for small to medium-sized quantum systems. The devised algorithm can largely be implemented using the recently-specified batched versions of the BLAS routines, and can therefore be easily ported to a variety of platforms. Our PARAllelized Matrix Exponentiation for Numerical Time evolution (PARAMENT) implementation runs on CUDA-enabled graphics processing units.", "venue": "Computer Physics Communications", "authors": ["Konstantin  Herb", "Pol  Welter"], "year": 2022, "n_citations": 0}
{"id": 2024563, "s2_id": "c2c18e683e1cb92bc149e4a8d3079609931c475e", "title": "Automatic differentiation of ODE integration", "abstract": "We discuss the calculation of the derivatives of ODE systems with the automatic differentiation tool ADiMat. Using the well-known Lotka-Volterra equations and the ode23 ODE solver as examples we show the analytic derivatives and detail how to differentiate a top-level function that calls ode23 somewhere with ADiMat. This involves the manual construction of substitution function to propagate the derivatives in forward and reverse mode. We also show how to use the reverse mode code to evaluate the Hessian in forward-over-reverse mode.", "venue": "ArXiv", "authors": ["Johannes  Willkomm"], "year": 2018, "n_citations": 0}
{"id": 2024600, "s2_id": "c20e213f056fe9a836fbf8e50a7a353d9f46ff5a", "title": "Formal Mathematics on Display: A Wiki for Flyspeck", "abstract": "The Agora system is a prototype \"Wiki for Formal Mathematics\", with an aim to support developing and documenting large formalizations of mathematics in a proof assistant. The functions implemented in Agora include in-browser editing, strong AI/ATP proof advice, verification, and HTML rendering. The HTML rendering contains hyperlinks and provides on-demand explanation of the proof state for each proof step. In the present paper we show the prototype Flyspeck Wiki as an instance of Agora for HOL Light formalizations. The wiki can be used for formalizations of mathematics and for writing informal wiki pages about mathematics. Such informal pages may contain islands of formal text, which is used here for providing an initial cross-linking between Hales's informal Flyspeck book, and the formal Flyspeck development. \n \nThe Agora platform intends to address distributed wiki-style collaboration on large formalization projects, in particular both the aspect of immediate editing, verification and rendering of formal code, and the aspect of gradual and mutual refactoring and correspondence of the initial informal text and its formalization. Here, we highlight these features within the Flyspeck Wiki.", "venue": "MKM/Calculemus/DML", "authors": ["Carst  Tankink", "Cezary  Kaliszyk", "Josef  Urban", "Herman  Geuvers"], "year": 2013, "n_citations": 20}
{"id": 2027521, "s2_id": "5c237ddb9cf266e5236cdc2dbac429d41d39c7f9", "title": "Run-Time Extensibility and Librarization of Simulation Software", "abstract": "Build-time configuration and environment assumptions are hampering progress and usability in scientific software. This situation, which would be utterly unacceptable in nonscientific software, somehow passes for the norm in scientific packages. The scientific software community needs reusable, easy-to-use software packages that are flexible enough to accommodate next-generation simulation and analysis demands.", "venue": "Computing in Science & Engineering", "authors": ["Jed  Brown", "Matthew G. Knepley", "Barry  Smith"], "year": 2015, "n_citations": 17}
{"id": 2028050, "s2_id": "f0d35b37fec26c3f1ed09253cbb9304fb62208d1", "title": "SciPy 1.0: fundamental algorithms for scientific computing in Python", "abstract": "SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments. This Perspective describes the development and capabilities of SciPy 1.0, an open source scientific computing library for the Python programming language.", "venue": "Nature Methods", "authors": ["Pauli  Virtanen", "Ralf  Gommers", "Travis E. Oliphant", "Matt  Haberland", "Tyler  Reddy", "David  Cournapeau", "Evgeni  Burovski", "Pearu  Peterson", "Warren  Weckesser", "Jonathan  Bright", "St\u00e9fan J. van der Walt", "Matthew  Brett", "Joshua  Wilson", "K. Jarrod Millman", "Nikolay  Mayorov", "Andrew R. J. Nelson", "Eric  Jones", "Robert  Kern", "Eric  Larson", "C J Carey", "\u0130lhan  Polat", "Yu  Feng", "Eric W. Moore", "Jake  VanderPlas", "Denis  Laxalde", "Josef  Perktold", "Robert  Cimrman", "Ian  Henriksen", "E. A. Quintero", "Charles R. Harris", "Anne M. Archibald", "Ant\u00f4nio H. Ribeiro", "Fabian  Pedregosa", "Paul  van Mulbregt", "Aditya Alessandro Pietro Alex Andreas Andreas Anthony Ant Vijaykumar Bardelli Rothberg Hilboll Kloeckner Sco", "Aditya  Vijaykumar", "Alessandro Pietro Bardelli", "Alex  Rothberg", "Andreas  Hilboll", "Andreas  Kloeckner", "Anthony  Scopatz", "Antony  Lee", "Ariel  Rokem", "C. Nathan Woods", "Chad  Fulton", "Charles  Masson", "Christian  H\u00e4ggstr\u00f6m", "Clark  Fitzgerald", "David A. Nicholson", "David R. Hagen", "Dmitrii V. Pasechnik", "Emanuele  Olivetti", "Eric  Martin", "Eric  Wieser", "Fabrice  Silva", "Felix  Lenders", "Florian  Wilhelm", "G.  Young", "Gavin A. Price", "Gert-Ludwig  Ingold", "Gregory E. Allen", "Gregory R. Lee", "Herv\u00e9  Audren", "Irvin  Probst", "J\u00f6rg P. Dietrich", "Jacob  Silterra", "James T Webber", "Janko  Slavi\u010d", "Joel  Nothman", "Johannes  Buchner", "Johannes  Kulick", "Johannes L. Sch\u00f6nberger", "Jos\u00e9 Vin\u00edcius de Miranda Cardoso", "Joscha  Reimer", "Joseph  Harrington", "Juan Luis Cano Rodr\u00edguez", "Juan  Nunez-Iglesias", "Justin  Kuczynski", "Kevin  Tritz", "Martin  Thoma", "Matthew  Newville", "Matthias  K\u00fcmmerer", "Maximilian  Bolingbroke", "Michael  Tartre", "Mikhail  Pak", "Nathaniel J. Smith", "Nikolai  Nowaczyk", "Nikolay  Shebanov", "Oleksandr  Pavlyk", "Per A. Brodtkorb", "Perry  Lee", "Robert T. McGibbon", "Roman  Feldbauer", "Sam  Lewis", "Sam  Tygier", "Scott  Sievert", "Sebastiano  Vigna", "Stefan  Peterson", "Surhud  More", "Tadeusz  Pudlik", "Takuya  Oshima", "Thomas J. Pingel", "Thomas P. Robitaille", "Thomas  Spura", "Thouis R. Jones", "Tim  Cera", "Tim  Leslie", "Tiziano  Zito", "Tom  Krauss", "Utkarsh  Upadhyay", "Yaroslav O. Halchenko", "Yoshiki  V\u00e1zquez-Baeza"], "year": 2020, "n_citations": 6283}
{"id": 2028814, "s2_id": "629d1e6427d15a201de098c3359962a95a6a4161", "title": "Strong pseudoprimes to twelve prime bases", "abstract": "Let $\\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases. This value is known for $1 \\leq m \\leq 11$. We extend this by finding $\\psi_{12}$ and $\\psi_{13}$. We also present an algorithm to find all integers $n\\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a reasonable heuristic assumption we can show that it takes at most $B^{2/3+o(1)}$ time.", "venue": "Math. Comput.", "authors": ["Jonathan P. Sorenson", "Jonathan  Webster"], "year": 2017, "n_citations": 18}
{"id": 2030836, "s2_id": "a38200d8d66b7b7f93f88c42c41ea6444e990739", "title": "emgr - The Empirical Gramian Framework", "abstract": "System Gramian matrices are a well-known encoding for properties of input-output systems such as controllability, observability or minimality. These so-called system Gramians were developed in linear system theory for applications such as model order reduction of control systems. Empirical Gramians are an extension to the system Gramians for parametric and nonlinear systems as well as a data-driven method of computation. The empirical Gramian framework - emgr - implements the empirical Gramians in a uniform and configurable manner, with applications such as Gramian-based (nonlinear) model reduction, decentralized control, sensitivity analysis, parameter identification and combined state and parameter reduction.", "venue": "Algorithms", "authors": ["Christian  Himpe"], "year": 2018, "n_citations": 16}
{"id": 2033602, "s2_id": "8110568d0e2df08e30ed44a57fa51b9641ed90a8", "title": "A Practical Approach to Testing Random Number Generators in Computer Algebra Systems", "abstract": "Abstract This paper has a practical aim. For a long time, implementations of pseudorandom number generators in standard libraries of programming languages had poor quality. The situation started to improve only recently. Up to now, a large number of libraries and weakly supported mathematical packages use outdated algorithms for random number generation. Four modern sets of statistical tests that can be used for verifying random number generators are described. It is proposed to use command line utilities, which makes it possible to avoid low-level programming in such languages as C or C++. Only free open source systems are considered.", "venue": "ArXiv", "authors": ["Migran N. Gevorkyan", "Dmitry S. Kulyabov", "Anastasia V. Demidova", "Anna V. Korolkova"], "year": 2020, "n_citations": 0}
{"id": 2033777, "s2_id": "5264224d2d9fa91701f7c86a20b326d7a7db0ec1", "title": "Fast Computational Algorithms for the Discrete Wavelet Transform and Applications of Localized Orthonormal Bases in Signal Classification", "abstract": "We construct an algorithm for implementing the discrete wavelet transform by means of matrices in SO_2(R) for orthonormal compactly supported wavelets and matrices in SL_m(R), m > = 2, for compactly supported biorthogonal wavelets. We show that in 1 dimension the total operation count using this algorithm can be reduced to about 50% of the conventional convolution and downsampling by 2-operation for both orthonormal and biorthogonal filters. In the special case of biorthogonal symmetric odd-odd filters, we show an implementation yielding a total operation count of about 38% of the conventional method. In 2 dimensions we show an implementation of this algorithm yielding a reduction in the total operation count of about 70% when the filters are orthonormal, a reduction of about 62% for general biorthogonal filters, and a reduction of about 70% if the filters are symmetric odd-odd length filters. We further extend these results to 3 dimensions. We also show how the SO_2(R)-method for implementing the discrete wavelet transform may be exploited to compute short FIR filters, and we construct edge mappings where we try to improve upon the degree of preservation of regularity in the conventional methods. We also consider a two-class waveform discrimination problem. A statistical space-frequency analysis is performed on a training data set using the LDB-algorithm of N.Saito and R.Coifman. The success of the algorithm on this particular problem is evaluated on a disjoint test data set.", "venue": "ArXiv", "authors": ["Eirik  Fossg\u00e5rd"], "year": 1999, "n_citations": 5}
{"id": 2037399, "s2_id": "06ad2e1d9bbeb558aee7e9ee4e26f7b1a0b41bff", "title": "BOAT: a cross-platform software for data analysis and numerical computing with arbitrary-precision", "abstract": "BOAT is a free cross-platform software for statistical data analysis and numerical computing. Thanks to its multiple-precision floating point engine, it allows arbitrary-precision calculations, whose digits of precision are only limited by the amount of memory of the host machine. At the core of the software is a simple and efficient expression language, whose use is facilitated by the assisted typing, the auto-complete engine and the built-in help for the syntax. In this paper a quick overview of the software is given. Detailed information, together with its applications to some case studies, is available at the BOAT web page.", "venue": "ArXiv", "authors": ["Davide  Pagano"], "year": 2015, "n_citations": 0}
{"id": 2038927, "s2_id": "62df84d6a4d26f95e4714796c2337c9848cc13b5", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems", "abstract": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. \nThis paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.", "venue": "ArXiv", "authors": ["Tianqi  Chen", "Mu  Li", "Yutian  Li", "Min  Lin", "Naiyan  Wang", "Minjie  Wang", "Tianjun  Xiao", "Bing  Xu", "Chiyuan  Zhang", "Zheng  Zhang"], "year": 2015, "n_citations": 1745}
{"id": 2054797, "s2_id": "26618871e93dfc563600329a8439d61826aab73f", "title": "FEAST As A Subspace Iteration Eigensolver Accelerated By Approximate Spectral Projection", "abstract": "The calculation of a segment of eigenvalues and their corresponding eigenvectors of a Hermitian matrix or matrix pencil has many applications. A new density-matrix-based algorithm has been proposed recently and a software package FEAST has been developed. The density-matrix approach allows FEAST's implementation to exploit a key strength of modern computer architectures, namely, multiple levels of parallelism. Consequently, the software package has been well received, especially in the electronic structure community. Nevertheless, theoretical analysis of FEAST has lagged. For instance, the FEAST algorithm has not been proven to converge. This paper offers a detailed numerical analysis of FEAST. In particular, we show that the FEAST algorithm can be understood as an accelerated subspace iteration algorithm in conjunction with the Rayleigh-Ritz procedure. The novelty of FEAST lies in its accelerator which is a rational matrix function that approximates the spectral projector onto the eigenspace in question. Analysis of the numerical nature of this approximate spectral projector and the resulting subspaces generated in the FEAST algorithm establishes the algorithm's convergence. This paper shows that FEAST is resilient against rounding errors and establishes properties that can be leveraged to enhance the algorithm's robustness. Finally, we propose an extension of FEAST to handle non-Hermitian problems and suggest some future research directions.", "venue": "SIAM J. Matrix Anal. Appl.", "authors": ["Ping Tak Peter Tang", "Eric  Polizzi"], "year": 2014, "n_citations": 107}
{"id": 2058291, "s2_id": "fcc6e40a55656015dfaaa08d8dd8cac8d7da9120", "title": "Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients", "abstract": "Applying differentiable programming techniques and machine learning algorithms to foreign programs requires developers to either rewrite their code in a machine learning framework, or otherwise provide derivatives of the foreign code. This paper presents Enzyme, a high-performance automatic differentiation (AD) compiler plugin for the LLVM compiler framework capable of synthesizing gradients of statically analyzable programs expressed in the LLVM intermediate representation (IR). Enzyme synthesizes gradients for programs written in any language whose compiler targets LLVM IR including C, C++, Fortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD capabilities in these languages. Unlike traditional source-to-source and operator-overloading tools, Enzyme performs AD on optimized IR. On a machine-learning focused benchmark suite including Microsoft's ADBench, AD on optimized IR achieves a geometric mean speedup of 4.5x over AD on IR before optimization allowing Enzyme to achieve state-of-the-art performance. Packaging Enzyme for PyTorch and TensorFlow provides convenient access to gradients of foreign code with state-of-the art performance, enabling foreign code to be directly incorporated into existing machine learning workflows.", "venue": "NeurIPS", "authors": ["William S. Moses", "Valentin  Churavy"], "year": 2020, "n_citations": 6}
{"id": 2058989, "s2_id": "eec7deedc0588f48acac6681ec3f1ab95c000340", "title": "A framework for the automation of generalised stability theory", "abstract": "The traditional approach to investigating the stability of a physical system is to linearise the equations about a steady base solution, and to examine the eigenvalues of the linearised operator. Over the past several decades, it has been recognised that this approach only determines the asymptotic stability of the system, and neglects the possibility of transient perturbation growth arising due to the nonnormality of the system. This observation motivated the development of a more powerful generalised stability theory (GST), which focusses instead on the singular value decomposition of the linearised propagator of the system. While GST has had significant successes in understanding the stability of phenomena in geophysical fluid dynamics, its more widespread applicability has been hampered by the fact that computing the SVD requires both the tangent linear operator and its adjoint: deriving the tangent linear and adjoint models is usually a considerable challenge, and manually embedding them inside an eigensolver is laborious. In this paper, we present a framework for the automation of generalised stability theory, which overcomes these difficulties. Given a compact high-level symbolic representation of a finite element discretisation implemented in the FEniCS system, efficient C++ code is automatically generated to assemble the forward, tangent linear and adjoint models; these models are then used to calculate the optimally growing perturbations to the forward model, and their growth rates. By automating the stability computations, we hope to make these powerful tools a more routine part of computational analysis. The efficiency and generality of the framework is demonstrated with applications drawn from geophysical fluid dynamics, phase separation and quantum mechanics.", "venue": "ArXiv", "authors": ["Patrick E. Farrell", "Colin J. Cotter", "Simon W. Funke"], "year": 2012, "n_citations": 2}
{"id": 2064701, "s2_id": "88c50ff75be303d43d47230a0e082fed5dc533eb", "title": "Parallel memory-efficient all-at-once algorithms for the sparse matrix triple products in multigrid methods", "abstract": "Multilevel/multigrid methods is one of the most popular approaches for solving a large sparse linear system of equations, typically, arising from the discretization of partial differential equations. One critical step in the multilevel/multigrid methods is to form coarse matrices through a sequence of sparse matrix triple products. A commonly used approach for the triple products explicitly involves two steps, and during each step a sparse matrix-matrix multiplication is employed. This approach works well for many applications with a good computational efficiency, but it has a high memory overhead since some auxiliary matrices need to be temporarily stored for accomplishing the calculations. In this work, we propose two new algorithms that construct a coarse matrix with taking one pass through the input matrices without involving any auxiliary matrices for saving memory. The new approaches are referred to as \"all-at-once\" and \"merged all-at-once\", and the traditional method is denoted as \"two-step\". The all-at-once and the merged all-at-once algorithms are implemented based on hash tables in PETSc as part of this work with a careful consideration on the performance in terms of the compute time and the memory usage. We numerically show that the proposed algorithms and their implementations are perfectly scalable in both the compute time and the memory usage with up to 32,768 processor cores for a model problem with 27 billions of unknowns. The scalability is also demonstrated for a realistic neutron transport problem with over 2 billion unknowns on a supercomputer with 10,000 processor cores. Compared with the traditional two-step method, the all-at-once and the merged all-at-once algorithms consume much less memory for both the model problem and the realistic neutron transport problem meanwhile they are able to maintain the computational efficiency.", "venue": "ArXiv", "authors": ["Fande  Kong"], "year": 2019, "n_citations": 2}
{"id": 2077540, "s2_id": "37f1a0d2fe0a7f59c731484eabff7faa18e0d729", "title": "What the new RooFit can do for your analysis", "abstract": "RooFit is a toolkit for statistical modelling and fitting, and together with RooStats it is used for measurements and statistical tests by most experiments in particle physics. Since one year, RooFit is being modernised. In this talk, improvements already released with ROOT will be discussed, such as faster data loading, vectorised computations and more standard-like interfaces. These allow for speeding up unbinned fits by several factors, and make RooFit easier to use from both C++ and Python.", "venue": "ArXiv", "authors": ["Stephan  Hageboeck"], "year": 2020, "n_citations": 2}
{"id": 2078086, "s2_id": "0016c4e41be51c01c5996501b020127592ec9e43", "title": "dMath: Distributed Linear Algebra for DL", "abstract": "The paper presents a parallel math library, dMath, that demonstrates leading scaling when using intranode, internode, and hybrid-parallelism for deep learning (DL). dMath provides easy-to-use distributed primitives and a variety of domain-specific algorithms including matrix multiplication, convolutions, and others allowing for rapid development of scalable applications like deep neural networks (DNNs). Persistent data stored in GPU memory and advanced memory management techniques avoid costly transfers between host and device. dMath delivers performance, portability, and productivity to its specific domain of support.", "venue": "ArXiv", "authors": ["Steven  Eliuk", "Cameron  Upright", "Hars  Vardhan", "Stephen  Walsh", "Trevor  Gale"], "year": 2016, "n_citations": 1}
{"id": 2082569, "s2_id": "5fe3256da3a7b6f3653aa2d8d83353d97e14100a", "title": "Achieving algorithmic resilience for temporal integration through spectral deferred corrections", "abstract": "Spectral deferred corrections (SDC) is an iterative approach for constructing higher- order accurate numerical approximations of ordinary differential equations. SDC starts with an initial approximation of the solution defined at a set of Gaussian or spectral collocation nodes over a time interval and uses an iterative application of lower-order time discretizations applied to a correction equation to improve the solution at these nodes. Each deferred correction sweep increases the formal order of accuracy of the method up to the limit inherent in the accuracy defined by the collocation points. In this paper, we demonstrate that SDC is well suited to recovering from soft (transient) hardware faults in the data. A strategy where extra correction iterations are used to recover from soft errors and provide algorithmic resilience is proposed. Specifically, in this approach the iteration is continued until the residual (a measure of the error in the approximation) is small relative to the residual on the first correction iteration and changes slowly between successive iterations. We demonstrate the effectiveness of this strategy for both canonical test problems and a comprehen- sive situation involving a mature scientific application code that solves the reacting Navier-Stokes equations for combustion research.", "venue": "ArXiv", "authors": ["Ray W. Grout", "Hemanth  Kolla", "Michael L. Minion", "John B. Bell"], "year": 2015, "n_citations": 12}
{"id": 2085862, "s2_id": "fd8a64193874719fdee3d926bd7135081f634fa1", "title": "Algorithms and Complexity for Turaev-Viro Invariants", "abstract": "The Turaev-Viro invariants are a powerful family of topological invariants for distinguishing between different 3-manifolds. They are invaluable for mathematical software, but current algorithms to compute them require exponential time.", "venue": "ICALP", "authors": ["Benjamin A. Burton", "Cl\u00e9ment  Maria", "Jonathan  Spreer"], "year": 2015, "n_citations": 7}
{"id": 2087655, "s2_id": "489c7727540dfc9fa51db05da42aa08bfb6451d3", "title": "The jsonlite Package: A Practical and Consistent Mapping Between JSON Data and R Objects", "abstract": "A naive realization of JSON data in R maps JSON arrays to an unnamed list, and JSON objects to a named list. However, in practice a list is an awkward, inefficient type to store and manipulate data. Most statistical applications work with (homogeneous) vectors, matrices or data frames. Therefore JSON packages in R typically define certain special cases of JSON structures which map to simpler R types. Currently there exist no formal guidelines, or even consensus between implementations on how R data should be represented in JSON. Furthermore, upon closer inspection, even the most basic data structures in R actually do not perfectly map to their JSON counterparts and leave some ambiguity for edge cases. These problems have resulted in different behavior between implementations and can lead to unexpected output. This paper explicitly describes a mapping between R classes and JSON data, highlights potential problems, and proposes conventions that generalize the mapping to cover all common structures. We emphasize the importance of type consistency when using JSON to exchange dynamic data, and illustrate using examples and anecdotes. The jsonlite R package is used throughout the paper as a reference implementation.", "venue": "ArXiv", "authors": ["Jeroen  Ooms"], "year": 2014, "n_citations": 132}
{"id": 2087995, "s2_id": "8bcd16e0e0efa7687ee0a4fdd40b0facdf39d890", "title": "Web-based Structural Identifiability Analyzer", "abstract": "Parameter identifiability describes whether, for a given differential model, one can determine parameter values from model equations. Knowing global or local identifiability properties allows construction of better practical experiments to identify parameters from experimental data. In this work, we present a web-based software tool that allows to answer specific identifiability queries. Concretely, our toolbox can determine identifiability of individual parameters of the model and also provide all functions of parameters that are identifiable (also called identifiable combinations) from single or multiple experiments. The program is freely available at https://maple.cloud/app/6509768948056064.", "venue": "CMSB", "authors": ["Ilia  Ilmer", "Alexey  Ovchinnikov", "Gleb  Pogudin"], "year": 2021, "n_citations": 2}
{"id": 2088763, "s2_id": "1ccfa868f4ca8e81014b0b5967877911b77bc8bf", "title": "Unstructured Overlapping Mesh Distribution in Parallel", "abstract": "We present a simple mathematical framework and API for parallel mesh and data distribution, load balancing, and overlap generation. It relies on viewing the mesh as a Hasse diagram, abstracting away information such as cell shape, dimension, and coordinates. The high level of abstraction makes our interface both concise and powerful, as the same algorithm applies to any representable mesh, such as hybrid meshes, meshes embedded in higher dimension, and overlapped meshes in parallel. We present evidence, both theoretical and experimental, that the algorithms are scalable and efficient. A working implementation can be found in the latest release of the PETSc libraries.", "venue": "ArXiv", "authors": ["Matthew G. Knepley", "Michael  Lange", "Gerard  Gorman"], "year": 2015, "n_citations": 6}
{"id": 2092443, "s2_id": "260777d460c039f8e2c7eab41082d3541830d8e7", "title": "PLQCD library for Lattice QCD on multi-core machines", "abstract": "PLQCD is a stand-alone software library developed under PRACE for lattice QCD. It provides an implementation of the Dirac operator for Wilson type fermions and few efficient linear solvers. The library is optimized for multi-core machines using a hybrid parallelization with OpenMP+MPI. The main objectives of the library is to provide a scalable implementation of the Dirac operator for efficient computation of the quark propagator. In this contribution, a description of the PLQCD library is given together with some benchmark results.", "venue": "ArXiv", "authors": ["Abdou  Abdel-Rehim", "Constantia  Alexandrou", "Nikos  Anastopoulos", "G.  Koutsou", "Ioannis  Liabotis", "N.  Papadopoulou"], "year": 2014, "n_citations": 0}
{"id": 2092609, "s2_id": "94f3a14a7bc3ce460a9f8891f45a852f9cf9ef0b", "title": "A framework for automated PDE-constrained optimisation", "abstract": "A generic framework for the solution of PDE-constrained optimisation problems based on the FEniCS system is presented. Its main features are an intuitive mathematical interface, a high degree of automation, and an efficient implementation of the generated adjoint model. The framework is based upon the extension of a domain-specific language for variational problems to cleanly express complex optimisation problems in a compact, high-level syntax. For example, optimisation problems constrained by the time-dependent Navier-Stokes equations can be written in tens of lines of code. Based on this high-level representation, the framework derives the associated adjoint equations in the same domain-specific language, and uses the FEniCS code generation technology to emit parallel optimised low-level C++ code for the solution of the forward and adjoint systems. The functional and gradient information so computed is then passed to the optimisation algorithm to update the parameter values. This approach works both for steady-state as well as transient, and for linear as well as nonlinear governing PDEs and a wide range of functionals and control parameters. We demonstrate the applicability and efficiency of this approach on classical textbook optimisation problems and advanced examples.", "venue": "ArXiv", "authors": ["Simon W. Funke", "Patrick E. Farrell"], "year": 2013, "n_citations": 43}
{"id": 2103251, "s2_id": "2d1ee8f8fadc6fbf50646b9e208081933ad6317c", "title": "RIFLE: Robust Inference from Low Order Marginals", "abstract": "The ubiquity of missing values in real-world datasets poses a challenge for statistical inference and can prevent similar datasets from being analyzed in the same study, precluding many existing datasets from being used for new analyses. While an extensive collection of packages and algorithms have been developed for data imputation, the overwhelming majority perform poorly if there are many missing values and low sample size, which are unfortunately common characteristics in empirical data. Such low-accuracy estimations adversely affect the performance of downstream statistical models. We develop a statistical inference framework for predicting the target variable without imputing missing values. Our framework, RIFLE (Robust InFerence via Low-order moment Estimations), estimates low-order moments with corresponding confidence intervals to learn a distributionally robust model. We specialize our framework to linear regression and normal discriminant analysis, and we provide convergence and performance guarantees. This framework can also be adapted to impute missing data. In numerical experiments, we compare RIFLE with state-of-the-art approaches (including MICE, Amelia, MissForest, KNN-imputer, MIDA, and Mean Imputer). Our experiments demonstrate that RIFLE outperforms other benchmark algorithms when the percentage of missing values is high and/or when the number of data points is relatively small. RIFLE is publicly available3.", "venue": "ArXiv", "authors": ["Sina  Baharlouei", "Kelechi  Ogudu", "Sze-chuan  Suen", "Meisam  Razaviyayn"], "year": 2021, "n_citations": 0}
{"id": 2106352, "s2_id": "a89078153e4c406a8c4a19f6ae9cb7d4bb821589", "title": "SODECL: An Open Source Library for Calculating Multiple Orbits of a System of Stochastic Differential Equations in Parallel", "abstract": "Stochastic differential equations (SDEs) are widely used to model systems affected by random processes. In general, the analysis of an SDE model requires numerical solutions to be generated many times over multiple parameter combinations. However, this process often requires considerable computational resources to be practicable. Due to the embarrassingly parallel nature of the task, devices such as multi-core processors and graphics processing units (GPUs) can be employed for acceleration. \nHere, we present SODECL, a software library that utilises such devices to calculate multiple orbits of an SDE model. To evaluate the acceleration provided by SODECL, we compared the time required to calculate multiple orbits of an exemplar stochastic model when one CPU core is used, to the time required when using all CPU cores or a GPU. In addition, to assess scalability, we investigated how the model size affected execution time on different parallel compute devices. \nOur results show that when using all 32 CPU cores of a high-end high-performance computing node, the task is accelerated by a factor of up to $\\simeq$6.7, compared to when using a single CPU core. Executing the task on a high-end GPU yielded accelerations of up to $\\simeq$4.5, compared to a single CPU core.", "venue": "ACM Trans. Math. Softw.", "authors": ["Eleftherios  Avramidis", "Marta  Lalik", "Ozgur E. Akman"], "year": 2020, "n_citations": 1}
{"id": 2107728, "s2_id": "f37e2a93d2c9819ea362da0d6988205e9ac7c977", "title": "GooFit 2.0", "abstract": "The GooFit package provides physicists a simple, familiar syntax for manipulating probability density functions and performing fits, and is highly optimized for data analysis on NVIDIA GPUs and multithreaded CPU backends. GooFit was updated to version 2.0, bringing a host of new features. A completely revamped and redesigned build system makes GooFit easier to install, develop with, and run on virtually any system. Unit testing, continuous integration, and advanced logging options are improving the stability and reliability of the system. Developing new PDFs now uses standard CUDA terminology and provides a lower barrier for new users. The system now has built-in support for multiple graphics cards or nodes using MPI, and is being tested on a wide range of different systems. GooFit also has significant improvements in performance on some GPU architectures due to optimized memory access. Support for time-dependent four-body amplitude analyses has also been added.", "venue": "Journal of Physics: Conference Series", "authors": ["Henry  Schreiner", "Christoph  Hasse", "Bradley  Hittle", "Himadri  Pandey", "Michael D. Sokoloff", "Karen  Tomko"], "year": 2018, "n_citations": 5}
{"id": 2110437, "s2_id": "203ece6cd9bbc84b0ba3acc7082c33a92d23ad7a", "title": "SPIKY: a graphical user interface for monitoring spike train synchrony.", "abstract": "Techniques for recording large-scale neuronal spiking activity are developing very fast. This leads to an increasing demand for algorithms capable of analyzing large amounts of experimental spike train data. One of the most crucial and demanding tasks is the identification of similarity patterns with a very high temporal resolution and across different spatial scales. To address this task, in recent years three time-resolved measures of spike train synchrony have been proposed, the ISI-distance, the SPIKE-distance, and event synchronization. The Matlab source codes for calculating and visualizing these measures have been made publicly available. However, due to the many different possible representations of the results the use of these codes is rather complicated and their application requires some basic knowledge of Matlab. Thus it became desirable to provide a more user-friendly and interactive interface. Here we address this need and present SPIKY, a graphical user interface that facilitates the application of time-resolved measures of spike train synchrony to both simulated and real data. SPIKY includes implementations of the ISI-distance, the SPIKE-distance, and the SPIKE-synchronization (an improved and simplified extension of event synchronization) that have been optimized with respect to computation speed and memory demand. It also comprises a spike train generator and an event detector that makes it capable of analyzing continuous data. Finally, the SPIKY package includes additional complementary programs aimed at the analysis of large numbers of datasets and the estimation of significance levels.", "venue": "Journal of neurophysiology", "authors": ["Thomas  Kreuz", "Mario  Mulansky", "Nebojsa  Bozanic"], "year": 2015, "n_citations": 33}
{"id": 2112408, "s2_id": "aed2b6e8db1f843185bd77e95eb49306ae56c56d", "title": "Recursive Algorithms for Distributed Forests of Octrees", "abstract": "The forest-of-octrees approach to parallel adaptive mesh refinement and coarsening has recently been demonstrated in the context of a number of large-scale PDE-based applications. Efficient reference software has been made freely available to the public both in the form of the standalone \\tt p4est library and more indirectly by the general-purpose finite element library \\tt deal.II, which has been equipped with a \\tt p4est backend. Although linear octrees, which store only leaf octants, have an underlying tree structure by definition, it is not fully exploited in previously published mesh-related algorithms. This is because tree branches are not explicitly stored and because the topological relationships in meshes, such as the adjacency between cells, introduce dependencies that do not respect the octree hierarchy. In this work, we combine hierarchical and topological relationships between octants to design efficient recursive algorithms that operate on distributed forests of octrees. We present three imp...", "venue": "SIAM J. Sci. Comput.", "authors": ["Tobin  Isaac", "Carsten  Burstedde", "Lucas C. Wilcox", "Omar  Ghattas"], "year": 2015, "n_citations": 65}
{"id": 2115704, "s2_id": "0e945bb9308376c017dbb9688a258246cc4e5b96", "title": "PyParSVD: A streaming, distributed and randomized singular-value-decomposition library", "abstract": "We introduce PyParSVD11https://github.com/Romit-Maulik/PyParSVD, a Python library that implements a streaming, distributed and randomized algorithm for the singular value decomposition. To demonstrate its effectiveness, we extract coherent structures from scientific data. Futhermore, we show weak scaling assessments on up to 256 nodes of the Theta machine at Argonne Leadership Computing Facility, demonstrating potential for large-scale data analyses of practical data sets.", "venue": "2021 7th International Workshop on Data Analysis and Reduction for Big Scientific Data (DRBSD-7)", "authors": ["Romit  Maulik", "Gianmarco  Mengaldo"], "year": 2021, "n_citations": 1}
{"id": 2117591, "s2_id": "46eed479615fa6785fbf6f64f208bfaa0cda1d6c", "title": "An efficient way to perform the assembly of finite element matrices in Matlab and Octave", "abstract": "We describe different optimization techniques to perform the assembly of finite element matrices in Matlab and Octave, from the standard approach to recent vectorized ones, without any low level language used. We finally obtain a simple and efficient vectorized algorithm able to compete in performance with dedicated software such as FreeFEM++. The principle of this assembly algorithm is general, we present it for different matrices in the P1 finite elements case and in linear elasticity. We present numerical results which illustrate the computational costs of the different approaches", "venue": "ArXiv", "authors": ["Fran\u00e7ois  Cuvelier", "Caroline  Japhet", "Gilles  Scarella"], "year": 2013, "n_citations": 17}
{"id": 2119225, "s2_id": "7356dc9ad152730296e15eabb46065be239682af", "title": "Computing Real Roots of Real Polynomials ... and now For Real!", "abstract": "Very recent work introduces an asymptotically fast subdivision algorithm, denoted ANewDsc, for isolating the real roots of a univariate real polynomial. The method combines Descartes? Rule of Signs to test intervals for the existence of roots, Newton iteration to speed up convergence against clusters of roots, and approximate computation to decrease the required precision. It achieves record bounds on the worst-case complexity for the considered problem, matching the complexity of Pan's method for computing all complex roots and improving upon the complexity of other subdivision methods by several magnitudes. In the article at hand, we report on an implementation of ANewDsc on top of the RS root isolator. RS is a highly efficient realization of the classical Descartes method and currently serves as the default real root solver in Maple. We describe crucial design changes within ANewDsc and RS that led to a high-performance implementation without harming the theoretical complexity of the underlying algorithm. With an excerpt of our extensive collection of benchmarks, available online at http://anewdsc.mpi-inf.mpg.de/, we illustrate that the theoretical gain in performance of ANewDsc over other subdivision methods also transfers into practice. These experiments also show that our new implementation outperforms both RS and mature competitors by magnitudes for notoriously hard instances with clustered roots. For all other instances, we avoid almost any overhead by integrating additional optimizations and heuristics.", "venue": "ISSAC", "authors": ["Alexander  Kobel", "Fabrice  Rouillier", "Michael  Sagraloff"], "year": 2016, "n_citations": 41}
{"id": 2120909, "s2_id": "6ba9e041d9e156e2015d89e6441451814534ce65", "title": "WinBioinfTools: Bioinformatics Tools for Windows High Performance Computing Server 2008", "abstract": "Open source bioinformatics tools running under MS Windows are rare to find, and those running under Windows HPC cluster are almost non-existing. This is despite the fact that the Windows is the most popular operating system used among life scientists. Therefore, we introduce in this initiative WinBioinfTools, a toolkit containing a number of bioinformatics tools running under Windows High Performance Computing Server 2008. It is an open source code package, where users and developers can share and add to. We currently start with three programs from the area of sequence analysis: 1) CoCoNUT for pairwise genome comparison, 2) parallel BLAST for biological database search, and 3) parallel global pairwise sequence alignment. In this report, we focus on technical aspects concerning how some components of these tools were ported from Linux/Unix environment to run under Windows. We also show the advantages of using the Windows HPC Cluster 2008. We demonstrate by experiments the performance gain achieved when using a computer cluster against a single machine. Furthermore, we show the results of comparing the performance of WinBioinfTools on the Windows and Linux Cluster.", "venue": "ArXiv", "authors": ["Mohamed Ibrahim Abouelhoda", "Hisham  Mohamed"], "year": 2009, "n_citations": 0}
{"id": 2123240, "s2_id": "e767e58686cdc798410a182fdbe0f1e17703863c", "title": "Java Implementation of a Parameter-less Evolutionary Portfolio", "abstract": "The Java implementation of a portfolio of parameter-less evolutionary algorithms is presented. The Parameter-less Evolutionary Portfolio implements a heuristic that performs adaptive selection of parameter-less evolutionary algorithms in accordance with performance criteria that are measured during running time. At present time, the portfolio includes three parameter-less evolutionary algorithms: Parameter-less Univariate Marginal Distribution Algorithm, Parameter-less Extended Compact Genetic Algorithm, and Parameter-less Hierarchical Bayesian Optimization Algorithm. Initial experiments showed that the parameter-less portfolio can solve various classes of problems without the need for any prior parameter setting technique and with an increase in computational effort that can be considered acceptable.", "venue": "ArXiv", "authors": ["Jos\u00e9 C. Pereira", "Fernando G. Lobo"], "year": 2015, "n_citations": 2}
{"id": 2129199, "s2_id": "cc18590484c96b9a2f506e881525f58f355601c6", "title": "A Functional Hitchhiker's Guide to Hereditarily Finite Sets, Ackermann Encodings and Pairing Functions", "abstract": "The paper is organized as a self-contained literate Haskell program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The code, tested under GHC 6.6.1, is available at this http URL . \nWe introduce ranking and unranking functions generalizing Ackermann's encoding to the universe of Hereditarily Finite Sets with Urelements. Then we build a lazy enumerator for Hereditarily Finite Sets with Urelements that matches the unranking function provided by the inverse of Ackermann's encoding and we describe functors between them resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation. \nKeywords: hereditarily finite sets, ranking and unranking functions, executable set theory, arithmetic encodings, Haskell data representations, functional programming and computational mathematics", "venue": "ArXiv", "authors": ["Paul  Tarau"], "year": 2008, "n_citations": 1}
{"id": 2130646, "s2_id": "090c6f4f5c72d00f4cbc5f0f0c9b2b0bdf8747c1", "title": "A multi-threaded version of MCFM", "abstract": "We report on our findings modifying MCFM using OpenMP to implement multi-threading. By using OpenMP, the modified MCFM will execute on any processor, automatically adjusting to the number of available threads. We modified the integration routine VEGAS to distribute the event evaluation over the threads, while combining all events at the end of every iteration to optimize the numerical integration. Special care has been taken that the results of the Monte Carlo integration are independent of the number of threads used, to facilitate the validation of the OpenMP version of MCFM.", "venue": "ArXiv", "authors": ["John M. Campbell", "R. Keith Ellis", "Walter T. Giele"], "year": 2015, "n_citations": 153}
{"id": 2133323, "s2_id": "9c7a6e94c4e3ecf3ab55afdfb89c1985cc9aab17", "title": "States and channels in quantum mechanics without complex numbers", "abstract": "In the presented note we aim at exploring the possibility of abandoning complex numbers in the representation of quantum states and operations. We demonstrate a simplified version of quantum mechanics in which the states are represented using real numbers only. The main advantage of this approach is that the simulation of the $n$-dimensional quantum system requires $n^2$ real numbers, in contrast to the standard case where $n^4$ real numbers are required. The main disadvantage is the lack of hermicity in the representation of quantum states. Using Mathematica computer algebra system we develop a set of functions for manipulating real-only quantum states. With the help of this tool we study the properties of the introduced representation and the induced representation of quantum channels.", "venue": "ArXiv", "authors": ["Jaroslaw Adam Miszczak"], "year": 2016, "n_citations": 2}
{"id": 2155355, "s2_id": "8bd31918d61f626bdee5cc98df8f0aa9f3441842", "title": "Faithful Polynomial Evaluation with Compensated Horner Algorithm", "abstract": "This paper presents two sufficient conditions to ensure a faithful evaluation of polynomial in IEEE-754 floating point arithmetic. Faithfulness means that the computed value is one of the two floating point neighbours of the exact result; it can be satisfied using a more accurate algorithm than the classic Horner scheme. One condition here provided is an a priori bound of the polynomial condition number derived from the error analysis of the compensated Horner algorithm. The second condition is both dynamic and validated to check at the running time the faithfulness of a given evaluation. Numerical experiments illustrate the behavior of these two conditions and that associated running time over-cost is really interesting.", "venue": "ArXiv", "authors": ["Philippe  Langlois", "Nicolas  Louvet"], "year": 2006, "n_citations": 1}
{"id": 2156103, "s2_id": "e16061a976129e1547ab3e137ea02891d10f95f1", "title": "PyGOM - A Python Package for Simplifying Modelling with Systems of Ordinary Differential Equations", "abstract": "Ordinary Differential Equations (ODE) are used throughout science where the capture of rates of change in states is sought. While both pieces of commercial and open software exist to study such systems, their efficient and accurate usage frequently requires deep understanding of mathematics and programming. The package we present here, PyGOM, seeks to remove these obstacles for models based on ODE systems. We provide a simple interface for the construction of such systems backed by a comprehensive and easy to use tool--box. This tool--box implements functions to easily perform common operations for ODE systems such as solving, parameter estimation, and stochastic simulation. The package source is freely available and organized in a way that permits easy extension. With both the algebraic and numeric calculations performed automatically (but still accessible), the end user is freed to focus on model development.", "venue": "ArXiv", "authors": ["Edwin  Tye", "Thomas  Finnie", "Ian  Hall", "Steve  Leach"], "year": 2018, "n_citations": 0}
{"id": 2157350, "s2_id": "8c7310477fd027193cd040288f0aa9824c80b91f", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code", "abstract": "This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel commands to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.", "venue": "2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)", "authors": ["Riyadh  Baghdadi", "Jessica  Ray", "Malek Ben Romdhane", "Emanuele Del Sozzo", "Abdurrahman  Akkas", "Yunming  Zhang", "Patricia  Suriana", "Shoaib  Kamil", "Saman P. Amarasinghe"], "year": 2019, "n_citations": 93}
{"id": 2170053, "s2_id": "79d45363fb46e70d0023cb486579c4de1977294b", "title": "A Kogbetliantz-type algorithm for the hyperbolic SVD", "abstract": "In this paper a two-sided, parallel Kogbetliantz-type algorithm for the hyperbolic singular value decomposition (HSVD) of real and complex square matrices is developed, with a single assumption that the input matrix, of order $n$, admits such a decomposition into the product of a unitary, a non-negative diagonal, and a $J$-unitary matrix, where $J$ is a given diagonal matrix of positive and negative signs. When $J=\\pm I$, the proposed algorithm computes the ordinary SVD. The paper's most important contribution---a derivation of formulas for the HSVD of $2\\times 2$ matrices---is presented first, followed by the details of their implementation in floating-point arithmetic. Next, the effects of the hyperbolic transformations on the columns of the iteration matrix are discussed. These effects then guide a redesign of the dynamic pivot ordering, being already a well-established pivot strategy for the ordinary Kogbetliantz algorithm, for the general, $n\\times n$ HSVD. A heuristic but sound convergence criterion is then proposed, which contributes to high accuracy demonstrated in the numerical testing results. Such a $J$-Kogbetliantz algorithm as presented here is intrinsically slow, but is nevertheless usable for matrices of small orders.", "venue": "Numerical Algorithms", "authors": ["Vedran  Novakovi'c", "Sanja  Singer"], "year": 2021, "n_citations": 1}
{"id": 2172087, "s2_id": "5c02517cfaee746677935095dac164fae73ad8f2", "title": "Advanced Techniques for Scientific Programming and Collaborative Development of Open Source Software Packages at the International Centre for Theoretical Physics (ICTP)", "abstract": "A large number of computational scientific research projects make use of open source software packages. However, the development process of such tools frequently di ers from conventional software development; partly because of the nature of research, where the problems being addressed are not always fully understood; partly because the majority of the development is often carried out by scientists with limited experience and exposure to best practices of software engineering. Often the software development su ers from the pressure to publish scientific results and that credit for software development is limited in comparison. Fundamental components of software engineering like modular and reusable design, validation, documentation, and software integration as well as e ective maintenance and user support tend to be disregarded due to lack of resources and qualified specialists. Thus innovative developments are often hindered by steep learning curves required to master development for legacy software packages full of ad hoc solutions. The growing complexity of research, however, requires suitable and maintainable computational tools, resulting in a widening gap between the potential users (often growing in number) and contributors to the development of such a package. In this paper we share our experiences aiming to improve the situation by training young scientists, through disseminating our own experiences at contributing to open source software packages and practicing key components of software engineering adapted for scientists and scientific software development. Specifically we summarize the outcome of the \u201cWorkshop in Advanced Techniques for Scientific Programming and Collaborative Development of Open Source Software Packages\u201d run at the Abdus Salam International Centre for Theoretical Physics in March 2013, and discuss our conclusions for future e orts.", "venue": "ArXiv", "authors": ["Ivan  Girotto", "Axel  Kohlmeyer", "David  Grellscheid", "Shawn T. Brown"], "year": 2013, "n_citations": 1}
{"id": 2172373, "s2_id": "ad141c2b13a89a8a8358396f014db51bfb9c1cf1", "title": "Extreme-Scale Multigrid Components within PETSc", "abstract": "Elliptic partial differential equations (PDEs) frequently arise in continuum descriptions of physical processes relevant to science and engineering. Multilevel preconditioners represent a family of scalable techniques for solving discrete PDEs of this type and thus are the method of choice for high-resolution simulations. The scalability and time-to-solution of massively parallel multilevel preconditioners can be adversely affected by using a coarse-level solver with sub-optimal algorithmic complexity. To maintain scalability, agglomeration techniques applied to the coarse level have been shown to be necessary. In this work, we present a new software component introduced within the Portable Extensible Toolkit for Scientific computation (PETSc) which permits agglomeration. We provide an overview of the design and implementation of this functionality, together with several use cases highlighting the benefits of agglomeration. Lastly, we demonstrate via numerical experiments employing geometric multigrid with structured meshes, the flexibility and performance gains possible using our MPI-rank agglomeration implementation.", "venue": "PASC", "authors": ["Dave A. May", "Patrick  Sanan", "Karl  Rupp", "Matthew G. Knepley", "Barry  Smith"], "year": 2016, "n_citations": 27}
{"id": 2175201, "s2_id": "f43d9e190be0a67a5811b59712f879aa96d4559f", "title": "Parallel Random Search Algorithm of Constrained Pseudo-Boolean Optimization for Some Distinctive Large-Scale Problems", "abstract": "In this paper, we consider an approach to the parallelizing of the algorithms realizing the modified probability changigng method with adaptation and partial rollback procedure for constrained pseudo-Boolean optimization problems. Existing optimization algorithms are adapted for the shared memory and clusters (PVM library). The parallel efficiency is estimated for the lagre-scale non-linear pseudo-Boolean optimization problems with linear constraints. Initially designed for unconstrained optimization, the probability changing method (MIVER) allows us finding the approximate solution of different linear and non-linear pseudo-Boolean optimization problems with constraints. Although, in case of large-scale problems, the computational demands are also very high and the precision of the result depends on the time spent. In case of the constrained optimization problem, even the search of any permissibly solution can take very large computational resources. The rapid development of the parallel processor systems which are often implemented even in the computer systems designed for home use allows to reduce significantly the time spent to find the acceptable solution with a speed-up close to ideal.", "venue": "ArXiv", "authors": ["Lev  Kazakovtsev"], "year": 2012, "n_citations": 0}
{"id": 2178174, "s2_id": "1cc0b748da2f7ab54eec5843b2d14b0a3e513d28", "title": "The iisignature library: efficient calculation of iterated-integral signatures and log signatures", "abstract": "We explain the meaning and calculation of the log signature of a path, and explain some accompanying software tools which demonstrate it.", "venue": "ACM Trans. Math. Softw.", "authors": ["Jeremy  Reizenstein", "Benjamin  Graham"], "year": 2020, "n_citations": 44}
{"id": 2181298, "s2_id": "43646798f334bba75401737909f812ed04acba4c", "title": "Loo.py: transformation-based code generation for GPUs and CPUs", "abstract": "Today's highly heterogeneous computing landscape places a burden on programmers wanting to achieve high performance on a reasonably broad cross-section of machines. To do so, computations need to be expressed in many different but mathematically equivalent ways, with, in the worst case, one variant per target machine. Loo.py, a programming system embedded in Python, meets this challenge by defining a data model for array-style computations and a library of transformations that operate on this model. Offering transformations such as loop tiling, vectorization, storage management, unrolling, instruction-level parallelism, change of data layout, and many more, it provides a convenient way to capture, parametrize, and re-unify the growth among code variants. Optional, deep integration with numpy and PyOpenCL provides a convenient computing environment where the transition from prototype to high-performance implementation can occur in a gradual, machine-assisted form.", "venue": "ARRAY@PLDI", "authors": ["Andreas  Kl\u00f6ckner"], "year": 2014, "n_citations": 43}
{"id": 2184942, "s2_id": "81000545a7e8663e8c18f2a4a6c2321fc7215cb3", "title": "Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU Co-Processing", "abstract": "In this paper, we present an OpenCL-based heterogeneous implementation of a computer vision algorithm \u2013 image inpainting-based object removal algorithm \u2013 on mobile devices. To take advantage of the computation power of the mobile processor, the algorithm workflow is partitioned between the CPU and the GPU based on the profiling results on mobile devices, so that the computationally-intensive kernels are accelerated by the mobile GPGPU (general-purpose computing using graphics processing units). By exploring the implementation trade-offs and utilizing the proposed optimization strategies at different levels including algorithm optimization, parallelism optimization, and memory access optimization, we significantly speed up the algorithm with the CPU-GPU heterogeneous implementation, while preserving the quality of the output images. Experimental results show that heterogeneous computing based on GPGPU co-processing can significantly speed up the computer vision algorithms and makes them practical on real-world mobile devices.", "venue": "J. Signal Process. Syst.", "authors": ["Guohui  Wang", "Yingen  Xiong", "Jay  Yun", "Joseph R. Cavallaro"], "year": 2014, "n_citations": 11}
{"id": 2186931, "s2_id": "d5e3a0896d0bbd7ef5c9df35191a0f91efeac02f", "title": "Medusa: A C++ Library for solving PDEs using Strong Form Mesh-Free methods", "abstract": "Medusa, a novel library for implementation of strong form mesh-free methods, is described. We identify and present common parts and patterns among many such methods reported in the literature, such as node positioning, stencil selection and stencil weight computation. Many different algorithms exist for each part and the possible combinations offer a plethora of possibilities for improvements of solution procedures that are far from fully understood. As a consequence there are still many unanswered questions in mesh-free community resulting in vivid ongoing research in the field. Medusa implements the core mesh-free elements as independent blocks, which offers users great flexibility in experimenting with the method they are developing, as well as easily comparing it with other existing methods. The paper describes the chosen abstractions and their usage, illustrates aspects of the philosophy and design, offers some executions time benchmarks and demonstrates the application of the library on cases from linear elasticity and fluid flow in irregular 2D and 3D domains.", "venue": "ACM Trans. Math. Softw.", "authors": ["Jure  Slak", "Gregor  Kosec"], "year": 2021, "n_citations": 5}
{"id": 2188375, "s2_id": "e839ea1ce1640b55efe38551de2a3cfd8cfea544", "title": "RationalizeRoots: Software Package for the Rationalization of Square Roots", "abstract": "The computation of Feynman integrals often involves square roots. One way to obtain a solution in terms of multiple polylogarithms is to rationalize these square roots by a suitable variable change. We present a program that can be used to find such transformations. After an introduction to the theoretical background, we explain in detail how to use the program in practice.", "venue": "Comput. Phys. Commun.", "authors": ["Marco  Besier", "Pascal  Wasser", "Stefan  Weinzierl"], "year": 2020, "n_citations": 23}
{"id": 2188912, "s2_id": "f9bcb78a1fb7b66e99f575abb915ddb32b44188b", "title": "Concurrent Alternating Least Squares for multiple simultaneous Canonical Polyadic Decompositions", "abstract": "Tensor decompositions, such as CANDECOMP/PARAFAC (CP), are widely used in a variety of applications, such as chemometrics, signal processing, and machine learning. A broadly used method for computing such decompositions relies on the Alternating Least Squares (ALS) algorithm. When the number of components is small, regardless of its implementation, ALS exhibits low arithmetic intensity, which severely hinders its performance and makes GPU offloading ineffective. We observe that, in practice, experts often have to compute multiple decompositions of the same tensor, each with a small number of components (typically fewer than 20), to ultimately find the best ones to use for the application at hand. In this paper, we illustrate how multiple decompositions of the same tensor can be fused together at the algorithmic level to increase the arithmetic intensity. Therefore, it becomes possible to make efficient use of GPUs for further speedups; at the same time the technique is compatible with many enhancements typically used in ALS, such as line search, extrapolation, and non-negativity constraints. We introduce the Concurrent ALS algorithm and library, which offers an interface to Matlab, and a mechanism to effectively deal with the issue that decompositions complete at different times. Experimental results on artificial and real datasets demonstrate a shorter time to completion due to increased arithmetic intensity.", "venue": "ArXiv", "authors": ["Christos  Psarras", "Lars  Karlsson", "Paolo  Bientinesi"], "year": 2020, "n_citations": 2}
{"id": 2191011, "s2_id": "31e39455b4403d9c9c5ee8b2478f5d84ff5a0a21", "title": "Radix conversion for IEEE754-2008 mixed radix floating-point arithmetic", "abstract": "Conversion between binary and decimal floating-point representations is ubiquitous. Floating-point radix conversion means converting both the exponent and the mantissa. We develop an atomic operation for FP radix conversion with simple straight-line algorithm, suitable for hardware design. Exponent conversion is performed with a small multiplication and a lookup table. It yields the correct result without error. Mantissa conversion uses a few multiplications and a small lookup table that is shared amongst all types of conversions. The accuracy changes by adjusting the computing precision.", "venue": "2013 Asilomar Conference on Signals, Systems and Computers", "authors": ["Olga  Kupriianova", "Christoph Quirin Lauter", "Jean-Michel  Muller"], "year": 2013, "n_citations": 3}
{"id": 2191690, "s2_id": "0b2199ba393f4d8ec9d41646dc48ebad72d36a8b", "title": "A multi-scale code for flexible hybrid simulations", "abstract": "Multi-scale computer simulations combine the computationally efficient classical algorithms with more expensive but also more accurate ab-initio quantum mechanical algorithms. This work describes one implementation of multi-scale computations using the Atomistic Simulation Environment (ASE). This implementation can mix classical codes like LAMMPS and the Density Functional Theory-based GPAW. Any combination of codes linked via the ASE interface however can be mixed. We also introduce a framework to easily add classical force fields calculators for ASE using LAMMPS, which also allows harnessing the full performance of classical-only molecular dynamics. Our work makes it possible to combine different simulation codes, quantum mechanical or classical, with great ease and minimal coding effort.", "venue": "ArXiv", "authors": ["Lauri  Leukkunen", "Tuukka  Verho", "Olga  Lopez-Acevedo"], "year": 2012, "n_citations": 0}
{"id": 2192000, "s2_id": "c887c74e2cccd156932bd8da94cbc560a70abd1a", "title": "Scilab and SIP for Image Processing", "abstract": "This paper is an overview of Image Processing and Analysis using Scilab, a free prototyping environment for numerical calculations similar to Matlab. We demonstrate the capabilities of SIP -- the Scilab Image Processing Toolbox -- which extends Scilab with many functions to read and write images in over 100 major file formats, including PNG, JPEG, BMP, and TIFF. It also provides routines for image filtering, edge detection, blurring, segmentation, shape analysis, and image recognition. Basic directions to install Scilab and SIP are given, and also a mini-tutorial on Scilab. Three practical examples of image analysis are presented, in increasing degrees of complexity, showing how advanced image analysis techniques seems uncomplicated in this environment.", "venue": "ArXiv", "authors": ["Ricardo  Fabbri", "Odemir Martinez Bruno", "Luciano da Fontoura Costa"], "year": 2012, "n_citations": 7}
{"id": 2205376, "s2_id": "c72dc3b8c8c13baec9585677c9f2f4c831252522", "title": "A Unified Software Framework for Empirical Gramians", "abstract": "A common approach in model reduction is balanced truncation, which is based on Gramian matrices classifying certain attributes of states or parameters of a given dynamic system. Initially restricted to linear systems, the empirical Gramians not only extended this concept to nonlinear systems but also provided a uniform computational method. This work introduces a unified software framework supplying routines for six types of empirical Gramians. The Gramian types will be discussed and applied in a model reduction framework for multiple-input multiple-output systems.", "venue": "ArXiv", "authors": ["Christian  Himpe", "Mario  Ohlberger"], "year": 2013, "n_citations": 23}
{"id": 2210674, "s2_id": "a8153797af683a5cc1f6b496bd7f28069008fe6d", "title": "Fast and rigorous arbitrary-precision computation of Gauss-Legendre quadrature nodes and weights", "abstract": "We describe a strategy for rigorous arbitrary-precision evaluation of Legendre polynomials on the unit interval and its application in the generation of Gauss-Legendre quadrature rules. Our focus is on making the evaluation practical for a wide range of realistic parameters, corresponding to the requirements of numerical integration to an accuracy of about 100 to 100 000 bits. Our algorithm combines the summation by rectangular splitting of several types of expansions in terms of hypergeometric series with a fixed-point implementation of Bonnet's three-term recurrence relation. We then compute rigorous enclosures of the Gauss-Legendre nodes and weights using the interval Newton method. We provide rigorous error bounds for all steps of the algorithm. The approach is validated by an implementation in the Arb library, which achieves order-of-magnitude speedups over previous code for computing Gauss-Legendre rules with simultaneous high degree and precision.", "venue": "SIAM J. Sci. Comput.", "authors": ["Fredrik  Johansson", "Marc  Mezzarobba"], "year": 2018, "n_citations": 9}
{"id": 2213741, "s2_id": "75c96811ffb5160f8005519adf9327b32c7111d4", "title": "Distributed-memory parallelization of the aggregated unfitted finite element method", "abstract": "Abstract The aggregated unfitted finite element method (AgFEM) is a methodology recently introduced in order to address conditioning and stability problems associated with embedded, unfitted, or extended finite element methods. The method is based on removal of basis functions associated with badly cut cells by introducing carefully designed constraints, which results in well-posed systems of linear algebraic equations, while preserving the optimal approximation order of the underlying finite element spaces. The specific goal of this work is to present the implementation and performance of the method on distributed-memory platforms aiming at the efficient solution of large-scale problems. In particular, we show that, by considering AgFEM, the resulting systems of linear algebraic equations can be effectively solved using standard algebraic multigrid preconditioners. This is in contrast with previous works that consider highly customized preconditioners in order to allow one the usage of iterative solvers in combination with unfitted techniques. Another novelty with respect to the methods available in the literature is the problem sizes that can be handled with the proposed approach. While most of previous references discussing linear solvers for unfitted methods are based on serial non-scalable algorithms, we propose a parallel distributed-memory method able to efficiently solve problems at large scales. This is demonstrated by means of a weak scaling test defined on complex 3D domains up to 300M degrees of freedom and one billion cells on 16K CPU cores in the Marenostrum-IV platform. The parallel implementation of the AgFEM method is available in the large-scale finite element package FEMPAR .", "venue": "ArXiv", "authors": ["Francesc  Verdugo", "Alberto F. Mart\u00edn", "Santiago  Badia"], "year": 2019, "n_citations": 16}
{"id": 2214453, "s2_id": "fc075ff3b2f352e3854dbb159a74f9d7d54e7ea3", "title": "Software Abstractions and Methodologies for HPC Simulation Codes on Future Architectures", "abstract": "Simulations with multi-physics modeling have become crucial to many science and engineering fields, and multi-physics capable scientific software is as important to these fields as instruments and facilities are to experimental sciences. The current generation of mature multi-physics codes would have sustainably served their target communities with modest amount of ongoing investment for enhancing capabilities. However, the revolution occurring in the hardware architecture has made it necessary to tackle the parallelism and performance management in these codes at multiple levels. The requirements of various levels are often at cross-purposes with one another, and therefore hugely complicate the software design. All of these considerations make it essential to approach this challenge cooperatively as a community. We conducted a series of workshops under an NSF-SI2 conceptualization grant to get input from various stakeholders, and to identify broad approaches that might lead to a solution. In this position paper we detail the major concerns articulated by the application code developers, and emerging trends in utilization of programming abstractions that we found through these workshops.", "venue": "ArXiv", "authors": ["Anshu  Dubey", "Steven R. Brandt", "Richard C. Brower", "M.  Giles", "Paul D. Hovland", "Don Q. Lamb", "Frank  L\u00f6ffler", "Boyana  Norris", "Brian  O'Shea", "Claudio  Rebbi", "Marc  Snir", "Rajeev  Thakur"], "year": 2013, "n_citations": 9}
{"id": 2221415, "s2_id": "8a4d981e841aa5ff3cc2d64dcc030ae9e4ae88cd", "title": "fenicsR13: A Tensorial Mixed Finite Element Solver for the Linear R13 Equations Using the FEniCS Computing Platform", "abstract": "We present a mixed finite element solver for the linearized R13 equations of non-equilibrium gas dynamics. The Python implementation builds upon the software tools provided by the FEniCS computing platform. We describe a new tensorial approach utilizing the extension capabilities of FEniCS's Unified Form Language (UFL) to define required differential operators for tensors above second degree. The presented solver serves as an example for the implementation of tensorial variational formulations in FEniCS, for which the documentation and literature seem to be very sparse. Using the software abstraction levels provided by the UFL allows an almost one-to-one correspondence between the underlying mathematics and the resulting source code. Test cases support the correctness of the proposed method using validation with exact solutions. To justify the usage of extended gas flow models, we discuss typical application cases involving rarefaction effects. The documented and validated solver is provided publicly.", "venue": "ACM Trans. Math. Softw.", "authors": ["Lambert  Theisen", "Manuel  Torrilhon"], "year": 2021, "n_citations": 2}
{"id": 2226092, "s2_id": "69de382c1a2c6a0fd6921c8d6ee1dc0711ebe3a3", "title": "Efficient recursive least squares solver for rank-deficient matrices", "abstract": "Abstract Updating a linear least-squares solution can be critical for near real-time signal-processing applications. The Greville algorithm proposes a simple formula for updating the pseudoinverse of a matrix A \u2208 R n \u00d7 m with rank r . In this paper, we explicitly derive a similar formula by maintaining a general rank factorization, which we call rank-Greville. Based on this formula, we implemented a recursive least-squares algorithm exploiting the rank-deficiency of A , achieving the update of the minimum-norm least-squares solution in O ( m r ) operations and, therefore, solving the linear least-squares problem from scratch in O ( n m r ) operations. We empirically confirmed that this algorithm displays a better asymptotic time complexity than LAPACK solvers for rank-deficient matrices. The numerical stability of rank-Greville was found to be comparable to Cholesky-based solvers. Nonetheless, our implementation supports exact numerical representations of rationals, due to its remarkable algebraic simplicity.", "venue": "Appl. Math. Comput.", "authors": ["Ruben  Staub", "Stephan N. Steinmann"], "year": 2021, "n_citations": 1}
{"id": 2249426, "s2_id": "f47647e953159c341c4dfd10eb5ab4f746aff6cd", "title": "kEDM: A Performance-portable Implementation of Empirical Dynamic Modeling using Kokkos", "abstract": "Empirical Dynamic Modeling (EDM) is a state-of-the-art non-linear time-series analysis framework. Despite its wide applicability, EDM was not scalable to large datasets due to its expensive computational cost. To overcome this obstacle, researchers have attempted and succeeded in accelerating EDM from both algorithmic and implementational aspects. In previous work, we developed a massively parallel implementation of EDM targeting HPC systems (mpEDM). However, mpEDM maintains different backends for different architectures. This design becomes a burden in the increasingly diversifying HPC systems, when porting to new hardware. In this paper, we design and develop a performance-portable implementation of EDM based on the Kokkos performance portability framework (kEDM), which runs on both CPUs and GPUs while based on a single codebase. Furthermore, we optimize individual kernels specifically for EDM computation, and use real-world datasets to demonstrate up to 5.5 \u00d7 speedup compared to mpEDM in convergent cross mapping computation.", "venue": "PEARC", "authors": ["Keichi  Takahashi", "Wassapon  Watanakeesuntorn", "Kohei  Ichikawa", "Joseph  Park", "Ryousei  Takano", "Jason  Haga", "George  Sugihara", "Gerald M. Pao"], "year": 2021, "n_citations": 0}
{"id": 2259686, "s2_id": "3472fbec6d1d3b13d85802403ee10cb8b036c2d3", "title": "Quantum Anticipation Explorer", "abstract": "Quantum anticipation explorer is a computer program allowing the numerical exploration of quantum anticipation which has been analyzed in arXiv:0810.183v1 and arXiv:1003.1090v1 for H-Atom, equidistant, random and custom spectra. This tool determines the anticipation strength at those times orthogonal evolution is possible. This paper is the user's guide explaining its capabilities, installation and usage, and documenting the mathematics and algorithms implemented in the software. A zip file containing the setup and documentation can be downloaded from this http URL free of cost.", "venue": "ArXiv", "authors": ["Hans-Rudolf  Thomann"], "year": 2011, "n_citations": 0}
{"id": 2260675, "s2_id": "4e66cfd1f0ab4d8b0383213817e5dc3f8ebdbafa", "title": "TSFC: a structure-preserving form compiler", "abstract": "A form compiler takes a high-level description of the weak form of partial differential equations and produces low-level code that carries out the finite element assembly. In this paper we present the Two-Stage Form Compiler (TSFC), a new form compiler with the main motivation being to maintain the structure of the input expression as long as possible. This facilitates the application of optimizations at the highest possible level of abstraction. TSFC features a novel, structure-preserving method for separating the contributions of a form to the subblocks of the local tensor in discontinuous Galerkin problems. This enables us to preserve the tensor structure of expressions longer through the compilation process than is possible with other form compilers. This is also achieved in part by a two-stage approach that cleanly separates the lowering of finite element constructs to tensor algebra in the first stage, from the scheduling of those tensor operations in the second stage. TSFC also efficiently traverse...", "venue": "SIAM J. Sci. Comput.", "authors": ["Mikl\u00f3s  Homolya", "Lawrence  Mitchell", "Fabio  Luporini", "David A. Ham"], "year": 2018, "n_citations": 42}
{"id": 2271499, "s2_id": "e3184e0fa5f2ed123716ccc1ee470e9d4a3b9323", "title": "Flexible, Scalable Mesh and Data Management using PETSc DMPlex", "abstract": "Designing a scientific software stack to meet the needs of the next-generation of mesh-based simulation demands, not only scalable and efficient mesh and data management on a wide range of platforms, but also an abstraction layer that makes it useful for a wide range of application codes. Common utility tasks, such as file I/O, mesh distribution, and work partitioning, should be delegated to external libraries in order to promote code re-use, extensibility and software interoperability. In this paper we demonstrate the use of PETSc's DMPlex data management API to perform mesh input and domain partitioning in Fluidity, a large scale CFD application. We demonstrate that raising the level of abstraction adds new functionality to the application code, such as support for additional mesh file formats and mesh reordering, while improving simlutation startup cost through more efficient mesh distribution. Moreover, the separation of concerns accomplished through this interface shifts critical performance and interoperability issues, such as scalable I/O and file format support, to a widely used and supported open source community library, improving the sustainability, performance, and functionality of Fluidity.", "venue": "ArXiv", "authors": ["Michael  Lange", "Matthew G. Knepley", "Gerard  Gorman"], "year": 2015, "n_citations": 13}
{"id": 2272313, "s2_id": "d0cdd5f55fd692ec56b3c068b1d2128f85030ba3", "title": "ManyClaw: Slicing and dicing Riemann solvers for next generation highly parallel architectures", "abstract": "Next generation computer architectures will include order of magnitude more intra-node parallelism; however, many application programmers have a difficult time keeping their codes current with the state-of-the-art machines. In this context, we analyze Hyperbolic PDE solvers, which are used in the solution of many important applications in science and engineering. We present ManyClaw, a project intended to explore the exploitation of intra-node parallelism in hyperbolic PDE solvers via the Clawpack software package for solving hyperbolic PDEs. Our goal is to separate the low level parallelism and the physical equations thus providing users the capability to leverage intra-node parallelism without explicitly writing code to take advantage of newer architectures.", "venue": "ArXiv", "authors": ["Andy R. Terrel", "Kyle T. Mandli"], "year": 2013, "n_citations": 3}
{"id": 2274453, "s2_id": "de8c67bfc818f5f710f3517c86bcbc213315ece3", "title": "Method for representing an exponent in a fifth-dimensional hypercomplex number systems using a hypercomplex computing software", "abstract": "Over time, this approach was generalized to other transcendental functions of a HV: trigonometric sine and cosine, hyperboic sine and cosine, and others, and is now generally accepted [2]. In many works, studies were carried out on the construction of an image of the exponential function of the quaternion. For this, various methods are used, which are based on the symmetry properties of quaternions [2,3]. Images of other transcendental functions of the quaternion are also constructed: logarithmic function, trigonometric sine and cosine [3]. These functions have found use not only in scientific applications (physics, mechanics), but also in technical ones(orientation of a rigid body in space, gyroscopy, robotics and others). The problem of constructing images of nonlinear functions of a HV is reduced to their definition from the point of view of the structure of computations over the hypercomplex argument and their image in the form of a hypercomplex function:", "venue": "ArXiv", "authors": ["Yuliia  Boiarinova", "Yakov  Kalinovskiy"], "year": 2021, "n_citations": 0}
{"id": 2275695, "s2_id": "a8afd0cc87fac5f125a081e39b6b476295261628", "title": "Faster Base64 Encoding and Decoding Using AVX2 Instructions", "abstract": "Web developers use base64 formats to include images, fonts, sounds, and other resources directly inside HTML, JavaScript, JSON, and XML files. We estimate that billions of base64 messages are decoded every day. We are motivated to improve the efficiency of base64 encoding and decoding. Compared to state-of-the-art implementations, we multiply the speeds of both the encoding (\u2248 10 \u00d70) and the decoding (\u2248 7 \u00d7). We achieve these good results by using the single-instruction-multiple-data instructions available on recent Intel processors (AVX2). Our accelerated software abides by the specification and reports errors when encountering characters outside of the base64 set. It is available online as free software under a liberal license.", "venue": "ACM Trans. Web", "authors": ["Wojciech  Mula", "Daniel  Lemire"], "year": 2018, "n_citations": 8}
{"id": 2276211, "s2_id": "d43a49f69403535a34caa03c95dcf5465237d4c2", "title": "A Unified 2D/3D Large-Scale Software Environment for Nonlinear Inverse Problems", "abstract": "Large-scale parameter estimation problems are among some of the most computationally demanding problems in numerical analysis. An academic researcher\u2019s domain-specific knowledge often precludes that of software design, which results in inversion frameworks that are technically correct but not scalable to realistically sized problems. On the other hand, the computational demands for realistic problems result in industrial codebases that are geared solely for high performance, rather than comprehensibility or flexibility. We propose a new software design for inverse problems constrained by partial differential equations that bridges the gap between these two seemingly disparate worlds. A hierarchical and modular design reduces the cognitive burden on the user while exploiting high-performance primitives at the lower levels. Our code has the added benefit of actually reflecting the underlying mathematics of the problem, which lowers the cognitive load on the user using it and reduces the initial startup period before a researcher can be fully productive. We also introduce a new preconditioner for the 3D Helmholtz equation that is suitable for fault-tolerant distributed systems. Numerical experiments on a variety of 2D and 3D test problems demonstrate the effectiveness of this approach on scaling algorithms from small- to large-scale problems with minimal code changes.", "venue": "ACM Trans. Math. Softw.", "authors": ["Curt Da Silva", "Felix J. Herrmann"], "year": 2019, "n_citations": 9}
{"id": 2281018, "s2_id": "e907594b87a73863025f70e8dee48449035ce96f", "title": "A multiresolution Discrete Element Method for triangulated objects with implicit timestepping", "abstract": "Simulations of many rigid bodies colliding with each other sometimes yield particularly interesting results if the colliding objects differ significantly in size and are nonspherical. The most expensive part within such a simulation code is the collision detection. We propose a family of novel multiscale collision detection algorithms that can be applied to triangulated objects within explicit and implicit time stepping methods. They are well-suited to handle objects that cannot be represented by analytical shapes or assemblies of analytical objects. Inspired by multigrid methods and adaptive mesh refinement, we determine collision points iteratively over a resolution hierarchy, and combine a functional minimisation plus penalty parameters with the actual comparision-based geometric distance calculation. Coarse surrogate geometry representations identify \u201cno collision\u201d scenarios early on and otherwise yield an educated guess which triangle subsets of the next finer level potentially yield collisions. They prune the search tree, and furthermore feed conservative contact force estimates into the iterative solve behind an implicit time stepping. Implicit time stepping and non-analytical shapes often yield prohibitive high compute cost for rigid body simulations. Our approach reduces these cost algorithmically by one to two orders of magnitude. It also exhibits high vectorisation efficiency due to its iterative nature.", "venue": "ArXiv", "authors": ["Peter J. Noble", "Tobias  Weinzierl"], "year": 2021, "n_citations": 0}
{"id": 2286176, "s2_id": "eddf7792143ce78bf4e949ded32ef6f354d19bf5", "title": "A GPU-based Multi-level Algorithm for Boundary Value Problems", "abstract": "A novel and scalable geometric multi-level algorithm is presented for the numerical solution of elliptic partial differential equations, specially designed to run with high occupancy of streaming processors inside Graphics Processing Units(GPUs). The algorithm consists of iterative, superposed operations on a single grid, and it is composed of two simple full-grid routines: a restriction and a coarsened interpolation-relaxation. The restriction is used to collect sources using recursive coarsened averages, and the interpolation-relaxation simultaneously applies coarsened finite-difference operators and interpolations. The routines are scheduled in a saw-like refining cycle. Convergence to machine precision is achieved repeating the full cycle using accumulated residuals and successively collecting the solution. Its total number of operations scale linearly with the number of nodes. It provides an attractive fast solver for Boundary Value Problems (BVPs), specially for simulations running entirely in the GPU. Applications shown in this work include the deformation of two-dimensional grids, the computation of three-dimensional streamlines for a singular trifoil-knot vortex and the calculation of three-dimensional electric potentials in heterogeneous dielectric media.", "venue": "J. Comput. Appl. Math.", "authors": ["J. T. Becerra-Sagredo", "Francisco  Mandujano", "Carlos  Malaga"], "year": 2020, "n_citations": 1}
{"id": 2287043, "s2_id": "89ba1769cd1ddf18ef1568067e529761aadabeec", "title": "Sampling Exactly from the Normal Distribution", "abstract": "An algorithm for sampling exactly from the normal distribution is given. The algorithm reads some number of uniformly distributed random digits in a given base and generates an initial portion of the representation of a normal deviate in the same base. Thereafter, uniform random digits are copied directly into the representation of the normal deviate. Thus, in contrast to existing methods, it is possible to generate normal deviates exactly rounded to any precision with a mean cost that scales linearly in the precision. The method performs no extended precision arithmetic, calls no transcendental functions, and uses no floating point arithmetic whatsoever; it uses only simple integer operations. It can easily be adapted to sample exactly from the discrete normal distribution whose parameters are rational numbers.", "venue": "ACM Trans. Math. Softw.", "authors": ["Charles F. F. Karney"], "year": 2016, "n_citations": 53}
{"id": 2287975, "s2_id": "c9f825514bf19b997f8607112ea41da6a9719475", "title": "Dynamic Automatic Differentiation of GPU Broadcast Kernels", "abstract": "We show how forward-mode automatic differentiation (AD) can be employed within larger reverse-mode computations to dynamically differentiate broadcast operations in a GPU-friendly manner. Our technique fully exploits the broadcast Jacobian's inherent sparsity structure, and unlike a pure reverse-mode approach, this \"mixed-mode\" approach does not require a backwards pass over the broadcasted operation's subgraph, obviating the need for several reverse-mode-specific programmability restrictions on user-authored broadcast operations. Most notably, this approach allows broadcast fusion in primal code despite the presence of data-dependent control flow. We discuss an experiment in which a Julia implementation of our technique outperformed pure reverse-mode TensorFlow and Julia implementations for differentiating through broadcast operations within an HM-LSTM cell update calculation.", "venue": "NIPS 2018", "authors": ["Jarrett  Revels", "Tim  Besard", "Valentin  Churavy", "Bjorn De Sutter", "Juan Pablo Vielma"], "year": 2018, "n_citations": 7}
{"id": 2289096, "s2_id": "67d6b05ff20bcf26ddd79f5518113782824c3134", "title": "A Java Implementation of Parameter-less Evolutionary Algorithms", "abstract": "The Parameter-less Genetic Algorithm was first presented by Harik and Lobo in 1999 as an alternative to the usual trial-and-error method of finding, for each given problem, an acceptable set-up of the parameter values of the genetic algorithm. Since then, the same strategy has been successfully applied to create parameter-less versions of other population-based search algorithms such as the Extended Compact Genetic Algorithm and the Hierarchical Bayesian Optimization Algorithm. This report describes a Java implementation, Parameter-less Evolutionary Algorithm (P-EAJava), that integrates several parameter-less evolutionary algorithms into a single platform. Along with a brief description of P-EAJava, we also provide detailed instructions on how to use it, how to implement new problems, and how to generate new parameter-less versions of evolutionary algorithms. \nAt present time, P-EAJava already includes parameter-less versions of the Simple Genetic Algorithm, the Extended Compact Genetic Algorithm, the Univariate Marginal Distribution Algorithm, and the Hierarchical Bayesian Optimization Algorithm. The source and binary files of the Java implementation of P-EAJava are available for free download at this https URL", "venue": "ArXiv", "authors": ["Jos\u00e9 C. Pereira", "Fernando G. Lobo"], "year": 2015, "n_citations": 7}
{"id": 2297022, "s2_id": "f69595af7646512c6e4247bd06a40ea92f639199", "title": "Boundary knot method for Laplace and biharmonic problems", "abstract": "The boundary knot method (BKM) [1] is a meshless boundary-type radial basis function (RBF) collocation scheme, where the nonsingular general solution is used instead of fundamental solution to evaluate the homogeneous solution, while the dual reciprocity method (DRM) is employed to approximation of particular solution. Despite the fact that there are not nonsingular RBF general solutions available for Laplace and biharmonic problems, this study shows that the method can be successfully applied to these problems. The high-order general and fundamental solutions of Burger and Winkler equations are also first presented here.", "venue": "ArXiv", "authors": ["Wen  Chen"], "year": 2003, "n_citations": 16}
{"id": 2299784, "s2_id": "61e27dbae190b82639c57f180ecf97e4c46fcad9", "title": "Pymoo: Multi-Objective Optimization in Python", "abstract": "Python has become the programming language of choice for research and industry projects related to data science, machine learning, and deep learning. Since optimization is an inherent part of these research fields, more optimization related frameworks have arisen in the past few years. Only a few of them support optimization of multiple conflicting objectives at a time, but do not provide comprehensive tools for a complete multi-objective optimization task. To address this issue, we have developed pymoo, a multi-objective optimization framework in Python. We provide a guide to getting started with our framework by demonstrating the implementation of an exemplary constrained multi-objective optimization scenario. Moreover, we give a high-level overview of the architecture of pymoo to show its capabilities followed by an explanation of each module and its corresponding sub-modules. The implementations in our framework are customizable and algorithms can be modified/extended by supplying custom operators. Moreover, a variety of single, multi- and many-objective test problems are provided and gradients can be retrieved by automatic differentiation out of the box. Also, pymoo addresses practical needs, such as the parallelization of function evaluations, methods to visualize low and high-dimensional spaces, and tools for multi-criteria decision making. For more information about pymoo, readers are encouraged to visit: https://pymoo.org.", "venue": "IEEE Access", "authors": ["Julian  Blank", "Kalyanmoy  Deb"], "year": 2020, "n_citations": 137}
{"id": 2302508, "s2_id": "67f1ac0514dd8038bf90d5f7b3065636c12c54f9", "title": "Dragon: A Computation Graph Virtual Machine Based Deep Learning Framework", "abstract": "Deep Learning has made a great progress for these years. However, it is still difficult to master the implement of various models because different researchers may release their code based on different frameworks or interfaces. In this paper, we proposed a computation graph based framework which only aims to introduce well-known interfaces. It will help a lot when reproducing a newly model or transplanting models that were implemented by other frameworks. Additionally, we implement numerous recent models covering both Computer Vision and Nature Language Processing. We demonstrate that our framework will not suffer from model-starving because it is much easier to make full use of the works that are already done.", "venue": "ArXiv", "authors": ["Ting  Pan"], "year": 2017, "n_citations": 0}
{"id": 2303054, "s2_id": "5c02f9642e569319cf6ecd2a0fa2747023fde839", "title": "Task Parallel Incomplete Cholesky Factorization using 2D Partitioned-Block Layout", "abstract": "We introduce a task-parallel algorithm for sparse incomplete Cholesky factorization that utilizes a 2D sparse partitioned-block layout of a matrix. Our factorization algorithm follows the idea of algorithms-by-blocks by using the block layout. The algorithm-by-blocks approach induces a task graph for the factorization. These tasks are inter-related to each other through their data dependences in the factorization algorithm. To process the tasks on various manycore architectures in a portable manner, we also present a portable tasking API that incorporates different tasking backends and device-specific features using an open-source framework for manycore platforms i.e., Kokkos. A performance evaluation is presented on both Intel Sandybridge and Xeon Phi platforms for matrices from the University of Florida sparse matrix collection to illustrate merits of the proposed task-based factorization. Experimental results demonstrate that our task-parallel implementation delivers about 26.6x speedup (geometric mean) over single-threaded incomplete Cholesky-by-blocks and 19.2x speedup over serial Cholesky performance which does not carry tasking overhead using 56 threads on the Intel Xeon Phi processor for sparse matrices arising from various application problems.", "venue": "ArXiv", "authors": ["Kyungjoo  Kim", "Sivasankaran  Rajamanickam", "George  Stelle", "H. Carter Edwards", "Stephen  Olivier"], "year": 2016, "n_citations": 13}
{"id": 2306580, "s2_id": "e388f29ad0e9cba1f7ad3d0df29efb382420a771", "title": "A Parallel Edge Orientation Algorithm for Quadrilateral Meshes", "abstract": "One approach to achieving correct finite element assembly is to ensure that the local orientation of facets relative to each cell in the mesh is consistent with the global orientation of that facet. Rognes et al. have shown how to achieve this for any mesh composed of simplex elements, and deal.II contains a serial algorithm for constructing a consistent orientation of any quadrilateral mesh of an orientable manifold. The core contribution of this paper is the extension of this algorithm for distributed memory parallel computers, which facilitates its seamless application as part of a parallel simulation system. Furthermore, our analysis establishes a link between the well-known Union-Find algorithm and the construction of a consistent orientation of a quadrilateral mesh. As a result, existing work on the parallelization of the Union-Find algorithm can be easily adapted to construct further parallel algorithms for mesh orientations.", "venue": "SIAM J. Sci. Comput.", "authors": ["Mikl\u00f3s  Homolya", "David A. Ham"], "year": 2016, "n_citations": 21}
{"id": 2309941, "s2_id": "94102862055932bc2a2a4f88f200640159725e09", "title": "Automatic Differentiation using Constraint Handling Rules in Prolog", "abstract": "Automatic differentiation is a technique which allows a programmer to define a numerical computation via compositions of a broad range of numeric and computational primitives and have the underlying system support the computation of partial derivatives of the result with respect to any of its inputs, without making any finite difference approximations, and without manipulating large symbolic expressions representing the computation. This note describes a novel approach to reverse mode automatic differentiation using constraint logic programmming, specifically, the constraint handling rules (CHR) library of SWI Prolog, resulting in a very small (50 lines of code) implementation. When applied to a differentiation-based implementation of the inside-outside algorithm for parameter learning in probabilistic grammars, the CHR based implementations outperformed two well-known frameworks for optimising differentiable functions, Theano and TensorFlow, by a large margin.", "venue": "ArXiv", "authors": ["Samer  Abdallah"], "year": 2017, "n_citations": 1}
{"id": 2315517, "s2_id": "8fc8e07df2ef76af018851dca92f4f6f95edf2a4", "title": "Parallel implementation of a compatible high-order meshless method for the Stokes' equations", "abstract": "A parallel implementation of a compatible discretization scheme for steady-state Stokes problems is presented in this work. The scheme uses generalized moving least squares to generate differential operators and apply boundary conditions. This meshless scheme allows a high-order convergence for both the velocity and pressure, while also incorporates finite-difference-like sparse discretization. Additionally, the method is inherently scalable: the stencil generation process requires local inversion of matrices amenable to GPU acceleration, and the divergence-free treatment of velocity replaces the traditional saddle point structure of the global system with elliptic diagonal blocks amenable to algebraic multigrid. The implementation in this work uses a variety of Trilinos packages to exploit this local and global parallelism, and benchmarks demonstrating high-order convergence and weak scalability are provided.", "venue": "ArXiv", "authors": ["Quang-Thinh  Ha", "Paul A. Kuberry", "Nathaniel A. Trask", "Emily M. Ryan"], "year": 2021, "n_citations": 0}
{"id": 2320563, "s2_id": "8099d78ce6d38cc3501888139ef0467632281cde", "title": "Efficient algorithms for computing rank-revealing factorizations on a GPU", "abstract": "Standard rank-revealing factorizations such as the singular value decomposition and column pivoted QR factorization are challenging to implement efficiently on a GPU. A major difficulty in this regard is the inability of standard algorithms to cast most operations in terms of the Level-3 BLAS. This paper presents two alternative algorithms for computing a rank-revealing factorization of the form A = UTV\u2217, where U and V are orthogonal and T is triangular. Both algorithms use randomized projection techniques to cast most of the flops in terms of matrix-matrix multiplication, which is exceptionally efficient on the GPU. Numerical experiments illustrate that these algorithms achieve an order of magnitude acceleration over finely tuned GPU implementations of the SVD while providing low rank approximation errors close to that of the SVD.", "venue": "ArXiv", "authors": ["Nathan  Heavner", "Chao  Chen", "Abinand  Gopal", "Per-Gunnar  Martinsson"], "year": 2021, "n_citations": 0}
{"id": 2333429, "s2_id": "4d8f2e379949390a62b5b0d3c1b16ec0f4766ceb", "title": "Efficient SIMD RNG for Varying-Parameter Streams: C++ Class BatchRNG", "abstract": "Single-Instruction, Multiple-Data (SIMD) random number generators (RNGs) take advantage of vector units to offer significant performance gain over non-vectorized libraries, but they often rely on batch production of deviates from distributions with fixed parameters. In many statistical applications such as Gibbs sampling, parameters of sampled distributions change from one iteration to the next, requiring that random deviates be generated one-at-a-time. This situation can render vectorized RNGs inefficient, and even inferior to their scalar counterparts. The C++ class BatchRNG uses buffers of base distributions such uniform, Gaussian and exponential to take advantage of vector units while allowing for sequences of deviates to be generated with varying parameters. These small buffers are consumed and replenished as needed during a program execution. Performance tests using Intel Vector Statistical Library (VSL) on various probability distributions illustrates the effectiveness of the proposed batching strategy.", "venue": "ArXiv", "authors": ["Alireza S. Mahani", "Mansour T. A. Sharabiani"], "year": 2014, "n_citations": 0}
{"id": 2335490, "s2_id": "b6d61c11c5f2b7f8a3f1fe61273e9305c648dd6e", "title": "Conforming restricted Delaunay mesh generation for piecewise smooth complexes", "abstract": "A Frontal-Delaunay refinement algorithm for mesh generation in piecewise smooth domains is described. Built using a restricted Delaunay framework, this new algorithm combines a number of novel features, including: (i) an unweighted, conforming restricted Delaunay representation for domains specified as a (non-manifold) collection of piecewise smooth surface patches and curve segments, (ii) a protection strategy for domains containing curve segments that subtend sharply acute angles, and (iii) a new class of off-centre refinement rules designed to achieve high-quality point-placement along embedded curve features. Experimental comparisons show that the new Frontal-Delaunay algorithm outperforms a classical (statically weighted) restricted Delaunay-refinement technique for a number of three-dimensional benchmark problems.", "venue": "ArXiv", "authors": ["Darren  Engwirda"], "year": 2016, "n_citations": 17}
{"id": 2337902, "s2_id": "a592199fc795e14ad1ccfe0ed0a703d4a0249461", "title": "An Efficient ADER-DG Local Time Stepping Scheme for 3D HPC Simulation of Seismic Waves in Poroelastic Media", "abstract": "Many applications from the fields of seismology and geoengineering require simulations of seismic waves in porous media. Biot\u2019s theory of poroelasticity describes the coupling between solid and fluid phases and introduces a stiff reactive source term (Darcy\u2019s Law) into the elastodynamic wave equations, thereby increasing computational cost of respective numerical solvers and motivating efficient methods utilising High-Performance Computing. We present a novel realisation of the discontinuous Galerkin scheme with Arbitrary High-Order DERivative time stepping (ADER-DG) that copes with stiff source terms. To integrate this source term with a reasonable time step size, we utilise an element-local space-time predictor, which needs to solve medium-sized linear systems \u2013 each with 1,000 to 10,000 unknowns \u2013 in each element update (i.e., billions of times). We present a novel block-wise back-substitution algorithm for solving these systems efficiently, thus enabling large-scale 3D simulations. In comparison to LU decomposition, we reduce the number of floating-point operations by a factor of up to 25, when using polynomials of degree 6. The block-wise back-substitution is mapped to a sequence of small matrix-matrix multiplications, for which code generators are available to generate highly optimised code. We verify the new solver thoroughly against analytical and semi-analytical reference solutions in problems of increasing complexity. We demonstrate high-order convergence of the scheme for 3D problems. We verify the correct treatment of point sources and boundary conditions, including homogeneous and heterogeneous full space problems as well as problems with traction-free boundary conditions. In addition, we compare against a finite difference solution for a newly defined 3D layer over half-space problem containing an internal material interface and free surface. We find that extremely high accuracy is required to accurately resolve the slow, diffusive P-wave at a or near a free surface, while we also demonstrate that solid particle velocities are not affected by coarser resolutions. We demonstrate that by using a clustered local time stepping scheme, time to solution is reduced by a factor of 6 to 10 compared to global time stepping. We conclude our study with a scaling and performance analysis on the SuperMUC-NG supercomputer, demonstrating our implementation\u2019s high computational efficiency and its potential for extreme-scale simulations.", "venue": "Journal of Computational Physics", "authors": ["Sebastian  Wolf", "Martin  Galis", "Carten  Uphoff", "Alice-Agnes  Gabriel", "Peter  Moczo", "David  Gregor", "Michael  Bader"], "year": 2021, "n_citations": 0}
{"id": 2339356, "s2_id": "dc885e9cbb14480592a332965ab6cebd7a04a0a9", "title": "TSSOS: a Julia library to exploit sparsity for large-scale polynomial optimization", "abstract": "The Julia library TSSOS aims at helping polynomial optimizers to solve large-scale problems with sparse input data. The underlying algorithmic framework is based on exploiting correlative and term sparsity to obtain a new moment-SOS hierarchy involving potentially much smaller positive semidefinite matrices. TSSOS can be applied to numerous problems ranging from power networks to eigenvalue and trace optimization of noncommutative polynomials, involving up to tens of thousands of variables and constraints.", "venue": "ArXiv", "authors": ["Victor  Magron", "Jie  Wang"], "year": 2021, "n_citations": 6}
{"id": 2339418, "s2_id": "eeb303ea6451630bc9f34bb262dc0f8b2c6f97ee", "title": "Specific \"scientific\" data structures, and their processing", "abstract": "Programming physicists use, as all programmers, arrays, lists, tuples, records, etc., and this requires some change in their thought patterns while converting their formulae into some code, since the \"data structures\" operated upon, while elaborating some theory and its consequences, are rather: power series and Pad\\'e approximants, differential forms and other instances of differential algebras, functionals (for the variational calculus), trajectories (solutions of differential equations), Young diagrams and Feynman graphs, etc. Such data is often used in a [semi-]numerical setting, not necessarily \"symbolic\", appropriate for the computer algebra packages. Modules adapted to such data may be \"just libraries\", but often they become specific, embedded sub-languages, typically mapped into object-oriented frameworks, with overloaded mathematical operations. Here we present a functional approach to this philosophy. We show how the usage of Haskell datatypes and - fundamental for our tutorial - the application of lazy evaluation makes it possible to operate upon such data (in particular: the \"infinite\" sequences) in a natural and comfortable manner.", "venue": "DSL", "authors": ["Jerzy  Karczmarczuk"], "year": 2011, "n_citations": 0}
{"id": 2341641, "s2_id": "48a33f0c8fde2bc630c006dee9a686236de4b00b", "title": "HPC optimal parallel communication algorithm for the simulation of fractional-order systems", "abstract": "A parallel numerical simulation algorithm is presented for fractional-order systems involving Caputo-type derivatives, based on the Adams\u2013Bashforth\u2013Moulton predictor\u2013corrector scheme. The parallel algorithm is implemented using several different approaches: a pure MPI version, a combination of MPI with OpenMP optimization and a memory saving speedup approach. All tests run on a BlueGene/P cluster, and comparative improvement results for the running time are provided. As an applied experiment, the solutions of a fractional-order version of a system describing a forced series LCR circuit are numerically computed, depicting cascades of period-doubling bifurcations which lead to the onset of chaotic behavior.", "venue": "The Journal of Supercomputing", "authors": ["Cosmin  Bonchis", "Eva  Kaslik", "Florin  Rosu"], "year": 2018, "n_citations": 7}
{"id": 2346110, "s2_id": "5d2974477ee209094ade22d1434f0515f81dd227", "title": "Practically efficient methods for performing bit-reversed permutation in C++11 on the x86-64 architecture", "abstract": "The bit-reversed permutation is a famous task in signal processing and is key to efficient implementation of the fast Fourier transform. This paper presents optimized C++11 implementations of five extant methods for computing the bit-reversed permutation: Stockham auto-sort, naive bitwise swapping, swapping via a table of reversed bytes, local pairwise swapping of bits, and swapping via a cache-localized matrix buffer. Three new strategies for performing the bit-reversed permutation in C++11 are proposed: an inductive method using the bitwise XOR operation, a template-recursive closed form, and a cache-oblivious template-recursive approach, which reduces the bit-reversed permutation to smaller bit-reversed permutations and a square matrix transposition. These new methods are compared to the extant approaches in terms of theoretical runtime, empirical compile time, and empirical runtime. The template-recursive cache-oblivious method is shown to be competitive with the fastest known method; however, we demonstrate that the cache-oblivious method can more readily benefit from parallelization on multiple cores and on the GPU.", "venue": "ArXiv", "authors": ["Christian  Knauth", "Boran  Adas", "Daniel  Whitfield", "Xuesong  Wang", "Lydia  Ickler", "Tim  Conrad", "Oliver  Serang"], "year": 2017, "n_citations": 4}
{"id": 2348076, "s2_id": "9af942fc96192aa5aa6f175eea50cb3365df0dfa", "title": "Program Verification of Numerical Computation", "abstract": "These notes outline a formal method for program verification of numerical computation. It forms the basis of the software package VPC in its initial phase of development. Much of the style of presentation is in the form of notes that outline the definitions and rules upon which VPC is based. The initial motivation of this project was to address some practical issues of computation, especially of numerically intensive programs that are commonplace in computer models. The project evolved into a wider area for program construction as proofs leading to a model of inference in a more general sense. Some basic results of machine arithmetic are derived as a demonstration of VPC.", "venue": "ArXiv", "authors": ["Garry  Pantelis"], "year": 2014, "n_citations": 1}
{"id": 2351896, "s2_id": "2248b9f86e0116d9ac9a206c25bda51b443b6433", "title": "Communication-hiding pipelined BiCGSafe methods for solving large linear systems", "abstract": "Recently, a new variant of the BiCGStab method, known as the pipelined BiCGStab, has been proposed. This method can achieve a higher degree of scalability and speed-up rates through a mechanism in which the communication phase for the computation of the inner product can be overlapped with computation of the matrix-vector product. Meanwhile, several generalized iteration methods with better convergence behavior than BiCGStab exist, such as ssBiCGSafe, BiCGSafe, and GPBi-CG. Among these methods, ssBiCGSafe, which requires a single phase of computing inner products per iteration, is best suited for high-performance computing systems. As described herein, inspired by the success of the pipelined BiCGStab method, we propose pipelined variations of the ssBiCGSafe method in which only one phase of inner product computation per iteration is required and this phase of inner product computation can be overlapped with the matrix-vector computation. Through numerical experimentation, we demonstrate that the proposed methods engender improvements in convergence behavior and execution time compared to the pipelined BiCGStab and ssBiCGSafe methods.", "venue": "ArXiv", "authors": ["VIET Q. H. HUYNH", "HIROSHI  SUITO"], "year": 2021, "n_citations": 0}
{"id": 2354850, "s2_id": "0bde8d9367d1004c7396dd69cb27ed97dc2f8d77", "title": "MatConvNet: Convolutional Neural Networks for MATLAB", "abstract": "MatConvNet is an open source implementation of Convolutional Neural Networks (CNNs) with a deep integration in the MATLAB environment. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing convolutions with filter banks, feature pooling, normalisation, and much more. MatConvNet can be easily extended, often using only MATLAB code, allowing fast prototyping of new CNN architectures. At the same time, it supports efficient computation on CPU and GPU, allowing to train complex models on large datasets such as ImageNet ILSVRC containing millions of training examples", "venue": "ACM Multimedia", "authors": ["Andrea  Vedaldi", "Karel  Lenc"], "year": 2015, "n_citations": 2713}
{"id": 2358298, "s2_id": "a17e53c7be7113614183568e2c6f48cee8809475", "title": "Function space bases in the dune-functions module", "abstract": "The dune-functions Dune module provides interfaces for functions and function space bases. It forms one abstraction level above grids, shape functions, and linear algebra, and provides infrastructure for full discretization frameworks like dune-pdelab and dune-fem. This document describes the function space bases provided by dune-functions. These are based on an abstract description of bases for product spaces as trees of simpler bases. From this description, many different numberings of degrees of freedom by multi-indices can be derived in a natural way. We describe the abstract concepts, document the programmer interface, and give a complete example program that solves the stationary Stokes equation using Taylor-Hood elements.", "venue": "ArXiv", "authors": ["Christian  Engwer", "Carsten  Gr\u00e4ser", "Steffen  M\u00fcthing", "Oliver  Sander"], "year": 2018, "n_citations": 6}
{"id": 2362940, "s2_id": "bc427f94b418080b0ab01c6e71160348d2327313", "title": "Developing a High Performance Software Library with MPI and CUDA for Matrix Computations", "abstract": "Nowadays, the paradigm of parallel computing is changing. CUDA is now a popular programming model for general purpose computations on GPUs and a great number of applications were ported to CUDA obtaining speedups of orders of magnitude comparing to optimized CPU implementations. Hybrid approaches that combine the message passing model with the shared memory model for parallel computing are a solution for very large applications. We considered a heterogeneous cluster that combines the CPU and GPU computations using MPI and CUDA for developing a high performance linear algebra library. Our library deals with large linear systems solvers because they are a common problem in the fields of science and engineering. Direct methods for computing the solution of such systems can be very expensive due to high memory requirements and computational cost. An efficient alternative are iterative methods which computes only an approximation of the solution. In this paper we present an implementation of a library that uses a hybrid model of computation using MPI and CUDA implementing both direct and iterative linear systems solvers. Our library implements LU and Cholesky factorization based solvers and some of the non-stationary iterative methods using the MPI/CUDA combination. We compared the performance of our MPI/CUDA implementation with classic programs written to be run on a single CPU.", "venue": "ArXiv", "authors": ["Bogdan  Oancea", "Tudorel  Andrei"], "year": 2015, "n_citations": 5}
{"id": 2369060, "s2_id": "e16c3b84ee11072117dc45e8b1e9fb8d472630d0", "title": "Random numbers from the tails of probability distributions using the transformation method", "abstract": "The speed of many one-line transformation methods for the production of, for example, L\u00e9vy alpha-stable random numbers, which generalize Gaussian ones, and Mittag-Leffler random numbers, which generalize exponential ones, is very high and satisfactory for most purposes. However, fast rejection techniques like the ziggurat by Marsaglia and Tsang promise a significant speed-up for the class of decreasing probability densities, if it is possible to complement them with a method that samples the tails of the infinite support. This requires the fast generation of random numbers greater or smaller than a certain value. We present a method to achieve this, and also to generate random numbers within any arbitrary interval. We demonstrate the method showing the properties of the transformation maps of the above mentioned distributions as examples of stable and geometric stable random numbers used for the stochastic solution of the space-time fractional diffusion equation.", "venue": "ArXiv", "authors": ["Daniel  Fulger", "Enrico  Scalas", "Guido  Germano"], "year": 2009, "n_citations": 5}
{"id": 2372674, "s2_id": "eddf1a55005f6526df5c60ddc55d2240641fdc03", "title": "GPU-Based Heuristic Solver for Linear Sum Assignment Problems Under Real-time Constraints", "abstract": "In this thesis, we explore the use of a centrally-coordinated peer-to-peer overlay as a possible solution to the live streaming problem. Our contribution lies in showing that such approach is indeed feasible given that a number of key challenges are met. The motivation behind exploring an alternative design is that, although a number of approaches have been investigated in the past, e.g. mesh-pull and tree-push, hybrids and best-of-both-worlds mesh-push, no consensus has been reached on the best solution for the problem of peer-to-peer live streaming, despite current deployments and reported successes. In the proposed system, we model sender/receiver peer assignments as an optimization problem. Optimized peer selection based on multiple utility factors, such as bandwidth availability, delays and connectivity compatibility, make it possible to achieve large source bandwidth savings and provide high quality of user experience. Clear benefits of our approach are observed when Network Address Translation constraints are present on the network. We have addressed key scalability issues of our platform by parallelizing the heuristic which is the core of our optimization engine and by implementing the resulting algorithm on commodity Graphic Processing Units (GPUs). The outcome is a Linear Sum Assignment Problem (LSAP) solver for time-constrained systems which produces near-optimal results and can be used for any instance of LSAP, i.e. not only in our system. As part of this work, we also present our experience in working with Network Address Translators (NATs) traversal in peer-to-peer systems. Our contribution in this context is threefold. First, we provide a semi-formal model of state of the art NAT behaviors. Second, we use our model to show which NAT combinations can be theoretically traversed and which not. Last, for each of the combinations, we state which traversal technique should be used. Our findings are confirmed by experimental results on a real network. Finally, we address the problem of reproducibility in testing, debugging and evaluation of our peer-to-peer application. We achieve this by providing a software framework which can be transparently integrated with any already-existing software and which is able to handle concurrency, system time and network events in a reproducible manner.", "venue": "ArXiv", "authors": ["Roberto  Roverso", "Amgad  Naiem", "Mohammed  El-Beltagy", "Sameh  El-Ansary"], "year": 2011, "n_citations": 0}
{"id": 2374374, "s2_id": "ca725ed426d0f9fc3c79052aaf438c13889065ca", "title": "Quasi-matrix-free Hybrid Multigrid on Dynamically Adaptive Cartesian Grids", "abstract": "We present a family of spacetree-based multigrid realizations using the tree\u2019s multiscale nature to derive coarse grids. They align with matrix-free geometric multigrid solvers as they never assemble the system matrices, which is cumbersome for dynamically adaptive grids and full multigrid. The most sophisticated realizations use BoxMG to construct operator-dependent prolongation and restriction in combination with Galerkin/Petrov-Galerkin coarse-grid operators. This yields robust solvers for nontrivial elliptic problems. We embed the algebraic, problem-dependent, and grid-dependent multigrid operators as stencils into the grid and evaluate all matrix-vector products in situ throughout the grid traversals. Such an approach is not literally matrix-free as the grid carries the matrix. We propose to switch to a hierarchical representation of all operators. Only differences of algebraic operators to their geometric counterparts are held. These hierarchical differences can be stored and exchanged with small memory footprint. Our realizations support arbitrary dynamically adaptive grids while they vertically integrate the multilevel operations through spacetree linearization. This yields good memory access characteristics, while standard colouring of mesh entities with domain decomposition allows us to use parallel many-core clusters. All realization ingredients are detailed such that they can be used by other codes.", "venue": "ACM Trans. Math. Softw.", "authors": ["Marion  Weinzierl", "Tobias  Weinzierl"], "year": 2018, "n_citations": 8}
{"id": 2383594, "s2_id": "d6a2a599e5f82b457b69c498a10c8700c16fe8d6", "title": "On the Complexity and Parallel Implementation of Hensel's Lemma and Weierstrass Preparation", "abstract": "Hensel\u2019s lemma, combined with repeated applications of Weierstrass preparation theorem, allows for the factorization of polynomials with multivariate power series coefficients. We present a complexity analysis for this method and leverage those results to guide the load-balancing of a parallel implementation to concurrently update all factors. In particular, the factorization creates a pipeline where the terms of degree k of the first factor are computed simultaneously with the terms of degree k \u2212 1 of the second factor, etc. An implementation challenge is the inherent irregularity of computational work between factors, as our complexity analysis reveals. Additional resource utilization and load-balancing is achieved through the parallelization of Weierstrass preparation. Experimental results show the efficacy of this mixed parallel scheme, achieving up to 9\u00d7 parallel speedup on a 12-core ma-", "venue": "CASC", "authors": ["Alexander  Brandt", "Marc Moreno Maza"], "year": 2021, "n_citations": 1}
{"id": 2390428, "s2_id": "0f63c4756bd50df69e713ba3529c898c06aca9dd", "title": "LSMR: An Iterative Algorithm for Sparse Least-Squares Problems", "abstract": "An iterative method LSMR is presented for solving linear systems $Ax=b$ and least-squares problems $\\min \\|Ax-b\\|_2$, with $A$ being sparse or a fast linear operator. LSMR is based on the Golub-Kahan bidiagonalization process. It is analytically equivalent to the MINRES method applied to the normal equation $A^T\\! Ax = A^T\\! b$, so that the quantities $\\|A^T\\! r_k\\|$ are monotonically decreasing (where $r_k = b - Ax_k$ is the residual for the current iterate $x_k$). We observe in practice that $\\|r_k\\|$ also decreases monotonically, so that compared to LSQR (for which only $\\|r_k\\|$ is monotonic) it is safer to terminate LSMR early. We also report some experiments with reorthogonalization.", "venue": "SIAM J. Sci. Comput.", "authors": ["David Chin-Lung Fong", "Michael A. Saunders"], "year": 2011, "n_citations": 129}
{"id": 2406615, "s2_id": "e591f327955c90e107d0ccd9aaa46a44316d2447", "title": "GPU-based Parallel Computation Support for Stan", "abstract": "This paper details an extensible OpenCL framework that allows Stan to utilize heterogeneous compute devices. It includes GPU-optimized routines for the Cholesky decomposition, its derivative, other matrix algebra primitives and some commonly used likelihoods, with more additions planned for the near future. Stan users can now benefit from speedups offered by GPUs with little effort and without changes to their existing Stan code. We demonstrate the practical utility of our work with two examples -- logistic regression and Gaussian process regression.", "venue": "ArXiv", "authors": ["Rok  Cesnovar", "Steve  Bronder", "Davor  Sluga", "Jure  Demsar", "Tadej  Ciglaric", "Sean  Talts", "Erik  Strumbelj"], "year": 2019, "n_citations": 2}
{"id": 2409674, "s2_id": "8395d06ec41a84e8edc2c2dea64e423aa1f5bddc", "title": "Benchmarking Python Tools for Automatic Differentiation", "abstract": "In this paper we compare several Python tools for automatic differentiation. In order to assess the difference in performance and precision, the problem of finding the optimal geometrical structure of the cluster with identical atoms is used as follows. First, we compare performance of calculating gradients for the objective function. We showed that the PyADOL-C and PyCppAD tools have much better performance for big clusters than the other ones. Second, we assess precision of these two tools by calculating the difference between the obtained at the optimal configuration gradient norms. We conclude that PyCppAD has the best performance among others, while having almost the same precision as the second- best performing tool - PyADOL-C.", "venue": "ArXiv", "authors": ["Andrei  Turkin", "Aung  Thu"], "year": 2016, "n_citations": 2}
{"id": 2409783, "s2_id": "f3e7829741e43e5b568f012526f35cb07eea345e", "title": "Building a Framework for Predictive Science", "abstract": "Key questions that scientists and engineers typically want to address can be formulated in terms of predictive science. Questions such as: \"How well does my computational model represent reality?\", \"What are the most important parameters in the problem?\", and \"What is the best next experiment to perform?\" are fundamental in solving scientific problems. Mystic is a framework for massively-parallel optimization and rigorous sensitivity analysis that enables these motivating questions to be addressed quantitatively as global optimization problems. Often realistic physics, engineering, and materials models may have hundreds of input parameters, hundreds of constraints, and may require execution times of seconds or longer. In more extreme cases, realistic models may be multi-scale, and require the use of high-performance computing clusters for their evaluation. Predictive calculations, formulated as a global optimization over a potential surface in design parameter space, may require an already prohibitively large simulation to be performed hundreds, if not thousands, of times. The need to prepare, schedule, and monitor thousands of model evaluations, and dynamically explore and analyze results, is a challenging problem that requires a software infrastructure capable of distributing and managing computations on large-scale heterogeneous resources. In this paper, we present the design behind an optimization framework, and also a framework for heterogeneous computing, that when utilized together, can make computationally intractable sensitivity and optimization problems much more tractable.", "venue": "ArXiv", "authors": ["Michael M. McKerns", "Leif  Strand", "Tim  Sullivan", "Alta  Fang", "Michael A. G. Aivazis"], "year": 2012, "n_citations": 102}
{"id": 2414181, "s2_id": "28fbc4b6d51c948261ac6a08299709aed1380076", "title": "STABLAB Documentation for KdV : Numerical proof of stability of roll waves in the small-amplitude limit for inclined thin film flow", "abstract": "We document the MATLAB code used in the following study: Numerical proof of stability of roll waves in the small-amplitude limit for inclined thin film flow.", "venue": "ArXiv", "authors": ["Blake  Barker"], "year": 2014, "n_citations": 20}
{"id": 2416249, "s2_id": "062ab4dae57dc57c89409165a68cc174f0195a68", "title": "Anisotropic mesh adaptation in Firedrake with PETSc DMPlex", "abstract": "Despite decades of research in this area, mesh adaptation capabilities are still rarely found in numerical simulation software. We postulate that the primary reason for this is lack of usability. Integrating mesh adaptation into existing software is difficult as non-trivial operators, such as error metrics and interpolation operators, are required, and integrating available adaptive remeshers is not straightforward. Our approach presented here is to first integrate Pragmatic, an anisotropic mesh adaptation library, into DMPlex, a PETSc object that manages unstructured meshes and their interactions with PETSc's solvers and I/O routines. As PETSc is already widely used, this will make anisotropic mesh adaptation available to a much larger community. As a demonstration of this we describe the integration of anisotropic mesh adaptation into Firedrake, an automated Finite Element based system for the portable solution of partial differential equations which already uses PETSc solvers and I/O via DMPlex. We present a proof of concept of this integration with a three-dimensional advection test case.", "venue": "ArXiv", "authors": ["Nicolas  Barral", "Matthew G. Knepley", "Michael  Lange", "Matthew D. Piggott", "Gerard  Gorman"], "year": 2016, "n_citations": 15}
{"id": 2419618, "s2_id": "e2144c255b00961092c2402bfb15b0a63dd1f4d9", "title": "Graphulo implementation of server-side sparse matrix multiply in the Accumulo database", "abstract": "The Apache Accumulo database excels at distributed storage and indexing and is ideally suited for storing graph data. Many big data analytics compute on graph data and persist their results back to the database. These graph calculations are often best performed inside the database server. The GraphBLAS standard provides a compact and efficient basis for a wide range of graph applications through a small number of sparse matrix operations. In this article, we discuss a server-side implementation of GraphBLAS sparse matrix multiplication that leverages Accumulo's native, high-performance iterators. We compare the mathematics and performance of inner and outer product implementations, and show how an outer product implementation achieves optimal performance near Accumulo's peak write rate. We offer our work as a core component to the Graphulo library that will deliver matrix math primitives for graph analytics within Accumulo.", "venue": "2015 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Dylan  Hutchison", "Jeremy  Kepner", "Vijay  Gadepally", "Adam  Fuchs"], "year": 2015, "n_citations": 30}
{"id": 2427064, "s2_id": "41b4cd4144cb34bd1e0163d33ba915c6f2e1d236", "title": "CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit", "abstract": "Continuous time Bayesian network classifiers are designed for temporal classification of multivariate streaming data when time duration of events matters and the class does not change over time. This paper introduces the CTBNCToolkit: an open source Java toolkit which provides a stand-alone application for temporal classification and a library for continuous time Bayesian network classifiers. CTBNCToolkit implements the inference algorithm, the parameter learning algorithm, and the structural learning algorithm for continuous time Bayesian network classifiers. The structural learning algorithm is based on scoring functions: the marginal log-likelihood score and the conditional log-likelihood score are provided. CTBNCToolkit provides also an implementation of the expectation maximization algorithm for clustering purpose. The paper introduces continuous time Bayesian network classifiers. How to use the CTBNToolkit from the command line is described in a specific section. Tutorial examples are included to facilitate users to understand how the toolkit must be used. A section dedicate to the Java library is proposed to help further code extensions.", "venue": "ArXiv", "authors": ["Daniele  Codecasa", "Fabio  Stella"], "year": 2014, "n_citations": 4}
{"id": 2430341, "s2_id": "95d1bbabf0099e54f0752a9d635039074f836297", "title": "Twofolds in C and C++", "abstract": "Here I propose C and C++ interfaces and experimental implementation for twofolds arithmetic. I introduce twofolds in my previous article entitled \"Twofold fast arithmetic\" for tracking floating-point inaccuracy. Testing shows, plain C enables high-performance computing with twofolds. C++ interface enables coding as easily as ordinary floating-point numbers. My goal is convincing you to try twofolds; I think assuring accuracy of math computations is worth its cost. Code and use examples available at my web site, references inside.", "venue": "ArXiv", "authors": ["Evgeny  Latkin"], "year": 2014, "n_citations": 0}
{"id": 2431232, "s2_id": "4c0fbbf00c471dcea388dbbc8ac573965d8741a0", "title": "Embedded Ensemble Propagation for Improving Performance, Portability, and Scalability of Uncertainty Quantification on Emerging Computational Architectures", "abstract": "Quantifying simulation uncertainties is a critical component of rigorous predictive simulation. A key component of this is forward propagation of uncertainties in simulation input data to output quantities of interest. Typical approaches involve repeated sampling of the simulation over the uncertain input data and can require numerous samples when accurately propagating uncertainties from large numbers of sources. Often simulation processes from sample to sample are similar, and much of the data generated from each sample evaluation could be reused. We explore a new method for implementing sampling methods that simultaneously propagates groups of samples together in an embedded fashion, which we call embedded ensemble propagation. We show how this approach takes advantage of properties of modern computer architectures to improve performance by enabling reuse between samples, reducing memory bandwidth requirements, improving memory access patterns, improving opportunities for fine-grained parallelization, ...", "venue": "SIAM J. Sci. Comput.", "authors": ["Eric T. Phipps", "M.  D'Elia", "H. Carter Edwards", "Mark  Hoemmen", "Jonathan J. Hu", "Sivasankaran  Rajamanickam"], "year": 2017, "n_citations": 12}
{"id": 2437378, "s2_id": "dfaea24115a79f46615c6ac79fe2636a9c7755dc", "title": "An openmath content dictionary for tensor concepts", "abstract": "We introduce a new OpenMath content dictionary named \"tensor1\" containing symbols for the expression of tensor formulas. These symbols support the expression of non-Cartesian coordinates and invariant, multilinear expressions in the context of coordinate transformations. While current OpenMath symbols support the expression of linear algebra formulas using matrices and vectors, we find that there is an underlying assumption of Cartesian, or standard, coordinates that makes the expression of general tensor formulas difficult, if not impossible. In introducing these new OpenMath symbols for the expression of tensor formulas, we attempt to maintain, as much as possible, consistency with prior OpenMath symbol definitions for linear algebra.", "venue": "AISC'10/MKM'10/Calculemus'10", "authors": ["Joseph B. Collins"], "year": 2010, "n_citations": 0}
{"id": 2438074, "s2_id": "3ae952da0e55329525071a70cf597805df98931d", "title": "Groebner bases in Java with applications in computer graphics", "abstract": "In this paper we present a Java implementation of the algorithm that computes Buchbereger's and reduced Groebner's basis step by step. The Java application enables graphical representation of the intersection of two surfaces in 3-dimensional space and determines conditions of existence and planarity of the intersection.", "venue": "ArXiv", "authors": ["Branko  Malesevic", "Ivana  Jovovic", "Milan  Campara"], "year": 2010, "n_citations": 0}
{"id": 2448652, "s2_id": "ba074caeb21ce3afbb1045b2186f2f50ed3ed47f", "title": "Fast Cubic Spline Interpolation", "abstract": "The Numerical Recipes series of books are a useful resource, but all the algorithms they contain cannot be used within open-source projects. In this paper we develop drop-in alternatives to the two algorithms they present for cubic spline interpolation, showing as much of our work as possible to allow for replication or criticsm. The output of the new algorithms is compared to the old, and found to be no different within the limits imposed by floating-point precision. Benchmarks of all these algorithms, plus variations which may run faster in certain instances, are performed. In general, all these algorithms have approximately the same execution time when interpolating curves with few control points on feature-rich Intel processors; as the number of control points increases or processor features are removed, the new algorithms become consistently faster than the old. Exceptions to that generalization are explored to create implementation guidelines, such as when to expect division to be faster than multiplication.", "venue": "ArXiv", "authors": ["Haysn  Hornbeck"], "year": 2020, "n_citations": 0}
{"id": 2452344, "s2_id": "a5d6ec621f365c01db9c46777a5b1cd931b9b879", "title": "Executable Set Theory and Arithmetic Encodings in Prolog", "abstract": "The paper is organized as a self-contained literate Prolog program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The complete Prolog code is available at this http URL . First, ranking and unranking functions for some \"mathematically elegant\" data types in the universe of Hereditarily Finite Sets with Urelements are provided, resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation. In the process, we uncover the surprising possibility of internally sharing isomorphic objects, independently of their language level types and meanings.", "venue": "ArXiv", "authors": ["Paul  Tarau"], "year": 2008, "n_citations": 0}
{"id": 2455393, "s2_id": "d6fa5d942d5277ff325298972a06b0ca4750fff5", "title": "Algorithms and software for projections onto intersections of convex and non-convex sets with applications to inverse problems", "abstract": "We propose algorithms and software for computing projections onto the intersection of multiple convex and non-convex constraint sets. The software package, called SetIntersectionProjection, is intended for the regularization of inverse problems in physical parameter estimation and image processing. The primary design criterion is working with multiple sets, which allows us to solve inverse problems with multiple pieces of prior knowledge. Our algorithms outperform the well known Dykstra's algorithm when individual sets are not easy to project onto because we exploit similarities between constraint sets. Other design choices that make the software fast and practical to use, include recently developed automatic selection methods for auxiliary algorithm parameters, fine and coarse grained parallelism, and a multilevel acceleration scheme. We provide implementation details and examples that show how the software can be used to regularize inverse problems. Results show that we benefit from working with all available prior information and are not limited to one or two regularizers because of algorithmic, computational, or hyper-parameter selection issues.", "venue": "ArXiv", "authors": ["Bas  Peters", "Felix J. Herrmann"], "year": 2019, "n_citations": 6}
{"id": 2455668, "s2_id": "791eb1a6c9149d7db23a5e79814fafd33520c6c5", "title": "The polymake XML File Format", "abstract": "We describe an XML file format for storing data from computations in algebra and geometry. We also present a formal specification based on a RELAX-NG schema.", "venue": "ICMS", "authors": ["Ewgenij  Gawrilow", "Simon  Hampe", "Michael  Joswig"], "year": 2016, "n_citations": 3}
{"id": 2461511, "s2_id": "56845cb8a22450ed0c752f2272f0586d119843db", "title": "Matrix-free construction of HSS representation using adaptive randomized sampling", "abstract": "We present new algorithms for the randomized construction of hierarchically semi-separable matrices, addressing several practical issues. The HSS construction algorithms use a partially matrix-free, adaptive randomized projection scheme to determine the maximum off-diagonal block rank. We develop both relative and absolute stopping criteria to determine the minimum dimension of the random projection matrix that is sufficient for the desired accuracy. Two strategies are discussed to adaptively enlarge the random sample matrix: repeated doubling of the number of random vectors, and iteratively incrementing the number of random vectors by a fixed number. The relative and absolute stopping criteria are based on probabilistic bounds for the Frobenius norm of the random projection of the Hankel blocks of the input matrix. We discuss parallel implementation and computation and communication cost of both variants. Parallel numerical results for a range of applications, including boundary element method matrices and quantum chemistry Toeplitz matrices, show the effectiveness, scalability and numerical robustness of the proposed algorithms.", "venue": "ArXiv", "authors": ["Christopher  Gorman", "Gustavo  Chavez", "Pieter  Ghysels", "Th\u00e9o  Mary", "Fran\u00e7ois-Henry  Rouet", "Xiaoye S. Li"], "year": 2018, "n_citations": 3}
{"id": 2461707, "s2_id": "32568a05fdf783291f33e6a6f55d5a806819f806", "title": "An Algorithm for the Optimization of Finite Element Integration Loops", "abstract": "We present an algorithm for the optimization of a class of finite-element integration loop nests. This algorithm, which exploits fundamental mathematical properties of finite-element operators, is proven to achieve a locally optimal operation count. In specified circumstances the optimum achieved is global. Extensive numerical experiments demonstrate significant performance improvements over the state of the art in finite-element code generation in almost all cases. This validates the effectiveness of the algorithm presented here and illustrates its limitations.", "venue": "ACM Trans. Math. Softw.", "authors": ["Fabio  Luporini", "David A. Ham", "Paul H. J. Kelly"], "year": 2017, "n_citations": 45}
{"id": 2465644, "s2_id": "cd9dd5cf8727068e6e9e6b3a7cafaa8ec3e4a907", "title": "Algorithms for discovering and proving theorems about permutation patterns", "abstract": "We present an algorithm, called BiSC, that describes the patterns avoided by a given set of permutations. It automatically conjectures the statements of known theorems such as the descriptions of stack-sortable (Knuth 1975) and West-2-stack-sortable permutations (West 1990), smooth (Lakshmibai and Sandhya 1990) and forest-like permutations (Bousquet-Melou and Butler 2007), and simsun permutations (Branden and Claesson 2011). The algorithm has also been used to discover new theorems and conjectures related to Young tableaux, Wilf-equivalences and sorting devices. We further give algorithms to prove a complete description of preimages of pattern classes under certain sorting devices. These generalize an algorithm of Claesson and Ulfarsson (2012) and allow us to prove a linear time algorithm for finding occurrences of the pattern 4312.", "venue": "ArXiv", "authors": ["Hjalti  Magnusson", "Henning  \u00dalfarsson"], "year": 2012, "n_citations": 6}
{"id": 2473617, "s2_id": "b294cba37a695ad0d4e9a2575725a2395c3d15d1", "title": "Function call overhead benchmarks with MATLAB, Octave, Python, Cython and C", "abstract": "We consider the overhead of function calls in the programming languages MATLAB/Octave, Python, Cython and C. In many applications a function has to be called very often inside a loop. One such application in numerical analysis is the finite element method where integrals have to be computed on each element in a loop. The called functions can often be evaluated efficiently but the function call itself may be time-consuming. We present a benchmark whose goal is to identify and quantify optimization potentials with respect to time consumption caused by function calls in the mentioned programming languages.", "venue": "ArXiv", "authors": ["Andr\u00e9  Gaul"], "year": 2012, "n_citations": 3}
{"id": 2483469, "s2_id": "5d1b4e94e2aee6aaa8f62766e2cd778d6ca2c6cc", "title": "Development of a Java Package for Matrix Programming", "abstract": "We had assembled a Java package, known as MatrixPak, of four classes for the purpose of numerical matrix computation. The classes are matrix, matrix_operations, StrToMatrix, and MatrixToStr; all of which are inherited from java.lang.Object class. Class matrix defines a matrix as a two-dimensional array of float types, and contains the following mathematical methods: transpose, adjoint, determinant, inverse, minor and cofactor. Class matrix_operations contains the following mathematical methods: matrix addition, matrix subtraction, matrix multiplication, and matrix exponential. Class StrToMatrix contains methods necessary to parse a string representation (for example, [[2 3 4]-[5 6 7]]) of a matrix into a matrix definition, whereas class MatrixToStr does the reverse.", "venue": "ArXiv", "authors": ["Ngee-Peng  Lim", "Maurice H. T. Ling", "Shawn Y. C. Lim", "Ji-Hee  Choi", "Henry B. K. Teo"], "year": 2003, "n_citations": 0}
{"id": 2490411, "s2_id": "fc4151c4eed01350931a2144ea9a9b0a6dd63149", "title": "LINPRO: linear inverse problem library for data contaminated by statistical noise", "abstract": "Abstract The library LINPRO which provides the solution to the linear inverse problem for data contaminated by a statistical noise is presented. The library makes use of two methods: Maximum Entropy Method and Singular Value Decomposition. As an example it has been applied to perform an analytic continuation of the imaginary time propagator obtained within the Quantum Monte Carlo method. Program summary Program title: LINPRO v1.0. Catalogue identifier: AEMT_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEMT_v1_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland. Licensing provisions: GNU Lesser General Public Licence. No. of lines in distributed program, including test data, etc.: 110620. No. of bytes in distributed program, including test data, etc.: 3208593. Distribution format: tar.gz. Programming language: C++. Computer: LINPRO library should compile on any computing system that has C++ compiler. Operating system: Linux or Unix. Classification: 4.9, 4.12, 4.13. External routines: OPT++: An Object-Oriented Nonlinear Optimization Library [1] (included in the distribution). Nature of problem: LINPRO library solves linear inverse problems with an arbitrary kernel and arbitrary external constraints imposed on the solution. Solution method: LINPRO library implements two complementary methods: Maximum Entropy Method and SVD method. Additional comments: Tested with compilers\u2013GNU Compiler g++, Intel Compiler icpc. Running time: Problem dependent, ranging from seconds to hours. Each of the examples takes less than a minute to run. References: [1] OPT++: An Object-Oriented Nonlinear Optimization Library, https://software.sandia.gov/opt++/ .", "venue": "Comput. Phys. Commun.", "authors": ["Piotr  Magierski", "Gabriel  Wlazlowski"], "year": 2012, "n_citations": 4}
{"id": 2500580, "s2_id": "5b9beb0f6a1a2500fedf0d5dceaf8b21462f6c75", "title": "An Object Oriented Parallel Finite Element Scheme for Computations of PDEs: Design and Implementation", "abstract": "Parallel finite element algorithms based on object-oriented concepts are presented. Moreover, the design and implementation of a data structure proposed are utilized in realizing a parallel geometric multigrid method. The ParFEMapper and the ParFECommunicator are the key components of the data structure in the proposed parallel scheme. These classes are constructed based on the type of finite elements (continuous or nonconforming or discontinuous) used. The proposed solver is compared with the open source direct solvers, MUMPS and PasTiX. Further, the performance of the parallel multigrid solver is analyzed up to 1080 processors. The solver shows a very good speedup up to 960 processors and the problem size has to be increased in order to maintain the good speedup when the number of processors are increased further. As a result, the parallel solver is able to handle large scale problems on massively parallel supercomputers. The proposed parallel finite element algorithms and multigrid solver are implemented in our in-house package ParMooN.", "venue": "2016 IEEE 23rd International Conference on High Performance Computing Workshops (HiPCW)", "authors": ["Sashikumaar  Ganesan", "Volker  John", "Gunar  Matthies", "Raviteja  Meesala", "Shamim  Abdus", "Ulrich  Wilbrandt"], "year": 2016, "n_citations": 25}
{"id": 2501311, "s2_id": "4976c5efbb1f7b41f5ab8f46da4cfb3c3a998593", "title": "Fast Strassen-based $A^t A$ Parallel Multiplication", "abstract": "Matrix multiplication $A^t A$ appears as intermediate operation during the solution of a wide set of problems. In this paper, we propose a new cache-oblivious algorithm for the $A^t A$ multiplication. Our algorithm, A$\\scriptstyle \\mathsf{T}$A, calls classical Strassen's algorithm as sub-routine, decreasing the computational cost %(expressed in number of performed products) of the conventional $A^t A$ multiplication to $\\frac{2}{7}n^{\\log_2 7}$. It works for generic rectangular matrices and exploits the peculiar symmetry of the resulting product matrix for sparing memory. We used the MPI paradigm to implement A$\\scriptstyle \\mathsf{T}$A in parallel, and we tested its performances on a small subset of nodes of the Galileo cluster. Experiments highlight good scalability and speed-up, also thanks to minimal number of exchanged messages in the designed communication system. Parallel overhead and inherently sequential time fraction are negligible in the tested configurations.", "venue": "ArXiv", "authors": ["Viviana  Arrigoni", "Annalisa  Massini"], "year": 2019, "n_citations": 1}
{"id": 2507026, "s2_id": "de49a3e58f713358cd636a5560f13031ce135eb7", "title": "Algorithms and Data Structures for Matrix-Free Finite Element Operators with MPI-Parallel Sparse Multi-Vectors", "abstract": "Traditional solution approaches for problems in quantum mechanics scale as O(M3), where M is the number of electrons. Various methods have been proposed to address this issue and obtain a linear scaling O(M). One promising formulation is the direct minimization of energy. Such methods take advantage of physical localization of the solution, allowing users to seek it in terms of non-orthogonal orbitals with local support. This work proposes a numerically efficient implementation of sparse parallel vectors within the open-source finite element library deal.II. The main algorithmic ingredient is the matrix-free evaluation of the Hamiltonian operator by cell-wise quadrature. Based on an a-priori chosen support for each vector, we develop algorithms and data structures to perform (i) matrix-free sparse matrix multivector products (SpMM), (ii) the projection of an operator onto a sparse sub-space (inner products), and (iii) post-multiplication of a sparse multivector with a square matrix. The node-level performance is analyzed using a roofline model. Our matrix-free implementation of finite element operators with sparse multivectors achieves a performance of 157 GFlop/s on an Intel Cascade Lake processor with 20 cores. Strong and weak scaling results are reported for a representative benchmark problem using quadratic and quartic finite element bases.", "venue": "ACM Trans. Parallel Comput.", "authors": ["Denis  Davydov", "Martin  Kronbichler"], "year": 2020, "n_citations": 1}
{"id": 2515778, "s2_id": "6b7c18d7e150c6fc2fc0f4ec10232862678a2b2d", "title": "A search for an optimal start system for numerical homotopy continuation", "abstract": "We use our recent implementation of a certified homotopy tracking algorithm to search for start systems that minimize the average complexity of finding all roots of a regular system of polynomial equations. While finding optimal start systems is a hard problem, our experiments show that it is possible to find start systems that deliver better average complexity than the ones that are commonly used in the existing homotopy continuation software.", "venue": "ArXiv", "authors": ["Anton  Leykin"], "year": 2011, "n_citations": 0}
{"id": 2518030, "s2_id": "84ef48865098059c930982f9cb32f2dcf9fcfa40", "title": "pySOT and POAP: An event-driven asynchronous framework for surrogate optimization", "abstract": "This paper describes Plumbing for Optimization with Asynchronous Parallelism (POAP) and the Python Surrogate Optimization Toolbox (pySOT). POAP is an event-driven framework for building and combining asynchronous optimization strategies, designed for global optimization of expensive functions where concurrent function evaluations are useful. POAP consists of three components: a worker pool capable of function evaluations, strategies to propose evaluations or other actions, and a controller that mediates the interaction between the workers and strategies. pySOT is a collection of synchronous and asynchronous surrogate optimization strategies, implemented in the POAP framework. We support the stochastic RBF method by Regis and Shoemaker along with various extensions of this method, and a general surrogate optimization strategy that covers most Bayesian optimization methods. We have implemented many different surrogate models, experimental designs, acquisition functions, and a large set of test problems. We make an extensive comparison between synchronous and asynchronous parallelism and find that the advantage of asynchronous computation increases as the variance of the evaluation time or number of processors increases. We observe a close to linear speed-up with 4, 8, and 16 processors in both the synchronous and asynchronous setting.", "venue": "ArXiv", "authors": ["David  Eriksson", "David  Bindel", "Christine A. Shoemaker"], "year": 2019, "n_citations": 23}
{"id": 2524111, "s2_id": "bcefe8448a7f93019b91a8aa41fa908adf7755fe", "title": "From NoSQL Accumulo to NewSQL Graphulo: Design and utility of graph algorithms inside a BigTable database", "abstract": "Google BigTable's scale-out design for distributed key-value storage inspired a generation of NoSQL databases. Recently the NewSQL paradigm emerged in response to analytic workloads that demand distributed computation local to data storage. Many such analytics take the form of graph algorithms, a trend that motivated the GraphBLAS initiative to standardize a set of matrix math kernels for building graph algorithms. In this article we show how it is possible to implement the GraphBLAS kernels in a BigTable database by presenting the design of Graphulo, a library for executing graph algorithms inside the Apache Accumulo database. We detail the Graphulo implementation of two graph algorithms and conduct experiments comparing their performance to two main-memory matrix math systems. Our results shed insight into the conditions that determine when executing a graph algorithm is faster inside a database versus an external system-in short, that memory requirements and relative I/O are critical factors.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Dylan  Hutchison", "Jeremy  Kepner", "Vijay  Gadepally", "Bill  Howe"], "year": 2016, "n_citations": 13}
{"id": 2530367, "s2_id": "9f3f8b252034c51573495bfc5648f9565094145d", "title": "Real Options for Project Schedules (ROPS)", "abstract": "Real Options for Project Schedules (ROPS) has three recursive sampling/optimization shells. An outer Adaptive Simulated Annealing (ASA) optimization shell optimizes parameters of strategic Plans containing multiple Projects containing ordered Tasks. A middle shell samples probability distributions of durations of Tasks. An inner shell samples probability distributions of costs of Tasks. PATHTREE is used to develop options on schedules. Algorithms used for Trading in Risk Dimensions (TRD) are applied to develop a relative risk analysis among projects.", "venue": "ArXiv", "authors": ["Lester  Ingber"], "year": 2007, "n_citations": 9}
{"id": 2530959, "s2_id": "0f001f4548c246d2f679b9e9f96e82a852f5c1be", "title": "Matrix Methods for Solving Algebraic Systems", "abstract": "The problem of computing all common zeros of a system of polynomials is of fundamental importance in a wide variety of scientific and engineering applications. This article surveys efficient methods based on the sparse resultant for computing all isolated solutions of an arbitrary system of n polynomials in n unknowns. In particular, we construct matrix formulae which yield nontrivial multiples of the resultant thus reducing root-finding to the eigendecomposition of a square matrix.", "venue": "Symbolic Algebraic Methods and Verification Methods", "authors": ["Ioannis Z. Emiris"], "year": 2001, "n_citations": 5}
{"id": 2531750, "s2_id": "3e335d6f76563af3e582276d5dfa56b0462e36ce", "title": "Automating embedded analysis capabilities and managing software complexity in multiphysics simulation, Part I: Template-based generic programming", "abstract": "An approach for incorporating embedded simulation and analysis capabilities in complex simulation codes through template-based generic programming is presented. This approach relies on templating and operator overloading within the C++ language to transform a given calculation into one that can compute a variety of additional quantities that are necessary for many state-of-the-art simulation and analysis algorithms. An approach for incorporating these ideas into complex simulation codes through general graph-based assembly is also presented. These ideas have been implemented within a set of packages in the Trilinos framework and are demonstrated on a simple problem from chemical engineering.", "venue": "Sci. Program.", "authors": ["Roger P. Pawlowski", "Eric T. Phipps", "Andrew G. Salinger"], "year": 2012, "n_citations": 15}
{"id": 2538975, "s2_id": "b99774ef4122b7c03c2dd76a7d9bcc4f2fe92291", "title": "Certification of bounds on expressions involving rounded operators", "abstract": "Gappa is a tool designed to formally verify the correctness of numerical software and hardware. It uses interval arithmetic and forward error analysis to bound mathematical expressions that involve rounded as well as exact operators. It then generates a theorem and its proof for each verified enclosure. This proof can be automatically checked with a proof assistant, such as Coq or HOL Light. It relies on a large companion library of facts that we have developed. This Coq library provides theorems dealing with addition, multiplication, division, and square root, for both fixed- and floating-point arithmetics. Gappa uses multiple-precision dyadic fractions for the endpoints of intervals and performs forward error analysis on rounded operators when necessary. When asked, Gappa reports the best bounds it is able to reach for a given expression in a given context. This feature can be used to identify where the set of facts and automatic techniques implemented in Gappa becomes insufficient. Gappa handles seamlessly additional properties expressed as interval properties or rewriting rules in order to establish more intricate bounds. Recent work showed that Gappa is suited to discharge proof obligations generated for small pieces of software. They may be produced by third-party tools and the first applications of Gappa use proof obligations written by designers or obtained from traces of execution.", "venue": "TOMS", "authors": ["Marc  Daumas", "Guillaume  Melquiond"], "year": 2010, "n_citations": 100}
{"id": 2539176, "s2_id": "635ec717dd165e8f3f8998db5bd8662de30cdaee", "title": "A Rewriting System for Convex Optimization Problems", "abstract": "We describe a modular rewriting system for translating optimization problems written in a domain-specific language to forms compatible with low-level solver interfaces. Translation is facilitated by reductions, which accept a category of problems and transform instances of that category to equivalent instances of another category. Our system proceeds in two key phases: analysis, in which we attempt to find a suitable solver for a supplied problem, and canonicalization, in which we rewrite the problem in the selected solver's standard form. We implement the described system in version 1.0 of CVXPY, a domain-specific language for mathematical and especially convex optimization. By treating reductions as first-class objects, our method makes it easy to match problems to solvers well-suited for them and to support solvers with a wide variety of standard forms.", "venue": "ArXiv", "authors": ["Akshay  Agrawal", "Robin  Verschueren", "Steven  Diamond", "Stephen  Boyd"], "year": 2017, "n_citations": 249}
{"id": 2544261, "s2_id": "95ad3f771a0ec22a3c86eca351efbbb52891dc5f", "title": "xSDK Foundations: Toward an Extreme-scale Scientific Software Development Kit", "abstract": "Extreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability. However, without coordination, many libraries cannot be easily composed. Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly. The Extreme-scale Scientific Software Development Kit xSDK defines community policies to improve code quality and compatibility across independently developed packages hypre, PETSc, SuperLU, Trilinos, and Alquimia and provides a foundation for addressing broader issues in software interoperability, performance portability, and sustainability. The xSDK provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability.", "venue": "Supercomput. Front. Innov.", "authors": ["Roscoe A. Bartlett", "Irina  Demeshko", "Todd  Gamblin", "Glenn  Hammond", "Michael A. Heroux", "Jeffrey  Johnson", "Alicia  Klinvex", "Xiaoye  Li", "Lois C. McInnes", "J. David Moulton", "Daniel  Osei-Kuffuor", "Jason  Sarich", "Barry  Smith", "James M. Willenbring", "Ulrike Meier Yang"], "year": 2017, "n_citations": 13}
{"id": 2552257, "s2_id": "fc51bd7a475340aad5b72f9c2523cc8c743bc3d3", "title": "A collection of challenging optimization problems in science, engineering and economics", "abstract": "Function optimization and finding simultaneous solutions of a system of nonlinear equations (SNE) are two closely related and important optimization problems. However, unlike in the case of function optimization in which one is required to find the global minimum and sometimes local minima, a database of challenging SNEs where one is required to find stationary points (extrama and saddle points) is not readily available. In this article, we initiate building such a database of important SNE (which also includes related function optimization problems), arising from Science, Engineering and Economics. After providing a short review of the most commonly used mathematical and computational approaches to find solutions of such systems, we provide a preliminary list of challenging problems by writing the Mathematical formulation down, briefly explaning the origin and importance of the problem and giving a short account on the currently known results, for each of the problems. We anticipate that this database will not only help benchmarking novel numerical methods for solving SNEs and function optimization problems but also will help advancing the corresponding research areas.", "venue": "2015 IEEE Congress on Evolutionary Computation (CEC)", "authors": ["Dhagash  Mehta", "Crina  Grosan"], "year": 2015, "n_citations": 9}
{"id": 2552324, "s2_id": "a0119f9711468b84b153c56781e01f7bab02e383", "title": "The Peano Software\u2014Parallel, Automaton-based, Dynamically Adaptive Grid Traversals", "abstract": "We discuss the design decisions, design alternatives, and rationale behind the third generation of Peano, a framework for dynamically adaptive Cartesian meshes derived from spacetrees. Peano ties the mesh traversal to the mesh storage and supports only one element-wise traversal order resulting from space-filling curves. The user is not free to choose a traversal order herself. The traversal can exploit regular grid subregions and shared memory as well as distributed memory systems with almost no modifications to a serial application code. We formalize the software design by means of two interacting automata\u2014one automaton for the multiscale grid traversal and one for the application-specific algorithmic steps. This yields a callback-based programming paradigm. We further sketch the supported application types and the two data storage schemes realized before we detail high-performance computing aspects and lessons learned. Special emphasis is put on observations regarding the used programming idioms and algorithmic concepts. This transforms our report from a \u201cone way to implement things\u201d code description into a generic discussion and summary of some alternatives, rationale, and design decisions to be made for any tree-based adaptive mesh refinement software.", "venue": "ACM Trans. Math. Softw.", "authors": ["Tobias  Weinzierl"], "year": 2019, "n_citations": 41}
{"id": 2555763, "s2_id": "9cee9a2d52846acb1bc79f753daf803135babe7f", "title": "Computer-Assisted Program Reasoning Based on a Relational Semantics of Programs", "abstract": "We present an approach to program reasoning which inserts between a program and its verification conditions an additional layer, the denotation of the program expressed in a declarative form. The program is first translated into its denotation from which subsequently the verification conditions are generated. However, even before (and independently of) any verification attempt, one may investigate the denotation itself to get insight into the \"semantic essence\" of the program, in particular to see whether the denotation indeed gives reason to believe that the program has the expected behavior. Errors in the program and in the meta-information may thus be detected and fixed prior to actually performing the formal verification. More concretely, following the relational approach to program semantics, we model the effect of a program as a binary relation on program states. A formal calculus is devised to derive from a program a logic formula that describes this relation and is subject for inspection and manipulation. We have implemented this idea in a comprehensive form in the RISC ProgramExplorer, a new program reasoning environment for educational purposes which encompasses the previously developed RISC ProofNavigator as an interactive proving assistant.", "venue": "ThEdu", "authors": ["Wolfgang  Schreiner"], "year": 2011, "n_citations": 7}
{"id": 2562572, "s2_id": "0ae8839e481ece71a8dc15fd07372c672524c19c", "title": "Faster remainder by direct computation: Applications to compilers and software libraries", "abstract": "On common processors, integer multiplication is many times faster than integer division. Dividing a numerator n by a divisor d is mathematically equivalent to multiplication by the inverse of the divisor (n/d=n\u22171/d). If the divisor is known in advance, or if repeated integer divisions will be performed with the same divisor, it can be beneficial to substitute a less costly multiplication for an expensive division. Currently, the remainder of the division by a constant is computed from the quotient by a multiplication and a subtraction. However, if just the remainder is desired and the quotient is unneeded, this may be suboptimal. We present a generally applicable algorithm to compute the remainder more directly. Specifically, we use the fractional portion of the product of the numerator and the inverse of the divisor. On this basis, we also present a new and simpler divisibility algorithm to detect nonzero remainders. We also derive new tight bounds on the precision required when representing the inverse of the divisor. Furthermore, we present simple C implementations that beat the optimized code produced by state\u2010of\u2010the\u2010art C compilers on recent x64 processors (eg, Intel Skylake and AMD Ryzen), sometimes by more than 25%. On all tested platforms, including 64\u2010bit ARM and POWER8, our divisibility test functions are faster than state\u2010of\u2010the\u2010art Granlund\u2010Montgomery divisibility test functions, sometimes by more than 50%.", "venue": "Softw. Pract. Exp.", "authors": ["Daniel  Lemire", "Owen  Kaser", "Nathan  Kurz"], "year": 2019, "n_citations": 8}
{"id": 2570942, "s2_id": "38b307d9797e910c5e070800bb5198b373594c43", "title": "The generating of Fractal Images Using MathCAD Program", "abstract": "This paper presents the graphic representation in the z-plane of the first three iterations of the algorithm that generates the Sierpinski Gasket. It analyzes the influence of the f(z) map when we represent fractal images.", "venue": "ArXiv", "authors": ["Laura  Stefan"], "year": 2009, "n_citations": 0}
{"id": 2571911, "s2_id": "faeba47b055b6eb9663ddf6af82e3334b37fe23b", "title": "Implementation of a unimodularity test", "abstract": "This paper describes implementation and computational results of a polynomial test of total unimodularity. The test is a simplified version of a prior method. The program also decides two related unimodularity properties. The software is available free of charge in source code form under the Boost Software License.", "venue": "Math. Program. Comput.", "authors": ["Matthias  Walter", "Klaus  Truemper"], "year": 2013, "n_citations": 19}
{"id": 2572862, "s2_id": "d6c4c76076efecb15655274adc648af8a445ed3a", "title": "CSR5: An Efficient Storage Format for Cross-Platform Sparse Matrix-Vector Multiplication", "abstract": "Sparse matrix-vector multiplication (SpMV) is a fundamental building block for numerous applications. In this paper, we propose CSR5 (Compressed Sparse Row 5), a new storage format, which offers high-throughput SpMV on various platforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is insensitive to the sparsity structure of the input matrix. Thus the single format can support an SpMV algorithm that is efficient both for regular matrices and for irregular matrices. Furthermore, we show that the overhead of the format conversion from the CSR to the CSR5 can be as low as the cost of a few SpMV operations. We compare the CSR5-based SpMV algorithm with 11 state-of-the-art formats and algorithms on four mainstream processors using 14 regular and 10 irregular matrices as a benchmark suite. For the 14 regular matrices in the suite, we achieve comparable or better performance over the previous work. For the 10 irregular matrices, the CSR5 obtains average performance improvement of 17.6%, 28.5%, 173.0% and 293.3% (up to 213.3%, 153.6%, 405.1% and 943.3%) over the best existing work on dual-socket Intel CPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For real-world applications such as a solver with only tens of iterations, the CSR5 format can be more practical because of its low-overhead for format conversion.", "venue": "ICS", "authors": ["Weifeng  Liu", "Brian  Vinter"], "year": 2015, "n_citations": 183}
{"id": 2573373, "s2_id": "df79f1fa205041d6d4774d3297da995a103c683f", "title": "High-Order Finite-differences on multi-threaded architectures using OCCA", "abstract": "High-order finite-difference methods are commonly used in wave propagator for industrial subsurface imaging algorithms. Computational aspects of the reduced linear elastic vertical transversely isotropic propagator are considered. Thread parallel algorithms suitable for implementing this propagator on multi-core and many-core processing devices are introduced. Portability is addressed through the use of the OCCA runtime programming interface. Finally, performance results are shown for various architectures on a representative synthetic test case.", "venue": "ArXiv", "authors": ["David S. Medina", "Amik  St.-Cyr", "Timothy C. Warburton"], "year": 2014, "n_citations": 6}
{"id": 2578384, "s2_id": "13ef15465b2d2a8cc4a9eb3caddc3002dfa62d80", "title": "A novel code generation methodology for block diagram modeler and simulators Scicos and VSS", "abstract": "Block operations during simulation in Scicos and VSS environments can naturally be described as Nsp functions. But the direct use of Nsp functions for simulation leads to poor performance since the Nsp language is interpreted, not compiled. The methodology presented in this paper is used to develop a tool for generating efficient compilable code, such as C and ADA, for Scicos and VSS models from these block Nsp functions. Operator overloading and partial evaluation are the key elements of this novel approach. This methodology may be used in other simulation environments such as Matlab/Simulink.", "venue": "ArXiv", "authors": ["Jean-Philippe  Chancelier", "Ramine  Nikoukhah"], "year": 2015, "n_citations": 1}
{"id": 2580686, "s2_id": "2b3fb600e314a86c33d7ade57a0ce4ddfaf6c125", "title": "GenASiSBasics: Object-oriented utilitarian functionality for large-scale physics simulations (Version 3)", "abstract": "GenASiS Basics provides Fortran 2003 classes furnishing extensible objectoriented utilitarian functionality for large-scale physics simulations on distributed memory supercomputers. This functionality includes physical units and constants; display to the screen or standard output device; message passing; I/O to disk; and runtime parameter management and usage statistics. This revision\u2014Version 3 of Basics\u2014includes a significant name change, some minor additions to functionality, and a major addition to functionality: infrastructure facilitating the offloading of computational kernels to devices such as GPUs.", "venue": "Comput. Phys. Commun.", "authors": ["Reuben D. Budiardja", "Christian Y. Cardall"], "year": 2019, "n_citations": 2}
{"id": 2583936, "s2_id": "e2195f71e62ea55d87c8265c64ec670ffafdf0c9", "title": "Index handling and assign optimization for Algorithmic Differentiation reuse index managers", "abstract": "For operator overloading Algorithmic Differentiation tools, the identification of primal variables and adjoint variables is usually done via indices. Two common schemes exist for their management and distribution. The linear approach is easy to implement and supports memory optimization with respect to copy statements. On the other hand, the reuse approach requires more implementation effort but results in much smaller adjoint vectors, which are more suitable for the vector mode of Algorithmic Differentiation. In this paper, we present both approaches, how to implement them, and discuss their advantages, disadvantages and properties of the resulting Algorithmic Differentiation type. In addition, a new management scheme is presented which supports copy optimizations and the reuse of indices, thus combining the advantages of the other two. The implementations of all three schemes are compared on a simple synthetic example and on a real world example using the computational fluid dynamics solver in SU2.", "venue": "ArXiv", "authors": ["Max  Sagebaum", "Johannes  Bl\u00fchdorn", "Nicolas R. Gauger"], "year": 2020, "n_citations": 1}
{"id": 2584870, "s2_id": "59aa5105132d521ad376dae525ae44f740adb037", "title": "High-Performance Partial Spectrum Computation for Symmetric eigenvalue problems and the SVD", "abstract": "Current dense symmetric eigenvalue (EIG) and singular value decomposition (SVD) implementations may suffer from the lack of concurrency during the reduction step toward the corresponding condensed matrix forms, i.e., tridiagonal and bidiagonal, respectively. This performance bottleneck is typical for the two-sided transformations due to the Level-2 BLAS calls. These memory-bound functions are inherently limited by the speed of the bus bandwidth and may already saturate the memory bandwidth with only a small number of processes. Therefore, the current state-of-the-art EIG and SVD implementations may achieve only a small fraction of the system\u2019s sustained peak performance. The QR-based Dynamically Weighted Halley (QDWH) algorithm may be used as a pre-processing step toward the EIG and SVD solvers, while mitigating the aforementioned bottleneck. QDWH-EIG and QDWH-SVD expose more parallelism, while relying on compute-bound matrix operations. Both run closer to the sustained peak performance of the system, but at the expense of performing more floating-point operations than the standard EIG and SVD algorithms. These algorithms are designed to compute the full SVD and eigendecomposition. In this paper, we introduce a new QDWH-based solver for computing the partial spectrum for EIG (QDWHpartial-EIG) and SVD (QDWHpartial-SVD) problems. By optimizing the rational function underlying the algorithms only in the desired part of the spectrum, QDWHpartial-EIG and QDWHpartial-SVD algorithms efficiently compute a fraction (say 1\u221220%) of the eigenspectrum as well as the most significant singular values/vectors, respectively. We develop high-performance implementations of QDWHpartial-EIG and QDWHpartial-SVD on distributed-memory manycore systems and demonstrate their numerical robustness. We perform a benchmarking campaign against their counterparts from the state-of-theart numerical libraries (i.e., ScaLAPACK, ELPA, KSVD) across various matrix sizes using up to 36K MPI processes. Experimental results show performance speedups for QDWHpartial-SVD up to 6X and 2X against PDGESVD from ScaLAPACK and KSVD, respectively. QDWHpartial-EIG outperforms PDSYEVD from ScaLAPACK up to 3.5X but remains slower compared to ELPA. QDWHpartial-EIG achieves, however, a better occupancy of the underlying hardware by extracting higher sustained peak performance than ELPA, which is critical moving forward with accelerator-based supercomputers.", "venue": "ArXiv", "authors": ["D.  Keyes", "H.  Ltaief", "Y.  Nakatsukasa", "D.  Sukkari"], "year": 2021, "n_citations": 0}
{"id": 2586370, "s2_id": "52718b4efcfe9a5ae88540c6ec9da9fe3f580839", "title": "Data sets of very large linear feasibility problems solved by projection methods", "abstract": "We give a link to a page on the Web on which we deposited a set of eight huge Linear Programming (LP) problems for IntensityModulated Proton Therapy (IMPT) treatment planning. These huge LP problems were employed in our recent research and we were asked to make them public.", "venue": "ArXiv", "authors": ["Wei  Chen"], "year": 2011, "n_citations": 0}
{"id": 2586522, "s2_id": "94f52780f4db98c3549a0a800c456177d0e9033a", "title": "SplineLib: A Modern Multi-Purpose C++ Spline Library", "abstract": "This paper provides the description of a novel, multi-purpose spline library. In accordance with the increasingly diverse modes of usage of splines, it is multi-purpose in the sense that it supports geometry representation, finite element analysis, and optimization. The library features reading and writing for various file formats and a wide range of spline manipulation algorithms. Further, a new efficient and objective-oriented algorithm for B-spline basis function evaluation is included. All features are available by a spline-type independent interface. The library is written in modern C++ with CMake as build system. This enables it for usage in typical scientific applications. It is provided as open-source library.", "venue": "Adv. Eng. Softw.", "authors": ["Markus  Frings", "Norbert  Hosters", "Corinna  M\u00fcller", "Max  Spahn", "Christoph  Susen", "Konstantin  Key", "Stefanie  Elgeti"], "year": 2020, "n_citations": 0}
{"id": 2589594, "s2_id": "488e753f3d90fb4a525347430583086dd38f9ceb", "title": "GPU Methodologies for Numerical Partial Differential Equations", "abstract": "In this thesis we develop techniques to efficiently solve numerical Partial Differential Equations (PDEs) using Graphical Processing Units (GPUs). Focus is put on both performance and re--usability of the methods developed, to this end a library, cuSten, for applying finite--difference stencils to numerical grids is presented herein. On top of this various batched tridiagonal and pentadiagonal matrix solvers are discussed. These have been benchmarked against the current state of the art and shown to improve performance in the solution of numerical PDEs. A variety of other benchmarks and use cases for the GPU methodologies are presented using the Cahn--Hilliard equation as a core example, but it is emphasised the methods are completely general. Finally through the application of the GPU methodologies to the Cahn--Hilliard equation new results are presented on the growth rates of the coarsened domains. In particular a statistical model is built up using batches of simulations run on GPUs from which the growth rates are extracted, it is shown that in a finite domain that the traditionally presented results of 1/3 scaling is in fact a distribution around this value. This result is discussed in conjunction with modelling via a stochastic PDE and sheds new light on the behaviour of the Cahn--Hilliard equation in finite domains.", "venue": "ArXiv", "authors": ["Andrew  Gloster"], "year": 2021, "n_citations": 0}
{"id": 2592884, "s2_id": "46c96da95adaa06a55cb12778e4626c283a75d96", "title": "dMath: A Scalable Linear Algebra and Math Library for Heterogeneous GP-GPU Architectures", "abstract": "A new scalable parallel math library, dMath, is presented in this paper that demonstrates leading scaling when using intranode, or internode, hybrid-parallelism for deep-learning. dMath provides easy-to-use distributed base primitives and a variety of domain-specific algorithms. These include matrix multiplication, convolutions, and others allowing for rapid development of highly scalable applications, including Deep Neural Networks (DNN), whereas previously one was restricted to libraries that provided effective primitives for only a single GPU, like Nvidia cublas and cudnn or DNN primitives from Nervana neon framework. Development of HPC software is difficult, labor-intensive work, requiring a unique skill set. dMath allows a wide range of developers to utilize parallel and distributed hardware easily. One contribution of this approach is that data is stored persistently on the GPU hardware, avoiding costly transfers between host and device. Advanced memory management techniques are utilized, including caching of transferred data and memory reuse through pooling. A key contribution of dMath is that it delivers performance, portability, and productivity to its specific domain of support. It enables algorithm and application programmers to quickly solve problems without managing the significant complexity associated with multi-level parallelism.", "venue": "ArXiv", "authors": ["Steven  Eliuk", "Cameron  Upright", "Anthony  Skjellum"], "year": 2016, "n_citations": 4}
{"id": 2597804, "s2_id": "75a26301e4ed7f89076ad4e97e90ecd35f4b6b63", "title": "Quantomatic: A proof assistant for diagrammatic reasoning", "abstract": "Monoidal algebraic structures consist of operations that can have multiple outputs as well as multiple inputs, which have applications in many areas including categorical algebra, programming language semantics, representation theory, algebraic quantum information, and quantum groups. String diagrams provide a convenient graphical syntax for reasoning formally about such structures, while avoiding many of the technical challenges of a term-based approach. Quantomatic is a tool that supports the (semi-)automatic construction of equational proofs using string diagrams. We briefly outline the theoretical basis of Quantomatic's rewriting engine, then give an overview of the core features and architecture and give a simple example project that computes normal forms for commutative bialgebras.", "venue": "CADE", "authors": ["Aleks  Kissinger", "Vladimir  Zamdzhiev"], "year": 2015, "n_citations": 80}
{"id": 2597865, "s2_id": "71995cef1c2a145556f3aa379f7ada2c5bbe92e3", "title": "LightSeq: Accelerated Training for Transformer-based Models on GPUs", "abstract": "Transformer-based models have proven to be powerful in many natural language, computer vision, and speech recognition applications. It is expensive to train these types of models due to unfixed input length, complex computation, and large numbers of parameters. Existing systems either only focus on efficient inference or optimize only BERT-like encoder models. In this paper, we present LightSeq, a system for efficient training of Transformer-based models on GPUs. We propose a series of GPU optimization techniques tailored to computation flow and memory access patterns of neural layers in Transformers. LightSeq supports a variety of network architectures, including BERT (encoder-only), GPT (decoder-only), and Transformer (encoder-decoder). Our experiments on GPUs with varying models and datasets show that LightSeq is 1.4-3.5x faster than previous systems. In particular, it gains 308% training speedup compared with existing systems on a large public machine translation benchmark (WMT14 English-German).", "venue": "ArXiv", "authors": ["Xiaohui  Wang", "Ying  Xiong", "Xian  Qian", "Yang  Wei", "Lei  Li", "Mingxuan  Wang"], "year": 2021, "n_citations": 0}
{"id": 2598521, "s2_id": "45f6d51bd5eb3fd4b024c9ffa42da7e906a20e65", "title": "On the efficient parallel computing of long term reliable trajectories for the Lorenz system", "abstract": "In this work we propose an efficient parallelization of multiple-precision Taylor series method with variable stepsize and fixed order. For given level of accuracy the optimal variable stepsize determines higher order of the method than in the case of optimal fixed stepsize. Although the used order of the method is greater then that in the case of fixed stepsize, and hence the computational work per step is greater, the reduced number of steps gives less overall work. Also the greater order of the method is beneficial in the sense that it increases the parallel efficiency. As a model problem we use the paradigmatic Lorenz system. With 256 CPU cores in Nestum cluster, Sofia, Bulgaria, we succeed to obtain a correct reference solution in the rather long time interval [0,11000]. To get this solution we performed two large computations: one computation with 4566 decimal digits of precision and 5240-th order method, and second computation for verification with 4778 decimal digits of precision and 5490-th order method.", "venue": "ArXiv", "authors": ["I.  Hristov", "R.  Hristova", "S.  Dimova", "P.  Armyanov", "N.  Shegunov", "I.  Puzynin", "T.  Puzynina", "Z.  Sharipov", "Z.  Tukhliev"], "year": 2021, "n_citations": 0}
{"id": 2599222, "s2_id": "7b276606027968af2695058fdc267d054f2d7b1a", "title": "LEoPart: a particle library for FEniCS", "abstract": "Abstract This paper introduces LE o P art , an add-on for the open-source finite element software library FE ni CS\u00a0to seamlessly integrate Lagrangian particle functionality with (Eulerian) mesh-based finite element (FE) approaches. LE o P art - which is so much as to say: \u2018Lagrangian\u2013Eulerian on Particles\u2019 - contains tools for efficient, accurate and scalable advection of Lagrangian particles on simplicial meshes. In addition, LE o P art \u00a0comes with several projection operators for exchanging information between the scattered particles and the mesh and vice versa. These projection operators are based on a variational framework, which allows extension to high-order accuracy. In particular, by implementing a dedicated PDE-constrained particle\u2013mesh projection operator, LE o P art \u00a0provides all the tools for diffusion-free advection, while simultaneously achieving optimal convergence and ensuring conservation of the projected particle quantities on the underlying mesh. A range of numerical examples that are prototypical to passive and active tracer methods highlight the properties and the parallel performance of the different tools in LE o P art . Lastly, future developments are identified. The source code for LE o P art \u00a0is actively maintained and available under an open-source license at https://bitbucket.org/jakob_maljaars/leopart .", "venue": "Comput. Math. Appl.", "authors": ["Jakob M. Maljaars", "Chris N. Richardson", "Nathan  Sime"], "year": 2021, "n_citations": 5}
{"id": 2600845, "s2_id": "f44bc4867d01e080a6c22aaef069aa5f466adb58", "title": "Achieving Efficient Realization of Kalman Filter on CGRA through Algorithm-Architecture Co-design", "abstract": "In this paper, we present efficient realization of Kalman Filter (KF) that can achieve up to 65% of the theoretical peak performance of underlying architecture platform. KF is realized using Modified Faddeeva Algorithm (MFA) as a basic building block due to its versatility and REDEFINE Coarse Grained Reconfigurable Architecture (CGRA) is used as a platform for experiments since REDEFINE is capable of supporting realization of a set algorithmic compute structures at run-time on a Reconfigurable Data-path (RDP). We perform several hardware and software based optimizations in the realization of KF to achieve 116% improvement in terms of Gflops over the first realization of KF. Overall, with the presented approach for KF, 4-105x performance improvement in terms of Gflops/watt over several academically and commercially available realizations of KF is attained. In REDEFINE, we show that our implementation is scalable and the performance attained is commensurate with the underlying hardware resources", "venue": "ARC", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan"], "year": 2018, "n_citations": 3}
{"id": 2602025, "s2_id": "d2ca463ddfd7832e870b6e2cbe9470d5ec72ccdb", "title": "A new object-oriented framework for solving multiphysics problems via combination of different numerical methods", "abstract": "Many interesting phenomena are characterized by the complex interaction of different physical processes, each often best modeled numerically via a specific approach. In this paper, we present the design and implementation of an object-oriented framework for performing multiphysics simulations that allows for the monolithic coupling of different numerical schemes. In contrast, most of the currently available simulation tools are tailored towards a specific numerical model, so that one must resort to coupling different codes externally based on operator splitting. The current framework has been developed following the C++11 standard, and its main aim is to provide an environment that affords enough flexibility for developers to implement complex models while at the same time giving end users a maximum amount of control over finer details of the simulation without having to write additional code. The main challenges towards realizing these objectives are discussed in the paper, together with the manner in which they are addressed. Along with core objects representing the framework skeleton, we present the various polymorphic classes that may be utilized by developers to implement new formulations, material models and solution algorithms. The code architecture is designed to allow achievement of the aforementioned functionalities with a minimum level of inheritance in order to improve the learning curve for programmers who are not acquainted with the software. Key capabilities of the framework are demonstrated via the solution of numerical examples dealing on composite torsion, Biot poroelasticity (featuring a combined finite element-finite volume formulation), and brittle crack propagation using a phase-field approach.", "venue": "ArXiv", "authors": ["Juan Michael Sargado"], "year": 2019, "n_citations": 2}
{"id": 2606590, "s2_id": "62aef78242506413a1ac2da504153f09b722661b", "title": "Scalable linear solvers for sparse linear systems from large-scale numerical simulations", "abstract": "This paper presents our work on designing scalable linear solvers for large-scale reservoir simulations. The main objective is to support implementation of parallel reservoir simulators on distributed-memory parallel systems, where MPI (Message Passing Interface) is employed for communications among computation nodes. Distributed matrix and vector modules are designed, which are the base of our parallel linear systems. Commonly-used Krylov subspace linear solvers are implemented, including the restarted GMRES method, the LGMRES method, and the BiCGSTAB method. It also has an interface to a parallel algebraic multigrid solver, BoomerAMG from HYPRE. Parallel general-purpose preconditioners and special preconditioners for reservoir simulations are also developed. The numerical experiments show that our linear solvers have excellent scalability using thousands of CPU cores.", "venue": "ArXiv", "authors": ["Hui  Liu", "Zhangxin  Chen"], "year": 2017, "n_citations": 1}
{"id": 2616716, "s2_id": "90c4e30e94f239633d569dd6532d0eb109591562", "title": "Sparse Approximate Multifrontal Factorization with Butterfly Compression for High Frequency Wave Equations", "abstract": "Author(s): Liu, Yang; Ghysels, Pieter; Claus, Lisa; Li, Xiaoye Sherry | Abstract: We present a fast and approximate multifrontal solver for large-scale sparse linear systems arising from finite-difference, finite-volume or finite-element discretization of high-frequency wave equations. The proposed solver leverages the butterfly algorithm and its hierarchical matrix extension for compressing and factorizing large frontal matrices via graph-distance guided entry evaluation or randomized matrix-vector multiplication-based schemes. Complexity analysis and numerical experiments demonstrate $\\mathcal{O}(N\\log^2 N)$ computation and $\\mathcal{O}(N)$ memory complexity when applied to an $N\\times N$ sparse system arising from 3D high-frequency Helmholtz and Maxwell problems.", "venue": "SIAM J. Sci. Comput.", "authors": ["Yang  Liu", "Pieter  Ghysels", "Lisa  Claus", "Xiaoye Sherry Li"], "year": 2021, "n_citations": 1}
{"id": 2619111, "s2_id": "98886f5561831b345bde54879dc14d31e6d75773", "title": "Density estimation with distribution element trees", "abstract": "The estimation of probability densities based on available data is a central task in many statistical applications. Especially in the case of large ensembles with many samples or high-dimensional sample spaces, computationally efficient methods are needed. We propose a new method that is based on a decomposition of the unknown distribution in terms of so-called distribution elements (DEs). These elements enable an adaptive and hierarchical discretization of the sample space with small or large elements in regions with smoothly or highly variable densities, respectively. The novel refinement strategy that we propose is based on statistical goodness-of-fit and pairwise (as an approximation to mutual) independence tests that evaluate the local approximation of the distribution in terms of DEs. The capabilities of our new method are inspected based on several examples of different dimensionality and successfully compared with other state-of-the-art density estimators.", "venue": "Stat. Comput.", "authors": ["Daniel W. Meyer"], "year": 2018, "n_citations": 7}
{"id": 2621666, "s2_id": "a27614bb4e9eddc6f5c6c342f174b4ea613c6281", "title": "Efficient Realization of Householder Transform Through Algorithm-Architecture Co-Design for Acceleration of QR Factorization", "abstract": "QR factorization is a ubiquitous operation in many engineering and scientific applications. In this paper, we present efficient realization of Householder Transform (HT) based QR factorization through algorithm-architecture co-design where we achieve performance improvement of 3-90x in-terms of Gflops/watt over state-of-the-art multicore, General Purpose Graphics Processing Units (GPGPUs), Field Programmable Gate Arrays (FPGAs), and ClearSpeed CSX700. Theoretical and experimental analysis of classical HT is performed for opportunities to exhibit higher degree of parallelism where parallelism is quantified as a number of parallel operations per level in the Directed Acyclic Graph (DAG) of the transform. Based on theoretical analysis of classical HT, an opportunity to re-arrange computations in the classical HT is identified that results in Modified HT (MHT) where it is shown that MHT exhibits 1.33x times higher parallelism than classical HT. Experiments in off-the-shelf multicore and General Purpose Graphics Processing Units (GPGPUs) for HT and MHT suggest that MHT is capable of achieving slightly better or equal performance compared to classical HT based QR factorization realizations in the optimized software packages for Dense Linear Algebra (DLA). We implement MHT on a customized platform for Dense Linear Algebra (DLA) and show that MHT achieves 1.3x better performance than native implementation of classical HT on the same accelerator. For custom realization of HT and MHT based QR factorization, we also identify macro operations in the DAGs of HT and MHT that are realized on a Reconfigurable Data-path (RDP). We also observe that due to re-arrangement in the computations in MHT, custom realization of MHT is capable of achieving 12 percent better performance improvement over multicore and GPGPUs than the performance improvement reported by General Matrix Multiplication (GEMM) over highly tuned DLA software packages for multicore and GPGPUs which is counter-intuitive.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan"], "year": 2018, "n_citations": 6}
{"id": 2622880, "s2_id": "c94393bc6439fecece2f4149837c90d83f742ee8", "title": "DLVM: A modern compiler infrastructure for deep learning systems", "abstract": "Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain-specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning.", "venue": "ICLR", "authors": ["Richard  Wei", "Vikram S. Adve", "Lane  Schwartz"], "year": 2018, "n_citations": 42}
{"id": 2629713, "s2_id": "eac30867bae5a7c65b7c6e320e31f73a6add4b87", "title": "Parallel Performance of Algebraic Multigrid Domain Decomposition (AMG-DD)", "abstract": "Algebraic multigrid (AMG) is a widely used scalable solver and preconditioner for large-scale linear systems resulting from the discretization of a wide class of elliptic PDEs. While AMG has optimal computational complexity, the cost of communication has become a significant bottleneck that limits its scalability as processor counts continue to grow on modern machines. This paper examines the design, implementation, and parallel performance of a novel algorithm, Algebraic Multigrid Domain Decomposition (AMG-DD), designed specifically to limit communication. The goal of AMG-DD is to provide a low-communication alternative to standard AMG V-cycles by trading some additional computational overhead for a significant reduction in communication cost. Numerical results show that AMG-DD achieves superior accuracy per communication cost compared to AMG, and speedup over AMG is demonstrated on a large GPU cluster.", "venue": "ArXiv", "authors": ["Wayne  Mitchell", "Robert  Strzodka", "Robert  Falgout", "Stephen  McCormick"], "year": 2019, "n_citations": 2}
{"id": 2645103, "s2_id": "0fa551f131093472bae832a49e9f6b4ff175d8c2", "title": "A Method for Fast Diagonalization of a 2x2 or 3x3 Real Symmetric Matrix", "abstract": "A method is presented for fast diagonalization of a 2x2 or 3x3 real symmetric matrix, that is determination of its eigenvalues and eigenvectors. The Euler angles of the eigenvectors are computed. A small computer algebra program is used to compute some of the identities, and a C++ program for testing the formulas has been uploaded to arXiv.", "venue": "ArXiv", "authors": ["Maarten  Kronenburg"], "year": 2013, "n_citations": 7}
{"id": 2646471, "s2_id": "f2a51de0eb9ab12af9abfa284f1502c5255ba5f2", "title": "Metaprogramming Applied to Numerical Problems", "abstract": "From the discovery that the template system of C++ forms a Turing complete language in 1994, a programming technique called Template Metaprogramming has emerged that allows for the creation of faster, more generic and better code. Here, we apply Template Metaprogramming to implement a generic Runge-Kutta scheme that can be used to numerically solve ordinary differential equations. We show that using Template Metaprogramming results in a significantly improved performance compared to a classical implementation.", "venue": "ArXiv", "authors": ["Mario  Mulansky", "Karsten  Ahnert"], "year": 2011, "n_citations": 4}
{"id": 2648626, "s2_id": "0cd5917b0480661f4a836ab19835c38b58e3a32b", "title": "Automatic generation of CUDA code performing tensor manipulations using C++ expression templates", "abstract": "We present a C++ library, TLoops, which uses a hierarchy of expression templates to represent operations upon tensorial quantities in single lines of C++ code that resemble analytic equations. These expressions may be run as-is, but may also be used to emit equivalent low-level C or CUDA code, which either performs the operations more quickly on the CPU, or allows them to be rapidly ported to run on NVIDIA GPUs. We detail the expression template and C++-class hierarchy that represents the expressions and which makes automatic code-generation possible. We then present benchmarks of the expression-template code, the automatically generated C code, and the automatically generated CUDA code running on several generations of NVIDIA GPU.", "venue": "ArXiv", "authors": ["Adam G. M. Lewis", "Harald P. Pfeiffer"], "year": 2018, "n_citations": 1}
{"id": 2657914, "s2_id": "274a40cbafaad12e26ad5e0f7f0d4eba16592360", "title": "Fast Block Linear System Solver Using Q-Learning Schduling for Unified Dynamic Power System Simulations", "abstract": "We present a fast block direct solver for the unified dynamic simulations of power systems. This solver uses a novel Q-learning based method for task scheduling. Unified dynamic simulations of power systems represent a method in which the electric-mechanical transient, medium-term and long-term dynamic phenomena are organically united. Due to the high rank and large numbers in solving, fast solution of these equations is the key to speeding up the simulation. The sparse systems of simulation contain complex nested block structure, which could be used by the solver to speed up. For the scheduling of blocks and frontals in the solver, we use a learning based task-tree scheduling technique in the framework of Markov Decision Process. That is, we could learn optimal scheduling strategies by offline training on many sample matrices. Then for any systems, the solver would get optimal task partition and scheduling on the learned model. Our learning-based algorithm could help improve the performance of sparse solver, which has been verified in some numerical experiments. The simulation on some large power systems shows that our solver is 2-6 times faster than KLU, which is the state-of-the-art sparse solver for circuit simulation problems.", "venue": "ArXiv", "authors": ["Yingshi  Chen", "Xinli  Song", "HanYang  Dai", "Tao  Liu", "Wuzhi  Zhong", "Guoyang  Wu"], "year": 2021, "n_citations": 0}
{"id": 2665033, "s2_id": "7b9e853b79d059709ebc3ba109c59e05ccfd9fcf", "title": "Expressing Sparse Matrix Computations for Productive Performance on Spatial Architectures", "abstract": "This paper addresses spatial programming of sparse matrix computations for productive performance. The challenge is how to express an irregular computation and its optimizations in a regular way. \nA sparse matrix has (non-zero) values and a structure. In this paper, we propose to classify the implementations of a computation on a sparse matrix into two categories: (1) structure-driven, or top-down, approach, which traverses the structure with given row and column indices and locates the corresponding values, and (2) values-driven, or bottom-up, approach, which loads and processes the values in parallel streams, and decodes the structure for the values' corresponding row and column indices. \nOn a spatial architecture like FPGAs, the values-driven approach is the norm. We show how to express a sparse matrix computation and its optimizations for a values-driven implementation. A compiler automatically synthesizes a code to decode the structure. In this way, programmers focus on optimizing the processing of the values, using familiar optimizations for dense matrices, while leaving the complex, irregular structure traversal to an automatic compiler. We also attempt to regularize the optimizations of the reduction for a dynamic number of values, which is common in a sparse matrix computation.", "venue": "ArXiv", "authors": ["Hongbo  Rong"], "year": 2018, "n_citations": 2}
{"id": 2667708, "s2_id": "32f31143fc653805ca3f5c8244285b4bf8182228", "title": "High Performance Solution of Skew-symmetric Eigenvalue Problems with Applications in Solving the Bethe-Salpeter Eigenvalue Problem", "abstract": "We present a high-performance solver for dense skew-symmetric matrix eigenvalue problems. Our work is motivated by applications in computational quantum physics, where one solution approach to solve the so-called Bethe-Salpeter equation involves the solution of a large, dense, skew-symmetric eigenvalue problem. The computed eigenpairs can be used to compute the optical absorption spectrum of molecules and crystalline systems. One state-of-the art high-performance solver package for symmetric matrices is the ELPA (Eigenvalue SoLvers for Petascale Applications) library. We extend the methods available in ELPA to skew-symmetric matrices. This way, the presented solution method can benefit from the optimizations available in ELPA that make it a well-established, efficient and scalable library, such as GPU support. We compare performance and scalability of our method to the only available high-performance approach for skew-symmetric matrices, an indirect route involving complex arithmetic. In total, we achieve a performance that is up to 3.67 higher than the reference method using Intel's ScaLAPACK implementation. The runtime to solve the Bethe-Salpeter-Eigenvalue problem can be improved by a factor of 10. Our method is freely available in the current release of the ELPA library.", "venue": "Parallel Comput.", "authors": ["Peter  Benner", "Claudia  Ambrosch-Draxl", "Andreas  Marek", "Carolin  Penke", "Christian  Vorwerk"], "year": 2020, "n_citations": 3}
{"id": 2668592, "s2_id": "dc7fad9e425aea1ee49185b2941a5a8f349641d1", "title": "Scipp: Scientific data handling with labeled multi-dimensional arrays for C++ and Python", "abstract": "Scipp is heavily inspired by the Python library xarray. It enriches raw NumPy-like multi-dimensional arrays of data by adding named dimensions and associated coordinates. Multiple arrays are combined into datasets. On top of this, scipp introduces (i) implicit handling of physical units, (ii) implicit propagation of uncertainties, (iii) support for histograms, i.e., bin-edge coordinate axes, which exceed the data's dimension extent by one, and (iv) support for event data. In conjunction these features enable a more natural and more concise user experience. The combination of named dimensions, coordinates, and units helps to drastically reduce the risk for programming errors. The core of scipp is written in C++ to open opportunities for performance improvements that a Python-based solution would not allow for. On top of the C++ core, scipp's Python components provide functionality for plotting and content representations, e.g., for use in Jupyter Notebooks. While none of scipp's concepts in isolation is novel per-se, we are not aware of any project combining all of these aspects in a single coherent software package.", "venue": "ArXiv", "authors": ["Simon  Heybrock", "Owen  Arnold", "Igor  Gudich", "Daniel  Nixon", "Neil  Vaytet"], "year": 2020, "n_citations": 1}
{"id": 2669075, "s2_id": "d105009149ff425b8564a308aafd28085fb02ae9", "title": "SOSTOOLS Version 3.00 Sum of Squares Optimization Toolbox for MATLAB", "abstract": "SOSTOOLS v3.00 is the latest release of the freely available MATLAB toolbox for formulating and solving sum of squares (SOS) optimization problems. Such problems arise naturally in the analysis and control of nonlinear dynamical systems, but also in other areas such as combinatorial optimization. Highlights of the new release include the ability to create polynomial matrices and formulate polynomial matrix inequalities, compatibility with MuPAD, the new MATLAB symbolic engine, as well as the multipoly toolbox v2.01. SOSTOOLS v3.00 can interface with five semidefinite programming solvers, and includes ten demonstration examples.", "venue": "ArXiv", "authors": ["Antonis  Papachristodoulou", "James  Anderson", "Giorgio  Valmorbida", "Stephen  Prajna", "Pete  Seiler", "Pablo A. Parrilo"], "year": 2013, "n_citations": 130}
{"id": 2671405, "s2_id": "c1643aa0996b330cb282b1979ff62d24d670cf28", "title": "Efficient Parallel 3D Computation of the Compressible Euler Equations with an Invariant-domain Preserving Second-order Finite-element Scheme", "abstract": "We discuss the efficient implementation of a high-performance second-order collocation-type finite-element scheme for solving the compressible Euler equations of gas dynamics on unstructured meshes. The solver is based on the convex-limiting technique introduced by Guermond et al. (SIAM J. Sci. Comput. 40, A3211\u2013A3239, 2018). As such, it is invariant-domain preserving; i.e., the solver maintains important physical invariants and is guaranteed to be stable without the use of ad hoc tuning parameters. This stability comes at the expense of a significantly more involved algorithmic structure that renders conventional high-performance discretizations challenging. We develop an algorithmic design that allows SIMD vectorization of the compute kernel, identify the main ingredients for a good node-level performance, and report excellent weak and strong scaling of a hybrid thread/MPI parallelization.", "venue": "ACM Trans. Parallel Comput.", "authors": ["Matthias  Maier", "Martin  Kronbichler"], "year": 2021, "n_citations": 3}
{"id": 2673517, "s2_id": "2d1923e057ef40410cd0a7ac776b5a0019baa3fa", "title": "fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation", "abstract": "Machine learning decision systems are getting omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are really eager to learn social biases present in historical data that can lead to increasing discrimination. If we want to create models responsibly then we need tools for in-depth validation of models also from the perspective of potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in classification models in an easy and flexible fashion. The fairmodels package offers a model-agnostic approach to bias detection, visualization and mitigation. The implemented set of functions and fairness metrics enables model fairness validation from different perspectives. The package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model. The package is designed not only to examine a single model, but also to facilitate comparisons between multiple models.", "venue": "ArXiv", "authors": ["Jakub  Wi'sniewski", "Przemyslaw  Biecek"], "year": 2021, "n_citations": 2}
{"id": 2678076, "s2_id": "9ac9458578d24e6ae486b9d737cf3a0d8fd6bc18", "title": "ShearLab: A Rational Design of a Digital Parabolic Scaling Algorithm", "abstract": "Multivariate problems are typically governed by anisotropic features such as edges in images. A common bracket of most of the various directional representation systems which have been proposed to deliver sparse approximations of such features is the utilization of parabolic scaling. One prominent example is the shearlet system. Our objective in this paper is threefold: We first develop a digital shearlet theory which is rationally designed in the sense that it is the digitization of the existing shearlet theory for continuous data. This implies that shearlet theory provides a unified treatment of both the continuum and digital realms. Second, we analyze the utilization of pseudo-polar grids and the pseudo-polar Fourier transform for digital implementations of parabolic scaling algorithms. We derive an isometric pseudo-polar Fourier transform by careful weighting of the pseudo-polar grid, allowing exploitation of its adjoint for the inverse transform. This leads to a digital implementation of the shearlet...", "venue": "SIAM J. Imaging Sci.", "authors": ["Gitta  Kutyniok", "Morteza  Shahram", "Xiaosheng  Zhuang"], "year": 2012, "n_citations": 112}
{"id": 2682066, "s2_id": "5a40874631908f9e52e241b7971d7475989b3b48", "title": "Benchmarking the graphulo processing framework", "abstract": "Graph algorithms have wide applicablity to a variety of domains and are often used on massive datasets. Recent standardization efforts such as the GraphBLAS specify a set of key computational kernels that hardware and software developers can adhere to. Graphulo is a processing framework that enables GraphBLAS kernels in the Apache Accumulo database. In our previous work, we have demonstrated a core Graphulo operation called TableMult that performs large-scale multiplication operations of database tables. In this article, we present the results of scaling the Graphulo engine to larger problems and scalablity when a greater number of resources is used. Specifically, we present two experiments that demonstrate Graphulo scaling performance is linear with the number of available resources. The first experiment demonstrates cluster processing rates through Graphulo's TableMult operator on two large graphs, scaled between 217 and 219 vertices. The second experiment uses TableMult to extract a random set of rows from a large graph (219 nodes) to simulate a cued graph analytic. These benchmarking results are of relevance to Graphulo users who wish to apply Graphulo to their graph problems.", "venue": "2016 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Timothy  Weale", "Vijay  Gadepally", "Dylan  Hutchison", "Jeremy  Kepner"], "year": 2016, "n_citations": 4}
{"id": 2682495, "s2_id": "ffe20c8f9298c0281464bf7af1aab6e303d96526", "title": "PyLops - A Linear-Operator Python Library for large scale optimization", "abstract": "Linear operators and optimisation are at the core of many algorithms used in signal and image processing, remote sensing, and inverse problems. For small to medium-scale problems, existing software packages (e.g., MATLAB, Python numpy and scipy) allow for explicitly building dense (or sparse) matrices and performing algebraic operations (e.g., computation of matrix-vector products and manipulation of matrices) with syntax that closely represents their corresponding analytical forms. However, many real application, large-scale operators do not lend themselves to explicit matrix representations, usually forcing practitioners to forego of the convenient linear-algebra syntax available for their explicit-matrix counterparts. PyLops is an open-source Python library providing a flexible and scalable framework for the creation and combination of so-called linear operators, class-based entities that represent matrices and inherit their associated syntax convenience, but do not rely on the creation of explicit matrices. We show that PyLops operators can dramatically reduce the memory load and CPU computations compared to explicit-matrix calculations, while still allowing users to seamlessly use their existing knowledge of compact matrix-based syntax that scales to any problem size because no explicit matrices are required.", "venue": "ArXiv", "authors": ["Matteo  Ravasi", "Ivan  Vasconcelos"], "year": 2019, "n_citations": 2}
{"id": 2687367, "s2_id": "a37e9c3c7c9ed6e48048b8594c7f7461db3d02cb", "title": "Approximating the Sum of Correlated Lognormals: An Implementation", "abstract": "Lognormal random variables appear naturally in many engineering disciplines, including wireless communications, reliability theory, and finance. So, too, does the sum of (correlated) lognormal random variables. Unfortunately, no closed form probability distribution exists for such a sum, and it requires approximation. Some approximation methods date back over 80 years and most take one of two approaches, either: 1) an approximate probability distribution is derived mathematically, or 2) the sum is approximated by a single lognormal random variable. In this research, we take the latter approach and review a fairly recent approximation procedure proposed by Mehta, Wu, Molisch, and Zhang (2007), then implement it using C++. The result is applied to a discrete time model commonly encountered within the field of financial economics.", "venue": "ArXiv", "authors": ["Christopher J. Rook", "Mitchell  Kerman"], "year": 2015, "n_citations": 4}
{"id": 2689386, "s2_id": "b8d65b8f98dc16af26a61abbe2d485e478854253", "title": "Cluster computing performances using virtual processors and mathematical software", "abstract": "In this paper I describe some results on the use of virtual processors technology for parallelize some SPMD computational programs in a cluster environment. The tested technology is the INTEL Hyper Threading on real processors, and the programs are MATLAB 6.5 Release 13 scripts for floating points computation. By the use of this technology, I tested that a cluster can run with benefit a number of concurrent processes double the amount of physical processors. The conclusions of the work concern on the utility and limits of the used approach. The main result is that using virtual processors is a good technique for improving parallel programs not only for memory-based computations, but in the case of massive disk-storage operations too.", "venue": "ArXiv", "authors": ["Gianluca  Argentini"], "year": 2004, "n_citations": 0}
{"id": 2701555, "s2_id": "c7f9de12e0858006a589ffa663caef8c1dc4a712", "title": "Introduction to StarNEig - A Task-based Library for Solving Nonsymmetric Eigenvalue Problems", "abstract": "In this paper, we present the StarNEig library for solving dense non-symmetric (generalized) eigenvalue problems. The library is built on top of the StarPU runtime system and targets both shared and distributed memory machines. Some components of the library support GPUs. The library is currently in an early beta state and only real arithmetic is supported. Support for complex data types is planned for a future release. This paper is aimed for potential users of the library. We describe the design choices and capabilities of the library, and contrast them to existing software such as ScaLAPACK. StarNEig implements a ScaLAPACK compatibility layer that should make it easy for a new user to transition to StarNEig. We demonstrate the performance of the library with a small set of computational experiments.", "venue": "PPAM", "authors": ["Mirko  Myllykoski", "Carl Christian Kjelgaard Mikkelsen"], "year": 2019, "n_citations": 7}
{"id": 2702388, "s2_id": "a272b377c0bc11e050bff9f2d9620660d52a661f", "title": "Coupling Methodology within the Software Platform Alliances", "abstract": "CEA, ANDRA and EDF are jointly developing the software platform ALLIANCES which aim is to produce a tool for the simulation of nuclear waste storage and disposal repository. This type of simulations deals with highly coupled thermo-hydro-mechanical and chemical (T-H-M-C) processes. A key objective of Alliances is to give the capability for coupling algorithms development between existing codes. The aim of this paper is to present coupling methodology use in the context of this software platform.", "venue": "ArXiv", "authors": ["Philippe  Montarnal", "Alain  Dimier", "Estelle  Deville", "Erwan  Adam", "J\u00e9r\u00f4me  Gaombalet", "Alain  Bengaouer", "Laurent  Loth", "Cl\u00e9ment  Chavant"], "year": 2006, "n_citations": 14}
{"id": 2703058, "s2_id": "4a379af42753cd7ed3a53be6dde0e2463f48b940", "title": "A library of Taylor models for PVS automatic proof checker", "abstract": "We present in this report a library to compute with Taylor models, a technique extending interval arithmetic to reduce decorrelation and to solve differential equations. Numerical software usually produces only numerical results. Our library can be used to produce both results and proofs. As seen during the development of Fermat's last theorem reported by Acz96, providing a proof is not sufficient. Our library provides a proof that has been thoroughly scrutinized by a trustworthy and tireless assistant. PVS is an automatic proof assistant that has been fairly developed and used and that has no internal connection with interval arithmetic or Taylor models. We built our library so that PVS validates each result as it is produced. As producing and validating a proof, is and will certainly remain a bigger task than just producing a numerical result our library will never be a replacement to imperative implementations of Taylor models such as Cosy Infinity. Our library should mainly be used to validate small to medium size results that are involved in safety or life critical applications.", "venue": "ArXiv", "authors": ["Francisco  Ch\u00e1ves", "Marc  Daumas"], "year": 2006, "n_citations": 5}
{"id": 2712722, "s2_id": "4cd2d5380db6f0b8bf70076648fdaeb26d8447ad", "title": "The mbsts package: Multivariate Bayesian Structural Time Series Models in R", "abstract": "The multivariate Bayesian structural time series (MBSTS) model (Qiu et al., 2018; Jammalamadaka et al., 2019) as a generalized version of many structural time series models, deals with inference and prediction for multiple correlated time series, where one also has the choice of using a different candidate pool of contemporaneous predictors for each target series. The MBSTS model has wide applications and is ideal for feature selection, time series forecasting, nowcasting, inferring causal impact, and others. This paper demonstrates how to use the R package mbsts for MBSTS modeling, establishing a bridge between user-friendly and developer-friendly functions in package and the corresponding methodology. A simulated dataset and object-oriented functions in the mbsts package are explained in the way that enables users to flexibly add or deduct some components, as well as to simplify or complicate some settings.", "venue": "ArXiv", "authors": ["Ning  Ning", "Jinwen  Qiu"], "year": 2021, "n_citations": 0}
{"id": 2713513, "s2_id": "b4e69bf63caa675446cde0ca3afb217b8562f3fc", "title": "Exposing and exploiting structure: optimal code generation for high-order finite element methods", "abstract": "Code generation based software platforms, such as Firedrake, have become popular tools for developing complicated finite element discretisations of partial differential equations. We extended the code generation infrastructure in Firedrake with optimisations that can exploit the structure inherent to some finite elements. This includes sum factorisation on cuboid cells for continuous, discontinuous, H(div) and H(curl) conforming elements. Our experiments confirm optimal algorithmic complexity for high-order finite element assembly. This is achieved through several novel contributions: the introduction of a more powerful interface between the form compiler and the library providing the finite elements; a more abstract, smarter library of finite elements called FInAT that explicitly communicates the structure of elements; and form compiler algorithms to automatically exploit this exposed structure.", "venue": "ArXiv", "authors": ["Mikl\u00f3s  Homolya", "Robert C. Kirby", "David A. Ham"], "year": 2017, "n_citations": 23}
{"id": 2718654, "s2_id": "06716c2f9e04e756c09abdece7e7cd1a97a363b8", "title": "Implementation of $hp$-adaptive discontinuous finite element methods in Dune-Fem", "abstract": "In this paper we describe generic algorithms and data structures for the implementation of $hp$-adaptive discontinuous finite element methods in the Dune-Fem library. Special attention is given to the often tedious and error-prone task of transferring user data during adaptation. Simultaneously, we generalize the approach to the restriction and prolongation of data currently implemented in Dune-Fem to the case of $p$- and $hp$-adaptation. The dune-fem-hpdg module described in this paper provides an extensible reference implementation of $hp$-adaptive discontinuous discrete function spaces. We give details on its implementation and the extended adaptive interface. As proof of concept we present the practical realization of an $hp$-adaptive interior penalty method for elliptic problems.", "venue": "ArXiv", "authors": ["Christoph  Gersbacher"], "year": 2016, "n_citations": 0}
{"id": 2722735, "s2_id": "b958b82b85690edfc3ad3a1aeb1532d2df275b6b", "title": "Brightening the Optical Flow through Posit Arithmetic", "abstract": "As new technologies are invented, their commercial viability needs to be carefully examined along with their technical merits and demerits. The ${posit}^{TM}$ data format, proposed as a drop-in replacement for IEEE $754 ^{TM}$ float format, is one such invention that requires extensive theoretical and experimental study to identify products that can benefit from the advantages of posits for specific market segments. In this paper, we present an extensive empirical study of posit-based arithmetic vis-\u00e0-vis IEEE 754 compliant arithmetic for the optical flow estimation method called Lucas-Kanade (LuKa). First, we use SoftPosit and SoftFloat format emulators to perform an empirical error analysis of the LuKa method. Our study shows that the average error in LuKa with SoftPosit is an order of magnitude lower than LuKa with SoftFloat. We then present the integration of the hardware implementation of a posit adder and multiplier in a RISC-V open-source platform. We make several recommendations, along with the analysis of LuKa in the RISC-V context, for future generation platforms incorporating posit arithmetic units.", "venue": "2021 22nd International Symposium on Quality Electronic Design (ISQED)", "authors": ["Vinay  Saxena", "Ankitha  Reddy", "Jonathan  Neudorfer", "John  Gustafson", "Sangeeth  Nambiar", "Rainer  Leupers", "Farhad  Merchant"], "year": 2021, "n_citations": 2}
{"id": 2724477, "s2_id": "baa005c89540ee8745df0604249110fe1a9af462", "title": "MIPROT: A Medical Image Processing Toolbox for MATLAB", "abstract": "This paper presents a Matlab toolbox to perform basic image processing and visualization tasks, particularly designed for medical image processing. The functionalities available are similar to basic functions found in other nonMatlab widely used libraries such as the Insight Toolkit (ITK). The toolbox is entirely written in native Matlab code, but is fast and flexible. Main use cases for the toolbox are illustrated here, including image input/output, pre-processing, filtering, image registration and visualisation. Both the code and sample data are made publicly available and open source.", "venue": "ArXiv", "authors": ["Alberto  Gomez"], "year": 2021, "n_citations": 0}
{"id": 2730285, "s2_id": "9eecd35d10bfc48383e2d65ff53771be19decf05", "title": "Fast Evaluation of Finite Element Weak Forms Using Python Tensor Contraction Packages", "abstract": "In finite element calculations, the integral forms are usually evaluated using nested loops over elements, and over quadrature points. Many such forms (e.g. linear or multi-linear) can be expressed in a compact way, without the explicit loops, using a single tensor contraction expression by employing the Einstein summation convention. To automate this process and leverage existing high performance codes, we first introduce a notation allowing trivial differentiation of multi-linear finite element forms. Based on that we propose and describe a new transpiler from Einstein summation based expressions, augmented to allow defining multi-linear finite element weak forms, to regular tensor contraction expressions. The resulting expressions are compatible with a number of Python scientific computing packages, that implement, optimize and in some cases parallelize the general tensor contractions. We assess the performance of those packages, as well as the influence of operand memory layouts and tensor contraction paths optimizations on the elapsed time and memory requirements of the finite element form evaluations. We also compare the efficiency of the transpiled weak form implementations to the C-based functions available in the finite element package SfePy.", "venue": "Adv. Eng. Softw.", "authors": ["Robert  Cimrman"], "year": 2021, "n_citations": 1}
{"id": 2733716, "s2_id": "fd24831970c9b1069223205cb1ec04770f841291", "title": "lrsarith: a small fixed/hybrid arithmetic C library", "abstract": "We describe lrsarith which is a small fixed precision and hybrid arithmetic C library for integers and rationals that we developed for use in the lrslib library for polyhedral computation. Using a generic set of operations, a program can be compiled with either 64-bit or 128-bit (if available) fixed precision, with an extended precision library such as GMP or the built-in MP routines. A simple scheme checks for overflow and either terminates the program or, in hybrid mode, changes to a higher precision arithmetic. Implementing these arithmetics in lrslib resulted in only minimal changes to the original code. We give computational results using lrs and mplrs, vertex/facet enumeration codes in lrslib, using 64 and 128 bit fixed integer arithmetic with and without overflow checking, GMP arithmetic, lrsarith hybrid arithmetic with both GMP and MP, and FLINT hybrid arithmetic. We give a small self-contained example C program using the lrsarith package in both fixed precision and hybrid mode.", "venue": "ArXiv", "authors": ["David  Avis", "Charles  Jordan"], "year": 2021, "n_citations": 0}
{"id": 2736808, "s2_id": "e6055974c9368425ca5e6ce76da411d63645d584", "title": "Performance Analysis of FEM Solvers on Practical Electromagnetic Problems", "abstract": "The paper presents a comparative analysis of different commercial and academic software. The comparison aims to examine how the integrated adaptive grid refinement methodologies can deal with challenging, electromagnetic-field related problems. For this comparison, two benchmark problems were examined in the paper. The first example is a solution of an L-shape domain like test problem, which has a singularity at a certain point in the geometry. The second problem is an induction heated aluminum rod, which accurate solution needs to solve a non-linear, coupled physical fields. The accurate solution of this problem requires applying adaptive mesh generation strategies or applying a very fine mesh in the electromagnetic domain, which can significantly increase the computational complexity. The results show that the fully-hp adaptive meshing strategies, which are integrated into Agros-suite, can significantly reduce the task's computational complexity compared to the automatic h-adaptivity, which is part of the examined, popular commercial solvers.", "venue": "ArXiv", "authors": ["Gergely M'at'e Kiss", "Jan  Kaska", "Roberto Andr'e Henrique de Oliveira", "Olena  Rubanenko", "Bal'azs  T'oth"], "year": 2020, "n_citations": 5}
{"id": 2737278, "s2_id": "e4dff54ac8b17c7099e4252cb2218261080ac111", "title": "High-performance symbolic-numerics via multiple dispatch", "abstract": "As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. Exploiting this feature, we demonstrate the ability to swap between classical term-rewriting simplifiers and e-graphbased term-rewriting simplifiers. We illustrate how this symbolic system improves numerical computing tasks by showcasing an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.", "venue": "ArXiv", "authors": ["Shashi  Gowda", "Yingbo  Ma", "Alessandro  Cheli", "Maja  Gw\u00f3zdz", "Viral B. Shah", "Alan  Edelman", "Christopher  Rackauckas"], "year": 2021, "n_citations": 5}
{"id": 2737387, "s2_id": "c71201b0f37d1557d9b1ab8ec9c2b2b442808377", "title": "A Difference-of-Convex Cutting Plane Algorithm for Mixed-Binary Linear Program", "abstract": "In this paper, we propose a cutting plane algorithm based on DC (Difference-of-Convex) programming and DC cut for globally solving MixedBinary Linear Program (MBLP). We first use a classical DC programming formulation via the exact penalization to formulate MBLP as a DC program, which can be solved by DCA algorithm. Then, we focus on the construction of DC cuts, which serves either as a local cut (namely type-I DC cut) at feasible local minimizer of MBLP, or as a global cut (namely type-II DC cut) at infeasible local minimizer of MBLP if some particular assumptions are verified. Otherwise, the constructibility of DC cut is still unclear, and we propose to use classical global cuts (such as the Lift-and-Project cut) instead. Combining DC cut and classical global cuts, a cutting plane algorithm, namely DCCUT, is established for globally solving MBLP. The convergence theorem of DCCUT is proved. Restarting DCA in DCCUT helps to quickly update the upper bound solution and to introduce more DC cuts for lower bound improvement. A variant of DCCUT by introducing more classical global cuts in each iteration is proposed, and parallel versions of DCCUT and its variant are also designed which use the power of multiple processors for better performance. Numerical simulations of DCCUT type algorithms comparing with the classical cutting plane algorithm using Lift-and-Project cuts are reported. Tests on some specific samples and the MIPLIB 2017 benchmark dataset demonstrate the benefits of DC cut and good performance of DCCUT algorithms. The authors are supported by the National Natural Science Foundation of China (Grant 11601327) and by the Key Construction National \u201c985\u201d Program of China (Grant WF220426001). Yi-Shuai Niu School of Mathematical Sciences & SJTU-Paristech, Shanghai Jiao Tong University, China E-mail: niuyishuai@sjtu.edu.cn Yu You School of Mathematical Sciences, Shanghai Jiao Tong University, China E-mail: youyu0828@sjtu.edu.cn ar X iv :2 10 3. 00 71 7v 1 [ m at h. O C ] 1 M ar 2 02 1 2 Yi-Shuai Niu, Yu You", "venue": "ArXiv", "authors": ["Yi-Shuai  Niu", "Yu  You"], "year": 2021, "n_citations": 0}
{"id": 2740949, "s2_id": "4f309bdaba4366a70876f2a0ff04b73ee35dfaa7", "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R", "abstract": "distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibility and scalability of R6, and the speed and efficiency of Rcpp. Over 50 probability distributions are currently implemented in the package with `core' methods including density, distribution, and generating functions, and more `exotic' ones including hazards and distribution function anti-derivatives. In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and product distributions. This paper presents the core functionality of the package and demonstrates examples for key use-cases. In addition this paper provides a critical review of the object-oriented programming paradigms in R and describes some novel implementations for design patterns and core object-oriented features introduced by the package for supporting distr6 components.", "venue": "R J.", "authors": ["Raphael  Sonabend", "Franz  Kiraly"], "year": 2021, "n_citations": 0}
{"id": 2747097, "s2_id": "e7fa17da090c59f068a4f564965d1731e4330e48", "title": "Automatic symbolic computation for discontinuous Galerkin finite element methods", "abstract": "The implementation of discontinuous Galerkin finite element methods (DGFEMs) represents a very challenging computational task, particularly for systems of coupled nonlinear PDEs, including multiphysics problems, whose parameters may consist of power series or functionals of the solution variables. Thereby, the exploitation of symbolic algebra to express a given DGFEM approximation of a PDE problem within a high level language, whose syntax closely resembles the mathematical definition, is an invaluable tool. Indeed, this then facilitates the automatic assembly of the resulting system of (nonlinear) equations, as well as the computation of Fr\\'echet derivative(s) of the DGFEM scheme, needed, for example, within a Newton-type solver. However, even exploiting symbolic algebra, the discretisation of coupled systems of PDEs can still be extremely verbose and hard to debug. Thereby, in this article we develop a further layer of abstraction by designing a class structure for the automatic computation of DGFEM formulations. This work has been implemented within the FEniCS package, based on exploiting the Unified Form Language. Numerical examples are presented which highlight the simplicity of implementation of DGFEMs for the numerical approximation of a range of PDE problems.", "venue": "SIAM J. Sci. Comput.", "authors": ["Paul  Houston", "Nathan  Sime"], "year": 2018, "n_citations": 9}
{"id": 2750688, "s2_id": "02e1edc15367041fc5724a378b90f1ae1d67e1f2", "title": "Classdesc and Graphcode: support for scientific programming in C++", "abstract": "Object-oriented programming languages such as Java and Objective C have become popular for implementing agent-based and other object-based simulations since objects in those languages can {\\em reflect} (i.e. make runtime queries of an object's structure). This allows, for example, a fairly trivial {\\em serialisation} routine (conversion of an object into a binary representation that can be stored or passed over a network) to be written. However C++ does not offer this ability, as type information is thrown away at compile time. Yet C++ is often a preferred development environment, whether for performance reasons or for its expressive features such as operator overloading. \nIn scientific coding, changes to a model's codes takes place constantly, as the model is refined, and different phenomena are studied. Yet traditionally, facilities such as checkpointing, routines for initialising model parameters and analysis of model output depend on the underlying model remaining static, otherwise each time a model is modified, a whole slew of supporting routines needs to be changed to reflect the new data structures. Reflection offers the advantage of the simulation framework adapting to the underlying model without programmer intervention, reducing the effort of modifying the model. \nIn this paper, we present the {\\em Classdesc} system which brings many of the benefits of object reflection to C++, {\\em ClassdescMP} which dramatically simplifies coding of MPI based parallel programs and {\\em \nGraphcode} a general purpose data parallel programming environment.", "venue": "ArXiv", "authors": ["Russell K. Standish", "Duraid  Madina"], "year": 2006, "n_citations": 4}
{"id": 2759955, "s2_id": "a517ce961685f3e60870547aa8f8d95e673dc096", "title": "Just Another Quantum Assembly Language (Jaqal)", "abstract": "The Quantum Scientific Computing Open User Testbed (QSCOUT) is a trapped-ion quantum computer testbed realized at Sandia National Laboratories on behalf of the Department of Energy's Office of Science and its Advanced Scientific Computing (ASCR) program. Here we describe Jaqal, for Just another quantum assembly language, the programming language we invented to specify programs executed on QSCOUT. Jaqal is useful beyond QSCOUT\u2014it can support mutliple hardware targets because it offloads gate names and their pulse-sequence definitions to external files. We describe the capabilities of the Jaqal language, our approach in designing it, and the reasons for its creation. To learn more about QSCOUT, Jaqal, or JaqalPaq, the metaprogramming Python package we developed for Jaqal, please visit qscout.sandia.gov, gitlab.com/jaqal, or send an e-mail to qscout@sandia.gov.", "venue": "2020 IEEE International Conference on Quantum Computing and Engineering (QCE)", "authors": ["Benjamin C. A. Morrison", "Andrew J. Landahl", "Daniel S. Lobser", "Kenneth M. Rudinger", "Antonio E. Russo", "Jay W. Van Der Wall", "Peter  Maunz"], "year": 2020, "n_citations": 5}
{"id": 2769430, "s2_id": "166ef9482d80950f2d12fe6ee852ec0f69adc814", "title": "MFEM: a modular finite element methods library", "abstract": "MFEM is an open-source, lightweight, flexible and scalable C++ library for modular finite element methods that features arbitrary high-order finite element meshes and spaces, support for a wide variety of discretization approaches and emphasis on usability, portability, and high-performance computing efficiency. MFEM's goal is to provide application scientists with access to cutting-edge algorithms for high-order finite element meshing, discretizations and linear solvers, while enabling researchers to quickly and easily develop and test new algorithms in very general, fully unstructured, high-order, parallel and GPU-accelerated settings. In this paper we describe the underlying algorithms and finite element abstractions provided by MFEM, discuss the software implementation, and illustrate various applications of the library.", "venue": "Comput. Math. Appl.", "authors": ["Robert W. Anderson", "Julian  Andrej", "Andrew T. Barker", "Jamie A. Bramwell", "Sylvain  Camier", "Jakub  Cerven\u00fd", "Veselin  Dobrev", "Yohann  Dudouit", "Aaron  Fisher", "Tzanio V. Kolev", "Will  Pazner", "Mark  Stowell", "Vladimir Z. Tomov", "Johann  Dahm", "David  Medina", "Stefano  Zampini"], "year": 2021, "n_citations": 75}
{"id": 2778084, "s2_id": "52990ce6963e8189b4768eacba4281d96f6e49d4", "title": "A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic", "abstract": "Within the past years, hardware vendors have started designing low precision special function units in response to the demand of the Machine Learning community and their demand for high compute power in low precision formats. Also the server-line products are increasingly featuring low-precision special function units, such as the NVIDIA tensor cores in ORNL's Summit supercomputer providing more than an order of magnitude higher performance than what is available in IEEE double precision. At the same time, the gap between the compute power on the one hand and the memory bandwidth on the other hand keeps increasing, making data access and communication prohibitively expensive compared to arithmetic operations. To start the multiprecision focus effort, we survey the numerical linear algebra community and summarize all existing multiprecision knowledge, expertise, and software capabilities in this landscape analysis report. We also include current efforts and preliminary results that may not yet be considered \"mature technology,\" but have the potential to grow into production quality within the multiprecision focus effort. As we expect the reader to be familiar with the basics of numerical linear algebra, we refrain from providing a detailed background on the algorithms themselves but focus on how mixed- and multiprecision technology can help improving the performance of these methods and present highlights of application significantly outperforming the traditional fixed precision methods.", "venue": "ArXiv", "authors": ["Ahmad  Abdelfattah", "Hartwig  Anzt", "Erik G. Boman", "Erin C. Carson", "Terry  Cojean", "Jack J. Dongarra", "Mark  Gates", "Thomas  Gr\u00fctzmacher", "Nicholas J. Higham", "Sherry  Li", "Neil  Lindquist", "Yang  Liu", "Jennifer A. Loe", "Piotr  Luszczek", "Pratik  Nayak", "Srikara  Pranesh", "Sivasankaran  Rajamanickam", "Tobias  Ribizel", "Barry  Smith", "Kasia  Swirydowicz", "Stephen  Thomas", "Stanimire  Tomov", "Yaohung M. Tsai", "Ichitaro  Yamazaki", "Urike Meier Yang"], "year": 2020, "n_citations": 20}
{"id": 2796097, "s2_id": "fa1c413a1cfa5ce4a9b3db9998da63b86c618a4b", "title": "MILJS : Brand New JavaScript Libraries for Matrix Calculation and Machine Learning", "abstract": "MILJS is a collection of state-of-the-art, platform-independent, scalable, fast JavaScript libraries for matrix calculation and machine learning. Our core library offering a matrix calculation is called Sushi, which exhibits far better performance than any other leading machine learning libraries written in JavaScript. Especially, our matrix multiplication is 177 times faster than the fastest JavaScript benchmark. Based on Sushi, a machine learning library called Tempura is provided, which supports various algorithms widely used in machine learning research. We also provide Soba as a visualization library. The implementations of our libraries are clearly written, properly documented and thus can are easy to get started with, as long as there is a web browser. These libraries are available from this http URL under the MIT license.", "venue": "ArXiv", "authors": ["Ken  Miura", "Tetsuaki  Mano", "Atsushi  Kanehira", "Yuichiro  Tsuchiya", "Tatsuya  Harada"], "year": 2015, "n_citations": 4}
{"id": 2798922, "s2_id": "b40c3fc949ec00e7f73d665140c3b075db39b9e1", "title": "SParSH-AMG: A library for hybrid CPU-GPU algebraic multigrid and preconditioned iterative methods", "abstract": "Hybrid CPU-GPU algorithms for Algebraic Multigrid methods (AMG) to efficiently utilize both CPU and GPU resources are presented. In particular, hybrid AMG framework focusing on minimal utilization of GPU memory with performance on par with GPU-only implementations is developed. The hybrid AMG framework can be tuned to operate at a significantly lower GPU-memory, consequently, enables to solve large algebraic systems. Combining the hybrid AMG framework as a preconditioner with Krylov Subspace solvers like Conjugate Gradient, BiCG methods provides a solver stack to solve a large class of problems. The performance of the proposed hybrid AMG framework is analysed for an array of matrices with different properties and size. Further, the performance of CPU-GPU algorithms are compared with the GPU-only implementations to illustrate the significantly lower memory requirements.", "venue": "ArXiv", "authors": ["SASHIKUMAAR  GANESAN", "MANAN  SHAH"], "year": 2020, "n_citations": 1}
{"id": 2799075, "s2_id": "7b53b391044d7c1ba3dfc0c08dbd9e1f41ec7222", "title": "Applying Sorting Networks to Synthesize Optimized Sorting Libraries", "abstract": "This paper shows an application of the theory of sorting networks to facilitate the synthesis of optimized general purpose sorting libraries. Standard sorting libraries are often based on combinations of the classic Quicksort algorithm with insertion sort applied as the base case for small fixed numbers of inputs. Unrolling the code for the base case by ignoring loop conditions eliminates branching and results in code which is equivalent to a sorting network. This enables the application of further program transformations based on sorting network optimizations, and eventually the synthesis of code from sorting networks. We show that if considering the number of comparisons and swaps then theory predicts no real advantage of this approach. However, significant speed-ups are obtained when taking advantage of instruction level parallelism and non-branching conditional assignment instructions, both of which are common in modern CPU architectures. We provide empirical evidence that using code synthesized from efficient sorting networks as the base case for Quicksort libraries results in significant real-world speed-ups.", "venue": "LOPSTR", "authors": ["Michael  Codish", "Lu\u00eds  Cruz-Filipe", "Markus  Nebel", "Peter  Schneider-Kamp"], "year": 2015, "n_citations": 9}
{"id": 2802201, "s2_id": "6b8befb91e4b526ba0c6402954cc5f8697d84716", "title": "Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave Scattering", "abstract": "Algorithmic and architecture-oriented optimizations are essential for achieving performance worthy of anticipated energy-austere exascale systems. In this paper, we present an extreme scale FMM-accelerated boundary integral equation solver for wave scattering, which uses FMM as a matrix-vector multiplication inside the GMRES iterative method. Our FMM Helmholtz kernels treat nontrivial singular and near-field integration points. We implement highly optimized kernels for both shared and distributed memory, targeting emerging Intel extreme performance HPC architectures. We extract the potential thread- and data-level parallelism of the key Helmholtz kernels of FMM. Our application code is well optimized to exploit the AVX-512 SIMD units of Intel Skylake and Knights Landing architectures. We provide different performance models for tuning the task-based tree traversal implementation of FMM, and develop optimal architecture-specific and algorithm aware partitioning, load balancing, and communication reducing mechanisms to scale up to 6,144 compute nodes of a Cray XC40 with 196,608 hardware cores. With shared memory optimizations, we achieve roughly 77% of peak single precision floating point performance of a 56-core Skylake processor, and on average 60% of peak single precision floating point performance of a 72-core KNL. These numbers represent nearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the baseline scalar code. With distributed memory optimizations, on the other hand, we report near-optimal efficiency in the weak scalability study with respect to both the logarithmic communication complexity as well as the theoretical scaling complexity of FMM. In addition, we exhibit up to 85% efficiency in strong scaling. We compute in excess of 2 billion DoF on the full-scale of the Cray XC40 supercomputer.", "venue": "SIAM J. Sci. Comput.", "authors": ["Mustafa Abdul Jabbar", "Mohammed A. Al Farhan", "Noha  Al-Harthi", "Rui  Chen", "Rio  Yokota", "Hakan  Bagci", "David E. Keyes"], "year": 2019, "n_citations": 12}
{"id": 2802271, "s2_id": "b8fe8888d270e7279a4cb4fb36ec6892e6d1e361", "title": "Optimal Checkpointing for Adjoint Multistage Time-Stepping Schemes", "abstract": "We consider checkpointing strategies that minimize the number of recomputations needed when performing discrete adjoint computations using multistage time-stepping schemes, which requires computing several substeps within one complete time step. In this case we propose two algorithms that can generate optimal checkpointing schedules under weak assumptions. The first is an extension of the seminal Revolve algorithm adapted to multistage schemes. The second algorithm, named CAMS, is developed based on dynamic programming, and it requires the least number of recomputations when compared with other algorithms. The CAMS algorithm is made publicly available in a library with bindings to C and Python. Numerical results illustrate that the proposed algorithms can deliver up to two times the speedup compared with that of classical Revolve. Moreover, we discuss a tailored implementation of an adjoint computation that is arguably better suited for mature scientific computing libraries by avoiding the central control assumed by the original checkpointing strategy. The proposed algorithms have been adopted by the PETSc TSAdjoint library. Their performance has been demonstrated with a large-scale PDE-constrained optimization problem on a leadership-class supercomputer.", "venue": "ArXiv", "authors": ["Hong  Zhang", "Emil  Constantinescu"], "year": 2021, "n_citations": 0}
{"id": 2804674, "s2_id": "9894ffe20894fd05896ad33245f2380fdfe31ef7", "title": "Some Software Packages for Partial SVD Computation", "abstract": "This technical report introduces some software packages for partial SVD computation, including optimized PROPACK, modified PROPACK for computing singular values above a threshold and the corresponding singular vectors, and block Lanczos with warm start (BLWS). The current version is preliminary. The details will be enriched soon.", "venue": "ArXiv", "authors": ["Zhouchen  Lin"], "year": 2011, "n_citations": 10}
{"id": 2806192, "s2_id": "33fb5d194ebc750d3fade8a7ae5640b1a244cd8e", "title": "Reachability of weakly nonlinear systems using Carleman linearization", "abstract": "In this article we introduce a solution method for a special class of nonlinear initial-value problems using set-based propagation techniques. The novelty of the approach is that we employ a particular embedding (Carleman linearization) to leverage recent advances of highdimensional reachability solvers for linear ordinary differential equations based on the support function. Using a global error bound for the Carleman linearization abstraction, we are able to describe the full set of behaviors of the system for sets of initial conditions and in dense time.", "venue": "RP", "authors": ["Marcelo  Forets", "Christian  Schilling"], "year": 2021, "n_citations": 2}
{"id": 2811444, "s2_id": "c20191b2849d10e5861bf6c6a2c311e0343f1417", "title": "COCO: a platform for comparing continuous optimizers in a black-box setting", "abstract": "We introduce COCO, an open-source platform for Comparing Continuous Optimizers in a black-box setting. COCO aims at automatizing the tedious and repetitive task of benchmarking numerical optimization algorithms to the greatest possible extent. The platform and the underlying methodology allow to benchmark in the same framework deterministic and stochastic solvers for both single and multiobjective optimization. We present the rationals behind the (decade-long) development of the platform as a general proposition for guidelines towards better benchmarking. We detail underlying fundamental concepts of COCO such as the definition of a problem as a function instance, the underlying idea of instances, the use of target values, and runtime defined by the number of function calls as the central performance measure. Finally, we give a quick overview of the basic code structure and the currently available test suites.", "venue": "Optim. Methods Softw.", "authors": ["Nikolaus  Hansen", "Anne  Auger", "Olaf  Mersmann", "Tea  Tusar", "Dimo  Brockhoff"], "year": 2021, "n_citations": 155}
{"id": 2811801, "s2_id": "42b5c857ddcf5b47744b9a88dcd29d281cd4decb", "title": "Solving Polynomial Systems by Penetrating Gradient Algorithm Applying Deepest Descent Strategy", "abstract": "An algorithm and associated strategy for solving polynomial systems within the optimization framework is presented. The algorithm and strategy are named, respectively, the penetrating gradient algorithm and the deepest descent strategy. The most prominent feature of penetrating gradient algorithm, after which it was named, is its ability to see and penetrate through the obstacles in error space along the line of search direction and to jump to the global minimizer in a single step. The ability to find the deepest point in an arbitrary direction, no matter how distant the point is and regardless of the relief of error space between the current and the best point, motivates movements in directions in which cost function can be maximally reduced, rather than in directions that seem to be the best locally (like, for instance, the steepest descent, i.e., negative gradient direction). Therefore, the strategy is named the deepest descent, in contrast but alluding to the steepest descent. Penetrating gradient algorithm is derived and its properties are proven mathematically, while features of the deepest descent strategy are shown by comparative simulations. Extensive benchmark tests confirm that the proposed algorithm and strategy jointly form an effective solver of polynomial systems. In addition, further theoretical considerations in Section 5 about solving linear systems by the proposed method reveal a surprising and interesting relation of proposed and Gauss-Seidel method.", "venue": "ArXiv", "authors": ["Nikica  Hlupic", "Ivo  Beros"], "year": 2015, "n_citations": 0}
{"id": 2816494, "s2_id": "2d3626dcd3f111100621f85cbc140b25be9b7d1a", "title": "A Domain-Specific Compiler for Linear Algebra Operations", "abstract": "We present a prototypical linear algebra compiler that automatically exploits domain-specific knowledge to generate high-performance algorithms. The input to the compiler is a target equation together with knowledge of both the structure of the problem and the properties of the operands. The output is a variety of high-performance algorithms, and the corresponding source code, to solve the target equation. Our approach consists in the decomposition of the input equation into a sequence of library-supported kernels. Since in general such a decomposition is not unique, our compiler returns not one but a number of algorithms. The potential of the compiler is shown by means of its application to a challenging equation arising within the genome-wide association study. As a result, the compiler produces multiple \u201cbest\u201d algorithms that outperform the best existing libraries.", "venue": "VECPAR", "authors": ["Diego  Fabregat-Traver", "Paolo  Bientinesi"], "year": 2012, "n_citations": 31}
{"id": 2822299, "s2_id": "bd405f391eb7a65614a747e1cfd7a080cfcbe45f", "title": "Tuning Technique for Multiple Precision Dense Matrix Multiplication using Prediction of Computational Time", "abstract": "Although reliable long precision floating-point arithmetic libraries such as QD and MPFR/GMP are necessary to solve ill-conditioned problems in numerical simulation, long precision BLAS-level computation such as matrix multiplication has not been fully optimized because tuning costs are very high compared to IEEE float and double precision arithmetic. In this study, we develop a technique to shorten this tuning time by using prediction of computational times in several block sizes for the blocking algorithm, and then selecting the fastest matrix multiplication method for tuning multiple precision dense real matrix multiplication in various precisions, matrix sizes, and degrees of parallelization.", "venue": "ArXiv", "authors": ["Tomonori  Kouya"], "year": 2017, "n_citations": 1}
{"id": 2830976, "s2_id": "230256dceb46fe887e4107f04b8bc1778228eb0d", "title": "Community Landscapes: An Integrative Approach to Determine Overlapping Network Module Hierarchy, Identify Key Nodes and Predict Network Dynamics", "abstract": "Background Network communities help the functional organization and evolution of complex networks. However, the development of a method, which is both fast and accurate, provides modular overlaps and partitions of a heterogeneous network, has proven to be rather difficult. Methodology/Principal Findings Here we introduce the novel concept of ModuLand, an integrative method family determining overlapping network modules as hills of an influence function-based, centrality-type community landscape, and including several widely used modularization methods as special cases. As various adaptations of the method family, we developed several algorithms, which provide an efficient analysis of weighted and directed networks, and (1) determine pervasively overlapping modules with high resolution; (2) uncover a detailed hierarchical network structure allowing an efficient, zoom-in analysis of large networks; (3) allow the determination of key network nodes and (4) help to predict network dynamics. Conclusions/Significance The concept opens a wide range of possibilities to develop new approaches and applications including network routing, classification, comparison and prediction.", "venue": "PloS one", "authors": ["Istv\u00e1n A. Kov\u00e1cs", "Robin  Palotai", "Mate S. Szalay", "Peter  Csermely"], "year": 2010, "n_citations": 292}
{"id": 2835554, "s2_id": "3b58e668f1ae519c1a9d7034cef686dd122be18a", "title": "Automatic and Transparent Transfer of Theorems along Isomorphisms in the Coq Proof Assistant", "abstract": "In mathematics, it is common practice to have several constructions for the same objects. Mathematicians will identify them modulo isomorphism and will not worry later on which construction they use, as theorems proved for one construction will be valid for all. \nWhen working with proof assistants, it is also common to see several data-types representing the same objects. This work aims at making the use of several isomorphic constructions as simple and as transparent as it can be done informally in mathematics. This requires inferring automatically the missing proof-steps. \nWe are designing an algorithm which finds and fills these missing proof-steps and we are implementing it as a plugin for Coq.", "venue": "CICM", "authors": ["Th\u00e9o  Zimmermann", "Hugo  Herbelin"], "year": 2015, "n_citations": 19}
{"id": 2837186, "s2_id": "28df16b2118fba7e910a1dcc715c4910047d8270", "title": "An Open Source Pattern Recognition Toolbox for MATLAB", "abstract": "AbstractPattern recognition and machine learning are becoming integralparts of algorithms in a wide range of applications. Di erent algo-rithms and approaches for machine learning include di erent tradeo sbetween performance and computation, so during algorithm develop-ment it is often necessary to explore a variety of di erent approachesto a given task. A toolbox with a uni ed framework across multiplepattern recognition techniques enables algorithm developers the abilityto rapidly evaluate di erent choices prior to deployment. MATLAB isa widely used environment for algorithm development and prototyp-ing, and although several MATLAB toolboxes for pattern recognitionare currently available these are either incomplete, expensive, or re-strictively licensed. In this work we describe a MATLAB toolbox forpattern recognition and machine learning known as the PRT (PatternRecognition Toolbox), licensed under the permissive MIT license. ThePRT includes many popular techniques for data preprocessing, super-vised learning, clustering, regression and feature selection, as well asa methodology for combining these components using a simple, uni-form syntax. The resulting algorithms can be evaluated using cross-validation and a variety of scoring metrics to ensure robust performancewhen the algorithm is deployed. This paper presents an overview ofthe PRT as well as an example of usage on Fisher\u2019s Iris dataset.", "venue": "ArXiv", "authors": ["Kenneth  Morton", "Peter  Torrione", "Leslie M. Collins", "Sam  Keene"], "year": 2014, "n_citations": 6}
{"id": 2844419, "s2_id": "ef5bedcffb7c8f89d191d6a134e8b6ee605b8e7b", "title": "Hipster: Integrating Theory Exploration in a Proof Assistant", "abstract": "This paper describes Hipster, a system integrating theory exploration with the proof assistant Isabelle/HOL. Theory exploration is a technique for automatically discovering new interesting lemmas in a given theory development. Hipster can be used in two main modes. The first is exploratory mode, used for automatically generating basic lemmas about a given set of datatypes and functions in a new theory development. The second is proof mode, used in a particular proof attempt, trying to discover the missing lemmas which would allow the current goal to be proved. Hipster\u2019s proof mode complements and boosts existing proof automation techniques that rely on automatically selecting existing lemmas, by inventing new lemmas that need induction to be proved. We show example uses of both modes.", "venue": "CICM", "authors": ["Moa  Johansson", "Dan  Ros\u00e9n", "Nicholas  Smallbone", "Koen  Claessen"], "year": 2014, "n_citations": 44}
{"id": 2850622, "s2_id": "dbddc1db2af5cabe5a38e158b74cb8371157c55a", "title": "DUNEuro\u2014A software toolbox for forward modeling in bioelectromagnetism", "abstract": "Accurate and efficient source analysis in electro- and magnetoencephalography using sophisticated realistic head geometries requires advanced numerical approaches. This paper presents DUNEuro, a free and open-source C++ software toolbox for the numerical computation of forward solutions in bioelectromagnetism. Building upon the DUNE framework, it provides implementations of modern fitted and unfitted finite element methods to efficiently solve the forward problems of electro- and magnetoencephalography. The user can choose between a variety of different source models that are implemented. The software\u2019s aim is to provide interfaces that are extendable and easy-to-use. In order to enable a closer integration into existing analysis pipelines, interfaces to Python and MATLAB are provided. The practical use is demonstrated by a source analysis example of somatosensory evoked potentials using a realistic six-compartment head model. Detailed installation instructions and example scripts using spherical and realistic head models are appended.", "venue": "PloS one", "authors": ["Sophie  Schrader", "Andreas  Westhoff", "Maria Carla Piastra", "Tuuli  Miinalainen", "Sampsa  Pursiainen", "Johannes  Vorwerk", "Heinrich  Brinck", "Carsten H Wolters", "Christian  Engwer"], "year": 2021, "n_citations": 2}
{"id": 2850828, "s2_id": "e9903f87c81f02fb552da8cd737a3ba8cb38dcdb", "title": "A Study of Mixed Precision Strategies for GMRES on GPUs", "abstract": "Support for lower precision computation is becoming more common in accelerator hardware due to lower power usage, reduced data movement and increased computational performance. However, computational science and engineering (CSE) problems require double precision accuracy in several domains. This conflict between hardware trends and application needs has resulted in a need for mixed precision strategies at the linear algebra algorithms level if we want to exploit the hardware to its full potential while meeting the accuracy requirements. In this paper, we focus on preconditioned sparse iterative linear solvers, a key kernel in several CSE applications. We present a study of mixed precision strategies for accelerating this kernel on an NVIDIA V100 GPU with a Power 9 CPU. We seek the best methods for incorporating multiple precisions into the GMRES linear solver; these include iterative refinement and parallelizable preconditioners. Our work presents strategies to determine when mixed precision GMRES will be effective and to choose parameters for a mixed precision iterative refinement solver to achieve better performance. We use an implementation that is based on the Trilinos library and employs Kokkos Kernels for performance portability of linear algebra kernels. Performance results demonstrate the promise of mixed precision approaches and demonstrate even further improvements are possible by optimizing low-level kernels.", "venue": "ArXiv", "authors": ["Jennifer A. Loe", "Christian A. Glusa", "Ichitaro  Yamazaki", "Erik G. Boman", "Sivasankaran  Rajamanickam"], "year": 2021, "n_citations": 0}
{"id": 2860921, "s2_id": "f1820e5ffec8a8b26d7b5e956009d3996c346fde", "title": "A mixed precision semi-Lagrangian algorithm and its performance on accelerators", "abstract": "In this paper we propose a mixed precision algorithm in the context of the semi-Lagrangian discontinuous Galerkin method. The performance of this approach is evaluated on a traditional dual socket workstation as well as on a Xeon Phi and an NVIDIA K80. We find that the mixed precision algorithm can be implemented efficiently on these architectures. This implies that, in addition to the considerable reduction in memory, a substantial increase in performance can be observed as well. Moreover, we discuss the relative performance of our implementations.", "venue": "2016 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Lukas  Einkemmer"], "year": 2016, "n_citations": 17}
{"id": 2862063, "s2_id": "acff0b9f1d1999bf5c026401132a87682b5dd196", "title": "dbcsp: User-friendly R package for Distance-Based Common Spacial Patterns", "abstract": "Common Spacial Patterns (CSP) is a widely used method to analyse electroencephalography (EEG) data, concerning the supervised classification of brain\u2019s activity. More generally, it can be useful to distinguish between multivariate signals recorded during a time span for two different classes. CSP is based on the simultaneous diagonalization of the average covariance matrices of signals from both classes and it allows to project the data into a low-dimensional subspace. Once data are represented in a low-dimensional subspace, a classification step must be carried out. The original CSP method is based on the Euclidean distance between signals and here, we extend it so that it can be applied on any appropriate distance for data at hand. Both, the classical CSP and the new Distance-Based CSP (DB-CSP) are implemented in an R package, called dbcsp.", "venue": "ArXiv", "authors": ["Itsaso  Rodriguez", "Itziar  Irigoien", "Basilio  Sierra", "Concepcion  Arenas"], "year": 2021, "n_citations": 0}
{"id": 2866377, "s2_id": "860f55ee84b8bc1c12f11ec2ae0b4bec747cbbb1", "title": "The full Low-carbon Expansion Generation Optimization (LEGO) model", "abstract": "This paper introduces the full Low-carbon Expansion Generation Optimization (LEGO) model available on Github. LEGO is a mixed integer quadratically constrained optimization problem and has been designed to be a multipurpose tool, like a Swiss army knife, that can be employed to study many different aspects of the energy sector. Ranging from short-term unit commitment to long-term generation and transmission expansion planning. The underlying modeling philosophies are: modularity and flexibility. Its unique temporal structure allows LEGO to function with either chronological hourly data, or all kinds of representative periods. LEGO is also composed of thematic modules that can be added or removed from the model easily via data options depending on the scope of the study. Those modules include: unit commitment constraints; DCor AC-OPF formulations; battery degradation; rate of change of frequency inertia constraints; demand-side management; or the hydrogen sector. LEGO also provides a plethora of model outputs (both primal and dual), which is the basis for both technical but also economic analyses. To our knowledge there is \u2217Corresponding author: URL: wogrin@tugraz.at (S. Wogrin) 1https://github.com/wogrin/LEGO Preprint submitted to xxx September 6, 2021 ar X iv :2 10 9. 01 36 8v 1 [ m at h. O C ] 3 S ep 2 02 1 no model that combines all of these capabilities, which we hereby make freely available to the scientific community.", "venue": "ArXiv", "authors": ["Sonja  Wogrin", "Diego A. Tejada-Arango", "Udo  Bachhiesl", "Benjamin F. Hobbs"], "year": 2021, "n_citations": 0}
{"id": 2867889, "s2_id": "e282291ceeee8022cbe77879b509586df65708c2", "title": "Solving ordinary differential equations on the Infinity Computer by working with infinitesimals numerically", "abstract": "There exists a huge number of numerical methods that iteratively construct approximations to the solution y(x) of an ordinary differential equation (ODE) y^'(x)=f(x,y) starting from an initial value y\"0=y(x\"0) and using a finite approximation step h that influences the accuracy of the obtained approximation. In this paper, a new framework for solving ODEs is presented for a new kind of a computer - the Infinity Computer (it has been patented and its working prototype exists). The new computer is able to work numerically with finite, infinite, and infinitesimal numbers giving so the possibility to use different infinitesimals numerically and, in particular, to take advantage of infinitesimal values of h. To show the potential of the new framework a number of results is established. It is proved that the Infinity Computer is able to calculate derivatives of the solution y(x) and to reconstruct its Taylor expansion of a desired order numerically without finding the respective derivatives analytically (or symbolically) by the successive derivation of the ODE as it is usually done when the Taylor method is applied. Methods using approximations of derivatives obtained thanks to infinitesimals are discussed and a technique for an automatic control of rounding errors is introduced. Numerical examples are given.", "venue": "Appl. Math. Comput.", "authors": ["Yaroslav D. Sergeyev"], "year": 2013, "n_citations": 49}
{"id": 2874535, "s2_id": "fabd59750502f5f113fbd9dd49d4e55cee004803", "title": "A Library for Implementing the Multiple Hypothesis Tracking Algorithm", "abstract": "The Multiple Hypothesis Tracking (MHT) algorithm is known to produce good results in difficult multi-target tracking situations. However, its implementation is not trivial, and is associated with a significant programming effort, code size and long implementation time. We propose a library which addresses these problems by providing a domain independent implementation of the most complex MHT operations. We also address the problem of applying clustering in domain independent manner.", "venue": "ArXiv", "authors": ["David Miguel Antunes", "David Martins de Matos", "Jos\u00e9 Ant\u00f3nio Gaspar"], "year": 2011, "n_citations": 13}
{"id": 2883738, "s2_id": "c75a12e217f1fbac4887721cba5838318eb0571a", "title": "Algorithm for Evaluation of the Interval Power Function of Unconstrained Arguments", "abstract": "We describe an algorithm for evaluation of the interval extension of the power function of variables x and y given by the expression x^y. Our algorithm reduces the general case to the case of non-negative bases.", "venue": "ArXiv", "authors": ["Evgueni  Petrov"], "year": 2007, "n_citations": 1}
{"id": 2887006, "s2_id": "35668e4724fe43607af5578fa0b63e9a51fd5943", "title": "A revision of the subtract-with-borrow random number generators", "abstract": "Abstract The most popular and widely used subtract-with-borrow generator, also known as RANLUX, is reimplemented as a linear congruential generator using large integer arithmetic with the modulus size of 576 bits. Modern computers, as well as the specific structure of the modulus inferred from RANLUX, allow for the development of a fast modular multiplication \u2014\u00a0the core of the procedure. This was previously believed to be slow and have too high cost in terms of computing resources. Our tests show a significant gain in generation speed which is comparable with other fast, high quality random number generators. An additional feature is the fast skipping of generator states leading to a seeding scheme which guarantees the uniqueness of random number sequences. Program summary/New version program summary Program Title: RANLUX++ Licensing provisions: GPLv3 Programming language: C++, C, Assembler", "venue": "Comput. Phys. Commun.", "authors": ["Alexei  Sibidanov"], "year": 2017, "n_citations": 5}
{"id": 2887096, "s2_id": "37c4ed0fd0b0ec5cb0f04b02223d0a62ae34289f", "title": "High-level python abstractions for optimal checkpointing in inversion problems", "abstract": "Inversion and PDE-constrained optimization problems often rely on solving the adjoint problem to calculate the gradient of the objec- tive function. This requires storing large amounts of intermediate data, setting a limit to the largest problem that might be solved with a given amount of memory available. Checkpointing is an approach that can reduce the amount of memory required by redoing parts of the computation instead of storing intermediate results. The Revolve checkpointing algorithm o ers an optimal schedule that trades computational cost for smaller memory footprints. Integrat- ing Revolve into a modern python HPC code and combining it with code generation is not straightforward. We present an API that makes checkpointing accessible from a DSL-based code generation environment along with some initial performance gures with a focus on seismic applications.", "venue": "ArXiv", "authors": ["Navjot  Kukreja", "Jan  H\u00fcckelheim", "Michael  Lange", "Mathias  Louboutin", "Andrea  Walther", "Simon W. Funke", "Gerard  Gorman"], "year": 2018, "n_citations": 13}
{"id": 2889813, "s2_id": "555bd31ad4de8277b8ada6e473b295645ba67bf4", "title": "Coupling parallel adaptive mesh refinement with a nonoverlapping domain decomposition solver", "abstract": "We study the effect of adaptive mesh refinement on a parallel domain decomposition solver of a linear system of algebraic equations. These concepts need to be combined within a parallel adaptive finite element software. A prototype implementation is presented for this purpose. It uses adaptive mesh refinement with one level of hanging nodes. Two and three-level versions of the Balancing Domain Decomposition based on Constraints (BDDC) method are used to solve the arising system of algebraic equations. The basic concepts are recalled and components necessary for the combination are studied in detail. Of particular interest is the effect of disconnected subdomains, a typical output of the employed mesh partitioning based on space-filling curves, on the convergence and solution time of the BDDC method. It is demonstrated using a large set of experiments that while both refined meshes and disconnected subdomains have a negative effect on the convergence of BDDC, the number of iterations remains acceptable. In addition, scalability of the three-level BDDC solver remains good on up to a few thousands of processor cores. The largest presented problem using adaptive mesh refinement has over 109 unknowns and is solved on 2048 cores.", "venue": "Adv. Eng. Softw.", "authors": ["Pavel  Kus", "Jakub  S\u00edstek"], "year": 2017, "n_citations": 2}
{"id": 2890786, "s2_id": "f548be752ab1990a32e8a370d81ad78078e6f85b", "title": "A simple technique for unstructured mesh generation via adaptive finite elements", "abstract": "This work describes a concise algorithm for the generation of triangular meshes with the help of standard adaptive finite element methods. We demonstrate that a generic adaptive finite element solver can be repurposed into a triangular mesh generator if a robust mesh smoothing algorithm is applied between the mesh refinement steps. We present an implementation of the mesh generator and demonstrate the resulting meshes via several examples.", "venue": "ArXiv", "authors": ["Tom  Gustafsson"], "year": 2020, "n_citations": 1}
{"id": 2892093, "s2_id": "75d24bcc3dc141d1b427b9f13a9bc540e83486cc", "title": "Distributed triangle counting in the Graphulo matrix math library", "abstract": "Triangle counting is a key algorithm for large graph analysis. The Graphulo library provides a framework for implementing graph algorithms on the Apache Accumulo distributed database. In this work we adapt two algorithms for counting triangles, one that uses the adjacency matrix and another that also uses the incidence matrix, to the Graphulo library for serverside processing inside Accumulo. Cloud-based experiments show a similar performance profile for these different approaches on the family of power law Graph500 graphs, for which data skew increasingly bottlenecks. These results motivate the design of skew-aware hybrid algorithms that we propose for future work.", "venue": "2017 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Dylan  Hutchison"], "year": 2017, "n_citations": 6}
{"id": 2895027, "s2_id": "95c93285db4f69574b60f2f9119943e00722013c", "title": "An Infra-Structure for Performance Estimation and Experimental Comparison of Predictive Models in R", "abstract": "This document describes an infra-structure provided by the R package performanceEstimation that allows to estimate the predictive performance of different approaches (workflows) to predictive tasks. The infra-structure is generic in the sense that it can be used to estimate the values of any performance metrics, for any workflow on different predictive tasks, namely, classification, regression and time series tasks. The package also includes several standard workflows that allow users to easily set up their experiments limiting the amount of work and information they need to provide. The overall goal of the infra-structure provided by our package is to facilitate the task of estimating the predictive performance of different modeling approaches to predictive tasks in the R environment.", "venue": "ArXiv", "authors": ["Lu\u00eds  Torgo"], "year": 2014, "n_citations": 37}
{"id": 2896276, "s2_id": "9adc13fd51fbce2a4d64703940a81325cdd8e1c9", "title": "mplrs: A scalable parallel vertex/facet enumeration code", "abstract": "We describe a new parallel implementation, mplrs, of the vertex enumeration code lrs that uses the MPI parallel environment and can be run on a network of computers. The implementation makes use of a C wrapper that essentially uses the existing lrs code with only minor modifications. mplrs was derived from the earlier parallel implementation plrs, written by G. Roumanis in C$${++}$$++ which runs on a shared memory machine. By improving load balancing we are able to greatly improve performance for medium to large scale parallelization of lrs. We report computational results comparing parallel and sequential codes for vertex/facet enumeration problems for convex polyhedra. The problems chosen span the range from simple to highly degenerate polytopes. For most problems tested, the results clearly show the advantage of using the parallel implementation mplrs of the reverse search based code lrs, even when as few as 8 cores are available. For some problems almost linear speedup was observed up to 1200 cores, the largest number of cores tested. The software that was reviewed as part of this submission is included in lrslib-062.tar.gz which has MD5 hash be5da7b3b90cc2be628dcade90c5d1b9.", "venue": "Math. Program. Comput.", "authors": ["David  Avis", "Charles  Jordan"], "year": 2018, "n_citations": 16}
{"id": 2897624, "s2_id": "accbfc0624c6bf214a63d6c7a83472b3be66972b", "title": "Streaming Data from HDD to GPUs for Sustained Peak Performance", "abstract": "In the context of the genome-wide association studies (GWAS), one has to solve long sequences of generalized least-squares problems; such a task has two limiting factors: execution time --often in the range of days or weeks-- and data management --data sets in the order of Terabytes. We present an algorithm that obviates both issues. By pipelining the computation, and thanks to a sophisticated transfer strategy, we stream data from hard disk to main memory to GPUs and achieve sustained peak performance; with respect to a highly-optimized CPU implementation, our algorithm shows a speedup of 2.6x. Moreover, the approach lends itself to multiple GPUs and attains almost perfect scalability. When using 4 GPUs, we observe speedups of 9x over the aforementioned implementation, and 488x over a widespread biology library.", "venue": "ArXiv", "authors": ["Lucas  Beyer", "Paolo  Bientinesi"], "year": 2013, "n_citations": 0}
{"id": 2904039, "s2_id": "50d5affcf8facf8ffdd6ec67abbe88961a78f833", "title": "Automated verification of termination certificates", "abstract": "Making sure that a computer program behaves as expected, especially in \ncritical applications (health, transport, energy, communications, etc.), is more \nand more important, all the more so since computer programs become more \nand more ubiquitous and essential to the functioning of modern societies. But \nhow to check that a program behaves as expected, in particular when the range \nof its inputs is very large or potentially infinite? To express with exactness \nwhat is the expected behavior of a program, one first needs to use some formal \nlogical language. However, as shown by Godel, in any formal system rich \nenough for doing arithmetic, there are valid formulas that cannot be proved. \nTherefore, there is no program that can decide whether any property is true or \nnot. However, we can have a program that decides whether any proof is correct \nor not, and the present work will use in an essential way such a program, namely \nCoq, to formally prove the correctness of some particular program. \nAn important program property, especially in systems with strong time constraints, \nis termination: will the program always provide an answer? (Un)fortunately, \nas shown by Turing, termination is not decidable: there is no \nTuring-machine that, for any pair (p; i) made of a finite program description \np and a finite input i for p, can tell in a finite amount of time whether p terminates \non i or not. This led to the development of many different heuristics \nand tools (e.g. AProVE, TTT2, . . . ) for trying to prove the termination \nof programs. In particular, term rewriting theory, introduced by \nKnuth as a tool for deciding algebraic equational theories, provides a general \nframework for studying program termination with applications to concrete \nprogramming languages like Prolog, Haskell, Java, . . . This \nis the framework that we will consider in this work. \nBut, in turn, how to guarantee that a program implementing such a heuristic \nis correct? One way to break this vicious circle relies on the fact that, for a given \nproblem, checking that a solution is correct is usually easier than finding such \na solution, because the latter involves some back-tracking mechanism while the \nformer only requires blind computations. Hence, we can imagine the following \nscenario: modify the tool so that it does not only answer YES or NO, but also \noutputs some data, called certificate, that can be used to verify the correctness \nof the answer; and design these certificates in such a way that their verification \nis amenable to a complete formalization and correctness proof. Althought it \nseems to only move the problem from one program to another, the certificate \nverifier, there is in fact a gain in complexity. For instance, finding a boolean \nassignment satisfying some boolean formula (SAT problem) is (in the worst case) \nexponential in the number of boolean variables, while verifying the correctness \nof a given assignment (the certificate) is linear in the size of the formula. This \nis the approach that various automated provers for the termination of firstorder \nterm rewrite systems started to adopt in 2007. A common formal \nlanguage, CPF (Certification Problem Format), has then been designed \nfor representing termination certificates, and certificate verifiers started to be \ndeveloped: Rainbow, CiME3 and CeTA. \nIn this work, we explain the development of a new, faster and formally \nproved version of Rainbow based on the extraction mechanism of Coq. The \nprevious version of Rainbow verified a CPF file in two steps (see Figure 1.1). \nFirst, it used a non-certified OCaml program to translate a CPF file into a \nCoq script, using the Coq libraries on rewriting theory and termination CoLoR \nand Coccinelle. Second, it called Coq to check the correctness \nof the script. This approach is interesting for it provides a way to reuse in \nCoq termination proofs generated by external tools. This is also the approach \nfollowed by CiME3. However, it suffers from a number of deficiencies. First, \nbecause in Coq functions are interpreted, computation is much slower than with \nprograms written in a standard programming language and compiled into binary \ncode. Second, because the translation from CPF to Coq is not certified, it may \ncontain errors and either lead to the rejection of valid certificates, or to the \nacceptance of wrong certificates. Moreover, the data type used for representing \ncertificates internally and the parsing function used to create a value of this data \ntype from a text file are written by hand. This is a possible source of errors \nand is time-consuming. To solve the latter problem, one needs to define and \nformally prove the correctness of a function checking whether a certificate is \nvalid or not. To avoid the problem of parser, one can develop a simple compiler \nfor generating a Coq data type definition for representing XML Schema data \ntypes, and an XML parser for CPF. To solve the former problem, \none needs to compile this function to binary code. The present work shows how \nto solve these two problems by using the proof assistant Coq and its extraction \nmechanism to the programming language OCaml. Indeed, data structures \nand functions defined in Coq can be translated to OCaml and then compiled to \nbinary code by using the OCaml compiler. A similar approach was first initiated \nin CeTA using the Isabelle proof assistant.", "venue": "ArXiv", "authors": ["Fr\u00e9d\u00e9ric  Blanqui", "Kim Quyen Ly"], "year": 2012, "n_citations": 3}
{"id": 2905069, "s2_id": "9dad9b6a619df76e50901ad0852abb481cfa5d0f", "title": "Using a Template Engine as a Computer Algebra Tool", "abstract": "Abstract In research problems that involve the use of numerical methods for solving systems of ordinary differential equations (ODEs), it is often required to select the most efficient method for a particular problem. To solve a Cauchy problem for a system of ODEs, Runge\u2013Kutta methods (explicit or implicit ones, with or without step-size control, etc.) are employed. In that case, it is required to search through many implementations of the numerical method and select coefficients or other parameters of its numerical scheme. This paper proposes a library and scripts for automated generation of routine functions in the Julia programming language for a set of numerical schemes of Runge\u2013Kutta methods. For symbolic manipulations, we use a template substitution tool. The proposed approach to automated generation of program code allows us to use a single template for editing, instead of modifying each individual function to be compared. On the one hand, this provides universality in the implementation of a numerical scheme and, on the other hand, makes it possible to minimize the number of errors in the process of modifying the compared implementations of the numerical method. We consider Runge\u2013Kutta methods without step-size control, embedded methods with step-size control, and Rosenbrock methods with step-size control. The program codes for the numerical schemes, which are generated automatically using the proposed library, are tested by solving numerically several well-known problems.", "venue": "Program. Comput. Softw.", "authors": ["M. N. Gevorkyan", "A. V. Korol\u2019kova", "D. S. Kulyabov"], "year": 2021, "n_citations": 0}
{"id": 2907733, "s2_id": "5dd76a8daaf4c9632eeedc9d76b9f14de986dc3b", "title": "qTorch: The quantum tensor contraction handler", "abstract": "Classical simulation of quantum computation is necessary for studying the numerical behavior of quantum algorithms, as there does not yet exist a large viable quantum computer on which to perform numerical tests. Tensor network (TN) contraction is an algorithmic method that can efficiently simulate some quantum circuits, often greatly reducing the computational cost over methods that simulate the full Hilbert space. In this study we implement a tensor network contraction program for simulating quantum circuits using multi-core compute nodes. We show simulation results for the Max-Cut problem on 3- through 7-regular graphs using the quantum approximate optimization algorithm (QAOA), successfully simulating up to 100 qubits. We test two different methods for generating the ordering of tensor index contractions: one is based on the tree decomposition of the line graph, while the other generates ordering using a straight-forward stochastic scheme. Through studying instances of QAOA circuits, we show the expected result that as the treewidth of the quantum circuit\u2019s line graph decreases, TN contraction becomes significantly more efficient than simulating the whole Hilbert space. The results in this work suggest that tensor contraction methods are superior only when simulating Max-Cut/QAOA with graphs of regularities approximately five and below. Insight into this point of equal computational cost helps one determine which simulation method will be more efficient for a given quantum circuit. The stochastic contraction method outperforms the line graph based method only when the time to calculate a reasonable tree decomposition is prohibitively expensive. Finally, we release our software package, qTorch (Quantum TensOR Contraction Handler), intended for general quantum circuit simulation. For a nontrivial subset of these quantum circuits, 50 to 100 qubits can easily be simulated on a single compute node.", "venue": "PloS one", "authors": ["E. Schuyler Fried", "Nicolas P. D. Sawaya", "Yudong  Cao", "Ian D. Kivlichan", "Jhonathan  Romero", "Al\u00e1n  Aspuru-Guzik"], "year": 2018, "n_citations": 38}
{"id": 2907759, "s2_id": "8d9c5042c2065020faabebbd3b1e6196fe5b8053", "title": "Acceleration of multiple precision matrix multiplication based on multi-component floating-point arithmetic using AVX2", "abstract": "In this paper, we report the results obtained from the acceleration of multi-binary64-type multiple precision matrix multiplication with AVX2. We target double-double (DD), triple-double (TD), and quad-double (QD) precision arithmetic designed by certain types of error-free transformation (EFT) arithmetic. Furthermore, we implement SIMDized EFT functions, which simultaneously compute with four binary64 numbers on x86 64 computing environment, and by using help of them, we also develop SIMDized DD, TD, and QD additions and multiplications. In addition, AVX2 load/store functions were adopted to efficiently speed up reading and storing matrix elements from/to memory. Owing to these combined techniques, our implemented multiple precision matrix multiplications have been accelerated more than three times compared with non-accelerated ones. Our accelerated matrix multiplication modifies the performance of parallelization with OpenMP.", "venue": "ICCSA", "authors": ["Tomonori  Kouya"], "year": 2021, "n_citations": 2}
{"id": 2920078, "s2_id": "1b8628b88893f2bde1cf920e812db01fc93dd776", "title": "Software system design based on patterns for Newton-type methods", "abstract": "A wide range of engineering applications use optimization techniques as part of their solution process. The researcher uses specialized software that implements well-known optimization techniques to solve his problem. However, when it comes to develop original optimization techniques that fit a particular problem the researcher has no option but to implement his own new method from scratch. This leads to large development times and error prone code that, in general, will not be reused for any other application. In this work, we present a novel methodology that simplifies, speeds up and improves the development process of scientific software. This methodology guides us on the identification of design patterns. The application of this methodology generates reusable, flexible and high quality scientific software. Furthermore, the produced software becomes a documented tool to transfer the knowledge on the development process of scientific software. We apply this methodology for the design of an optimization framework implementing Newton\u2019s type methods which can be used as a fast prototyping tool of new optimization techniques based on Newton\u2019s type methods. The abstraction, re-useability and flexibility of the developed framework is measured by means of Martin\u2019s metric. The results indicate that the developed software is highly reusable.", "venue": "Computing", "authors": ["Ricardo  Serrato-Barrera", "Gustavo  Rodr\u00edguez-G\u00f3mez", "Julio C\u00e9sar P\u00e9rez-Sansalvador", "Sa\u00fal  Pomares-Hern\u00e1ndez", "Leticia  Flores-Pulido", "Antonio  Mu\u00f1oz"], "year": 2019, "n_citations": 0}
{"id": 2925555, "s2_id": "822f9271466004b34a193072fb55ae6af77e363c", "title": "Simulation of Self-Assembly in the Abstract Tile Assembly Model with ISU TAS", "abstract": "Since its introduction by Erik Winfree in 1998, the abstract Tile Assembly Model (aTAM) has inspired a wealth of research. As an abstract model for tile based self-assembly, it has proven to be remarkably powerful and expressive in terms of the structures which can self-assemble within it. As research has progressed in the aTAM, the self-assembling structures being studied have become progressively more complex. This increasing complexity, along with a need for standardization of definitions and tools among researchers, motivated the development of the Iowa State University Tile Assembly Simulator (ISU TAS). ISU TAS is a graphical simulator and tile set editor for designing and building 2-D and 3-D aTAM tile assembly systems and simulating their self-assembly. This paper reviews the features and functionality of ISU TAS and describes how it can be used to further research into the complexities of the aTAM. Software and source code are available at this http URL", "venue": "ArXiv", "authors": ["Matthew J. Patitz"], "year": 2011, "n_citations": 32}
{"id": 2929005, "s2_id": "1aa50c254279224dec2e9d32c18bd7f1fc11b176", "title": "rlsm: R package for least squares Monte Carlo", "abstract": "This short paper briefly describes the implementation of the least squares Monte Carlo method in the rlsm package. This package provides users with an easy manner to experiment with the large amount of R regression tools on any regression basis and reward functions. This package also computes lower and upper bounds for the true value function via duality methods.", "venue": "ArXiv", "authors": ["Jeremy  Yee"], "year": 2018, "n_citations": 1}
{"id": 2930631, "s2_id": "9a9b60125600d109f8d5550dd8ccc18d4235912c", "title": "Automated Code Generation for Discontinuous Galerkin Methods", "abstract": "A compiler approach for generating low-level computer code from high-level input for discontinuous Galerkin finite element forms is presented. The input language mirrors conventional mathematical notation, and the compiler generates efficient code in a standard programming language. This facilitates the rapid generation of efficient code for general equations in varying spatial dimensions. Key concepts underlying the compiler approach and the automated generation of computer code are elaborated. The approach is demonstrated for a range of common problems, including the Poisson, biharmonic, advection-diffusion, and Stokes equations.", "venue": "SIAM J. Sci. Comput.", "authors": ["Kristian B. \u00d8lgaard", "Anders  Logg", "Garth N. Wells"], "year": 2008, "n_citations": 53}
{"id": 2934078, "s2_id": "bea4e8780d2869fd3f360a9d12409af45aa045ef", "title": "Semi-Analytical Computation of Acoustic Scattering by Spheroids and Disks", "abstract": "Analytical solutions to acoustic scattering problems involving spheroids and disks have long been known and have many applications. However, these solutions require special functions that are not easily computable. Therefore, their asymptotic forms are typically used instead since they are more readily available. In this paper, these solutions are explored, and computational software is provided for calculating their nonasymptotic forms, which are accurate over a wide range of frequencies and distances. This software, which runs in MATLAB, computes the solutions to acoustic scattering problems involving spheroids and disks by semi-analytical means, and is freely available from the authors' webpage.", "venue": "The Journal of the Acoustical Society of America", "authors": ["Ross  Adelman", "Nail A. Gumerov", "Ramani  Duraiswami"], "year": 2014, "n_citations": 11}
{"id": 2937497, "s2_id": "a86e9254adb19d4661653f8e2a7941af534a1a7e", "title": "Toward Performance-Portable PETSc for GPU-based Exascale Systems", "abstract": "The Portable Extensible Toolkit for Scientific computation (PETSc) library delivers scalable solvers for nonlinear time-dependent differential and algebraic equations and for numerical optimization.The PETSc design for performance portability addresses fundamental GPU accelerator challenges and stresses flexibility and extensibility by separating the programming model used by the application from that used by the library, and it enables application developers to use their preferred programming model, such as Kokkos, RAJA, SYCL, HIP, CUDA, or OpenCL, on upcoming exascale systems. A blueprint for using GPUs from PETSc-based codes is provided, and case studies emphasize the flexibility and high performance achieved on current GPU-based systems.", "venue": "Parallel Comput.", "authors": ["Richard Tran Mills", "Mark F. Adams", "Satish  Balay", "Jed  Brown", "Alp  Dener", "Matthew  Knepley", "Scott E. Kruger", "Hannah  Morgan", "Todd  Munson", "Karl  Rupp", "Barry F. Smith", "Stefano  Zampini", "Hong  Zhang", "Junchao  Zhang"], "year": 2021, "n_citations": 3}
{"id": 2939706, "s2_id": "a3dc0bf91d913752b07e173bbdadf3d549d2bb55", "title": "Bembel: The Fast Isogeometric Boundary Element C++ Library for Laplace, Helmholtz, and Electric Wave Equation", "abstract": "In this article, we present Bembel, the C++ library featuring higher order isogeometric Galerkin boundary element methods for Laplace, Helmholtz, and Maxwell problems. Bembel is compatible with geometries from the Octave NURBS package and provides an interface to the Eigen template library for linear algebra operations. For computational efficiency, it applies an embedded fast multipole method tailored to the isogeometric analysis framework and a parallel matrix assembly based on OpenMP.", "venue": "SoftwareX", "authors": ["J.  D\u00f6lz", "H.  Harbrecht", "S.  Kurz", "M.  Multerer", "S.  Sch\u00f6ps", "F.  Wolf"], "year": 2020, "n_citations": 18}
{"id": 2946925, "s2_id": "d44bb1c762acc3158d96cf4cef9dab5e754f0822", "title": "The development of a fuzzy regulator with an entry and an output in Fislab", "abstract": "The present article is a sequel of the article \"Fislab the Fuzzy Inference Tool-Box for Scilab\" and it represents the practical application of:\"The development of the Fuzzy regulator with an input and an output in Fislab\". The article contains, besides this application, some functions to be used in the program, namely Scilab functions for the fuzzification of the firm information, functions for the operation of de-fuzzification and functions for the implementation of.", "venue": "ArXiv", "authors": ["Simona  Apostol"], "year": 2009, "n_citations": 0}
{"id": 2956049, "s2_id": "30dd8eb91620b51971586368084e896c4c0b3ebd", "title": "How to obtain efficient GPU kernels: An illustration using FMM & FGT algorithms", "abstract": "Computing on graphics processors is maybe one of the most important developments in computational science to happen in decades. Not since the arrival of the Beowulf cluster, which combined open source software with commodity hardware to truly democratize high-performance computing, has the community been so electrified. Like then, the opportunity comes with challenges. The formulation of scientific algorithms to take advantage of the performance o ered by the new architecture requires rethinking core methods. Here, we have tackled fast summation algorithms (fast multipole method and fast Gauss transform), and applied algorithmic redesign for attaining performance on gpus. The progression of performance improvements attained illustrates the exercise of formulating algorithms for the massively parallel architecture of the gpu. The end result has been gpu kernels that run at over 500 Gop/s on one nvidia tesla C1060 card, thereby reaching close to practical peak.", "venue": "Comput. Phys. Commun.", "authors": ["Felipe A. Cruz", "Simon K. Layton", "Lorena A. Barba"], "year": 2011, "n_citations": 17}
{"id": 2958462, "s2_id": "5b2bf7328a79608308faad7a3495d33b0dce9e46", "title": "Complexity and algorithms for Euler characteristic of simplicial complexes", "abstract": "We consider the problem of computing the Euler characteristic of an abstract simplicial complex given by its vertices and facets. We show that this problem is #P-complete and present two new practical algorithms for computing Euler characteristic. The two new algorithms are derived using combinatorial commutative algebra and we also give a second description of them that requires no algebra. We present experiments showing that the two new algorithms can be implemented to be faster than previous Euler characteristic implementations by a large margin.", "venue": "J. Symb. Comput.", "authors": ["Bjarke Hammersholt Roune", "Eduardo  S\u00e1enz-de-Cabez\u00f3n"], "year": 2013, "n_citations": 11}
{"id": 2961571, "s2_id": "1ec2ff4f94d76e652e69452e143bc988a14eaf9b", "title": "Optimizer Fusion: Efficient Training with Better Locality and Parallelism", "abstract": "Machine learning frameworks adopt iterative optimizers to train neural networks. Conventional eager execution separates the updating of trainable parameters from forward and backward computations. However, this approach introduces nontrivial training time overhead due to the lack of data locality and computation parallelism. In this work, we propose to fuse the optimizer with forward or backward computation to better leverage locality and parallelism during training. By reordering the forward computation, gradient calculation, and parameter updating, our proposed method improves the efficiency of iterative optimizers. Experimental results demonstrate that we can achieve an up to 20% training time reduction on various configurations. Since our methods do not alter the optimizer algorithm, they can be used as a general \u201cplug-in\u201d technique to the training process.", "venue": "ArXiv", "authors": ["Zixuan  Jiang", "Jiaqi  Gu", "Mingjie  Liu", "Keren  Zhu", "David Z. Pan"], "year": 2021, "n_citations": 0}
{"id": 2964937, "s2_id": "47a89dec1412646c3c14a77caf34f6c09d4a2aca", "title": "Reducing local minima in fitness landscapes of parameter estimation by using piecewise evaluation and state estimation", "abstract": "Ordinary differential equations (ODE) are widely used for modeling in Systems Biology. As most commonly only some of the kinetic parameters are measurable or precisely known, parameter estimation techniques are applied to parametrize the model to experimental data. A main challenge for the parameter estimation is the complexity of the parameter space, especially its high dimensionality and local minima. \nParameter estimation techniques consist of an objective function, measuring how well a certain parameter set describes the experimental data, and an optimization algorithm that optimizes this objective function. A lot of effort has been spent on developing highly sophisticated optimization algorithms to cope with the complexity in the parameter space, but surprisingly few articles address the influence of the objective function on the computational complexity in finding global optima. We extend a recently developed multiple shooting for stochastic systems (MSS) objective function for parameter estimation of stochastic models and apply it to parameter estimation of ODE models. This MSS objective function treats the intervals between measurement points separately. This separate treatment allows the ODE trajectory to stay closer to the data and we show that it reduces the complexity of the parameter space. \nWe use examples from Systems Biology, namely a Lotka-Volterra model, a FitzHugh-Nagumo oscillator and a Calcium oscillation model, to demonstrate the power of the MSS approach for reducing the complexity and the number of local minima in the parameter space. The approach is fully implemented in the COPASI software package and, therefore, easily accessible for a wide community of researchers.", "venue": "ArXiv", "authors": ["Christoph  Zimmer", "Frank T. Bergmann", "Sven  Sahle"], "year": 2016, "n_citations": 3}
{"id": 2966058, "s2_id": "3ea665c04e43c0fba31e8467518c75574ce8be1c", "title": "Hybridization of Interval CP and Evolutionary Algorithms for Optimizing Difficult Problems", "abstract": "The only rigorous approaches for achieving a numerical proof of optimality in global optimization are interval-based methods that interleave branching of the search-space and pruning of the subdomains that cannot contain an optimal solution. State-of-the-art solvers generally integrate local optimization algorithms to compute a good upper bound of the global minimum over each subspace. In this document, we propose a cooperative framework in which interval methods cooperate with evolutionary algorithms. The latter are stochastic algorithms in which a population of candidate solutions iteratively evolves in the search-space to reach satisfactory solutions. \nWithin our cooperative solver Charibde, the evolutionary algorithm and the interval-based algorithm run in parallel and exchange bounds, solutions and search-space in an advanced manner via message passing. A comparison of Charibde with state-of-the-art interval-based solvers (GlobSol, IBBA, Ibex) and NLP solvers (Couenne, BARON) on a benchmark of difficult COCONUT problems shows that Charibde is highly competitive against non-rigorous solvers and converges faster than rigorous solvers by an order of magnitude.", "venue": "CP", "authors": ["Charlie  Vanaret", "Jean-Baptiste  Gotteland", "Nicolas  Durand", "Jean-Marc  Alliot"], "year": 2015, "n_citations": 3}
{"id": 2966433, "s2_id": "fa8da095b5f6b940f95f989d19a802c1ec559529", "title": "Numerical Calculation With Arbitrary Precision", "abstract": "The vast use of computers on scientific numerical computation makes the awareness of the limited precision that these machines are able to provide us an essential matter. A limited and insufficient precision allied to the truncation and rounding errors may induce the user to incorrect interpretation of his/hers answer. In this work, we have developed a computational package to minimize this kind of error by offering arbitrary precision numbers and calculation. This is very important in Physics where we can work with numbers too small and too big simultaneously.", "venue": "ArXiv", "authors": ["B. O. Rodrigues", "L. A. C. P. da Mota", "L. G. S. Duarte"], "year": 2007, "n_citations": 0}
{"id": 2969011, "s2_id": "416c7edddc2c4cf0761b39196e7433813851c9e5", "title": "ODE Test Problems: a MATLAB suite of initial value problems", "abstract": "ODE Test Problems (OTP) is an object-oriented MATLAB package offering a broad range of initial value problems which can be used to test numerical methods such as time integration methods and data assimilation (DA) methods. It includes problems that are linear and nonlinear, homogeneous and nonhomogeneous, autonomous and nonautonomous, scalar and high-dimensional, stiff and nonstiff, and chaotic and nonchaotic. Many are real-world problems from fields such as chemistry, astrophysics, meteorology, and electrical engineering. OTP also supports partitioned ODEs for testing IMEX methods, multirate methods, and other multimethods. Functions for plotting solutions and creating movies are available for all problems, and exact solutions are provided when available. OTP is desgined for ease of use-meaning that working with and modifying problems is simple and intuitive.", "venue": "ArXiv", "authors": ["Steven  Roberts", "Andrey A. Popov", "Adrian  Sandu"], "year": 2019, "n_citations": 13}
{"id": 2970384, "s2_id": "b17be31f1901d5dc31bf6f0f70443ab034da8c89", "title": "Decomposition algorithms for tensors and polynomials", "abstract": "We give algorithms to compute decompositions of a given polynomial, or more generally mixed tensor, as sum of rank one tensors, and to establish whether such a decomposition is unique. In particular, we present methods to compute the decomposition of a general plane quintic in seven powers, and of a general space cubic in five powers; the two decompositions of a general plane sextic of rank nine, and the five decompositions of a general plane septic. Furthermore, we give Magma implementations of all our algorithms.", "venue": "ArXiv", "authors": ["Antonio  Laface", "Alex  Massarenti", "Rick  Rischter"], "year": 2021, "n_citations": 0}
{"id": 2971218, "s2_id": "06db6cb75bdd7a5bac8898df99d319c0b0688244", "title": "A Maple Package for Computing Groebner Bases for Linear Recurrence Relations", "abstract": "A Maple package for computing Grobner bases of linear difference ideals is described. The underlying algorithm is based on Janet and Janet-like monomial divisions associated with finite difference operators. The package can be used, for example, for automatic generation of difference schemes for linear partial differential equations and for reduction of multiloop Feynman integrals. These two possible applications are illustrated by simple examples of the Laplace equation and a one-loop scalar integral of propagator type.", "venue": "ArXiv", "authors": ["Vladimir P. Gerdt", "Daniel  Robertz"], "year": 2005, "n_citations": 22}
{"id": 2975691, "s2_id": "3239233b0efa53fbe5e8dd50094310beb6afbb83", "title": "Designing and building the mlpack open-source machine learning library", "abstract": "mlpack is an open-source C++ machine learning library with an emphasis on speed and flexibility. Since its original inception in 2007, it has grown to be a large project implementing a wide variety of machine learning algorithms, from standard techniques such as decision trees and logistic regression to modern techniques such as deep neural networks as well as other recently-published cutting-edge techniques not found in any other library. mlpack is quite fast, with benchmarks showing mlpack outperforming other libraries' implementations of the same methods. mlpack has an active community, with contributors from around the world---including some from PUST. This short paper describes the goals and design of mlpack, discusses how the open-source community functions, and shows an example usage of mlpack for a simple data science problem.", "venue": "ArXiv", "authors": ["Ryan R. Curtin", "Marcus  Edel"], "year": 2017, "n_citations": 3}
{"id": 2979796, "s2_id": "f24baf796b66e73cda28c1859e20bd463dc98288", "title": "Parallel Matrix-Free Implementation of Frequency-Domain Finite Difference Methods for Cluster Computing", "abstract": "Full-wave 3D electromagnetic simulations of complex planar devices, multilayer interconnects, and chip packages are presented for wide-band frequency-domain analysis using the finite difference integration technique developed in the PETSc software package. Initial reordering of the index assignment to the unknowns makes the resulting system matrix diagonally dominant. The rearrangement also facilitates the decomposition of large domain into slices for passing the mesh information to different machines. Matrix-free methods are then exploited to minimize the number of element-wise multiplications and memory requirements in the construction of the system of linear equations. Besides, the recipes provide extreme ease of modifications in the kernel of the code. The applicability of different Krylov subspace solvers is investigated. The accuracy is checked through comparisons with CST MICROWAVE STUDIO transient solver results. The parallel execution of the compiled code on specific number of processors in multi-core distributed-memory architectures demonstrate high scalability of the computational algorithm.", "venue": "ArXiv", "authors": ["Amir  Geranmayeh"], "year": 2017, "n_citations": 0}
{"id": 2979803, "s2_id": "a117bd4932ced1aba959808fe3e8452c09cfdbe4", "title": "Lipschitz gradients for global optimization in a one-point-based partitioning scheme", "abstract": "A global optimization problem is studied where the objective function f(x) is a multidimensional black-box function and its gradient f^'(x) satisfies the Lipschitz condition over a hyperinterval with an unknown Lipschitz constant K. Different methods for solving this problem by using an a priori given estimate of K, its adaptive estimates, and adaptive estimates of local Lipschitz constants are known in the literature. Recently, the authors have proposed a one-dimensional algorithm working with multiple estimates of the Lipschitz constant for f^'(x) (the existence of such an algorithm was a challenge for 15 years). In this paper, a new multidimensional geometric method evolving the ideas of this one-dimensional scheme and using an efficient one-point-based partitioning strategy is proposed. Numerical experiments executed on 800 multidimensional test functions demonstrate quite a promising performance in comparison with popular DIRECT-based methods.", "venue": "J. Comput. Appl. Math.", "authors": ["Dmitri E. Kvasov", "Yaroslav D. Sergeyev"], "year": 2012, "n_citations": 54}
{"id": 2982001, "s2_id": "659dfb80577ba30d7fd6306b4fe216628e7c5bf8", "title": "Estudo e Implementacao de Algoritmos de Roteamento sobre Grafos em um Sistema de Informacoes Geograficas", "abstract": "This article presents an implementation of a graphical software with various algorithms in Operations research, like minimum path, minimum tree, chinese postman problem and travelling salesman.", "venue": "ArXiv", "authors": ["Rudini Menezes Sampaio", "Horacio Hideki Yanasse"], "year": 2005, "n_citations": 0}
{"id": 2988484, "s2_id": "4921051698261c2a93f391683ae8d91c0d29a506", "title": "Enhancements to ACL2 in Versions 6.2, 6.3, and 6.4", "abstract": "We report on improvements to ACL2 made since the 2013 ACL2 Workshop.", "venue": "ACL2", "authors": ["Matt  Kaufmann", "J Strother Moore"], "year": 2014, "n_citations": 2}
{"id": 2999650, "s2_id": "f1af22e570c49c9c1a5fcdc36cafe9d0a02f10cf", "title": "Test Problem Construction for Single-Objective Bilevel Optimization", "abstract": "In this paper, we propose a procedure for designing controlled test problems for single-objective bilevel optimization. The construction procedure is flexible and allows its user to control the different complexities that are to be included in the test problems independently of each other. In addition to properties that control the difficulty in convergence, the procedure also allows the user to introduce difficulties caused by interaction of the two levels. As a companion to the test problem construction framework, the paper presents a standard test suite of 12 problems, which includes eight unconstrained and four constrained problems. Most of the problems are scalable in terms of variables and constraints. To provide baseline results, we have solved the proposed test problems using a nested bilevel evolutionary algorithm. The results can be used for comparison, while evaluating the performance of any other bilevel optimization algorithm. The code related to the paper may be accessed from the website http://bilevel.org.", "venue": "Evolutionary Computation", "authors": ["Ankur  Sinha", "Pekka  Malo", "Kalyanmoy  Deb"], "year": 2014, "n_citations": 60}
{"id": 3001267, "s2_id": "1dbb89efd5c6b541accf867ff18647888ae2567f", "title": "SPUX Framework: a Scalable Package for Bayesian Uncertainty Quantification and Propagation", "abstract": "We present SPUX a modular framework for Bayesian inference enabling uncertainty quantification and propagation in linear and nonlinear, deterministic and stochastic models, and supporting Bayesian model selection. SPUX can be coupled to any serial or parallel application written in any programming language, (e.g. including Python, R, Julia, C/C++, Fortran, Java, or a binary executable), scales effortlessly from serial runs on a personal computer to parallel high performance computing clusters, and aims to provide a platform particularly suited to support and foster reproducibility in computational science. We illustrate SPUX capabilities for a simple yet representative random walk model, describe how to couple different types of user applications, and showcase several readily available examples from environmental sciences. In addition to available state-of-the-art numerical inference algorithms including EMCEE, PMCMC (PF) and SABC, the open source nature of the SPUX framework and the explicit description of the hierarchical parallel SPUX executors should also greatly simplify the implementation and usage of other inference and optimization techniques.", "venue": "ArXiv", "authors": ["Jonas  vSukys", "Marco  Bacci"], "year": 2021, "n_citations": 0}
{"id": 3002526, "s2_id": "1a21c3258a359d49ae36b30f62270bcbc282637d", "title": "One Method for Proving Inequalities by Computer", "abstract": "We consider a numerical method for proving a class of analytical inequalities via minimax rational approximations. All numerical calculations in this paper are given by Maple computer program.", "venue": "ArXiv", "authors": ["Branko J. Malesevic"], "year": 2006, "n_citations": 25}
{"id": 3003279, "s2_id": "c2717add615f51b3606ab0a2d20f7b0c2e3b0f17", "title": "Making the case of GPUs in courses on computational physics", "abstract": "The process of materials fracture is not yet understood across all levels. This thesis contains detailed description on a model of in-plane fracture along with results obtained using this model. The results from the model are in very good agreement with experimental observations, both with respect to the static scaling of the front (morphology) and a dynamic study of the underlying processes. This is quite remarkable, considering our model is quasistatic, meaning that the dynamics are time independent.Using this model, I have found two scaling regimes which corresponds to the two different regimes found experimentally for in-plane fracture. This is the first model to successfully reproduce these two scaling regimes, allowing us to clearly state the important processes in this constrained form of fracture. Only the geometry is constrained, any material obeying the quite general assumptions in the model should contain the same processes and fracture in the same way.The results indicate that a percolation process is controlling the fracture on small scales. At larger scales, the elastic material properties leads to a stress concentration which eventually constrains damage formation to the immediate area near the fracture front. In the large scale regime I have measured a roughness exponent oflarge = 0.39 \u00b1 0.04 .In the small scale regime, I show data consistent with and present evidence based on several different analyses for a roughness exponent of small = 2/3.", "venue": "ArXiv", "authors": ["Knut Skogstrand Gjerden"], "year": 2013, "n_citations": 0}
{"id": 3008141, "s2_id": "b4be03653d043602787c2f4c5667295dc322e965", "title": "Computation of the Marcum Q-function", "abstract": "Methods and an algorithm for computing the generalized Marcum $Q-$function ($Q_{\\mu}(x,y)$) and the complementary function ($P_{\\mu}(x,y)$) are described. These functions appear in problems of different technical and scientific areas such as, for example, radar detection and communications, statistics and probability theory, where they are called the non-central chi-square or the non central gamma cumulative distribution functions. \nThe algorithm for computing the Marcum functions combines different methods of evaluation in different regions: series expansions, integral representations, asymptotic expansions, and use of three-term homogeneous recurrence relations. A relative accuracy close to $10^{-12}$ can be obtained in the parameter region $(x,y,\\mu) \\in [0,\\,A]\\times [0,\\,A]\\times [1,\\,A]$, $A=200$, while for larger parameters the accuracy decreases (close to $10^{-11}$ for $A=1000$ and close to $5\\times 10^{-11}$ for $A=10000$).", "venue": "ArXiv", "authors": ["Amparo  Gil", "Javier  Segura", "Nico M. Temme"], "year": 2013, "n_citations": 17}
{"id": 3011860, "s2_id": "2f97bab93123d909b5f4bf251496467df5f06d55", "title": "The M4RIE library for dense linear algebra over small fields with even characteristic", "abstract": "We describe algorithms and implementations for linear algebra with dense matrices over F<sub>2</sub>e for 2 \u2264 <i>e</i> \u2264 10. Our main contributions are: (1) a specialisation of precomputation tables to F<sub>2</sub>e, called Newton-John tables in this work, to avoid scalar multiplications in Gaussian elimination and matrix multiplication, (2) an efficient implementation of Karatsuba-style multiplication for matrices over extension fields of F<sub>2</sub> and (3) a description of an open-source library -- called M4RIE -- providing the fastest known implementation of dense linear algebra over F<sub>2</sub>e with 2 \u2264 <i>e</i> \u2264 10.", "venue": "ISSAC", "authors": ["Martin R. Albrecht"], "year": 2012, "n_citations": 11}
{"id": 3012194, "s2_id": "31ad1d344abf0ef0d35c12cf018f4dc2595a95b0", "title": "The Creation of Puffin, the Automatic Uncertainty Compiler", "abstract": "An uncertainty compiler is a tool that automatically translates original computer source code lacking explicit uncertainty analysis into code containing appropriate uncertainty representations and uncertainty propagation algorithms. We have developed an prototype uncertainty compiler along with an associated object-oriented uncertainty language in the form of a stand-alone Python library. It handles the specifications of input uncertainties and inserts calls to intrusive uncertainty quantification algorithms in the library. The uncertainty compiler can apply intrusive uncertainty propagation methods to codes or parts of codes and therefore more comprehensively and flexibly address both epistemic and aleatory uncertainties.", "venue": "ArXiv", "authors": ["Nicholas  Gray", "Marco De Angelis", "Scott  Ferson"], "year": 2021, "n_citations": 0}
{"id": 3012864, "s2_id": "58ec0dd296365b050903f4cd71600005106bd952", "title": "Computing with Harmonic Functions", "abstract": "This document is the manual for a free Mathematica package for computing with harmonic functions. This package allows the user to make calculations that would take a prohibitive amount of time if done without a computer. For example, the Poisson integral of any polynomial can be computed exactly. This software can find exact solutions to Dirichlet, Neumann, and biDirichlet problems in R^n with polynomial data on balls, ellipsoids, and annular regions. It can also find bases for spaces of spherical harmonics, compute projections onto the harmonic Bergman space, and perform other manipulations with harmonic functions.", "venue": "ArXiv", "authors": ["Sheldon  Axler"], "year": 2015, "n_citations": 1}
{"id": 3015338, "s2_id": "edd11d0f7621dae3d7bc3093215cf7961c2c491b", "title": "Novel Matrix Hit and Run for Sampling Polytopes and Its GPU Implementation", "abstract": "We propose and analyze a new Markov Chain Monte Carlo algorithm that generates a uniform sample over full and non-full dimensional polytopes. This algorithm, termed \u201dMatrix Hit and Run\u201d (MHAR), is a modification of the Hit and Run framework. For the regime n 1 3 m, MHAR has a lower asymptotic cost per sample in terms of soft-O notation (O\u2217) than do existing sampling algorithms after a warm start. MHAR is designed to take advantage of matrix multiplication routines that require less computational and memory resources. Our tests show this implementation to be substantially faster than the hitandrun R package, especially for higher dimensions. Finally, we provide a python library based on Pytorch and a Colab notebook with the implementation ready for deployment in architectures with GPU or just CPU.", "venue": "ArXiv", "authors": ["Mario Vazquez Corte", "Luis V. Montiel"], "year": 2021, "n_citations": 0}
{"id": 3019484, "s2_id": "2d3a862ca06178a6418d9812b1adec2a27239f42", "title": "A modified ziggurat algorithm for generating exponentially and normally distributed pseudorandom numbers", "abstract": "The ziggurat algorithm is a very fast rejection sampling method for generating pseudorandom numbers (PRNs) from statistical distributions. In the algorithm, rectangular sampling domains are layered on top of each other (resembling a ziggurat) to encapsulate the desired probability density function. Random values within these layers are sampled and then returned if they lie beneath the graph of the probability density function. Here, we present an implementation where ziggurat layers reside completely beneath the probability density function, thereby eliminating the need for any rejection test within the ziggurat layers. In the new algorithm, small overhanging segments of probability density remain to the right of each ziggurat layer, which can be efficiently sampled with triangularly shaped sampling domains. Median runtimes of the new algorithm for exponential and normal variates is reduced to 58% and 53%, respectively (collective range: 41\u201393%). An accessible C library, along with extensions into Python and MATLAB/Octave are provided.", "venue": "Journal of statistical computation and simulation", "authors": ["Christopher D. McFarland"], "year": 2016, "n_citations": 4}
{"id": 3019954, "s2_id": "d1bd41ef76b9f277ece037f6868dffb0c3937ceb", "title": "Approaches to the implementation of generalized complex numbers in the Julia language", "abstract": "In problems of mathematical physics, to study the structures of spaces using the Cayley-Klein models in theoretical calculations, the use of generalized complex numbers is required. In the case of computational experiments, such tasks require their high-quality implementation in a programming language. The proposed small implementation of generalized complex numbers in modern programming languages have several disadvantages. In this article, we propose using the Julia language as the language for implementing generalized complex numbers, not least because it supports the multiple dispatch mechanism. The paper demonstrates the approach to the implementation of one of the types of generalized complex numbers, namely dual numbers. We place particular emphasis on the description of the use of the multiple dispatch mechanism to implement a new numerical type. The resulting implementation of dual numbers can be considered as a prototype for a complete software module for supporting generalized complex numbers.", "venue": "ITTMM", "authors": ["Migran N. Gevorkyan", "Anna V. Korolkova", "Dmitry S. Kulyabov"], "year": 2020, "n_citations": 0}
{"id": 3026877, "s2_id": "0164ecbc6228222e60a432001af7248c5db63db5", "title": "The deal.II Library, Version 8.1", "abstract": "This paper provides an overview of the new features of the finite element library deal.II version 8.1.", "venue": "ArXiv", "authors": ["Wolfgang  Bangerth", "Timo  Heister", "Luca  Heltai", "Guido  Kanschat", "Martin  Kronbichler", "Matthias  Maier", "Bruno  Turcksin", "Toby D. Young"], "year": 2013, "n_citations": 109}
{"id": 3029580, "s2_id": "843f2759c7a84d03acb9d3f557662889a2c01301", "title": "The landscape of software for tensor computations", "abstract": "Tensors (also commonly seen as multi-linear operators or as multi-dimensional arrays) are ubiquitous in scientific computing and in data science, and so are the software efforts for tensor operations. Particularly in recent years, we have observed an explosion in libraries, compilers, packages, and toolboxes; unfortunately these efforts are very much scattered among the different scientific domains, and inevitably suffer from replication, suboptimal implementations, and in many cases, limited visibility. As a first step towards countering these inefficiencies, here we survey and loosely classify software packages related to tensor computations. Our aim is to assemble a comprehensive and up-to-date snapshot of the tensor software landscape, with the intention of helping both users and developers. Aware of the difficulties inherent in any multi-discipline survey, we very much welcome the reader\u2019s help in amending and expanding our software list, which currently features 72 projects.", "venue": "ArXiv", "authors": ["Christos  Psarras", "Lars  Karlsson", "Paolo  Bientinesi"], "year": 2021, "n_citations": 4}
{"id": 3034638, "s2_id": "83c0e08d85431344736e20a796d1c57052d34be3", "title": "On Designing GPU Algorithms with Applications to Mesh Refinement", "abstract": "We present a set of rules to guide the design of GPU algorithms. These rules are grounded on the principle of reducing waste in GPU utility to achieve good speed up. In accordance to these rules, we propose GPU algorithms for 2D constrained, 3D constrained and 3D Restricted Delaunay refinement problems respectively. Our algorithms take a 2D planar straight line graph (PSLG) or 3D piecewise linear complex (PLC) $\\mathcal{G}$ as input, and generate quality meshes conforming or approximating to $\\mathcal{G}$. The implementation of our algorithms shows that they are the first to run an order of magnitude faster than current state-of-the-art counterparts in sequential and parallel manners while using similar numbers of Steiner points to produce triangulations of comparable qualities. It thus reduces the computing time of mesh refinement from possibly hours to a few seconds or minutes for possible use in interactive graphics applications.", "venue": "ArXiv", "authors": ["Zhenghai  Chen", "Tiow-Seng  Tan", "Hong-Yang  Ong"], "year": 2020, "n_citations": 0}
{"id": 3042544, "s2_id": "30f291f5008d4da8b012b9c00e8fc7e199e18b02", "title": "Devito: Automated Fast Finite Difference Computation", "abstract": "Domain specific languages have successfully been used in a variety of fields to cleanly express scientific problems as well as to simplify implementation and performance optimization on different computer architectures. Although a large number of stencil languages are available, finite difference domain specific languages have proved challenging to design because most practical use cases require additional features that fall outside the finite difference abstraction. Inspired by the complexity of real-world seismic imaging problems, we introduce Devito, a domain specific language in which high level equations are expressed using symbolic expressions from the SymPy package. Complex equations are automatically manipulated, optimized, and translated into highly optimized C code that aims to perform comparably or better than hand-tuned code. All this is transparent to users, who only see concise symbolic mathematical expressions.", "venue": "2016 Sixth International Workshop on Domain-Specific Languages and High-Level Frameworks for High Performance Computing (WOLFHPC)", "authors": ["Navjot  Kukreja", "Mathias  Louboutin", "Felippe  Vieira", "Fabio  Luporini", "Michael  Lange", "Gerard  Gorman"], "year": 2016, "n_citations": 23}
{"id": 3047454, "s2_id": "3866b0f3a90507cdce8a924eb3e702a4db6a7ad8", "title": "Geometric Computing with Chain Complexes: Design and Features of a Julia Package", "abstract": "Geometric computing with chain complexes allows for the computation of the whole chain of linear spaces and (co)boundary operators generated by a space decomposition into a cell complex. The space decomposition is stored and handled with LAR (Linear Algebraic Representation), i.e. with sparse integer arrays, and allows for using cells of a very general type, even non convex and with internal holes. In this paper we discuss the features and the merits of this approach, and describe the goals and the implementation of a software package aiming at providing for simple and efficient computational support of geometric computing with any kind of meshes, using linear algebra tools with sparse matrices. The library is being written in Julia, the novel efficient and parallel language for scientific computing. This software, that is being ported on hybrid architectures (CPU+GPU) of last generation, is yet under development.", "venue": "ArXiv", "authors": ["Francesco  Furiani", "Giulio  Martella", "Alberto  Paoluzzi"], "year": 2017, "n_citations": 1}
{"id": 3051630, "s2_id": "d7c2fac1fb89729921a8cfc0b5ba011d083cef98", "title": "Two-dimensional mesh generator in generalized coordinates implemented in Python", "abstract": "Through mathematical models, it is possible to turn a problem of the physical domain to the computational domain. In this context, the paper presents a two-dimensional mesh generator in generalized coordinates, which uses the Parametric Linear Spline method and partial differential equations. The generator is automated and able to treat real complex domains. The code was implemented in Python, applying the Numpy and Matplotlib libraries to matrix manipulations and graphical plots, respectively. Applications are made for monoblock meshes (two-dimensional shape of a bottle) and multiblock meshes (geometry of Igap\u00f3 I lake, Londrina, Paran\u00e1, Brazil). keyword:Automated two-dimensional mesh generator, Parametric Linear Spline, generalized coordinates, Python language, applications.", "venue": "ArXiv", "authors": ["Gustavo Taiji Naozuka", "Saulo Martiello Mastelini", "Eliandro Rodrigues Cirilo", "Neyva Maria Lopes Romeiro", "Paulo Laerte Natti"], "year": 2021, "n_citations": 0}
{"id": 3058648, "s2_id": "9508d4f9b86c24cdbfc349fa589233fd18bea313", "title": "Ideas by Statistical Mechanics (Ism)", "abstract": "Ideas by Statistical Mechanics (ISM) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions. The program is based on the author's work in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the author's Adaptive Simulated Annealing (ASA) code for optimizations of training sets, as well as for importance-sampling to apply the author's copula financial riskmanagement codes, Trading in Risk Dimensions (TRD), for assessments of risk and uncertainty. This product can be used for decision support for projects ranging from diplomatic, information, military, and economic (DIME) factors of propagation/evolution of ideas, to commercial sales, trading indicators across sectors of financial markets, advertising and political campaigns, etc. It seems appropriate to base an approach for propagation of ideas on the only system so far demonstrated to develop and nurture ideas, i.e., the neocortical brain. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, is the proposed model. ISM develops subsets of macrocolumnar activity of multivariate stochastic descriptions of defined populations, with macrocolumns defined by their local parameters within specific regions and with parameterized endogenous inter-regional and exogenous external connectivities. Parameters of subsets of macrocolumns will be fit using ASA to patterns representing ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Tools of financial risk management, developed by the author to process correlated multivariate systems with differing non-Gaussian distributions using modern copula analysis, importancesampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes.", "venue": "Trans. SDPS", "authors": ["Lester  Ingber"], "year": 2007, "n_citations": 25}
{"id": 3061755, "s2_id": "3f54553c5b3e8145d2352eea3552fa97b5189ded", "title": "NLCertify: A Tool for Formal Nonlinear Optimization", "abstract": "NLCertify is a software package for handling formal certification of nonlinear inequalities involving transcendental multivariate functions. The tool exploits sparse semialgebraic optimization techniques with approximation methods for transcendental functions, as well as formal features. Given a box and a transcendental multivariate function as input, NLCertify provides OCamllibraries that produce nonnegativity certificates for the function over the box, which can be ultimately proved correct inside the Coq proof assistant.", "venue": "ICMS", "authors": ["Victor  Magron"], "year": 2014, "n_citations": 9}
{"id": 3062312, "s2_id": "ab36437e3a3d27564b802bdf0f2d28c70707a99f", "title": "Linear solvers for power grid optimization problems: a review of GPU-accelerated linear solvers", "abstract": "The linear equations that arise in interior methods for constrained optimization are sparse symmetric indefinite and become extremely ill-conditioned as the interior method converges. These linear systems present a challenge for existing solver frameworks based on sparse LU or LDLT decompositions. We benchmark five well known direct linear solver packages using matrices extracted from power grid optimization problems. The achieved solution accuracy varies greatly among the packages. None of the tested packages delivers significant GPU acceleration for our test cases.", "venue": "Parallel Computing", "authors": ["Kasia  Swirydowicz", "Eric  Darve", "Wesley  Jones", "Jonathan  Maack", "Shaked  Regev", "Michael A. Saunders", "Stephen J. Thomas", "Slaven  Peles"], "year": 2021, "n_citations": 2}
{"id": 3069881, "s2_id": "ab629f8249a59fbd8ee50dc44d7f82de5dec8f15", "title": "Accelerating linear solvers for Stokes problems with C++ metaprogramming", "abstract": "The efficient solution of large sparse saddle point systems is very important in computational fluid mechanics. The discontinuous Galerkin finite element methods have become increasingly popular for incompressible flow problems but their application is limited due to high computational cost. We describe the C++ programming techniques that may help to accelerate linear solvers for such problems. The approach is based on the policy-based design pattern and partial template specialization, and is implemented in the open source AMGCL library. The efficiency is demonstrated with the example of accelerating an iterative solver of a discontinuous Galerkin finite element method for the Stokes problem. The implementation allows selecting algorithmic components of the solver by adjusting template parameters without any changes to the codebase. It is possible to switch to block values, or use a mixed precision solution, which results in up to 4 times speedup, and reduces the memory footprint of the algorithm by about 40\\%. We evaluate both monolithic and composite preconditioning strategies for the 3 benchmark problems. The performance of the proposed solution is compared with a multithreaded direct Pardiso solver and a parallel iterative PETSc solver.", "venue": "J. Comput. Sci.", "authors": ["Denis  Demidov", "Lin  Mu", "Bin  Wang"], "year": 2021, "n_citations": 3}
{"id": 3070534, "s2_id": "dbd07549bcdd5461cc9a5300aaf7eb3c5d5bb82b", "title": "Type-II/III DCT/DST algorithms with reduced number of arithmetic operations", "abstract": "We present algorithms for the discrete cosine transform (DCT) and discrete sine transform (DST), of types II and III, that achieve a lower count of real multiplications and additions than previously published algorithms, without sacrificing numerical accuracy. Asymptotically, the operation count is reduced from 2Nlog\"2N+O(N) to 179Nlog\"2N+O(N) for a power-of-two transform size N. Furthermore, we show that an additional N multiplications may be saved by a certain rescaling of the inputs or outputs, generalizing a well-known technique for N=8 by Arai et al. These results are derived by considering the DCT to be a special case of a DFT of length 4N, with certain symmetries, and then pruning redundant operations from a recent improved fast Fourier transform algorithm (based on a recursive rescaling of the conjugate-pair split-radix algorithm). The improved algorithms for the DCT-III, DST-II, and DST-III follow immediately from the improved count for the DCT-II.", "venue": "Signal Process.", "authors": ["Xuancheng  Shao", "Steven G. Johnson"], "year": 2008, "n_citations": 58}
{"id": 3073892, "s2_id": "ab68934973961524b0436ce484299d3f870f47b4", "title": "HLinear: Exact Dense Linear Algebra in Haskell", "abstract": "We present an implementation in the functional programming language Haskell of the PLE decomposition of matrices over division rings. Our benchmarks indicate that it is competitive with the C-based implementation provided in Flint. Describing the guiding principles of our work, we introduce the reader to basic ideas from high-performance functional programming.", "venue": "ArXiv", "authors": ["Alexandru  Ghitza", "Martin  Westerholt-Raum"], "year": 2016, "n_citations": 0}
{"id": 3078629, "s2_id": "dc92ef5c59e24fba9ca34e0f6208079923fc3b7c", "title": "YGGDRASIL-A statistical package for learning Split Models", "abstract": "There are two main objectives of this paper. The first is to present a statistical framework for models with context specific independence structures, i.e. conditional independencies holding only for specific values of the conditioning variables. This framework is constituted by the class of split models. Split models are an extension of graphical models for contingency tables and allow for a more sophisticated modelling than graphical models. The treatment of split models include estimation, representation and a Markov property for reading off those independencies holding in a specific context. The second objective is to present a software package named YGGDRASIL which is designed for statistical inference in split models, i.e. for learning such models on the basis of data.", "venue": "UAI", "authors": ["Soren  Hojsgaard"], "year": 2000, "n_citations": 1}
{"id": 3079290, "s2_id": "cda9d8e1ee0e4c757af90ded7909a034ef5af28e", "title": "On the Parallelization of Triangular Decomposition of Polynomial Systems", "abstract": "We discuss the parallelization of algorithms for solving polynomial systems symbolically by way of triangular decomposition. Algorithms for solving polynomial systems combine low-level routines for performing arithmetic operations on polynomials and high-level procedures which produce the different components (points, curves, surfaces) of the solution set. The latter \"component-level\" parallelization of triangular decompositions, our focus here, belongs to the class of dynamic irregular parallel applications. Possible speedup factors depend on geometrical properties of the solution set (number of components, their dimensions and degrees); these algorithms do not scale with the number of processors. In this paper we combine two different concurrency schemes, the fork-join model and producer-consumer patterns, to better capture opportunities for component-level parallelization. We report on our implementation with the publicly available BPAS library. Our experimentation with 340 systems yields promising results.", "venue": "ArXiv", "authors": ["Mohammadali  Asadi", "Alexander  Brandt", "Robert H. C. Moir", "Marc Moreno Maza", "Yuzhen  Xie"], "year": 2019, "n_citations": 5}
{"id": 3079521, "s2_id": "1cdac77562c9055291b28bcbab4af777954d2270", "title": "Subspace-preserving sparsification of matrices with minimal perturbation to the near null-space. Part II: Approximation and Implementation", "abstract": "This is the second of two papers to describe a matrix sparsification algorithm that takes a general real or complex matrix as input and produces a sparse output matrix of the same size. The first paper presented the original algorithm, its features, and theoretical results. \nSince the output of this sparsification algorithm is a matrix rather than a vector, it can be costly in memory and run-time if an implementation does not exploit the structural properties of the algorithm and the matrix. Here we show how to modify the original algorithm to increase its efficiency. This is possible by computing an approximation to the exact result. We introduce extra constraints that are automatically determined based on the input matrix. This addition reduces the number of unknown degrees of freedom but still preserves many matrix subspaces. We also describe our open-source library that implements this sparsification algorithm and has interfaces in C++, C, and MATLAB.", "venue": "ArXiv", "authors": ["Chetan  Jhurani"], "year": 2013, "n_citations": 1}
{"id": 3086215, "s2_id": "286c39548e90ae30f8dddc465d74fafa762abcd0", "title": "Learning the Markov Decision Process in the Sparse Gaussian Elimination", "abstract": "We propose a learning-based approach for the sparse Gaussian Elimination. There are many hard combinatorial optimization problems in modern sparse solver. These NP-hard problems could be handled in the framework of Markov Decision Process, especially the Q-Learning technique. We proposed some Q-Learning algorithms for the main modules of sparse solver: minimum degree ordering, task scheduling and adaptive pivoting. Finally, we recast the sparse solver into the framework of Q-Learning. Our study is the first step to connect these two classical mathematical models: Gaussian Elimination and Markov Decision Process. Our learning-based algorithm could help improve the performance of sparse solver, which has been verified in some numerical experiments.", "venue": "ArXiv", "authors": ["Yingshi  Chen"], "year": 2021, "n_citations": 1}
{"id": 3089229, "s2_id": "44c7b6a27e44c216041f9471f87f7420a08a1b78", "title": "Faster Arbitrary-Precision Dot Product and Matrix Multiplication", "abstract": "We present algorithms for real and complex dot product and matrix multiplication in arbitrary-precision floating-point and ball arithmetic. A low-overhead dot product is implemented on the level of GMP limb arrays; it is about twice as fast as previous code in MPFR and Arb at precision up to several hundred bits. Up to 128 bits, it is 3-4 times as fast, costing 20-30 cycles per term for floating-point evaluation and 40-50 cycles per term for balls. We handle large matrix multiplications even more efficiently via blocks of scaled integer matrices. The new methods are implemented in Arb and significantly speed up polynomial operations and linear algebra.", "venue": "2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)", "authors": ["Fredrik  Johansson"], "year": 2019, "n_citations": 4}
{"id": 3089787, "s2_id": "849eee81838eb933f44eb0deb03584609f1edf6f", "title": "Strongly stable ideals and Hilbert polynomials", "abstract": "The \\texttt{StronglyStableIdeals} package for \\textit{Macaulay2} provides a method to compute all saturated strongly stable ideals in a given polynomial ring with a fixed Hilbert polynomial. A description of the main method and auxiliary tools is given.", "venue": "Journal of Software for Algebra and Geometry", "authors": ["Davide  Alberelli", "Paolo  Lella"], "year": 2019, "n_citations": 7}
{"id": 3089945, "s2_id": "b48519116e1fcbb60838f874535b244e5ea8e8df", "title": "Automatic Differentiation With Higher Infinitesimals, or Computational Smooth Infinitesimal Analysis in Weil Algebra", "abstract": "We propose an algorithm to compute the C\u221e-ring structure of arbitrary Weil algebra. It allows us to do some analysis with higher infinitesimals numerically and symbolically. To that end, we first give a brief description of the (Forward-mode) automatic differentiation (AD) in terms of C\u221e-rings. The notion of a C\u221e-ring was introduced by Lawvere [10] and used as the fundamental building block of smooth infinitesimal analysis and synthetic differential geometry [11]. We argue that interpreting AD in terms of C\u221e-rings gives us a unifying theoretical framework and modular ways to express multivariate partial derivatives. In particular, we can \u201cpackage\u201d higher-order Forward-mode AD as a Weil algebra, and take tensor products to compose them to achieve multivariate higher-order AD. The algorithms in the present paper can also be used for a pedagogical purpose in learning and studying smooth infinitesimal analysis as well.", "venue": "CASC", "authors": ["Hiromi  Ishii"], "year": 2021, "n_citations": 1}
{"id": 3095155, "s2_id": "fb4489d6b4260412f7d754142ead5784f6fc9f92", "title": "PyClaw: Accessible, Extensible, Scalable Tools for Wave Propagation Problems", "abstract": "Development of scientific software involves tradeoffs between ease of use, generality, and performance. We describe the design of a general hyperbolic PDE solver that can be operated with the convenience of MATLAB yet achieves efficiency near that of hand-coded Fortran and scales to the largest supercomputers. This is achieved by using Python for most of the code while employing automatically wrapped Fortran kernels for computationally intensive routines, and using Python bindings to interface with a parallel computing library and other numerical packages. The software described here is PyClaw, a Python-based structured grid solver for general systems of hyperbolic PDEs [K. T. Mandli et al., PyClaw Software, Version 1.0, http://numerics.kaust.edu.sa/pyclaw/ (2011)]. PyClaw provides a powerful and intuitive interface to the algorithms of the existing Fortran codes Clawpack and SharpClaw, simplifying code development and use while providing massive parallelism and scalable solvers via the PETSc library. The...", "venue": "SIAM J. Sci. Comput.", "authors": ["David I. Ketcheson", "Kyle T. Mandli", "Aron J. Ahmadia", "Amal  Alghamdi", "Manuel Quezada de Luna", "Matteo  Parsani", "Matthew G. Knepley", "Matthew  Emmett"], "year": 2012, "n_citations": 58}
{"id": 3098238, "s2_id": "291a770d8194650f3ddfcd64efd11be8d41ad4a3", "title": "Program Generation for Linear Algebra Using Multiple Layers of DSLs", "abstract": "Numerical software in computational science and engineering often relies on highly-optimized building blocks from libraries such as BLAS and LAPACK, and while such libraries provide portable performance for a wide range of computing architectures, they still present limitations in terms of flexibility. We advocate a domain-specific program generator capable of producing library routines tailored to the specific needs of the application in terms of sizes, interface, and target architecture.", "venue": "ArXiv", "authors": ["Daniele G. Spampinato", "Diego  Fabregat-Traver", "Markus  P\u00fcschel", "Paolo  Bientinesi"], "year": 2019, "n_citations": 0}
{"id": 3099403, "s2_id": "cdfe3819f4145250e5793490a8b920da5a22c339", "title": "Parallel Computation of Finite Element Navier-Stokes codes using MUMPS Solver", "abstract": "The study deals with the parallelization of 2D and 3D finite element based Navier-Stokes codes using direct solvers. Development of sparse direct solvers using multifrontal solvers has significantly reduced the computational time of direct solution methods. Although limited by its stringent memory requirements, multifrontal solvers can be computationally efficient. First the performance of MUltifrontal Massively Parallel Solver (MUMPS) is evaluated for both 2D and 3D codes in terms of memory requirements and CPU times. The scalability of both Newton and modified Newton algorithms is tested.", "venue": "ArXiv", "authors": ["Mandhapati P. Raju"], "year": 2009, "n_citations": 8}
{"id": 3103800, "s2_id": "96c16c882518ad01a2ee9e38564ddf1bcd13f508", "title": "Recursive function templates as a solution of linear algebra expressions in C++", "abstract": "The article deals with a kind of recursive function templates in C++, where the recursion is realized corresponding template parameters to achieve better computational performance. Some specialization of these template functions ends the recursion and can be implemented using optimized hardware dependent or independent routines. The method is applied in addition to the known expression templates technique to solve linear algebra expressions with the help of the BLAS library. The whole implementation produces a new library, which keeps object-oriented benefits and has a higher computational speed represented in the tests.", "venue": "ArXiv", "authors": ["Volodymyr  Myrnyy"], "year": 2003, "n_citations": 3}
{"id": 3105709, "s2_id": "b8ac80a3f20e434be4d73a482eb507df0cfc62bc", "title": "Automatic Generation of Loop-Invariants for Matrix Operations", "abstract": "In recent years it has been shown that for many linear algebra operations it is possible to create families of algorithms following a very systematic procedure. We do not refer to the fine tuning of a known algorithm, but to a methodology for the actual generation of both algorithms and routines to solve a given target matrix equation. Although systematic, the methodology relies on complex algebraic manipulations and non-obvious pattern matching, making the procedure challenging to be performed by hand, our goal is the development of a fully automated system that from the sole description of a target equation creates multiple algorithms and routines. We present CL1ck, a symbolic system written in Mathematica, that starts with an equation, decomposes it into multiple equations, and returns a set of loop-invariants for the algorithms -- yet to be generated -- that will solve the equation. In a successive step each loop-invariant is then mapped to its corresponding algorithm and routine. For a large class of equations, the methodology generates known algorithms as well as many previously unknown ones. Most interestingly, the methodology unifies algorithms traditionally developed in isolation. As an example, the five well known algorithms for the LU factorization are for the first time unified under a common root.", "venue": "2011 International Conference on Computational Science and Its Applications", "authors": ["Diego  Fabregat-Traver", "Paolo  Bientinesi"], "year": 2011, "n_citations": 24}
{"id": 3109584, "s2_id": "8f0c990ce169e9e78ec6c3d30f2664419c7b8636", "title": "ruptures: change point detection in Python", "abstract": "ruptures is a Python library for offline change point detection. This package provides methods for the analysis and segmentation of non-stationary signals. Implemented algorithms include exact and approximate detection for various parametric and non-parametric models. ruptures focuses on ease of use by providing a well-documented and consistent interface. In addition, thanks to its modular structure, different algorithms and models can be connected and extended within this package.", "venue": "ArXiv", "authors": ["Charles  Truong", "Laurent  Oudre", "Nicolas  Vayatis"], "year": 2018, "n_citations": 30}
{"id": 3111651, "s2_id": "06b28a241918bbe56248942d3a29b526bf6ce3d1", "title": "Minimizing Communication for Eigenproblems and the Singular Value Decomposition", "abstract": "Algorithms have two costs: arithmetic and communication. The latter represents the cost of moving data, either between levels of a memory hierarchy, or between processors over a network. Communication often dominates arithmetic and represents a rapidly increasing proportion of the total cost, so we seek algorithms that minimize communication. In [4] lower bounds were presented on the amount of communication required for essentially all O(n 3 )-like algorithms for linear algebra, including eigenvalue problems and the SVD. Conventional algorithms, including those currently implemented in (Sca)LAPACK, perform asymptotically more communication than these lower bounds require. In this paper we present parallel and sequential eigenvalue algorithms (for pencils, nonsymmetric matrices, and symmetric matrices) and SVD algorithms that do attain these lower bounds, and analyze their convergence and communication costs.", "venue": "ArXiv", "authors": ["Grey  Ballard", "James  Demmel", "Ioana  Dumitriu"], "year": 2010, "n_citations": 18}
{"id": 3112102, "s2_id": "e516bd71893a060bf53bc8983cf46a77c6d79983", "title": "Efficient Stochastic Programming in Julia", "abstract": "We present StochasticPrograms.jl, an open-source framework for stochastic programming written in the Julia language. The framework includes both modeling tools and structure-exploiting optimization algorithms. We show how stochastic programming models can be efficiently formulated using expressive syntax. Defined models can be instantiated, inspected, and analyzed interactively. The framework was implemented to scale seamlessly to distributed environments. As a result, stochastic programs are efficiently memory-distributed on supercomputers or cloud architectures and solved using parallel optimization algorithms. These structure-exploiting solvers are based on variations of the classical L-shaped and progressive-hedging algorithms. We provide a concise mathematical background for the various tools and constructs available in the framework, along with code listings exemplifying their usage. Both software innovations related to the implementation of the framework and algorithmic innovations related to the structured solvers are highlighted. We conclude by performing numerical benchmarks of the distributed algorithms in a multi-node setup. We showcase strong scaling properties of the solvers and outline techniques for further speedups.", "venue": "ArXiv", "authors": ["Martin  Biel", "Mikael  Johansson"], "year": 2019, "n_citations": 7}
{"id": 3116219, "s2_id": "e70ca3ee5355846df26c69d1b96d84a8e07a4b4b", "title": "Tree-based Arithmetic and Compressed Representations of Giant Numbers", "abstract": "Can we do arithmetic in a completely different way, with a radically different data structure? Could this approach provide practical benefits, like operations on giant numbers while having an average performance similar to traditional bitstring representations? \nWhile answering these questions positively, our tree based representation described in this paper comes with a few extra benefits: it compresses giant numbers such that, for instance, the largest known prime number as well as its related perfect number are represented as trees of small sizes. The same also applies to Fermat numbers and important computations like exponentiation of two become constant time operations. \nAt the same time, succinct representations of sparse sets, multisets and sequences become possible through bijections to our tree-represented natural numbers.", "venue": "ArXiv", "authors": ["Paul  Tarau"], "year": 2013, "n_citations": 0}
{"id": 3117164, "s2_id": "ce3c97d6e20144c86a4c92b46629523bbb2a52cc", "title": "FASTA: A Generalized Implementation of Forward-Backward Splitting", "abstract": "where | \u00b7 | denotes the `1 norm, \u2016\u00b7\u2016 denotes the `2 norm, A is a matrix, b is a vector, and \u03bc is a scalar parameter. This problem is of the form (1) with g(z) = \u03bc|z|, and f(z) = 12\u2016z\u2212 b\u2016 . More generally, any problem of the form (1) can be solved by FASTA, provided the user can provide function handles to f, g, A and A . The solver FASTA contains numerous enhancements of FBS to improve convergence speed and usability. These include adaptive stepsize choice, acceleration (i.e., of the type used by the solver FISTA), backtracking line search, and numerous automated stopping conditions, and many other improvements reviews in the article A Field Guide to Forward-Backward Splitting with a FASTA Implementation.", "venue": "ArXiv", "authors": ["Tom  Goldstein", "Christoph  Studer", "Richard G. Baraniuk"], "year": 2015, "n_citations": 27}
{"id": 3118417, "s2_id": "05a5f4b5b67e5e22e19ed5e2f5c9e16d8dc2083f", "title": "FluidFFT: common API (C++ and Python) for Fast Fourier Transform HPC libraries", "abstract": "The Python package fluidfft provides a common Python API for performing Fast Fourier Transforms (FFT) in sequential, in parallel and on GPU with different FFT libraries (FFTW, P3DFFT, PFFT, cuFFT). fluidfft is a comprehensive FFT framework which allows Python users to easily and efficiently perform FFT and the associated tasks, such as as computing linear operators and energy spectra. We describe the architecture of the package composed of C++ and Cython FFT classes, Python \"operator\" classes and Pythran functions. The package supplies utilities to easily test itself and benchmark the different FFT solutions for a particular case and on a particular machine. We present a performance scaling analysis on three different computing clusters and a microbenchmark showing that fluidfft is an interesting solution to write efficient Python applications using FFT.", "venue": "ArXiv", "authors": ["Ashwin Vishnu Mohanan", "Cyrille  Bonamy", "Pierre  Augier"], "year": 2018, "n_citations": 5}
{"id": 3119767, "s2_id": "cabddd9ed02a1e5c1c881ef933f2db3d7871fe31", "title": "Multivariate sparse interpolation using randomized Kronecker substitutions", "abstract": "We present new techniques for reducing a multivariate sparse polynomial to a univariate polynomial. The reduction works similarly to the classical and widely-used Kronecker substitution, except that we choose the degrees randomly based on the number of nonzero terms in the multivariate polynomial. The resulting univariate polynomial often has a significantly lower degree than the Kronecker substitution polynomial, at the expense of a small number of term collisions. As an application, we give a new algorithm for multivariate interpolation which uses these new techniques along with any existing univariate interpolation algorithm.", "venue": "ISSAC", "authors": ["Andrew  Arnold", "Daniel S. Roche"], "year": 2014, "n_citations": 21}
{"id": 3122884, "s2_id": "a21f072bad0e1372584161332559860444691863", "title": "Partial Reuse AMG Setup Cost Amortization Strategy for the Solution of Non-Steady State Problems", "abstract": "The partial reuse algebraic multigrid (AMG) setup cost amortization strategy is presented for the solution of non-steady state problems. The transfer operators are reused from the previous time steps, and the system matrices and the smoother operators are rebuilt on each of the AMG hierarchy levels. It is shown on the example of modelling a two-fluid dam break scenario that the strategy may decrease the AMG preconditioner setup cost by 40% to 200%. The total compute time is decreased by up to 20%, but the specific outcome depends on the fraction of time that the setup step initially takes. 2010 Mathematical Subject Classification: 35-04, 65-04, 65Y05, 65Y10, 65Y15, 97N80", "venue": "Lobachevskii Journal of Mathematics", "authors": ["D. E. Demidov"], "year": 2021, "n_citations": 0}
{"id": 3128596, "s2_id": "849a81a3f889674c64052d568cb988c7b41bbd79", "title": "A set of R packages to estimate population counts from mobile phone data", "abstract": "In this paper, we describe the software implementation of the methodological framework designed to incorporate mobile phone data into the current production chain of official statistics during the ESSnet Big Data II project. We present an overview of the architecture of the software stack, its components, the interfaces between them, and show how they can be used. Our software implementation consists in four R packages: destim for estimation of the spatial distribution of the mobile devices, deduplication for classification of the devices as being in 1:1 or 2:1 correspondence with its owner, aggregation for estimation of the number of individuals detected by the network starting from the geolocation probabilities and the duplicity probabilities and inference which combines the number of individuals provided by the previous package with other information like the population counts from an official register and the mobile operator penetration rates to provide an estimation of the target population counts.", "venue": "ArXiv", "authors": ["Bogdan  Oancea", "David  Salgado", "Luis Sanguiao Sande", "Sandra  Barragan"], "year": 2021, "n_citations": 0}
{"id": 3130939, "s2_id": "1925ce1f1a68b6ab17817372b8c44552b7224d91", "title": "Efficient Implementation of Elementary Functions in the Medium-Precision Range", "abstract": "We describe a new implementation of the elementary transcendental functions exp, sin, cos, log and atan for variable precision up to approximately 4096 bits. Compared to the MPFR library, we achieve a maximum speedup ranging from a factor 3 for cos to 30 for atan. Our implementation uses table-based argument reduction together with rectangular splitting to evaluate Taylor series. We collect denominators to reduce the number of divisions in the Taylor series, and avoid overhead by doing all multiprecision arithmetic using the mpn layer of the GMP library. Our implementation provides rigorous error bounds.", "venue": "2015 IEEE 22nd Symposium on Computer Arithmetic", "authors": ["Fredrik  Johansson"], "year": 2015, "n_citations": 11}
{"id": 3134333, "s2_id": "161ac28de6243ce405027912b65d404949ec2d53", "title": "Interoperability in the OpenDreamKit Project: The Math-in-the-Middle Approach", "abstract": "OpenDreamKit \u2013 \u201cOpen Digital Research Environment Toolkit for the Advancement of Mathematics\u201d \u2013 is an H2020 EU Research Infrastructure project that aims at supporting, over the period 2015\u20132019, the ecosystem of open-source mathematical software systems. OpenDreamKit will deliver a flexible toolkit enabling research groups to set up Virtual Research Environments, customised to meet the varied needs of research projects in pure mathematics and applications.", "venue": "CICM", "authors": ["Paul-Olivier  Dehaye", "Michael  Kohlhase", "Alexander  Konovalov", "Samuel  Lelievre", "Markus  Pfeiffer", "Nicolas M. Thi'ery"], "year": 2016, "n_citations": 21}
{"id": 3136607, "s2_id": "cc85f1d6e1699b8eeef801dff6110580e89ca8d1", "title": "RCHOL: Randomized Cholesky Factorization for Solving SDD Linear Systems", "abstract": "We introduce a randomized algorithm, namely {\\tt rchol}, to construct an approximate Cholesky factorization for a given sparse Laplacian matrix (a.k.a., graph Laplacian). The (exact) Cholesky factorization for the matrix introduces a clique in the associated graph after eliminating every row/column. By randomization, {\\tt rchol} samples a subset of the edges in the clique. We prove {\\tt rchol} is breakdown free and apply it to solving linear systems with symmetric diagonally-dominant matrices. In addition, we parallelize {\\tt rchol} based on the nested dissection ordering for shared-memory machines. Numerical experiments demonstrated the robustness and the scalability of {\\tt rchol}. For example, our parallel code scaled up to 64 threads on a single node for solving the 3D Poisson equation, discretized with the 7-point stencil on a $1024\\times 1024 \\times 1024$ grid, or \\textbf{one billion} unknowns.", "venue": "SIAM Journal on Scientific Computing", "authors": ["Chao  Chen", "Tianyu  Liang", "George  Biros"], "year": 2021, "n_citations": 1}
{"id": 3138678, "s2_id": "78fef155d70cd0bbf8f94bc9fd9a5ce75ebe1285", "title": "The Space of Mathematical Software Systems - A Survey of Paradigmatic Systems", "abstract": "Mathematical software systems are becoming more and more important in pure and applied mathematics in order to deal with the complexity and scalability issues inherent in mathematics. In the last decades we have seen a cambric explosion of increasingly powerful but also diverging systems. To give researchers a guide to this space of systems, we devise a novel conceptualization of mathematical software that focuses on five aspects: inference covers formal logic and reasoning about mathematical statements via proofs and models, typically with strong emphasis on correctness; computation covers algorithms and software libraries for representing and manipulating mathematical objects, typically with strong emphasis on efficiency; concretization covers generating and maintaining collections of mathematical objects conforming to a certain pattern, typically with strong emphasis on complete enumeration; narration covers describing mathematical contexts and relations, typically with strong emphasis on human readability; finally, organization covers representing mathematical contexts and objects in machine-actionable formal languages, typically with strong emphasis on expressivity and system interoperability. Despite broad agreement that an ideal system would seamlessly integrate all these aspects, research has diversified into families of highly specialized systems focusing on a single aspect and possibly partially integrating others, each with their own communities, challenges, and successes. In this survey, we focus on the commonalities and differences of these systems from the perspective of a future multi-aspect system.", "venue": "ArXiv", "authors": ["Katja  Bercic", "Jacques  Carette", "William M. Farmer", "Michael  Kohlhase", "Dennis  M\u00fcller", "Florian  Rabe", "Yasmine  Sharoda"], "year": 2020, "n_citations": 4}
{"id": 3146838, "s2_id": "2803aea7ed22ec122e15796bfb4dae01b22d6b4f", "title": "The Dune Python Module", "abstract": "In this paper we present the new Dune-Python module which provides Python bindings for the Dune core, which is a C++ environment for solving partial differential equations. The aim of this new module is to firstly provide the general infrastructure for exporting realizations of statically polymorphic interfaces based on just-in-time compilation and secondly to provide bindings for the central interfaces of the dune core modules. In the first release we focus on the grid interface. Our aim is to only introduce a thin layer when passing objects into Python which can be removed when the object is passed back into a C++ algorithm. Thus no efficiency is lost and little additional code maintenance cost is incurred. To make the transition for Dune users to the Python environment straightforward the Python classes provide a very similar interface to their C++ counterparts. In addition, vectorized versions of many interfaces allow for more efficient code on the Python side. The infrastructure for exporting these interfaces and the resulting bindings for a Dune grid are explained in detail in this paper for both experienced Dune users and others interested in a flexible Python environment for implementing grid based schemes for solving partial differential equations.", "venue": "ArXiv", "authors": ["Andreas  Dedner", "Martin  Nolte"], "year": 2018, "n_citations": 14}
{"id": 3148160, "s2_id": "9950abf7431124c935305fcdc54394d5493bb4b9", "title": "Multivariate Polynomials in Sage", "abstract": "We have developed a patch implementing multivariate polynomials seen as a multi-base algebra. The patch is to be released into the software Sage and can already be found within the Sage-Combinat distribution. One can use our patch to define a polynomial in a set of indexed variables and expand it into a linear basis of the multivariate polynomials. So far, we have the Schubert polynomials, the Key polynomials of types A, B, C, or D, the Grothendieck polynomials and the non-symmetric Macdonald polynomials. One can also use a double set of variables and work with specific double-linear bases like the double Schubert polynomials or double Grothendieck polynomials. Our implementation is based on a definition of the basis using divided difference operators and one can also define new bases using these operators.", "venue": "ArXiv", "authors": ["Viviane  Pons"], "year": 2012, "n_citations": 6}
{"id": 3153999, "s2_id": "1150eef906240eb69ecb3163b1f86e506d552771", "title": "tym: Typed Matlab", "abstract": "Although, many scientists and engineers use Octave or MATLAB as their preferred programming language, dynamic nature of these languages can lead to slower running-time of programs written in these languages compared to programs written in languages which are not as dynamic, like C, C++ and Fortran. In this work we developed a translator for a new programming language (tym) which tries to address performance issues, common in scientific programs, by adding new constructs to a subset of Octave/MATLAB language. Our translator compiles programs written in tym, to efficient C++ code.", "venue": "ArXiv", "authors": ["Hamid A. Toussi"], "year": 2011, "n_citations": 0}
{"id": 3159511, "s2_id": "e244ca7e29348023d4d75d987b50678d63ab2ec1", "title": "Formally Verified Argument Reduction with a Fused Multiply-Add", "abstract": "The Cody and Waite argument reduction technique works perfectly for reasonably large arguments, but as the input grows, there are no bits left to approximate the constant with enough accuracy. Under mild assumptions, we show that the result computed with a fused multiply-add provides a fully accurate result for many possible values of the input with a constant almost accurate to the full working precision. We also present an algorithm for a fully accurate second reduction step to reach full double accuracy (all the significand bits of two numbers are accurate) even in the worst cases of argument reduction. Our work recalls the common algorithms and presents proofs of correctness. All the proofs are formally verified using the Coq automatic proof checker.", "venue": "IEEE Transactions on Computers", "authors": ["Sylvie  Boldo", "Marc  Daumas", "Ren-Cang  Li"], "year": 2009, "n_citations": 14}
{"id": 3164209, "s2_id": "5a1f07f2f1a7863f7d3c70853579c95b4da53371", "title": "Compiling LATEX to computer algebra-enabled HTML5", "abstract": "This document explains how to create or modify an existing LATEX document with commands enabling computations in the HTML5 output: when the reader opens the HTML5 output, he can run a computation in his browser, or modify the command to be executed and run it. This is done by combining different softwares: hevea for compilation to HTML5, giac.js for the CAS computing kernel (itself compiled from the C++ Giac library with emscripten), and a modified version of itex2MML for fast and nice rendering in MathML in browsers that support MathML.", "venue": "ArXiv", "authors": ["Bernard  Parisse"], "year": 2017, "n_citations": 0}
{"id": 3165103, "s2_id": "93efcb9feedabb29e1de1b71ff17868f95b96ba4", "title": "A Robust Complex Division in Scilab", "abstract": "The most widely used algorithm for floating point complex division, known as Smith's method, may fail more often than expected. This document presents two improved complex division algorithms. We present a proof of the robustness of the first improved algorithm. Numerical simulations show that this algorithm performs well in practice and is significantly more robust than other known implementations. By combining additionnal scaling methods with this first algorithm, we were able to create a second algorithm, which rarely fails.", "venue": "ArXiv", "authors": ["Michael  Baudin", "Robert L. Smith"], "year": 2012, "n_citations": 7}
{"id": 3166477, "s2_id": "1a37c16afd521d020349ef9c8ddc86863fd16506", "title": "Fast Linear Transformations in Python", "abstract": "This paper introduces a new free library for the Python programming language, which provides a collection of structured linear transforms, that are not represented as explicit two dimensional arrays but in a more efficient way by exploiting the structural knowledge. \nThis allows fast and memory savy forward and backward transformations while also provding a clean but still flexible interface to these effcient algorithms, thus making code more readable, scable and adaptable. \nWe first outline the goals of this library, then how they were achieved and lastly we demonstrate the performance compared to current state of the art packages available for Python. \nThis library is released and distributed under a free license.", "venue": "ArXiv", "authors": ["Christoph  Wagner", "Sebastian  Semper"], "year": 2017, "n_citations": 8}
{"id": 3177546, "s2_id": "1a3ba16bf69940b84509fbc9aec69483c0f703a9", "title": "Simflowny 2: An upgraded platform for scientific modeling and simulation", "abstract": "Abstract Simflowny is an open platform which automatically generates efficient parallel code of scientific dynamical models for different simulation frameworks. Here we present major upgrades on this software to support simultaneously a quite generic family of partial differential equations. These equations can be discretized using: (i) standard finite-difference for systems with derivatives up to any order, (ii) High-Resolution-Shock-Capturing methods to deal with shocks and discontinuities of balance law equations, and (iii) particle-based methods. We have improved the adaptive-mesh-refinement algorithms to preserve the convergence order of the numerical methods, which is a requirement for improving scalability. Finally, we have also extended our graphical user interface (GUI) to accommodate these and future families of equations. This paper summarizes the formal representation and implementation of these new families, providing several validation results. Program summary Program Title: Simflowny CPC Library link to program files: http://dx.doi.org/10.17632/g9mcw8s64f.2 Licensing provisions: Apache License, 2.0 Programming language: Java, C++ and JavaScript Journal Reference of previous version: Comput. Phys. Comm. 184 (2013) 2321\u20132331, Comput. Phys. Comm. 229 (2018), 170\u2013181 Does the new version supersede the previous version?: Yes Reasons for the new version: Additional features Summary of revisions: Expanded support for Partial Differential Equations, meshless particles and advanced Adaptive Mesh Refinement. Nature of problem: Simflowny generates numerical simulation code for a wide range of models. Solution method: Any discretization scheme based on either Finite Volume Methods, Finite Difference Methods, or meshless methods for Partial Differential Equations. Additional comments: Simflowny runs in any computer with Docker [1]. Installation details can be checked in the documentation of Simflowny [2]. It can also be compiled from scratch on any Linux system, provided dependences are properly installed as indicated in the documentation. The generated code runs on any Linux platform ranging from personal workstations to clusters and parallel supercomputers. The software architecture is easily extensible for future additional model families and simulation frameworks. Full documentation is available in the wiki home of the Simflowny project [2]. References: [1] https://www.docker.com/ [online] (2020) [2] https://bitbucket.org/iac3/simflowny/wiki/Home [online] (2020)", "venue": "Comput. Phys. Commun.", "authors": ["A.  Arbona", "B.  Minano", "A.  Rigo", "C.  Bona", "C.  Palenzuela", "A.  Artigues", "C.  Bona-Casas", "J.  Mass'o"], "year": 2018, "n_citations": 19}
{"id": 3179748, "s2_id": "9a4a0a708c85921f2a774c519638e99ea97fa54f", "title": "Combining the Mersenne Twister and the Xorgens Designs", "abstract": "We combine the design of two \\emph{random number generators}, \\emph{Mersenne Twister} and \\emph{Xorgens}, to obtain a new class of generators with heavy-weight characteristic polynomials (exceeded only by the {\\sc well} generators) and high speed (comparable with the originals). Tables with parameter combinations are included for state sizes ranging from 521 to 44497 bits and each of the word lengths 32, 64, 128. These generators passed all tests of the \\emph{TestU01}-package for each 32-bit integer part and each 64-bit derived real part of the output. We determine \\emph{dimension gaps} for 32-bit words, neglecting the non-linear tempering, and compare with an alternative experimental linear tempering.", "venue": "ArXiv", "authors": ["Marcel Van de Vel"], "year": 2020, "n_citations": 0}
{"id": 3180069, "s2_id": "c48bed3ca6603b1040a652414bb52d771b309aa7", "title": "AuTO: A Framework for Automatic differentiation in Topology Optimization", "abstract": "A critical step in topology optimization (TO) is finding sensitivities. Manual derivation and implementation of the sensitivities can be quite laborious and error-prone, especially for non-trivial objectives, constraints and material models. An alternate approach is to utilize automatic differentiation (AD). While AD has been around for decades, and has also been applied in TO, wider adoption has largely been absent. In this educational paper, we aim to reintroduce AD for TO, and make it easily accessible through illustrative codes. In particular, we employ JAX, a high-performance Python library for automatically computing sensitivities from a user defined TO problem. The resulting framework, referred to here as AuTO, is illustrated through several examples in compliance minimization, compliant mechanism design and microstructural design.", "venue": "Structural and Multidisciplinary Optimization", "authors": ["Aaditya  Chandrasekhar", "Saketh  Sridhara", "Krishnan  Suresh"], "year": 2021, "n_citations": 3}
{"id": 3181448, "s2_id": "3d89bd1b49147b96306d7773f66b07645051ea5a", "title": "A Distributed-Memory Package for Dense Hierarchically Semi-Separable Matrix Computations Using Randomization", "abstract": "We present a distributed-memory library for computations with dense structured matrices. A matrix is considered structured if its off-diagonal blocks can be approximated by a rank-deficient matrix with low numerical rank. Here, we use Hierarchically Semi-Separable (HSS) representations. Such matrices appear in many applications, for example, finite-element methods, boundary element methods, and so on. Exploiting this structure allows for fast solution of linear systems and/or fast computation of matrix-vector products, which are the two main building blocks of matrix computations. The compression algorithm that we use, that computes the HSS form of an input dense matrix, relies on randomized sampling with a novel adaptive sampling mechanism. We discuss the parallelization of this algorithm and also present the parallelization of structured matrix-vector product, structured factorization, and solution routines. The efficiency of the approach is demonstrated on large problems from different academic and industrial applications, on up to 8,000 cores. This work is part of a more global effort, the STRUctured Matrices PACKage (STRUMPACK) software package for computations with sparse and dense structured matrices. Hence, although useful on their own right, the routines also represent a step in the direction of a distributed-memory sparse solver.", "venue": "ACM Trans. Math. Softw.", "authors": ["Fran\u00e7ois-Henry  Rouet", "Xiaoye S. Li", "Pieter  Ghysels", "Artem  Napov"], "year": 2016, "n_citations": 95}
{"id": 3185181, "s2_id": "7b99feba181fccdf6c2576b971b5b77a7b2c7008", "title": "MRRR-based eigensolvers for multi-core processors and supercomputers", "abstract": "The real symmetric tridiagonal eigenproblem is of outstanding importance in numerical computations; it arises frequently as part of eigensolvers for standard and generalized dense Hermitian eigenproblems that are based on a reduction to tridiagonal form. For its solution, the algorithm of Multiple Relatively Robust Representations (MRRR or MR3 in short) - introduced in the late 1990s - is among the fastest methods. To compute k eigenpairs of a real n-by-n tridiagonal T, MRRR only requires O(kn) arithmetic operations; in contrast, all the other practical methods require O(k^2 n) or O(n^3) operations in the worst case. This thesis centers around the performance and accuracy of MRRR.", "venue": "ArXiv", "authors": ["Matthias  Petschow"], "year": 2013, "n_citations": 1}
{"id": 3188262, "s2_id": "731ce1fe85e0c77539eca452091295775703c6fd", "title": "Code Generation for Generally Mapped Finite Elements", "abstract": "Many classical finite elements such as the Argyris and Bell elements have long been absent from high-level PDE software. Building on recent theoretical work, we describe how to implement very general finite-element transformations in FInAT and hence into the Firedrake finite-element system. Numerical results evaluate the new elements, comparing them to existing methods for classical problems. For a second-order model problem, we find that new elements give smooth solutions at a mild increase in cost over standard Lagrange elements. For fourth-order problems, however, the newly enabled methods significantly outperform interior penalty formulations. We also give some advanced use cases, solving the nonlinear Cahn-Hilliard equation and some biharmonic eigenvalue problems (including Chladni plates) using C1 discretizations.", "venue": "ACM Trans. Math. Softw.", "authors": ["Robert C. Kirby", "Lawrence  Mitchell"], "year": 2019, "n_citations": 11}
{"id": 3188661, "s2_id": "46a9a0a513f6894dafba2dc16d95bc68dc9d61c0", "title": "Efficient Realization of Givens Rotation through Algorithm-Architecture Co-design for Acceleration of QR Factorization", "abstract": "We present efficient realization of Generalized Givens Rotation (GGR) based QR factorization that achieves 3-100x better performance in terms of Gflops/watt over state-of-the-art realizations on multicore, and General Purpose Graphics Processing Units (GPGPUs). GGR is an improvement over classical Givens Rotation (GR) operation that can annihilate multiple elements of rows and columns of an input matrix simultaneously. GGR takes 33% lesser multiplications compared to GR. For custom implementation of GGR, we identify macro operations in GGR and realize them on a Reconfigurable Data-path (RDP) tightly coupled to pipeline of a Processing Element (PE). In PE, GGR attains speed-up of 1.1x over Modified Householder Transform (MHT) presented in the literature. For parallel realization of GGR, we use REDEFINE, a scalable massively parallel Coarse-grained Reconfigurable Architecture, and show that the speed-up attained is commensurate with the hardware resources in REDEFINE. GGR also outperforms General Matrix Multiplication (gemm) by 10% in-terms of Gflops/watt which is counter-intuitive.", "venue": "ArXiv", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan", "Rainer  Leupers"], "year": 2018, "n_citations": 5}
{"id": 3197803, "s2_id": "6c9b1f57db7865ff601f62b91e23d45b9fb1bde8", "title": "Mlf-core: a Framework for Deterministic Machine Learning", "abstract": "Machine learning has shown extensive growth in recent years1. However, previously existing studies highlighted a reproducibility crisis in machine learning2. The reasons for irreproducibility are manifold. Major machine learning libraries default to the usage of non-deterministic algorithms based on atomic operations. Solely fixing all random seeds is not sufficient for deterministic machine learning. To overcome this shortcoming, various machine learning libraries released deterministic counterparts to the non-deterministic algorithms. We evaluated the effect of these algorithms on determinism and runtime. Based on these results, we formulated a set of requirements for reproducible machine learning and developed a new software solution, the mlf-core ecosystem, which aids machine learning projects to meet and keep these requirements. We applied mlf-core to develop fully reproducible models in various biomedical fields including a single cell autoencoder with TensorFlow, a PyTorch-based U-Net model for liver-tumor segmentation in CT scans, and a liver cancer classifier based on gene expression profiles with XGBoost.", "venue": "ArXiv", "authors": ["Lukas  Heumos", "Philipp  Ehmele", "Kevin  Menden", "Luis Kuhn Cuellar", "Edmund  Miller", "Steffen  Lemke", "Gisela  Gabernet", "Sven  Nahnsen"], "year": 2021, "n_citations": 0}
{"id": 3198521, "s2_id": "cb1376a7ac8ed0c6e7a5f859709bf9c39f70f3f4", "title": "Matrix representation of a solution of a combinatorial problem of the group theory", "abstract": "An equivalence relation in the symmetric group, where is a positive integer has been considered. An algorithm for calculation of the number of the equivalence classes by this relation for arbitrary integer has been described.", "venue": "ArXiv", "authors": ["Krasimir  Yordzhev", "Lilyana  Totina"], "year": 2012, "n_citations": 1}
{"id": 3203925, "s2_id": "d862dd0b1499137d2b6d2e2e18414708dea5eb95", "title": "Formal proof of SCHUR conjugate function", "abstract": "The main goal of our work is to formally prove the correctness of the key commands of the SCHUR software, an interactive program for calculating with characters of Lie groups and symmetric functions. The core of the computations relies on enumeration and manipulation of combinatorial structures. As a first \"proof of concept\", we present a formal proof of the conjugate function, written in C. This function computes the conjugate of an integer partition. To formally prove this program, we use the Frama-C software. It allows us to annotate C functions and to generate proof obligations, which are proved using several automated theorem provers. In this paper, we also draw on methodology, discussing on how to formally prove this kind of program.", "venue": "AISC'10/MKM'10/Calculemus'10", "authors": ["Franck  Butelle", "Florent  Hivert", "Micaela  Mayero", "Fr\u00e9d\u00e9ric  Toumazet"], "year": 2010, "n_citations": 6}
{"id": 3213392, "s2_id": "5a801d0f9407dcdd8c8a34b3f0361ad4bc27a03d", "title": "Formal Verification of an Iterative Low-Power x86 Floating-Point Multiplier with Redundant Feedback", "abstract": "We present the formal verification of a low-power x86 floating-point multiplier. The multiplier operates iteratively and feeds back intermediate results in redundant representation. It supports x87 and SSE instructions in various precisions and can block the issuing of new instructions. The design has been optimized for low-power operation and has not been constrained by the formal verification effort. Additional improvements for the implementation were identified through formal verification. The formal verification of the design also incorporates the implementation of clock-gating and control logic. The core of the verification effort was based on ACL2 theorem proving. Additionally, model checking has been used to verify some properties of the floating-point scheduler that are relevant for the correct operation of the unit.", "venue": "ACL2", "authors": ["Peter-Michael  Seidel"], "year": 2011, "n_citations": 3}
{"id": 3237808, "s2_id": "6473fbb16cd6e48ad86635dabdfa73be729ebe1c", "title": "Towards new solutions for scientific computing: the case of Julia", "abstract": "This year marks the consolidation of Julia (this https URL), a programming language designed for scientific computing, as the first stable version (1.0) has been released, in August 2018. Among its main features, expressiveness and high execution speeds are the most prominent: the performance of Julia code is similar to statically compiled languages, yet Julia provides a nice interactive shell and fully supports Jupyter; moreover, it can transparently call external codes written in C, Fortran, and even Python and R without the need of wrappers. The usage of Julia in the astronomical community is growing, and a GitHub organization named JuliaAstro takes care of coordinating the development of packages. In this paper, we present the features and shortcomings of this language and discuss its application in astronomy and astrophysics.", "venue": "ArXiv", "authors": ["Maurizio  Tomasi", "Mos\u00e8  Giordano"], "year": 2018, "n_citations": 1}
{"id": 3238894, "s2_id": "3d5e97a76321ec7bd638364374d8f8324f55ace6", "title": "Parallelizing Mizar", "abstract": "This paper surveys and describes the implementation of parallelization of the Mizar proof checking and of related Mizar utilities. The implementation makes use of Mizar's compiler-like division into several relatively independent passes, with typically quite different processing speeds. The information produced in earlier (typically much faster) passes can be used to parallelize the later (typically much slower) passes. The parallelization now works by splitting the formalization into a suitable number of pieces that are processed in parallel, assembling from them together the required results. The implementation is evaluated on examples from the Mizar library, and future extensions are discussed.", "venue": "ArXiv", "authors": ["Josef  Urban"], "year": 2012, "n_citations": 4}
{"id": 3247021, "s2_id": "8d97d60902535297e4dd113bc726b92eb601c7fb", "title": "High-Performance Level-1 and Level-2 BLAS", "abstract": "The introduction of the Basic Linear Algebra Subroutine (BLAS) in the 1970s laid the path to the different libraries to solve the same problem with an improved approach and improved hardware. The new BLAS implementation led to High-Performance Computing (HPC) innovation, and all the love went to the level-3 BLAS due to its humongous application in different fields not bound to computer science.However, level-1 and level-2 got neglected, and we here tried to solve the problem by introducing the new algorithm for the vector-vector dot product, vector-vector outer product and matrix-vector product, which improves the performance of these operations in a significant way. We are not introducing any library but algorithms, which improves upon the current state-of-art algorithms. Also, we rely on the FMA instruction, OpenMP, and the compiler to optimize the code rather than implementing the algorithm in assembly. Therefore, our current implementation is machine oblivious and depends on the compiler\u2019s ability to optimize the code. This paper makes the following contribution:", "venue": "ArXiv", "authors": ["Amit  Singh", "Cem  Bassoy"], "year": 2021, "n_citations": 0}
{"id": 3259171, "s2_id": "277273abd3b48717c10b1f85b562fd012b176fc5", "title": "Isogenies of Elliptic Curves: A Computational Approach", "abstract": "Isogenies, the mappings of elliptic curves, have become a useful tool in cryptology. These mathematical objects have been proposed for use in computing pairings, constructing hash functions and random number generators, and analyzing the reducibility of the elliptic curve discrete logarithm problem. With such diverse uses, understanding these objects is important for anyone interested in the field of elliptic curve cryptography. This paper, targeted at an audience with a knowledge of the basic theory of elliptic curves, provides an introduction to the necessary theoretical background for understanding what isogenies are and their basic properties. This theoretical background is used to explain some of the basic computational tasks associated with isogenies. Herein, algorithms for computing isogenies are collected and presented with proofs of correctness and complexity analyses. As opposed to the complex analytic approach provided in most texts on the subject, the proofs in this paper are primarily algebraic in nature. This provides alternate explanations that some with a more concrete or computational bias may find more clear.", "venue": "IACR Cryptol. ePrint Arch.", "authors": ["Daniel  Shumow"], "year": 2009, "n_citations": 7}
{"id": 3259594, "s2_id": "e22ca7ea1efae2ac2a5969b2c6331c222a413470", "title": "Algorithmic Based Fault Tolerance Applied to High Performance Computing", "abstract": "We present a new approach to fault tolerance for High Performance Computing system. Our approach is based on a careful adaptation of the Algorithmic Based Fault Tolerance technique (Huang and Abraham, 1984) to the need of parallel distributed computation. We obtain a strongly scalable mechanism for fault tolerance. We can also detect and correct errors (bit-flip) on the fly of a computation. To assess the viability of our approach, we have developed a fault tolerant matrix-matrix multiplication subroutine and we propose some models to predict its running time. Our parallel fault-tolerant matrix-matrix multiplication scores 1.4 TFLOPS on 484 processors (cluster jacquard.nersc.gov) and returns a correct result while one process failure has happened. This represents 65% of the machine peak efficiency and less than 12% overhead with respect to the fastest failure-free implementation. We predict (and have observed) that, as we increase the processor count, the overhead of the fault tolerance drops significantly.", "venue": "ArXiv", "authors": ["George  Bosilca", "R\u00e9mi  Delmas", "Jack J. Dongarra", "Julien  Langou"], "year": 2008, "n_citations": 30}
{"id": 3263906, "s2_id": "5e08a9073a8258d83d15b3fa9644984687d7128a", "title": "Accelerating Domain Propagation: An Efficient GPU-Parallel Algorithm over Sparse Matrices", "abstract": "Fast domain propagation of linear constraints has become a crucial component of today's best algorithms and solvers for mixed integer programming and pseudo-boolean optimization to achieve peak solving performance. Irregularities in the form of dynamic algorithmic behaviour, dependency structures, and sparsity patterns in the input data make efficient implementations of domain propagation on GPUs and, more generally, on parallel architectures challenging. This is one of the main reasons why domain propagation in state-of-the-art solvers is single thread only. In this paper, we present a new algorithm for domain propagation which (a) avoids these problems and allows for an efficient implementation on GPUs, and is (b) capable of running propagation rounds entirely on the GPU, without any need for synchronization or communication with the CPU. We present extensive computational results which demonstrate the effectiveness of our approach and show that ample speedups are possible on practically relevant problems: on state-of-the-art GPUs, our geometric mean speed-up for reasonably-large instances is around 10x to 20x and can be as high as 195x on favorably-large instances.", "venue": "2020 IEEE/ACM 10th Workshop on Irregular Applications: Architectures and Algorithms (IA3)", "authors": ["Boro  Sofranac", "Ambros  Gleixner", "Sebastian  Pokutta"], "year": 2020, "n_citations": 3}
{"id": 3264919, "s2_id": "ba47d16970726fccc4acaceb6c24e3316aeb27a9", "title": "Isogeometric analysis: An overview and computer implementation aspects", "abstract": "Isogeometric analysis (IGA) represents a recently developed technology in computational mechanics that offers the possibility of integrating methods for analysis and Computer Aided Design (CAD) into a single, unified process. The implications to practical engineering design scenarios are profound, since the time taken from design to analysis is greatly reduced, leading to dramatic gains in efficiency. In this manuscript, through a self-contained Matlab\u00ae implementation, we present an introduction to IGA applied to simple analysis problems and the related computer implementation aspects. Furthermore, implementation of the extended IGA which incorporates enrichment functions through the partition of unity method (PUM) is also presented, where several examples for both two-dimensional and three-dimensional fracture are illustrated. We also describe the use of IGA in the context of strong-form (collocation) formulations, which has been an area of research interest due to the potential for significant efficiency gains offered by these methods. The code which accompanies the present paper can be applied to one, two and three-dimensional problems for linear elasticity, linear elastic fracture mechanics, structural mechanics (beams/plates/shells including large displacements and rotations) and Poisson problems with or without enrichment. The Bezier extraction concept that allows the FE analysis to be performed efficiently on T-spline geometries is also incorporated. The article includes a summary of recent trends and developments within the field of IGA.", "venue": "Math. Comput. Simul.", "authors": ["Vinh Phu Nguyen", "Cosmin  Anitescu", "St\u00e9phane P. A. Bordas", "Timon  Rabczuk"], "year": 2015, "n_citations": 392}
{"id": 3267217, "s2_id": "050c641e23616f4364d27383fb9260da865486ae", "title": "Efficient Implementation of a Higher-Order Language with Built-In AD", "abstract": "We show that Automatic Differentiation (AD) operators can be provided in a dynamic language without sacrificing numeric performance. To achieve this, general forward and reverse AD functions are added to a simple high-level dynamic language, and support for them is included in an aggressive optimizing compiler. Novel technical mechanisms are discussed, which have the ability to migrate the AD transformations from run-time to compile-time. The resulting system, although only a research prototype, exhibits startlingly good performance. In fact, despite the potential inefficiencies entailed by support of a functional-programming language and a first-class AD operator, performance is competitive with the fastest available preprocessor-based Fortran AD systems. On benchmarks involving nested use of the AD operators, it can even dramatically exceed their performance.", "venue": "ArXiv", "authors": ["Jeffrey Mark Siskind", "Barak A. Pearlmutter"], "year": 2016, "n_citations": 11}
{"id": 3279447, "s2_id": "cb9cf7d03717d8b678b5ddc462a02a83ca3d8164", "title": "Computing All Space Curve Solutions of Polynomial Systems by Polyhedral Methods", "abstract": "A polyhedral method to solve a system of polynomial equations exploits its sparse structure via the Newton polytopes of the polynomials. We propose a hybrid symbolic-numeric method to compute a Puiseux series expansion for every space curve that is a solution of a polynomial system. The focus of this paper concerns the difficult case when the leading powers of the Puiseux series of the space curve are contained in the relative interior of a higher dimensional cone of the tropical prevariety. We show that this difficult case does not occur for polynomials with generic coefficients. To resolve this case, we propose to apply polyhedral end games to recover tropisms hidden in the tropical prevariety.", "venue": "CASC", "authors": ["Nathan  Bliss", "Jan  Verschelde"], "year": 2016, "n_citations": 1}
{"id": 3280944, "s2_id": "f1a17434749572bc79f36c9ff9ba1a41688c25f6", "title": "An Experimental Comparison of SONC and SOS Certificates for Unconstrained Optimization", "abstract": "Finding the minimum of a multivariate real polynomial is a well-known hard problem with various applications. We present a polynomial time algorithm to approximate such lower bounds via sums of nonnegative circuit polynomials (SONC). As a main result, we carry out the first large-scale comparison of SONC, using this algorithm and different geometric programming (GP) solvers, with the classical sums of squares (SOS) approach, using several of the most common semidefinite programming (SDP) solvers. SONC yields bounds competitive to SOS in several cases, but using significantly less time and memory. In particular, SONC/GP can handle much larger problem instances than SOS/SDP.", "venue": "ArXiv", "authors": ["Henning  Seidler", "Timo de Wolff"], "year": 2018, "n_citations": 18}
{"id": 3284298, "s2_id": "a4ccbc5488c742541c265d3a5fd7cdd4a704da5e", "title": "Iterative Methods for Systems' Solving - a C# approach", "abstract": "This work wishes to support various mathematical issues concerning the iterative methods with the help of new programming languages. We consider a way to show how problems in math have an answer by using different academic resources and different thoughts. Here we treat methods like Gauss-Seidel's, Cramer's and Gauss-Jordan's.", "venue": "ArXiv", "authors": ["Claudiu  Chirilov"], "year": 2009, "n_citations": 0}
{"id": 3287559, "s2_id": "e5295fce5fbe967c2ac9278410c36a9b8a694d3e", "title": "MACE 2.0 Reference Manual and Guide", "abstract": "MACE is a program that searches for finite models of first-order statements. The statement to be modeled is first translated to clauses, then to relational clauses; finally for the given domain size, the ground instances are constructed. A Davis-Putnam-Loveland-Logeman procedure decides the propositional problem, and any models found are translated to first-order models. MACE is a useful complement to the theorem prover Otter, with Otter searching for proofs and MACE looking for countermodels.", "venue": "ArXiv", "authors": ["William  McCune"], "year": 2001, "n_citations": 76}
{"id": 3290955, "s2_id": "6c69761efe97010470f50ab6a76905efb92476b4", "title": "NIFTY - Numerical Information Field Theory - a versatile Python library for signal inference", "abstract": "NIFTy, \u201cNumerical Information Field Theory\u201d, is a software package designed to enable the development of signal inference algorithms that operate regardless of the underlying spatial grid and its resolution. Its object-oriented framework is written in Python, although it accesses libraries written in Cython, C++, and C for eciency. NIFTy oers a toolkit that abstracts discretized representations of continuous spaces, fields in these spaces, and operators acting on fields into classes. Thereby, the correct normalization of operations on fields is taken care of automatically without concerning the user. This allows for an abstract formulation and programming of inference algorithms, including those derived within information field theory. Thus, NIFTy permits its user to rapidly prototype algorithms in 1D, and then apply the developed code in higher-dimensional settings of real world problems. The set of spaces on which NIFTy operates comprises point sets, n-dimensional regular grids, spherical spaces, their harmonic counterparts, and product spaces constructed as combinations of those. The functionality and diversity of the package is demonstrated by a Wiener filter code example that successfully runs without modification regardless of the space on which the inference problem is defined.", "venue": "ArXiv", "authors": ["Marco  Selig", "Michael R. Bell", "Henrik  Junklewitz", "Niels  Oppermann", "Martin  Reinecke", "Maksim  Greiner", "Carlos  Pachajoa", "Torsten A. En\u00dflin"], "year": 2013, "n_citations": 46}
{"id": 3293900, "s2_id": "3a03064fe692618fc3e827e5f760e2e868939c7d", "title": "Scheduling Massively Parallel Multigrid for Multilevel Monte Carlo Methods", "abstract": "The computational complexity of naive, sampling-based uncertainty quantification for 3D partial differential equations is extremely high. Multilevel approaches, such as multilevel Monte Carlo (MLMC), can reduce the complexity significantly when they are combined with a fast multigrid solver, but to exploit them fully in a parallel environment, sophisticated scheduling strategies are needed. We optimize the concurrent execution across the three layers of the MLMC method: parallelization across levels, across samples, and across the spatial grid. In a series of numerical tests, the influence on the overall performance of the \u201cscalability window\u201d of the multigrid solver (i.e., the range of processor numbers over which good parallel efficiency can be maintained) is illustrated. Different homogeneous and heterogeneous scheduling strategies are proposed and discussed. Finally, large 3D scaling experiments are carried out, including adaptivity.", "venue": "SIAM J. Sci. Comput.", "authors": ["Daniel  Drzisga", "Bj\u00f6rn  Gmeiner", "Ulrich  R\u00fcde", "Robert  Scheichl", "Barbara I. Wohlmuth"], "year": 2017, "n_citations": 26}
{"id": 3294479, "s2_id": "00ce3bc4c39d35311026be4e0ddaae28ee3d5946", "title": "Neural Networks for Beginners. A fast implementation in Matlab, Torch, TensorFlow", "abstract": "This report provides an introduction to some Machine Learning tools within the most common development environments. It mainly focuses on practical problems, skipping any theoretical introduction. It is oriented to both students trying to approach Machine Learning and experts looking for new frameworks.", "venue": "ArXiv", "authors": ["Francesco  Giannini", "Vincenzo  Laveglia", "Alessandro  Rossi", "Dario  Zanca", "Andrea  Zugarini"], "year": 2017, "n_citations": 5}
{"id": 3295565, "s2_id": "0ea06a5f79833bfd957ebfe692d233f0ff5cbba5", "title": "Scalable Local Timestepping on Octree Grids", "abstract": "Numerical solutions of hyperbolic partial differential equations(PDEs) are ubiquitous in science and engineering. Method of lines is a popular approach to discretize PDEs defined in spacetime, where space and time are discretized independently. When using explicit timesteppers on adaptive grids, the use of a global timestep-size dictated by the finest grid-spacing leads to inefficiencies in the coarser regions. Even though adaptive space discretizations are widely used in computational sciences, temporal adaptivity is less common due to its sophisticated nature. In this paper, we present highly scalable algorithms to enable local timestepping (LTS) for explicit timestepping schemes on fully adaptive octrees. We demonstrate the accuracy of our methods as well as the scalability of our framework across 16K cores in TACC's Frontera. We also present a speed up estimation model for LTS, which predicts the speedup compared to global timestepping (GTS) with an average of 0.1 relative error.", "venue": "ArXiv", "authors": ["Milinda  Fernando", "Hari  Sundar"], "year": 2020, "n_citations": 0}
{"id": 3303013, "s2_id": "3d9007e551b15af2fcfd488d7b6bb146a48cc701", "title": "lbmpy: Automatic code generation for efficient parallel lattice Boltzmann methods", "abstract": "Lattice Boltzmann methods are a popular mesoscopic alternative to macroscopic computational fluid dynamics solvers. Many variants have been developed that vary in complexity, accuracy, and computational cost. Extensions are available to simulate multi-phase, multi-component, turbulent, or non-Newtonian flows. In this work we present lbmpy, a code generation package that supports a wide variety of different methods and provides a generic development environment for new schemes as well. A high-level domain-specific language allows the user to formulate, extend and test various lattice Boltzmann schemes. The method specification is represented in a symbolic intermediate representation. Transformations that operate on this intermediate representation optimize and parallelize the method, yielding highly efficient lattice Boltzmann compute kernels not only for single- and two-relaxation-time schemes but also for multi-relaxation-time, cumulant, and entropically stabilized methods. An integration into the HPC framework waLBerla makes massively parallel, distributed simulations possible, which is demonstrated through scaling experiments on the SuperMUC-NG supercomputing system", "venue": "J. Comput. Sci.", "authors": ["Martin  Bauer", "Harald  K\u00f6stler", "Ulrich  R\u00fcde"], "year": 2021, "n_citations": 4}
{"id": 3305229, "s2_id": "ae7846370c2bbc8daeb8388514fdba42eea7fb6a", "title": "DGO: A new DIRECT-type MATLAB toolbox for derivative-free global optimization", "abstract": "In this work, we introduce DGO, a new MATLAB toolbox for derivative-free global optimization. DGO collects various deterministic derivative-free DIRECT-type algorithms for box-constrained, generallyconstrained, and problems with hidden constraints. Each sequential algorithm is implemented in two different ways: using static and dynamic data structures for more efficient information storage and organization. Furthermore, parallel schemes are applied to some promising algorithms within DGO. The toolbox is equipped with a graphical user interface (GUI), which ensures the user-friendly use of all functionalities available in DGO. Available features are demonstrated in detailed computational studies using a created comprehensive library of global optimization problems. Additionally, eleven classical engineering design problems are used to illustrate the potential of DGO to solve challenging real-world problems.", "venue": "ArXiv", "authors": ["Linas  Stripinis", "Remigijus  Paulavicius"], "year": 2021, "n_citations": 1}
{"id": 3309359, "s2_id": "d4b95b1a029aa4f7c080e370e97cf00a7a84b406", "title": "Certified Roundoff Error Bounds Using Bernstein Expansions and Sparse Krivine-Stengle Representations", "abstract": "Floating point error is a notable drawback of embedded systems implementation. Computing rigorous upper bounds of roundoff errors is absolutely necessary for the validation of critical software. This problem of computing rigorous upper bounds is even more challenging when addressing non-linear programs. In this paper, we propose and compare two new methods based on Bernstein expansions and sparse Krivine-Stengle representations, adapted from the field of the global optimization, to compute upper bounds of roundoff errors for programs implementing polynomial functions. We release two related software package FPBern and FPKriSten, and compare them with state of the art tools. We show that these two methods achieve competitive performance, while computing accurate upper bounds by comparison with other tools.", "venue": "2017 IEEE 24th Symposium on Computer Arithmetic (ARITH)", "authors": ["Alexandre  Rocca", "Victor  Magron", "Thao  Dang"], "year": 2017, "n_citations": 9}
{"id": 3310186, "s2_id": "c9ae920d60c0a8f568d21cb2dcdf10ff3598eee9", "title": "Parallel solver for shifted systems in a hybrid CPU-GPU framework", "abstract": "This paper proposes a combination of a hybrid CPU--GPU and a pure GPU software implementation of a direct algorithm for solving shifted linear systems $(A - \\sigma I)X = B$ with large number of complex shifts $\\sigma$ and multiple right-hand sides. Such problems often appear e.g. in control theory when evaluating the transfer function, or as a part of an algorithm performing interpolatory model reduction, as well as when computing pseudospectra and structured pseudospectra, or solving large linear systems of ordinary differential equations. The proposed algorithm first jointly reduces the general full $n\\times n$ matrix $A$ and the $n\\times m$ full right-hand side matrix $B$ to the controller Hessenberg canonical form that facilitates efficient solution: $A$ is transformed to a so-called $m$-Hessenberg form and $B$ is made upper-triangular. This is implemented as blocked highly parallel CPU--GPU hybrid algorithm; individual blocks are reduced by the CPU, and the necessary updates of the rest of the matrix are split among the cores of the CPU and the GPU. To enhance parallelization, the reduction and the updates are overlapped. In the next phase, the reduced $m$-Hessenberg--triangular systems are solved entirely on the GPU, with shifts divided into batches. The benefits of such load distribution are demonstrated by numerical experiments. In particular, we show that our proposed implementation provides an excellent basis for efficient implementations of computational methods in systems and control theory, from evaluation of transfer function to the interpolatory model reduction.", "venue": "SIAM J. Sci. Comput.", "authors": ["Nela  Bosner", "Zvonimir  Bujanovic", "Zlatko  Drmac"], "year": 2018, "n_citations": 1}
{"id": 3311239, "s2_id": "a80278337b6d3950b24171ccfbd135da8791f1b3", "title": "Acceleration of tensor-product operations for high-order finite element methods", "abstract": "This article is devoted to graphics processing unit (GPU) kernel optimization and performance analysis of three tensor-product operations arising in finite element methods. We provide a mathematical background to these operations and implementation details. Achieving close to peak performance for these operators requires extensive optimization because of the operators\u2019 properties: low arithmetic intensity, tiered structure, and the need to store intermediate results during the kernel execution. We give a guided overview of optimization strategies and we present a performance model that allows us to compare the efficacy of these optimizations against an empirically calibrated roofline.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Kasia  Swirydowicz", "Noel  Chalmers", "Ali  Karakus", "Timothy C. Warburton"], "year": 2019, "n_citations": 29}
{"id": 3315753, "s2_id": "2ad30f8ec2e02987d89c02218660bc24c6f1c3fc", "title": "Assembly of multiscale linear PDE operators", "abstract": "In numerous applications the mathematical model consists of different processes coupled across a lower dimensional manifold. Due to the multiscale coupling, finite element discretization of such models presents a challenge. Assuming that only singlescale finite element forms can be assembled we present here a simple algorithm for representing multiscale models as linear operators suitable for Krylov methods. Flexibility of the approach is demonstrated by numerical examples with coupling across dimensionality gap 1 and 2. Preconditioners for several of the problems are discussed.", "venue": "ENUMATH", "authors": ["Miroslav  Kuchta"], "year": 2019, "n_citations": 6}
{"id": 3318693, "s2_id": "10b14f0df382021cc39374418e869d97c448a96a", "title": "Supporting mixed-datatype matrix multiplication within the BLIS framework", "abstract": "We approach the problem of implementing mixed-datatype support within the general matrix multiplication (GEMM) operation of the BLIS framework, whereby each matrix operand A, B, and C may be stored as single- or double-precision real or complex values. Another factor of complexity, whereby the computation is allowed to take place in a precision different from the storage precisions of either A or B, is also included in the discussion. We first break the problem into mostly orthogonal dimensions, considering the mixing of domains separately from mixing precisions. Support for all combinations of matrix operands stored in either the real or complex domain is mapped out by enumerating the cases and describing an implementation approach for each. Supporting all combinations of storage and computation precisions is handled by typecasting the matrices at key stages of the computation---during packing and/or accumulation, as needed. Several optional optimizations are also documented. Performance results gathered on a 56-core Marvell ThunderX2 and a 52-core Intel Xeon Platinum demonstrate that high performance is mostly preserved, with modest slowdowns incurred from unavoidable typecast instructions. The mixed-datatype implementation confirms that combinatoric intractability is avoided, with the framework relying on only two assembly microkernels to implement 128 datatype combinations.", "venue": "ArXiv", "authors": ["Field G. Van Zee", "Devangi N. Parikh", "Robert A. van de Geijn"], "year": 2019, "n_citations": 0}
{"id": 3328940, "s2_id": "4315a2febf26efc1ddbb9b183f33a854818cb750", "title": "Series crimes", "abstract": "Puiseux series are power series in which the exponents can be fractional and/or negative rational numbers. Several computer algebra systems have one or more built-in or loadable functions for computing truncated Puiseux series. Some are generalized to allow coefficients containing functions of the series variable that are dominated by any power of that variable, such as logarithms and nested logarithms of the series variable. Some computer algebra systems also have built-in or loadable functions that compute infinite Puiseux series. Unfortunately, there are some little-known pitfalls in computing Puiseux series. The most serious of these is expansions within branch cuts or at branch points that are incorrect for some directions in the complex plane. For example with each series implementation accessible to you.\n Compare the value of (z2 + z3)3/2 with that of its truncated series expansion about z = 0, approximated at z = ?0.01. Does the series converge to a value that is the negative of the correct value?\n Compare the value of ln(z2 + z3) with its truncated series expansion about z = 0, approximated at z = ?0.01 + 0.1i. Does the series converge to a value that is incorrect by 2\u03c0i?\n Compare arctanh(?2 + ln(z)z) with its truncated series expansion about z = 0, approximated at z = ?0.01. Does the series converge to a value that is incorrect by about \u03c0i?\n At the time of this writing, most implementations that accommodate such series exhibit such errors. This article describes how to avoid these errors both for manual derivation of series and when implementing series packages.", "venue": "ACCA", "authors": ["David R. Stoutemyer"], "year": 2012, "n_citations": 1}
{"id": 3334031, "s2_id": "e49f36752408f551c3c740c9efe48c5c4599cdfc", "title": "CS-TSSOS: Correlative and term sparsity for large-scale polynomial optimization", "abstract": "This work proposes a new moment-SOS hierarchy, called CS-TSSOS, for solving large-scale sparse polynomial optimization problems. Its novelty is to exploit simultaneously correlative sparsity and term sparsity by combining advantages of two existing frameworks for sparse polynomial optimization. The former is due to Waki et al. while the latter was initially proposed by Wang et al. and later exploited in the TSSOS hierarchy. In doing so we obtain CS-TSSOS -- a two-level hierarchy of semidefinite programming relaxations with (i), the crucial property to involve quasi block-diagonal matrices and (ii), the guarantee of convergence to the global optimum. We demonstrate its efficiency on several large-scale instances of the celebrated Max-Cut problem and the important industrial optimal power flow problem, involving up to several thousands of variables and ten thousands of constraints.", "venue": "ArXiv", "authors": ["Jie  Wang", "Victor  Magron", "Jean B. Lasserre", "Ngoc Hoang Anh Mai"], "year": 2020, "n_citations": 22}
{"id": 3335466, "s2_id": "28f53314a76461917b909bb360a0fefc556ed1c4", "title": "A monadic, functional implementation of real numbers", "abstract": "Large scale real number computation is an essential ingredient in several modern mathematical proofs. Because such lengthy computations cannot be verified by hand, some mathematicians want to use software proof assistants to verify the correctness of these proofs. This paper develops a new implementation of the constructive real numbers and elementary functions for such proofs by using the monad properties of the completion operation on metric spaces. Bishop and Bridges's notion (Bishop and Bridges 1985) of regular sequences is generalised to what I call regular functions, which form the completion of any metric space. Using the monad operations, continuous functions on length spaces (which are a common subclass of metric spaces) are created by lifting continuous functions on the original space. A prototype Haskell implementation has been created. I believe that this approach yields a real number library that is reasonably efficient for computation, and still simple enough to verify its correctness easily.", "venue": "Mathematical Structures in Computer Science", "authors": ["Russell  O'Connor"], "year": 2007, "n_citations": 23}
{"id": 3336813, "s2_id": "2e274a043eec23df425799fcbd639810e0f98549", "title": "Compressed Basis GMRES on High Performance GPUs", "abstract": "Krylov methods provide a fast and highly parallel numerical tool for the iterative solution of many large-scale sparse linear systems. To a large extent, the performance of practical realizations of these methods is constrained by the communication bandwidth in all current computer architectures, motivating the recent investigation of sophisticated techniques to avoid, reduce, and/or hide the message-passing costs (in distributed platforms) and the memory accesses (in all architectures). \nThis paper introduces a new communication-reduction strategy for the (Krylov) GMRES solver that advocates for decoupling the storage format (i.e., the data representation in memory) of the orthogonal basis from the arithmetic precision that is employed during the operations with that basis. Given that the execution time of the GMRES solver is largely determined by the memory access, the datatype transforms can be mostly hidden, resulting in the acceleration of the iterative step via a lower volume of bits being retrieved from memory. Together with the special properties of the orthonormal basis (whose elements are all bounded by 1), this paves the road toward the aggressive customization of the storage format, which includes some floating point as well as fixed point formats with little impact on the convergence of the iterative process. \nWe develop a high performance implementation of the \"compressed basis GMRES\" solver in the Ginkgo sparse linear algebra library and using a large set of test problems from the SuiteSparse matrix collection we demonstrate robustness and performance advantages on a modern NVIDIA V100 GPU of up to 50% over the standard GMRES solver that stores all data in IEEE double precision.", "venue": "ArXiv", "authors": ["Jos\u00e9 Ignacio Aliaga", "Hartwig  Anzt", "Thomas  Gr\u00fctzmacher", "Enrique S. Quintana-Ort\u00ed", "Andr\u00e9s E. Tom\u00e1s"], "year": 2020, "n_citations": 3}
{"id": 3339493, "s2_id": "614a37772df418f0f08332411b56a52656e42b38", "title": "Enhancements to ACL2 in Versions 5.0, 6.0, and 6.1", "abstract": "We report on highlights of the ACL2 enhancements introduced in ACL2 releases since the 2011 ACL2 Workshop. Although many enhancements are critical for soundness or robustness, we focus in this paper on those improvements that could benefit users who are aware of them, but that might not be discovered in everyday practice.", "venue": "ACL2", "authors": ["Matt  Kaufmann", "J Strother Moore"], "year": 2013, "n_citations": 4}
{"id": 3341043, "s2_id": "ecaf8d7f87b5e3461bbd2a17a36b3e55fb69afa2", "title": "Ripser: efficient computation of Vietoris-Rips persistence barcodes", "abstract": "We present an algorithm for the computation of Vietoris\u2013Rips persistence barcodes and describe its implementation in the software Ripser. The method relies on implicit representations of the coboundary operator and the filtration order of the simplices, avoiding the explicit construction and storage of the filtration coboundary matrix. Moreover, it makes use of apparent pairs, a simple but powerful method for constructing a discrete gradient field from a total order on the simplices of a simplicial complex, which is also of independent interest. Our implementation shows substantial improvements over previous software both in time and memory usage.\n", "venue": "J. Appl. Comput. Topol.", "authors": ["Ulrich  Bauer"], "year": 2021, "n_citations": 109}
{"id": 3342166, "s2_id": "2f58f0ba0a9a1e807dbeb31b689b134d73b2d927", "title": "Solver Composition Across the PDE/Linear Algebra Barrier", "abstract": "The efficient solution of discretizations of coupled systems of partial differential equations (PDEs) is at the core of much of numerical simulation. Significant effort has been expended on scalable algorithms to precondition Krylov iterations for the linear systems that arise. With few exceptions, the reported numerical implementation of such solution strategies is specific to a particular model setup, and intimately ties the solver strategy to the discretization and PDE, especially when the preconditioner requires auxiliary operators. In this paper, we present recent improvements in the Firedrake finite element library that allow for straightforward development of the building blocks of extensible, composable preconditioners that decouple the solver from the model formulation. Our implementation extends the algebraic composability of linear solvers offered by the PETSc library by augmenting operators, and hence preconditioners, with the ability to provide any necessary auxiliary operators. Rather than s...", "venue": "SIAM J. Sci. Comput.", "authors": ["Robert C. Kirby", "Lawrence  Mitchell"], "year": 2018, "n_citations": 35}
{"id": 3342924, "s2_id": "46388658656e7f720158eeec3cf18f1eecc363fb", "title": "Vectorized OpenCL implementation of numerical integration for higher order finite elements", "abstract": "In our work we analyze computational aspects of the problem of numerical integration in finite element calculations and consider an OpenCL implementation of related algorithms for processors with wide vector registers. As a platform for testing the implementation we choose the PowerXCell processor, being an example of the Cell Broadband Engine (CellBE) architecture. Although the processor is considered old for today\u2019s standards (its design dates back to year 2001), we investigate its performance due to two features that it shares with recent Xeon Phi family of coprocessors: wide vector units and relatively slow connection of computing cores with main global memory. The performed analysis of parallelization options can also be used for designing numerical integration algorithms for other processors with vector registers, such as contemporary x86 microprocessors. We consider higher order finite element approximations and implement the standard algorithm of numerical integration for prismatic elements. Original contributions of the paper include the analysis of data movement and vector operations performed during code execution. Several versions of the implementation are developed and tested in practice.", "venue": "Comput. Math. Appl.", "authors": ["Filip  Kruzel", "Krzysztof  Banas"], "year": 2013, "n_citations": 11}
{"id": 3345357, "s2_id": "6edf244e27403cafef9831f68881e13b59ab4bfd", "title": "Marathon: An Open Source Software Library for the Analysis of Markov-Chain Monte Carlo Algorithms", "abstract": "We present the software library marathon, which is designed to support the analysis of sampling algorithms that are based on the Markov-Chain Monte Carlo principle. The main application of this library is the computation of properties of so-called state graphs, which represent the structure of Markov chains. We demonstrate applications and the usefulness of marathon by investigating the quality of several bounding methods on four well-known Markov chains for sampling perfect matchings and bipartite graphs. In a set of experiments, we compute the total mixing time and several of its bounds for a large number of input instances. We find that the upper bound gained by the famous canonical path method is often several magnitudes larger than the total mixing time and deteriorates with growing input size. In contrast, the spectral bound is found to be a precise approximation of the total mixing time.", "venue": "PloS one", "authors": ["Steffen  Rechner", "Annabell  Berger"], "year": 2016, "n_citations": 6}
{"id": 3351488, "s2_id": "dad2c58a6a833be5c24ed4078f31ae39fab2e33e", "title": "Interpolation of Shifted-Lacunary Polynomials", "abstract": "Abstract.Given a \u201cblack box\u201d function to evaluate an unknown rational polynomial $$f \\in {\\mathbb{Q}}[x]$$ at points modulo a prime p, we exhibit algorithms to compute the representation of the polynomial in the sparsest shifted power basis. That is, we determine the sparsity $$t \\in {\\mathbb{Z}}_{>0}$$, the shift $$\\alpha \\in {\\mathbb{Q}}$$, the exponents $${0 \\leq e_{1} < e_{2} < \\cdots < e_{t}}$$, and the coefficients $$c_{1}, \\ldots , c_{t} \\in {\\mathbb{Q}} \\setminus \\{0\\}$$ such that\n$$f(x) = c_{1}(x-\\alpha)^{e_{1}}+c_{2}(x-\\alpha)^{e_{2}}+ \\cdots +c_{t}(x-\\alpha)^{e_{t}}$$.The computed sparsity t is absolutely minimal over any shifted power basis. The novelty of our algorithm is that the complexity is polynomial in the (sparse) representation size, which may be logarithmic in the degree of f. Our method combines previous celebrated results on sparse interpolation and computing sparsest shifts, and provides a way to handle polynomials with extremely high degree which are, in some sense, sparse in information.", "venue": "computational complexity", "authors": ["Mark  Giesbrecht", "Daniel S. Roche"], "year": 2010, "n_citations": 26}
{"id": 3352401, "s2_id": "d4a9b52f95c7d7cdba16677a4b64079d2dda4458", "title": "Languages for modeling the RED active queue management algorithms: Modelica vs. Julia", "abstract": "This work is devoted to the study of the capabilities of the Modelica and Julia programming languages for the implementation of a continuously discrete paradigm in modeling hybrid systems that contain both continuous and discrete aspects of behavior. A system consisting of an incoming stream that is processed according to the Transmission Control Protocol (TCP) and a router that processes traffic using the Random Early Detection (RED) algorithm acts as a simulated threshold system.", "venue": "ArXiv", "authors": ["Anna Maria Yu. Apreutesey", "Anna V. Korolkova", "Dmitry S. Kulyabov"], "year": 2020, "n_citations": 0}
{"id": 3354226, "s2_id": "a5a477e5492ce732ce6a4f1547e052614d904801", "title": "LAGraph: Linear Algebra, Network Analysis Libraries, and the Study of Graph Algorithms", "abstract": "Graph algorithms can be expressed in terms of linear algebra. GraphBLAS is a library of low-level building blocks for such algorithms that targets algorithm developers. LAGraph builds on top of the GraphBLAS to target users of graph algorithms with high-level algorithms common in network analysis. In this paper, we describe the first release of the LAGraph library, the design decisions behind the library, and performance using the GAP benchmark suite. LAGraph, however, is much more than a library. It is also a project to document and analyze the full range of algorithms enabled by the GraphBLAS. To that end, we have developed a compact and intuitive notation for describing these algorithms. In this paper, we present that notation with examples from the GAP benchmark suite.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["G'abor  Sz'arnyas", "David A. Bader", "Timothy A. Davis", "James  Kitchen", "Timothy G. Mattson", "Scott  McMillan", "Erik  Welch"], "year": 2021, "n_citations": 1}
{"id": 3362543, "s2_id": "2ffd59da4a926f6ae8afb839388644b1f4551e02", "title": "Flexible Performant GEMM Kernels on GPUs", "abstract": "General Matrix Multiplication or GEMM kernels take center place in high performance computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by the two-language problem: it requires either low-level programming which implies low programmer productivity or using libraries that only offer a limited set of components. Because rephrasing algorithms in terms of established components often introduces overhead, the libraries' lack of flexibility limits the freedom to explore new algorithms. Researchers using GEMMs can hence not enjoy programming productivity, high performance, and research flexibility at once. \nIn this paper we solve this problem. We present three sets of abstractions and interfaces to program GEMMs within the scientific Julia programming language. The interfaces and abstractions are co-designed for researchers' needs and Julia's features to achieve sufficient separation of concerns and flexibility to easily extend basic GEMMs in many different ways without paying a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS and CUTLASS, we demonstrate that our performance is mostly on par with, and in some cases even exceeds, the libraries, without having to write a single line of code in CUDA C++ or assembly, and without facing flexibility limitations.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Thomas  Faingnaert", "Tim  Besard", "Bjorn De Sutter"], "year": 2021, "n_citations": 1}
{"id": 3365610, "s2_id": "b114bb386e0d301c0f16430eae5d2640fe014ab8", "title": "OpenMP GNU and Intel Fortran programs for solving the time-dependent Gross-Pitaevskii equation", "abstract": "We present Open Multi-Processing (OpenMP) version of Fortran 90 programs for solving the Gross\u2013Pitaevskii (GP) equation for a Bose\u2013Einstein condensate in one, two, and three spatial dimensions, optimized for use with GNU and Intel compilers. We use the split-step Crank\u2013Nicolson algorithm for imaginary- and real-time propagation, which enables efficient calculation of stationary and non-stationary solutions, respectively. The present OpenMP programs are designed for computers with multi-core processors and optimized for compiling with both commercially-licensed Intel Fortran and popular free open-source GNU Fortran compiler. The programs are easy to use and are elaborated with helpful comments for the users. All input parameters are listed at the beginning of each program. Different output files provide physical quantities such as energy, chemical potential, root-mean-square sizes, densities, etc. We also present speedup test results for new versions of the programs.", "venue": "Comput. Phys. Commun.", "authors": ["Luis E. Young-S.", "Paulsamy  Muruganandam", "Sadhan K. Adhikari", "Vladimir  Loncar", "Dusan  Vudragovic", "Antun  Balaz"], "year": 2017, "n_citations": 27}
{"id": 3366093, "s2_id": "f46fc8c6ffc985b27669db7ee91358607703478f", "title": "Alpaka -- An Abstraction Library for Parallel Kernel Acceleration", "abstract": "Porting applications to new hardware or programming models is a tedious and error prone process. Every help that eases these burdens is saving developer time that can then be invested into the advancement of the application itself instead of preserving the status-quo on a new platform. The Alpaka library defines and implements an abstract hierarchical redundant parallelism model. The model exploits parallelism and memory hierarchies on a node at all levels available in current hardware. By doing so, it allows to achieve platform and performance portability across various types of accelerators by ignoring specific unsupported levels and utilizing only the ones supported on a specific accelerator. All hardware types (multi-and many-core CPUs, GPUs and other accelerators) are supported for and can be programmed in the same way. The Alpaka C++ template interface allows for straightforward extension of the library to support other accelerators and specialization of its internals for optimization. Running Alpaka applications on a new (and supported) platform requires the change of only one source code line instead of a lot of #ifdefs.", "venue": "2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Erik  Zenker", "Benjamin  Worpitz", "Ren\u00e9  Widera", "Axel  Huebl", "Guido  Juckeland", "Andreas  Kn\u00fcpfer", "Wolfgang E. Nagel", "Michael  Bussmann"], "year": 2016, "n_citations": 32}
{"id": 3366709, "s2_id": "d7094d265196fce065dd25680ea876704a4ea319", "title": "TuckerMPI: A Parallel C++/MPI Software Package for Large-scale Data Compression via the Tucker Tensor Decomposition", "abstract": "Our goal is compression of massive-scale grid-structured data, such as the multi-terabyte output of a high-fidelity computational simulation. For such data sets, we have developed a new software package called TuckerMPI, a parallel C++/MPI software package for compressing distributed data. The approach is based on treating the data as a tensor, i.e., a multidimensional array, and computing its truncated Tucker decomposition, a higher-order analogue to the truncated singular value decomposition of a matrix. The result is a low-rank approximation of the original tensor-structured data. Compression efficiency is achieved by detecting latent global structure within the data, which we contrast to most compression methods that are focused on local structure. In this work, we describe TuckerMPI, our implementation of the truncated Tucker decomposition, including details of the data distribution and in-memory layouts, the parallel and serial implementations of the key kernels, and analysis of the storage, communication, and computational costs. We test the software on 4.5 terabyte and 6.7 terabyte data sets distributed across 100s of nodes (1000s of MPI processes), achieving compression rates between 100-200,000$\\times$ which equates to 99-99.999% compression (depending on the desired accuracy) in substantially less time than it would take to even read the same dataset from a parallel filesystem. Moreover, we show that our method also allows for reconstruction of partial or down-sampled data on a single node, without a parallel computer so long as the reconstructed portion is small enough to fit on a single machine, e.g., in the instance of reconstructing/visualizing a single down-sampled time step or computing summary statistics.", "venue": "ACM Trans. Math. Softw.", "authors": ["Grey  Ballard", "Alicia  Klinvex", "Tamara G. Kolda"], "year": 2020, "n_citations": 18}
{"id": 3368997, "s2_id": "42f18018ff47a2b5dbeda7e086e3ae9909994e81", "title": "Custom-Precision Mathematical Library Explorations for Code Profiling and Optimization", "abstract": "The typical processors used for scientific computing have fixed-width data-paths. This implies that mathematical libraries were specifically developed to target each of these fixed precisions (binary16, binary 32, binary64). However, to address the increasing energy consumption and throughput requirements of scientific applications, library and hardware designers are moving beyond this one-size-fits-all approach. In this article we propose to study the effects and benefits of using user-defined floating-point formats and target accuracies in calculations involving mathematical functions. Our tool collects input-data profiles and iteratively explores lower precisions for each call-site of a mathematical function in user applications. This profiling data will be a valuable asset for specializing and fine-tuning mathematical function implementations for a given application. We demonstrate the tool\u2019s capabilities on SGP4, a satellite tracking application. The profile data shows the potential for specialization and provides insight into answering where it is useful to provide variable-precision designs for elementary function evaluation.", "venue": "2020 IEEE 27th Symposium on Computer Arithmetic (ARITH)", "authors": ["David  Defour", "Pablo de Oliveira Castro", "Matei  Istoan", "Eric  Petit"], "year": 2020, "n_citations": 1}
{"id": 3376706, "s2_id": "431680275fc398a485033c8c080f02afe2ab9627", "title": "A Massively Parallel Algorithm for the Approximate Calculation of Inverse p-th Roots of Large Sparse Matrices", "abstract": "We present the submatrix method, a highly parallelizable method for the approximate calculation of inverse p-th roots of large sparse symmetric matrices which are required in different scientific applications. Following the idea of Approximate Computing, we allow imprecision in the final result in order to utilize the sparsity of the input matrix and to allow massively parallel execution. For an n x n matrix, the proposed algorithm allows to distribute the calculations over n nodes with only little communication overhead. The result matrix exhibits the same sparsity pattern as the input matrix, allowing for efficient reuse of allocated data structures. We evaluate the algorithm with respect to the error that it introduces into calculated results, as well as its performance and scalability. We demonstrate that the error is relatively limited for well-conditioned matrices and that results are still valuable for error-resilient applications like preconditioning even for ill-conditioned matrices. We discuss the execution time and scaling of the algorithm on a theoretical level and present a distributed implementation of the algorithm using MPI and OpenMP. We demonstrate the scalability of this implementation by running it on a high-performance compute cluster comprised of 1024 CPU cores, showing a speedup of 665x compared to single-threaded execution.", "venue": "PASC", "authors": ["Michael  Lass", "Stephan  Mohr", "Thomas D. K\u00fchne", "Christian  Plessl"], "year": 2018, "n_citations": 4}
{"id": 3377450, "s2_id": "4db3bdf0b4196e463fa1c7328c3c4626a664d5c3", "title": "Task\u2010based, GPU\u2010accelerated and robust library for solving dense nonsymmetric eigenvalue problems", "abstract": "In this paper, we present the StarNEig library for solving dense nonsymmetric standard and generalized eigenvalue problems. The library is built on top of the StarPU runtime system and targets both shared and distributed memory machines. Some components of the library have support for GPU acceleration. The library currently applies to real matrices with real and complex eigenvalues and all calculations are done using real arithmetic. Support for complex matrices is planned for a future release. This paper is aimed at potential users of the library. We describe the design choices and capabilities of the library, and contrast them to existing software such as LAPACK and ScaLAPACK. StarNEig implements a ScaLAPACK compatibility layer which should assist new users in the transition to StarNEig. We demonstrate the performance of the library with a sample of computational experiments.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Mirko  Myllykoski", "Carl Christian Kjelgaard Mikkelsen"], "year": 2021, "n_citations": 3}
{"id": 3379664, "s2_id": "8cfde3070ec24dcf889f4d862cc439cc97b75638", "title": "An Efficient Multicore Implementation of a Novel HSS-Structured Multifrontal Solver Using Randomized Sampling", "abstract": "We present a sparse linear system solver that is based on a multifrontal variant of Gaussian elimination and exploits low-rank approximation of the resulting dense frontal matrices. We use hierarchically semiseparable (HSS) matrices, which have low-rank off-diagonal blocks, to approximate the frontal matrices. For HSS matrix construction, a randomized sampling algorithm is used together with interpolative decompositions. The combination of the randomized compression with a fast ULV HSS factorization leads to a solver with lower computational complexity than the standard multifrontal method for many applications, resulting in speedups up to sevenfold for problems in our test suite. The implementation targets many-core systems by using task parallelism with dynamic runtime scheduling. Numerical experiments show performance improvements over state-of-the-art sparse direct solvers. The implementation achieves high performance and good scalability on a range of modern shared memory parallel systems, including ...", "venue": "SIAM J. Sci. Comput.", "authors": ["Pieter  Ghysels", "Xiaoye S. Li", "Fran\u00e7ois-Henry  Rouet", "Samuel  Williams", "Artem  Napov"], "year": 2016, "n_citations": 83}
{"id": 3389713, "s2_id": "c4a4698d6fa024e81483956167ccaa8aefc53366", "title": "POLYANA - A tool for the calculation of molecular radial distribution functions based on Molecular Dynamics trajectories", "abstract": "Abstract We present an application for the calculation of radial distribution functions for molecular centres of mass, based on trajectories generated by molecular simulation methods (Molecular Dynamics, Monte Carlo). When designing this application, the emphasis was placed on ease of use as well as ease of further development. In its current version, the program can read trajectories generated by the well-known DL_POLY package, but it can be easily extended to handle other formats. It is also very easy to \u2018hack\u2019 the program so it can compute intermolecular radial distribution functions for groups of interaction sites rather than whole molecules. Program summary Program title: Polyana Catalogue identifier: AEXP_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEXP_v1_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: MIT License No. of lines in distributed program, including test data, etc.: 33638 No. of bytes in distributed program, including test data, etc.: 574799 Distribution format: tar.gz Programming language: Fortran. Computer: Any computer that can run a Fortran compiler. Operating system: Tested on CentOS 6.6, Ubuntu 12.04 and Ubuntu 15.04. RAM: Proportional to the size of the simulated system (number of atoms). Classification: 4.14, 7.7. Nature of problem: Computation of radial distribution functions of molecular centres of mass in systems subjected to Periodic Boundary Conditions. Solution method: Molecules of arbitrary topology are \u2018unfolded\u2019 using a generic algorithm and their centres of mass are computed; then, standard procedures are applied. Additional comments: The code has been designed with ease of use in mind; in most cases, no user input will be required, except the simulation input/output files. Abbreviations: RDF: Radial Distribution Function PBC: Periodic Boundary Conditions Running time: Of the order of seconds to minutes, depending on platform and simulation size.", "venue": "Comput. Phys. Commun.", "authors": ["Christos  Dimitroulis", "Theophanes  Raptis", "Vasilios  Raptis"], "year": 2015, "n_citations": 3}
{"id": 3391679, "s2_id": "0fe36ba6726f92028140130bc7994b4a4aa25837", "title": "m-arcsinh: An Efficient and Reliable Function for SVM and MLP in scikit-learn", "abstract": "This paper describes the 'm-arcsinh', a modified ('m-') version of the inverse hyperbolic sine function ('arcsinh'). Kernel and activation functions enable Machine Learning (ML)-based algorithms, such as Support Vector Machine (SVM) and Multi-Layer Perceptron (MLP), to learn from data in a supervised manner. m-arcsinh, implemented in the open source Python library 'scikit-learn', is hereby presented as an efficient and reliable kernel and activation function for SVM and MLP respectively. Improvements in reliability and speed to convergence in classification tasks on fifteen (N = 15) datasets available from scikit-learn and the University California Irvine (UCI) Machine Learning repository are discussed. Experimental results demonstrate the overall competitive classification performance of both SVM and MLP, achieved via the proposed function. This function is compared to gold standard kernel and activation functions, demonstrating its overall competitive reliability regardless of the complexity of the classification tasks involved.", "venue": "ArXiv", "authors": ["Luca  Parisi"], "year": 2020, "n_citations": 5}
{"id": 3395297, "s2_id": "65f03e2b0d352ebdff2dd225eef4ca635f5c9013", "title": "Accelerating polynomial homotopy continuation on a graphics processing unit with double double and quad double arithmetic", "abstract": "Numerical continuation methods track a solution path defined by a homotopy. The systems we consider are defined by polynomials in several variables with complex coefficients. For larger dimensions and degrees, the numerical conditioning worsens and hardware double precision becomes often insufficient to reach the end of the solution path. With double double and quad double arithmetic, we can solve larger problems that we could not solve with hardware double arithmetic, but at a higher computational cost. This cost overhead can be compensated by acceleration on a Graphics Processing Unit (GPU). We describe our implementation and report on computational results on benchmark polynomial systems.", "venue": "PASCO", "authors": ["Jan  Verschelde", "Xiangcheng  Yu"], "year": 2015, "n_citations": 8}
{"id": 3395526, "s2_id": "8c00586169ceed105859daaf9114abbc8f3deb22", "title": "GURLS: a least squares library for supervised learning", "abstract": "We present GURLS, a least squares, modular, easy-to-extend software library for efficient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efficient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD license and is available for download at https://github.com/LCSL/GURLS.", "venue": "J. Mach. Learn. Res.", "authors": ["Andrea  Tacchetti", "Pavan Kumar Mallapragada", "Matteo  Santoro", "Lorenzo  Rosasco"], "year": 2013, "n_citations": 58}
{"id": 3400214, "s2_id": "7af3abb21905499e30686ea0fd964ebe9d47eee7", "title": "A Flexible Primal-Dual Toolbox", "abstract": "\\textbf{FlexBox} is a flexible MATLAB toolbox for finite dimensional convex variational problems in image processing and beyond. Such problems often consist of non-differentiable parts and involve linear operators. The toolbox uses a primal-dual scheme to avoid (computationally) inefficient operator inversion and to get reliable error estimates. From the user-side, \\textbf{FlexBox} expects the primal formulation of the problem, automatically decouples operators and dualizes the problem. For large-scale problems, \\textbf{FlexBox} also comes with a \\cpp-module, which can be used stand-alone or together with MATLAB via MEX-interfaces. Besides various pre-implemented data-fidelities and regularization-terms, \\textbf{FlexBox} is able to handle arbitrary operators while being easily extendable, due to its object-oriented design. The toolbox is available at \\href{http://www.flexbox.im}{this http URL}", "venue": "ArXiv", "authors": ["Hendrik  Dirks"], "year": 2016, "n_citations": 11}
{"id": 3402593, "s2_id": "e3fdb492396b0dce46da0ab1fa8e663cfd36c9eb", "title": "Best Practices for Replicability, Reproducibility and Reusability of Computer-Based Experiments Exemplified by Model Reduction Software", "abstract": "Over the recent years the importance of numerical experiments has gradually been more recognized. Nonetheless, sufficient documentation of how computational results have been obtained is often not available. Especially in the scientific computing and applied mathematics domain this is crucial, since numerical experiments are usually employed to verify the proposed hypothesis in a publication. This work aims to propose standards and best practices for the setup and publication of numerical experiments. Naturally, this amounts to a guideline for development, maintenance, and publication of numerical research software. Such a primer will enable the replicability and reproducibility of computer-based experiments and published results and also promote the reusability of the associated software.", "venue": "ArXiv", "authors": ["J\u00f6rg  Fehr", "Jan  Heiland", "Christian  Himpe", "Jens  Saak"], "year": 2016, "n_citations": 29}
{"id": 3407522, "s2_id": "253d3ba7a0a1e18ceba7d29dc7b3fac5c5c65de8", "title": "FluidFFT: Common API (C++ and Python) for Fast Fourier Transform HPC Libraries", "abstract": "The Python package fluidfft provides a common Python API for performing Fast Fourier Transforms (FFT) in sequential, in parallel and on GPU with different FFT libraries (FFTW, P3DFFT, PFFT, cuFFT). fluidfft is a comprehensive FFT framework which allows Python users to easily and efficiently perform FFT and the associated tasks, such as as computing linear operators and energy spectra. We describe the architecture of the package composed of C++ and Cython FFT classes, Python \"operator\" classes and Pythran functions. The package supplies utilities to easily test itself and benchmark the different FFT solutions for a particular case and on a particular machine. We present a performance scaling analysis on three different computing clusters and a microbenchmark showing that fluidfft is an interesting solution to write efficient Python applications using FFT.", "venue": "Journal of Open Research Software", "authors": ["Ashwin Vishnu Mohanan", "Cyrille  Bonamy", "Pierre  Augier"], "year": 2019, "n_citations": 4}
{"id": 3413022, "s2_id": "be4d952296db8c4aca32af44ae11ddb516f42781", "title": "Arithmetic algorithms for hereditarily binary natural numbers", "abstract": "In a typed functional language we specify a new tree-based number representation, hereditarily binary numbers, defined by applying recursively run-length encoding of bijective base-2 digits. Hereditarily binary numbers support arithmetic operations that are limited by the structural complexity of their operands, rather than their bitsizes. As a result, they enable new algorithms that, in the best case, collapse the complexity of arithmetic computations by super-exponential factors and in the worst case are within a constant factor of their traditional counterparts.", "venue": "SAC", "authors": ["Paul  Tarau", "Bill P. Buckles"], "year": 2014, "n_citations": 8}
{"id": 3413087, "s2_id": "80ebc4c54346c1b8bdb39d068c26de3ae0a13e05", "title": "Landau Collision Integral Solver with Adaptive Mesh Refinement on Emerging Architectures", "abstract": "The Landau collision integral is an accurate model for the small-angle dominated Coulomb collisions in fusion plasmas. We investigate a high order accurate, fully conservative, finite element discretization of the nonlinear multispecies Landau integral with adaptive mesh refinement using the PETSc library (\u016dlwww.mcs.anl.gov/petsc). We develop algorithms and techniques to efficiently utilize emerging architectures with an approach that minimizes memory usage and movement and is suitable for vector processing. The Landau collision integral is vectorized with Intel AVX-512 intrinsics and the solver sustains as much as 22% of the theoretical peak flop rate of the Second Generation Intel Xeon Phi (``Knights Landing'') processor.", "venue": "SIAM J. Sci. Comput.", "authors": ["Mark F. Adams", "Eero  Hirvijoki", "Matthew G. Knepley", "Jed  Brown", "Tobin  Isaac", "Richard T. Mills"], "year": 2017, "n_citations": 4}
{"id": 3423524, "s2_id": "e13cadc89e7499e6a73d51f020e8f934add97db0", "title": "The deal.II Library, Version 8.4", "abstract": "Abstract This paper provides an overview of the new features of the finite element library deal.II version 8.4.", "venue": "J. Num. Math.", "authors": ["Wolfgang  Bangerth", "Denis  Davydov", "Timo  Heister", "Luca  Heltai", "Guido  Kanschat", "Martin  Kronbichler", "Matthias  Maier", "Bruno  Turcksin", "David  Wells"], "year": 2016, "n_citations": 203}
{"id": 3424365, "s2_id": "9dea79987f9faba3a317b7260f7c9538f4efe542", "title": "Fast MATLAB assembly of FEM matrices in 2D and 3D: Edge elements", "abstract": "We propose an effective and flexible way to assemble finite element stiffness and mass matrices in MATLAB. We apply this for problems discretized by edge finite elements. Typical edge finite elements are Raviart-Thomas elements used in discretizations of H ( div ) spaces and Nedelec elements in discretizations of H ( curl ) spaces. We explain vectorization ideas and comment on a freely available MATLAB code which is fast and scalable with respect to time.", "venue": "Appl. Math. Comput.", "authors": ["Immanuel  Anjam", "Jan  Valdman"], "year": 2015, "n_citations": 41}
{"id": 3429899, "s2_id": "742c05a640bf63317a541b92422afa25547d81c4", "title": "Temporal vectorization for stencils", "abstract": "Stencil computations represent a very common class of nested loops in scientific and engineering applications. Exploiting vector units in modern CPUs is crucial to achieving peak performance. Previous vectorization approaches often consider the data space, in particular the innermost unit-strided loop. It leads to the well-known data alignment conflict problem that vector loads are overlapped due to the data sharing between continuous stencil computations. This paper proposes a novel temporal vectorization scheme for stencils. It vectorizes the stencil computation in the iteration space and assembles points with different time coordinates in one vector. The temporal vectorization leads to a small fixed number of vector reorganizations that is irrelevant to the vector length, stencil order, and dimension. Furthermore, it is also applicable to Gauss-Seidel stencils, whose vectorization is not well-studied. The effectiveness of the temporal vectorization is demonstrated by various Jacobi and Gauss-Seidel stencils.", "venue": "SC", "authors": ["Liang  Yuan", "Hang  Cao", "Yunquan  Zhang", "Kun  Li", "Pengqi  Lu", "Yue  Yue"], "year": 2021, "n_citations": 0}
{"id": 3431344, "s2_id": "f419789b7e3537f66d8aa9c182fcd31fc01a3e4e", "title": "Experiences from Software Engineering of Large Scale AMR Multiphysics Code Frameworks", "abstract": "Many research problems are being pursued through simulations that require multi-physics capabilities in codes, that are also able to run on HPC platforms. Multiphysics implies many solvers with divergent, and sometimes conflicting, demands on the software infrastructure. Additionally many multiphysics simulation codes make use of structured adaptive mesh refinement to achieve maximum resolution where needed within resource constraints, which places even more demands on software infrastructure. The software architecture and process for these codes, therefore, is a challenging task. In this experience report we detail the challenges faced, design choices made, and insights from two such major software efforts, FLASH and Chombo.", "venue": "ArXiv", "authors": ["Anshu  Dubey", "Brian van Straalen"], "year": 2013, "n_citations": 6}
{"id": 3432126, "s2_id": "1e037c31796165ee6972bd7e4b8d46f9159ea77b", "title": "Optimizations of the Eigensolvers in the ELPA Library", "abstract": "Abstract The solution of (generalized) eigenvalue problems for symmetric or Hermitian matrices is a common subtask of many numerical calculations in electronic structure theory or materials science. Depending on the scientific problem, solving the eigenvalue problem can easily amount to a sizeable fraction of the whole numerical calculation, and quite often is even the dominant part by far. For researchers in the field of computational materials science, an efficient and scalable solution of the eigenvalue problem is thus of major importance. The ELPA-library (Eigenvalue SoLvers for Petaflop-Applications) is a well-established dense direct eigenvalue solver library, which has proven to be very efficient and scalable up to very large core counts. It is in a wide-spread production use on a large variety of HPC systems worldwide, and is applied by many codes in the field of materials science. In this paper, we describe the latest optimizations of the ELPA-library for new HPC architectures of the Intel Skylake processor family with an AVX-512 SIMD instruction set, or for HPC systems accelerated with recent GPUs. Apart from those direct hardware-related optimizations, we also describe a complete redesign of the API in a modern modular way, which, apart from a much simpler and more flexible usability, leads to a new path to access system-specific performance optimizations. In order to ensure optimal performance for a particular scientific setting or a specific HPC system, the new API allows the user to influence in a straightforward way the internal details of the algorithms and of performance-critical parameters used in the ELPA-library. On top of that, we introduce an autotuning functionality, which allows for finding the best settings in a self-contained automated way, without the need of much user effort. In situations where many eigenvalue problems with similar settings have to be solved consecutively, the autotuning process of the ELPA-library can be done \u201con-the-fly\u201d, without the need of preceding the simulation with an \u201cartificial\u201d autotuning step. Practical applications from materials science which rely on reaching a numerical convergence limit by so-called self-consistency iterations can profit from the on-the-fly autotuning. On some examples of scientific interest, simulated with the FHI-aims application, the advantages of the latest optimizations of the ELPA-library are demonstrated.", "venue": "Parallel Comput.", "authors": ["Pavel  Kus", "Andreas  Marek", "Simone S. K\u00f6cher", "Hagen-Henrik  Kowalski", "Christian  Carbogno", "Christoph  Scheurer", "Karsten  Reuter", "Matthias  Scheffler", "Hermann  Lederer"], "year": 2019, "n_citations": 11}
{"id": 3436319, "s2_id": "4f15c7dedbe588a9929d4df04c33616b8e451171", "title": "Directional Consistency for Continuous Numerical Constraints", "abstract": "Bounds consistency is usually enforced on continuous constraints by first decomposing them into binary and ternary primitives. This decomposition has long been shown to drastically slow down the computation of solutions. To tackle this, Benhamou et al. have introduced an algorithm that avoids formally decomposing constraints. Its better efficiency compared to the former method has already been experimentally demonstrated. It is shown here that their algorithm implements a strategy to enforce on a continuous constraint a consistency akin to Directional Bounds Consistency as introduced by Dechter and Pearl for discrete problems. The algorithm is analyzed in this framework, and compared with algorithms that enforce bounds consistency. These theoretical results are eventually contrasted with new experimental results on standard benchmarks from the interval constraint community.", "venue": "ArXiv", "authors": ["Fr\u00e9d\u00e9ric  Goualard", "Laurent  Granvilliers"], "year": 2004, "n_citations": 0}
{"id": 3436895, "s2_id": "0c315ab16f6e1ddf1190cfb7812c7fab887302da", "title": "Wiki.openmath.org - How It Works, How You Can Participate", "abstract": "At this http URL, the OpenMath 2 and 3 Content Dictionaries are accessible via a semantic wiki interface, powered by the SWiM system. We shortly introduce the inner workings of the system, then describe how to use it, and conclude with first experiences gained from OpenMath society members working with the system and an outlook to further development plans.", "venue": "ArXiv", "authors": ["Christoph  Lange"], "year": 2010, "n_citations": 6}
{"id": 3439928, "s2_id": "35ccdcea6a329bcd546f682d80c00edfd0aaceb8", "title": "MizAR 40 for Mizar 40", "abstract": "As a present to Mizar on its 40th anniversary, we develop an AI/ATP system that in 30 seconds of real time on a 14-CPU machine automatically proves 40 % of the theorems in the latest official version of the Mizar Mathematical Library (MML). This is a considerable improvement over previous performance of large-theory AI/ATP methods measured on the whole MML. To achieve that, a large suite of AI/ATP methods is employed and further developed. We implement the most useful methods efficiently, to scale them to the 150000 formulas in MML. This reduces the training times over the corpus to 1\u20133 seconds, allowing a simple practical deployment of the methods in the online automated reasoning service for the Mizar users (MizAR$\\mathbb {A}\\mathbb {R}$).", "venue": "Journal of Automated Reasoning", "authors": ["Cezary  Kaliszyk", "Josef  Urban"], "year": 2015, "n_citations": 109}
{"id": 3447710, "s2_id": "936998936c4ed601ec8dccee3d24c71815b483ea", "title": "BSF-skeleton: A template for parallelization of iterative numerical algorithms on cluster computing systems", "abstract": "This article describes a method for creating applications for cluster computing systems using the parallel BSF-skeleton based on the original BSF (Bulk Synchronous Farm) model of parallel computations developed by the author earlier. This model uses the master/slave paradigm. The main advantage of the BSF model is that it allows to estimate the scalability of a parallel algorithm before its implementation. Another important feature of the BSF model is the representation of problem data in the form of lists that greatly simplifies the logic of building applications. The BSF-skeleton is designed for creating parallel programs in C++ using the MPI library. The scope of the BSF-skeleton is iterative numerical algorithms of high computational complexity. The BSF-skeleton has the following distinctive features. \u2022 The BSF-skeleton completely encapsulates all aspects that are associated with parallelizing a program. \u2022 The BSF-skeleton allows error-free compilation at all stages of application development. \u2022 The BSF-skeleton supports OpenMP programming model and workflows.", "venue": "MethodsX", "authors": ["Leonid B. Sokolinsky"], "year": 2021, "n_citations": 1}
{"id": 3454805, "s2_id": "463d84f45a38580e3196ad48123742c44b6d1ca1", "title": "CUDACLAW: A high-performance programmable GPU framework for the solution of hyperbolic PDEs", "abstract": "We present cudaclaw, a CUDA-based high performance data-parallel framework for the solution of multidimensional hyperbolic partial differential equation (PDE) systems, equations describing wave motion. cudaclaw allows computational scientists to solve such systems on GPUs without being burdened by the need to write CUDA code, worry about thread and block details, data layout, and data movement between the different levels of the memory hierarchy. The user defines the set of PDEs to be solved via a CUDA- independent serial Riemann solver and the framework takes care of orchestrating the computations and data transfers to maximize arithmetic throughput. cudaclaw treats the different spatial dimensions separately to allow suitable block sizes and dimensions to be used in the different directions, and includes a number of optimizations to minimize access to global memory.", "venue": "ArXiv", "authors": ["H. Gorune Ohannessian", "George M. Turkiyyah", "Aron J. Ahmadia", "David I. Ketcheson"], "year": 2018, "n_citations": 1}
{"id": 3459832, "s2_id": "e1960deb3ff5b5f5fe7332050e738df6c047ec28", "title": "GooFit: A library for massively parallelising maximum-likelihood fits", "abstract": "Fitting complicated models to large datasets is a bottleneck of many analyses. We present GooFit, a library and tool for constructing arbitrarily-complex probability density functions (PDFs) to be evaluated on nVidia GPUs or on multicore CPUs using OpenMP. The massive parallelisation of dividing up event calculations between hundreds of processors can achieve speedups of factors 200-300 in real-world problems.", "venue": "ArXiv", "authors": ["Rolf E. Andreassen", "Brian T. Meadows", "Weeraddana Manjula de Silva", "Michael D. Sokoloff", "Karen A. Tomko"], "year": 2013, "n_citations": 18}
{"id": 3466779, "s2_id": "aeb89c3190a875aeff5e2e0d1416d521ef78e69f", "title": "Design of a High-Performance GEMM-like Tensor\u2013Tensor Multiplication", "abstract": "We present \u201cGEMM-like Tensor\u2013Tensor multiplication\u201d (GETT), a novel approach for dense tensor contractions that mirrors the design of a high-performance general matrix\u2013matrix multiplication (GEMM). The critical insight behind GETT is the identification of three index sets, involved in the tensor contraction, which enable us to systematically reduce an arbitrary tensor contraction to loops around a highly tuned \u201cmacro-kernel.\u201d This macro-kernel operates on suitably prepared (\u201cpacked\u201d) sub-tensors that reside in a specified level of the cache hierarchy. In contrast to previous approaches to tensor contractions, GETT exhibits desirable features such as unit-stride memory accesses, cache-awareness, as well as full vectorization, without requiring auxiliary memory. We integrate GETT alongside the so-called Transpose\u2013Transpose-GEMM-Transpose and Loops-over-GEMM approaches into an open source \u201cTensor Contraction Code Generator.\u201d The performance results for a wide range of tensor contractions suggest that GETT has the potential of becoming the method of choice: While GETT exhibits excellent performance across the board, its effectiveness for bandwidth-bound tensor contractions is especially impressive, outperforming existing approaches by up to 12.4\u00d7. More precisely, GETT achieves speedups of up to 1.41\u00d7 over an equivalent-sized GEMM for bandwidth-bound tensor contractions while attaining up to 91.3% of peak floating-point performance for compute-bound tensor contractions.", "venue": "ACM Trans. Math. Softw.", "authors": ["Paul  Springer", "Paolo  Bientinesi"], "year": 2018, "n_citations": 59}
{"id": 3466826, "s2_id": "e592cd3ca6aef4d5244f8447f2a83020345c4b9c", "title": "Lambert W Function for Applications in Physics", "abstract": "Abstract The Lambert W ( x ) function and its possible applications in physics are presented. The actual numerical implementation in C++ consists of Halley\u2019s and Fritsch\u2019s iterations with initial approximations based on branch-point expansion, asymptotic series, rational fits, and continued-logarithm recursion. Program summary Program title: LambertW Catalogue identifier: AENC_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AENC_v1_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 No. of lines in distributed program, including test data, etc.: 1335 No. of bytes in distributed program, including test data, etc.: 25\u00a0283 Distribution format: tar.gz Programming language: C++ (with suitable wrappers it can be called from C, Fortran etc.), the supplied command-line utility is suitable for other scripting languages like sh, csh, awk, perl etc. Computer: All systems with a C++ compiler. Operating system: All Unix flavors, Windows. It might work with others. RAM: Small memory footprint, less than 1\u00a0MB Classification: 1.1, 4.7, 11.3, 11.9. Nature of problem: Find fast and accurate numerical implementation for the Lambert W function. Solution method: Halley\u2019s and Fritsch\u2019s iterations with initial approximations based on branch-point expansion, asymptotic series, rational fits, and continued logarithm recursion. Additional comments: Distribution file contains the command-line utility lambert-w. Doxygen comments, included in the source files. Makefile. Running time: The tests provided take only a few seconds to run.", "venue": "Comput. Phys. Commun.", "authors": ["Darko  Veberic"], "year": 2012, "n_citations": 92}
{"id": 3467340, "s2_id": "7f8d4af54337b935ec94291bb643a1d53ac783b3", "title": "The Vectorization of the Tersoff Multi-body Potential: An Exercise in Performance Portability", "abstract": "Molecular dynamics simulations, an indispensable research tool in computational chemistry and materials science, consume a significant portion of the supercomputing cycles around the world. We focus on multi-body potentials and aim at achieving performance portability. Compared with well-studied pair potentials, multibody potentials deliver increased simulation accuracy but are too complex for effective compiler optimization. Because of this, achieving cross-platform performance remains an open question. By abstracting from target architecture and computing precision, we develop a vectorization scheme applicable to both CPUs and accelerators. We present results for the Tersoff potential within the molecular dynamics code LAMMPS on several architectures, demonstrating efficiency gains not only for computational kernels, but also for large-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver is between 3 and 5 times faster than the pure MPI reference.", "venue": "SC16: International Conference for High Performance Computing, Networking, Storage and Analysis", "authors": ["Markus  H\u00f6hnerbach", "Ahmed E. Ismail", "Paolo  Bientinesi"], "year": 2016, "n_citations": 32}
{"id": 3471219, "s2_id": "e729cbebfc7671d58d269791dfe7742e2da3f4cf", "title": "Accelerating advection for atmospheric modelling on Xilinx and Intel FPGAs", "abstract": "Reconfigurable architectures, such as FPGAs, execute code at the electronics level, avoiding assumptions imposed by the general purpose black-box micro-architectures of CPUs and GPUs. Such tailored execution can result in increased performance and power efficiency, and as the HPC community moves towards exascale an important question is the role these hardware technologies can play in future supercomputers.In this paper we explore the porting of the PW advection kernel, an important code component used in a variety of atmospheric simulations and accounting for around 40% of the runtime of the popular Met Office NERC Cloud model (MONC). Building upon previous work which ported this kernel to an older generation of Xilinx FPGA, we target latest generation Xilinx Alveo U280 and Intel Stratix 10 FPGAs. Designing around the abstraction of an Application Specific Dataflow Machine (ASDM), we develop a design which is performance portable between vendors and explore implementation differences between the tool chains and compare kernel performance between FPGA hardware. This is followed by a more general performance comparison, scaling up the number of kernels on the Xilinx Alveo and Intel Stratix 10, against a 24 core Xeon Platinum Cascade Lake CPU and NVIDIA Tesla V100 GPU. When overlapping the transfer of data to and from the boards with compute, the FPGA solutions considerably outperform the CPU and, whilst falling short of the GPU in terms of performance, demonstrate power usage benefits, with the Alveo being especially power efficient. The result of this work is a comparison and set of design techniques that apply both to this specific atmospheric advection kernel on Xilinx and Intel FPGAs, and that are also of interest more widely when looking to accelerate HPC codes on a variety of reconfigurable architectures.", "venue": "2021 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Nick  Brown"], "year": 2021, "n_citations": 0}
{"id": 3473209, "s2_id": "719289c3fe21c960dd4e543daeb2295ef5971d6f", "title": "tvopt: A Python Framework for Time-Varying Optimization", "abstract": "This paper introduces tvopt, a Python framework for prototyping and benchmarking time-varying (or online) optimization algorithms. The paper first describes the theoretical approach that informed the development of tvopt. Then it discusses the different components of the framework and their use for modeling and solving time-varying optimization problems. In particular, tvopt provides functionalities for defining both centralized and distributed online problems, and a collection of built-in algorithms to solve them, for example gradient-based methods, ADMM and other splitting methods. Moreover, the framework implements prediction strategies to improve the accuracy of the online solvers. The paper then proposes some numerical results on a benchmark problem and discusses their implementation using tvopt. The code for tvopt is available at this https URL.", "venue": "ArXiv", "authors": ["Nicola  Bastianello"], "year": 2020, "n_citations": 1}
{"id": 3486899, "s2_id": "3dbe819a46204ef002a6c926d3b36187c45d6d60", "title": "DDscat.C++ User and programmer guide", "abstract": "DDscat.C++ 7.3.0 is a freely available open-source C++ software package applying the \"discrete dipole approximation\" (DDA) to calculate scattering and absorption of electromagnetic waves by targets with arbitrary geometries and a complex refractive index. DDscat.C++ is a clone of well known DDscat Fortran-90 software. We refer to DDscat as to the parent code in this document. Versions 7.3.0 of both codes have the identical functionality but the quite different implementation. Started as a teaching project, the DDscat.C++ code differs from the parent code DDscat in programming techniques and features, essential for C++ but quite seldom in Fortran. \nAs DDscat.C++ in its current version is just a clone, usage of DDscat.C++ for electromagnetic calculations is the same as of DDscat. Please, refer to \"User Guide for the Discrete Dipole Approximation Code DDSCAT 7.3\" to start using the code(s). \nThis document consists of two parts. In the first part we present Quick start guide for users who want to begin to use the code. Only differencies between DDscat.C++ and DDscat are explained. The second part of the document explains programming tips for the persons who want to change the code, to add the functionality or help the author with code refactoring and debugging.", "venue": "ArXiv", "authors": ["Vasyl  Choliy"], "year": 2014, "n_citations": 0}
{"id": 3494879, "s2_id": "eb0d69d2cfb5378ef055d40cf9cb23f073108162", "title": "Stochastic rounding and reduced-precision fixed-point arithmetic for solving neural ordinary differential equations", "abstract": "Although double-precision floating-point arithmetic currently dominates high-performance computing, there is increasing interest in smaller and simpler arithmetic types. The main reasons are potential improvements in energy efficiency and memory footprint and bandwidth. However, simply switching to lower-precision types typically results in increased numerical errors. We investigate approaches to improving the accuracy of reduced-precision fixed-point arithmetic types, using examples in an important domain for numerical computation in neuroscience: the solution of ordinary differential equations (ODEs). The Izhikevich neuron model is used to demonstrate that rounding has an important role in producing accurate spike timings from explicit ODE solution algorithms. In particular, fixed-point arithmetic with stochastic rounding consistently results in smaller errors compared to single-precision floating-point and fixed-point arithmetic with round-to-nearest across a range of neuron behaviours and ODE solvers. A computationally much cheaper alternative is also investigated, inspired by the concept of dither that is a widely understood mechanism for providing resolution below the least significant bit in digital signal processing. These results will have implications for the solution of ODEs in other subject areas, and should also be directly relevant to the huge range of practical problems that are represented by partial differential equations. This article is part of a discussion meeting issue \u2018Numerical algorithms for high-performance computational science\u2019.", "venue": "Philosophical Transactions of the Royal Society A", "authors": ["Michael  Hopkins", "Mantas  Mikaitis", "Dave R. Lester", "Steve  Furber"], "year": 2020, "n_citations": 24}
{"id": 3497254, "s2_id": "94087b402d6beb0ffe7d69c5872da84d53b892c4", "title": "A systematic review of Python packages for time series analysis", "abstract": "This paper presents a systematic review of Python packages focused on time series analysis. The objective is first to provide an overview of the different time series analysis tasks and preprocessing methods implemented, but also to give an overview of the development characteristics of the packages (e.g., dependencies, community size, etc.). This review is based on a search of literature databases as well as GitHub repositories. After the filtering process, 40 packages were analyzed. We classified the packages according to the analysis tasks implemented, the methods related to data preparation, and the means to evaluate the results produced (methods and access to evaluation data). We also reviewed the licenses, the packages community size, and the dependencies used. Among other things, our results show that forecasting is by far the most implemented task, that half of the packages provide access to real datasets or allow generating synthetic data, and that many packages depend on a few libraries (the most used ones being numpy, scipy and pandas). One of the lessons learned from this review is that the process of finding a given implementation is not inherently simple, and we hope that this review can help practitioners and researchers navigate the space of Python packages dedicated to time series analysis.", "venue": "ArXiv", "authors": ["Julien  Siebert", "Janek  Gross", "Christof  Schroth"], "year": 2021, "n_citations": 0}
{"id": 3502556, "s2_id": "c3fcb7e4a9cdefe44aad2218138d57b433f764e9", "title": "Multi-Threaded Dense Linear Algebra Libraries for Low-Power Asymmetric Multicore Processors", "abstract": "Dense linear algebra libraries, such as BLAS and LAPACK, provide a relevant collection of numerical tools for many scientific and engineering applications. While there exist high performance implementations of the BLAS (and LAPACK) functionality for many current multi-threaded architectures,the adaption of these libraries for asymmetric multicore processors (AMPs)is still pending. In this paper we address this challenge by developing an asymmetry-aware implementation of the BLAS, based on the BLIS framework, and tailored for AMPs equipped with two types of cores: fast/power hungry versus slow/energy efficient. For this purpose, we integrate coarse-grain and fine-grain parallelization strategies into the library routines which, respectively, dynamically distribute the workload between the two core types and statically repartition this work among the cores of the same type. \nOur results on an ARM big.LITTLE processor embedded in the Exynos 5422 SoC, using the asymmetry-aware version of the BLAS and a plain migration of the legacy version of LAPACK, experimentally assess the benefits, limitations, and potential of this approach.", "venue": "J. Comput. Sci.", "authors": ["Sandra  Catal'an", "Jos'e R. Herrero", "Francisco D. Igual", "Rafael  Rodr'iguez-S'anchez", "Enrique S. Quintana-Ort'i"], "year": 2018, "n_citations": 5}
{"id": 3504972, "s2_id": "fd9c96959efda56bb869c32720857aac3a84cd0e", "title": "Numerical integration on GPUs for higher order finite elements", "abstract": "The paper considers the problem of implementation on graphics processors of numerical integration routines for higher order finite element approximations. The design of suitable GPU kernels is investigated in the context of general purpose integration procedures, as well as particular example applications. The most important characteristic of the problem investigated is the large variation of required processor and memory resources associated with different degrees of approximating polynomials. The questions that we try to answer are whether it is possible to design a single integration kernel for different GPUs and different orders of approximation and what performance can be expected in such a case.", "venue": "Comput. Math. Appl.", "authors": ["Krzysztof  Banas", "Przemyslaw  Plaszewski", "Pawel  Maciol"], "year": 2014, "n_citations": 25}
{"id": 3507704, "s2_id": "0e58d974226178fa2ad09d4f0462eefb11f67e1f", "title": "SGDLibrary: A MATLAB library for stochastic gradient descent algorithms", "abstract": "We consider the problem of finding the minimizer of a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the form $\\min f(w) = \\frac{1}{n}\\sum_{i}f_i({w})$. This problem has been studied intensively in recent years in machine learning research field. One typical but promising approach for large-scale data is stochastic optimization algorithm. SGDLibrary is a flexible, extensible and efficient pure-Matlab library of a collection of stochastic optimization algorithms. The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment of those algorithms on various machine learning problems.", "venue": "ArXiv", "authors": ["Hiroyuki  Kasai"], "year": 2017, "n_citations": 1}
{"id": 3508747, "s2_id": "7b7ca53c71d94817ba46704e0f5dc6f43f9fe5ac", "title": "Multiprecision Arithmetic for Cryptology in C++ - Compile-Time Computations and Beating the Performance of Hand-Optimized Assembly at Run-Time", "abstract": "We describe a new C++ library for multiprecision arithmetic for numbers in the order of 100--500 bits, i.e., representable with just a few limbs. The library is written in \"optimizing-compiler-friendly\" C++, with an emphasis on the use of fixed-size arrays and particular function-argument-passing styles (including the avoidance of naked pointers) to allow the limbs to be allocated on the stack or even in registers. Depending on the particular functionality, we get close to, or significantly beat the performance of existing libraries for multiprecision arithmetic that employ hand-optimized assembly code. \nMost functions in the library are constant-time, which is a necessity for secure implementations of cryptographic protocols. \nBeyond the favorable runtime performance, our library is, to the best of the author's knowledge, the first library that offers big-integer computations during compile-time. For example, when implementing finite-field arithmetic with a fixed modulus, this feature enables the automatic precomputation (at compile time) of the special modulus-dependent constants required for Barrett and Montgomery reduction. Another application is to parse (at compile-time) a base-10-encoded big-integer literal.", "venue": "ArXiv", "authors": ["Niek J. Bouman"], "year": 2018, "n_citations": 1}
{"id": 3511563, "s2_id": "98884f4c55421ed48cec1483238d609dc1c1f0d9", "title": "Heterogeneous Highly Parallel Implementation of Matrix Exponentiation Using GPU", "abstract": "The vision of super computer at every desk can be realized by powerful and highly parallel CPUs or GPUs or APUs. Graphics processors once specialized for the graphics applications only, are now used for the highly computational intensive general purpose applications. Very expensive GFLOPs and TFLOP performance has become very cheap with the GPGPUs. Current work focuses mainly on the highly parallel implementation of Matrix Exponentiation. Matrix Exponentiation is widely used in many areas of scientific community ranging from highly critical flight, CAD simulations to financial, statistical applications. Proposed solution for Matrix Exponentiation uses OpenCL for exploiting the hyper parallelism offered by the many core GPGPUs. It employs many general GPU optimizations and architectural specific optimizations. This experimentation covers the optimizations targeted specific to the Scientific Graphics cards (Tesla-C2050). Heterogeneous Highly Parallel Matrix Exponentiation method has been tested for matrices of different sizes and with different powers. The devised Kernel has shown 1000X speedup and 44 fold speedup with the naive GPU Kernel.", "venue": "ArXiv", "authors": ["Chittampally Vasanth Raja", "Srinivas  Balasubramanian", "Prakash  Raghavendra"], "year": 2012, "n_citations": 1}
{"id": 3511624, "s2_id": "b9640f3c1ee5eb8db8bb532bdba26aff2f9986ce", "title": "Mean and variance estimation by kriging", "abstract": "The aim of the paper is to derive the numerical least-squares estimator for mean and variance of random variable. In order to do so the following questions have to be answered: (i) what is the statistical model for the estimation procedure? (ii) what are the properties of the estimator, like optimality (in which class) or asymptotic properties? (iii) how does the estimator work in practice, how compared to competing estimators?", "venue": "ArXiv", "authors": ["Tomasz  Suslo"], "year": 2004, "n_citations": 1}
{"id": 3512447, "s2_id": "17ecc93f6623615ff0a50811fbd6fbe38567b7ac", "title": "HIFIR: Hybrid Incomplete Factorization with Iterative Refinement for Preconditioning Ill-conditioned and Singular Systems", "abstract": "We introduce a software package called HIFIR for preconditioning sparse, unsymmetric, ill-conditioned, and potentially singular systems. HIFIR computes a hybrid incomplete factorization, which combines multilevel incomplete LU factorization with a truncated, rank-revealing QR factorization on the final Schur complement. This novel hybridization is based on the new theory of approximate generalized inverse and \u03b5-accuracy. It enables near-optimal preconditioners for consistent systems and enables flexible GMRES to solve inconsistent systems when coupled with iterative refinement. In this paper, we focus on some practical algorithmic and software issues of HIFIR. In particular, we introduce a new inverse-based rook pivoting into ILU, which improves the robustness and the overall efficiency for some ill-conditioned systems by significantly reducing the size of the final Schur complement for some systems. We also describe the software design of HIFIR in terms of its efficient data structures for supporting rook pivoting in a multilevel setting, its template-based generic programming interfaces for mixed-precision real and complex values in C++, and its user-friendly high-level interfaces in MATLAB and Python. We demonstrate the effectiveness of HIFIR for ill-conditioned or singular systems arising from several applications, including the Helmholtz equation, linear elasticity, stationary incompressible Navier\u2013Stokes equations, and time-dependent advection-diffusion equation.", "venue": "ArXiv", "authors": ["Qiao  Chen", "Xiangmin  Jiao"], "year": 2021, "n_citations": 0}
{"id": 3522499, "s2_id": "de5633a0d8a5b880b5b476cb186b64330e38ad53", "title": "RNGSSELIB: Program library for random number generation. More generators, parallel streams of random numbers and Fortran compatibility", "abstract": "In this update, we present the new version of the random number generator (RNG) library RNGSSELIB, which, in particular, contains fast SSE realizations of a number of modern and most reliable generators [1]. The new features are: (i) Fortran compatibility and examples of using the library in Fortran; (ii) new modern and reliable generators; (iii) the abilities to jump ahead inside a RNG sequence and to initialize up to 1019 independent random number streams with block splitting method. \nNew version program summary \nProgram title: RNGSSELIB \n \nCatalogue identifier: AEIT_v2_0 \n \nProgram summary URL:http://cpc.cs.qub.ac.uk/summaries/AEIT_v2_0.html \n \nProgram obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland \n \nLicensing provisions: Standard CPC license, http://cpc.cs.qub.ac.uk/licence/licence.html \n \nNo. of lines in distributed program, including test data, etc.: 9299 \n \nNo. of bytes in distributed program, including test data, etc.: 1768030 \n \nDistribution format: tar.gz \n \nProgramming language: C, Fortran \n \nComputer: PC, laptop, workstation, or server with Intel or AMD processor \n \nOperating system: Unix, Windows \n \nRAM: 4 Mbytes \n \nClassification: 4.13 \n \nDoes the new version supersede the previous version?: Yes \n \nCatalogue identifier of previous version: AEIT_v1_0 \n \nJournal reference of previous version: Comput. Phys. Comm. 182 (2011) 1518 \n \nNature of problem: \n \nAny calculation requiring uniform pseudorandom number generator, in particular, Monte Carlo calculations. Any calculation requiring parallel streams of uniform pseudorandom numbers. \n \nSolution method: \n \nThe library contains realizations of the following modern and reliable generators: MT19937 [2], MRG32K3A [3], LFSR113 [4], GM19, GM31, GM61 [1], and GM29, GM55, GQ58.1, GQ58.3, GQ58.4 [5, 6]. The library contains both the usual realizations and realizations based on the SSE command set. Usage of SSE commands allows for substantially improved performance of all generators. Also, the updated library contains the abilities to jump ahead inside a RNG sequence and to initialize independent random number streams with block splitting method for each of the RNGs. \n \nReasons for new version: \n1. \nIn order to implement Monte Carlo calculations, the implementation of independent streams of random numbers is necessary. Such an implementation of initializing random number streams with block splitting method is added to the new version for each of the RNGs. Jumping ahead inside a RNG sequence, which is necessary for the block splitting method, was also added for each of the RNGs. \n \n2. \nUsers asked us to add Fortran compatibility to the library. Fortran compatibility and the examples of using the library in Fortran for each of the RNGs are included in this version. \n \n3. \nDuring the last few years, the method of random number generation based on using the ensemble of transformations of a two-dimensional torus was essentially improved [5, 6]. Important properties, such as high-dimensional equidistribution, were established for the RNGs of this type. The proper choice of parameters was determined, which resulted in the validity of the high-dimensional equidistribution property, and, correspondingly, the new high-quality RNGs GM29, GM55.4, GQ58.1, GQ58.3, and GQ58.4 were proposed. These RNGs are now included in the RNGSSELIB library. \n \n \n \n \nSummary of revisions: \n1. \nWe added Fortran compatibility and examples of using the library in Fortran for each of the generators. \n \n2. \nNew modern and reliable generators GM29, GM55.4, GQ58.1, GQ58.3, and GQ58.4, which were introduced in [5] were added to the library. \n \n3. \nThe ability to jump ahead inside a RNG sequence and to initialize independent random number streams with block splitting method are added for each of the RNGs. \n \n \n \n \nRestrictions: \n \nFor SSE realizations of the generators, the Intel or AMD CPU supporting SSE2 command set is required. In order to use the SSE realization for the lfsr113 generator, the CPU must support the SSE4.1 command set. \n \nAdditional comments: \n \nThe function call interface has been slightly modified compared to the previous version in order to support Fortran compatibility. For each of the generators, RNGSSELIB supports the following functions, where rng should be replaced by the particular name of the RNG: \n \nvoid rng_skipahead_(rng_state* state, unsigned long long offset); \n \nvoid rng_init_(rng_state* state); \n \nvoid rng_init_sequence_(rng_state* state,unsigned long long SequenceNumber); \n \nunsigned int rng_generate_(rng_state* state); \n \nfloat rng_generate_uniform_float_(rng_state* state); \n \nunsigned int rng_sse_generate_(rng_sse_state* state); \n \nvoid rng_get_sse_state_(rng_state* state,rng_sse_state* sse_state); \n \nvoid rng_print_state_(rng_state* state); \n \nvoid rng_print_sse_state_(rng_sse_state* state); \n \nThere are a few peculiarities for some of the RNGs. For example, the function \n \nvoid mt19937_skipahead_(mt19937_state* state, unsigned long long a, unsigned b); \n \nskips ahead N=a\u22c52bN=a\u22c52b numbers, where N<2512N<2512, and the function \n \nvoid gm55_skipahead_(gm55_state* state, unsigned long long offset64, unsigned long long offset0); \n \nskips ahead N=264\u22c5N=264\u22c5 offset64+offset0 numbers. \n \nThe detailed function call interface can be found in the header files of the include directory. The examples of using the library can be found in the examples directory. \n \n \n \nTable\u00a01. \nInitialization of pseudorandom streams for RNGs. \n \n \n Function initializing sequence Number of sequences Maximal length \ngm19_init_sequence_ 10001000 6\u22c51066\u22c5106 \ngm29_init_short_sequence_ 108108 8\u22c51078\u22c5107 \ngm29_init_medium_sequence_ 106106 8\u22c51098\u22c5109 \ngm29_init_long_sequence_ 104104 8\u22c510118\u22c51011 \ngm31_init_short_sequence_ 109109 8\u22c51078\u22c5107 \ngm31_init_medium_sequence_ 107107 8\u22c51098\u22c5109 \ngm31_init_long_sequence_ 105105 8\u22c510118\u22c51011 \ngm55_init_short_sequence_ 10181018 10101010 \ngm55_init_long_sequence_ 4\u22c51094\u22c5109 10201020 \ngq58x1_init_short_sequence_ 108108 8\u22c51078\u22c5107 \ngq58x1_init_medium_sequence_ 106106 8\u22c51098\u22c5109 \ngq58x1_init_long_sequence_ 104104 8\u22c510118\u22c51011 \ngq58x3_init_short_sequence_ 2\u22c51082\u22c5108 8\u22c51078\u22c5107 \ngq58x3_init_medium_sequence_ 2\u22c51062\u22c5106 8\u22c51098\u22c5109 \ngq58x3_init_long_sequence_ 2\u22c51042\u22c5104 8\u22c510118\u22c51011 \ngq58x4_init_short_sequence_ 3\u22c51083\u22c5108 8\u22c51078\u22c5107 \ngq58x4_init_medium_sequence_ 3\u22c51063\u22c5106 8\u22c51098\u22c5109 \ngq58x4_init_long_sequence_ 3\u22c51043\u22c5104 8\u22c510118\u22c51011 \ngm61_init_sequence_ 1.8\u22c510191.8\u22c51019 10101010 \ngm61_init_long_sequence_ 4\u22c51094\u22c5109 3\u22c510253\u22c51025 \nlfsr113_init_sequence_ 3.8\u22c510183.8\u22c51018 10101010 \nlfsr113_init_long_sequence_ 4\u22c51094\u22c5109 10241024 \nmrg32k3a_init_sequence_ 10191019 10381038 \nmt19937_init_sequence_ 10191019 1013010130 \n \nFull-size table \n \nTable options \n \n \n \nView in workspace \n \nDownload as CSV \n \n \n \n \n \n \n \n \nTable 1 shows maximal number of sequences and maximal length of each sequence for each function initializing pseudorandom stream. \n \nThe algorithms used to jump ahead in the RNG sequence and to initialize parallel streams of pseudorandom numbers are described in detail in [7] and in [8]. \n \nThis work was partially supported by the Russian Foundation for Basic Research projects No. 12-07-13121 and 13-07-00570 and by the Supercomputing Center of Lomonosov Moscow State University [9]. \n \nRunning time: \n \nRunning time is of the order of 20\u00a0s for generating 109 pseudorandom numbers with a PC based on an Intel Core i7-940 CPU. Running time is analyzed in detail in [1] and in [5]. \n \nReferences: \n[1] \nL.Yu. Barash, L.N. Shchur, RNGSSELIB: Program library for random number generation, SSE2 realization, Computer Physics Communications, 182 (7) (2011) 1518\u20131527. \n \n[2] \nM. Matsumoto and T. Tishimura, Mersenne Twister: A 623-dimensionally equidistributed uniform pseudorandom number generator, ACM Trans. on Mod. and Comp. Simul. 8(1) (1998) 3\u201330. \n \n[3] \nP. L\u2019Ecuyer, Good Parameter Sets for Combined Multiple Recursive Random Number Generators, Oper. Res. 47(1) (1999) 159\u2013164. \n \n[4] \nP. L\u2019Ecuyer, Tables of Maximally-Equidistributed Combined LFSR Generators, Math. of Comp., 68(255) (1999) 261\u2013269. \n \n[5] \nL.Yu. Barash, Applying dissipative dynamical systems to pseudorandom number generation: Equidistribution property and statistical independence of bits at distances up to logarithm of mesh size, Europhysics Letters (EPL) 95 (2011) 10003. \n \n[6] \nL.Yu. Barash, Geometric and statistical properties of pseudorandom number generators based on multiple recursive transformations, Springer Proceedings in Mathematics and Statistics, Springer-Verlag, Berlin, Heidelberg, Vol. 23, (2012) 265\u2013280. \n \n[7] \nL.Yu. Barash, L.N. Shchur, On the generation of parallel streams of pseudorandom numbers, Programmnaya inzheneriya, 1 (2013) 24\u201332 (in Russian). \n \n[8] \nL.Yu. Barash, L.N. Shchur, PRAND: GPU accelerated parallel random number generation library: Using most reliable algorithms and applying parallelism of modern GPUs and CPUs, submitted to Comp. Phys. Commun. (2013). \n \n[9] \nVl.V. Voevodin, S.A. Zhumatiy, S.I. Sobolev, A.S. Antonov, P.A. Bryzgalov, D.A. Nikitenko, K.S. Stefanov, Vad.V. Voevodin, Practice of \u201cLomonosov\u201d Supercomputer, Open Systems J., Moscow: Open Systems Publ., 7 (2012) (In Russian).", "venue": "Comput. Phys. Commun.", "authors": ["Lev Yu. Barash", "Lev N. Shchur"], "year": 2013, "n_citations": 11}
{"id": 3525294, "s2_id": "fc964a3623667771e630b30e34343ca2ce3d8ac5", "title": "Understanding performance variability in standard and pipelined parallel Krylov solvers", "abstract": "In this work, we collect data from runs of Krylov subspace methods and pipelined Krylov algorithms in an effort to understand and model the impact of machine noise and other sources of variability on performance. We find large variability of Krylov iterations between compute nodes for standard methods that is reduced in pipelined algorithms, directly supporting conjecture, as well as large variation between statistical distributions of runtimes across iterations. Based on these results, we improve upon a previously introduced nondeterministic performance model by allowing iterations to fluctuate over time. We present our data from runs of various Krylov algorithms across multiple platforms as well as our updated non-stationary model that provides good agreement with observations. We also suggest how it can be used as a predictive tool.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Hannah  Morgan", "Patrick  Sanan", "Matthew  Knepley", "Richard Tran Mills"], "year": 2021, "n_citations": 1}
{"id": 3527881, "s2_id": "82029914b77caeac85ec6a7f3eb6aa47aef5cf77", "title": "SLEEF: A Portable Vectorized Library of C Standard Mathematical Functions", "abstract": "In this article, we present techniques used to implement our portable vectorized library of C standard mathematical functions written entirely in C language. In order to make the library portable while maintaining good performance, intrinsic functions of vector extensions are abstracted by inline functions or preprocessor macros. We implemented the functions so that they can use sub-features of vector extensions such as fused multiply-add, mask registers, and extraction of mantissa. In order to make computation with SIMD instructions efficient, the library only uses a small number of conditional branches, and all the computation paths are vectorized. We devised a variation of the Payne-Hanek argument reduction for trigonometric functions and a floating point remainder, both of which are suitable for vector computation. We compare the performance with our library to Intel SVML.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Naoki  Shibata", "Francesco  Petrogalli"], "year": 2020, "n_citations": 4}
{"id": 3530506, "s2_id": "9f6f09db2e6c1db8fba09844fbb02ac0260635a7", "title": "M-Learning: A New Paradigm of Learning Mathematics in Malaysia", "abstract": "M-Learning is a new learning paradigm of the new social structure with mobile and wireless technologies.Smart school is one of the four flagship applications for Multimedia Super Corridor (MSC) under Malaysian government initiative to improve education standard in the country. With the advances of mobile devices technologies, mobile learning could help the government in realizing the initiative. This paper discusses the prospect of implementing mobile learning for primary school students. It indicates significant and challenges and analysis of user perceptions on potential mobile applications through a survey done in primary school context. The authors propose the m-Learning for mathematics by allowing the extension of technology in the traditional classroom in term of learning and teaching.", "venue": "ArXiv", "authors": ["Saipunidzam  Mahamad", "Mohammad Noor Ibrahim", "Shakirah Mohd Taib"], "year": 2010, "n_citations": 59}
{"id": 3540212, "s2_id": "947bbc5ac0d3a055af1249fd59300e2a7c53f34d", "title": "An OpenGL and C++ based function library for curve and surface modeling in a large class of extended Chebyshev spaces", "abstract": "We propose a platform-independent multi-threaded function library that provides data structures to generate, differentiate and render both the ordinary basis and the normalized B-basis of a user-specified extended Chebyshev (EC) space that comprises the constants and can be identified with the solution space of a constant-coefficient homogeneous linear differential equation defined on a sufficiently small interval. Using the obtained normalized B-bases, our library can also generate, (partially) differentiate, modify and visualize a large family of so-called B-curves and tensor product B-surfaces. Moreover, the library also implements methods that can be used to perform dimension elevation, to subdivide B-curves and B-surfaces by means of de Casteljau-like B-algorithms, and to generate basis transformations for the B-representation of arbitrary integral curves and surfaces that are described in traditional parametric form by means of the ordinary bases of the underlying EC spaces. Independently of the algebraic, exponential, trigonometric or mixed type of the applied EC space, the proposed library is numerically stable and efficient up to a reasonable dimension number and may be useful for academics and engineers in the fields of Approximation Theory, Computer Aided Geometric Design, Computer Graphics, Isogeometric and Numerical Analysis.", "venue": "ACM Trans. Math. Softw.", "authors": ["\u00c1goston  R\u00f3th"], "year": 2019, "n_citations": 6}
{"id": 3542135, "s2_id": "93c2c8aecf101e301b69adce466b014c666e8d1f", "title": "Format abstraction for sparse tensor algebra compilers", "abstract": "This paper shows how to build a sparse tensor algebra compiler that is agnostic to tensor formats (data layouts). We develop an interface that describes formats in terms of their capabilities and properties, and show how to build a modular code generator where new formats can be added as plugins. We then describe six implementations of the interface that compose to form the dense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants thereof. With these implementations at hand, our code generator can generate code to compute any tensor algebra expression on any combination of the aforementioned formats. To demonstrate our technique, we have implemented it in the taco tensor algebra compiler. Our modular code generator design makes it simple to add support for new tensor formats, and the performance of the generated code is competitive with hand-optimized implementations. Furthermore, by extending taco to support a wider range of formats specialized for different application and data characteristics, we can improve end-user application performance. For example, if input data is provided in the COO format, our technique allows computing a single matrix-vector multiplication directly with the data in COO, which is up to 3.6\u00d7 faster than by first converting the data to CSR.", "venue": "Proc. ACM Program. Lang.", "authors": ["Stephen  Chou", "Fredrik  Kjolstad", "Saman P. Amarasinghe"], "year": 2018, "n_citations": 45}
{"id": 3544360, "s2_id": "fee1d74f17bafcf43eaeff180a7359aa2bd1f81e", "title": "A generic and fast C++ optimization framework", "abstract": "The development of the mlpack C++ machine learning library (this http URL) has required the design and implementation of a flexible, robust optimization system that is able to solve the types of arbitrary optimization problems that may arise all throughout machine learning problems. In this paper, we present the generic optimization framework that we have designed for mlpack. A key priority in the design was ease of implementation of both new optimizers and new objective functions to be optimized; therefore, implementation of a new optimizer requires only one method and implementation of a new objective function requires at most four functions. This leads to simple and intuitive code, which, for fast prototyping and experimentation, is of paramount importance. When compared to optimization frameworks of other libraries, we find that mlpack's supports more types of objective functions, is able to make optimizations that other frameworks do not, and seamlessly supports user-defined objective functions and optimizers.", "venue": "ArXiv", "authors": ["Ryan R. Curtin", "Shikhar  Bhardwaj", "Marcus  Edel", "Yannis  Mentekidis"], "year": 2017, "n_citations": 3}
{"id": 3544521, "s2_id": "1a5c9c9df6a81a2d0816b5538b1fb23bb4d7b793", "title": "A Domain-Specific Language and Editor for Parallel Particle Methods", "abstract": "Domain-specific languages (DSLs) are of increasing importance in scientific high-performance computing to reduce development costs, raise the level of abstraction, and, thus, ease scientific programming. However, designing DSLs is not easy, as it requires knowledge of the application domain and experience in language engineering and compilers. Consequently, many DSLs follow a weak approach using macros or text generators, which lack many of the features that make a DSL comfortable for programmers. Some of these features\u2014e.g., syntax highlighting, type inference, error reporting\u2014are easily provided by language workbenches, which combine language engineering techniques and tools in a common ecosystem. In this article, we present the Parallel Particle-Mesh Environment (PPME), a DSL and development environment for numerical simulations based on particle methods and hybrid particle-mesh methods. PPME uses the Meta Programming System, a projectional language workbench. PPME is the successor of the Parallel Particle-Mesh Language, a Fortran-based DSL that uses conventional implementation strategies. We analyze and compare both languages and demonstrate how the programmer\u2019s experience is improved using static analyses and projectional editing, i.e., code-structure editing, constrained by syntax, as opposed to free-text editing. We present an explicit domain model for particle abstractions and the first formal type system for particle methods.", "venue": "ACM Trans. Math. Softw.", "authors": ["Tobias  Nett", "Sven  Karol", "Jer\u00f3nimo  Castrill\u00f3n", "Ivo F. Sbalzarini"], "year": 2018, "n_citations": 13}
{"id": 3545252, "s2_id": "e16e6f088eb007415f5e51234e62a5e7376ca051", "title": "SymJAX: symbolic CPU/GPU/TPU programming", "abstract": "SymJAX is a symbolic programming version of JAX simplifying graph input/output/updates and providing additional functionalities for general machine learning and deep learning applications. From an user perspective SymJAX provides a la Theano experience with fast graph optimization/compilation and broad hardware support, along with Lasagne-like deep learning functionalities.", "venue": "ArXiv", "authors": ["Randall  Balestriero"], "year": 2020, "n_citations": 4}
{"id": 3550242, "s2_id": "909f2d0b440a08756da0de5b6ea44771cf3eb827", "title": "An algorithm for autonomously plotting solution sets in the presence of turning points", "abstract": "Plotting solution sets for particular equations may be complicated by the existence of turning points. Here we describe an algorithm which not only overcomes such problematic points, but does so in the most general of settings. Applications of the algorithm are highlighted through two examples: the first provides verification, while the second demonstrates a non-trivial application. The latter is followed by a thorough run-time analysis. While both examples deal with bivariate equations, it is discussed how the algorithm may be generalized for space curves in $\\R^{3}$.", "venue": "Appl. Math. Comput.", "authors": ["Steven  Pollack", "Daniel  Badali", "Jonathan  Pollack"], "year": 2014, "n_citations": 0}
{"id": 3552570, "s2_id": "966855b4171a259aace6e50375dd2834221677e0", "title": "Simultaneous computation of the row and column rank profiles", "abstract": "Gaussian elimination with full pivoting generates a PLUQ matrix decomposition. Depending on the strategy used in the search for pivots, the permutation matrices can reveal some information about the row or the column rank profiles of the matrix. We propose a new pivoting strategy that makes it possible to recover at the same time both row and column rank profiles of the input matrix and of any of its leading sub-matrices. We propose a rank-sensitive and quad-recursive algorithm that computes the latter PLUQ triangular decomposition of an m x n matrix of rank r in O(mnr\u03c9-2) field operations, with \u03c9 the exponent of matrix multiplication. Compared to the LEU decomposition by Malashonock, sharing a similar recursive structure, its time complexity is rank sensitive and has a lower leading constant. Over a word size finite field, this algorithm also improves the practical efficiency of previously known implementations.", "venue": "ISSAC '13", "authors": ["Jean-Guillaume  Dumas", "Cl\u00e9ment  Pernet", "Ziad  Sultan"], "year": 2013, "n_citations": 22}
{"id": 3554922, "s2_id": "8932d019e634185f3071726ebc200979458310b9", "title": "Applications of polyhedral computations to the analysis and verification of hardware and software systems", "abstract": "Convex polyhedra are the basis for several abstractions used in static analysis and computer-aided verification of complex and sometimes mission-critical systems. For such applications, the identification of an appropriate complexity-precision trade-off is a particularly acute problem, so that the availability of a wide spectrum of alternative solutions is mandatory. We survey the range of applications of polyhedral computations in this area; give an overview of the different classes of polyhedra that may be adopted; outline the main polyhedral operations required by automatic analyzers and verifiers; and look at some possible combinations of polyhedra with other numerical abstractions that have the potential to improve the precision of the analysis. Areas where further theoretical investigations can result in important contributions are highlighted.", "venue": "Theor. Comput. Sci.", "authors": ["Roberto  Bagnara", "Patricia M. Hill", "Enea  Zaffanella"], "year": 2009, "n_citations": 39}
{"id": 3558483, "s2_id": "7eaadb2bf6823b23f1ea0a0251e56ef9ea29b1db", "title": "TLib: A Flexible C++ Tensor Framework for Numerical Tensor Calculus", "abstract": "Numerical tensor calculus comprise basic tensor operations such as the entrywise addition and contraction of higher-order tensors. We present, TLib, flexible tensor framework with generic tensor functions and tensor classes that assists users to implement generic and flexible tensor algorithms in C++. The number of dimensions, the extents of the dimensions of the tensors and the contraction modes of the tensor operations can be runtime variable. Our framework provides tensor classes that simplify the management of multidimensional data and utilization of tensor operations using object-oriented and generic programming techniques. Additional stream classes help the user to verify and compare of numerical results with MATLAB. Tensor operations are implemented with generic tensor functions and in terms of multidimensional iterator types only, decoupling data storage representation and computation. The user can combine tensor functions with different tensor types and extend the framework without further modification of the classes or functions. We discuss the design and implementation of the framework and demonstrate its usage with examples that have been discussed in the literature.", "venue": "ArXiv", "authors": ["Cem  Bassoy"], "year": 2017, "n_citations": 0}
{"id": 3561546, "s2_id": "2ef3fb64a368db97e0da4b670cf3fab95b895fde", "title": "Strassen's Algorithm for Tensor Contraction", "abstract": "Tensor contraction (TC) is an important computational kernel widely used in numerous applications. It is a multidimensional generalization of matrix multiplication (GEMM). While Strassen's algorithm for GEMM is well studied in theory and practice, extending it to accelerate TC has not been previously pursued. Thus, we believe this to be the first paper to demonstrate how one can in practice speed up TC with Strassen's algorithm. By adopting a block-scatter-matrix format, a novel matrix-centric tensor layout, we can conceptually view TC as GEMM for a general stride storage, with an implicit tensor-to-matrix transformation. This insight enables us to tailor a recent state-of-the-art implementation of Strassen's algorithm to a recent state-of-the-art TC, avoiding explicit transpositions (permutations) and extra workspace, and reducing the overhead of memory movement that is incurred. Performance benefits are demonstrated with a performance model as well as in practice on modern single core, multicore, and di...", "venue": "SIAM J. Sci. Comput.", "authors": ["Jianyu  Huang", "Devin A. Matthews", "Robert A. van de Geijn"], "year": 2018, "n_citations": 10}
{"id": 3564001, "s2_id": "756cd23882001c00c7490419baf6526ea5a1dd1c", "title": "AutoMat - Automatic Differentiation for Generalized Standard Materials on GPUs", "abstract": "We propose a universal method for the evaluation of generalized standard materials that greatly simplifies the material law implementation process. By means of automatic differentiation and a numerical integration scheme, AutoMat reduces the implementation effort to two potential functions. By moving AutoMat to the GPU, we close the performance gap to conventional evaluation routines and demonstrate in detail that the expression level reverse mode of automatic differentiation as well as its extension to second order derivatives can be applied inside CUDA kernels. We underline the effectiveness and the applicability of AutoMat by integrating it into the FFT-based homogenization scheme of Moulinec and Suquet and discuss the benefits of using AutoMat with respect to runtime and solution accuracy for an elasto-viscoplastic example.", "venue": "Computational Mechanics", "authors": ["Johannes  Bl\u00fchdorn", "Nicolas R. Gauger", "Matthias  Kabel"], "year": 2021, "n_citations": 2}
{"id": 3568347, "s2_id": "3e377e5b9603c43541c11806a5d9f216b366d6ed", "title": "Blackbox: A procedure for parallel optimization of expensive black-box functions", "abstract": "This note provides a description of a procedure that is designed to efficiently optimize expensive black-box functions. It uses the response surface methodology by incorporating radial basis functions as the response model. A simple method based on a Latin hypercube is used for initial sampling. A modified version of CORS algorithm with space rescaling is used for the subsequent sampling. The procedure is able to scale on multicore processors by performing multiple function evaluations in parallel. The source code of the procedure is written in Python.", "venue": "ArXiv", "authors": ["Paul  Knysh", "Yannis  Korkolis"], "year": 2016, "n_citations": 23}
{"id": 3573836, "s2_id": "d8eb3f1e68a4e43a7efe98f3e8a5a89538db3e50", "title": "Functional Design of Computation Graph", "abstract": "Representing the control flow of a computer program as a computation graph can bring many benefits in a broad variety of domains where performance is critical. This technique is a core component of most major numerical libraries (TensorFlow, PyTorch, Theano, MXNet,...) and is successfully used to speed up and optimise many computationally-intensive tasks. However, different design choices in each of these libraries lead to noticeable differences in efficiency and in the way an end user writes efficient code. In this report, we detail the implementation and features of the computation graph support in OCaml's numerical library Owl, a recent entry in the world of scientific computing.", "venue": "ArXiv", "authors": ["Pierre  Vandenhove"], "year": 2018, "n_citations": 1}
{"id": 3578978, "s2_id": "1590c742d3c56d5dc15a8b3dfa759ece5e28be1c", "title": "Computing Sparse Jacobians and Hessians Using Algorithmic Differentiation", "abstract": "Stochastic scientific models and machine learning optimization estimators have a large number of variables; hence computing large sparse Jacobians and Hessians is important. Algorithmic differentiation (AD) greatly reduces the programming effort required to obtain the sparsity patterns and values for these matrices. We present forward, reverse, and subgraph methods for computing sparse Jacobians and Hessians. Special attention is given the the subgraph method because it is new. The coloring and compression steps are not necessary when computing sparse Jacobians and Hessians using subgraphs. Complexity analysis shows that for some problems the subgraph method is expected to be much faster. We compare C++ operator overloading implementations of the methods in the ADOL-C and CppAD software packages using some of the MINPACK-2 test problems. The experiments are set up in a way that makes them easy to run on different hardware, different systems, different compilers, other test problem and other AD packages. The setup time is the time to record the graph, compute sparsity, coloring, compression, and optimization of the graph. If the setup is necessary for each evaluation, the subgraph implementation has similar run times for sparse Jacobians and faster run times for sparse Hessians.", "venue": "ArXiv", "authors": ["Bradley M. Bell", "Kasper  Kristensen"], "year": 2021, "n_citations": 0}
{"id": 3580271, "s2_id": "d7007775479fe3d672ef6ab4a7ea1b43bd661618", "title": "Bspline solids manipulation with Mathematica", "abstract": "Bspline solids are used for solid objects modeling in R3. Mathematica incorporates a several commands to manipulate symbolic and graphically Bspline basis functions and to graphically manipulate Bsplines curves and surfaces; however, it does not incorporate any command to the graphical manipulation of Bspline solids. In this paper, we describe a new Mathematica program to compute and plotting the Bspline solids. The output obtained is consistent with Mathematica's notation. The performance of the commands are discussed by using some illustrative examples.", "venue": "ArXiv", "authors": ["Robert  Ipanaqu\u00e9", "Ricardo  Velezmoro", "R. T. Urbina"], "year": 2019, "n_citations": 0}
{"id": 3587548, "s2_id": "e68054eb8159adbf98ff7dbd44f18030cf53fd58", "title": "NetEvo: A computational framework for the evolution of dynamical complex networks", "abstract": "NetEvo is a computational framework designed to help understand the evolution of dynamical complex networks. It provides flexible tools for the simulation of dynamical processes on networks and methods for the evolution of underlying topological structures. The concept of a supervisor is used to bring together both these aspects in a coherent way. It is the job of the supervisor to rewire the network topology and alter model parameters such that a user specified performance measure is minimised. This performance measure can make use of current topological information and simulated dynamical output from the system. Such an", "venue": "ArXiv", "authors": ["Thomas E. Gorochowski", "Mario di Bernardo", "Claire S. Grierson"], "year": 2009, "n_citations": 6}
{"id": 3588153, "s2_id": "575b555beae0d658d7bbced4361299230e66c657", "title": "Automatically harnessing sparse acceleration", "abstract": "Sparse linear algebra is central to many scientific programs, yet compilers fail to optimize it well. High-performance libraries are available, but adoption costs are significant. Moreover, libraries tie programs into vendor-specific software and hardware ecosystems, creating non-portable code. In this paper, we develop a new approach based on our specification Language for implementers of Linear Algebra Computations (LiLAC). Rather than requiring the application developer to (re)write every program for a given library, the burden is shifted to a one-off description by the library implementer. The LiLAC-enabled compiler uses this to insert appropriate library routines without source code changes. LiLAC provides automatic data marshaling, maintaining state between calls and minimizing data transfers. Appropriate places for library insertion are detected in compiler intermediate representation, independent of source languages. We evaluated on large-scale scientific applications written in FORTRAN; standard C/C++ and FORTRAN benchmarks; and C++ graph analytics kernels. Across heterogeneous platforms, applications and data sets we show speedups of 1.1\u00d7to over 10\u00d7without user intervention.", "venue": "CC", "authors": ["Philip  Ginsbach", "Bruce  Collie", "Michael F. P. O'Boyle"], "year": 2020, "n_citations": 3}
{"id": 3597864, "s2_id": "82bf74448ec8bbbd41513f80e70f7c96a83fc461", "title": "A domain-decomposing parallel sparse linear system solver", "abstract": "The solution of large sparse linear systems is often the most time-consuming part of many science and engineering applications. Computational fluid dynamics, circuit simulation, power network analysis, and material science are just a few examples of the application areas in which large sparse linear systems need to be solved effectively. In this paper, we introduce a new parallel hybrid sparse linear system solver for distributed memory architectures that contains both direct and iterative components. We show that by using our solver one can alleviate the drawbacks of direct and iterative solvers, achieving better scalability than with direct solvers and more robustness than with classical preconditioned iterative solvers. Comparisons to well-known direct and iterative solvers on a parallel architecture are provided.", "venue": "J. Comput. Appl. Math.", "authors": ["Murat  Manguoglu"], "year": 2011, "n_citations": 24}
{"id": 3598505, "s2_id": "bb21d5e1bc445d9ce95162882236992d55fda093", "title": "ChASE: Chebyshev Accelerated Subspace iteration Eigensolver for sequences of Hermitian eigenvalue problems", "abstract": "Solving dense Hermitian eigenproblems arranged in a sequence with direct solvers fails to take advantage of those spectral properties which are pertinent to the entire sequence, and not just to the single problem. When such features take the form of correlations between the eigenvectors of consecutive problems, as is the case in many real-world applications, the potential benefit of exploiting them can be substantial. We present ChASE, a modern algorithm and library based on subspace iteration with polynomial acceleration. Novel to ChASE is the computation of the spectral estimates that enter in the filter and an optimization of the polynomial degree which further reduces the necessary FLOPs. ChASE is written in C++ using the modern software engineering concepts which favor a simple integration in application codes and a straightforward portability over heterogeneous platforms. When solving sequences of Hermitian eigenproblems for a portion of their extremal spectrum, ChASE greatly benefits from the sequence's spectral properties and outperforms direct solvers in many scenarios. The library ships with two distinct parallelization schemes, supports execution over distributed GPUs, and it is easily extensible to other parallel computing architectures.", "venue": "ACM Trans. Math. Softw.", "authors": ["Jan  Winkelmann", "Paul  Springer", "Edoardo Di Napoli"], "year": 2019, "n_citations": 6}
{"id": 3600442, "s2_id": "e5eb2dbc7948963c3c0a4d7f399f9201e7cdb7c1", "title": "Fast MATLAB evaluation of nonlinear energies using FEM in 2D and 3D: nodal elements", "abstract": "Nonlinear energy functionals appearing in the calculus of variations can be discretized by the finite element (FE) method and formulated as a sum of energy contributions from local elements. A fast evaluation of energy functionals containing the first order gradient terms is a central part of this contribution. We describe a vectorized implementation using the simplest P1 elements in which all energy contributions are evaluated all at once without the loop over triangular or tetrahedral elements. Furthermore, in connection to the firstorder optimization methods, the discrete gradient of the energy functional is assembled in a way that gradient components are evaluated over all degrees of freedom all at once. The key ingredient is the vectorization of exact or approximate energy gradients over nodal patches. It leads to a time-efficient implementation at higher memory-cost. Provided codes in MATLAB related to 2D/3D hyperelasticity and 2D p-Laplacian problem are available for download and structured in a way it can be easily extended to other types of vector or scalar forms of energies.", "venue": "ArXiv", "authors": ["Alexej  Moskovka", "Jan  Valdman"], "year": 2021, "n_citations": 0}
{"id": 3603991, "s2_id": "fd02ceac2c1f35131f9b4023f2ec6acfa02b419f", "title": "PyFAI: a Python library for high performance azimuthal integration on GPU", "abstract": "PyFAI is an open-source Python library for Fast Azimuthal Integration which provides 1D- and 2D-azimuthal regrouping with a clean programming interface and tools for calibration. The library is suitable for interactive use in Python. In optimising the speed of the algorithms there has been no compromise on the accuracy compared to reference software. Fast integrations are obtained by the combination of an algorithm ensuring that each pixel from the detector provides a direct contribution to the final diffraction pattern and an OpenCL implementation that can use graphics cards for acceleration. This contribution describes how the algorithms were modified to work better in parallel.", "venue": "Powder Diffraction", "authors": ["J\u00e9r\u00f4me  Kieffer", "Giannis  Ashiotis"], "year": 2013, "n_citations": 66}
{"id": 3607961, "s2_id": "dade3556e38c7c36be9453c80850750d159e6339", "title": "Automated Derivation of the Adjoint of High-Level Transient Finite Element Programs", "abstract": "In this paper we demonstrate a new technique for deriving discrete adjoint and tangent linear models of a finite element model. The technique is significantly more efficient and automatic than standard algorithmic differentiation techniques. The approach relies on a high-level symbolic representation of the forward problem. In contrast to developing a model directly in Fortran or C++, high-level systems allow the developer to express the variational problems to be solved in near-mathematical notation. As such, these systems have a key advantage: since the mathematical structure of the problem is preserved, they are more amenable to automated analysis and manipulation. The framework introduced here is implemented in a freely available software package named dolfin-adjoint, based on the FEniCS Project. Our approach to automated adjoint derivation relies on run-time annotation of the temporal structure of the model and employs the FEniCS finite element form compiler to automatically generate the low-level co...", "venue": "SIAM J. Sci. Comput.", "authors": ["Patrick E. Farrell", "David A. Ham", "Simon W. Funke", "Marie E. Rognes"], "year": 2013, "n_citations": 146}
{"id": 3616472, "s2_id": "cb23159a6b1f02199a3e9961786ab6b0aa63954b", "title": "BSEPACK User's Guide", "abstract": "Author(s): Shao, Meiyue; Yang, Chao | Abstract: This is the user manual for the software package BSEPACK (Bethe--Salpeter Eigenvalue Solver Package).", "venue": "ArXiv", "authors": ["Meiyue  Shao", "Chao  Yang"], "year": 2016, "n_citations": 6}
{"id": 3628700, "s2_id": "8ba4a85e01f3611f2be0672dc33d576696fb7b58", "title": "Accelerating eigenvector and pseudospectra computation using blocked multi-shift triangular solves", "abstract": "Multi-shift triangular solves are basic linear algebra calculations with applications in eigenvector and pseudospectra computation. We propose blocked algorithms that efficiently exploit Level 3 BLAS to perform multi-shift triangular solves and safe multi-shift triangular solves. Numerical experiments indicate that computing triangular eigenvectors with a safe multi-shift triangular solve achieves speedups by a factor of 60 relative to LAPACK. This algorithm accelerates the calculation of general eigenvectors threefold. When using multi-shift triangular solves to compute pseudospectra, we report ninefold speedups relative to EigTool.", "venue": "ArXiv", "authors": ["Tim  Moon", "Jack  Poulson"], "year": 2016, "n_citations": 4}
{"id": 3631724, "s2_id": "5c4c3b557f1934aebf1bb34a38b9226866bb56d3", "title": "Binomial Checkpointing for Arbitrary Programs with No User Annotation", "abstract": "Heretofore, automatic checkpointing at procedure-call boundaries, to reduce the space complexity of reverse mode, has been provided by systems like Tapenade. However, binomial checkpointing, or treeverse, has only been provided in Automatic Differentiation (AD) systems in special cases, e.g., through user-provided pragmas on DO loops in Tapenade, or as the nested taping mechanism in adol-c for time integration processes, which requires that user code be refactored. We present a framework for applying binomial checkpointing to arbitrary code with no special annotation or refactoring required. This is accomplished by applying binomial checkpointing directly to a program trace. This trace is produced by a general-purpose checkpointing mechanism that is orthogonal to AD.", "venue": "ArXiv", "authors": ["Jeffrey Mark Siskind", "Barak A. Pearlmutter"], "year": 2016, "n_citations": 2}
{"id": 3633214, "s2_id": "8bd458bbeefdc0310d4c1939911b22b61d23b411", "title": "Fireflies: New Software for Interactively Exploring Dynamical Systems Using GPU Computing", "abstract": "In non-linear systems, where explicit analytic solutions usually can't be found, visualisation is a powerful approach which can give insights into the dynamical behaviour of models; it is also crucial for teaching this area of mathematics. In this paper we present new software, Fireflies, which exploits the power of graphical processing unit (GPU) computing to produce spectacular interactive visualisations of arbitrary systems of ordinary differential equations. In contrast to typical phase portraits, Fireflies draws the current position of trajectories (projected onto 2D or 3D space) as single points of light, which move as the system is simulated. Due to the massively parallel nature of GPU hardware, Fireflies is able to simulate millions of trajectories in parallel (even on standard desktop computer hardware), producing \"swarms\" of particles that move around the screen in real-time according to the equations of the system. Particles that move forwards in time reveal stable attractors (e.g. fixed points and limit cycles), while the option of integrating another group of trajectories backwards in time can reveal unstable objects (repellers). Fireflies allows the user to change the parameters of the system as it is running, in order to see the effect that they have on the dynamics and to observe bifurcations. We demonstrate the capabilities of the software with three examples: a two-dimensional \"mean field\" model of neuronal activity, the classical Lorenz system, and a 15-dimensional model of three interacting biologically realistic neurons.", "venue": "Int. J. Bifurc. Chaos", "authors": ["Robert  Merrison-Hort"], "year": 2015, "n_citations": 2}
{"id": 3646408, "s2_id": "08a5b1ed9208c90fda60cb52208a4ebd65a565f7", "title": "hIPPYlib-MUQ: A Bayesian Inference Software Framework for Integration of Data with Complex Predictive Models under Uncertainty", "abstract": "Bayesian inference provides a systematic framework for integration of data with mathematical models to quantify the uncertainty in the solution of the inverse problem. However, solution of Bayesian inverse problems governed by complex forward models described by partial differential equations (PDEs) remains prohibitive with black-box Markov chain Monte Carlo (MCMC) methods. We present hIPPYlib-MUQ, an extensible and scalable software framework that contains implementations of state-of-the art algorithms aimed to overcome the challenges of high-dimensional, PDE-constrained Bayesian inverse problems. These algorithms accelerate MCMC sampling by exploiting the geometry and intrinsic low-dimensionality of parameter space via derivative information and low rank approximation. The software integrates two complementary opensource software packages, hIPPYlib and MUQ. hIPPYlib solves PDE-constrained inverse problems using automatically-generated adjoint-based derivatives, but it lacks full Bayesian capabilities. MUQ provides a spectrum of powerful Bayesian inversion models and algorithms, but expects forward models to come equipped with gradients and Hessians to permit large-scale solution. By combining these two complementary libraries, we created a robust, scalable, and efficient software framework that realizes the benefits of each and allows us to tackle complex large-scale Bayesian inverse problems across a broad spectrum of scientific and engineering disciplines. To illustrate the capabilities of hIPPYlib-MUQ, we present a comparison of a number of MCMC methods available in the integrated software on several high-dimensional Bayesian inverse problems. These include problems characterized by both linear and nonlinear PDEs, low and high levels of data noise, and different parameter dimensions. The results demonstrate that large (\u223c 50\u00d7) speedups over conventional black box and gradient-based MCMC algorithms can be obtained by exploiting Hessian information (from the log-posterior), underscoring the power of the integrated hIPPYlib-MUQ framework.", "venue": "ArXiv", "authors": ["Ki-Tae  Kim", "Umberto  Villa", "Matthew  Parno", "Youssef  Marzouk", "Omar  Ghattas", "Noemi  Petra"], "year": 2021, "n_citations": 0}
{"id": 3648751, "s2_id": "1a62fa3b81471924d55a281049e401cd7eb3d4c7", "title": "JuMP: A Modeling Language for Mathematical Optimization", "abstract": "JuMP is an open-source modeling language that allows users to express a wide range of optimization problems (linear, mixed-integer, quadratic, conic-quadratic, semidefinite, and nonlinear) in a high-level, algebraic syntax. JuMP takes advantage of advanced features of the Julia programming language to offer unique functionality while achieving performance on par with commercial modeling tools for standard tasks. In this work we will provide benchmarks, present the novel aspects of the implementation, and discuss how JuMP can be extended to new problem classes and composed with state-of-the-art tools for visualization and interactivity.", "venue": "SIAM Rev.", "authors": ["Iain  Dunning", "Joey  Huchette", "Miles  Lubin"], "year": 2017, "n_citations": 808}
{"id": 3657190, "s2_id": "9508121968bcc6c21638eeda489dd76f75e44aec", "title": "From physics model to results: An optimizing framework for cross-architecture code generation", "abstract": "Starting from a high-level problem description in terms of partial differential equations using abstract tensor notation, the Chemora framework discretizes, optimizes, and generates complete high performance codes for a wide range of compute architectures. Chemora extends the capabilities of Cactus, facilitating the usage of large-scale CPU/GPU systems in an efficient manner for complex applications, without low-level code tuning. Chemora achieves parallelism through MPI and multi-threading, combining OpenMP, and CUDA. Optimizations include high-level code transformations, efficient loop traversal strategies, dynamically selected data and instruction cache usage strategies, and JIT compilation of GPU code tailored to the problem characteristics. The discretization is based on higher-order finite differences on multi-block domains. Chemora's capabilities are demonstrated by simulations of black hole collisions. This problem provides an acid test of the framework, as the Einstein equations contain hundreds of variables and thousands of terms.", "venue": "Sci. Program.", "authors": ["Marek  Blazewicz", "Ian  Hinder", "David M. Koppelman", "Steven R. Brandt", "Milosz  Ciznicki", "Michal  Kierzynka", "Frank  L\u00f6ffler", "Erik  Schnetter", "Jian  Tao"], "year": 2013, "n_citations": 11}
{"id": 3658197, "s2_id": "aa2992270ac22fe3692b62858241b86c4b53f517", "title": "Python for education: permutations", "abstract": "Python implementation of permutations is presented. Three classes are introduced: Perm for permutations, Group for permutation groups, and PermError to report any errors for both classes. The class Perm is based on Python dictionaries and utilize cycle notation. The methods of calculation for the perm order, parity, ranking and unranking are given. A random permutation generation is also shown. The class Group is very simple and it is also based on dictionaries. It is mainly the presentation of the permutation groups interface with methods for the group order, subgroups (normalizer, centralizer, center, stabilizer), orbits, and several tests. The corresponding Python code is contained in the modules perms and groups.", "venue": "ArXiv", "authors": ["Andrzej  Kapanowski"], "year": 2013, "n_citations": 1}
{"id": 3658581, "s2_id": "ba2c9de891a29b44f525732090e849d7cbfde0c5", "title": "c-lasso - a Python package for constrained sparse and robust regression and classification", "abstract": "We introduce c-lasso, a Python package that enables sparse and robust linear regression and classification with linear equality constraints. The underlying statistical forward model is assumed to be of the following form: \\[ y = X \\beta + \\sigma \\epsilon \\qquad \\textrm{subject to} \\qquad C\\beta=0 \\] Here, $X \\in \\mathbb{R}^{n\\times d}$is a given design matrix and the vector $y \\in \\mathbb{R}^{n}$ is a continuous or binary response vector. The matrix $C$ is a general constraint matrix. The vector $\\beta \\in \\mathbb{R}^{d}$ contains the unknown coefficients and $\\sigma$ an unknown scale. Prominent use cases are (sparse) log-contrast regression with compositional data $X$, requiring the constraint $1_d^T \\beta = 0$ (Aitchion and Bacon-Shone 1984) and the Generalized Lasso which is a special case of the described problem (see, e.g, (James, Paulson, and Rusmevichientong 2020), Example 3). The c-lasso package provides estimators for inferring unknown coefficients and scale (i.e., perspective M-estimators (Combettes and Muller 2020a)) of the form \\[ \\min_{\\beta \\in \\mathbb{R}^d, \\sigma \\in \\mathbb{R}_{0}} f\\left(X\\beta - y,{\\sigma} \\right) + \\lambda \\left\\lVert \\beta\\right\\rVert_1 \\qquad \\textrm{subject to} \\qquad C\\beta = 0 \\] for several convex loss functions $f(\\cdot,\\cdot)$. This includes the constrained Lasso, the constrained scaled Lasso, and sparse Huber M-estimators with linear equality constraints.", "venue": "J. Open Source Softw.", "authors": ["L\u00e9o  Simpson", "Patrick L. Combettes", "Christian L. M\u00fcller"], "year": 2021, "n_citations": 5}
{"id": 3664810, "s2_id": "48029fde3b214efdf58742f4972b3b66de399b28", "title": "A Qualitative Comparison of the Suitability of Four Theorem Provers for Basic Auction Theory", "abstract": "Novel auction schemes are constantly being designed. Their design has significant consequences for the allocation of goods and the revenues generated. But how to tell whether a new design has the desired properties, such as efficiency, i.e. allocating goods to those bidders who value them most? We say: by formal, machine-checked proofs. We investigated the suitability of the Isabelle, Theorema, Mizar, and Hets/CASL/ TPTP theorem provers for reproducing a key result of auction theory: Vickrey's 1961 theorem on the properties of second-price auctions. Based on our formalisation experience, taking an auction designer's perspective, we give recommendations on what system to use for formalising auctions, and outline further steps towards a complete auction theory toolbox.", "venue": "MKM/Calculemus/DML", "authors": ["Christoph  Lange", "Marco B. Caminati", "Manfred  Kerber", "Till  Mossakowski", "Colin  Rowat", "Markus  Wenzel", "Wolfgang  Windsteiger"], "year": 2013, "n_citations": 20}
{"id": 3669401, "s2_id": "728ef71f8a52d922bbfe518e625af1fce475b92f", "title": "Tuning the performance of a computational persistent homology package", "abstract": "In recent years, persistent homology has become an attractive method for data analysis. It captures topological features, such as connected components, holes, and voids, from point cloud data and summarizes the way in which these features appear and disappear in a filtration sequence. In this project, we focus on improving the performance of Eirene, a computational package for persistent homology. Eirene is a 5000\u2010line open\u2010source software library implemented in the dynamic programming language Julia. We use the Julia profiling tools to identify performance bottlenecks and develop novel methods to manage them, including the parallelization of some time\u2010consuming functions on multicore/manycore hardware. Empirical results show that performance can be greatly improved.", "venue": "Softw. Pract. Exp.", "authors": ["Alan  Hylton", "Greg  Henselman-Petrusek", "Janche  Sang", "Robert  Short"], "year": 2019, "n_citations": 4}
{"id": 3675584, "s2_id": "d91c612680fe267492efc352116207c2ba362caf", "title": "A benchmark of selected algorithmic differentiation tools on some problems in computer vision and machine learning", "abstract": "Algorithmic differentiation (AD) allows exact computation of derivatives given only an implementation of an objective function. Although many AD tools are available, a proper and efficient implementation of AD methods is not straightforward. The existing tools are often too different to allow for a general test suite. In this paper, we compare 15 ways of computing derivatives including 11 automatic differentiation tools implementing various methods and written in various languages (C++, F#, MATLAB, Julia and Python), 2 symbolic differentiation tools, finite differences and hand-derived computation. We look at three objective functions from computer vision and machine learning. These objectives are for the most part simple, in the sense that no iterative loops are involved, and conditional statements are encapsulated in functions such as abs or logsumexp. However, it is important for the success of AD that such \u2018simple\u2019 objective functions are handled efficiently, as so many problems in computer vision and machine learning are of this form.", "venue": "Optim. Methods Softw.", "authors": ["Filip  Srajer", "Zuzana  Kukelova", "Andrew W. Fitzgibbon"], "year": 2018, "n_citations": 13}
{"id": 3676117, "s2_id": "84a7bfcb58c18bbee2e55e2f4f2baf5f09a1c174", "title": "Small superposition dimension and active set construction for multivariate integration under modest error demand", "abstract": "Abstract Constructing active sets is a key part of the Multivariate Decomposition Method. An algorithm for constructing optimal or quasi-optimal active sets is proposed in the paper. By numerical experiments, it is shown that the new method can provide sets that are significantly smaller than the sets constructed by the already existing method. The experiments also show that the superposition dimension could surprisingly be very small, at most 3, when the error demand is not smaller than 1 0 \u2212 3 and the weights decay sufficiently fast.", "venue": "J. Complex.", "authors": ["Alexander D. Gilbert", "Greg W. Wasilkowski"], "year": 2017, "n_citations": 10}
{"id": 3677874, "s2_id": "47614b13fca7b77a784553a159ea559a17c3f973", "title": "ProofPeer - A Cloud-based Interactive Theorem Proving System", "abstract": "ProofPeer strives to be a system for cloud-based interactive theorem proving. After illustrating why such a system is needed, the paper presents some of the design challenges that ProofPeer needs to meet to succeed. Contexts are presented as a solution to the problem of sharing proof state among the users of ProofPeer. Chronicles are introduced as a way to organize and version contexts.", "venue": "ArXiv", "authors": ["Steven  Obua"], "year": 2012, "n_citations": 2}
{"id": 3680089, "s2_id": "9969e848ace0ea9daf0973c8eeaeba21f34e2104", "title": "Transpose-free Fast Fourier Transform for Turbulence Simulation", "abstract": "Pseudo-spectral method is one of the most accurate techniques for simulating turbulent flows. Fast Fourier transform (FFT) is an integral part of this method. In this paper, we present a new procedure to compute FFT in which we save operations during interprocess communications by avoiding transpose of the array. As a result, our transpose-free FFT is 15\\% to 20\\% faster than FFTW.", "venue": "ArXiv", "authors": ["A. G. Chatterjee", "M. K. Verma", "M.  Chaudhuri"], "year": 2014, "n_citations": 0}
{"id": 3682245, "s2_id": "e314038a7f88e16d9b37895d4571c09682739e06", "title": "Programming the Adapteva Epiphany 64-Core Network-on-Chip Coprocessor", "abstract": "With energy efficiency and power consumption being the primary impediment in the path to exascale systems, low-power high performance embedded systems are of increasing interest. The Parallella System-on-module (SoM) created by Adapteva combines the Epiphany-IV 64-core coprocessor with a host ARM processor housed in a Zynq System-on-chip. The Epiphany integrates low-power RISC cores on a 2D mesh network and promises up to 70 GFLOPS/Watt of processing efficiency. However, with just 32 KB of memory per eCore for storing both data and code, and only low level inter-core communication support, programming the Epiphany system presents several challenges. In this paper we evaluate the performance of the Epiphany system for a variety of basic compute and communication operations. Guided by this data we explore various strategies for implementing stencil based application codes on the Epiphany system. With future systems expected to house 4096 eCores, the merits of the Epiphany architecture as a path to exascale is compared to other competing power efficient systems.", "venue": "IPDPS Workshops", "authors": ["Anish  Varghese", "Bob  Edwards", "Gaurav  Mitra", "Alistair P. Rendell"], "year": 2014, "n_citations": 25}
{"id": 3682997, "s2_id": "dda76f94cfe3b07511f39a4ae43425c73120c4c8", "title": "pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models", "abstract": "Bayesian Knowledge Tracing, a model used for cognitive mastery estimation, has been a hallmark of adaptive learning research and an integral component of deployed intelligent tutoring systems (ITS). In this paper, we provide a brief history of knowledge tracing model research and introduce pyBKT, an accessible and computationally efficient library of model extensions from the literature. The library provides data generation, fitting, prediction, and cross-validation routines, as well as a simple to use data helper interface to ingest typical tutor log dataset formats. We evaluate the runtime with various dataset sizes and compare to past implementations. Additionally, we conduct sanity checks of the model using experiments with simulated data to evaluate the accuracy of its EM parameter learning and use real-world data to validate its predictions, comparing pyBKT\u2019s supported model variants with results from the papers in which they were originally introduced. The library is open source and open license for the purpose of making knowledge tracing more accessible to communities of research and practice and to facilitate progress in the field through easier replication of past approaches.", "venue": "ArXiv", "authors": ["Anirudhan  Badrinath", "Frederic  Wang", "Zachary  Pardos"], "year": 2021, "n_citations": 0}
{"id": 3683392, "s2_id": "20b77a75501677905018bf4884423231b87155d4", "title": "Achieving near native runtime performance and cross-platform performance portability for random number generation through SYCL interoperability", "abstract": "High-performance computing (HPC) is a major driver accelerating scientific research and discovery, from quantum simulations to medical therapeutics. While the increasing availability of HPC resources is in many cases pivotal to successful science, even the largest collaborations lack the computational expertise required for maximal exploitation of current hardware capabilities. The need to maintain multiple platform-specific codebases further complicates matters, potentially adding constraints on machines that can be utilized. Fortunately, numerous programming models are under development that aim to facilitate portable codes for heterogeneous computing. One in particular is SYCL, an open standard, C++-based single-source programming paradigm. Among SYCL\u2019s features is interoperability, a mechanism through which applications and third-party libraries coordinate sharing data and execute collaboratively. In this paper, we leverage the SYCL programming model to demonstrate cross-platform performance portability across heterogeneous resources. We detail our NVIDIA and AMD random number generator extensions to the oneMKL open-source interfaces library. Performance portability is measured relative to platform-specific baseline applications executed on four major hardware platforms using two different compilers supporting SYCL. The utility of our extensions are exemplified in a real-world setting via a high-energy physics simulation application. We show the performance of implementations that capitalize on SYCL interoperability are at par with native implementations, attesting to the cross-platform performance portability of a SYCL-based approach to scientific codes.", "venue": "ArXiv", "authors": ["Vincent R. Pascuzzi", "Mehdi  Goli"], "year": 2021, "n_citations": 0}
{"id": 3688996, "s2_id": "c1cc53ec5584b7e1d6469923185e3cbe02746075", "title": "The MathScheme Library: Some Preliminary Experiments", "abstract": "We present some of the e\u7870eriments we have performed to best test our design for a library for MathScheme, the mechanized mathe- matics software s\u7973tem we are building. We wish for our librar y design to use and reect, as much as possible, the mathematical structure present in the objects which populate the library.", "venue": "ArXiv", "authors": ["Jacques  Carette", "William M. Farmer", "Filip  Jeremic", "Vincent  Maccio", "Russell  O'Connor", "Quang M. Tran"], "year": 2011, "n_citations": 9}
{"id": 3690305, "s2_id": "2eeab9cb8ddae295f183ce09f20376b9f23dcac8", "title": "SeqROCTM: A Matlab toolbox for the analysis of Sequence of Random Objects driven by Context Tree Models", "abstract": "In several research problems we face probabilistic sequences of inputs (e.g., sequence of stimuli) from which an agent generates a corresponding sequence of responses and it is of interest to model/discover some kind of relation between them. To model such relation in the context of statistical learning in neuroscience, a new class of stochastic process have been introduced [5], namely sequences of random objects driven by context tree models. In this paper we introduce a freely available Matlab toolbox (SeqROCTM) that implements three model selection methods to make inference about the parameters of this kind of stochastic process.", "venue": "ArXiv", "authors": ["Noslen  Hern\u00e1ndez", "Aline  Duarte"], "year": 2020, "n_citations": 0}
{"id": 3692525, "s2_id": "9c9709e8c4a03aecb5a02de8651b85da18351e01", "title": "The interface for functions in the dune-functions module", "abstract": "The dune-functions dune module introduces a new programmer interface for discrete and non-discrete functions. Unlike the previous interfaces considered in the existing dune modules, it is based on overloading operator(), and returning values by-value. This makes user code much more readable, and allows the incorporation of newer C++ features such as lambda expressions. Run-time polymorphism is implemented not by inheritance, but by type erasure, generalizing the ideas of the std::function class from the C++11 standard library. We describe the new interface, show its possibilities, and measure the performance impact of type erasure and return-by-value.", "venue": "ArXiv", "authors": ["Christian  Engwer", "Carsten  Gr\u00e4ser", "Steffen  M\u00fcthing", "Oliver  Sander"], "year": 2015, "n_citations": 12}
{"id": 3694274, "s2_id": "c78bb88b35af2be1ec583a67cfc85a0398554ce9", "title": "An Integer Arithmetic-Based Sparse Linear Solver Using a GMRES Method and Iterative Refinement", "abstract": "In this paper, we develop a (preconditioned) GM-RES solver based on integer arithmetic, and introduce an iterative refinement framework for the solver. We describe the data format for the coefficient matrix and vectors for the solver that is based on integer or fixed-point numbers. To avoid overflow in calculations, we introduce initial scaling and logical shifts (adjustments) of operands in arithmetic operations. We present the approach for operand shifts, considering the characteristics of the GMRES algorithm. Numerical tests demonstrate that the integer arithmetic-based solver with iterative refinement has comparable solver performance in terms of convergence to the standard solver based on floating-point arithmetic. Moreover, we show that preconditioning is important, not only for improving convergence but also reducing the risk of overflow.", "venue": "2020 IEEE/ACM 11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA)", "authors": ["Takeshi  Iwashita", "Kengo  Suzuki", "Takeshi  Fukaya"], "year": 2020, "n_citations": 1}
{"id": 3695804, "s2_id": "46917dd2c72c9e73abe18baf9b0c95e904f2e930", "title": "Enhancing R with Advanced Compilation Tools and Methods", "abstract": "I describe an approach to compiling common idioms in R code directly to native machine code and illustrate it with several examples. Not only can this yield significant performance gains, but it allows us to use new approaches to computing in R. Importantly, the compilation requires no changes to R itself, but is done entirely via R packages. This allows others to experiment with different compilation strategies and even to define new domain-specific languages within R. We use the Low-Level Virtual Machine (LLVM) compiler toolkit to create the native code and perform sophisticated optimizations on the code. By adopting this widely used software within R, we leverage its ability to generate code for different platforms such as CPUs and GPUs, and will continue to benefit from its ongoing development. This approach potentially allows us to develop high-level R code that is also fast, that can be compiled to work with different data representations and sources, and that could even be run outside of R. The approach aims to both provide a compiler for a limited subset of the R language and also to enable R programmers to write other compilers. This is another approach to help us write high-level descriptions of what we want to compute, not how.", "venue": "ArXiv", "authors": ["Duncan Temple Lang"], "year": 2014, "n_citations": 3}
{"id": 3704324, "s2_id": "ebc641c3637de6b86fe296b67b6e6cffb18dcd2e", "title": "Programming parallel dense matrix factorizations with look-ahead and OpenMP", "abstract": "We investigate a parallelization strategy for dense matrix factorization (DMF) algorithms, using OpenMP, that departs from the legacy (or conventional) solution, which simply extracts concurrency from a multi-threaded version of basic linear algebra subroutines (BLAS). The proposed approach is also different from the more sophisticated runtime-based implementations, which decompose the operation into tasks and identify dependencies via directives and runtime support. Instead, our strategy attains high performance by explicitly embedding a static look-ahead technique into the DMF code, in order to overcome the performance bottleneck of the panel factorization, and realizing the trailing update via a cache-aware multi-threaded implementation of the BLAS. Although the parallel algorithms are specified with a high level of abstraction, the actual implementation can be easily derived from them, paving the road to deriving a high performance implementation of a considerable fraction of linear algebra package (LAPACK) functionality on any multicore platform with an OpenMP-like runtime.", "venue": "Cluster Computing", "authors": ["Sandra  Catal\u00e1n", "Adri\u00e1n  Castell\u00f3", "Francisco D. Igual", "Rafael  Rodr\u00edguez-S\u00e1nchez", "Enrique S. Quintana-Ort\u00ed"], "year": 2019, "n_citations": 3}
{"id": 3704454, "s2_id": "8eec51209759807b612c9f04c5dbfd1564b17437", "title": "Clone and graft: Testing scientific applications as they are built", "abstract": "This article describes our experience developing and maintaining automated tests for scientific applications. The main idea evolves around building on already existing tests by cloning and grafting. The idea is demonstrated on a minimal model problem written in Python.", "venue": "ArXiv", "authors": ["Bruno  Turcksin", "Timo  Heister", "Wolfgang  Bangerth"], "year": 2015, "n_citations": 0}
{"id": 3717824, "s2_id": "942d3e921b1d691a59a51d0c1cb0b51c853cf0c5", "title": "Demonstrating the Usefulness of CAELinux for Computer Aided Engineering using an Example of the Three Dimensional Reconstruction of a Pig Liver", "abstract": "CAELinux is a Linux distribution which is bundled with free software packages related to Computer Aided Engineering (CAE). The free software packages include software that can build a three dimensional solid model, programs that can mesh a geometry, software for carrying out Finite Element Analysis (FEA), programs that can carry out image processing etc. Present work has two goals: 1) To give a brief description of CAELinux 2) To demonstrate that CAELinux could be useful for Computer Aided Engineering, using an example of the three dimensional reconstruction of a pig liver from a stack of CT-scan images. One can note that instead of using CAELinux, using commercial software for reconstructing the liver would cost a lot of money. One can also note that CAELinux is a free and open source operating system and all software packages that are included in the operating system are also free. Hence one can conclude that CAELinux could be a very useful tool in application areas like surgical simulation which require three dimensional reconstructions of biological organs. Also, one can see that CAELinux could be a very useful tool for Computer Aided Engineering, in general.", "venue": "ArXiv", "authors": ["P.  KiranaKumara"], "year": 2012, "n_citations": 2}
{"id": 3720292, "s2_id": "e1cb4d4a1a83da1d437a159f81d3b870fa1323bd", "title": "Local congruence of chain complexes", "abstract": "The object of this paper is to transform a set of local chain complexes to a single global complex using an equivalence relation of congruence of cells, solving topologically the numerical inaccuracies of floating-point arithmetics. While computing the space arrangement generated by a collection of cellular complexes, one may start from independently and efficiently computing the intersection of each single input 2-cell with the others. The topology of these intersections is codified within a set of (0-2)-dimensional chain complexes. The target of this paper is to merge the local chains by using the equivalence relations of {\\epsilon}-congruence between 0-, 1-, and 2-cells (elementary chains). In particular, we reduce the block-diagonal coboundary matrices [\\Delta_0] and [\\Delta_1], used as matrix accumulators of the local coboundary chains, to the global matrices [\\delta_0] and [\\delta_1], representative of congruence topology, i.e., of congruence quotients between all 0-,1-,2-cells, via elementary algebraic operations on their columns. This algorithm is codified using the Julia porting of the SuiteSparse:GraphBLAS implementation of the GraphBLAS standard, conceived to efficiently compute algorithms on large graphs using linear algebra and sparse matrices [1, 2].", "venue": "ArXiv", "authors": ["Gianmaria  DelMonte", "Elia  Onofri", "Giorgio  Scorzelli", "Alberto  Paoluzzi"], "year": 2020, "n_citations": 1}
{"id": 3720664, "s2_id": "9088f7c118c33604052e966a89e41db983ec6a92", "title": "Why the Standard Data Processing should be changed", "abstract": "The basic statistical methods of data representation did not change since their emergence. Their simplicity was dictated by the intricacies of computations in the before computers epoch. It turns out that such approach is not uniquely possible in the presence of quick computers. The suggested here method improves significantly the reliability of data processing and their graphical representation. In this paper we show problems of the standard data processing which can bring to incorrect results. A method solving these problems is proposed. It is based on modification of data representation. The method was implemented in a computer program Consensus5. The program performances are illustrated through varied examples.", "venue": "ArXiv", "authors": ["Yefim  Bakman"], "year": 2007, "n_citations": 1}
{"id": 3722485, "s2_id": "a4b9fb2480c3dabe0efa35d061dbca33b057c07f", "title": "rcss: Subgradient and duality approach for dynamic programming", "abstract": "This short paper gives an introduction to the \\emph{rcss} package. The R package \\emph{rcss} provides users with a tool to approximate the value functions in the Bellman recursion using convex piecewise linear functions formed using operations on tangents. A pathwise method is then used to gauge the quality of the numerical results.", "venue": "ArXiv", "authors": ["Juri  Hinz", "Jeremy  Yee"], "year": 2018, "n_citations": 0}
{"id": 3722866, "s2_id": "cf35d6ffe7601b893cca0577b7f1c0190eddc5db", "title": "Programs in C++ for matrix computations in min plus algebra", "abstract": "The main purpose of this paper is to propose six programs in C++ for matrix computations and solving recurrent equations systems with entries in min plus algebra.", "venue": "ArXiv", "authors": ["Mihai  Ivan", "Gheorghe  Ivan"], "year": 2013, "n_citations": 0}
{"id": 3727513, "s2_id": "0bbeb4ae0fe863045caf52d4438f39994f07ad38", "title": "Zolotarev Quadrature Rules and Load Balancing for the FEAST Eigensolver", "abstract": "The FEAST method for solving large sparse eigenproblems is equivalent to subspace iteration with an approximate spectral projector and implicit orthogonalization. This relation allows to characterize the convergence of this method in terms of the error of a certain rational approximant to an indicator function. We propose improved rational approximants leading to FEAST variants with faster convergence, in particular, when using rational approximants based on the work of Zolotarev. Numerical experiments demonstrate the possible computational savings especially for pencils whose eigenvalues are not well separated and when the dimension of the search space is only slightly larger than the number of wanted eigenvalues. The new approach improves both convergence robustness and load balancing when FEAST runs on multiple search intervals in parallel.", "venue": "SIAM J. Sci. Comput.", "authors": ["Stefan  G\u00fcttel", "Eric  Polizzi", "Ping Tak Peter Tang", "Gautier  Viaud"], "year": 2015, "n_citations": 79}
{"id": 3729409, "s2_id": "79d75ac189a36c2b0a41aeda7faf9053fdc6ef97", "title": "High Precision Arithmetic for Scientific Applications", "abstract": "All but a few digital computers used for scientific computations have supported floating-point and digital arithmetic of rather limited numerical precision. The underlying assumptions were that the systems being studied were basically deterministic and of limited complexity. The ideal scientific paradigm was the orbits of the major planets, which could be observed with high precision, predicted for thousands of years into the future, and extrapolated for thousands of years into the past. Much the same technology that has made computers possible has also provided instrumentation that has vastly expanded the scope and precision of scientific analysis. Complex nonlinear systems exhibiting so-called chaotic dynamics are now fair game for scientists and engineers in every discipline. Today it seems that computers need to enhance the precision of their numerical computations to support the needs of science. However, there is no need to wait for the necessary updates in both hardware and software; it is easy enough to monitor numerical precision with a few minor modifications to existing software.", "venue": "ArXiv", "authors": ["Foster  Morrison"], "year": 2013, "n_citations": 0}
{"id": 3738568, "s2_id": "5e0e6bcb464fa090e299fa513f92fac5e7461148", "title": "The OpenCPU System: Towards a Universal Interface for Scientific Computing through Separation of Concerns", "abstract": "Applications integrating analysis components require a programmable interface which defines statistical operations independently of any programming language. By separating concerns of scientific computing from application and implementation details we can derive an interoperable API for data analysis. But what exactly are the concerns of scientific computing? To answer this question, the paper starts with an exploration of the purpose, problems, characteristics, struggles, culture, and community of this unique branch of computing. By mapping out the domain logic, we try to unveil the fundamental principles and concepts behind statistical software. Along the way we highlight important problems and bottlenecks that need to be addressed by the system in order to facilitate reliable and scalable analysis units. Finally, the OpenCPU software is introduced as an example implementation that builds on HTTP and R to expose a simple, abstracted interface for scientific computing.", "venue": "ArXiv", "authors": ["Jeroen  Ooms"], "year": 2014, "n_citations": 54}
{"id": 3747450, "s2_id": "6c5afbd32d4eb34a243f65b2cafced86de504360", "title": "Integrating DGSs and GATPs in an Adaptative and Collaborative Blended-Learning Web-Environment", "abstract": "The area of geometry with its very strong and appealing visual contents and its also strong and appealing connection between the visual content and its formal specification, is an area where computational tools can enhance, in a significant way, the learning environments. \nThe dynamic geometry software systems (DGSs) can be used to explore the visual contents of geometry. This already mature tools allows an easy construction of geometric figures build from free objects and elementary constructions. The geometric automated theorem provers (GATPs) allows formal deductive reasoning about geometric constructions, extending the reasoning via concrete instances in a given model to formal deductive reasoning in a geometric theory. \nAn adaptative and collaborative blended-learning environment where the DGS and GATP features could be fully explored would be, in our opinion a very rich and challenging learning environment for teachers and students. \nIn this text we will describe the Web Geometry Laboratory a Web environment incorporating a DGS and a repository of geometric problems, that can be used in a synchronous and asynchronous fashion and with some adaptative and collaborative features. \nAs future work we want to enhance the adaptative and collaborative aspects of the environment and also to incorporate a GATP, constructing a dynamic and individualised learning environment for geometry.", "venue": "ThEdu", "authors": ["Vanda  Santos", "Pedro  Quaresma"], "year": 2011, "n_citations": 8}
{"id": 3749062, "s2_id": "5b0806ed6f0b116b8887977be57adb9da3344e81", "title": "Acceleration of Univariate Global Optimization Algorithms Working with Lipschitz Functions and Lipschitz First Derivatives", "abstract": "This paper deals with two kinds of the one-dimensional global optimization problems over a closed finite interval: (i) the objective function $f(x)$ satisfies the Lipschitz condition with a constant $L$; (ii) the first derivative of $f(x)$ satisfies the Lipschitz condition with a constant $M$. In the paper, six algorithms are presented for the case (i) and six algorithms for the case (ii). In both cases, auxiliary functions are constructed and adaptively improved during the search. In the case (i), piece-wise linear functions are constructed and in the case (ii) smooth piece-wise quadratic functions are used. The constants $L$ and $M$ either are taken as values known a priori or are dynamically estimated during the search. A recent technique that adaptively estimates the local Lipschitz constants over different zones of the search region is used to accelerate the search. A new technique called the \\emph{local improvement} is introduced in order to accelerate the search in both cases (i) and (ii). The algorithms are described in a unique framework, their properties are studied from a general viewpoint, and convergence conditions of the proposed algorithms are given. Numerical experiments executed on 120 test problems taken from the literature show quite a promising performance of the new accelerating techniques.", "venue": "SIAM J. Optim.", "authors": ["Daniela  Lera", "Yaroslav D. Sergeyev"], "year": 2013, "n_citations": 72}
{"id": 3762084, "s2_id": "02e0b47f67e20656974b1c70dcf1c0efb2053fdf", "title": "PyIT2FLS: A New Python Toolkit for Interval Type 2 Fuzzy Logic Systems", "abstract": "Fuzzy logic is an accepted and well-developed approach for constructing verbal models. Fuzzy based methods are getting more popular, while the engineers deal with more daily life tasks. This paper presents a new Python toolkit for Interval Type 2 Fuzzy Logic Systems (IT2FLS). Developing software tools is an important issue for facilitating the practical use of theoretical results. There are limited tools for implementing IT2FLSs in Python. The developed PyIT2FLS is providing a set of tools for fast and easy modeling of fuzzy systems. This paper includes a brief description of how developed toolkit can be used. Also, three examples are given showing the usage of the developed toolkit for simulating IT2FLSs. First, a simple rule-based system is developed and it's codes are presented in the paper. The second example is the prediction of the Mackey-Glass chaotic time series using IT2FLS. In this example, the Particle Swarm Optimization (PSO) algorithm is used for determining system parameters while minimizing the mean square error. In the last example, an IT2FPID is designed and used for controlling a linear time-delay system. The code for the examples are available on toolkit's GitHub page: \\url{this https URL}. The simulations and their results confirm the ability of the developed toolkit to be used in a wide range of the applications.", "venue": "ArXiv", "authors": ["Amir Arslan Haghrah", "Sehraneh  Ghaemi"], "year": 2019, "n_citations": 1}
{"id": 3764842, "s2_id": "2284b0934be36acdfc80f9d296688d5d531e0098", "title": "The Probabilistic Model Checker Storm (Extended Abstract)", "abstract": "We present a new probabilistic model checker Storm. Using state-of-the-art libraries, we aim for both high performance and versatility. This extended abstract gives a brief overview of the features of Storm.", "venue": "ArXiv", "authors": ["Christian  Dehnert", "Sebastian  Junges", "Joost-Pieter  Katoen", "Matthias  Volk"], "year": 2016, "n_citations": 6}
{"id": 3768184, "s2_id": "3a00f378039d83ff3be3c69949cec620352e09d8", "title": "Two-Stage Gauss-Seidel Preconditioners and Smoothers for Krylov Solvers on a GPU cluster", "abstract": "Gauss-Seidel (GS) relaxation is often employed as a preconditioner for a Krylov solver or as a smoother for Algebraic Multigrid (AMG). However, the requisite sparse triangular solve is difficult to parallelize on many-core architectures such as graphics processing units (GPUs). In the present study, the performance of the sequential GS relaxation based on a triangular solve is compared with two-stage variants, replacing the direct triangular solve with a fixed number of inner Jacobi-Richardson (JR) iterations. When a small number of inner iterations is sufficient to maintain the Krylov convergence rate, the two-stage GS (GS2) often outperforms the sequential algorithm on many-core architectures. The GS2 algorithm is also compared with JR. When they perform the same number of flops for SpMV (e.g. three JR sweeps compared to two GS sweeps with one inner JR sweep), the GS2 iterations, and the Krylov solver preconditioned with GS2, may converge faster than the JR iterations. Moreover, for some problems (e.g. elasticity), it was found that JR may diverge with a damping factor of one, whereas two-stage GS may improve the convergence with more inner iterations. Finally, to study the performance of the two-stage smoother and preconditioner for a practical problem, these were applied to incompressible fluid flow simulations on GPUs.", "venue": "ArXiv", "authors": ["Luc  Berger-Vergiat", "Brian  Kelley", "Sivasankaran  Rajamanickam", "Jonathan  Hu", "Katarzyna  Swirydowicz", "Paul  Mullowney", "Stephen  Thomas", "Ichitaro  Yamazaki"], "year": 2021, "n_citations": 1}
{"id": 3774546, "s2_id": "06f6f236b12d56108e3f2043b2636acfe8b8a413", "title": "A new framework for the computation of Hessians", "abstract": "We investigate the computation of Hessian matrices via Automatic Differentiation, using a graph model and an algebraic model. The graph model reveals the inherent symmetries involved in calculating the Hessian. The algebraic model, based on Griewank and Walther's [Evaluating derivatives, in Principles and Techniques of Algorithmic Differentiation, 2nd ed., Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2008] state transformations synthesizes the calculation of the Hessian as a formula. These dual points of view, graphical and algebraic, lead to a new framework for Hessian computation. This is illustrated by developing edge_pushing, a new truly reverse Hessian computation algorithm that fully exploits the Hessian's symmetry. Computational experiments compare the performance of edge_pushing on 16 functions from the CUTE collection [I. Bongartz et al. Cute: constrained and unconstrained testing environment, ACM Trans. Math. Softw. 21(1) (1995), pp. 123\u2013160] against two algorithms available as drivers of the software ADOL-C [A. Griewank et al. ADOL-C: A package for the automatic differentiation of algorithms written in C/C++, Technical report, Institute of Scientific Computing, Technical University Dresden, 1999. Updated version of the paper published in ACM Trans. Math. Softw. 22, 1996, pp. 131\u2013167; A. Walther, Computing sparse Hessians with automatic differentiation, ACM Trans. Math. Softw. 34(1) (2008), pp. 1\u201315; A.H. Gebremedhin et al. Efficient computation of sparse Hessians using coloring and automatic differentiation, INFORMS J. Comput. 21(2) (2009), pp. 209\u2013223], and the results are very promising.", "venue": "Optim. Methods Softw.", "authors": ["Robert M. Gower", "M. P. Mello"], "year": 2012, "n_citations": 18}
{"id": 3775115, "s2_id": "d70777bdb0eb653bd2ac47241d7e3be2596c420d", "title": "An algebraic programming style for numerical software and its optimization", "abstract": "textabstract The abstract mathematical theory of partial differential equations (PDEs) is formulated in terms of manifolds, scalar fields, tensors, and the like, but these algebraic structures are hardly recognizable in actual PDE solvers. The general aim of the Sophus programming style is to bridge the gap between theory and practice in the domain of PDE solvers. Its main ingredients are a library of abstract datatypes corresponding to the algebraic structures used in the mathematical theory and an algebraic expression style similar to the expression style used in the mathematical theory. Because of its emphasis on abstract datatypes, Sophus is most naturally combined with object-oriented languages or other languages supporting abstract datatypes. The resulting source code patterns are beyond the scope of current compiler optimizations, but are sufficiently specific for a dedicated source-to-source optimizer. The limited, domain-specific, character of Sophus is the key to success here. This kind of optimization has been tested on computationally intensive Sophus style code with promising results. The general approach may be useful for other styles and in other application domains as well.", "venue": "Sci. Program.", "authors": ["T. B. Dinesh", "Magne  Haveraaen", "Jan  Heering"], "year": 2000, "n_citations": 31}
{"id": 3778217, "s2_id": "0bc4943b1e47297897fca44e70e8fa72c5cbde0e", "title": "Matrix Equations, Sparse Solvers: M-M.E.S.S.-2.0.1 - Philosophy, Features and Application for (Parametric) Model", "abstract": "Matrix equations are omnipresent in (numerical) linear algebra and systems theory. Especially in model order reduction (MOR) they play a key role in many balancing based reduction methods for linear dynamical systems. When these systems arise from spatial discretizations of evolutionary partial differential equations, their coefficient matrices are typically large and sparse. Moreover, the numbers of inputs and outputs of these systems are typically far smaller than the number of spatial degrees of freedom. Then, in many situations the solutions of the corresponding large-scale matrix equations are observed to have low (numerical) rank. This feature is exploited by M-M.E.S.S. to find successively larger low-rank factorizations approximating the solutions. This contribution describes the basic philosophy behind the implementation and the features of the package, as well as its application in the model order reduction of large-scale linear time-invariant (LTI) systems and parametric LTI systems.", "venue": "ArXiv", "authors": ["Peter  Benner", "Martin  K\u00f6hler", "Jens  Saak"], "year": 2020, "n_citations": 6}
{"id": 3778428, "s2_id": "c2c384075ac7ca2151d0f79d8f7211e6bbc6147b", "title": "Regular and Singular Boundary Problems in Maple", "abstract": "We describe a new Maple package for treating boundary problems for linear ordinary differential equations, allowing two-/multipoint as well as Stieltjes boundary conditions. For expressing differential operators, boundary conditions, and Green's operators, we employ the algebra of integro-differential operators. The operations implemented for regular boundary problems include computing Green's operators as well as composing and factoring boundary problems. Our symbolic approach to singular boundary problems is new; it provides algorithms for computing compatibility conditions and generalized Green's operators.", "venue": "CASC", "authors": ["Anja  Korporal", "Georg  Regensburger", "Markus  Rosenkranz"], "year": 2011, "n_citations": 13}
{"id": 3778735, "s2_id": "b7d803683a35d3d22f6dff4cc188bafdb67a94a1", "title": "Discontinuous Galerkin methods on graphics processing units for nonlinear hyperbolic conservation laws", "abstract": "We present a novel implementation of the modal discontinuous Galerkin (DG) method for hyperbolic conservation laws in two dimensions on graphics processing units (GPUs) using NVIDIA's Compute Unified Device Architecture (CUDA). Both flexible and highly accurate, DG methods accommodate parallel architectures well as their discontinuous nature produces element-local approximations. High performance scientific computing suits GPUs well, as these powerful, massively parallel, cost-effective devices have recently included support for double-precision floating point numbers. Computed examples for Euler equations over unstructured triangle meshes demonstrate the effectiveness of our implementation on an NVIDIA GTX 580 device. Profiling of our method reveals performance comparable to an existing nodal DG-GPU implementation for linear problems.", "venue": "ArXiv", "authors": ["Martin  Fuhry", "Andrew  Giuliani", "Lilia  Krivodonova"], "year": 2016, "n_citations": 36}
{"id": 3785462, "s2_id": "5709e46f3e84959ad89f7bd5aa1500463e671b62", "title": "Accelerating quantum many-body configuration interaction with directives", "abstract": "Many-Fermion Dynamics\u2014nuclear, or MFDn, is a configuration interaction (CI) code for nuclear structure calculations. It is a platform-independent Fortran 90 code using a hybrid MPI+X programming model. For CPU platforms the application has a robust and optimized OpenMP implementation for shared memory parallelism. As part of the NESAP application readiness program for NERSC\u2019s latest Perlmutter system, MFDn has been updated to take advantage of accelerators. The current mainline GPU port is based on OpenACC. In this work we describe some of the key challenges of creating an efficient GPU implementation. Additionally, we compare the support of OpenMP and OpenACC on AMD and NVIDIA GPUs.", "venue": "ArXiv", "authors": ["Brandon  Cook", "Patrick J. Fasano", "Pieter  Maris", "Chao  Yang", "Dossay  Oryspayev"], "year": 2021, "n_citations": 0}
{"id": 3798779, "s2_id": "23de1b7fc41768e3087ebbbc1767c8aebdb1d637", "title": "Sundials/ML: Connecting OCaml to the Sundials Numeric Solvers", "abstract": "This paper describes the design and implementation of a comprehensive OCaml interface to the Sundials library of numeric solvers for ordinary differential equations, differential algebraic equations, and non-linear equations. The interface provides a convenient and memory-safe alternative to using Sundials directly from C and facilitates application development by integrating with higher-level language features, like garbage-collected memory management, algebraic data types, and exceptions. Our benchmark results suggest that the interface overhead is acceptable: the standard examples are rarely twice as slow in OCaml than in C, and often less than 50% slower. The challenges in interfacing with Sundials are to efficiently and safely share data structures between OCaml and C, to support multiple implementations of vector operations and linear solvers through a common interface, and to manage calls and error signalling to and from OCaml. We explain how we overcame these difficulties using a combination of standard techniques such as phantom types and polymorphic variants, and carefully crafted data representations.", "venue": "ML/OCAML", "authors": ["Timothy  Bourke", "Jun  Inoue", "Marc  Pouzet"], "year": 2016, "n_citations": 0}
{"id": 3803416, "s2_id": "e8c492c7ce38f39d665d575102869cbe82b04de2", "title": "Particle-based and Meshless Methods with Aboria", "abstract": "Aboria is a powerful and flexible C++ library for the implementation of particle-based numerical methods. The particles in such methods can represent actual particles (e.g. Molecular Dynamics) or abstract particles used to discretise a continuous function over a domain (e.g. Radial Basis Functions). Aboria provides a particle container, compatible with the Standard Template Library, spatial search data structures, and a Domain Specific Language to specify non-linear operators on the particle set. This paper gives an overview of Aboria's design, an example of use, and a performance benchmark.", "venue": "SoftwareX", "authors": ["Martin  Robinson", "Maria  Bruna"], "year": 2017, "n_citations": 12}
{"id": 3809207, "s2_id": "fb5b55cd5e07d66b37948ba6a0d7581e908b89e3", "title": "Automatic finite element implementation of hyperelastic material with a double numerical differentiation algorithm", "abstract": "In order to accelerate implementation of hyperelastic materials for finite element analysis, we developed an automatic numerical algorithm that only requires the strain energy function. This saves the effort on analytical derivation and coding of stress and tangent modulus, which is time-consuming and prone to human errors. Using the one-sided Newton difference quotients, the proposed algorithm first perturbs deformation gradients and calculate the difference on strain energy to approximate stress. Then, we perturb again to get difference in stress to approximate tangent modulus. Accuracy of the approximations were evaluated across the perturbation parameter space, where we find the optimal amount of perturbation being $10^{-6}$ to obtain stress and $10^{-4}$ to obtain tangent modulus. Single element verification in ABAQUS with Neo-Hookean material resulted in a small stress error of only $7\\times10^{-5}$ on average across uniaxial compression and tension, biaxial tension and simple shear situations. A full 3D model with Holzapfel anisotropic material for artery inflation generated a small relative error of $4\\times10^{-6}$ for inflated radius at $25 kPa$ pressure. Results of the verification tests suggest that the proposed numerical method has good accuracy and convergence performance, therefore a good material implementation algorithm in small scale models and a useful debugging tool for large scale models.", "venue": "ArXiv", "authors": ["Yuxiang  Wang", "Gregory J. Gerling"], "year": 2016, "n_citations": 1}
{"id": 3810161, "s2_id": "9af5cb114046da92f6915ccc2da146cf6bbae186", "title": "Efficient and scalable data structures and algorithms for goal-oriented adaptivity of space-time FEM codes", "abstract": "The cost- and memory-efficient numerical simulation of coupled volume-based multi-physics problems like flow, transport, wave propagation and others remains a challenging task with finite element method (FEM) approaches. Goal-oriented space and time adaptive methods derived from the dual weighted residual (DWR) method appear to be a shiny key technology to generate optimal space-time meshes to minimise costs. Current implementations for challenging problems of numerical screening tools including the DWR technology broadly suffer in their extensibility to other problems, in high memory consumption or in missing system solver technologies. This work contributes to the efficient embedding of DWR space-time adaptive methods into numerical screening tools for challenging problems of physically relevance with a new approach of flexible data structures and algorithms on them, a modularised and complete implementation as well as illustrative examples to show the performance and efficiency.", "venue": "SoftwareX", "authors": ["Uwe  K\u00f6cher", "Marius Paul Bruchh\u00e4user", "Markus  Bause"], "year": 2019, "n_citations": 10}
{"id": 3811279, "s2_id": "a6aec0f8a244eb9e5a356c0224fd154ab95cde0b", "title": "BoxLib with Tiling: An AMR Software Framework", "abstract": "Author(s): Zhang, Weiqun; Almgren, Ann S; Day, Marcus; Nguyen, Tan; Shalf, John; Unat, Didem", "venue": "ArXiv", "authors": ["Weiqun  Zhang", "Ann S. Almgren", "Marcus  Day", "Tan  Nguyen", "John  Shalf", "Didem  Unat"], "year": 2016, "n_citations": 11}
{"id": 3813222, "s2_id": "114ba8fa050cc75024843a7c053589f34b29b071", "title": "SPSMAT: GNU Octave software package for spectral and pseudospectral methods", "abstract": "SPSMAT (Spectral/Pseudospectral matrix method) is an add-on for Octave, that helps you solve nonfractional-/fractional ordinary/partial differential/integral equations. In this version, as the first version, the well-defined spectral or pseudospectral algorithms are considered to solve differential and integral equations. The motivation is that there are few software packages available that make such methods easy to use for practitioners in the field of scientific computing. Additionally, one of the most practical platforms in computation, MATLAB, is currently not supporting beneficial and free numerical method for the solution of differential equations--to the best author's knowledge. To remedy this situation, this paper provides a description of its relevant uploaded open source software package and is a broad guidance to describe how to work with this toolbox.", "venue": "ArXiv", "authors": ["Sobhan  Latifi", "Mehdi  Delkhosh"], "year": 2019, "n_citations": 2}
{"id": 3814190, "s2_id": "a3093f26220a2bfab9970ef9255864aa6af02875", "title": "Using Java for distributed computing in the Gaia satellite data processing", "abstract": "In recent years Java has matured to a stable easy-to-use language with the flexibility of an interpreter (for reflection etc.) but the performance and type checking of a compiled language. When we started using Java for astronomical applications around 1999 they were the first of their kind in astronomy. Now a great deal of astronomy software is written in Java as are many business applications. We discuss the current environment and trends concerning the language and present an actual example of scientific use of Java for high-performance distributed computing: ESA\u2019s mission Gaia. The Gaia scanning satellite will perform a galactic census of about 1,000 million objects in our galaxy. The Gaia community has chosen to write its processing software in Java. We explore the manifold reasons for choosing Java for this large science collaboration. Gaia processing is numerically complex but highly distributable, some parts being embarrassingly parallel. We describe the Gaia processing architecture and its realisation in Java. We delve into the astrometric solution which is the most advanced and most complex part of the processing. The Gaia simulator is also written in Java and is the most mature code in the system. This has been successfully running since about 2005 on the supercomputer \u201cMarenostrum\u201d in Barcelona. We relate experiences of using Java on a large shared machine. Finally we discuss Java, including some of its problems, for scientific computing.", "venue": "ArXiv", "authors": ["William  O'Mullane", "Xavier  Luri", "Paul  Parsons", "Uwe  Lammers", "John  Hoar", "Jose  Hernandez"], "year": 2011, "n_citations": 6}
{"id": 3820244, "s2_id": "dc3bfee7184cca2f1be1bfa0bf630d597e69f6f1", "title": "Finding the \"truncated\" polynomial that is closest to a function", "abstract": "When implementing regular enough functions (e.g., elementary or special functions) on a computing system, we frequently use polynomial approximations. In most cases, the polynomial that best approximates (for a given distance and in a given interval) a function has coefficients that are not exactly representable with a finite number of bits. And yet, the polynomial approximations that are actually implemented do have coefficients that are represented with a finite - and sometimes small - number of bits: this is due to the finiteness of the floating-point representations (for software implementations), and to the need to have small, hence fast and/or inexpensive, multipliers (for hardware implementations). We then have to consider polynomial approximations for which the degree i coefficient has at most m_i fractional bits (in other words, it is a rational number with denominator 2^m_i).We provide a method for finding the best polynomial approximation under this constraint.", "venue": "ArXiv", "authors": ["Nicolas  Brisebarre", "Jean-Michel  Muller"], "year": 2003, "n_citations": 3}
{"id": 3823177, "s2_id": "1f605c64772906be58dc6ed9413434fb1dbba130", "title": "Implicit Low-Order Unstructured Finite-Element Multiple Simulation Enhanced by Dense Computation Using OpenACC", "abstract": "In this paper, we develop a low-order three-dimensional finite-element solver for fast multiple-case crust deformation analysis on GPU-based systems. Based on a high-performance solver designed for massively parallel CPU based systems, we modify the algorithm to reduce random data access, and then insert OpenACC directives. The developed solver on ten Reedbush-H nodes (20 P100 GPUs) attained speedup of 14.2 times from 20 K computer nodes, which is high considering the peak memory bandwidth ratio of 11.4 between the two systems. On the newest Volta generation V100 GPUs, the solver attained a further 2.45 times speedup from P100 GPUs. As a demonstrative example, we computed 368 cases of crustal deformation analyses of northeast Japan with 400 million degrees of freedom. The total procedure of algorithm modification and porting implementation took only two weeks; we can see that high performance improvement was achieved with low development cost. With the developed solver, we can expect improvement in reliability of crust-deformation analyses by many-case analyses on a wide range of GPU-based systems.", "venue": "WACCPD@SC", "authors": ["Takuma  Yamaguchi", "Kohei  Fujita", "Tsuyoshi  Ichimura", "Muneo  Hori", "Lalith  Maddegedara", "Kengo  Nakajima"], "year": 2017, "n_citations": 2}
{"id": 3825067, "s2_id": "65d090649e05ae7d254d04a9dbd8a8204a14d0c9", "title": "GPU implementation of algorithm SIMPLE-TS for calculation of unsteady, viscous, compressible and heat-conductive gas flows", "abstract": "The recent trend of using Graphics Processing Units (GPU's) for high performance computations is driven by the high ratio of price performance for these units, complemented by their cost effectiveness. At first glance, computational fluid dynamics (CFD) solvers match perfectly to GPU resources because these solvers make intensive calculations and use relatively little memory. Nevertheless, there are scarce results about the practical use of this serious advantage of GPU over CPU, especially for calculations of viscous, compressible, heat-conductive gas flows with double precision accuracy. In this paper, two GPU algorithms according to time approximation of convective terms were presented: explicit and implicit scheme. To decrease data transfers between device memories and increase the arithmetic intensity of a GPU code we minimize the number of kernels. The GPU algorithm was implemented in one kernel for the implicit scheme and two kernels for the explicit scheme. The numerical equations were put together using macros and optimization, data copy from global to private memory, and data reuse were left to the compiler. Thus keeps the code simpler with excellent maintenance. As a test case, we model the flow past squares in a microchannel at supersonic speed. The tests show that overall speedup of AMD Radeon R9 280X is up to 102x compared to Intel Core i5-4690 core and up to 184x compared to Intel Core i7-920 core, while speedup of NVIDIA Tesla M2090 is up to 11x compared to Intel Core i5-4690 core and up to 20x compared to Intel Core i7-920 core. Memory requirements of GPU code are improved compared to CPU one. It requires 1[GB] global memory for 5.9 million finite volumes that are two times less compared to C++ CPU code. After all the code is simple, portable (written in OpenCL), memory efficient and easily modifiable moreover demonstrates excellent performance.", "venue": "ArXiv", "authors": ["Kiril S. Shterev"], "year": 2018, "n_citations": 0}
{"id": 3826270, "s2_id": "c06d8b387b00ddc248fb09e8d2971d81ec41a5d5", "title": "Performance Optimization and Parallelization of a Parabolic Equation Solver in Computational Ocean Acoustics on Modern Many-core Computer", "abstract": "As one of open-source codes widely used in computational ocean acoustics, FOR3D can provide a very good estimate for underwater acoustic propagation. In this paper, we propose a performance optimization and parallelization to speed up the running of FOR3D. We utilized a variety of methods to enhance the entire performance, such as using a multi-threaded programming model to exploit the potential capability of the many-core node of high-performance computing (HPC) system, tuning compile options, using efficient tuned mathematical library and utilizing vectorization optimization instruction. In addition, we extended the application from single-frequency calculation to multi-frequency calculation successfully by using OpenMP+MPI hybrid programming techniques on the mainstream HPC platform. A detailed performance evaluation was performed and the results showed that the proposed parallelization obtained good accelerated effect of 25.77 \u00d7 when testing a typical three-dimensional medium-sized case on Tianhe-2 supercomputer. It also showed that the tuned parallel version has a weak-scalability. The speed of calculation of underwater sound field can be greatly improved by the strategy mentioned in this paper. The method used in this paper is not only applicable to other similar computing models in computational ocean acoustics but also a guideline of performance enhancement for scientific and engineering application running on modern many-core-computing platform.", "venue": "ArXiv", "authors": ["Min  Xu", "Yong-Yian  Wang", "Anthony T. Chronopoulos", "Hao  Yue"], "year": 2017, "n_citations": 1}
{"id": 3827253, "s2_id": "009b460ec488136bd7b2c692e704591a04511f19", "title": "Fitting Splines to Axonal Arbors Quantifies Relationship Between Branch Order and Geometry", "abstract": "Neuromorphology is crucial to identifying neuronal subtypes and understanding learning. It is also implicated in neurological disease. However, standard morphological analysis focuses on macroscopic features such as branching frequency and connectivity between regions, and often neglects the internal geometry of neurons. In this work, we treat neuron trace points as a sampling of differentiable curves and fit them with a set of branching B-splines. We designed our representation with the Frenet-Serret formulas from differential geometry in mind. The Frenet-Serret formulas completely characterize smooth curves, and involve two parameters, curvature and torsion. Our representation makes it possible to compute these parameters from neuron traces in closed form. These parameters are defined continuously along the curve, in contrast to other parameters like tortuosity which depend on start and end points. We applied our method to a dataset of cortical projection neurons traced in two mouse brains, and found that the parameters are distributed differently between primary, collateral, and terminal axon branches, thus quantifying geometric differences between different components of an axonal arbor. The results agreed in both brains, further validating our representation. The code used in this work can be readily applied to neuron traces in SWC format and is available in our open-source Python package brainlit: http://brainlit.neurodata.io/.", "venue": "Frontiers in Neuroinformatics", "authors": ["Thomas L. Athey", "Jacopo  Teneggi", "Joshua T. Vogelstein", "Daniel  Tward", "Ulrich  Mueller", "Michael I. Miller"], "year": 2021, "n_citations": 1}
{"id": 3828960, "s2_id": "25cac11fc8c6ad29d544468946cf0e32fa9d1c35", "title": "Software for the Gale transform of fewnomial systems and a Descartes rule for fewnomials", "abstract": "We give a Descartes\u2019-like bound on the number of positive solutions to a system of fewnomials that holds when its exponent vectors are not in convex position and a sign condition is satisfied. This was discovered while developing algorithms and software for computing the Gale transform of a fewnomial system, which is our main goal. This software is a component of a package we are developing for Khovanskii-Rolle continuation, which is a numerical algorithm to compute the real solutions to a system of fewnomials.", "venue": "Numerical Algorithms", "authors": ["Daniel J. Bates", "Jonathan D. Hauenstein", "Matthew E. Niemerg", "Frank  Sottile"], "year": 2015, "n_citations": 4}
{"id": 3831521, "s2_id": "390aa47d9f80a5b3b526d2b5dd6fdadd59059f16", "title": "A New Recursive Algorithm For Inverting A General Comrade Matrix", "abstract": "In this paper, the author present a reliable symbolic computational algorithm for inverting a general comrade matrix by using parallel computing along with recursion. The computational cost of our algorithm is O(n^2). The algorithm is implementable to the Computer Algebra System (CAS) such as MAPLE, MATLAB and MATHEMATICA. Three examples are presented for the sake of illustration.", "venue": "Ars Comb.", "authors": ["A. A. Karawia"], "year": 2017, "n_citations": 1}
{"id": 3832086, "s2_id": "0c2c2197e11b1b1e65c7a246119e19e0842879e9", "title": "Core Imaging Library - Part I: a versatile Python framework for tomographic imaging", "abstract": "We present the Core Imaging Library (CIL), an open-source Python framework for tomographic imaging with particular emphasis on reconstruction of challenging datasets. Conventional filtered back-projection reconstruction tends to be insufficient for highly noisy, incomplete, non-standard or multi-channel data arising for example in dynamic, spectral and in situ tomography. CIL provides an extensive modular optimization framework for prototyping reconstruction methods including sparsity and total variation regularization, as well as tools for loading, preprocessing and visualizing tomographic data. The capabilities of CIL are demonstrated on a synchrotron example dataset and three challenging cases spanning golden-ratio neutron tomography, cone-beam X-ray laminography and positron emission tomography. This article is part of the theme issue \u2018Synergistic tomographic image reconstruction: part 2\u2019.", "venue": "Philosophical Transactions of the Royal Society A", "authors": ["Jakob S. Jorgensen", "Evelina  Ametova", "Genoveva  Burca", "Gemma  Fardell", "Evangelos  Papoutsellis", "Edoardo  Pasca", "Kris  Thielemans", "Martin  Turner", "Ryan  Warr", "William R. B. Lionheart", "Philip J. Withers"], "year": 2021, "n_citations": 10}
{"id": 3833776, "s2_id": "9fc7d61b6b51c6a99d502b548f99d95e02eec467", "title": "M@th Desktop and MD Tools - Mathematics and Mathematica Made Easy for Students", "abstract": "We present two add-ons for Mathematica for teaching mathematics to undergraduate and high school students. These two applications, M@th Desktop (MD) and M@th Desktop Tools (MDTools), include several palettes and notebooks covering almost every field. The underlying didactic concept is so-called \"blended learning\", in which these tools are meant to be used as a complement to the professor or teacher rather than as a replacement, which other e-learning applications do. They enable students to avoid the usual problem of computer-based learning, namely that too large an amount of time is wasted struggling with computer and program errors instead of actually learning the mathematical concepts. \nM@th Desktop Tools is palette-based and provides easily accessible and user-friendly templates for the most important functions in the fields of Analysis, Algebra, Linear Algebra and Statistics. M@th Desktop, in contrast, is a modern, interactive teaching and learning software package for mathematics classes. It is comprised of modules for Differentiation, Integration, and Statistics, and each module presents its topic with a combination of interactive notebooks and palettes. \nBoth packages can be obtained from Deltasoft's homepage at this http URL .", "venue": "ArXiv", "authors": ["Reinhold  Kainhofer", "Reinhard V. Simonovits"], "year": 2004, "n_citations": 1}
{"id": 3837296, "s2_id": "64f5665d8e888ce79f072c292714e74d2759b4e8", "title": "cliquematch: Finding correspondence via cliques in large graphs", "abstract": "The maximum clique problem finds applications in computer vision, bioinformatics, and network analysis, many of which involve the construction of correspondence graphs to find similarities between two given objects. cliquematch is a Python package designed for this purpose: it provides a simple framework to construct correspondence graphs, and implements an algorithm to find and enumerate maximum cliques in C++, that can process graphs of a few million edges on consumer hardware, with comparable performance to publicly available methods.", "venue": "ArXiv", "authors": ["Gautham  Venkatasubramanian"], "year": 2021, "n_citations": 1}
{"id": 3843081, "s2_id": "7884a4ab86e8f8de60ad716fc3e52cc834051cc2", "title": "Operand Folding Hardware Multipliers", "abstract": "This paper describes a new accumulate-and-add multiplication algorithm. The method partitions one of the operands and re-combines the results of computations done with each of the partitions. The resulting design turns-out to be both compact and fast. \n \nWhen the operands' bit-length m is 1024, the new algorithm requires only 0.194m+56 additions (on average), this is about half the number of additions required by the classical accumulate-and-add multiplication algorithm ( $\\frac{m}2$ ).", "venue": "Cryptography and Security", "authors": ["Byungchun  Chung", "Sandra  Marcello", "Amir-Pasha  Mirbaha", "David  Naccache", "Karim  Sabeg"], "year": 2012, "n_citations": 1}
{"id": 3848819, "s2_id": "66e48d6413c46b66f8bbb42b29279d4e07f0691d", "title": "swMATH - A New Information Service for Mathematical Software", "abstract": "An information service for mathematical software is presented. Publications and software are two closely connected facets of mathematical knowledge. This relation can be used to identify mathematical software and find relevant information about it. The approach and the state of the art of the information service are described here.", "venue": "MKM/Calculemus/DML", "authors": ["Sebastian  B\u00f6nisch", "Michael  Brickenstein", "Hagen  Chrapary", "Gert-Martin  Greuel", "Wolfram  Sperber"], "year": 2013, "n_citations": 7}
{"id": 3852087, "s2_id": "32f728cb6547eb021182cd9501ed2a0d33068976", "title": "Towards OpenMath Content Dictionaries as Linked Data", "abstract": "The term 'Linked Data' refers to a set of best practices for publishing and connecting structured data on the web\" (7). Linked Data make the Semantic Web work practically, which means that informa- tion can be retrieved without complicated lookup mechanisms, that a lightweight semantics enables scalable reasoning, and that the decentral nature of the Web is respected. OpenMath Content Dictionaries (CDs) have the same characteristics - in principle, but not yet in practice. The Linking Open Data movement has made a considerable practical impact: Governments, broadcasting stations, scientific publishers, and many more actors are already contributing to the \"Web of Data\". Queries can be answered in a distributed way, and services aggregating data from dierent sources are replacing hard-coded mashups. However, these services are currently entirely lacking mathematical functionality. I will discuss real-world scenarios, where today's RDF-based Linked Data do not quite get their job done, but where an integration of OpenMath would help - were it not for certain conceptual and practical restrictions. I will point out conceptual shortcomings in the OpenMath 2 specification and common bad practices in publishing CDs and then propose concrete steps to overcome them and to contribute OpenMath CDs to the Web of Data.", "venue": "ArXiv", "authors": ["Christoph  Lange"], "year": 2010, "n_citations": 6}
{"id": 3852443, "s2_id": "e6f3a2b3b5b24dfa1ac759b3c632ab7837be448b", "title": "Randomized Matrix Decompositions using R", "abstract": "Matrix decompositions are fundamental tools in the area of applied mathematics, statistical computing, and machine learning. In particular, low-rank matrix decompositions are vital, and widely used for data analysis, dimensionality reduction, and data compression. Massive datasets, however, pose a computational challenge for traditional algorithms, placing significant constraints on both memory and processing power. Recently, the powerful concept of randomness has been introduced as a strategy to ease the computational load. The essential idea of probabilistic algorithms is to employ some amount of randomness in order to derive a smaller matrix from a high-dimensional data matrix. The smaller matrix is then used to compute the desired low-rank approximation. Such algorithms are shown to be computationally efficient for approximating matrices with low-rank structure. We present the \\proglang{R} package rsvd, and provide a tutorial introduction to randomized matrix decompositions. Specifically, randomized routines for the singular value decomposition, (robust) principal component analysis, interpolative decomposition, and CUR decomposition are discussed. Several examples demonstrate the routines, and show the computational advantage over other methods implemented in R.", "venue": "Journal of Statistical Software", "authors": ["N. Benjamin Erichson", "Sergey  Voronin", "Steven L. Brunton", "J. Nathan Kutz"], "year": 2019, "n_citations": 80}
{"id": 3852485, "s2_id": "2b084fc79e4a88955889417841c26478056c25da", "title": "GuiTeNet: A graphical user interface for tensor networks", "abstract": "We introduce a graphical user interface for constructing arbitrary tensor networks and specifying common operations like contractions or splitting, denoted GuiTeNet. Tensors are represented as nodes with attached legs, corresponding to the ordered dimensions of the tensor. GuiTeNet visualizes the current network, and instantly generates Python/NumPy source code for the hitherto sequence of user actions. Support for additional programming languages is planned for the future. We discuss the elementary operations on tensor networks used by GuiTeNet, together with high-level optimization strategies. The software runs directly in web browsers and is available online at this http URL.", "venue": "ArXiv", "authors": ["Lisa  Sahlmann", "Christian B. Mendl"], "year": 2018, "n_citations": 0}
{"id": 3862209, "s2_id": "ce668dd118a09433e829a0a6bb0189f7a39e76af", "title": "Computing the Lambert W function in arbitrary-precision complex interval arithmetic", "abstract": "We describe an algorithm to evaluate all the complex branches of the Lambert W function with rigorous error bounds in arbitrary-precision interval arithmetic or ball arithmetic. The classic 1996 paper on the Lambert W function by Corless et al. provides a thorough but partly heuristic numerical analysis of the Lambert W function which needs to be complemented with some explicit inequalities and practical observations about managing precision and branch cuts. An implementation is provided in the Arb library.", "venue": "Numerical Algorithms", "authors": ["Fredrik  Johansson"], "year": 2019, "n_citations": 9}
{"id": 3868266, "s2_id": "2c19bc9c9fd25175ef83a054ad455eb6071ccfd5", "title": "Recent Developments in Iterative Methods for Reducing Synchronization", "abstract": "On modern parallel architectures, the cost of synchronization among processors can often dominate the cost of floating-point computation. Several modifications of the existing methods have been proposed in order to keep the communication cost as low as possible. This paper aims at providing a brief overview of recent advances in parallel iterative methods for solving large-scale problems. We refer the reader to the related references for more details on the derivation, implementation, performance, and analysis of these techniques.", "venue": "2019 18th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)", "authors": ["Qinmeng  Zou", "Frederic  Magoules"], "year": 2019, "n_citations": 1}
{"id": 3871438, "s2_id": "62f46038892c19913dec7ba8dceb28f03627fb1d", "title": "Parallel Algorithms for Constructing Data Structures for Fast Multipole Methods", "abstract": "We present efficient algorithms to build data structures and the lists needed for fast multipole methods. The algorithms are capable of being efficiently implemented on both serial, data parallel GPU and on distributed architectures. With these algorithms it is possible to map the FMM efficiently on to the GPU or distributed heterogeneous CPU-GPU systems. Further, in dynamic problems, as the distribution of the particles change, the reduced cost of building the data structures improves performance. Using these algorithms, we demonstrate example high fidelity simulations with large problem sizes by using FMM on both single and multiple heterogeneous computing facilities equipped with multi-core CPU and many-core GPUs.", "venue": "ArXiv", "authors": ["Qi  Hu", "Nail A. Gumerov", "Ramani  Duraiswami"], "year": 2013, "n_citations": 3}
{"id": 3878853, "s2_id": "6a087fe59a42b7468d469b5ec00ada624aaf6509", "title": "bertha: Project skeleton for scientific software", "abstract": "Science depends heavily on reliable and easy-to-use software packages, such as mathematical libraries or data analysis tools. Developing such packages requires a lot of effort, which is too often avoided due to the lack of funding or recognition. In order to reduce the efforts required to create sustainable software packages, we present a project skeleton that ensures the best software engineering practices from the start of a project, or serves as reference for existing projects.", "venue": "PloS one", "authors": ["Michael  Riesch", "Tien Dat Nguyen", "Christian  Jirauschek"], "year": 2020, "n_citations": 9}
{"id": 3880499, "s2_id": "d34237e07976a16793e5520b528d470bbea30862", "title": "Towards an Efficient Tile Matrix Inversion of Symmetric Positive Definite Matrices on Multicore Architectures", "abstract": "The algorithms in the current sequential numerical linear algebra libraries (e.g. LAPACK) do not parallelize well on multicore architectures. A new family of algorithms, the tile algorithms, has recently been introduced. Previous research has shown that it is possible to write efficient and scalable tile algorithms for performing a Cholesky factorization, a (pseudo) LU factorization, a QR factorization, and computing the inverse of a symmetric positive definite matrix. In this extended abstract, we revisit the computation of the inverse of a symmetric positive definite matrix. We observe that, using a dynamic task scheduler, it is relatively painless to translate existing LAPACK code to obtain a ready-to-be-executed tile algorithm. However we demonstrate that, for some variants, non trivial compiler techniques (array renaming, loop reversal and pipelining) need then to be applied to further increase the parallelism of the application. We present preliminary experimental results.", "venue": "VECPAR", "authors": ["Emmanuel  Agullo", "Henricus  Bouwmeester", "Jack J. Dongarra", "Jakub  Kurzak", "Julien  Langou", "Lee  Rosenberg"], "year": 2010, "n_citations": 28}
{"id": 3882681, "s2_id": "3112641bc0be2c1675fe059764dc416f6c9d2223", "title": "Using the pyMIC Offload Module in PyFR", "abstract": "PyFR is an open-source high-order accurate computational fluid dynamics solver for unstructured grids. It is designed to efficiently solve the compressible Navier-Stokes equations on a range of hardware platforms, including GPUs and CPUs. In this paper we will describe how the Python Offload Infrastructure for the Intel Many Integrated Core Architecture (pyMIC) was used to enable PyFR to run with near native performance on the Intel Xeon Phi coprocessor. We will introduce the architecture of both pyMIC and PyFR and present a variety of examples showcasing the capabilities of pyMIC. Further, we will also compare the contrast pyMIC to other approaches including native execution and OpenCL. The process of adding support for pyMIC into PyFR will be described in detail. Benchmark results show that for a standard cylinder flow problem PyFR with pyMIC is able achieve 240 GFLOP/s of sustained double precision floating point performance; for a 1.85 times improvement over PyFR with C/OpenMP on a 12 core Intel Xeon E5-2697 v2 CPU.", "venue": "ArXiv", "authors": ["Michael  Klemm", "Freddie D. Witherden", "Peter E. Vincent"], "year": 2016, "n_citations": 2}
{"id": 3885611, "s2_id": "52aaa1cb29f095995c0cf2e7c7a8d2dc8084e492", "title": "SClib, a hack for straightforward embedded C functions in Python", "abstract": "We present SClib, a simple hack that allows easy and straightforward evaluation of C functions within Python code, boosting flexibility for better trade-off between computation power and feature availability, such as visualization and existing computation routines in SciPy. We also present two cases were SClib has been used. In the first set of applications we use SClib to write a port to Python of a Schr\\\"odinger equation solver that has been extensively used the literature, the resulting script presents a speed-up of about 150x with respect to the original one. A review of the situations where the speeded-up script has been used is presented. We also describe the solution to the related problem of solving a set of coupled Schr\\\"odinger-like equations where SClib is used to implement the speed-critical parts of the code. We argue that when using SClib within IPython we can use NumPy and Matplotlib for the manipulation and visualization of the solutions in an interactive environment with no performance compromise. The second case is an engineering application. We use SClib to evaluate the control and system derivatives in a feedback control loop for electrical motors. With this and the integration routines available in SciPy, we can run simulations of the control loop a la Simulink. The use of C code not only boosts the speed of the simulations, but also enables to test the exact same code that we use in the test rig to get experimental results. Again, integration with IPython gives us the flexibility to analyze and visualize the data.", "venue": "ArXiv", "authors": ["Esteban  Fuentes", "Hector E. Martinez"], "year": 2014, "n_citations": 4}
{"id": 3885762, "s2_id": "f199b0b8a64bf14f9310bc38589af2cf2255f129", "title": "Issues in the software implementation of stochastic numerical Runge-Kutta", "abstract": "This paper discusses the application of stochastic Runge-Kutta-like numerical methods with weak and strong convergences for systems of stochastic differential equations in Ito form. At the beginning a brief overview of available publications about stochastic numerical methods and information from the theory of stochastic differential equations are given. Then the difficulties that arise when trying to implement stochastic numerical methods and motivate to use source code generation are described. We discuss some implementation details, such as program languages (Python, Julia) and libraries (Jinja2, Numpy). Also the link to the repository with source code is provided in the article.", "venue": "ArXiv", "authors": ["Migran N. Gevorkyan", "Anastasiya V. Demidova", "Anna V. Korolkova", "Dmitry S. Kulyabov"], "year": 2018, "n_citations": 1}
{"id": 3887245, "s2_id": "4ef541c956f01f76fb42e7d66e0bf706ad423249", "title": "AMReX: Block-structured adaptive mesh refinement for multiphysics applications", "abstract": "Block-structured adaptive mesh refinement (AMR) provides the basis for the temporal and spatial discretization strategy for a number of Exascale Computing Project applications in the areas of accelerator design, additive manufacturing, astrophysics, combustion, cosmology, multiphase flow, and wind plant modeling. AMReX is a software framework that provides a unified infrastructure with the functionality needed for these and other AMR applications to be able to effectively and efficiently utilize machines from laptops to exascale architectures. AMR reduces the computational cost and memory footprint compared to a uniform mesh while preserving accurate descriptions of different physical processes in complex multiphysics algorithms. AMReX supports algorithms that solve systems of partial differential equations in simple or complex geometries and those that use particles and/or particle\u2013mesh operations to represent component physical processes. In this article, we will discuss the core elements of the AMReX framework such as data containers and iterators as well as several specialized operations to meet the needs of the application projects. In addition, we will highlight the strategy that the AMReX team is pursuing to achieve highly performant code across a range of accelerator-based architectures for a variety of different applications.", "venue": "The International Journal of High Performance Computing Applications", "authors": ["Weiqun  Zhang", "Andrew  Myers", "Kevin  Gott", "Ann  Almgren", "John  Bell"], "year": 2021, "n_citations": 7}
{"id": 3887781, "s2_id": "b332f34ed641fe59d256688ba1e55b3cf7931dd1", "title": "Differentiation of the Cholesky decomposition", "abstract": "We review strategies for differentiating matrix-based computations, and derive symbolic and algorithmic update rules for differentiating expressions containing the Cholesky decomposition. We recommend new `blocked' algorithms, based on differentiating the Cholesky algorithm DPOTRF in the LAPACK library, which uses `Level 3' matrix-matrix operations from BLAS, and so is cache-friendly and easy to parallelize. For large matrices, the resulting algorithms are the fastest way to compute Cholesky derivatives, and are an order of magnitude faster than the algorithms in common usage. In some computing environments, symbolically-derived updates are faster for small matrices than those based on differentiating Cholesky algorithms. The symbolic and algorithmic approaches can be combined to get the best of both worlds.", "venue": "ArXiv", "authors": ["Iain  Murray"], "year": 2016, "n_citations": 30}
{"id": 3898343, "s2_id": "708779405897d45aff3a8c8daecbd89cb05d86ae", "title": "Symbolic spectral decomposition of 3x3 matrices", "abstract": "Spectral decomposition of matrices is a recurring and important task in applied mathematics, physics and engineering. Many application problems require the consideration of matrices of size three with spectral decomposition over the real numbers. If the functional dependence of the spectral decomposition on the matrix elements has to be preserved, then closed-form solution approaches must be considered. Existing closed-form expressions are based on the use of principal matrix invariants which suffer from a number of deficiencies when evaluated in the framework of finite precision arithmetic. This paper introduces an alternative form for the computation of the involved matrix invariants (in particular the discriminant) in terms of sum-of-products expressions as function of the matrix elements. We prove and demonstrate by numerical examples that this alternative approach leads to increased floating point accuracy, especially in all important limit cases (e.g. eigenvalue multiplicity). It is believed that the combination of symbolic algorithms with the accuracy improvements presented in this paper can serve as a powerful building block for many engineering tasks.", "venue": "ArXiv", "authors": ["Michal  Habera", "Andreas  Zilian"], "year": 2021, "n_citations": 0}
{"id": 3900511, "s2_id": "4d44d473cbec2fa9b30261dcbaf7d975fadfdc1e", "title": "GPTIPS 2: An Open-Source Software Platform for Symbolic Data Mining", "abstract": "GPTIPS is a free, open source MATLAB based software platform for symbolic data mining (SDM). It uses a multigene variant of the biologically inspired machine learning method of genetic programming (MGGP) as the engine that drives the automatic model discovery process. Symbolic data mining is the process of extracting hidden, meaningful relationships from data in the form of symbolic equations. In contrast to other data-mining methods, the structural transparency of the generated predictive equations can give new insights into the physical systems or processes that generated the data. Furthermore, this transparency makes the models very easy to deploy outside of MATLAB.", "venue": "Handbook of Genetic Programming Applications", "authors": ["Dominic P. Searson"], "year": 2015, "n_citations": 135}
{"id": 3904953, "s2_id": "87515768a6f7a03894430a2d327745325be16573", "title": "External Use of TOPCAT's Plotting Library", "abstract": "The table analysis application TOPCAT uses a custom Java plotting library for highly configurable high-performance interactive or exported visualisations in two and three dimensions. We present here a variety of ways for end users or application developers to make use of this library outside of the TOPCAT application: via the command-line suite STILTS or its Jython variant JyStilts, via a traditional Java API, or by programmatically assigning values to a set of parameters in java code or using some form of inter-process communication. The library has been built with large datasets in mind; interactive plots scale well up to several million points, and static output to standard graphics formats is possible for unlimited sized input data.", "venue": "ArXiv", "authors": ["M. B. Taylor"], "year": 2014, "n_citations": 4}
{"id": 3906026, "s2_id": "d65fdbf8e816c178221fee30758351cbc2586d2e", "title": "Parallel Integer Polynomial Multiplication", "abstract": "We propose a new algorithm for multiplying densepolynomials with integer coefficients in a parallel fashion, targetingmulti-core processor architectures. Complexity estimates andexperimental comparisons demonstrate the advantages of this newapproach.", "venue": "2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)", "authors": ["Changbo  Chen", "Svyatoslav  Covanov", "Farnam  Mansouri", "Marc Moreno Maza", "Ning  Xie", "Yuzhen  Xie"], "year": 2016, "n_citations": 7}
{"id": 3907349, "s2_id": "49a3130c553e8cfc0e23bc4a56c874217dc0cdc6", "title": "The swept rule for breaking the latency barrier in time advancing PDEs", "abstract": "This article investigates the swept rule of space-time domain decomposition, an idea to break the latency barrier via communicating less often when explicitly solving time-dependent PDEs. The swept rule decomposes space and time among computing nodes in ways that exploit the domains of influence and the domain of dependency, making it possible to communicate once per many timesteps without redundant computation. The article presents simple theoretical analysis to the performance of the swept rule which then was shown to be accurate by conducting numerical experiments.", "venue": "J. Comput. Phys.", "authors": ["Maitham Makki Alhubail", "Qiqi  Wang"], "year": 2016, "n_citations": 14}
{"id": 3908913, "s2_id": "ea65acac5bc7c28dcc228839af98d9ef6944a607", "title": "bayes4psy\u2014An Open Source R Package for Bayesian Statistics in Psychology", "abstract": "Research in psychology generates complex data and often requires unique statistical analyses. These tasks are often very specific, so appropriate statistical models and methods cannot be found in accessible Bayesian tools. As a result, the use of Bayesian methods is limited to researchers and students that have the technical and statistical fundamentals that are required for probabilistic programming. Such knowledge is not part of the typical psychology curriculum and is a difficult obstacle for psychology students and researchers to overcome. The goal of the bayes4psy package is to bridge this gap and offer a collection of models and methods to be used for analysing data that arises from psychological experiments and as a teaching tool for Bayesian statistics in psychology. The package contains the Bayesian t-test and bootstrapping along with models for analysing reaction times, success rates, and tasks utilizing colors as a response. It also provides the diagnostic, analytic and visualization tools for the modern Bayesian data analysis workflow.", "venue": "Frontiers in Psychology", "authors": ["Jure  Demsar", "Grega  Repovs", "Erik  Strumbelj"], "year": 2020, "n_citations": 0}
{"id": 3909116, "s2_id": "c6f930044ebd4363450bedb91b7f67639fcb45f8", "title": "Documentation Generator Focusing on Symbols for the HTML-ized Mizar Library", "abstract": "The purpose of this project is to collect symbol information in the Mizar Mathematical Library and manipulate it into practical and organized documentation. Inspired by the MathWiki project and API reference systems for computer programs, we developed a documentation generator focusing on symbols for the HTML-ized Mizar library. The system has several helpful features, including a symbol list, incremental search, and a referrer list. It targets those who use proof assistance systems, the volume of whose libraries has been rapidly increasing year byi\u00be?year.", "venue": "CICM", "authors": ["Kazuhisa  Nakasho", "Yasunari  Shidama"], "year": 2015, "n_citations": 3}
{"id": 3913053, "s2_id": "26a289e5695025360f4722cf3db176a717a543f5", "title": "A persistence landscapes toolbox for topological statistics", "abstract": "Topological data analysis provides a multiscale description of the geometry and topology of quantitative data. The persistence landscape is a topological summary that can be easily combined with tools from statistics and machine learning. We give efficient algorithms for calculating persistence landscapes, their averages, and distances between such averages. We discuss an implementation of these algorithms and some related procedures. These are intended to facilitate the combination of statistics and machine learning with topological data analysis. We present an experiment showing that the low-dimensional persistence landscapes of points sampled from spheres (and boxes) of varying dimensions differ.", "venue": "J. Symb. Comput.", "authors": ["Peter  Bubenik", "Pawel  Dlotko"], "year": 2017, "n_citations": 98}
{"id": 3913093, "s2_id": "9bcbc04f98146968f75b35a8d286b7ea7281af7d", "title": "Efficient gluing of numerical continuation and a multiple solution method for elliptic PDEs", "abstract": "Numerical continuation calculations for ordinary differential equations (ODEs) are, by now, an established tool for bifurcation analysis in dynamical systems theory as well as across almost all natural and engineering sciences. Although several excellent standard software packages are available for ODEs, there are - for good reasons - no standard numerical continuation toolboxes available for partial differential equations (PDEs), which cover a broad range of different classes of PDEs automatically. A natural ach to this problem is to look for efficient gluing computation approaches, with independent components developed by researchers in numerical analysis, dynamical systems, scientific computing and mathematical modeling. In this paper, we shall study several elliptic PDEs (Lane-Emden-Fowler, Lane-Emden-Fowler with microscopic force, Caginalp) via the numerical continuation software pde2path and develop a gluing component to determine a set of starting solutions for the continuation by exploiting the variational structures of the PDEs. In particular, we solve the initialization problem of numerical continuation for PDEs via a minimax algorithm to find multiple unstable solution. Furthermore, for the Caginalp system, we illustrate the efficient gluing link of pde2path to the underlying mesh generation and the FEM MatLab pdetoolbox. Even though the approach works efficiently due to the high-level programming language and without developing any new algorithms, we still obtain interesting bifurcation diagrams and directly applicable conclusions about the three elliptic PDEs we study, in particular with respect to symmetry-breaking. In particular, we show for a modified Lane-Emden-Fowler equation with an asymmetric microscopic force, how a fully connected bifurcation diagram splits up into C-shaped isolas on which localized pattern deformation appears towards two different regimes. We conclude with a section on future software development issues that would be helpful to be addressed to simplify interfaces to allow for more efficient, time-saving, gluing computation for dynamical systems analysis of PDEs in the near future.", "venue": "Appl. Math. Comput.", "authors": ["Christian  Kuehn"], "year": 2015, "n_citations": 18}
{"id": 3917861, "s2_id": "d0e2b721e54cd14fad5d9bae30ef74ee41272093", "title": "Similarity Downselection: A Python implementation of a heuristic search algorithm for finding the set of the n most dissimilar items with an application in conformer sampling", "abstract": "Finding the set of the n items most dissimilar from each other out of a larger population becomes increasingly difficult and computationally expensive as either n or the population size grows large. Finding the set of the n most dissimilar items is different than simply sorting an array of numbers because there exists a pairwise relationship between each item and all other items in the population. For instance, if you have a set of the most dissimilar n=4 items, one or more of the items from n=4 might not be in the set n=5. An exact solution would have to search all possible combinations of size n in the population, exhaustively. We present an open-source software called similarity downselection (SDS), written in Python and freely available on GitHub. SDS implements a heuristic algorithm for quickly finding the approximate set(s) of the n most dissimilar items. We benchmark SDS against a Monte Carlo method, which attempts to find the exact solution through repeated random sampling. We show that for SDS to find the set of n most dissimilar conformers, our method is not only orders of magnitude faster, but is also more accurate than running the Monte Carlo for 1,000,000 iterations, each searching for set sizes n=3\u2013 7 out of a population of 50,000. We also benchmark SDS against the exact solution for example small populations, showing SDS produces a solution close to the exact solution in these instances.", "venue": "ArXiv", "authors": ["Felicity F. Nielson", "Sean M. Colby", "Ryan S. Renslow", "Thomas O. Metz"], "year": 2021, "n_citations": 0}
{"id": 3919276, "s2_id": "950081d080248f0493d52238ddd29086d499bd2c", "title": "Containers for Portable, Productive, and Performant Scientific Computing", "abstract": "Containers are an emerging technology that holds promise for improving productivity and code portability in scientific computing. The authors examine Linux container technology for the distribution of a nontrivial scientific computing software stack and its execution on a spectrum of platforms from laptop computers through high-performance computing systems. For Python code run on large parallel computers, the runtime is reduced inside a container due to faster library imports. The software distribution approach and data that the authors present will help developers and users decide on whether container technology is appropriate for them. The article also provides guidance for vendors of HPC systems that rely on proprietary libraries for performance on what they can do to make containers work seamlessly and without performance penalty.", "venue": "Computing in Science & Engineering", "authors": ["Jack S. Hale", "Lizao  Li", "Chris N. Richardson", "Garth N. Wells"], "year": 2017, "n_citations": 31}
{"id": 3924759, "s2_id": "310e2276c660e127b1fe424e32c334760a68fe0c", "title": "Shared Memory Pipelined Parareal", "abstract": "For the parallel-in-time integration method Parareal, pipelining can be used to hide some of the cost of the serial correction step and improve its efficiency. The paper introduces a basic OpenMP implementation of pipelined Parareal and compares it to a standard MPI-based variant. Both versions yield almost identical runtimes, but, depending on the compiler, the OpenMP variant consumes about 7% less energy and has a significantly smaller memory footprint. However, its higher implementation complexity might make it difficult to use in legacy codes and in combination with spatial parallelisation.", "venue": "Euro-Par", "authors": ["Daniel  Ruprecht"], "year": 2017, "n_citations": 3}
{"id": 3929555, "s2_id": "60b3022a0e63e37debb6bee96c075b4f9e77fbe4", "title": "Solving polynomial systems via homotopy continuation and monodromy", "abstract": "We study methods for finding the solution set of a generic system in a family of polynomial systems with parametric coefficients. We present a framework for describing monodromy based solvers in terms of decorated graphs. Under the theoretical assumption that monodromy actions are generated uniformly, we show that the expected number of homotopy paths tracked by an algorithm following this framework is linear in the number of solutions. We demonstrate that our software implementation is competitive with the existing state-of-the-art methods implemented in other software packages.", "venue": "ArXiv", "authors": ["Timothy  Duff", "Cvetelina  Hill", "Anders Nedergaard Jensen", "Kisun  Lee", "Anton  Leykin", "Jeff  Sommars"], "year": 2016, "n_citations": 43}
{"id": 3931104, "s2_id": "e1de266d6e22aa06fff0d335f5361ed066228e00", "title": "An open source C++ implementation of multi-threaded Gaussian mixture models, k-means and expectation maximisation", "abstract": "Modelling of multivariate densities is a core component in many signal processing, pattern recognition and machine learning applications. The modelling is often done via Gaussian mixture models (GMMs), which use computationally expensive and potentially unstable training algorithms. We provide an overview of a fast and robust implementation of GMMs in the C++ language, employing multi-threaded versions of the Expectation Maximisation (EM) and k-means training algorithms. Multi-threading is achieved through reformulation of the EM and k-means algorithms into a MapReduce-like framework. Furthermore, the implementation uses several techniques to improve numerical stability and modelling accuracy. We demonstrate that the multi-threaded implementation achieves a speedup of an order of magnitude on a recent 16 core machine, and that it can achieve higher modelling accuracy than a previously well-established publically accessible implementation. The multi-threaded implementation is included as a user-friendly class in recent releases of the open source Armadillo C++ linear algebra library. The library is provided under the permissive Apache 2.0 license, allowing unencumbered use in commercial products.", "venue": "2017 11th International Conference on Signal Processing and Communication Systems (ICSPCS)", "authors": ["Conrad  Sanderson", "Ryan R. Curtin"], "year": 2017, "n_citations": 5}
{"id": 3932172, "s2_id": "261e92397b032444a594ccf06535fe9992799dd6", "title": "Hyperbolic Diffusion in Flux Reconstruction: Optimisation through Kernel Fusion within Tensor-Product Elements", "abstract": "Novel methods are presented in this initial study for the fusion of GPU kernels in the artificial compressibility method (ACM), using tensor product elements with constant Jacobians and flux reconstruction. This is made possible through the hyperbolisation of the diffusion terms, which eliminates the expensive algorithmic steps needed to form the viscous stresses. Two fusion approaches are presented, which offer differing levels of parallelism. This is found to be necessary for the change in workload as the order of accuracy of the elements is increased. Several further optimisations of these approaches are demonstrated, including a generation time memory manager which maximises resource usage. The fused kernels are able to achieve 3-4 times speedup, which compares favourably with a theoretical maximum speedup of 4. In three dimensional test cases, the generated fused kernels are found to reduce total runtime by ${\\sim}25\\%$, and, when compared to the standard ACM formulation, simulations demonstrate that a speedup of $2.3$ times can be achieved.", "venue": "Computer Physics Communications", "authors": ["Will  Trojak", "Rob  Watson", "Freddie  Witherden"], "year": 2021, "n_citations": 1}
{"id": 3932769, "s2_id": "71dd4bbdbd110e13da49b215ade8aab763cf6018", "title": "Efficient space-time reduced order model for linear dynamical systems in Python using less than 120 lines of code", "abstract": "A classical reduced order model (ROM) for dynamical problems typically involves only the spatial reduction of a given problem. Recently, a novel space\u2013time ROM for linear dynamical problems has been developed [Choi et al., Space\u2013tume reduced order model for large-scale linear dynamical systems with application to Boltzmann transport problems, Journal of Computational Physics, 2020], which further reduces the problem size by introducing a temporal reduction in addition to a spatial reduction without much loss in accuracy. The authors show an order of a thousand speed-up with a relative error of less than 10\u22125 for a large-scale Boltzmann transport problem. In this work, we present for the first time the derivation of the space\u2013time least-squares Petrov\u2013Galerkin (LSPG) projection for linear dynamical systems and its corresponding block structures. Utilizing these block structures, we demonstrate the ease of construction of the space\u2013time ROM method with two model problems: 2D diffusion and 2D convection diffusion, with and without a linear source term. For each problem, we demonstrate the entire process of generating the full order model (FOM) data, constructing the space\u2013time ROM, and predicting the reduced-order solutions, all in less than 120 lines of Python code. We compare our LSPG method with the traditional Galerkin method and show that the space\u2013time ROMs can achieve O(10\u22123) to O(10\u22124) relative errors for these problems. Depending on parameter\u2013separability, online speed-ups may or may not be achieved. For the FOMs with parameter\u2013separability, the space\u2013time ROMs can achieve O(10) online speed-ups. Finally, we present an error analysis for the space\u2013time LSPG projection and derive an error bound, which shows an improvement compared to traditional spatial Galerkin ROM methods.", "venue": "Mathematics", "authors": ["Youngkyu  Kim", "Karen May Wang", "Youngsoo  Choi"], "year": 2021, "n_citations": 5}
{"id": 3940295, "s2_id": "435994b608e5a90e6694a793f06eccc0c1092ef3", "title": "A multiprecision matrix calculation library and its extension library for a matrix-product-state simulation of quantum computing", "abstract": "A C++ library, named ZKCM, has been developed for the purpose of multiprecision matrix calculations, which is based on the GNU MP and MPFR libraries. It is especially convenient for writing programs involving tensor-product operations, tracing-out operations, and singular-value decompositions. Its extension library, ZKCM_QC, for simulating quantum computing has been developed using the time-dependent matrix-product-state simulation method. This report gives a brief introduction to the libraries with sample programs.", "venue": "ArXiv", "authors": ["Akira  Saitoh"], "year": 2011, "n_citations": 1}
{"id": 3947087, "s2_id": "610f766a4a998b581311ba0187ae5238bf85b7e8", "title": "The Two-Dimensional Swept Rule Applied on Heterogeneous Architectures", "abstract": "The partial differential equations describing compressible fluid flows can be notoriously difficult to resolve on a pragmatic scale and often require the use of high performance computing systems and/or accelerators. However, these systems face scaling issues such as latency, the fixed cost of communicating information between devices in the system. The swept rule is a technique designed to minimize these costs by obtaining a solution to unsteady equations at as many possible spatial locations and times prior to communicating. In this study, we implemented and tested the swept rule for solving two-dimensional problems on heterogeneous computing systems across two distinct systems. Our solver showed a speedup range of 0.22\u20132.71 for the heat diffusion equation and 0.52\u20131.46 for the compressible Euler equations. We can conclude from this study that the swept rule offers both potential for speedups and slowdowns and that care should be taken when designing such a solver to maximize benefits. These results can help make decisions to maximize these benefits and inform designs.", "venue": "ArXiv", "authors": ["Anthony S. Walker", "Kyle E. Niemeyer"], "year": 2021, "n_citations": 0}
{"id": 3950546, "s2_id": "2dea6a76e000b3c6dbf9318938d3ee6046507ff3", "title": "Computer Algebra for Microhydrodynamics", "abstract": "I describe a method for computer algebra that helps with laborious calculations typically encountered in theoretical microhydrodynamics. The program mimics how humans calculate by matching patterns and making replacements according to the rules of algebra and calculus. This note gives an overview and walks through an example, while the accompanying code repository contains the implementation details, a tutorial, and more examples. The code repository is attached as supplementary material to this note, and maintained at this https URL", "venue": "ArXiv", "authors": ["Jonas  Einarsson"], "year": 2017, "n_citations": 1}
{"id": 3952608, "s2_id": "2e1b0997eee91186320a4d2b5565a023e42d330b", "title": "On the Different Shapes Arising in a Family of Rational Curves Depending on a Parameter", "abstract": "Given a family of rational curves depending on a real parameter, defined by its parametric equations, we provide an algorithm to compute a finite partition of the parameter space (${\\Bbb R}$, in general) so that the shape of the family stays invariant along each element of the partition. So, from this partition the topology types in the family can be determined. The algorithm is based on a geometric interpretation of previous work (\\cite{JGRS}) for the implicit case. However, in our case the algorithm works directly with the parametrization of the family, and the implicit equation does not need to be computed. Timings comparing the algorithm in the implicit and the parametric cases are given; these timings show that the parametric algorithm developed here provides in general better results than the known algorithm for the implicit case.", "venue": "ArXiv", "authors": ["Juan Gerardo Alc\u00e1zar"], "year": 2009, "n_citations": 0}
{"id": 3956114, "s2_id": "4407d119ba9682af2557514b609d56783bf392cd", "title": "LSRN: A Parallel Iterative Solver for Strongly Over- or Underdetermined Systems", "abstract": "We describe a parallel iterative least squares solver named LSRN that is based on random normal projection. LSRN computes the min-length solution to min x\u2208\u211d n \u2016Ax - b\u20162, where A \u2208 \u211d m \u00d7 n with m \u226b n or m \u226a n, and where A may be rank-deficient. Tikhonov regularization may also be included. Since A is involved only in matrix-matrix and matrix-vector multiplications, it can be a dense or sparse matrix or a linear operator, and LSRN automatically speeds up when A is sparse or a fast linear operator. The preconditioning phase consists of a random normal projection, which is embarrassingly parallel, and a singular value decomposition of size \u2308\u03b3 min(m, n)\u2309 \u00d7 min(m, n), where \u03b3 is moderately larger than 1, e.g., \u03b3 = 2. We prove that the preconditioned system is well-conditioned, with a strong concentration result on the extreme singular values, and hence that the number of iterations is fully predictable when we apply LSQR or the Chebyshev semi-iterative method. As we demonstrate, the Chebyshev method is particularly efficient for solving large problems on clusters with high communication cost. Numerical results show that on a shared-memory machine, LSRN is very competitive with LAPACK's DGELSD and a fast randomized least squares solver called Blendenpik on large dense problems, and it outperforms the least squares solver from SuiteSparseQR on sparse problems without sparsity patterns that can be exploited to reduce fill-in. Further experiments show that LSRN scales well on an Amazon Elastic Compute Cloud cluster.", "venue": "SIAM J. Sci. Comput.", "authors": ["Xiangrui  Meng", "Michael A. Saunders", "Michael W. Mahoney"], "year": 2014, "n_citations": 118}
{"id": 3962280, "s2_id": "d228d7fc601bbc2c2774f76c4074ae2a71ec5617", "title": "HeAT \u2013 a Distributed and GPU-accelerated Tensor Framework for Data Analytics", "abstract": "To cope with the rapid growth in available data, the efficiency of data analysis and machine learning libraries has recently received increased attention. Although great advancements have been made in traditional array-based computations, most are limited by the resources available on a single computation node. Consequently, novel approaches must be made to exploit distributed resources, e.g. distributed memory architectures. To this end, we introduce HeAT, an array-based numerical programming framework for large-scale parallel processing with an easy-to-use NumPy-like API. HeAT utilizes PyTorch as a node-local eager execution engine and distributes the workload on arbitrarily large high-performance computing systems via MPI. It provides both low-level array computations, as well as assorted higher-level algorithms. With HeAT, it is possible for a NumPy user to take full advantage of their available resources, significantly lowering the barrier to distributed data analysis. When compared to similar frameworks, HeAT achieves speedups of up to two orders of magnitude.", "venue": "2020 IEEE International Conference on Big Data (Big Data)", "authors": ["Markus  Goetz", "Charlotte  Debus", "Daniel  Coquelin", "Kai  Krajsek", "Claudia  Comito", "Philipp  Knechtges", "Bj\u00f6rn  Hagemeier", "Michael  Tarnawa", "Simon  Hanselmann", "Martin  Siggel", "Achim  Basermann", "Achim  Streit"], "year": 2020, "n_citations": 0}
{"id": 3964694, "s2_id": "f9692de1e3f63d4bf2bea0a8dfbf1b28b07f1b43", "title": "Rust-Bio: a fast and safe bioinformatics library", "abstract": "SUMMARY\nWe present Rust-Bio, the first general purpose bioinformatics library for the innovative Rust programming language. Rust-Bio leverages the unique combination of speed, memory safety and high-level syntax offered by Rust to provide a fast and safe set of bioinformatics algorithms and data structures with a focus on sequence analysis.\n\n\nAVAILABILITY AND IMPLEMENTATION\nRust-Bio is available open source under the MIT license at https://rust-bio.github.io.\n\n\nCONTACT\nkoester@jimmy.harvard.edu\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.", "venue": "Bioinform.", "authors": ["Johannes  K\u00f6ster"], "year": 2016, "n_citations": 10}
{"id": 3964966, "s2_id": "95a526e4d41f74c0808aeb771c1cd15f272116c3", "title": "Direct high-order edge-preserving regularization for tomographic image reconstruction", "abstract": "In this paper we present a new two-level iterative algorithm for tomographic image reconstruction. The algorithm uses a regularization technique, which we call edge-preserving Laplacian, that preserves sharp edges between objects while damping spurious oscillations in the areas where the reconstructed image is smooth. Our numerical simulations demonstrate that the proposed method outperforms total variation (TV) regularization and it is competitive with the combined TV-L2 penalty. Obtained reconstructed images show increased signal-to-noise ratio and visually appealing structural features. Computer implementation and parameter control of the proposed technique is straightforward, which increases the feasibility of it across many tomographic applications. In this paper, we applied our method to the under-sampled computed tomography (CT) projection data and also considered a case of reconstruction in emission tomography The MATLAB code is provided to support obtained results.", "venue": "ArXiv", "authors": ["Daniil  Kazantsev", "Evgueni  Ovtchinnikov", "William R. B. Lionheart", "Philip J. Withers", "Peter D. Lee"], "year": 2015, "n_citations": 0}
{"id": 3965008, "s2_id": "2fc6fb6bd73401ecfac27fd213e23b329453e8a0", "title": "volesti: Volume Approximation and Sampling for Convex Polytopes in R", "abstract": "Sampling from high dimensional distributions and volume approximation of convex bodies are fundamental operations that appear in optimization, finance, engineering and machine learning. In this paper we present volesti, a C++ package with an R interface that provides efficient, scalable algorithms for volume estimation, uniform and Gaussian sampling from convex polytopes. volesti scales to hundreds of dimensions, handles efficiently three different types of polyhedra and provides non existing sampling routines to R. We demonstrate the power of volesti by solving several challenging problems using the R language.", "venue": "ArXiv", "authors": ["Apostolos  Chalkis", "Vissarion  Fisikopoulos"], "year": 2020, "n_citations": 4}
{"id": 3972316, "s2_id": "032c26b35a4c922b5943f03466bd373a2300b61d", "title": "Neko: A Modern, Portable, and Scalable Framework for High-Fidelity Computational Fluid Dynamics", "abstract": "Recent trends and advancement in including more diverse and heterogeneous hardware in High-Performance Computing is challenging software developers in their pursuit for good performance and numerical stability. The well-known maxim \"software outlives hardware\" may no longer necessarily hold true, and developers are today forced to re-factor their codebases to leverage these powerful new systems. CFD is one of the many application domains a!ected. In this paper, we present Neko, a portable framework for high-order spectral element \"ow simulations. Unlike prior works, Neko adopts a modern object-oriented approach, allowingmulti-tier abstractions of the solver stack and facilitating hardware backends ranging from general-purpose processors down to exotic vector processors and FPGAs. We show that Neko\u2019s performance and accuracy are comparable to NekRS, and thus on-par with Nek5000\u2019s successor on modern CPU machines. Furthermore, we develop a performance model, which we use to discuss challenges and opportunities for high-order solvers on emerging hardware.", "venue": "ArXiv", "authors": ["Niclas  Jansson", "Martin  Karp", "Artur  Podobas", "Stefano  Markidis", "Philipp  Schlatter"], "year": 2021, "n_citations": 2}
{"id": 3975082, "s2_id": "56da0b32d55d4a16ee748002e758ca2b57ade79e", "title": "A deterministic global optimization using smooth diagonal auxiliary functions", "abstract": "In many practical decision-making problems it happens that functions involved in optimization process are black-box with unknown analytical representations and hard to evaluate. In this paper, a global optimization problem is considered where both the goal function f (x) and its gradient f \ufffd (x )a re black-box functions. It is supposed that f \ufffd (x) satisfies the Lipschitz condition over the search hyperinterval with an unknown Lipschitz constant K. A new deterministic \u2018Divide-the-Best\u2019 algorithm based on efficient diagonal partitions and smooth auxiliary functions is proposed in its basic version, its convergence conditions are studied and numerical experiments executed on eight hundred test functions are presented.", "venue": "Commun. Nonlinear Sci. Numer. Simul.", "authors": ["Yaroslav D. Sergeyev", "Dmitri E. Kvasov"], "year": 2015, "n_citations": 60}
{"id": 3977777, "s2_id": "944c78bbf27442ade541ec25131ba75cb3f752ca", "title": "preCICE v2: A Sustainable and User-Friendly Coupling Library", "abstract": "preCICE is a free/open-source coupling library. It enables creating partitioned multi-physics simulations by gluing together separate software packages. This paper summarizes the development efforts in preCICE of the past five years. During this time span, we have turned the software from a working prototype \u2013 sophisticated numerical coupling methods and scalability on ten thousands of compute cores \u2013 to a sustainable and user-friendly software project with a steadily-growing community. Today, we know through forum discussions, conferences, workshops, and publications of more than 100 research groups using preCICE. We cover the fundamentals of the software alongside a performance and accuracy analysis of different data mapping methods. Afterwards, we describe ready-to-use integration with widely-used external simulation software packages, tests and continuous integration from unit to system level, and community building measures, drawing an overview of the current preCICE ecosystem.", "venue": "ArXiv", "authors": ["Gerasimos  Chourdakis", "Kyle  Davis", "Benjamin  Rodenberg", "Miriam  Schulte", "Fr'ed'eric  Simonis", "Benjamin  Uekermann", "Georg  Abrams", "Hans-Joachim  Bungartz", "Lucia Cheung Yau", "Ishaan  Desai", "Konrad  Eder", "Richard  Hertrich", "Florian  Lindner", "Alexander  Rusch", "Dmytro  Sashko", "David  Schneider", "Amin  Totounferoush", "Dominik  Volland", "Peter  Vollmer", "Oguz Ziya Koseomur"], "year": 2021, "n_citations": 0}
{"id": 3983305, "s2_id": "abc69f7e127e27a3cf5821d23bccc1bc7038f5c6", "title": "Formal Verification of Medina's Sequence of Polynomials for Approximating Arctangent", "abstract": "The verification of many algorithms for calculating transcendental functions is based on polynomial approximations to these functions, often Taylor series approximations. However, computing and verifying approximations to the arctangent function are very challenging problems, in large part because the Taylor series converges very slowly to arctangent-a 57th-degree polynomial is needed to get three decimal places for arctan(0.95). Medina proposed a series of polynomials that approximate arctangent with far faster convergence-a 7th-degree polynomial is all that is needed to get three decimal places for arctan(0.95). We present in this paper a proof in ACL2(r) of the correctness and convergence rate of this sequence of polynomials. The proof is particularly beautiful, in that it uses many results from real analysis. Some of these necessary results were proven in prior work, but some were proven as part of this effort.", "venue": "ACL2", "authors": ["Ruben  Gamboa", "John R. Cowles"], "year": 2014, "n_citations": 3}
{"id": 3991937, "s2_id": "dc88388e5277c8929faf19021538e8583273dd07", "title": "Multithreaded Filtering Preconditioner for Diffusion Equation on Structured Grid", "abstract": "A parallel and nested version of a frequency filtering preconditioner is proposed for linear systems corresponding to diffusion equation on a structured grid. The proposed preconditioner is found to be robust with respect to jumps in the diffusion coefficients. The storage requirement for the preconditioner is O(N),where N is number of rows of matrix, hence, a fairly large problem of size more than 42 million unknowns has been solved on a quad core machine with 64GB RAM. The parallelism is achieved using twisted factorization and SIMD operations. The preconditioner achieves a speedup of 3.3 times on a quad core processor clocked at 4.2 GHz, and compared to a well known algebraic multigrid method, it is significantly faster in both setup and solve times for diffusion equations with jumps.", "venue": "ArXiv", "authors": ["Abhinav  Aggarwal", "Shivam  Kakkar", "Pawan  Kumar"], "year": 2019, "n_citations": 0}
{"id": 3994262, "s2_id": "0505b6f1e5acb2182462e83d88253f061f7e48f5", "title": "FDBB: Fluid Dynamics Building Blocks", "abstract": "High-performance computing platforms are becoming more and more heterogeneous, which makes it very difficult for researchers and scientific software developers to keep up with the rapid changes on the hardware market. In this paper, the open-source project FDBB (Fluid Dynamics Building Blocks) is presented, which eases the development of fluid dynamics applications for heterogeneous systems. It consists of a low-level API that provides a unified interface to many different linear algebra back-ends and a lightweight and extendible high-level expression template library, which provides largely customizable fluid dynamics building blocks, like transformations between primary and secondary variables as well as expressions for Riemann invariants, equations of state, inviscid fluxes and their flux-Jacobians. The performance of the developed approach is assessed both for synthetic micro-benchmarks and within mini-applications.", "venue": "ArXiv", "authors": ["Matthias  M\u00f6ller", "Andrzej  Jaeschke"], "year": 2018, "n_citations": 0}
{"id": 3998082, "s2_id": "20bb074d51a6e5a48b7dd4a2d5c4cf119a34ac7e", "title": "Implementing Cryptographic Pairings at Standard Security Levels", "abstract": "This study reports on an implementation of cryptographic pairings in a general purpose computer algebra system. For security levels equivalent to the different AES flavours, we exhibit suitable curves in parametric families and show that optimal ate and twisted ate pairings exist and can be efficiently evaluated. We provide a correct description of Miller's algorithm for signed binary expansions such as the NAF and extend a recent variant due to Boxall et al. to addition-subtraction chains. We analyse and compare several algorithms proposed in the literature for the final exponentiation. Finally, we ive recommendations on which curve and pairing to choose at each security level.", "venue": "SPACE", "authors": ["Andreas  Enge", "J\u00e9r\u00f4me  Milan"], "year": 2014, "n_citations": 10}
{"id": 4003453, "s2_id": "3ef6bbf57b3d77b98f75cca0790d09e295219df0", "title": "A Massively Parallel Time-Domain Coupled Electrodynamics-Micromagnetics Solver", "abstract": "We present a new, high-performance coupled electrodynamics-micromagnetics solver for full physical modeling of signals in microelectronic circuitry. The overall strategy couples a finite-difference time-domain (FDTD) approach for Maxwell\u2019s equations to a magnetization model described by the Landau-Lifshitz-Gilbert (LLG) equation. The algorithm is implemented in the Exascale Computing Project software framework, AMReX, which provides effective scalability on manycore and GPU-based supercomputing architectures. Furthermore, the code leverages ongoing developments of the Exascale Application Code, WarpX, primarily developed for plasma wakefield accelerator modeling. Our novel temporal coupling scheme provides second-order accuracy in space and time by combining the integration steps for the magnetic field and magnetization into an iterative sub-step that includes a trapezoidal discretization for the magnetization. The performance of the algorithm is demonstrated by the excellent scaling results on NERSC multicore and GPU systems, with a significant (59x) speedup on the GPU using a node-by-node comparison. We demonstrate the utility of our code by performing simulations of an electromagnetic waveguide and a magnetically tunable filter.", "venue": "ArXiv", "authors": ["Zhi  Yao", "Revathi  Jambunathan", "Yadong  Zeng", "Andrew  Nonaka"], "year": 2021, "n_citations": 0}
{"id": 4009505, "s2_id": "9041682f3bee1e9d822480280cd7a07eb8d81d66", "title": "Reproducibility of parallel preconditioned conjugate gradient in hybrid programming environments", "abstract": "The Preconditioned Conjugate Gradient method is often employed for the solution of linear systems of equations arising in numerical simulations of physical phenomena. While being widely used, the solver is also known for its lack of accuracy while computing the residual. In this article, we propose two algorithmic solutions that originate from the ExBLAS project to enhance the accuracy of the solver as well as to ensure its reproducibility in a hybrid MPI + OpenMP tasks programming environment. One is based on ExBLAS and preserves every bit of information until the final rounding, while the other relies upon floating-point expansions and, hence, expands the intermediate precision. Instead of converting the entire solver into its ExBLAS-related implementation, we identify those parts that violate reproducibility/non-associativity, secure them, and combine this with the sequential executions. These algorithmic strategies are reinforced with programmability suggestions to assure deterministic executions. Finally, we verify these approaches on two modern HPC systems: both versions deliver reproducible number of iterations, residuals, direct errors, and vector-solutions for the overhead of less than 37.7% on 768 cores.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Roman  Iakymchuk", "Maria  Barreda", "Stef  Graillat", "Jos\u00e9 Ignacio Aliaga", "Enrique S. Quintana-Ort\u00ed"], "year": 2020, "n_citations": 1}
{"id": 4009872, "s2_id": "c4bde15f96fba8c20117695dc96062b2271795e5", "title": "A Density Matrix-based Algorithm for Solving Eigenvalue Problems", "abstract": "A new numerical algorithm for solving the symmetric eigenvalue problem is presented. The technique deviates fundamentally from the traditional Krylov subspace iteration based techniques (Arnoldi and Lanczos algorithms) or other Davidson-Jacobi techniques, and takes its inspiration from the contour integration and density matrix representation in quantum mechanics. It will be shown that this new algorithm - named FEAST - exhibits high efficiency, robustness, accuracy and scalability on parallel architectures. Examples from electronic structure calculations of Carbon nanotubes (CNT) are presented, and numerical performances and capabilities are discussed.", "venue": "ArXiv", "authors": ["Eric  Polizzi"], "year": 2009, "n_citations": 289}
{"id": 4012732, "s2_id": "a1bd67a9e042f8b64decc4a3f011a4f91a1fa791", "title": "Computing isomorphisms and embeddings of finite fields", "abstract": "Let <i>q</i> be a prime power and let F<sub><i>q</i></sub> be a field with <i>q</i> elements. Let <i>f</i> and <i>g</i> be irreducible polynomials in F<sub><i>q</i></sub>[<i>X</i>], with deg <i>f</i> dividing deg <i>g.</i> Define <i>k</i> = F<sub><i>q</i></sub>[<i>X</i>]/<i>f</i> and <i>K</i> = F<sub><i>q</i></sub>[<i>X</i>]/<i>g</i>, then there is an embedding <i>\u03c6</i> : <i>k</i> [EQUATION] <i>K</i>, unique up to F<sub><i>q</i></sub>-automorphisms of <i>k.</i> Our goal is to describe algorithms to efficiently represent and evaluate one such embedding.", "venue": "ACCA", "authors": ["Ludovic  Brieulle", "Luca De Feo", "Javad  Doliskani", "Jean-Pierre  Flori", "\u00c9ric  Schost"], "year": 2019, "n_citations": 0}
{"id": 4015746, "s2_id": "628f00875a832d69047e62ead06eaa2ee9a89f08", "title": "Elmer FEM-Dakota: A unified open-source computational framework for electromagnetics and data analytics", "abstract": "Open-source electromagnetic design software, Elmer FEM, was interfaced with data analytics toolkit, Dakota. Furthermore, the coupled software was validated against a benchmark test. The interface developed provides a unified open-source computational framework for electromagnetics and data analytics. Its key features include uncertainty quantification, surrogate modelling and parameter studies. This framework enables a richer understanding of model predictions to better design electric machines in a time sensitive manner.", "venue": "ArXiv", "authors": ["Anjali  Sandip"], "year": 2020, "n_citations": 0}
{"id": 4017830, "s2_id": "a5586576e0e574141b7fb270ca9411e2dd5293b6", "title": "pyMOR - Generic Algorithms and Interfaces for Model Order Reduction", "abstract": "Reduced basis methods are projection-based model order reduction techniques for reducing the computational complexity of solving parametrized partial differential equation problems. In this work we discuss the design of pyMOR, a freely available software library of model order reduction algorithms, in particular reduced basis methods, implemented with the Python programming language. As its main design feature, all reduction algorithms in pyMOR are implemented generically via operations on well-defined vector array, operator and discretization interface classes. This allows for an easy integration with existing open-source high-performance partial differential equation solvers without adding any model reduction specific code to these solvers. Besides an in-depth discussion of pyMOR's design philosophy and architecture, we present several benchmark results and numerical examples showing the feasibility of our approach.", "venue": "SIAM J. Sci. Comput.", "authors": ["Ren\u00e9  Milk", "Stephan  Rave", "Felix  Schindler"], "year": 2016, "n_citations": 45}
{"id": 4018324, "s2_id": "dc84fc413889502ab677909e260fe9bc18ea293e", "title": "On Two Infinite Families of Pairing Bijections", "abstract": "We describe two general mechanisms for producing pairing bijections (bijective functions defined from N x N to N). \nThe first mechanism, using n-adic valuations results in parameterized algorithms generating a countable family of distinct pairing bijections. \nThe second mechanism, using characteristic functions of subsets of N provides 2^N distinct pairing bijections. \nMechanisms to combine such pairing functions and their application to generate families of permutations of N are also described. \nThe paper uses a small subset of the functional language Haskell to provide type checked executable specifications of all the functions defined in a literate programming style. The self-contained Haskell code extracted from the paper is available at this http URL .", "venue": "ArXiv", "authors": ["Paul  Tarau"], "year": 2013, "n_citations": 1}
{"id": 4019438, "s2_id": "e07a1e34c15c975f1715b84ae49956f2c8feb9c8", "title": "TeXmacs interfaces to Maxima, MuPAD and REDUCE", "abstract": "GNU TeXmacs is a free wysiwyg word processor providing an excellent typesetting quality of texts and formulae. It can also be used as an interface to Computer Algebra Systems (CASs). In the present work, interfaces to three general-purpose CASs have been implemented.", "venue": "ArXiv", "authors": ["A. G. Grozin"], "year": 2001, "n_citations": 6}
{"id": 4020506, "s2_id": "780d8e04e8ae446169db326d4f2ffdfc56fe1bf5", "title": "ATC: an Advanced Tucker Compression library for multidimensional data", "abstract": "We present ATC, a C++ library for advanced Tucker-based compression of multidimensional numerical data, based on the sequentially truncated higher-order singular value decomposition (ST-HOSVD) and bit plane truncation. Several techniques are proposed to improve compression rate, speed, memory usage and error control. First, a hybrid truncation scheme is described which combines Tucker rank truncation and TTHRESH quantization [Ballester-Ripoll et al., IEEE Trans. Visual. Comput. Graph., 2020]. We derive a novel expression to approximate the error of truncated Tucker decompositions in the case of core and factor perturbations. Furthermore, a Householder-reflector-based approach is proposed to compress the orthogonal Tucker factors. Certain key improvements to the quantization procedure are also discussed. Moreover, particular implementation aspects are described, such as ST-HOSVD procedure using only a single transposition. We also discuss several usability features of ATC, including the presence of multiple interfaces, extensive data type support and integrated downsampling of the decompressed data. Numerical results show that ATC maintains state-of-the-art Tucker compression rates, while providing average speed-ups of 2.6-3.6 and halving memory usage. Furthermore, our compressor provides precise error control, only deviating 1.4% from the requested error on average. Finally, ATC often achieves significantly higher compression than non-Tucker-based compressors in the high-error domain.", "venue": "ArXiv", "authors": ["Wouter  Baert", "Nick  Vannieuwenhoven"], "year": 2021, "n_citations": 0}
{"id": 4025621, "s2_id": "e331636ce642d84f18a241a501484f026e217892", "title": "Revisiting the Upper Bounding Process in a Safe Branch and Bound Algorithm", "abstract": "Finding feasible points for which the proof succeeds is a critical issue in safe Branch and Bound algorithms which handle continuous problems. In this paper, we introduce a new strategy to compute very accurate approximations of feasible points. This strategy takes advantage of the Newton method for under-constrained systems of equations and inequalities. More precisely, it exploits the optimal solution of a linear relaxation of the problem to compute efficiently a promising upper bound. First experiments on the Coconuts benchmarks demonstrate that this approach is very effective.", "venue": "CP", "authors": ["Alexandre  Goldsztejn", "Yahia  Lebbah", "Claude  Michel", "Michel  Rueher"], "year": 2008, "n_citations": 9}
{"id": 4036221, "s2_id": "90e6c12a7ce33d19c3303df11cbe9df0b2403844", "title": "Evaluation of the Intel Xeon Phi 7120 and NVIDIA K80 as accelerators for two-dimensional panel codes", "abstract": "To optimize the geometry of airfoils for a specific application is an important engineering problem. In this context genetic algorithms have enjoyed some success as they are able to explore the search space without getting stuck in local optima. However, these algorithms require the computation of aerodynamic properties for a significant number of airfoil geometries. Consequently, for low-speed aerodynamics, panel methods are most often used as the inner solver. In this paper we evaluate the performance of such an optimization algorithm on modern accelerators (more specifically, the Intel Xeon Phi 7120 and the NVIDIA K80). For that purpose, we have implemented an optimized version of the algorithm on the CPU and Xeon Phi (based on OpenMP, vectorization, and the Intel MKL library) and on the GPU (based on CUDA and the MAGMA library). We present timing results for all codes and discuss the similarities and differences between the three implementations. Overall, we observe a speedup of approximately 2.5 for adding an Intel Xeon Phi 7120 to a dual socket workstation and a speedup between 3.4 and 3.8 for adding a NVIDIA K80 to a dual socket workstation.", "venue": "PloS one", "authors": ["Lukas  Einkemmer"], "year": 2017, "n_citations": 6}
{"id": 4039683, "s2_id": "5672d49f94d3ed9c233c699c35c5026524da5d12", "title": "Sphynx: a parallel multi-GPU graph partitioner for distributed-memory systems", "abstract": "Abstract Graph partitioning has been an important tool to partition the work among several processors to minimize the communication cost and balance the workload. While accelerator-based supercomputers are emerging to be the standard, the use of graph partitioning becomes even more important as applications are rapidly moving to these architectures. However, there is no distributed-memory-parallel, multi-GPU graph partitioner available for applications. We developed a spectral graph partitioner, Sphynx, using the portable, accelerator-friendly stack of the Trilinos framework. In Sphynx, we allow using different preconditioners and exploit their unique advantages. We use Sphynx to systematically evaluate the various algorithmic choices in spectral partitioning with a focus on the GPU performance. We perform those evaluations on two distinct classes of graphs: regular (such as meshes, matrices from finite element methods) and irregular (such as social networks and web graphs), and show that different settings and preconditioners are needed for these graph classes. The experimental results on the Summit supercomputer show that Sphynx is the fastest alternative on irregular graphs in an application-friendly setting and obtains a partitioning quality close to ParMETIS on regular graphs. When compared to nvGRAPH on a single GPU, Sphynx is faster and obtains better balance and better quality partitions. Sphynx provides a good and robust partitioning method across a wide range of graphs for applications looking for a GPU-based partitioner.", "venue": "Parallel Comput.", "authors": ["Seher  Acer", "Erik G Boman", "Christian A Glusa", "Sivasankaran  Rajamanickam"], "year": 2021, "n_citations": 2}
{"id": 4046765, "s2_id": "238b807d0633a58de13a3279dd02f37433ae6315", "title": "Straightforward Bibliography Management in R with the RefManageR Package", "abstract": "This work introduces the R package RefManageR, which provides tools for importing and working with bibliographic references. It extends the bibentry class in R in a number of useful ways, including providing R with previously unavailable support for BibLaTeX. BibLaTeX provides a superset of the functionality of BibTeX, including full Unicode support, no memory limitations, additional fields and entry types, and more sophisticated sorting of references. RefManageR provides functions for citing and generating a bibliography with hyperlinks for documents prepared with RMarkdown or RHTML. Existing .bib files can be read into R and converted from BibTeX to BibLaTeX and vice versa. References can also be imported via queries to NCBI's Entrez, Zotero libraries, Google Scholar, and CrossRef. Additionally, references can be created by reading PDFs stored on the user's machine with the help of Poppler. Entries stored in the reference manager can be easily searched by any field, by date ranges, and by various formats for name lists (author by last names, translator by full names, etc.). Entries can also be updated, combined, sorted, printed in a number of styles, and exported.", "venue": "ArXiv", "authors": ["Mathew W. McLean"], "year": 2014, "n_citations": 2}
{"id": 4051084, "s2_id": "1c09084e2bac63d5de2c8cbec8d36776083e5674", "title": "Distributed dynamic load balancing for task parallel programming", "abstract": "In this paper, we derive and investigate approaches to dynamically load balance a distributed task parallel application software. The load balancing strategy is based on task migration. Busy processes export parts of their ready task queue to idle processes. Idle--busy pairs of processes find each other through a random search process that succeeds within a few steps with high probability. We evaluate the load balancing approach for a block Cholesky factorization implementation and observe a reduction in execution time on the order of 5\\% in the selected test cases.", "venue": "ArXiv", "authors": ["Afshin  Zafari", "Elisabeth  Larsson"], "year": 2018, "n_citations": 1}
{"id": 4051278, "s2_id": "f5fbcd9ff72c5820a21b9d6871d2a3d475c9bb7f", "title": "CatBoost: gradient boosting with categorical features support", "abstract": "In this paper we present CatBoost, a new open-sourced gradient boosting library that successfully handles categorical features and outperforms existing publicly available implementations of gradient boosting in terms of quality on a set of popular publicly available datasets. The library has a GPU implementation of learning algorithm and a CPU implementation of scoring algorithm, which are significantly faster than other gradient boosting libraries on ensembles of similar sizes.", "venue": "ArXiv", "authors": ["Anna Veronika Dorogush", "Vasily  Ershov", "Andrey  Gulin"], "year": 2018, "n_citations": 327}
{"id": 4052012, "s2_id": "67a67a0e0e85cf588dbc06f0f8306c5ed99bd92f", "title": "Confederated Modular Differential Equation APIs for Accelerated Algorithm Development and Benchmarking", "abstract": "Performant numerical solving of differential equations is required for large-scale scientific modeling. In this manuscript we focus on two questions: (1) how can researchers empirically verify theoretical advances and consistently compare methods in production software settings and (2) how can users (scientific domain experts) keep up with the state-of-the-art methods to select those which are most appropriate? Here we describe how the confederated modular API of DifferentialEquations.jl addresses these concerns. We detail the package-free API which allows numerical methods researchers to readily utilize and benchmark any compatible method directly in full-scale scientific applications. In addition, we describe how the complexity of the method choices is abstracted via a polyalgorithm. We show how scientific tooling built on top of DifferentialEquations.jl, such as packages for dynamical systems quantification and quantum optics simulation, both benefit from this structure and provide themselves as convenient benchmarking tools.", "venue": "Adv. Eng. Softw.", "authors": ["Christopher  Rackauckas", "Qing  Nie"], "year": 2019, "n_citations": 9}
{"id": 4053465, "s2_id": "b478f8d51a98528b61b5e4260f1e543021979392", "title": "A model-driven approach for a new generation of adaptive libraries", "abstract": "Efficient high-performance libraries often expose multiple tunable parameters to provide highly optimized routines. These can range from simple loop unroll factors or vector sizes all the way to algorithmic changes, given that some implementations can be more suitable for certain devices by exploiting hardware characteristics such as local memories and vector units. Traditionally, such parameters and algorithmic choices are tuned and then hard-coded for a specific architecture and for certain characteristics of the inputs. However, emerging applications are often data-driven, thus traditional approaches are not effective across the wide range of inputs and architectures used in practice. In this paper, we present a new adaptive framework for data-driven applications which uses a predictive model to select the optimal algorithmic parameters by training with synthetic and real datasets. We demonstrate the effectiveness of a BLAS library and specifically on its matrix multiplication routine. We present experimental results for two GPU architectures and show significant performance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an embedded ARM Mali GPU) when compared to a traditionally optimized library.", "venue": "ArXiv", "authors": ["Marco  Cianfriglia", "Flavio  Vella", "Cedric  Nugteren", "Anton  Lokhmotov", "Grigori  Fursin"], "year": 2018, "n_citations": 5}
{"id": 4056200, "s2_id": "93fdde114aa46ffc517cfb3d8af199625ca75164", "title": "A randomized Halton algorithm in R", "abstract": "Randomized quasi-Monte Carlo (RQMC) sampling can bring orders of magnitude reduction in variance compared to plain Monte Carlo (MC) sampling. The extent of the efficiency gain varies from problem to problem and can be hard to predict. This article presents an R function rhalton that produces scrambled versions of Halton sequences. On some problems it brings efficiency gains of several thousand fold. On other problems, the efficiency gain is minor. The code is designed to make it easy to determine whether a given integrand will benefit from RQMC sampling. An RQMC sample of n points in $[0,1]^d$ can be extended later to a larger n and/or d.", "venue": "ArXiv", "authors": ["Art B. Owen"], "year": 2017, "n_citations": 10}
{"id": 4064758, "s2_id": "2bc5e5550ec783e6cc35f365517b70f504862ec3", "title": "Replicated Computational Results (RCR) Report for \u201cCode Generation for Generally Mapped Finite Elements\u201d", "abstract": "\u201cCode Generation for Generally Mapped Finite Elements\u201d includes performance results for the finite element methods discussed in that manuscript. The authors provided a Zenodo archive with the Firedrake components and dependencies used, as well as the scripts that generated the results. The software was installed on two similar platforms; then, new results were gathered and compared to the original results. After completing this process, the results have been deemed replicable by the reviewer.", "venue": "ACM Trans. Math. Softw.", "authors": ["Neil  Lindquist"], "year": 2019, "n_citations": 0}
{"id": 4065362, "s2_id": "0bd6f2de71a013a54fed4de69f19784d1290ccdf", "title": "PFASST-ER: combining the parallel full approximation scheme in space and time with parallelization across the method", "abstract": "To extend prevailing scaling limits when solving time-dependent partial differential equations, the parallel full approximation scheme in space and time (PFASST) has been shown to be a promising parallel-in-time integrator. Similar to space\u2013time multigrid, PFASST is able to compute multiple time-steps simultaneously and is therefore in particular suitable for large-scale applications on high performance computing systems. In this work we couple PFASST with a parallel spectral deferred correction (SDC) method, forming an unprecedented doubly time-parallel integrator. While PFASST provides global, large-scale \u201cparallelization across the step\u201d, the inner parallel SDC method allows integrating each individual time-step \u201cparallel across the method\u201d using a diagonalized local Quasi-Newton solver. This new method, which we call \u201cPFASST with Enhanced concuRrency\u201d (PFASST-ER), therefore exposes even more temporal concurrency. For two challenging nonlinear reaction-diffusion problems, we show that PFASST-ER works more efficiently than the classical variants of PFASST and can use more processors than time-steps.", "venue": "ArXiv", "authors": ["Ruth  Sch\u00f6bel", "Robert  Speck"], "year": 2019, "n_citations": 4}
{"id": 4066437, "s2_id": "055f8249f26a56fda875d017b256ec618201eaa7", "title": "Compiling Diderot: From Tensor Calculus to C", "abstract": "Diderot is a parallel domain-specific language for analysis and visualization of multidimensional scientific images, such as those produced by CT and MRI scanners. In particular, it supports algorithms where tensor fields (i.e., functions from 3D points to tensor values) are used to represent the underlying physical objects that were scanned by the imaging device. Diderot supports higher-order programming where tensor fields are first-class values and where differential operators and lifted linear-algebra operators can be used to express mathematical reasoning directly in the language. While such lifted field operations are central to the definition and computation of many scientific visualization algorithms, to date they have required extensive manual derivations and laborious implementation. \nThe challenge for the Diderot compiler is to effectively translate the high-level mathematical concepts that are expressible in the surface language to a low-level and efficient implementation in C. This paper describes our approach to this challenge, which is based around the careful design of an intermediate representation (IR), called EIN, and a number of compiler transformations that lower the program from tensor calculus to C while avoiding combinatorial explosion in the size of the IR. We describe the challenges in compiling a language like Diderot, the design of EIN, and the transformation used by the compiler. We also present an evaluation of EIN with respect to both compiler efficiency and quality of generated code.", "venue": "ArXiv", "authors": ["Charisee  Chiw", "Gordon L. Kindlmann", "John H. Reppy"], "year": 2018, "n_citations": 2}
{"id": 4076383, "s2_id": "38618db2a49e883fca8fed698b13ee7cb31b387b", "title": "Universal algorithms, mathematics of semirings and parallel computations", "abstract": "This isaut]Grigory L. Litvinovaut]Victor P. Maslovaut]Anatoly Ya. Rodionovaut]Andrei N. Sobolevskii a survey paper on applications of mathematics of semirings to numerical analysis and computing. Concepts of universal algorithm and generic program are discussed. Relations between these concepts and mathematics of semirings are examined. A very brief introduction to mathematics of semirings (including idempotent and tropical mathematics) is presented. Concrete applications to optimization problems, idempotent linear algebra and interval analysis are indicated. It is known that some nonlinear problems (and especially optimization problems) become linear over appropriate semirings with idempotent addition (the so-called idempotent superposition principle). This linearity over semirings is convenient for parallel computations.", "venue": "ArXiv", "authors": ["Grigori L. Litvinov", "Victor P. Maslov", "A. Ya. Rodionov", "Andrei N. Sobolevski"], "year": 2010, "n_citations": 12}
{"id": 4077942, "s2_id": "9718ec5986e9fc11f513b9ad175a2605277a282c", "title": "On Computing the Hermite Form of a Matrix of Differential Polynomials", "abstract": "Given a matrix over the ring of differential polynomials, we show how to compute the Hermite form H of A and a unimodular matrix U such that UA = H . The algorithm requires a polynomial number of operations in F in terms of n , , . When F = *** it require time polynomial in the bit-length of the rational coefficients as well.", "venue": "CASC", "authors": ["Mark  Giesbrecht", "Myung Sub Kim"], "year": 2009, "n_citations": 12}
{"id": 4079878, "s2_id": "10951aed140d2f669cca48042cad3b81fb76e6d6", "title": "Vectorization and Minimization of Memory Footprint for Linear High-Order Discontinuous Galerkin Schemes", "abstract": "We present a sequence of optimizations to the performance-critical compute kernels of the high-order discontinuous Galerkin solver of the hyperbolic PDE engine ExaHyPE \u2013 successively tackling bottlenecks due to SIMD operations, cache hierarchies and restrictions in the software design.Starting from a generic scalar implementation of the numerical scheme, our first optimized variant applies state-of the-art optimization techniques by vectorizing loops, improving the data layout and using Loop-over-GEMM to perform tensor contractions via highly optimized matrix multiplication functions provided by the LIBXSMM library. We show that memory stalls due to a memory footprint exceeding our L2 cache size hindered the vectorization gains. We therefore introduce a new kernel that applies a sum factorization approach to reduce the kernel\u2019s memory footprint and improve its cache locality. With the L2 cache bottleneck removed, we were able to exploit additional vectorization opportunities, by introducing a hybrid Array-of-Structure-of-Array data layout that solves the data layout conflict between matrix multiplications kernels and the point-wise functions to implement PDE-specific terms.With this last kernel, evaluated in a benchmark simulation at high polynomial order, only 2% of the floating point operations are still performed using scalar instructions and 22.5% of the available performance is achieved.", "venue": "2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Jean-Matthieu  Gallard", "Leonhard  Rannabauer", "Anne  Reinarz", "Michael  Bader"], "year": 2020, "n_citations": 0}
{"id": 4085070, "s2_id": "59dea61a471aea0a2bcf827e006e530fb3dc43a3", "title": "Stochastic Runge-Kutta Software Package for Stochastic Differential Equations", "abstract": "As a result of the application of a technique of multistep processes stochastic models construction the range of models, implemented as a self-consistent differential equations, was obtained. These are partial differential equations (master equation, the Fokker--Planck equation) and stochastic differential equations (Langevin equation). However, analytical methods do not always allow to research these equations adequately. It is proposed to use the combined analytical and numerical approach studying these equations. For this purpose the numerical part is realized within the framework of symbolic computation. It is recommended to apply stochastic Runge--Kutta methods for numerical study of stochastic differential equations in the form of the Langevin. Under this approach, a program complex on the basis of analytical calculations metasystem Sage is developed. For model verification logarithmic walks and Black--Scholes two-dimensional model are used. To illustrate the stochastic \"predator--prey\" type model is used. The utility of the combined numerical-analytical approach is demonstrated.", "venue": "DepCoS-RELCOMEX", "authors": ["M. N. Gevorkyan", "Tatiana R. Velieva", "Anna V. Korolkova", "Dmitry S. Kulyabov", "Leonid A. Sevastyanov"], "year": 2016, "n_citations": 15}
{"id": 4092856, "s2_id": "1389564b7e36e9ea9774f646599aa3dab0c7352e", "title": "Formalization and Implementation of Algebraic Methods in Geometry", "abstract": "We describe our ongoing project of formalization of algebraic methods for geometry theorem proving (Wu's method and the Groebner bases method), their implementation and integration in educational tools. The project includes formal verification of the algebraic methods within Isabelle/HOL proof assistant and development of a new, open-source Java implementation of the algebraic methods. The project should fill-in some gaps still existing in this area (e.g., the lack of formal links between algebraic methods and synthetic geometry and the lack of self-contained implementations of algebraic methods suitable for integration with dynamic geometry tools) and should enable new applications of theorem proving in education.", "venue": "ThEdu", "authors": ["Filip  Maric", "Ivan  Petrovic", "Danijela  Petrovic", "Predrag  Janicic"], "year": 2011, "n_citations": 17}
{"id": 4094966, "s2_id": "2fee34be6eb9be0215ee1d730cdb8521275262db", "title": "A Bernstein Polynomial Collocation Method for the Solution of Elliptic Boundary Value Problems", "abstract": "In this article, a formulation of a point-collocation method in which the unknown function is approximated using global expansion in tensor product Bernstein polynomial basis is presented. Bernstein polynomials used in this study are defined over general interval [a,b]. Method incorporates several ideas that enable higher numerical efficiency compared to Bernstein polynomial methods that have been previously presented. The approach is illustrated by a solution of Poisson, Helmholtz and Biharmonic equations with Dirichlet and Neumann type boundary conditions. Comparisons with analytical solutions are given to demonstrate the accuracy and convergence properties of the current procedure. The method is implemented in an open-source code, and a library for manipulation of Bernstein polynomials bernstein-poly, developed by the authors.", "venue": "ArXiv", "authors": ["Nikola  Mirkov", "Bosko  Rasuo"], "year": 2012, "n_citations": 7}
{"id": 4103570, "s2_id": "a2fcf53f0aef0bfaec6353676c4f1d4e36aab5c0", "title": "scikit-image: image processing in Python", "abstract": "scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.", "venue": "PeerJ", "authors": ["St\u00e9fan van der Walt", "Johannes L. Sch\u00f6nberger", "Juan  Nunez-Iglesias", "Fran\u00e7ois  Boulogne", "Joshua D. Warner", "Neil  Yager", "Emmanuelle  Gouillart", "Tony  Yu"], "year": 2014, "n_citations": 2334}
{"id": 4108800, "s2_id": "ff34cfb87a032edc2855266f3c9940b81cc13f1d", "title": "PetRBF--A parallel O(N) algorithm for radial basis function interpolation", "abstract": "Abstract We have developed a parallel algorithm for radial basis function ( rbf ) interpolation that exhibits O ( N ) complexity, requires O ( N ) storage, and scales excellently up to a thousand processes. The algorithm uses a gmres iterative solver with a restricted additive Schwarz method ( rasm ) as a preconditioner and a fast matrix-vector algorithm. Previous fast rbf methods \u2014 achieving at most O ( N log N ) complexity \u2014 were developed using multiquadric and polyharmonic basis functions. In contrast, the present method uses Gaussians with a small variance with respect to the domain, but with sufficient overlap. This is a common choice in particle methods for fluid simulation, our main target application. The fast decay of the Gaussian basis function allows rapid convergence of the iterative solver even when the subdomains in the rasm are very small. At the same time we show that the accuracy of the interpolation can achieve machine precision. The present method was implemented in parallel using the pets c library (developer version). Numerical experiments demonstrate its capability in problems of rbf interpolation with more than 50\u00a0million data points, timing at 106\u00a0s (19 iterations for an error tolerance of 10 \u2212\u00a015 ) on 1024 processors of a Blue Gene/L (700\u00a0MHz PowerPC processors). The parallel code is freely available in the open-source model.", "venue": "ArXiv", "authors": ["Rio  Yokota", "Lorena A. Barba", "Matthew G. Knepley"], "year": 2009, "n_citations": 56}
{"id": 4123951, "s2_id": "40e06b02cc75beb0336d3be078a2db5d1b17b6e1", "title": "NLOptControl: A modeling language for solving optimal control problems", "abstract": "Current direct-collocation-based optimal control software is either easy to use or fast, but not both. This is a major limitation for users that are trying to formulate complex optimal control problems (OCPs) for use in on-line applications. This paper introduces NLOptControl, an open-source modeling language that allows users to both easily formulate and quickly solve nonlinear OCPs using direct-collocation methods. To achieve these attributes, NLOptControl (1) is written in an efficient, dynamically-typed computing language called Julia, (2) extends an optimization modeling language called JuMP to provide a natural algebraic syntax for modeling nonlinear OCPs; and (3) uses reverse automatic differentiation with the acrylic-coloring method to exploit sparsity in the Hessian matrix. This work explores the novel design features of NLOptControl and compares its syntax and speed to those of PROPT. The syntax comparisons shows that NLOptControl models OCPs more concisely than PROPT. The speeds of various collocation methods within PROPT and NLOptControl are benchmarked over a range of collocation points using performance profiles; overall, NLOptControl's single, two, and four interval pseudospectral methods are roughly $14$, $26$, and $36$ times faster than PROPT's, respectively. NLOptControl is well-suited to improve existing off-line and on-line control systems and to engender new ones.", "venue": "ArXiv", "authors": ["Huckleberry  Febbo", "Paramsothy  Jayakumar", "Jeffrey L. Stein", "Tulga  Ersal"], "year": 2020, "n_citations": 2}
{"id": 4129000, "s2_id": "f3751b95da537e2eebe9313cd7411acd9edc79ea", "title": "GPdoemd: a python package for design of experiments for model discrimination", "abstract": "Abstract Model discrimination identifies a mathematical model that usefully explains and predicts a given system\u2019s behaviour. Researchers will often have several models, i.e. hypotheses, about an underlying system mechanism, but insufficient experimental data to discriminate between the models, i.e. discard inaccurate models. Given rival mathematical models and an initial experimental data set, optimal design of experiments suggests maximally informative experimental observations that maximise a design criterion weighted by prediction uncertainty. The model uncertainty requires gradients, which may not be readily available for black-box models. This paper (i) proposes a new design criterion using the Jensen-Renyi divergence, and (ii) develops a novel method replacing black-box models with Gaussian process surrogates. Using the surrogates, we marginalise out the model parameters with approximate inference. Results show these contributions working well for both classical and new test instances. We also (iii) introduce and discuss GPdoemd, the open-source implementation of the Gaussian process surrogate method.", "venue": "Comput. Chem. Eng.", "authors": ["Simon  Olofsson", "Ruth  Misener"], "year": 2019, "n_citations": 8}
{"id": 4132686, "s2_id": "025034f4397936152ea6c77e5f7a880d5b272088", "title": "DoWhy: An End-to-End Library for Causal Inference", "abstract": "In addition to efficient statistical estimators of a treatment's effect, successful application of causal inference requires specifying assumptions about the mechanisms underlying observed data and testing whether they are valid, and to what extent. However, most libraries for causal inference focus only on the task of providing powerful statistical estimators. We describe DoWhy, an open-source Python library that is built with causal assumptions as its first-class citizens, based on the formal framework of causal graphs to specify and test causal assumptions. DoWhy presents an API for the four steps common to any causal analysis---1) modeling the data using a causal graph and structural assumptions, 2) identifying whether the desired effect is estimable under the causal model, 3) estimating the effect using statistical estimators, and finally 4) refuting the obtained estimate through robustness checks and sensitivity analyses. In particular, DoWhy implements a number of robustness checks including placebo tests, bootstrap tests, and tests for unoberved confounding. DoWhy is an extensible library that supports interoperability with other implementations, such as EconML and CausalML for the the estimation step. The library is available at this https URL", "venue": "ArXiv", "authors": ["Amit  Sharma", "Emre  Kiciman"], "year": 2020, "n_citations": 11}
{"id": 4138704, "s2_id": "d2f6899dbdacbb590b3c4de6160aef957e67f088", "title": "Parameter Sensitivity Analysis of the SparTen High Performance Sparse Tensor Decomposition Software: Extended Analysis", "abstract": "Tensor decomposition models play an increasingly important role in modern data science applications. One problem of particular interest is fitting a low-rank Canonical Polyadic (CP) tensor decomposition model when the tensor has sparse structure and the tensor elements are nonnegative count data. SparTen is a high-performance C++ library which computes a low-rank decomposition using different solvers: a first-order quasi-Newton or a second-order damped Newton method, along with the appropriate choice of runtime parameters. Since default parameters in SparTen are tuned to experimental results in prior published work on a single real-world dataset conducted using MATLAB implementations of these methods, it remains unclear if the parameter defaults in SparTen are appropriate for general tensor data. Furthermore, it is unknown how sensitive algorithm convergence is to changes in the input parameter values. This report addresses these unresolved issues with large-scale experimentation on three benchmark tensor data sets. Experiments were conducted on several different CPU architectures and replicated with many initial states to establish generalized profiles of algorithm convergence behavior.", "venue": "ArXiv", "authors": ["Jeremy M. Myers", "Daniel M. Dunlavy", "Keita  Teranishi", "D. S. Hollman"], "year": 2020, "n_citations": 0}
{"id": 4145920, "s2_id": "bd028497e8a79cf05ed5e5d9019f38a016ea392e", "title": "Automatic differentiation of Sylvester, Lyapunov, and algebraic Riccati equations", "abstract": "Sylvester, Lyapunov, and algebraic Riccati equations are the bread and butter of control theorists. They are used to compute infinite-horizon Gramians, solve optimal control problems in continuous or discrete time, and design observers. While popular numerical computing frameworks (e.g., scipy) provide efficient solvers for these equations, these solvers are still largely missing from most automatic differentiation libraries. Here, we derive the forward and reverse-mode derivatives of the solutions to all three types of equations, and showcase their application on an inverse control problem.", "venue": "ArXiv", "authors": ["Ta-Chu  Kao", "Guillaume  Hennequin"], "year": 2020, "n_citations": 1}
{"id": 4151450, "s2_id": "c19e8fb462213b4df3701d7c9c41b8022305ffbc", "title": "Highly Parallel Sparse Matrix-Matrix Multiplication", "abstract": "Generalized sparse matrix-matrix multiplication is a key primitive for many high performance graph algorithms as well as some linear solvers such as multigrid. We present the first parallel algorithms that achieve increasing speed ups for an unbounded number of processors. Our algorithms are based on two-dimensional block distribution of sparse matrices where serial sections use a novel hypersparse kernel for scalability. We give a state-of-the-art MPI implementation of one of our algorithms. Our experiments show scaling up to thousands of processors on a variety of test scenarios.", "venue": "ArXiv", "authors": ["Aydin  Bulu\u00e7", "John R. Gilbert"], "year": 2010, "n_citations": 29}
{"id": 4160237, "s2_id": "2ee2c14966021fc5fd0749243a23c2c6e5e8f399", "title": "Fast Matlab compatible sparse assembly on multicore computers", "abstract": "We develop and implement in this paper a fast sparse assembly algorithm, the fundamental operation which creates a compressed matrix from raw index data. Since it is often a quite demanding and sometimes critical operation, it is of interest to design a highly efficient implementation. We show how to do this, and moreover, we show how our implementation can be parallelized to utilize the power of modern multicore computers. Our freely available code, fully Matlab compatible, achieves about a factor of 5 \u00d7 in speedup on a typical 6-core machine and 10 \u00d7 on a dual-socket 16-core machine compared to the built-in serial implementation.", "venue": "Parallel Comput.", "authors": ["Stefan  Engblom", "Dimitar  Lukarski"], "year": 2016, "n_citations": 10}
{"id": 4162215, "s2_id": "c9f02007f45f8854f27e5364c7c339421577da5b", "title": "Support for Non-conformal Meshes in PETSc's DMPlex Interface", "abstract": "PETSc's DMPlex interface for unstructured meshes has been extended to support non-conformal meshes. The topological construct that DMPlex implements---the CW-complex---is by definition conformal, so representing non- conformal meshes in a way that hides complexity requires careful attention to the interface between DMPlex and numerical methods such as the finite element method. Our approach---which combines a tree structure for subset- superset relationships and a \"reference tree\" describing the types of non-conformal interfaces---allows finite element code written for conformal meshes to extend automatically: in particular, all \"hanging-node\" constraint calculations are handled behind the scenes. We give example code demonstrating the use of this extension, and use it to convert forests of quadtrees and forests of octrees from the p4est library to DMPlex meshes.", "venue": "ArXiv", "authors": ["Tobin  Isaac", "Matthew G. Knepley"], "year": 2015, "n_citations": 4}
{"id": 4163605, "s2_id": "599fd79a1695c0ea86dcdc269ba0c27589a7bd16", "title": "Automatic Differentiation Tools in Optimization Software", "abstract": "We discuss the role of automatic differentiation tools in optimization software. We emphasize issues that are important to large-scale optimization and that have proved useful in the installation of nonlinear solvers in the NEOS Server. Our discussion centers on the computation of the gradient and Hessian matrix for partially separable functions and shows that the gradient and Hessian matrix can be computed with guaranteed bounds in time and memory requirements.", "venue": "ArXiv", "authors": ["Jorge J. Mor\u00e9"], "year": 2001, "n_citations": 11}
{"id": 4167625, "s2_id": "8ccdc2f76744358418a1ab3988338d646aa29ed7", "title": "PyMGRIT: A Python Package for the parallel-in-time method MGRIT", "abstract": "In this paper, we introduce the Python framework PyMGRIT, which implements the multigrid-reduction-in-time (MGRIT) algorithm for solving the (non-)linear systems arising from the discretization of time-dependent problems. The MGRIT algorithm is a reduction-based iterative method that allows parallel-in-time simulations, i. e., calculating multiple time steps simultaneously in a simulation, by using a time-grid hierarchy. The PyMGRIT framework features many different variants of the MGRIT algorithm, ranging from different multigrid cycle types and relaxation schemes, as well as various coarsening strategies, including time-only and space-time coarsening, to using different time integrators on different levels in the multigrid hierachy. PyMGRIT allows serial runs for prototyping and testing of new approaches, as well as parallel runs using the Message Passing Interface (MPI). Here, we describe the implementation of the MGRIT algorithm in PyMGRIT and present the usage from both user and developer point of views. Three examples illustrate different aspects of the package, including pure time parallelism as well as space-time parallelism by coupling PyMGRIT with PETSc or Firedrake, which enable spatial parallelism through MPI.", "venue": "ArXiv", "authors": ["Jens  Hahne", "Stephanie  Friedhoff", "Matthias  Bolten"], "year": 2020, "n_citations": 6}
{"id": 4169677, "s2_id": "8a9485207367ac2aa99f0497f0f0b7c269380e4b", "title": "Paradiseo: from a modular framework for evolutionary computation to the automated design of metaheuristics: 22 years of Paradiseo", "abstract": "The success of metaheuristic optimization methods has led to the development of a large variety of algorithm paradigms. However, no algorithm clearly dominates all its competitors on all problems. Instead, the underlying variety of landscapes of optimization problems calls for a variety of algorithms to solve them efficiently. It is thus of prior importance to have access to mature and flexible software frameworks which allow for an efficient exploration of the algorithm design space. Such frameworks should be flexible enough to accommodate any kind of metaheuristics, and open enough to connect with higher-level optimization, monitoring and evaluation softwares. This article summarizes the features of the Paradiseo framework, a comprehensive C++ free software which targets the development of modular metaheuristics. Paradiseo provides a highly modular architecture, a large set of components, speed of execution and automated algorithm design features, which are key to modern approaches to metaheuristics development.", "venue": "GECCO Companion", "authors": ["Johann  Dreo", "Arnaud  Liefooghe", "S'ebastien  Verel", "Marc  Schoenauer", "Juan J. Merelo", "Alexandre  Quemy", "Benjamin  Bouvier", "Jan  Gmys"], "year": 2021, "n_citations": 0}
{"id": 4170962, "s2_id": "975e7f07495ef99dfcca0fd325e27f9e0eca7e7a", "title": "Factorization of Non-Commutative Polynomials", "abstract": "We describe an algorithm for the factorization of non-commutative polynomials over a field. The first sketch of this algorithm appeared in an unpublished manuscript (literally hand written notes) by James H. Davenport more than 20 years ago. This version of the algorithm contains some improvements with respect to the original sketch. An improved version of the algorithm has been fully implemented in the Axiom computer algebra system.", "venue": "ArXiv", "authors": ["Fabrizio  Caruso"], "year": 2010, "n_citations": 5}
{"id": 4181705, "s2_id": "60da826c41cf83f40af66dfe79cae9b580899949", "title": "PySPH: A Python-based Framework for Smoothed Particle Hydrodynamics", "abstract": "PySPH is an open-source, Python-based, framework for particle methods in general and Smoothed Particle Hydrodynamics (SPH) in particular. PySPH allows a user to define a complete SPH simulation using pure Python. High-performance code is generated from this high-level Python code and executed on either multiple cores, or on GPUs, seamlessly. It also supports distributed execution using MPI. PySPH supports a wide variety of SPH schemes and formulations. These include, incompressible and compressible fluid flow, elastic dynamics, rigid body dynamics, shallow water equations, and other problems. PySPH supports a variety of boundary conditions including mirror, periodic, solid wall, and inlet/outlet boundary conditions. The package is written to facilitate reuse and reproducibility. This article discusses the overall design of PySPH and demonstrates many of its features. Several example results are shown to demonstrate the range of features that PySPH provides.", "venue": "ACM Trans. Math. Softw.", "authors": ["Prabhu  Ramachandran", "Kunal  Puri", "Aditya  Bhosale", "A  Dinesh", "Abhinav  Muta", "Pawan  Negi", "Rahul  Govind", "Suraj  Sanka", "Pankaj  Pandey", "Chandrashekhar  Kaushik", "Anshuman  Kumar", "Ananyo  Sen", "Rohan  Kaushik", "Mrinalgouda  Patil", "Deep  Tavker", "Dileep  Menon", "Vikas  Kurapati", "Amal S Sebastian", "Arkopal  Dutt", "Arpit  Agarwal"], "year": 2021, "n_citations": 13}
{"id": 4184502, "s2_id": "7786931f5f9a9deea4e189e0c049cc75a97e0f4e", "title": "Numerical Algebraic Geometry for Macaulay2", "abstract": "Numerical Algebraic Geometry uses numerical data to describe algebraic varieties. It is based on the methods of numerical polynomial homotopy continuation, an alternative to the classical symbolic approaches of computational algebraic geometry. We present a package, the driving idea behind which is to interlink the existing symbolic methods of Macaulay2 and the powerful engine of numerical approximate computations. The core procedures of the package exhibit performance competitive with the other homotopy continuation software.", "venue": "ArXiv", "authors": ["Anton  Leykin"], "year": 2009, "n_citations": 17}
{"id": 4194232, "s2_id": "d0bd7c5f51c80c3236e99009298734291745e2e7", "title": "Applying the Polyhedral Model to Tile Time Loops in Devito", "abstract": "The run time of many scientific computation applications for numerical methods is heavily dependent on just a few multi-dimensional loop nests. Since these applications are often limited by memory bandwidth rather than computational resources they can benefit greatly from any optimizations which decrease the run time of their loops by improving data reuse and thus reducing the total memory traffic. Some of the most effective of these optimizations are not suitable for development by hand or require advanced software engineering knowledge which is beyond the level of many researchers who are not specialists in code optimization. Several tools exist to automate the generation of high-performance code for numerical methods, such as Devito which produces code for finite-difference approximations typically used in the seismic imaging domain. We present a loop-tiling optimization which can be applied to Devito-generated loops and improves run time by up to 27.5%, and options for automating this optimization in the Devito framework.", "venue": "ArXiv", "authors": ["Dylan  McCormick"], "year": 2017, "n_citations": 1}
{"id": 4200395, "s2_id": "308e07e3d3427e4b62710aa91b2fe085ad7fe56d", "title": "A Java library to perform S-expansions of Lie algebras", "abstract": "The contraction method is a procedure that allows to establish non-trivial relations between Lie algebras and has had succesful applications in both mathematics and theoretical physics. This work deals with generalizations of the contraction procedure with a main focus in the so called S-expansion method as it includes most of the other generalized contractions. Basically, the S-exansion combines a Lie algebra $\\mathcal{G}$ with a finite abelian semigroup $S$ in order to define new S-expanded algebras. After giving a description of the main ingredients used in this paper, we present a Java library that automatizes the S-expansion procedure. With this computational tool we are able to represent Lie algebras and semigroups, so we can perform S-expansions of Lie algebras using arbitrary semigroups. We explain how the library methods has been constructed and how they work; then we give a set of example programs aimed to solve different problems. They are presented so that any user can easily modify them to perform his own calculations, without being necessarily an expert in Java. Finally, some comments about further developements and possible new applications are made.", "venue": "Journal of Physics: Conference Series", "authors": ["Carlos  Inostroza", "Igor  Kondrashuk", "Nelson  Merino", "Felip  Nadal"], "year": 2018, "n_citations": 8}
{"id": 4200404, "s2_id": "966409a76a956fc7147fb30b9e9ef6031e539f4a", "title": "Efficient Construction, Update and Downdate Of The Coefficients Of Interpolants Based On Polynomials Satisfying A Three-Term Recurrence Relation", "abstract": "In this paper, we consider methods to compute the coefficients of interpolants relative to a basis of polynomials satisfying a three-term recurrence relation. Two new algorithms are presented: the first constructs the coefficients of the interpolation incrementally and can be used to update the coefficients whenever a nodes is added to or removed from the interpolation. The second algorithm, which constructs the interpolation coefficients by decomposing the Vandermonde-like matrix iteratively, can not be used to update or downdate an interpolation, yet is more numerically stable than the first algorithm and is more efficient when the coefficients of multiple interpolations are to be computed over the same set of nodes.", "venue": "ArXiv", "authors": ["Pedro  Gonnet"], "year": 2010, "n_citations": 2}
{"id": 4201665, "s2_id": "aaf912df21a033617884b099e0073158a38a53c2", "title": "Cluster-level tuning of a shallow water equation solver on the Intel MIC architecture", "abstract": "The paper demonstrates the optimization of the execution environment of a hybrid OpenMP+MPI computational fluid dynamics code (shallow water equation solver) on a cluster enabled with Intel Xeon Phi coprocessors. The discussion includes: 1. Controlling the number and affinity of OpenMP threads to optimize access to memory bandwidth;", "venue": "ArXiv", "authors": ["Andrey  Vladimirov", "Cliff  Addison"], "year": 2014, "n_citations": 2}
{"id": 4209284, "s2_id": "09c5931307cba3f80d3ecc14d02eecfa46463cfe", "title": "MLPACK: a scalable C++ machine learning library", "abstract": "MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.", "venue": "J. Mach. Learn. Res.", "authors": ["Ryan R. Curtin", "James R. Cline", "N. P. Slagle", "William B. March", "Parikshit  Ram", "Nishant A. Mehta", "Alexander G. Gray"], "year": 2013, "n_citations": 148}
{"id": 4214779, "s2_id": "96e6ff2b5a227aceb6d868f12c163d5beb946849", "title": "Interactive Simplifier Tracing and Debugging in Isabelle", "abstract": "The Isabelle proof assistant comes equipped with a very powerful tactic for term simplification. While tremendously useful, the results of simplifying a term do not always match the user\u2019s expectation: sometimes, the resulting term is not in the form the user expected, or the simplifier fails to apply a rule. We describe a new, interactive tracing facility which offers insight into the hierarchical structure of the simplification with user-defined filtering, memoization and search. The new simplifier trace is integrated into the Isabelle/jEdit Prover IDE.", "venue": "CICM", "authors": ["Lars  Hupel"], "year": 2014, "n_citations": 7}
{"id": 4214851, "s2_id": "68a2a410c0beba9622856ef3a714fb02d4006d62", "title": "The basic principles and the structure and algorithmically software of computing by hypercomplex number", "abstract": "In article the basic principles put in a basis of algorithmicallysoftware of hypercomplex number calculations, structure of a software, structure of functional subsystems are considered. The most important procedures included in subsystems are considered, program listings and examples of their application are given.", "venue": "ArXiv", "authors": ["Yakiv O. Kalinovsky", "Yuliya E. Boyarinova", "A.  Sukalo", "Ya.  Hitsko"], "year": 2017, "n_citations": 0}
{"id": 4216030, "s2_id": "6fe188ef190cbbfeab24fc889eaa47f907210288", "title": "Domain Decomposition method on GPU cluster", "abstract": "Pallalel GPGPU computing for lattice QCD simulations has a bottleneck on the GPU to GPU data communication due to the lack of the direct data exchanging facility. In this work we investigate the performance of quark solver using the restricted additive Schwarz (RAS) preconditioner on a low cost GPU cluster. We expect that the RAS preconditioner with appropriate domaindecomposition and task distribution reduces the communication bottleneck. The GPU cluster we constructed is composed of four PC boxes, two GPU cards are attached to each box, and we have eight GPU cards in total. The compute nodes are connected with rather slow but low cost Gigabit-Ethernet. We include the RAS preconditioner in the single-precision part of the mixedprecision nested-BiCGStab algorithm and the single-precision task is distributed to the multiple GPUs. The benchmarking is done with the O(a)-improved Wilson quark on a randomly generated gauge configuration with the size of $32^4$. We observe a factor two improvment on the solver performance with the RAS precoditioner compared to that without the preconditioner and find that the improvment mainly comes from the reduction of the communication bottleneck as we expected.", "venue": "ArXiv", "authors": ["Yusuke  Osaki", "Ken-Ichi  Ishikawa"], "year": 2010, "n_citations": 13}
{"id": 4222144, "s2_id": "945f072801a319a48d77e3a1b6c3cf11897b79bd", "title": "Signal processing for a reverse\u2010GPS wildlife tracking system: CPU and GPU implementation experiences", "abstract": "We present robust high-performance implementations of signal-processing tasks performed by a high-throughput wildlife tracking system called ATLAS. The system tracks radio transmitters attached to wild animals by estimating the time of arrival of radio packets to multiple receivers (base stations). Time-of-arrival estimation of wideband radio signals is computationally expensive, especially in acquisition mode (when the time of transmission is not known, not even approximately). These computations are a bottleneck that limits the throughput of the system. We developed a sequential high-performance CPU implementation of the computations a few years back, and more recencely a GPU implementation. Both strive to balance performance with simplicity, maintainability, and development effort, as most real-world codes do. The paper reports on the two implementations and carefully evaluates their performance. The evaluations indicates that the GPU implementation dramatically improves performance and power-performance relative to the sequential CPU implementation running on a desktop CPU typical of the computers in current base stations. Performance improves by more than 50X on a high-end GPU and more than 4X with a GPU platform that consumes almost 5 times less power than the CPU platform. Performance-per-Watt ratios also improve (by more than 16X), and so do the price-performance ratios.", "venue": "Concurrency and Computation: Practice and Experience", "authors": ["Yaniv  Rubinpur", "Sivan  Toledo"], "year": 2021, "n_citations": 0}
{"id": 4226413, "s2_id": "f4a0980f6319ecb031fe3b29880863c81683ffe4", "title": "PyDEC: Software and Algorithms for Discretization of Exterior Calculus", "abstract": "This article describes the algorithms, features, and implementation of PyDEC, a Python library for computations related to the discretization of exterior calculus. PyDEC facilitates inquiry into both physical problems on manifolds as well as purely topological problems on abstract complexes. We describe efficient algorithms for constructing the operators and objects that arise in discrete exterior calculus, lowest-order finite element exterior calculus, and in related topological problems. Our algorithms are formulated in terms of high-level matrix operations which extend to arbitrary dimension. As a result, our implementations map well to the facilities of numerical libraries such as NumPy and SciPy. The availability of such libraries makes Python suitable for prototyping numerical methods. We demonstrate how PyDEC is used to solve physical and topological problems through several concise examples.", "venue": "TOMS", "authors": ["Nathan  Bell", "Anil N. Hirani"], "year": 2012, "n_citations": 38}
{"id": 4229571, "s2_id": "29f10087f684c1c0f5b4cf77dc33763f5f04d629", "title": "Batched Kronecker product for 2-D matrices and 3-D arrays on NVIDIA GPUs", "abstract": "We describe an interface and an implementation for performing Kronecker product actions on NVIDIA GPUs for multiple small 2-D matrices and 3-D arrays processed in parallel as a batch. This method is suited to cases where the Kronecker product component matrices are identical but the operands in a matrix-free application vary in the batch. Any batched GEMM (General Matrix Multiply) implementation, for example ours [1] or the one in cuBLAS, can also be used for performing batched Kronecker products on GPUs. However, the specialized implementation presented here is faster and uses less memory. Partly this is because a simple GEMM based approach would require extra copies to and from main memory. We focus on matrix sizes less than or equal to 16, since these are the typical polynomial degrees in Finite Elements, but the implementation can be easily extended for other sizes. We obtain 143 and 285 GFlop/s for single precision real when processing matrices of size 10 and 16, respectively on NVIDIA Tesla K20c using CUDA 5.0. The corresponding speeds for 3-D array Kronecker products are 126 and 268 GFlop/s, respectively. Double precision is easily supported using the C++ template mechanism.", "venue": "ArXiv", "authors": ["Chetan  Jhurani"], "year": 2013, "n_citations": 0}
{"id": 4231397, "s2_id": "ac82b1fa2b2a018bd5e30fef1d99082d4d3f49e5", "title": "Users Guide for SnadiOpt: A Package Adding Automatic Differentiation to Snopt", "abstract": "SnadiOpt is a package that supports the use of the automatic differentiation package ADIFOR with the optimization package Snopt. Snopt is a general-purpose system for solving optimization problems with many variables and constraints. It minimizes a linear or nonlinear function subject to bounds on the variables and sparse linear or nonlinear constraints. It is suitable for large-scale linear and quadratic programming and for linearly constrained optimization, as well as for general nonlinear programs. The method used by Snopt requires the first derivatives of the objective and constraint functions to be available. The SnadiOpt package allows users to avoid the time-consuming and error-prone process of evaluating and coding these derivatives. Given Fortran code for evaluating only the values of the objective and constraints, SnadiOpt automatically generates the code for evaluating the derivatives and builds the relevant Snopt input files and sparse data structures.", "venue": "ArXiv", "authors": ["E. Michael Gertz", "Philip E. Gill", "Julia  Muetherig"], "year": 2001, "n_citations": 9}
{"id": 4235801, "s2_id": "866e30cc7e7d400df57ca0a9d4a89655e62343f2", "title": "Increasing precision of uniform pseudorandom number generators", "abstract": "A general method to produce uniformly distributed pseudorandom numbers with extended precision by combining two pseudorandom numbers with lower precision is proposed. In particular, this method can be used for pseudorandom number generation with extended precision on graphics processing units (GPU), where the performance of single and double precision operations can vary significantly.", "venue": "ArXiv", "authors": ["Vadim  Demchik", "Alexey  Gulov"], "year": 2014, "n_citations": 1}
{"id": 4238886, "s2_id": "8d974dc926b2001cd6ff1763a57aaad1979809e7", "title": "Performance of Python runtimes on a non-numeric scientific code", "abstract": "The Python library FatGHol (FatGHoL) used in (Murri2012) to reckon the rational homology of the moduli space of Riemann surfaces is an example of a non-numeric scientific code: most of the processing it does is generating graphs (represented by complex Python objects) and computing their isomor- phisms (a triple of Python lists; again a nested data structure). These operations are repeated many times over: for example, the spaces M0;6 and M1;4 are triangulated by 4'583'322 and 747'664 graphs, respectively. This is an opportunity for every Python runtime to prove its strength in optimization. The purpose of this experiment was to assess the maturity of alternative Python runtimes, in terms of: compatibility with the language as implemented in CPython 2.7, and performance speedup. This paper compares the results and experiences from running FatGHol with different Python runtimes: CPython 2.7.5, PyPy 2.1, Cython 0.19, Numba 0.11, Nuitka 0.4.4 and Falcon.", "venue": "ArXiv", "authors": ["Riccardo  Murri"], "year": 2014, "n_citations": 2}
{"id": 4244794, "s2_id": "61ac3246d6cf9858d9d7f33cf256b6296dce11a2", "title": "Tegula - exploring a galaxy of two-dimensional periodic tilings", "abstract": "Periodic tilings play a role in the decorative arts, in construction and in crystal structures. Combinatorial tiling theory allows the systematic generation, visualization and exploration of such tilings of the plane, sphere and hyperbolic plane, using advanced algorithms and software.Here we present a \"galaxy\" of tilings that consists of the set of all 2.4 billion different types of periodic tilings that have Dress complexity up to 24. We make these available in a database and provide a new program called Tegula that can be used to search and visualize such tilings. \nAvailability: All tilings and software and are open source and available here: this https URL.", "venue": "Comput. Aided Geom. Des.", "authors": ["R\u00fcdiger  Zeller", "Olaf  Delgado-Friedrichs", "Daniel H. Huson"], "year": 2021, "n_citations": 1}
{"id": 4247720, "s2_id": "7d4e6c09435a5cd7667d7a1b6ece55e633b5307b", "title": "Assessing Excel VBA Suitability for Monte Carlo Simulation", "abstract": "Monte Carlo (MC) simulation includes a wide range of stochastic techniques used to quantitatively evaluate the behavior of complex systems or processes. Microsoft Excel spreadsheets with Visual Basic for Applications (VBA) software is, arguably, the most commonly employed general purpose tool for MC simulation. Despite the popularity of the Excel in many industries and educational institutions, it has been repeatedly criticized for its flaws and often described as questionable, if not completely unsuitable, for statistical problems. The purpose of this study is to assess suitability of the Excel (specifically its 2010 and 2013 versions) with VBA programming as a tool for MC simulation. The results of the study indicate that Microsoft Excel (versions 2010 and 2013) is a strong Monte Carlo simulation application offering a solid framework of core simulation components including spreadsheets for data input and output, VBA development environment and summary statistics functions. This framework should be complemented with an external high-quality pseudo-random number generator added as a VBA module. A large and diverse category of Excel incidental simulation components that includes statistical distributions, linear and non-linear regression and other statistical, engineering and business functions require execution of due diligence to determine their suitability for a specific MC project.", "venue": "ArXiv", "authors": ["Alexei  Botchkarev"], "year": 2015, "n_citations": 7}
{"id": 4257537, "s2_id": "f16d5d1f2b1f4793d4eccfd994b4d7ba4d5115e3", "title": "FRaGenLP: A Generator of Random Linear Programming Problems for Cluster Computing Systems", "abstract": "The article presents and evaluates a scalable FRaGenLP algorithm for generating random linear programming problems of large dimension n on cluster computing systems. To ensure the consistency of the problem and the boundedness of the feasible region, the constraint system includes 2n + 1 standard inequalities, called support inequalities. New random inequalities are generated and added to the system in a manner that ensures the consistency of the constraints. Furthermore, the algorithm uses two likeness metrics to prevent the addition of a new random inequality that is similar to one already present in the constraint system. The algorithm also rejects random inequalities that cannot affect the solution of the linear programming problem bounded by the support inequalities. The parallel implementation of the FRaGenLP algorithm is performed in C++ through the parallel BSF-skeleton, which encapsulates all aspects related to the MPI-based parallelization of the program. We provide the results of large-scale computational experiments on a cluster computing system to study the scalability of the FRaGenLP algorithm.", "venue": "Communications in Computer and Information Science", "authors": ["Leonid B. Sokolinsky", "Irina M. Sokolinskaya"], "year": 2021, "n_citations": 1}
{"id": 4258165, "s2_id": "01eb9fe0ca761b1403e9dfba837640dc6b95211a", "title": "Scalable parallel algorithm for solving non-stationary systems of linear inequalities", "abstract": "In this paper, a scalable iterative projection-type algorithm for solving non-stationary sys-tems of linear inequalities is considered. A non-stationary system is understood as a large-scale system of inequalities in which coefficients and constant terms can change during the calculation process. The proposed parallel algorithm uses the concept of pseudo-projection which generalizes the notion of orthogonal projection. The parallel pseudo-projection algorithm is implemented using the parallel BSF-skeleton. An analytical esti-mation of the algorithm scalability boundary is obtained on the base of the BSF cost met-ric. The large-scale computational experiments were performed on a cluster computing system. The obtained results confirm the efficiency of the proposed approach.", "venue": "Lobachevskii Journal of Mathematics", "authors": ["Leonid B. Sokolinsky", "Irina M. Sokolinskaya"], "year": 2020, "n_citations": 3}
{"id": 4266948, "s2_id": "1e831b4663dcfbc7ad6b792eac99f6a5fa740599", "title": "The Chunks and Tasks Matrix Library 2.0", "abstract": "We present a C++ header-only parallel sparse matrix library, based on sparse quadtree representation of matrices using the Chunks and Tasks programming model. The library implements a number of sparse matrix algorithms for distributed memory parallelization that are able to dynamically exploit data locality to avoid movement of data. This is demonstrated for the example of block-sparse matrix-matrix multiplication applied to three sequences of matrices with different nonzero structure, using the CHT-MPI 2.0 runtime library implementation of the Chunks and Tasks model. The runtime library succeeds to dynamically load balance the calculation regardless of the sparsity structure.", "venue": "ArXiv", "authors": ["Emanuel H. Rubensson", "Elias  Rudberg", "Anastasia  Kruchinina", "Anton G. Artemov"], "year": 2020, "n_citations": 0}
{"id": 4267647, "s2_id": "32a21871e7d2e650d196047675a0ea3a1463cbcb", "title": "The ITensor Software Library for Tensor Network Calculations", "abstract": "ITensor is a system for programming tensor network calculations with an interface modeled on tensor diagram notation, which allows users to focus on the connectivity of a tensor network without manually bookkeeping tensor indices. The ITensor interface rules out common programming errors and enables rapid prototyping of tensor network algorithms. After discussing the philosophy behind the ITensor approach, we show examples of each part of the interface including Index objects, the ITensor product operator, tensor factorizations, tensor storage types, algorithms for matrix product state (MPS) and matrix product operator (MPO) tensor networks, quantum number conserving block-sparse tensors, and the NDTensors library. We also review publications that have used ITensor for quantum many-body physics and for other areas where tensor networks are increasingly applied. To conclude we discuss promising features and optimizations to be added in the future.", "venue": "ArXiv", "authors": ["Matthew  Fishman", "Steven R. White", "E. Miles Stoudenmire"], "year": 2020, "n_citations": 124}
{"id": 4273957, "s2_id": "f5608d1458c8a86c6e6c55ce4e21e3f1f7f24d2e", "title": "Fast Multiplication of Matrices with Decay", "abstract": "A fast algorithm for the approximate multiplication of matrices with decay is introduced; the Sparse Approximate Matrix Multiply (SpAMM) reduces complexity in the product space, a different approach from current methods that economize within the matrix space through truncation or rank reduction. Matrix truncation (element dropping) is compared to SpAMM for quantum chemical matrices with approximate exponential and algebraic decay. For matched errors in the electronic total energy, SpAMM is found to require fewer to far fewer floating point operations relative to dropping. The challenges and opportunities afforded by this new approach are discussed, including the potential for high performance implementations.", "venue": "ArXiv", "authors": ["Matt  Challacombe", "Nicolas  Bock"], "year": 2010, "n_citations": 12}
{"id": 4274472, "s2_id": "d293a060e52bb0ea1bcdde87bfbe7c6bdaa7e253", "title": "Writing Reusable Digital Geometry Algorithms in a Generic Image Processing Framework", "abstract": "Digital Geometry software should reflect the generality of the underlying mathematics: mapping the latter to the former requires genericity. By designing generic solutions, one can effectively reuse digital geometry data structures and algorithms. We propose an image processing framework focused on the Generic Programming paradigm in which an algorithm on the paper can be turned into a single code, written once and usable with various input types. This approach enables users to design and implement new methods at a lower cost, try cross-domain experiments and help generalize results.", "venue": "ArXiv", "authors": ["Roland  Levillain", "Thierry  G\u00e9raud", "Laurent  Najman"], "year": 2012, "n_citations": 6}
{"id": 4296486, "s2_id": "43d61cb32467ee1e8fd881abe1349c14aec5a6ef", "title": "Tensors Come of Age: Why the AI Revolution will help HPC", "abstract": "This article discusses how the automation of tensor algorithms, based on A Mathematics of Arrays and Psi Calculus, and a new way to represent numbers, Unum Arithmetic, enables mechanically provable, scalable, portable, and more numerically accurate software.", "venue": "ArXiv", "authors": ["John L. Gustafson", "Lenore M. Restifo Mullin"], "year": 2017, "n_citations": 2}
{"id": 4300526, "s2_id": "7dfc8b5094ec8c934781a665258e183c38ae7eaf", "title": "Automatic Theorem Proving in Walnut", "abstract": "Walnut is a software package that implements a mechanical decision procedure for deciding certain combinatorial properties of some special words referred to as automatic words or automatic sequences. Walnut is written in Java and is open source. It is licensed under GNU General Public License.", "venue": "ArXiv", "authors": ["Hamoon  Mousavi"], "year": 2016, "n_citations": 56}
{"id": 4303513, "s2_id": "90d8190207e334bec36ca13aa083bcd94952b990", "title": "lsjk - a C++ library for arbitrary-precision numeric evaluation of the generalized log-sine functions", "abstract": "Abstract Generalized log-sine functions Ls j ( k ) ( \u03b8 ) appear in higher order \u025b -expansion of different Feynman diagrams. We present an algorithm for the numerical evaluation of these functions for real arguments. This algorithm is implemented as a C++ library with arbitrary-precision arithmetics for integer 0 \u2a7d k \u2a7d 9 and j \u2a7e 2 . Some new relations and representations of the generalized log-sine functions are given. Program summary Title of program: lsjk Catalogue number: ADVS Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ADVS Program obtained from: CPC Program Library, Queen's University of Belfast, N. Ireland Licensing terms: GNU General Public License Computers: all Operating systems: POSIX Programming language: C++ Memory required to execute: Depending on the complexity of the problem, at least 32 MB RAM recommended No. of lines in distributed program, including testing data, etc.: 41\u2009975 No. of bytes in distributed program, including testing data, etc.: 309\u2009156 Distribution format: tar.gz Other programs called: The CLN library for arbitrary-precision arithmetics is required at version 1.1.5 or greater External files needed: none Nature of the physical problem: Numerical evaluation of the generalized log-sine functions for real argument in the region 0 \u03b8 \u03c0 . These functions appear in Feynman integrals Method of solution: Series representation for the real argument in the region 0 \u03b8 \u03c0 Restriction on the complexity of the problem: Limited up to Ls j ( 9 ) ( \u03b8 ) , and j is an arbitrary integer number. Thus, all function up to the weight 12 in the region 0 \u03b8 \u03c0 can be evaluated. The algorithm can be extended up to higher values of k ( k > 9 ) without modification Typical running time: Depending on the complexity of problem. See text below.", "venue": "Comput. Phys. Commun.", "authors": ["M. Yu. Kalmykov", "A.  Sheplyakov"], "year": 2005, "n_citations": 30}
{"id": 4309766, "s2_id": "e7924a71ff89f37f66298a6b42bcd26fa7c0f33b", "title": "River: machine learning for streaming data in Python", "abstract": "River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of the two most popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.", "venue": "ArXiv", "authors": ["Jacob  Montiel", "Max  Halford", "Saulo Martiello Mastelini", "Geoffrey  Bolmier", "Raphael  Sourty", "Robin  Vaysse", "Adil  Zouitine", "Heitor Murilo Gomes", "Jesse  Read", "Talel  Abdessalem", "Albert  Bifet"], "year": 2020, "n_citations": 14}
{"id": 4315999, "s2_id": "09d3f4231b51e48927204ed8944e994884ae0b33", "title": "PURRS: Towards Computer Algebra Support for Fully Automatic Worst-Case Complexity Analysis", "abstract": "Fully automatic worst-case complexity analysis has a number of applications in computer-assisted program manipulation. A classical and powerful approach to complexity analysis consists in formally deriving, from the program syntax, a set of constraints expressing bounds on the resources required by the program, which are then solved, possibly applying safe approximations. In several interesting cases, these constraints take the form of recurrence relations. While techniques for solving recurrences are known and implemented in several computer algebra systems, these do not completely fulfill the needs of fully automatic complexity analysis: they only deal with a somewhat restricted class of recurrence relations, or sometimes require user intervention, or they are restricted to the computation of exact solutions that are often so complex to be unmanageable, and thus useless in practice. In this paper we briefly describe PURRS, a system and software library aimed at providing all the computer algebra services needed by applications performing or exploiting the results of worst-case complexity analyses. The capabilities of the system are illustrated by means of examples derived from the analysis of programs written in a domain-specific functional programming language for real-time embedded systems.", "venue": "ArXiv", "authors": ["Roberto  Bagnara", "Andrea  Pescetti", "Alessandro  Zaccagnini", "Enea  Zaffanella"], "year": 2005, "n_citations": 25}
{"id": 4317441, "s2_id": "2de051acd0bf2908b4e7bcf4a986640c3d296035", "title": "Odeint - Solving ordinary differential equations in C++", "abstract": "Many physical, biological or chemical systems are modeled by ordinary differential equations (ODEs) and finding their solution is an every-day-task for many scientists. Here, we introduce a new C++ library dedicated to find numerical solutions of initial value problems of ODEs: odeint (www.odeint.com). odeint is implemented in a highly generic way and provides extensive interoperability at top performance. For example, due to it's modular design it can be easily parallized with OpenMP and even runs on CUDA GPUs. Despite that, it provides a convenient interface that allows for a simple and easy usage.", "venue": "ArXiv", "authors": ["Karsten  Ahnert", "Mario  Mulansky"], "year": 2011, "n_citations": 166}
{"id": 4323660, "s2_id": "c2e7e8b7d57e3fd2ac2731c4f1319508b119f73a", "title": "An Analysis of Publication Venues for Automatic Differentiation Research", "abstract": "We present the results of our analysis of publication venues for papers on automatic differentiation (AD), covering academic journals and conference proceedings. Our data are collected from the AD publications database maintained by the autodiff.org community website. The database is purpose-built for the AD field and is expanding via submissions by AD researchers. Therefore, it provides a relatively noise-free list of publications relating to the field. However, it does include noise in the form of variant spellings of journal and conference names. We handle this by manually correcting and merging these variants under the official names of corresponding venues. We also share the raw data we get after these corrections.", "venue": "ArXiv", "authors": ["Atilim Gunes Baydin", "Barak A. Pearlmutter"], "year": 2014, "n_citations": 0}
{"id": 4333117, "s2_id": "f25d379a1e0b6d4bc1beeb1044c0ed8ea8d8d9ee", "title": "Concurrent Cuba", "abstract": "The parallel version of the multidimensional numerical integration package Cuba is presented and achievable speed-ups discussed.", "venue": "Comput. Phys. Commun.", "authors": ["Thomas  Hahn"], "year": 2016, "n_citations": 57}
{"id": 4338287, "s2_id": "9c2e3932a265926234e098ac4bf3545a0016e268", "title": "Feast Eigensolver for Non-Hermitian Problems", "abstract": "A detailed new upgrade of the FEAST eigensolver targeting non-Hermitian eigenvalue problems is presented and thoroughly discussed. It aims at broadening the class of eigenproblems that can be addressed within the framework of the FEAST algorithm. The algorithm is ideally suited for computing selected interior eigenvalues and their associated right/left bi-orthogonal eigenvectors,located within a subset of the complex plane. It combines subspace iteration with efficient contour integration techniques that approximate the left and right spectral projectors. We discuss the various algorithmic choices that have been made to improve the stability and usability of the new non-Hermitian eigensolver. The latter retains the convergence property and multi-level parallelism of Hermitian FEAST, making it a valuable new software tool for the scientific community.", "venue": "SIAM J. Sci. Comput.", "authors": ["James  Kestyn", "Eric  Polizzi", "Ping Tak Peter Tang"], "year": 2016, "n_citations": 43}
{"id": 4339958, "s2_id": "d592ff1ba69bdda428cfa8f64628a031db6fd70a", "title": "Exact sparse matrix-vector multiplication on GPU's and multicore architectures", "abstract": "We propose different implementations of the sparse matrix-dense vector multiplication (SpMV) for finite fields and rings Z /m Z. We take advantage of graphic card processors (GPU) and multi-core architectures. Our aim is to improve the speed of SpMV in the LinBox library, and henceforth the speed of its black-box algorithms. Besides, we use this library and a new parallelisation of the sigma-basis algorithm in a parallel block Wiedemann rank implementation over finite fields.", "venue": "PASCO", "authors": ["Brice  Boyer", "Jean-Guillaume  Dumas", "Pascal  Giorgi"], "year": 2010, "n_citations": 24}
{"id": 4353824, "s2_id": "cf1018e05804bdfa7ef6f9f81634c372f1e68eef", "title": "A Fast Multigrid Algorithm for Energy Minimization Under Planar Density Constraints", "abstract": "The two-dimensional layout optimization problem reinforced by the efficient space utilization demand has a wide spectrum of practical applications. Formulating the problem as a nonlinear minimization problem under planar equality and/or inequality density constraints, we present a linear time multigrid algorithm for solving correction to this problem. The method is demonstrated on various graph drawing (visualization) instances.", "venue": "Multiscale Model. Simul.", "authors": ["Dorit  Ron", "Ilya  Safro", "Achi  Brandt"], "year": 2010, "n_citations": 8}
{"id": 4355432, "s2_id": "7b637bf47f7841e0aadd866166211c4a01e19e35", "title": "Automating embedded analysis capabilities and managing software complexity in multiphysics simulation, Part II: Application to partial differential equations", "abstract": "A template-based generic programming approach was presented in a previous paper that separates the development effort of programming a physical model from that of computing additional quantities, such as derivatives, needed for embedded analysis algorithms. In this paper, we describe the implementation details for using the template-based generic programming approach for simulation and analysis of partial differential equations (PDEs). We detail several of the hurdles that we have encountered, and some of the software infrastructure developed to overcome them. We end with a demonstration where we present shape optimization and uncertainty quantification results for a 3D PDE application.", "venue": "Sci. Program.", "authors": ["Roger P. Pawlowski", "Eric T. Phipps", "Andrew G. Salinger", "Steven J. Owen", "Christopher M. Siefert", "Matthew L. Staten"], "year": 2012, "n_citations": 19}
{"id": 4360168, "s2_id": "3eea984be8af3c81dae64274131b40d9c37b8d55", "title": "Theory Presentation Combinators", "abstract": "We motivate and give semantics to theory presentation combinators as the foundational building blocks for a scalable library of theories. The key observation is that the category of contexts and fibered categories are the ideal theoretical tools for this purpose.", "venue": "AISC/MKM/Calculemus", "authors": ["Jacques  Carette", "Russell  O'Connor"], "year": 2012, "n_citations": 15}
{"id": 4366398, "s2_id": "9243f5ccf4ed01118e9bfcec4fe5a93ae1247c52", "title": "On PyTorch Implementation of Density Estimators for von Mises-Fisher and Its Mixture", "abstract": "The von Mises-Fisher (vMF) is a well-known density model for directional random variables. The recent surge of the deep embedding methodologies for highdimensional structured data such as images or texts, aimed at extracting salient directional information, can make the vMF model even more popular. In this article, we will review the vMF model and its mixture, provide detailed recipes of how to train the models, focusing on the maximum likelihood estimators, in Python/PyTorch. In particular, implementation of vMF typically suffers from the notorious numerical issue of the Bessel function evaluation in the density normalizer, especially when the dimensionality is high, and we address the issue using the MPMath library that supports arbitrary precision. For the mixture learning, we provide both minibatch-based large-scale SGD learning, as well as the EM algorithm which is a full batch estimator. For each estimator/methodology, we test our implementation on some synthetic data, while we also demonstrate the use case in a more realistic scenario of image clustering. Our code is publicly available in https://github.com/minyoungkim21/vmf-lib. 1 Definition of von Mises-Fisher Density The von Mises-Fisher (vMF for short) distribution, defined over the unit hypersphere in R, has the following density function: p(x;\u03bc, \u03ba) = Cd(\u03ba) \u00b7 exp(\u03ba\u03bc>x), Cd(\u03ba) = \u03bad/2\u22121 (2\u03c0)Id/2\u22121(\u03ba) , (1) where I\u03b1 is the modified Bessel function of the first kind with order \u03b1. Here, \u03ba (scalar) and \u03bc \u2208 R constitute the parameters of the vMF density, and they are constrained as: \u03ba \u2265 0 and ||\u03bc|| = 1. Obviously, the density has a single mode at x = \u03bc, and any hyperplane normal to \u03bc forms a level set, that is, the likelihood remains unchanged for {x : \u03bc>x = \u03b3} with |\u03b3| \u2264 1. Due to the symmetry of the density along the vector \u03bc, the mean of the density is also \u03bc. And, \u03ba determines how peaky the distribution is around its mode/mean \u03bc, thus it is often called the concentration parameter (i.e., large \u03ba means that the density is more concentrated around \u03bc, and vice versa). Note also that the normalizer Cd(\u03ba) depends only on \u03ba, not \u03bc, due to the symmetry of the density function.", "venue": "ArXiv", "authors": ["Minyoung  Kim"], "year": 2021, "n_citations": 0}
{"id": 4366788, "s2_id": "675a69b76da4201c71eda467a31270e08354902d", "title": "geomstats: a Python Package for Riemannian Geometry in Machine Learning", "abstract": "We introduce Geomstats, an open-source Python toolbox for computations and statistics on nonlinear manifolds, such as hyperbolic spaces, spaces of symmetric positive definite matrices, Lie groups of transformations, and many more. We provide object-oriented and extensively unit-tested implementations. Among others, manifolds come equipped with families of Riemannian metrics, with associated exponential and logarithmic maps, geodesics and parallel transport. Statistics and learning algorithms provide methods for estimation, clustering and dimension reduction on manifolds. All associated operations are vectorized for batch computation and provide support for different execution backends, namely NumPy, PyTorch and TensorFlow, enabling GPU acceleration. This paper presents the package, compares it with related libraries and provides relevant code examples. We show that Geomstats provides reliable building blocks to foster research in differential geometry and statistics, and to democratize the use of Riemannian geometry in machine learning applications. The source code is freely available under the MIT license at http://geomstats.ai.", "venue": "ArXiv", "authors": ["Nina  Miolane", "Johan  Mathe", "Claire  Donnat", "Mikael  Jorda", "Xavier  Pennec"], "year": 2018, "n_citations": 40}
{"id": 4367546, "s2_id": "f7de41b7fa4d243e73a8869c36cb30fe8c93d1b9", "title": "An Optimized, Parallel Computation of the Ghost Layer for Adaptive Hybrid Forest Meshes", "abstract": "We discuss parallel algorithms to gather topological information about off-process mesh neighbor elements. This information is commonly called the ghost layer, whose creation is a fundamental, necessary task in executing most parallel, element-based computer simulations. Approaches differ in that the ghost layer may either be inherently part of the mesh data structure that is maintained and modified, or kept separate and constructed/deleted as needed. \nIn this work, we present an updated design following the latter approach, which we favor for its modularity of algorithms and data structures. We target arbitrary adaptive, non-conforming forest-of-(oc)trees meshes of mixed element shapes, such as cubes, prisms, and tetrahedra, and restrict ourselves to face-ghosts. Our algorithm has low complexity and redundancy since we reduce it to generic codimension-1 subalgorithms that can be flexibly combined. We cover several existing solutions as special cases and optimize further using recursive, amortized tree searches and traversals.", "venue": "SIAM Journal on Scientific Computing", "authors": ["Johannes  Holke", "David  Knapp", "Carsten  Burstedde"], "year": 2021, "n_citations": 0}
{"id": 4377126, "s2_id": "65ff4d6c24bca18ad901253b6aa3f02e8b9288bb", "title": "Seeing the forest in the tree: applying VRML to mathematical problems in number theory", "abstract": "Hamming claimed 'the purpose of computing is insight, not numbers.' In a variant of that aphorism, we show how the Virtual Reality Modeling Language (VRML) can provide powerful insight into the mathematical properties of numbers. The mathematical problem we consider is the relatively recent conjecture colloquially known as the '3x + 1 problem'. It refers to an iterative integer function that also can be though of as a digraph rooted at unity with the other numbers in any iteration sequence locate at seemingly randomized positions throughout the tree. The mathematical conjecture states that there is a unique cycle at unity. So far, a proof for this otherwise simple function has remained intractable. Many difficult problems in number theory, however, have been cracked with the aid of geometrical representations. Here, we show that any arbitrary portion of the 3x + 1 digraph can be constructed by iterative application of a unique subgraph called the G-cell generator - similar in concept to a fractal geometry generator. We describe the G-cell generator and present some examples of the VRML worlds developed programmatically with it. Perhaps surprisingly, this seem to be one of the few attempts to apply VRML to problems in number theory.", "venue": "Electronic Imaging", "authors": ["Neil J. Gunther"], "year": 1999, "n_citations": 0}
{"id": 4383512, "s2_id": "37c0f114878e7e387e1f1361518b862224d2e0a7", "title": "Twofold exp and log", "abstract": "This article is about twofold arithmetic. Here I introduce algorithms and experimental code for twofold variant of C/C++ standard functions exp() and log(), and expm1() and log1p(). Twofold function $y_0+y_1 \\approx f(x_0+x_1)$ is nearly 2x-precise so can assess accuracy of standard one. Performance allows assessing on-fly: twofold texp() over double is ~10x times faster than expq() by GNU quadmath.", "venue": "ArXiv", "authors": ["Evgeny  Latkin"], "year": 2015, "n_citations": 0}
{"id": 4388726, "s2_id": "57c1c777b9dd5caee4a20096efba793d5c347894", "title": "Evaluation of interval extension of the power function by graph decomposition", "abstract": "The subject of our talk is the correct evaluation of interval extension of the function specified by the expression x^y without any constraints on the values of x and y. The core of our approach is a decomposition of the graph of x^y into a small number of parts which can be transformed into subsets of the graph of x^y for non-negative bases x. Because of this fact, evaluation of interval extension of x^y, without any constraints on x and y, is not much harder than evaluation of interval extension of x^y for non-negative bases x.", "venue": "ArXiv", "authors": ["Evgueni  Petrov"], "year": 2006, "n_citations": 0}
{"id": 4393446, "s2_id": "16606891aae022b85d65f571eb01c8aabba9ac56", "title": "Real solution isolation with multiplicity of zero-dimensional triangular systems", "abstract": "Existing algorithms for isolating real solutions of zero-dimensional polynomial systems do not compute the multiplicities of the solutions. In this paper, we define in a natural way the multiplicity of solutions of zero-dimensional triangular polynomial systems and prove that our definition is equivalent to the classical definition of local (intersection) multiplicity. Then we present an effective and complete algorithm for isolating real solutions with multiplicities of zero-dimensional triangular polynomial systems using our definition. The algorithm is based on interval arithmetic and square-free factorization of polynomials with real algebraic coefficients. The computational results on some examples from the literature are presented.", "venue": "Science China Information Sciences", "authors": ["Zhihai  Zhang", "Tian  Fang", "Bican  Xia"], "year": 2010, "n_citations": 7}
{"id": 4397660, "s2_id": "ac83d8969c7225a7d8811e5bb0f74c7126e1e14c", "title": "A Flexible Framework for Parallel Multi-Dimensional DFTs", "abstract": "Multi-dimensional discrete Fourier transforms (DFT) are typically decomposed into multiple 1D transforms. Hence, parallel implementations of any multi-dimensional DFT focus on parallelizing within or across the 1D DFT. Existing DFT packages exploit the inherent parallelism across the 1D DFTs and offer rigid frameworks, that cannot be extended to incorporate both forms of parallelism and various data layouts to enable some of the parallelism. However, in the era of exascale, where systems have thousand of nodes and intricate network topologies, flexibility and parallel efficiency are key aspects all multi-dimensional DFT frameworks need to have in order to map and scale the computation appropriately. In this work, we present a flexible framework, built on the Redistribution Operations and Tensor Expressions (ROTE) framework, that facilitates the development of a family of parallel multi-dimensional DFT algorithms by 1) unifying the two parallelization schemes within a single framework, 2) exploiting the two different parallelization schemes to different degrees and 3) using different data layouts to distribute the data across the compute nodes. We demonstrate the need of a versatile framework and thus a need for a family of parallel multi-dimensional DFT algorithms on the K-Computer, where we show almost linear strong scaling results for problem sizes of 1024^3 on 32k compute nodes.", "venue": "ArXiv", "authors": ["Doru-Thom  Popovici", "Martin D. Schatz", "Franz  Franchetti", "Tze Meng Low"], "year": 2019, "n_citations": 1}
{"id": 4398545, "s2_id": "c6a1ea32f0ba550f0eb92a58fa88f00c5a616aee", "title": "Integer formula encoding SageTeX package", "abstract": "The following SageTeX document accompanies the papers [GZ, GRS], available from Gnang\u2019s websites. Please report bugs to gnang at cs dot rutgers dot edu. The most current version of the SageTeX document are available from Gnang\u2019s website", "venue": "ArXiv", "authors": ["Edinah K. Gnang"], "year": 2014, "n_citations": 1}
{"id": 4400246, "s2_id": "080055008545d67b8e95fccf892275fa02bde437", "title": "Some Comments on C. S. Wallace's Random Number Generators", "abstract": "We outline some of Chris Wallace\u2019s contributions to pseudo-random number generation. In particular, we consider his recent idea for generating normally distributed variates without relying on a source of uniform random numbers, and compare it with more conventional methods for generating normal random numbers. Implementations of Wallace\u2019s idea can be very fast (approximately as fast as good uniform generators). We discuss the statistical quality of the output, and mention how certain pitfalls can be avoided.", "venue": "Comput. J.", "authors": ["Richard P. Brent"], "year": 2008, "n_citations": 10}
{"id": 4402098, "s2_id": "f61b460b587683e35d662baf676c680df846e882", "title": "Computing Theta Functions with Julia", "abstract": "We present a new package Theta.jl for computing with the Riemann theta function. It is implemented in Julia and offers accurate numerical evaluation of theta functions with characteristics and their derivatives of arbitrary order. Our package is optimized for multiple evaluations of theta functions for the same Riemann matrix, in small dimensions. As an application, we report on experimental approaches to the Schottky problem in genus five.", "venue": "ArXiv", "authors": ["Daniele  Agostini", "Lynn  Chua"], "year": 2019, "n_citations": 9}
{"id": 4405654, "s2_id": "4d0ab977ab9fcbbd7a5910bc09346e99212db192", "title": "Pocket Guide to Solve Inverse Problems with GlobalBioIm", "abstract": "GlobalBioIm is an open-source MATLAB library for solving inverse problems. The library capitalizes on the strong commonalities between forward models to standardize the resolution of a wide range of imaging inverse problems. Endowed with an operator-algebra mechanism, GlobalBioIm allows one to easily solve inverse problems by combining elementary modules in a lego-like fashion. This user-friendly toolbox gives access to cutting-edge reconstruction algorithms, while its high modularity makes it easily extensible to new modalities and novel reconstruction methods. We expect GlobalBioIm to respond to the needs of imaging scientists looking for reliable and easy-to-use computational tools for solving their inverse problems. In this paper, we present in detail the structure and main features of the library. We also illustrate its flexibility with examples from multichannel deconvolution microscopy.", "venue": "Inverse Problems", "authors": ["Emmanuel  Soubies", "Ferr\u00e9ol  Soulez", "Michael T. McCann", "Thanh-An  Pham", "Laur\u00e8ne  Donati", "Thomas  Debarre", "Daniel  Sage", "Michael  Unser"], "year": 2019, "n_citations": 25}
{"id": 4406357, "s2_id": "46eb78e6a030dec53e822cc1ab19082690966bf6", "title": "Realms: A Structure for Consolidating Knowledge about Mathematical Theories", "abstract": "Since there are different ways of axiomatizing and developing a mathematical theory, knowledge about a such a theory may reside in many places and in many forms within a library of formalized mathematics. We introduce the notion of a realm as a structure for consolidating knowledge about a mathematical theory. A realm contains several axiomatizations of a theory that are separately developed. Views interconnect these developments and establish that the axiomatizations are equivalent in the sense of being mutually interpretable. A realm also contains an external interface that is convenient for users of the library who want to apply the concepts and facts of the theory without delving into the details of how the concepts and facts were developed. We illustrate the utility of realms through a series of examples. We also give an outline of the mechanisms that are needed to create and maintain realms.", "venue": "CICM", "authors": ["Jacques  Carette", "William M. Farmer", "Michael  Kohlhase"], "year": 2014, "n_citations": 13}
{"id": 4409328, "s2_id": "ca7fd29ec1460159815f8424f1dfb93177623efd", "title": "The JuliaConnectoR: a functionally oriented interface for integrating Julia in R", "abstract": "Like many groups considering the new programming language Julia, we faced the challenge of accessing the algorithms that we develop in Julia from R. Therefore, we developed the R package JuliaConnectoR, available from the CRAN repository and GitHub (this https URL), in particular for making advanced deep learning tools available. For maintainability and stability, we decided to base communication between R and Julia on TCP, using an optimized binary format for exchanging data. Our package also specifically contains features that allow for a convenient interactive use in R. This makes it easy to develop R extensions with Julia or to simply call functionality from Julia packages in R. With its functionally oriented design, the JuliaConnectoR enables a clean programming style by avoiding state in Julia that is not visible in the R workspace. We illustrate the further features of our package with code examples, and also discuss advantages over the two alternative packages JuliaCall and XRJulia. Finally, we demonstrate the usage of the package with a more extensive example for employing neural ordinary differential equations, a recent deep learning technique that has received much attention. This example also provides more general guidance for integrating deep learning techniques from Julia into R.", "venue": "ArXiv", "authors": ["Stefan  Lenz", "Maren  Hackenberg", "Harald  Binder"], "year": 2020, "n_citations": 2}
{"id": 4410702, "s2_id": "ec48a2d1833c3a96ad398aaaaa731d4f7f464e02", "title": "GPU Acceleration of Newton's Method for Large Systems of Polynomial Equations in Double Double and Quad Double Arithmetic", "abstract": "In order to compensate for the higher cost of double double and quad double arithmetic when solving large polynomial systems, we investigate the application of the NVIDIA Tesla K20C graphics processing unit (GPU). The focus on this paper is on Newton's method, which requires the evaluation of the polynomials, their derivatives, and the solution of a linear system to compute the update to the current approximation for the solution. The reverse mode of algorithmic differentiation for a product of variables is rewritten in a binary tree fashion so all threads in a block can collaborate in the computation. For double arithmetic, the evaluation and differentiation problem is memory bound, whereas for complex quad double arithmetic the problem is compute bound. With acceleration we can double the dimension and get results that are twice as accurate in about the same time.", "venue": "2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)", "authors": ["Jan  Verschelde", "Xiangcheng  Yu"], "year": 2014, "n_citations": 5}
{"id": 4413208, "s2_id": "a7118a1f2e390f1a06eb6543fcf82fd9e559266a", "title": "Computing accurate Horner form approximations to special functions in finite precision arithmetic", "abstract": "In various applications, computers are required to compute approximations to univariate elementary and special functions such as $\\exp$ and $\\arctan$ to modest accuracy. This paper proposes a new heuristic for automating the design of such implementations. This heuristic takes a certain restricted specification of program structure and the desired error properties as input and takes explicit account of roundoff error during evaluation.", "venue": "ArXiv", "authors": ["Tor G. J. Myklebust"], "year": 2015, "n_citations": 1}
{"id": 4413395, "s2_id": "2bd9a987717d00b36c140d7f7e01e0c1e6abc3ba", "title": "PRAND: GPU accelerated parallel random number generation library: Using most reliable algorithms and applying parallelism of modern GPUs and CPUs", "abstract": "The library PRAND for pseudorandom number generation for modern CPUs and GPUs is presented. It contains both single-threaded and multi-threaded realizations of a number of modern and most reliable generators recently proposed and studied in Barash (2011), Matsumoto and Tishimura (1998), L\u2019Ecuyer (1999,1999), Barash and Shchur (2006) and the efficient SIMD realizations proposed in Barash and Shchur (2011). One of the useful features for using PRAND in parallel simulations is the ability to initialize up to 10191019 independent streams. Using massive parallelism of modern GPUs and SIMD parallelism of modern CPUs substantially improves performance of the generators. \nProgram summary \nProgram title: PRAND \n \nCatalogue identifier: AESB_v1_0 \n \nProgram summary URL:http://cpc.cs.qub.ac.uk/summaries/AESB_v1_0.html \n \nProgram obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland \n \nLicensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html \n \nNo. of lines in distributed program, including test data, etc.: 45979 \n \nNo. of bytes in distributed program, including test data, etc.: 23953564 \n \nDistribution format: tar.gz \n \nProgramming language: Cuda C, Fortran. \n \nComputer: PC, workstation, laptop, or server with NVIDIA GPU (tested on Tesla X2070, Fermi C2050, GeForce GT540M) and with Intel or AMD processor. \n \nOperating system: Linux with CUDA version 5.0 or later. Should also run on MacOs, Windows, or UNIX. \n \nRAM: 4 Mbytes \n \nClassification: 4.13. \n \nNature of problem: \n \nAny calculation requiring uniform pseudorandom number generator, in particular, Monte Carlo calculations. Any calculation or simulation requiring uncorrelated parallel streams of uniform pseudorandom numbers. \n \nSolution method: \n \nThe library contains realization of a number of modern and reliable generators: MT19937, MRG32K3A and LFSR113. Also new realizations of the method based on parallel evolution of an ensemble of transformations of two-dimensional torus are included in the library: GM19, GM29, GM31, GM61, GM55, GQ58.1, GQ58.3 and GQ58.4. The library contains: single-threaded and multi-threaded realizations for GPU, single-threaded realizations for CPU, realizations for CPU based on SSE command set. Also, the library contains the abilities to jump ahead inside RNG sequence and to initialize independent random number streams with block splitting method for each of the RNGs. \n \nRestrictions: \n \nNvidia Cuda Toolkit version 5.0 or later should be installed. To use GPU realizations, Nvidia GPU supporting CUDA and the corresponding Nvidia driver should be installed. For SSE realizations of the generators, Intel or AMD CPU supporting SSE2 command set is required. In order to use the SSE realization of LFSR113, CPU must support SSE4 command set. \n \nAdditional comments: \n \nA version of this program, which only contains the realizations for CPUs, is held in the Library as Catalog Id., AEIT_v2_0 (RNGSSELIB). It does not require a GPU device or CUDA compiler. \n \nRunning time: \n \nThe tests and the examples included in the package take less or about one minute to run. Running time is analyzed in detail in Section\u00a0 8 of the paper.", "venue": "Comput. Phys. Commun.", "authors": ["Lev Yu. Barash", "Lev N. Shchur"], "year": 2014, "n_citations": 26}
{"id": 4416083, "s2_id": "030db0a2f489fba4d8eb53ae2c8ec29695cd2709", "title": "libdlr: Efficient imaginary time calculations using the discrete Lehmann representation", "abstract": "We introduce libdlr, a library implementing the recently introduced discrete Lehmann representation (DLR) of imaginary time Green\u2019s functions. The DLR basis consists of a collection of exponentials chosen by the interpolative decomposition to ensure stable and efficient recovery of Green\u2019s functions from imaginary time or Matsbuara frequency samples. The library provides subroutines to build the DLR basis and grids, and to carry out various standard operations. The simplicity of the DLR makes it straightforward to incorporate into existing codes as a replacement for less efficient representations of imaginary time Green\u2019s functions, and libdlr is intended to facilitate this process. libdlr is written in Fortran, and contains a Python module pydlr.", "venue": "ArXiv", "authors": ["Jason  Kaye", "Hugo U. R. Strand"], "year": 2021, "n_citations": 0}
{"id": 4420628, "s2_id": "7125ebc13fa5f0452da4a76b95a1457b649e13b0", "title": "An efficient hybrid tridiagonal divide-and-conquer algorithm on distributed memory architectures", "abstract": "Abstract In this paper, we propose an efficient divide-and-conquer (DC) algorithm for symmetric tridiagonal matrices based on ScaLAPACK and the hierarchically semiseparable (HSS) matrices. HSS is an important type of rank-structured matrices. The most computationally intensive part of the DC algorithm is computing the eigenvectors via matrix\u2013matrixmultiplications (MMM). In our parallel hybrid DC (PHDC) algorithm, MMM is accelerated by using HSS matrix techniques when the intermediate matrix is large. All the HSS computations are performed via the package STRUMPACK . PHDC has been tested by using many different matrices. Compared with the DC implementation in MKL, PHDC can be faster for some matrices with few deflations when using hundreds of processes. However, the gains decrease as the number of processes increases. The comparisons of PHDC with ELPA (the Eigenvalue soLvers for Petascale Applications library) are similar. PHDC is usually slower than MKL and ELPA when using 300 or more processes on the Tianhe-2 supercomputer.", "venue": "J. Comput. Appl. Math.", "authors": ["Shengguo  Li", "Fran\u00e7ois-Henry  Rouet", "Jie  Liu", "Chun  Huang", "Xingyu  Gao", "Xuebin  Chi"], "year": 2018, "n_citations": 61}
{"id": 4426031, "s2_id": "7a26494159c7e1858892f994d9a1256afcae2b9b", "title": "Making RooFit Ready for Run 3", "abstract": "RooFit and RooStats, the toolkits for statistical modelling in ROOT, are used in most searches and measurements at the Large Hadron Collider. The data to be collected in Run 3 will enable measurements with higher precision and models with larger complexity, but also require faster data processing. In this work, first results on modernising RooFit's collections, restructuring data flow and vectorising likelihood fits in RooFit will be discussed. These improvements will enable the LHC experiments to process larger datasets without having to compromise with respect to model complexity, as fitting times would increase significantly with the large datasets to be expected in Run 3.", "venue": "ArXiv", "authors": ["Stephan  Hageboeck", "Lorenzo  Moneta"], "year": 2020, "n_citations": 4}
{"id": 4429692, "s2_id": "11cced1ab64eecfa4bcd5572be940937929c0ec4", "title": "How good are MatLab, Octave and Scilab for Computational Modelling?", "abstract": "In this article we test the accuracy of three platforms used in computational modelling: MatLab, Octave and Scilab, running on i386 architecture and three operating systems (Windows, Ubuntu and Mac OS). We submitted them to numerical tests using standard data sets and using the functions provided by each platform. A Monte Carlo study was conducted in some of the datasets in order to verify the stability of the results with respect to small departures from the original input. We propose a set of operations which include the computation of matrix determinants and eigenvalues, whose results are known. We also used data provided by NIST (National Institute of Standards and Technology), a protocol which includes the computation of basic univariate statistics (mean, standard deviation and first-lag correlation), linear regression and extremes of probability distributions. The assessment was made comparing the results computed by the platforms with certified values, that is, known results, computing the number of correct significant digits. Mathematical subject classification: Primary: 06B10; Secondary: 06D05.", "venue": "ArXiv", "authors": ["Eliana S. de Almeida", "Antonio C. Medeiros", "Alejandro C\u00e9sar Frery"], "year": 2012, "n_citations": 7}
{"id": 4430419, "s2_id": "9f9d5596fea8c8b0948914397e9f5abb39dec074", "title": "A canonical form for some piecewise defined functions", "abstract": "We define a canonical form for piecewise defined functions. We show that this has a wider range of application as well as better complexity properties than previous work.", "venue": "ISSAC 2007", "authors": ["Jacques  Carette"], "year": 2007, "n_citations": 3}
{"id": 4432522, "s2_id": "d887a814701dc711bbae53c6adac0f4bd98ea21e", "title": "Accelerating the SpMV kernel on standard CPUs by exploiting the partially diagonal structures", "abstract": "Sparse Matrix Vector multiplication (SpMV) is one of basic building blocks in scientific computing, and acceleration of SpMV has been continuously required. In this research, we aim for accelerating SpMV on recent CPUs for sparse matrices that have a specific sparsity structure, namely a diagonally structured sparsity pattern. We focus a hybrid storage format that combines the DIA and CSR formats, so-called the HDC format. First, we recall the importance of introducing cache blocking techniques into HDC-based SpMV kernels. Next, based on the observation of the cache blocked kernel, we present a modified version of the HDC formats, which we call the M-HDC format, in which partial diagonal structures are expected to be more efficiently picked up. For these SpMV kernels, we theoretically analyze the expected performance improvement based on performance models. Then, we conduct comprehensive experiments on state-of-the-art multi-core CPUs. By the experiments using typical matrices, we clarify the detailed performance characteristics of each SpMV kernel. We also evaluate the performance for matrices appearing in practical applications and demonstrate that our approach can accelerate SpMV for some of them. Through the present paper, we demonstrate the effectiveness of exploiting partial diagonal structures by the M-HDC format as a promising approach to accelerating SpMV on CPUs for a certain kind of practical sparse matrices.", "venue": "ArXiv", "authors": ["Takeshi  Fukaya", "Koki  Ishida", "Akie  Miura", "Takeshi  Iwashita", "Hiroshi  Nakashima"], "year": 2021, "n_citations": 0}
{"id": 4441091, "s2_id": "b3bb8bb842798c9d1a4ac1a6de18fbe8d005df0b", "title": "About the generalized LM-inverse and the weighted Moore-Penrose inverse", "abstract": "The recursive method for computing the generalized LM-inverse of a constant rectangular matrix augmented by a column vector is proposed in Udwadia and Phohomsiri (2007) [16,17]. The corresponding algorithm for the sequential determination of the generalized LM-inverse is established in the present paper. We prove that the introduced algorithm for computing the generalized LM-inverse and the algorithm for the computation of the weighted Moore-Penrose inverse developed by Wang and Chen (1986) in [23] are equivalent algorithms. Both of the algorithms are implemented in the present paper using the package MATHEMATICA. Several rational test matrices and randomly generated constant matrices are tested and the CPU time is compared and discussed.", "venue": "Appl. Math. Comput.", "authors": ["Milan B. Tasic", "Predrag S. Stanimirovic", "Selver H. Pepic"], "year": 2010, "n_citations": 5}
{"id": 4443164, "s2_id": "90b62e3797e912b75663783e3c74b5b17961e23e", "title": "Standards for graph algorithm primitives", "abstract": "It is our view that the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks. This paper is a position paper defining the problem and announcing our intention to launch an open effort to define this standard.", "venue": "2013 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Timothy G. Mattson", "David A. Bader", "Jonathan W. Berry", "Aydin  Bulu\u00e7", "Jack J. Dongarra", "Christos  Faloutsos", "John  Feo", "John R. Gilbert", "Joseph  Gonzalez", "Bruce  Hendrickson", "Jeremy  Kepner", "Charles E. Leiserson", "Andrew  Lumsdaine", "David A. Padua", "Stephen W. Poole", "Steven P. Reinhardt", "Michael  Stonebraker", "Steve  Wallach", "Andrew  Yoo"], "year": 2013, "n_citations": 72}
{"id": 4444816, "s2_id": "1234c69e05ce719f50eb46914de76f75278fc79f", "title": "Hyper-systolic matrix multiplication", "abstract": "Abstract A novel parallel algorithm for matrix multiplication is presented. It is based on a 1-D hyper-systolic processor abstraction. The procedure can be implemented on all types of parallel systems.", "venue": "Parallel Comput.", "authors": ["Thomas  Lippert", "Nicolai  Petkov", "Paolo  Palazzari", "Klaus  Schilling"], "year": 2001, "n_citations": 16}
{"id": 4446119, "s2_id": "a0e3e4e2d5e4f10ade7b4c3c1541c7c52dfa996e", "title": "TRIOT: Faster tensor manipulation in C++11", "abstract": "[abridged] Context: Multidimensional arrays are used by many different algorithms. As such, indexing and broadcasting complex operations over multidimensional arrays are ubiquitous tasks and can be performance limiting. Inquiry: Simultaneously indexing two or more multidimensional arrays with different shapes (e.g., copying data from one tensor to another larger, zero padded tensor in anticipation of a convolution) is difficult to do efficiently: Hard-coded nested for loops in C, Fortran, and Go cannot be applied when the dimension of a tensor is unknown at compile time. Likewise, boost::multi_array cannot be used unless the dimensions of the array are known at compile time, and the style of implementation restricts the user from using the index tuple inside a vectorized operation (as would be required to compute an expected value of a multidimensional distribution). On the other hand, iteration methods that do not require the dimensionality or shape to be known at compile time (e.g., incrementing and applying carry operations to index tuples or remapping integer indices in the flat array), can be substantially slower than hard-coded nested for loops. ... Importance: Manipulation of multidimensional arrays is a common task in software, especially in high performance numerical methods. This paper proposes a novel way to leverage template recursion to iterate over and apply operations to multidimensional arrays, and then demonstrates the superior performance and flexibility of operations that can be achieved using this new approach.", "venue": "Art Sci. Eng. Program.", "authors": ["Oliver  Serang", "Florian  Heyl"], "year": 2017, "n_citations": 1}
{"id": 4447409, "s2_id": "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "title": "Scikit-learn: Machine Learning in Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.", "venue": "J. Mach. Learn. Res.", "authors": ["Fabian  Pedregosa", "Ga\u00ebl  Varoquaux", "Alexandre  Gramfort", "Vincent  Michel", "Bertrand  Thirion", "Olivier  Grisel", "Mathieu  Blondel", "Peter  Prettenhofer", "Ron  Weiss", "Vincent  Dubourg", "Jacob  VanderPlas", "Alexandre  Passos", "David  Cournapeau", "Matthieu  Brucher", "Matthieu  Perrot", "Edouard  Duchesnay"], "year": 2011, "n_citations": 38914}
{"id": 4449656, "s2_id": "dc6438ea0edf261ce8325c061ed238e217d13fa9", "title": "Computational science and re-discovery: open-source implementations of ellipsoidal harmonics for problems in potential theory", "abstract": "We present two open-source (BSD) implementations of ellipsoidal harmonic expansions for solving problems of potential theory using separation of variables. Ellipsoidal harmonics are used surprisingly infrequently, considering their substantial value for problems ranging in scale from molecules to the entire solar system. In this article, we suggest two possible reasons for the paucity relative to spherical harmonics. The first is essentially historical---ellipsoidal harmonics developed during the late 19th century and early 20th, when it was found that only the lowest-order harmonics are expressible in closed form. Each higher-order term requires the solution of an eigenvalue problem, and tedious manual computation seems to have discouraged applications and theoretical studies. The second explanation is practical: even with modern computers and accurate eigenvalue algorithms, expansions in ellipsoidal harmonics are significantly more challenging to compute than those in Cartesian or spherical coordinates. The present implementations reduce the \"barrier to entry\" by providing an easy and free way for the community to begin using ellipsoidal harmonics in actual research. We demonstrate our implementation using the specific and physiologically crucial problem of how charged proteins interact with their environment, and ask: what other analytical tools await re-discovery in an era of inexpensive computation?", "venue": "ArXiv", "authors": ["Jaydeep P. Bardhan", "Matthew G. Knepley"], "year": 2012, "n_citations": 17}
{"id": 4450938, "s2_id": "aa18cb8c6c87487e752ecde9e6127ab85d452fe6", "title": "sin[n Delta t sin (n Delta t1)] as a source of unpredictable dynamics", "abstract": "We investigate the ability of the function sin[n Delta t sin (n Delta t1)], where n is an integer and growing number, to produce unpredictable sequences of numbers. Classical mathematical tools for distinguishing periodic from chaotic or random behaviour, such as sensitivity to the initial conditions, Fourier analysis, and autocorrelation are used. Moreover, the function acos{sin[n Delta t sin (n Delta t1)]}/pigreek is introduced to have an uniform density of numbers in the interval [0,1], so it can be submitted to a battery of widely used tests for random number generators. All these tools show that a proper choice of Delta t and Delta t1, can produce a sequence of numbers behaving as unpredictable dynamics.", "venue": "ArXiv", "authors": ["Stefano  Morosetti"], "year": 2011, "n_citations": 0}
{"id": 4451378, "s2_id": "2be551a16e3f50553c3532a09c089150ab5ca59c", "title": "Quasi-Monte Carlo Software", "abstract": "Practitioners wishing to experience the efficiency gains from using low discrepancy sequences need correct, robust, well-written software. This article, based on our MCQMC 2020 tutorial, describes some of the better quasi-Monte Carlo (QMC) software available. We highlight the key software components required by QMC to approximate multivariate integrals or expectations of functions of vector random variables. We have combined these components in QMCPy, a Python open-source library, which we hope will draw the support of the QMC community. Here we introduce QMCPy. Sou-Cheng T. Choi Department of Applied Mathematics, Illinois Institute of Technology, RE 220, 10 W. 32nd St., Chicago, IL 60616; and Kamakura Corporation, 2222 Kalakaua Ave, Suite 1400, Honolulu, HI 96815 e-mail: schoi32@iit.edu Fred J. Hickernell Center for Interdisciplinary Scientific Computation and Department of Applied Mathematics, Illinois Institute of Technology RE 220, 10 W. 32nd St., Chicago, IL 60616 e-mail: hickernell@iit.edu R. Jagadeeswaran Department of Applied Mathematics, Illinois Institute of Technology, RE 220, 10 W. 32nd St., Chicago, IL 60616 e-mail: jrathin1@iit.edu; and Wi-Tronix LLC, 631 E Boughton Rd, Suite 240, Bolingbrook, IL 60440 Michael J. McCourt SigOpt, an Intel company, 100 Bush St., Suite 1100, San Francisco, CA 94104 e-mail: mccourt@sigopt.com Aleksei G. Sorokin Department of Applied Mathematics, Illinois Institute of Technology, RE 220, 10 W. 32nd St., Chicago, IL 60616 e-mail: asorokin@hawk.iit.edu 1 ar X iv :2 10 2. 07 83 3v 2 [ cs .M S] 2 9 Se p 20 21 2 S.-C. T. Choi et al.", "venue": "ArXiv", "authors": ["Sou-Cheng T. Choi", "Fred J. Hickernell", "R.  Jagadeeswaran", "Michael J. McCourt", "Aleksei G. Sorokin"], "year": 2021, "n_citations": 3}
{"id": 4455566, "s2_id": "394c5d8528a2921d865e708bcfdbe6db1f416d52", "title": "Probabilistic Inference Using Generators - The Statues Algorithm", "abstract": "We present here a new probabilistic inference algorithm that gives exact results in the domain of discrete probability distributions. This algorithm, named the Statues algorithm, calculates the marginal probability distribution on probabilistic models defined as direct acyclic graphs. These models are made up of well-defined primitives that allow to express, in particular, joint probability distributions, Bayesian networks, discrete Markov chains, conditioning and probabilistic arithmetic. The Statues algorithm relies on a variable binding mechanism based on the generator construct, a special form of coroutine; being related to the enumeration algorithm, this new algorithm brings important improvements in terms of efficiency, which makes it valuable in regard to other exact marginalization algorithms. After introduction of several definitions, primitives and compositional rules, we present in details the Statues algorithm. Then, we briefly discuss the interest of this algorithm compared to others and we present possible extensions. Finally, we introduce Lea and MicroLea, two Python libraries implementing the Statues algorithm, along with several use cases. A proof of the correctness of the algorithm is provided in appendix.", "venue": "SAI", "authors": ["Pierre  Denis"], "year": 2020, "n_citations": 0}
{"id": 4458204, "s2_id": "e77b10da5078e50b49a9cb618e17c8a02f0614e5", "title": "Increasing the Efficiency of Sparse Matrix-Matrix Multiplication with a 2.5D Algorithm and One-Sided MPI", "abstract": "Matrix-matrix multiplication is a basic operation in linear algebra and an essential building block for a wide range of algorithms in various scientific fields. Theory and implementation for the dense, square matrix case are well-developed. If matrices are sparse, with application-specific sparsity patterns, the optimal implementation remains an open question. Here, we explore the performance of communication reducing 2.5D algorithms and one-sided MPI communication in the context of linear scaling electronic structure theory. In particular, we extend the DBCSR sparse matrix library, which is the basic building block for linear scaling electronic structure theory and low scaling correlated methods in CP2K. The library is specifically designed to efficiently perform block-sparse matrix-matrix multiplication of matrices with a relatively large occupation. Here, we compare the performance of the original implementation based on Cannon's algorithm and MPI point-to-point communication, with an implementation based on MPI one-sided communications (RMA), in both a 2D and a 2.5D approach. The 2.5D approach trades memory and auxiliary operations for reduced communication, which can lead to a speedup if communication is dominant. The 2.5D algorithm is somewhat easier to implement with one-sided communications. A detailed description of the implementation is provided, also for non ideal processor topologies, since this is important for actual applications. Given the importance of the precise sparsity pattern, and even the actual matrix data, which decides the effective fill-in upon multiplication, the tests are performed within the CP2K package with application benchmarks. Results show a substantial boost in performance for the RMA based 2.5D algorithm, up to 1.80x, which is observed to increase with the number of processes involved in the parallelization.", "venue": "PASC", "authors": ["Alfio  Lazzaro", "Joost  VandeVondele", "J\u00fcrg  Hutter", "Ole  Sch\u00fctt"], "year": 2017, "n_citations": 19}
{"id": 4460181, "s2_id": "5faea49feb5e0176709ece161799a28ce6dacb5c", "title": "The GF Mathematics Library", "abstract": "This paper is devoted to present the Mathematics Grammar Library, a system for multilingual mathematical text processing. We explain the context in which it originated, its current design and functionality and the current development goals. We also present two prototype services and comment on possible future applications in the area of artificial mathematics assistants.", "venue": "ThEdu", "authors": ["Jordi  Saludes", "Sebastian  Xamb\u00f3"], "year": 2011, "n_citations": 10}
{"id": 4461551, "s2_id": "482ba656579580d27e4f308c15a4e7d28044e8b2", "title": "semopy 2: A Structural Equation Modeling Package with Random Effects in Python", "abstract": "Structural Equation Modeling (SEM) is an umbrella term that includes numerous multivariate statistical techniques that are employed throughout a plethora of research areas, ranging from social to natural sciences. Until recently, SEM software was either commercial or restricted to niche languages, and the lack of SEM packages compatible with more mainstream programming languages was dire. To combat that, we introduced a Python package semopy v1 that surpassed other state-of-the-art software in terms of performance and estimation accuracy. Yet, it was lacking in functionality and its usage was burdened with unnecessary boilerplate code. Here, we introduce a complete overhaul of semopy that improves upon the previous results and comes with lots of new capabilities. Furthermore, we propose a novel SEM model that combines in itself a notion of random effects from linear mixed models (LMMs) to model numerous phenomena, such as spatial data, time series or population stratification in genetics.", "venue": "ArXiv", "authors": ["Georgy  Meshcheryakov", "Anna A. Igolkina", "Maria G. Samsonova"], "year": 2021, "n_citations": 0}
{"id": 4463009, "s2_id": "e5088d4d4866147babd3d7709f0fad9c60a552e9", "title": "The Parma Polyhedra Library: Toward a complete set of numerical abstractions for the analysis and verification of hardware and software systems", "abstract": "Since its inception as a student project in 2001, initially just for the handling (as the name implies) of convex polyhedra, the Parma Polyhedra Library has been continuously improved and extended by joining scrupulous research on the theoretical foundations of (possibly non-convex) numerical abstractions to a total adherence to the best available practices in software development. Even though it is still not fully mature and functionally complete, the Parma Polyhedra Library already offers a combination of functionality, reliability, usability and performance that is not matched by similar, freely available libraries. In this paper, we present the main features of the current version of the library, emphasizing those that distinguish it from other similar libraries and those that are important for applications in the field of analysis and verification of hardware and software systems.", "venue": "Sci. Comput. Program.", "authors": ["Roberto  Bagnara", "Patricia M. Hill", "Enea  Zaffanella"], "year": 2008, "n_citations": 462}
{"id": 4473567, "s2_id": "02ca8227638d1f1d0da8831163c8cc734ae715ff", "title": "A New Vectorization Technique for Expression Templates in C++", "abstract": "Vector operations play an important role in high performance computing and are typically provided by highly optimized libraries that implement the Basic Linear Algebra Subprograms (BLAS) interface. In C++ templates and operator overloading allow the implementation of these vector operations as expression templates which construct custom loops at compile time and providing a more abstract interface. Unfortunately existing expression template libraries lack the performance of fast BLAS implementations. This paper presents a new approach - Statically Accelerated Loop Templates (SALT) - to close this performance gap by combining expression templates with an aggressive loop unrolling technique. Benchmarks were conducted using the Intel C++ compiler and GNU Compiler Collection to assess the performance of our library relative to Intel's Math Kernel Library as well as the Eigen template library. The results show that the approach is able to provide optimization comparable to the fastest available BLAS implementations, while retaining the convenience and flexibility of a template library.", "venue": "ArXiv", "authors": ["J.  Progsch", "Yves  Ineichen", "Andreas  Adelmann"], "year": 2011, "n_citations": 3}
{"id": 4475856, "s2_id": "428b2a5501575cdf6fe63096aa17eca1c02ef051", "title": "Zgoubi: A startup guide for the complete beginner", "abstract": "Zgoubi is a code which can be used to model accelerators and beam lines, comprised of magnetic and electrostatic elements. It has been extensively developed since the mid-1980s to include circular accelerators and related beam physics. It has been made freely available by its author on a code development site, including a Users' Guide, a data treatment/graphic interfacing tool, and many examples. This startup guide give directions to install the required elements onto a Windows or Unix system to enable running of the Zgoubi code with examples of code written to model the EMMA accelerator based at the Cockcroft Institute.", "venue": "ArXiv", "authors": ["Annette  Pressman", "Kai  Hock"], "year": 2014, "n_citations": 1}
{"id": 4477958, "s2_id": "a4fae0df73757ba4580513f9b2a5ebdd0a6360d2", "title": "Large Scale Parallel Computations in R through Elemental", "abstract": "Even though in recent years the scale of statistical analysis problems has increased tremendously, many statistical software tools are still limited to single-node computations. However, statistical analyses are largely based on dense linear algebra operations, which have been deeply studied, optimized and parallelized in the high-performance-computing community. To make high-performance distributed computations available for statistical analysis, and thus enable large scale statistical computations, we introduce RElem, an open source package that integrates the distributed dense linear algebra library Elemental into R. While on the one hand, RElem provides direct wrappers of Elemental's routines, on the other hand, it overloads various operators and functions to provide an entirely native R experience for distributed computations. We showcase how simple it is to port existing R programs to Relem and demonstrate that Relem indeed allows to scale beyond the single-node limitation of R with the full performance of Elemental without any overhead.", "venue": "ArXiv", "authors": ["Rodrigo  Canales", "Elmar  Peise", "Paolo  Bientinesi"], "year": 2016, "n_citations": 2}
{"id": 4479063, "s2_id": "f70b50df154e26bdc3210e559baec2ed8bc7b5ad", "title": "The Graph Grammar Library - a generic framework for chemical graph rewrite systems", "abstract": "Graph rewrite systems are powerful tools to model and study complex problems in various fields of research. Their successful application to chemical reaction modelling on a molecular level was shown but no appropriate and simple system is available at the moment. \nThe presented Graph Grammar Library (GGL) implements a generic Double Push Out approach for general graph rewrite systems. The framework focuses on a high level of modularity as well as high performance, using state-of-the-art algorithms and data structures, and comes with extensive documentation. The large GGL chemistry module enables extensive and detailed studies of chemical systems. It well meets the requirements and abilities envisioned by Yadav et al. (2004) for such chemical rewrite systems. Here, molecules are represented as undirected labeled graphs while chemical reactions are described by according graph grammar rules. Beside the graph transformation, the GGL offers advanced cheminformatics algorithms for instance to estimate energies ofmolecules or aromaticity perception. These features are illustrated using a set of reactions from polyketide chemistry a huge class of natural compounds of medical relevance. \nThe graph grammar based simulation of chemical reactions offered by the GGL is a powerful tool for extensive cheminformatics studies on a molecular level. The GGL already provides rewrite rules for all enzymes listed in the KEGG LIGAND database is freely available at this http URL", "venue": "ICMT", "authors": ["Martin  Mann", "Heinz  Ekker", "Christoph  Flamm"], "year": 2013, "n_citations": 18}
{"id": 4483535, "s2_id": "c9e827d5ad5c9f6a11992522f45a4423bc148ce9", "title": "Grid: A next generation data parallel C++ QCD library", "abstract": "In this proceedings we discuss the motivation, implementation details, and performance of a new physics code base called Grid. It is intended to be more performant, more general, but similar in spirit to QDP++\\cite{QDP}. Our approach is to engineer the basic type system to be consistently fast, rather than bolt on a few optimised routines, and we are attempt to write all our optimised routines directly in the Grid framework. It is hoped this will deliver best known practice performance across the next generation of supercomputers, which will provide programming challenges to traditional scalar codes. \nWe illustrate the programming patterns used to implement our goals, and advances in productivity that have been enabled by using new features in C++11.", "venue": "ArXiv", "authors": ["Peter A. Boyle", "Azusa  Yamaguchi", "Guido  Cossu", "Antonin  Portelli"], "year": 2015, "n_citations": 57}
{"id": 4487104, "s2_id": "eb6889a1b69058ed00a3e703239af0171a425e0d", "title": "Mill.jl and JsonGrinder.jl: automated differentiable feature extraction for learning from raw JSON data", "abstract": "Learning from raw data input, thus limiting the need for manual feature engineering, is one of the key components of many successful applications of machine learning methods. While machine learning problems are often formulated on data that naturally translate into a vector representation suitable for classifiers, there are data sources, for example in cybersecurity, that are naturally represented in diverse files with a unifying hierarchical structure, such as XML, JSON, and Protocol Buffers. Converting this data to vector (tensor) representation is generally done by manual feature engineering, which is laborious, lossy, and prone to human bias about the importance of particular features. Mill.jl and JsonGrinder.jl is a tandem of libraries, which fully automates the conversion. Starting with an arbitrary set of JSON samples, they create a differentiable machine learning model capable of infer from further JSON samples in their raw form. In the spirit of the Julia language, the framework is split into two packages \u2014 Mill.jl implementing the hierarchical multiple instance learning paradigm, offering a theoretically justified approach for building machine learning models for this type of data, and JsonGrinder.jl summarizing the structure in a set of JSON samples and reflecting it in a Mill.jl model.", "venue": "ArXiv", "authors": ["Simon  Mandlik", "Matej  Racinsky", "Viliam  Lisy", "Tomas  Pevny"], "year": 2021, "n_citations": 0}
{"id": 4498526, "s2_id": "05272de903f0d6ce2bfe6651b53e9147d0d233a5", "title": "Nemo/Hecke: Computer Algebra and Number Theory Packages for the Julia Programming Language", "abstract": "We introduce two new packages, Nemo and Hecke, written in the Julia programming language for computer algebra and number theory. We demonstrate that high performance generic algorithms can be implemented in Julia, without the need to resort to a low-level C implementation. For specialised algorithms, we use Julia's efficient native C interface to wrap existing C/C++ libraries such as Flint, Arb, Antic and Singular. We give examples of how to use Hecke and Nemo and discuss some algorithms that we have implemented to provide high performance basic arithmetic.", "venue": "ISSAC", "authors": ["Claus  Fieker", "William  Hart", "Tommy  Hofmann", "Fredrik  Johansson"], "year": 2017, "n_citations": 52}
{"id": 4498918, "s2_id": "8d06c80dd077d25212759007a09b8375dacfeada", "title": "QDOT: Quantized Dot Product Kernel for Approximate High-Performance Computing", "abstract": "Approximate computing techniques have been successful in reducing computation and power costs in several domains. However, error sensitive applications in high-performance computing are unable to benefit from existing approximate computing strategies that are not developed with guaranteed error bounds. While approximate computing techniques can be developed for individual high-performance computing applications by domain specialists, this often requires additional theoretical analysis and potentially extensive software modification. Hence, the development of lowlevel error-bounded approximate computing strategies that can be introduced into any high-performance computing application without requiring additional analysis or significant software alterations is desirable. In this paper, we provide a contribution in this direction by proposing a general framework for designing error-bounded approximate computing strategies and apply it to the dot product kernel to develop qdot \u2014 an error-bounded approximate dot product kernel. Following the introduction of qdot, we perform a theoretical analysis that yields a deterministic bound on the relative approximation error introduced by qdot. Empirical tests are performed to illustrate the tightness of the derived error bound and to demonstrate the effectiveness of qdot on a synthetic dataset, as well as two scientific benchmarks \u2013 Conjugate Gradient (CG) and the Power method. In particular, using qdot for the dot products in CG can result in a majority of components being perforated or quantized to half precision without increasing the iteration count required for convergence to the same solution as CG using a double precision dot product.", "venue": "ArXiv", "authors": ["James  Diffenderfer", "Daniel  Osei-Kuffuor", "Harshitha  Menon"], "year": 2021, "n_citations": 0}
{"id": 4501527, "s2_id": "c213a65c3a37b5e0f8cef4d042acce11c4fa6b50", "title": "The formulator MathML editor project: user-friendly authoring of content markup documents", "abstract": "Implementation of an editing process for Content MathML formulas in common visual style is a real challenge for a software developer who does not really want the user to have to understand the structure of Content MathML in order to edit an expression, since it is expected that users are often not that technically minded. In this paper, we demonstrate how this aim is achieved in the context of the Formulator project and discuss features of this MathML editor, which provides a user with a WYSIWYG editing style while authoring MathML documents with Content or mixed markup. We also present the approach taken to enhance availability of the MathML editor to end-users, demonstrating an online version of the editor that runs inside a Web browser.", "venue": "AISC'10/MKM'10/Calculemus'10", "authors": ["Andriy  Kovalchuk", "Vyacheslav  Levitsky", "Igor  Samolyuk", "Valentyn  Yanchuk"], "year": 2010, "n_citations": 5}
{"id": 4506517, "s2_id": "dc600857dca13084b2e6bde37b0c8e8dbe4748a6", "title": "Highly Scalable Multiplication for Distributed Sparse Multivariate Polynomials on Many-Core Systems", "abstract": "We present a highly scalable algorithm for multiplying sparse multivariate polynomials represented in a distributed format. This algorithm targets not only the shared memory multicore computers, but also computers clusters or specialized hardware attached to a host computer, such as graphics processing units or many-core coprocessors. The scalability on the large number of cores is ensured by the lacks of synchronizations, locks and false-sharing during the main parallel step.", "venue": "CASC", "authors": ["Micka\u00ebl  Gastineau", "Jacques  Laskar"], "year": 2013, "n_citations": 15}
{"id": 4517295, "s2_id": "ec79cc479e743b6264ef521b202c7ebda3a904c3", "title": "Energy efficiency of finite difference algorithms on multicore CPUs, GPUs, and Intel Xeon Phi processors", "abstract": "In addition to hardware wall-time restrictions commonly seen in high-performance computing systems, it is likely that future systems will also be constrained by energy budgets. In the present work, finite difference algorithms of varying computational and memory intensity are evaluated with respect to both energy efficiency and runtime on an Intel Ivy Bridge CPU node, an Intel Xeon Phi Knights Landing processor, and an NVIDIA Tesla K40c GPU. The conventional way of storing the discretised derivatives to global arrays for solution advancement is found to be inefficient in terms of energy consumption and runtime. In contrast, a class of algorithms in which the discretised derivatives are evaluated on-the-fly or stored as thread-/process-local variables (yielding high compute intensity) is optimal both with respect to energy consumption and runtime. On all three hardware architectures considered, a speed-up of ~2 and an energy saving of ~2 are observed for the high compute intensive algorithms compared to the memory intensive algorithm. The energy consumption is found to be proportional to runtime, irrespective of the power consumed and the GPU has an energy saving of ~5 compared to the same algorithm on a CPU node.", "venue": "ArXiv", "authors": ["Satya P. Jammy", "Christian T. Jacobs", "David J. Lusher", "Neil D. Sandham"], "year": 2017, "n_citations": 2}
{"id": 4518296, "s2_id": "adc0c3ed4bf604089a23fe25e9aa17afffa231b8", "title": "Can Fortran's 'do concurrent' replace directives for accelerated computing?", "abstract": "Recently, there has been growing interest in using standard language constructs (e.g. C++\u2019s Parallel Algorithms and Fortran\u2019s do concurrent) for accelerated computing as an alternative to directivebased APIs (e.g. OpenMP and OpenACC). These constructs have the potential to be more portable, and some compilers already (or have plans to) support such standards. Here, we look at the current capabilities, portability, and performance of replacing directives with Fortran\u2019s do concurrent using a mini-app that currently implements OpenACC for GPU-acceleration and OpenMP for multi-core CPU parallelism. We replace as many directives as possible with do concurrent, testing various configurations and compiler options within three major compilers: GNU\u2019s gfortran, NVIDIA\u2019s nvfortran, and Intel\u2019s ifort. We find that with the right compiler versions and flags, many directives can be replaced without loss of performance or portability, and, in the case of nvfortran, they can all be replaced. We discuss limitations that may apply to more complicated codes and future language additions that may mitigate them. The software and Singularity containers are publicly provided to allow the results to be reproduced.", "venue": "ArXiv", "authors": ["Miko M. Stulajter", "Ronald M. Caplan", "Jon A. Linker"], "year": 2021, "n_citations": 0}
{"id": 4522501, "s2_id": "22f911f5f86436cb7ae049994a4cdcbd9ad79e3b", "title": "Proceedings of the 7th European Conference on Python in Science (EuroSciPy 2014)", "abstract": "These are the proceedings of the 7th European Conference on Python in Science, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).", "venue": "ArXiv", "authors": ["Pierre de Buyl", "Nelle  Varoquaux"], "year": 2014, "n_citations": 0}
{"id": 4522691, "s2_id": "a9f1686a307cb234a6c49686d20d460b7e0f7894", "title": "Core Imaging Library - Part II: multichannel reconstruction for dynamic and spectral tomography", "abstract": "The newly developed core imaging library (CIL) is a flexible plug and play library for tomographic imaging with a specific focus on iterative reconstruction. CIL provides building blocks for tailored regularized reconstruction algorithms and explicitly supports multichannel tomographic data. In the first part of this two-part publication, we introduced the fundamentals of CIL. This paper focuses on applications of CIL for multichannel data, e.g. dynamic and spectral. We formalize different optimization problems for colour processing, dynamic and hyperspectral tomography and demonstrate CIL\u2019s capabilities for designing state-of-the-art reconstruction methods through case studies and code snapshots. This article is part of the theme issue \u2018Synergistic tomographic image reconstruction: part 2\u2019.", "venue": "Philosophical Transactions of the Royal Society A", "authors": ["Evangelos  Papoutsellis", "Evelina  Ametova", "Claire  Delplancke", "Gemma  Fardell", "Jakob S. J\u00f8rgensen", "Edoardo  Pasca", "Martin  Turner", "Ryan  Warr", "William R. B. Lionheart", "Philip J. Withers"], "year": 2021, "n_citations": 7}
{"id": 4532560, "s2_id": "f5cc036a82e0994f7abf0404b4a562e2266a04f7", "title": "A High Performance Implementation of Spectral Clustering on CPU-GPU Platforms", "abstract": "Spectral clustering is one of the most popular graph clustering algorithms, which achieves the best performance for many scientific and engineering applications. However, existing implementations in commonly used software platforms such as Matlab and Python do not scale well for many of the emerging Big Data applications. In this paper, we present a fast implementation of the spectral clustering algorithm on a CPU-GPU heterogeneous platform. Our implementation takes advantage of the computational power of the multi-core CPU and the massive multithreading and SIMD capabilities of GPUs. Given the input as data points in high dimensional space, we propose a parallel scheme to build a sparse similarity graph represented in a standard sparse representation format. Then we compute the smallest k eigenvectors of the Laplacian matrix by utilizing the reverse communication interfaces of ARPACK software and cuSPARSE library, where k is typically very large. Moreover, we implement a very fast parallelized k-means algorithm on GPUs. Our implementation is shown to be significantly faster compared to the best known Matlab and Python implementations for each step. In addition, our algorithm scales to problems with a very large number of clusters.", "venue": "2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Yu  Jin", "Joseph  J\u00e1J\u00e1"], "year": 2016, "n_citations": 17}
{"id": 4538676, "s2_id": "f77e18dcbe83d88121249ec36ca87ed684264552", "title": "Studies on the energy and deep memory behaviour of a cache-oblivious, task-based hyperbolic PDE solver", "abstract": "We study the performance behaviour of a seismic simulation using the ExaHyPE engine with a specific focus on memory characteristics and energy needs. ExaHyPE combines dynamically adaptive mesh refinement (AMR) with ADER-DG. It is parallelized using tasks, and it is cache efficient. AMR plus ADER-DG yields a task graph which is highly dynamic in nature and comprises both arithmetically expensive tasks and tasks which challenge the memory\u2019s latency. The expensive tasks and thus the whole code benefit from AVX vectorization, although we suffer from memory access bursts. A frequency reduction of the chip improves the code\u2019s energy-to-solution. Yet, it does not mitigate burst effects. The bursts\u2019 latency penalty becomes worse once we add Intel Optane technology, increase the core count significantly or make individual, computationally heavy tasks fall out of close caches. Thread overbooking to hide away these latency penalties becomes contra-productive with noninclusive caches as it destroys the cache and vectorization character. In cases where memory-intense and computationally expensive tasks overlap, ExaHyPE\u2019s cache-oblivious implementation nevertheless can exploit deep, noninclusive, heterogeneous memory effectively, as main memory misses arise infrequently and slow down only few cores. We thus propose that upcoming supercomputing simulation codes with dynamic, inhomogeneous task graphs are actively supported by thread runtimes in intermixing tasks of different compute character, and we propose that future hardware actively allows codes to downclock the cores running particular task types.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Dominic E Charrier", "Benjamin  Hazelwood", "Ekaterina  Tutlyaeva", "Michael  Bader", "Michael  Dumbser", "Andrey  Kudryavtsev", "Alexander  Moskovsky", "Tobias  Weinzierl"], "year": 2019, "n_citations": 11}
{"id": 4542220, "s2_id": "37461268bdec2e9b8ef1e551a74c5ca3ff148aba", "title": "A Java Implementation of the SGA, UMDA, ECGA, and HBOA", "abstract": "The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm, the Extended Compact Genetic Algorithm, and the Hierarchical Bayesian Optimization Algorithm are all well known Evolutionary Algorithms. \nIn this report we present a Java implementation of these four algorithms with detailed instructions on how to use each of them to solve a given set of optimization problems. Additionally, it is explained how to implement and integrate new problems within the provided set. The source and binary files of the Java implementations are available for free download at this https URL", "venue": "ArXiv", "authors": ["Jos\u00e9 C. Pereira", "Fernando G. Lobo"], "year": 2015, "n_citations": 1}
{"id": 4549023, "s2_id": "345bd65a9380feda3fee18b62095cb0878a1e0a0", "title": "A Method for Solving Cyclic Block Penta-diagonal Systems of Linear Equations", "abstract": "A method for solving cyclic block three-diagonal systems of equations is generalized for solving a block cyclic penta-diagonal system of equations. Introducing a special form of two new variables the original system is split into three block pentagonal systems, which can be solved by the known methods. As such method belongs to class of direct methods without pivoting. Implementation of the algorithm is discussed in some details and the numerical examples are present.", "venue": "ArXiv", "authors": ["Milan  Batista"], "year": 2008, "n_citations": 8}
{"id": 4550788, "s2_id": "af265990ea9cf8c7a52ed63e993fc16ccf1253fa", "title": "HOL Light QE", "abstract": "We are interested in algorithms that manipulate mathematical expressions in mathematically meaningful ways. Expressions are syntactic, but most logics do not allow one to discuss syntax. ${\\rm CTT}_{\\rm qe}$ is a version of Church's type theory that includes quotation and evaluation operators, akin to quote and eval in the Lisp programming language. Since the HOL logic is also a version of Church's type theory, we decided to add quotation and evaluation to HOL Light to demonstrate the implementability of ${\\rm CTT}_{\\rm qe}$ and the benefits of having quotation and evaluation in a proof assistant. The resulting system is called HOL Light QE. Here we document the design of HOL Light QE and the challenges that needed to be overcome. The resulting implementation is freely available.", "venue": "ITP", "authors": ["Jacques  Carette", "William M. Farmer", "Patrick  Laskowski"], "year": 2018, "n_citations": 5}
{"id": 4552903, "s2_id": "492c95a8981c4397db510bfb35f4f7ddbf13d29b", "title": "Visualizing Marden's theorem with Scilab", "abstract": "A theorem which is named after the American Mathematician Moris Marden states a very surprising and interesting fact concerning the relationship between the points of a triangle in the complex plane and the zeros of two complex polynomials related to this triangle: \"Suppose the zeroes z1, z2, and z3 of a third-degree polynomial p(z) are non-collinear. There is a unique ellipse inscribed in the triangle with vertices z1, z2, z3 and tangent to the sides at their midpoints: the Steiner in-ellipse. The foci of that ellipse are the zeroes of the derivative p'(z).\" (Wikipedia contributors, \"Marden's theorem\", this http URL). This document describes how Scilab, a popular and powerful open source alternative to MATLAB, can be used to visualize the above stated theorem for arbitrary complex numbers z1, z2, and z3 which are not collinear. It is further demonstrated how the equations of the Steiner ellipses of a triangle in the complex plane can be calculated and plotted by applying this theorem.", "venue": "ArXiv", "authors": ["Klaus  Rohe"], "year": 2015, "n_citations": 0}
{"id": 4557213, "s2_id": "5a28f14f238f33c9c21e972a03ecadb7e30b520b", "title": "H2OPUS-TLR: High Performance Tile Low Rank Symmetric Factorizations using Adaptive Randomized Approximation", "abstract": "Tile low rank (TLR) representations of dense matrices partition them into blocks of roughly uniform size, where each off-diagonal tile is compressed and stored as its own low rank factorization. They offer an attractive representation for many data-sparse dense operators that appear in practical applications, where substantial compression and a much smaller memory footprint can be achieved. TLR matrices are a compromise between the simplicity of a regular perfectly-strided data structure and the optimal complexity of the unbalanced trees of hierarchically low rank matrices, and provide a convenient performance-tuning parameter through their tile size that can be proportioned to take into account the cache size where the tiles reside in the memory hierarchy. Despite their utility however, there are currently no high performance algorithms that can generate their Cholesky and LDLT factorizations and operate on them efficiently, particularly on GPUs. The difficulties in achieving high performance when factoring TLR matrices come from the expensive compression operations that must be performed during the factorization process and the adaptive rank distribution of the tiles that causes an irregular work pattern for the processing cores. In this work, we develop a dynamic batching operation and combine it with batched adaptive randomized approximations to remedy these difficulties and achieve high performance both on GPUs and CPUs. Our implementation attains over 1.2 TFLOP/s in double precision on the V100 GPU, and is limited primarily by the underlying performance of batched GEMM operations. The time-to-solution also shows substantial speedup compared to regular dense factorizations. The Cholesky factorization of covariance matrix of size N = 131K arising in 2D or 3D spatial statistics, for example, can be factored to an accuracy = 10\u22122 in just a few seconds. We believe the proposed GEMM-centric algorithm allows it to be readily ported to newer hardware such as the tensor cores that are optimized for small GEMM operations. King Abdullah University of Science and Technology, Extreme Computing Research Center, Thuwal, Saudi Arabia", "venue": "ArXiv", "authors": ["Wajih Bou Karam", "Stefano  Zampini", "George M. Turkiyyah", "David E. Keyes"], "year": 2021, "n_citations": 1}
{"id": 4562555, "s2_id": "ed726b3ada3d15466b1f31e281257c6e255a7c3a", "title": "An Experimental Exploration of Marsaglia's xorshift Generators, Scrambled", "abstract": "Marsaglia proposed xorshift generators are a class of very fast, good-quality pseudorandom number generators. Subsequent analysis by Panneton and L'Ecuyer has lowered the expectations raised by Marsaglia's article, showing several weaknesses of such generators. Nonetheless, many of the weaknesses of xorshift generators fade away if their result is scrambled by a nonlinear operation (as originally suggested by Marsaglia). In this article we explore the space of possible generators obtained by multiplying the result of a xorshift generator by a suitable constant. We sample generators at 100 points of their state space and obtain detailed statistics that lead us to choices of parameters that improve on the current ones. We then explore for the first time the space of high-dimensional xorshift generators, following another suggestion in Marsaglia's article, finding choices of parameters providing periods of length 21024 \u2212 1 and 24096 \u2212 1. The resulting generators are of extremely high quality, faster than current similar alternatives, and generate long-period sequences passing strong statistical tests using only eight logical operations, one addition, and one multiplication by a constant.", "venue": "ACM Trans. Math. Softw.", "authors": ["Sebastiano  Vigna"], "year": 2016, "n_citations": 43}
{"id": 4569739, "s2_id": "403354b0fdc4a0f9bc7980d4ba0d450750fa359b", "title": "MFST: A Python OpenFST Wrapper With Support for Custom Semirings and Jupyter Notebooks", "abstract": "This paper introduces mFST, a new Python library for working with Finite-State Machines based on OpenFST. mFST is a thin wrapper for OpenFST and exposes all of OpenFST's methods for manipulating FSTs. Additionally, mFST is the only Python wrapper for OpenFST that exposes OpenFST's ability to define a custom semirings. This makes mFST ideal for developing models that involve learning the weights on a FST or creating neuralized FSTs. mFST has been designed to be easy to get started with and has been previously used in homework assignments for a NLP class as well in projects for integrating FSTs and neural networks. In this paper, we exhibit mFST API and how to use mFST to build a simple neuralized FST with PyTorch.", "venue": "ArXiv", "authors": ["Matthew  Francis-Landau"], "year": 2020, "n_citations": 1}
{"id": 4576784, "s2_id": "09d9c6f17239b5e67aa84a92a58b68e6d39d85bc", "title": "SimTensor: A synthetic tensor data generator", "abstract": "SimTensor is a multi-platform, open-source software for generating artificial tensor data (either with CP/PARAFAC or Tucker structure) for reproducible research on tensor factorization algorithms. SimTensor is a stand-alone application based on MATALB. It provides a wide range of facilities for generating tensor data with various configurations. It comes with a user-friendly graphical user interface, which enables the user to generate tensors with complicated settings in an easy way. It also has this facility to export generated data to universal formats such as CSV and HDF5, which can be imported via a wide range of programming languages (C, C++, Java, R, Fortran, MATLAB, Perl, Python, and many more). The most innovative part of SimTensor is this that can generate temporal tensors with periodic waves, seasonal effects and streaming structure. it can apply constraints such as non-negativity and different kinds of sparsity to the data. SimTensor also provides this facility to simulate different kinds of change-points and inject various types of anomalies. The source code and binary versions of SimTensor is available for download in this http URL.", "venue": "ArXiv", "authors": ["Hadi  Fanaee-T", "Jo\u00e3o  Gama"], "year": 2016, "n_citations": 6}
{"id": 4584762, "s2_id": "5d12f3093fb15fa380e40199c60fe7aef7fb9bde", "title": "Nonequispaced Fast Fourier Transform (NFFT) Interface for Julia", "abstract": "This report describes the newly added Julia interface to the NFFT3 library. We explain the multidimensional NFFT algorithm and basics of the interface. Furthermore, we go into detail about the different parameters and how to adjust them properly.", "venue": "ArXiv", "authors": ["Michael  Schmischke"], "year": 2018, "n_citations": 0}
{"id": 4587019, "s2_id": "f7c7cc3aed20591cbbb06f950b5c1fb05ce9b74a", "title": "An open-source ABAQUS implementation of the scaled boundary finite element method to study interfacial problems using polyhedral meshes", "abstract": "Abstract The scaled boundary finite element method (SBFEM) is capable of generating polyhedral elements with an arbitrary number of surfaces. This salient feature significantly alleviates the meshing burden being a bottleneck in the analysis pipeline in the standard finite element method (FEM). In this paper, we implement polyhedral elements based on the SBFEM into the commercial finite element software ABAQUS. To this end, user elements are provided through the user subroutine UEL. Detailed explanations regarding the data structures and implementational aspects of the procedures are given. The focus of the current implementation is on interfacial problems and therefore, element-based surfaces are created on polyhedral user elements to establish interactions. This is achieved by an overlay of standard finite elements with negligible stiffness, provided in the ABAQUS element library, with polyhedral user elements. By means of several numerical examples, the advantages of polyhedral elements regarding the treatment of non-matching interfaces and automatic mesh generation are clearly demonstrated. Thus, the performance of ABAQUS for problems involving interfaces is augmented based on the availability of polyhedral meshes. Due to the implementation of polyhedral user elements, ABAQUS can directly handle complex geometries given in the form of digital images or stereolithography (STL) files. In order to facilitate the use of the proposed approach, the code of the UEL is published open-source and can be downloaded from https://github.com/ShukaiYa/SBFEM-UEL .", "venue": "ArXiv", "authors": ["Shukai  Ya", "Sascha  Eisentr\u00e4ger", "Chongmin  Song", "Jianbo  Li"], "year": 2021, "n_citations": 3}
{"id": 4587435, "s2_id": "b7ce2a931b73c42e2c5a63f07cd71990c6b0a436", "title": "Investigating the OPS intermediate representation to target GPUs in the Devito DSL", "abstract": "The Devito DSL is a code generation tool for the solution of partial differential equations using the finite difference method specifically aimed at seismic inversion problems. \nIn this work we investigate the integration of OPS, an API to generate highly optimized code for applications running on structured meshes targeting various platforms, within Devito as a mean of bringing it to the GPU realm by providing an implementation of a OPS backend in Devito, obtaining considerable speed ups compared to the core Devito backend.", "venue": "ArXiv", "authors": ["Vincenzo  Pandolfo"], "year": 2019, "n_citations": 0}
{"id": 4590136, "s2_id": "7d3f43bbf15809c5ead286ada28fbcdfb9c7298a", "title": "High performance Python for direct numerical simulations of turbulent flows", "abstract": "Abstract Direct Numerical Simulations (DNS) of the Navier Stokes equations is an invaluable research tool in fluid dynamics. Still, there are few publicly available research codes and, due to the heavy number crunching implied, available codes are usually written in low-level languages such as C/C++ or Fortran. In this paper we describe a pure scientific Python pseudo-spectral DNS code that nearly matches the performance of C++ for thousands of processors and billions of unknowns. We also describe a version optimized through Cython, that is found to match the speed of C++. The solvers are written from scratch in Python, both the mesh, the MPI domain decomposition, and the temporal integrators. The solvers have been verified and benchmarked on the Shaheen supercomputer at the KAUST supercomputing laboratory, and we are able to show very good scaling up to several thousand cores. A very important part of the implementation is the mesh decomposition (we implement both slab and pencil decompositions) and 3D parallel Fast Fourier Transforms (FFT). The mesh decomposition and FFT routines have been implemented in Python using serial FFT routines (either NumPy, pyFFTW or any other serial FFT module), NumPy array manipulations and with MPI communications handled by MPI for Python ( mpi4py ). We show how we are able to execute a 3D parallel FFT in Python for a slab mesh decomposition using 4 lines of compact Python code, for which the parallel performance on Shaheen is found to be slightly better than similar routines provided through the FFTW library. For a pencil mesh decomposition 7 lines of code is required to execute a transform.", "venue": "Comput. Phys. Commun.", "authors": ["Mikael  Mortensen", "Hans Petter Langtangen"], "year": 2016, "n_citations": 28}
{"id": 4592458, "s2_id": "3442d29e6c8ec9b41e9cdd9045370988531b458c", "title": "A Modular and Extensible Software Architecture for Particle Dynamics", "abstract": "Creating a highly parallel and flexible discrete element software requires an interdisciplinary approach, where expertise from different disciplines is combined. On the one hand domain specialists provide interaction models between particles. On the other hand high-performance computing specialists optimize the code to achieve good performance on different hardware architectures. In particular, the software must be carefully crafted to achieve good scaling on massively parallel supercomputers. Combining all this in a flexible and extensible, widely usable software is a challenging task. In this article we outline the design decisions and concepts of a newly developed particle dynamics code MESA-PD that is implemented as part of the waLBerla multi-physics framework. Extensibility, flexibility, but also performance and scalability are primary design goals for the new software framework. In particular, the new modular architecture is designed such that physical models can be modified and extended by domain scientists without understanding all details of the parallel computing functionality and the underlying distributed data structures that are needed to achieve good performance on current supercomputer architectures. This goal is achieved by combining the high performance simulation framework waLBerla with code generation techniques. All code and the code generator are released as open source under GPLv3 within the publicly available waLBerla framework (this http URL).", "venue": "ArXiv", "authors": ["Sebastian  Eibl", "Ulrich  R\u00fcde"], "year": 2019, "n_citations": 7}
{"id": 4593082, "s2_id": "8f1692be8abd2b2130107c3cf36e31b3abf86903", "title": "mlpy: Machine Learning Python", "abstract": "mlpy is a Python Open Source Machine Learning library built on top of NumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of state-of-the-art machine learning methods for supervised and unsupervised problems and it is aimed at finding a reasonable compromise among modularity, maintainability, reproducibility, usability and efficiency.mlpy is multiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at the website http://mlpy.fbk.eu.", "venue": "ArXiv", "authors": ["Davide  Albanese", "Roberto  Visintainer", "Stefano  Merler", "Samantha  Riccadonna", "Giuseppe  Jurman", "Cesare  Furlanello"], "year": 2012, "n_citations": 72}
{"id": 4595701, "s2_id": "f5c7d301879ee9316d293efcb9c1af5adc675850", "title": "Hamilton Operators, Discrete Symmetries, Brute Force and SymbolicC++", "abstract": "To find the discrete symmetries of a Hamilton operator \u0124 is of central importance in quantum theory. Here, we describe and implement a brute force method to determine the discrete symmetries given by permutation matrices for Hamilton operators acting in a finite-dimensional Hilbert space. Spin and Fermi systems are considered as examples. A computer algebra implementation in SymbolicC++ is provided.", "venue": "ArXiv", "authors": ["Willi-Hans  Steeb", "Yorick  Hardy"], "year": 2012, "n_citations": 0}
{"id": 4604436, "s2_id": "7d16988a2d19f69e0926e899f10a205ffff760b1", "title": "UBL: an R package for Utility-based Learning", "abstract": "This document describes the R package UBL that allows the use of several methods for handling utility-based learning problems. Classification and regression problems that assume non-uniform costs and/or benefits pose serious challenges to predictive analytic tasks. In the context of meteorology, finance, medicine, ecology, among many other, specific domain information concerning the preference bias of the users must be taken into account to enhance the models predictive performance. To deal with this problem, a large number of techniques was proposed by the research community for both classification and regression tasks. The main goal of UBL package is to facilitate the utility-based predictive analytic task by providing a set of methods to deal with this type of problems in the R environment. It is a versatile tool that provides mechanisms to handle both regression and classification (binary and multiclass) tasks. Moreover, UBL package allows the user to specify his domain preferences, but it also provides some automatic methods that try to infer those preference bias from the domain, considering some common known settings.", "venue": "ArXiv", "authors": ["Paula  Branco", "Rita P. Ribeiro", "Lu\u00eds  Torgo"], "year": 2016, "n_citations": 39}
{"id": 4630278, "s2_id": "3a2177f886ca246c254cc0259ed251553f6175ff", "title": "A symbolic transformation language and its application to a multiscale method", "abstract": "The context of this work is the design of a software, called MEMSALab, dedicated to the automatic derivation of multiscale models of arrays of micro- and nanosystems. In this domain a model is a partial differential equation. Multiscale methods approximate it by another partial differential equation which can be numerically simulated in a reasonable time. The challenge consists in taking into account a wide range of geometries combining thin and periodic structures with the possibility of multiple nested scales. In this paper we present a transformation language that will make the development of MEMSALab more feasible. It is proposed as a Maple package for rule-based programming, rewriting strategies and their combination with standard Maple code. We illustrate the practical interest of this language by using it to encode two examples of multiscale derivations, namely the two-scale limit of the derivative operator and the two-scale model of the stationary heat equation.", "venue": "J. Symb. Comput.", "authors": ["Walid  Belkhir", "Alain  Giorgetti", "Michel  Lenczner"], "year": 2014, "n_citations": 18}
{"id": 4635162, "s2_id": "3f63434495cd647222d58bc06a1e5bd8a34c603a", "title": "A Parallel Algorithm for Calculation of Large Determinants with High Accuracy for GPUs and MPI clusters", "abstract": "We present a parallel algorithm for calculating very large determinants with arbitrary precision on computer clusters. This algorithm minimises data movements between the nodes and computes not only the determinant but also all minors corresponding to a particular row or column at a little extra cost, and also the determinants and minors of all submatrices in the top left corner at no extra cost. We implemented the algorithm in arbitrary precision arithmetic, suitable for very ill conditioned matrices, and empirically estimated the loss of precision. The algorithm was applied to studies of Riemann's zeta function.", "venue": "ArXiv", "authors": ["Gleb  Beliakov", "Yuri V. Matiyasevich"], "year": 2013, "n_citations": 9}
{"id": 4638916, "s2_id": "cf4537874312afcddcff8a09ba1017916766b6dc", "title": "Development of Krylov and AMG linear solvers for large-scale sparse matrices on GPUs", "abstract": "This paper introduces our work on developing Krylov subspace and AMG solvers on NVIDIA GPUs. As SpMV is a crucial part for these iterative methods, SpMV algorithms for a single GPU and multiple GPUs are implemented. A HEC matrix format and a communication mechanism are established. Also, a set of specific algorithms for solving preconditioned systems in parallel environments are designed, including ILU(k), RAS and parallel triangular solvers. Based on these work, several Krylov solvers and AMG solvers are developed. According to numerical experiments, favorable acceleration performance is obtained from our Krylov solver and AMG solver under various parameter conditions.", "venue": "HPCA", "authors": ["Bo  Yang", "Hui  Liu", "Zhangxin  Chen"], "year": 2015, "n_citations": 0}
{"id": 4639445, "s2_id": "30beaaeebeb37dd3be9d5f2b126b3ab0efb02fd5", "title": "PBBFMM3D: A parallel black-box algorithm for kernel matrix-vector multiplication", "abstract": "Abstract Kernel matrix-vector product is ubiquitous in many science and engineering applications. However, a naive method requires O ( N 2 ) operations, which becomes prohibitive for large-scale problems. To reduce the computation cost, we introduce a parallel method that provably requires O ( N ) operations and delivers an approximate result within a prescribed tolerance. The distinct feature of our method is that it requires only the ability to evaluate the kernel function, offering a black-box interface to users. Our parallel approach targets multi-core shared-memory machines and is implemented using OpenMP . Numerical results demonstrate up to 19\u00d7 speedup on 32 cores. We also present a real-world application in geo-statistics, where our parallel method was used to deliver fast principle component analysis of covariance matrices.", "venue": "J. Parallel Distributed Comput.", "authors": ["Ruoxi  Wang", "Chao  Chen", "Jonghyun  Lee", "Eric  Darve"], "year": 2021, "n_citations": 6}
{"id": 4640891, "s2_id": "6def08a60c8a6be0cb8d766d3e700185da851f0a", "title": "Maintaining a Library of Formal Mathematics", "abstract": "The Lean mathematical library mathlib is developed by a community of users with very different backgrounds and levels of experience. To lower the barrier of entry for contributors and to lessen the burden of reviewing contributions, we have developed a number of tools for the library which check proof developments for subtle mistakes in the code and generate documentation suited for our varied audience.", "venue": "CICM", "authors": ["Floris van Doorn", "Gabriel  Ebner", "Robert Y. Lewis"], "year": 2020, "n_citations": 6}
{"id": 4643482, "s2_id": "8e7568be047c28f84eba7318fa0b780a201dda37", "title": "Communication-Avoiding Cholesky-QR2 for Rectangular Matrices", "abstract": "Scalable QR factorization algorithms for solving least squares and eigenvalue problems are critical given the increasing parallelism within modern machines. We introduce a more general parallelization of the CholeskyQR2 algorithm and show its effectiveness for a wide range of matrix sizes. Our algorithm executes over a 3D processor grid, the dimensions of which can be tuned to trade-off costs in synchronization, interprocessor communication, computational work, and memory footprint. We implement this algorithm, yielding a code that can achieve a factor of \u0398(P^1/6) less interprocessor communication on P processors than any previous parallel QR implementation. Our performance study on Intel Knights-Landing and Cray XE supercomputers demonstrates the effectiveness of this CholeskyQR2 parallelization on a large number of nodes. Specifically, relative to ScaLAPACK's QR, on 1024 nodes of Stampede2, our CholeskyQR2 implementation is faster by 2.6x-3.3x in strong scaling tests and by 1.1x-1.9x in weak scaling tests.", "venue": "2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Edward  Hutter", "Edgar  Solomonik"], "year": 2019, "n_citations": 4}
{"id": 4643632, "s2_id": "124798a62b96122a0e503a6ac9a7e1cbe1c8de74", "title": "Porting a sparse linear algebra math library to Intel GPUs", "abstract": "With the announcement that the Aurora Supercomputer will be composed of general purpose Intel CPUs complemented by discrete high performance Intel GPUs, and the deployment of the oneAPI ecosystem, Intel has committed to enter the arena of discrete high performance GPUs. A central requirement for the scientific computing community is the availability of production-ready software stacks and a glimpse of the performance they can expect to see on Intel high performance GPUs. In this paper, we present the first platform-portable open source math library supporting Intel GPUs via the DPC++ programming environment. We also benchmark some of the developed sparse linear algebra functionality on different Intel GPUs to assess the efficiency of the DPC++ programming ecosystem to translate raw performance into application performance. Aside from quantifying the efficiency within the hardware-specific roofline model, we also compare against routines providing the same functionality that ship with Intel\u2019s oneMKL vendor library.", "venue": "ArXiv", "authors": ["Yuhsiang M. Tsai", "Terry  Cojean", "Hartwig  Anzt"], "year": 2021, "n_citations": 1}
{"id": 4645668, "s2_id": "223a15e466ab44eaefe7696b780fea1de5dfe314", "title": "Symmetric QR Algorithm with Permutations", "abstract": "In this paper, we present the QR Algorithm with Permutations that shows an improved convergence rate compared to the classical QR algorithm. We determine a bound for performance based on best instantaneous convergence, and develop low complexity methods for computing the permutation matrices at every iteration. We use simulations to verify the improvement, and to compare the performance of proposed algorithms to the classical QR algorithm.", "venue": "ArXiv", "authors": ["Aravindh  Krishnamoorthy"], "year": 2014, "n_citations": 0}
{"id": 4654675, "s2_id": "2267e8cd1ecdbadd2ece8b7d5b4ca91cd089a605", "title": "Implementing Strassen's Algorithm with BLIS", "abstract": "We dispel with \"street wisdom\" regarding the practical implementation of Strassen's algorithm for matrix-matrix multiplication (DGEMM). Conventional wisdom: it is only practical for very large matrices. Our implementation is practical for small matrices. Conventional wisdom: the matrices being multiplied should be relatively square. Our implementation is practical for rank-k updates, where k is relatively small (a shape of importance for libraries like LAPACK). Conventional wisdom: it inherently requires substantial workspace. Our implementation requires no workspace beyond buffers already incorporated into conventional high-performance DGEMM implementations. Conventional wisdom: a Strassen DGEMM interface must pass in workspace. Our implementation requires no such workspace and can be plug-compatible with the standard DGEMM interface. Conventional wisdom: it is hard to demonstrate speedup on multi-core architectures. Our implementation demonstrates speedup over conventional DGEMM even on an Intel(R) Xeon Phi(TM) coprocessor utilizing 240 threads. We show how a distributed memory matrix-matrix multiplication also benefits from these advances.", "venue": "ArXiv", "authors": ["Jianyu  Huang", "Tyler M. Smith", "Greg M. Henry", "Robert A. van de Geijn"], "year": 2016, "n_citations": 6}
{"id": 4654837, "s2_id": "f785548c1e136bd88e7774122e2c92c52176e0d2", "title": "Integrating generic sensor fusion algorithms with sound state representations through encapsulation of manifolds", "abstract": "Common estimation algorithms, such as least squares estimation or the Kalman filter, operate on a state in a state space S that is represented as a real-valued vector. However, for many quantities, most notably orientations in 3D, S is not a vector space, but a so-called manifold, i.e. it behaves like a vector space locally but has a more complex global topological structure. For integrating these quantities, several ad hoc approaches have been proposed. Here, we present a principled solution to this problem where the structure of the manifold S is encapsulated by two operators, state displacement :SxR^n->S and its inverse :SxS->R^n. These operators provide a local vector-space view @[email\u00a0protected][email\u00a0protected] around a given state x. Generic estimation algorithms can then work on the manifold S mainly by replacing +/- with / where appropriate. We analyze these operators axiomatically, and demonstrate their use in least-squares estimation and the Unscented Kalman Filter. Moreover, we exploit the idea of encapsulation from a software engineering perspective in the Manifold Toolkit, where the / operators mediate between a ''flat-vector'' view for the generic algorithm and a ''named-members'' view for the problem specific functions.", "venue": "Inf. Fusion", "authors": ["Christoph  Hertzberg", "Ren\u00e9  Wagner", "Udo  Frese", "Lutz  Schr\u00f6der"], "year": 2013, "n_citations": 186}
{"id": 4655896, "s2_id": "ea68999cd09c2fcbc9f932ffb59f64f6bae6b7b1", "title": "Experimental Evaluation of Multiprecision Strategies for GMRES on GPUs", "abstract": "Support for lower precision computation is becoming more common in accelerator hardware due to lower power usage, reduced data movement and increased computational performance. However, computational science and engineering (CSE) problems require double precision accuracy in several domains. This conflict between hardware trends and application needs has resulted in a need for multiprecision strategies at the linear algebra algorithms level if we want to exploit the hardware to its full potential while meeting the accuracy requirements. In this paper, we focus on preconditioned sparse iterative linear solvers, a key kernel in several CSE applications. We present a study of multiprecision strategies for accelerating this kernel on GPUs. We seek the best methods for incorporating multiple precisions into the GMRES linear solver; these include iterative refinement and parallelizable preconditioners. Our work presents strategies to determine when multiprecision GMRES will be effective and to choose parameters for a multiprecision iterative refinement solver to achieve better performance. We use an implementation that is based on the Trilinos library and employs Kokkos Kernels for performance portability of linear algebra kernels. Performance results demonstrate the promise of multiprecision approaches and demonstrate even further improvements are possible by optimizing low-level kernels.", "venue": "2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Jennifer A. Loe", "Christian A. Glusa", "Ichitaro  Yamazaki", "Erik G. Boman", "Sivasankaran  Rajamanickam"], "year": 2021, "n_citations": 3}
{"id": 4658294, "s2_id": "70b45b61d38bb3d44794b57fdd1dbf9d1212b18b", "title": "JIDT: An Information-Theoretic Toolkit for Studying the Dynamics of Complex Systems", "abstract": "Complex systems are increasingly being viewed as distributed information processing systems, particularly in the domains of computational neuroscience, bioinformatics and Artificial Life. This trend has resulted in a strong uptake in the use of (Shannon) information-theoretic measures to analyse the dynamics of complex systems in these fields. We introduce the Java Information Dynamics Toolkit (JIDT): a Google code project which provides a standalone, (GNU GPL v3 licensed) open-source code implementation for empirical estimation of information-theoretic measures from time-series data. While the toolkit provides classic information-theoretic measures (e.g. entropy, mutual information, conditional mutual information), it ultimately focusses on implementing higher-level measures for information dynamics. That is, JIDT focusses on quantifying information storage, transfer and modification, and the dynamics of these operations in space and time. For this purpose, it includes implementations of the transfer entropy and active information storage, their multivariate extensions and local or pointwise variants. JIDT provides implementations for both discrete and continuous-valued data for each measure, including various types of estimator for continuous data (e.g. Gaussian, box-kernel and Kraskov-Stoegbauer-Grassberger) which can be swapped at run-time due to Java's object-oriented polymorphism. Furthermore, while written in Java, the toolkit can be used directly in MATLAB, GNU Octave, Python and other environments. We present the principles behind the code design, and provide several examples to guide users.", "venue": "Front. Robot. AI", "authors": ["Joseph T. Lizier"], "year": 2014, "n_citations": 242}
{"id": 4660403, "s2_id": "045ea0e679d08b3f426b1be42ed79db251556c67", "title": "A Subdivision Solver for Systems of Large Dense Polynomials", "abstract": "We describe here the package {\\tt subdivision\\_solver} for the mathematical software {\\tt SageMath}. \nIt provides a solver on real numbers for square systems of large dense polynomials. \nBy large polynomials we mean multivariate polynomials with large degrees, which coefficients have large bit-size. \nWhile staying robust, symbolic approaches to solve systems of polynomials see their performances dramatically affected by high degree and bit-size of input polynomials. \nAvailable numeric approaches suffer from the cost of the evaluation of large polynomials and their derivatives. \nOur solver is based on interval analysis and bisections of an initial compact domain of $\\R^n$ where solutions are sought. \nEvaluations on intervals with Horner scheme is performed by the package {\\tt fast\\_polynomial} for {\\tt SageMath}. \nThe non-existence of a solution within a box is certified by an evaluation scheme that uses a Taylor expansion at order 2, and existence and uniqueness of a solution within a box is certified with krawczyk operator. \nThe precision of the working arithmetic is adapted on the fly during the subdivision process and we present a new heuristic criterion to decide if the arithmetic precision has to be increased.", "venue": "ArXiv", "authors": ["R\u00e9mi  Imbach"], "year": 2016, "n_citations": 2}
{"id": 4665993, "s2_id": "644af4906b87f5e5d829d2d4d12106deb1a3c532", "title": "dame-flame: A Python Library Providing Fast Interpretable Matching for Causal Inference", "abstract": "dame-flame is a Python package for performing matching for observational causal inference on datasets containing discrete covariates. This package implements the Dynamic Almost Matching Exactly (DAME) and Fast, Large-Scale Almost Matching Exactly (FLAME) algorithms, which match treatment and control units on subsets of the covariates. The resulting matched groups are interpretable, because the matches are made on covariates (rather than, for instance, propensity scores), and high-quality, because machine learning is used to determine which covariates are important to match on. DAME solves an optimization problem that matches units on as many covariates as possible, prioritizing matches on important covariates. FLAME approximates the solution found by DAME via a much faster backward feature selection procedure. The package provides several adjustable parameters to adapt the algorithms to specific applications, and can calculate treatment effects after matching. Descriptions of these parameters, details on estimating treatment effects, and further examples, can be found in the documentation at https://almost-matching-exactly.github.io/DAME-FLAME-Python-Package/.", "venue": "ArXiv", "authors": ["Neha R. Gupta", "Vittorio  Orlandi", "Chia-Rui  Chang", "Tianyu  Wang", "Marco  Morucci", "Pritam  Dey", "Thomas J. Howell", "Xian  Sun", "Angikar  Ghosal", "Sudeepa  Roy", "Cynthia  Rudin", "Alexander  Volfovsky"], "year": 2021, "n_citations": 1}
{"id": 4669042, "s2_id": "449bdc7bd7f3efa46abfc014deb6134155870ea1", "title": "SoAx: A generic C++ Structure of Arrays for handling particles in HPC codes", "abstract": "Abstract The numerical study of physical problems often require integrating the dynamics of a large number of particles evolving according to a given set of equations. Particles are characterized by the information they are carrying such as an identity, a position other. There are generally speaking two different possibilities for handling particles in high performance computing (HPC) codes. The concept of an Array of Structures (AoS) is in the spirit of the object-oriented programming (OOP) paradigm in that the particle information is implemented as a structure. Here, an object (realization of the structure) represents one particle and a set of many particles is stored in an array. In contrast, using the concept of a Structure of Arrays (SoA), a single structure holds several arrays each representing one property (such as the identity) of the whole set of particles. The AoS approach is often implemented in HPC codes due to its handiness and flexibility. For a class of problems, however, it is known that the performance of SoA is much better than that of AoS. We confirm this observation for our particle problem. Using a benchmark we show that on modern Intel Xeon processors the SoA implementation is typically several times faster than the AoS one. On Intel\u2019s MIC co-processors the performance gap even attains a factor of ten. The same is true for GPU computing, using both computational and multi-purpose GPUs. Combining performance and handiness, we present the library SoAx that has optimal performance (on CPUs, MICs, and GPUs) while providing the same handiness as AoS. For this, SoAx uses modern C++ design techniques such template meta programming that allows to automatically generate code for user defined heterogeneous data structures. Program summary Program Title: SoAx Program Files doi: http://dx.doi.org/10.17632/m463pc4mv8.1 Licensing provisions: GPLv3 Programming language: C++ Nature of problem: Structures of arrays (SoA) are generally faster than arrays of structures (AoS) while AoS are more handy. This library (SoAx) combines the advantages of both. By means of C++(11) meta-template programming SoAx achieves maximal performance (efficient use of vector units and cache of modern CPUs) while providing a very convenient user interface (including object-oriented element handling) and flexibility. It has been designed to handle list-like sets of particles (similar to struct int id; double[3] pos; float[3] vel;;) in the context of high-performance numerical simulations. It can be applied to many other problems. Solution method: Template Metaprogramming, Expression Templates", "venue": "Comput. Phys. Commun.", "authors": ["Holger  Homann", "Francois  Laenen"], "year": 2018, "n_citations": 18}
{"id": 4687014, "s2_id": "5ae10271e45210931c987aff2f3976ee62ae6ee0", "title": "Introducing LambdaTensor1.0 - A package for explicit symbolic and numeric Lie algebra and Lie group calculations", "abstract": "Due to the occurrence of large exceptional Lie groups in supergravity, calculations involving explicit Lie algebra and Lie group element manipulations easily become very complicated and hence also error-prone if done by hand. Research on the extremal structure of maximal gauged supergravity theories in various dimensions sparked the development of a library for efficient abstract multilinear algebra calculations involving sparse and non-sparse higher-rank tensors, which is presented here.", "venue": "ArXiv", "authors": ["Thomas  Fischbacher"], "year": 2002, "n_citations": 12}
{"id": 4690223, "s2_id": "3aa778dfd7cc955a1b3983c4c01a60b1f8de618e", "title": "On the asymptotic and practical complexity of solving bivariate systems over the reals", "abstract": "This paper is concerned with exact real solving of well-constrained, bivariate polynomial systems. The main problem is to isolate all common real roots in rational rectangles, and to determine their intersection multiplicities. We present three algorithms and analyze their asymptotic bit complexity, obtaining a bound of [email\u00a0protected]?\"B(N^1^4) for the purely projection-based method, and [email\u00a0protected]?\"B(N^1^2) for two subresultant-based methods: this notation ignores polylogarithmic factors, where N bounds the degree, and the bitsize of the polynomials. The previous record bound was [email\u00a0protected]?\"B(N^1^4). Our main tool is signed subresultant sequences. We exploit recent advances on the complexity of univariate root isolation, and extend them to sign evaluation of bivariate polynomials over algebraic numbers, and real root counting for polynomials over an extension field. Our algorithms apply to the problem of simultaneous inequalities; they also compute the topology of real plane algebraic curves in [email\u00a0protected]?\"B(N^1^2), whereas the previous bound was [email\u00a0protected]?\"B(N^1^4). All algorithms have been implemented in maple, in conjunction with numeric filtering. We compare them against fgb/rs, system solvers from synaps, and maple libraries insulate and top, which compute curve topology. Our software is among the most robust, and its runtimes are comparable, or within a small constant factor, with respect to the C/C++ libraries.", "venue": "J. Symb. Comput.", "authors": ["Dimitrios I. Diochnos", "Ioannis Z. Emiris", "Elias P. Tsigaridas"], "year": 2009, "n_citations": 61}
{"id": 4690571, "s2_id": "b35caf5ce9b707a656ecadebfcd618ce43c690d7", "title": "A High-Performance Implementation of a Robust Preconditioner for Heterogeneous Problems", "abstract": "We present an efficient implementation of the highly robust and scalable GenEO preconditioner in the high-performance PDE framework DUNE. The GenEO coarse space is constructed by combining low energy solutions of a local generalised eigenproblem using a partition of unity. In this paper we demonstrate both weak and strong scaling for the GenEO solver on over 15,000 cores by solving an industrially motivated problem with over 200 million degrees of freedom. Further, we show that for highly complex parameter distributions arising in certain real-world applications, established methods become intractable while GenEO remains fully effective. The purpose of this paper is two-fold: to demonstrate the robustness and high parallel efficiency of the solver and to document the technical details that are crucial to the efficiency of the code.", "venue": "PPAM", "authors": ["Linus  Seelinger", "Anne  Reinarz", "Robert  Scheichl"], "year": 2019, "n_citations": 0}
{"id": 4692155, "s2_id": "ed5bb002484c85b4b153fa24f1c1348996602edb", "title": "Minkowski sum of polytopes defined by their vertices", "abstract": "Minkowski sums are of theoretical interest and have applications in fields related to industrial backgrounds. In this paper we focus on the specific case of summing polytopes as we want to solve the tolerance analysis problem described in [1]. Our approach is based on the use of linear programming and is solvable in polynomial time. The algorithm we developed can be implemented and parallelized in a very easy way.", "venue": "ArXiv", "authors": ["Vincent  Delos", "Denis  Teissandier"], "year": 2014, "n_citations": 19}
{"id": 4695725, "s2_id": "9d5b50ef55d49e2b0fdb9aaca6d99c67aebe8dac", "title": "LazySets.jl: Scalable Symbolic-Numeric Set Computations", "abstract": "LazySets.jl is a Julia library that provides ways to symbolically represent sets of points as geometric shapes, with a special focus on convex sets and polyhedral approximations. LazySets provides methods to apply common set operations, convert between different set representations, and efficiently compute with sets in high dimensions using specialized algorithms based on the set types. LazySets is the core library of JuliaReach, a cutting-edge software addressing the fundamental problem of reachability analysis: computing the set of states that are reachable by a dynamical system from all initial states and for all admissible inputs and parameters. While the library was originally designed for reachability and formal verification, its scope goes beyond such topics. LazySets is an easy-to-use, general-purpose and scalable library for computations that mix symbolics and numerics. In this article we showcase the basic functionality, highlighting some of the key design choices.", "venue": "JuliaCon Proceedings", "authors": ["Marcelo  Forets", "Christian  Schilling"], "year": 2021, "n_citations": 2}
{"id": 4695923, "s2_id": "a670363351fc95715441256a2a9f02f89e72415b", "title": "The Declaratron, semantic specification for scientific computation using MathML", "abstract": "We introduce the Declaratron, a system which takes a declarative approach to specifying mathematically based scientific computation. This uses displayable mathematical notation (Content MathML) and is both executable and semantically well defined. We combine domain specific representations of physical science (e.g. CML, Chemical Markup Language), MathML formulae and computational specifications (DeXML) to create executable documents which include scientific data and mathematical formulae. These documents preserve the provenance of the data used, and build tight semantic links between components of mathematical formulae and domain objects---in effect grounding the mathematical semantics in the scientific domain. The Declaratron takes these specifications and i) carries out entity resolution and decoration to prepare for computation ii) uses a MathML execution engine to run calculations over the revised tree iii) outputs domain objects and the complete document to give both results and an encapsulated history of the computation. A short description of a case study is given to illustrate how the system can be used. Many scientific problems require frequent change of the mathematical functional form and the Declaratron provides this without requiring changes to code. Additionally, it supports reproducible science, machine indexing and semantic search of computations, makes implicit assumptions visible, and separates domain knowledge from computational techniques. We believe that the Declaratron could replace much conventional procedural code in science.", "venue": "CICM Workshops", "authors": ["David  Murray-Rust", "Peter  Murray-Rust"], "year": 2013, "n_citations": 0}
{"id": 4696749, "s2_id": "ca00623e95cbd0d38e4101000bb2ca0e01f145e3", "title": "Numerical resolution of some BVP using Bernstein polynomials", "abstract": "In this work we present a method, based on the use of Bernstein polynomials, for the numerical resolution of some boundary values problems. The computations have not need of particular approximations of derivatives, such as finite differences, or particular techniques, such as finite elements. Also, the method doesn't require the use of matrices, as in resolution of linear algebraic systems, nor the use of like-Newton algorithms, as in resolution of non linear sets of equations. An initial equation is resolved only once, then the method is based on iterated evaluations of appropriate polynomials.", "venue": "ArXiv", "authors": ["Gianluca  Argentini"], "year": 2005, "n_citations": 1}
{"id": 4700490, "s2_id": "f12939c65f166418c7724f231b53632d434f2581", "title": "A framework for practical parallel fast matrix multiplication", "abstract": "Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical.", "venue": "PPoPP 2015", "authors": ["Austin R. Benson", "Grey  Ballard"], "year": 2015, "n_citations": 55}
{"id": 4702471, "s2_id": "09c0f61d53db9085f1925aa57b594f759be0f0dd", "title": "A Simple Methodology for Computing Families of Algorithms", "abstract": "Discovering \"good\" algorithms for an operation is often considered an art best left to experts. What if there is a simple methodology, an algorithm, for systematically deriving a family of algorithms as well as their cost analyses, so that the best algorithm can be chosen? We discuss such an approach for deriving loop-based algorithms. The example used to illustrate this methodology, evaluation of a polynomial, is itself simple yet the best algorithm that results is surprising to a non-expert: Horner's rule. We finish by discussing recent advances that make this approach highly practical for the domain of high-performance linear algebra software libraries.", "venue": "ArXiv", "authors": ["Devangi N. Parikh", "Margaret E. Myers", "Richard W. Vuduc", "Robert A. van de Geijn"], "year": 2018, "n_citations": 0}
{"id": 4707352, "s2_id": "1a63709f84f25749a9bb745e416a09ce4906641c", "title": "Methods of Matrix Multiplication: An Overview of Several Methods and their Implementation", "abstract": "In this overview article we present several methods for multiplying matrices and the implementation of these methods in C. Also a little test program is given to compare their running time and the numerical stability. \nThe methods are: naive method, naive method working on arrays, naive method with the \\textsc{Kahan} trick, three methods with loop unrolling, winograd method and the scaled variant, original \\textsc{Strassen} method and the \\textsc{Strassen}-\\textsc{Winograd} variant. \nPlease note, that this is the FIRST version. The algorithms are not well tested and the implementation is not optimized. If you like to join the project, please contact me.", "venue": "ArXiv", "authors": ["Ivo  Hedtke"], "year": 2011, "n_citations": 0}
{"id": 4708023, "s2_id": "e226637ffa96b7c25da4d8a9dcf9186862999061", "title": "NCVX: A User-Friendly and Scalable Package for Nonconvex Optimization in Machine Learning", "abstract": "Optimizing nonconvex (NCVX) problems, especially those nonsmooth (NSMT) and constrained (CSTR), is an essential part of machine learning and deep learning. But it is hard to reliably solve this type of problems without optimization expertise. Existing general-purpose NCVX optimization packages are powerful, but typically cannot handle nonsmoothness. GRANSO is among the first packages targeting NCVX, NSMT, CSTR problems. However, it has several limitations such as the lack of auto-differentiation and GPU acceleration, which preclude the potential broad deployment by non-experts. To lower the technical barrier for the machine learning community, we revamp GRANSO into a userfriendly and scalable python package named NCVX, featuring auto-differentiation, GPU acceleration, tensor input, scalable QP solver, and zero dependency on proprietary packages. As a highlight, NCVX can solve general CSTR deep learning problems, the first of its kind. NCVX is available at https://ncvx.org, with detailed documentation and numerous examples from machine learning and other fields.", "venue": "ArXiv", "authors": ["Buyun  Liang", "Ju  Sun"], "year": 2021, "n_citations": 0}
{"id": 4708247, "s2_id": "65ed3c48c33c1cbd256daa93d842d234507c4de8", "title": "The software design of Gridap: a Finite Element package based on the Julia JIT compiler", "abstract": "We present the software design of Gridap, a novel finite element library written exclusively in the Julia programming language, which is being used by several research groups world-wide to simulate complex physical phenomena such as magnetohydrodynamics, photonics, weather modeling, non-linear solid mechanics, and fluid-structure interaction problems. The library provides a feature-rich set of discretization techniques for the numerical approximation of a wide range of mathematical models governed by Partial Differential Equations (PDEs), including linear, nonlinear, single-field, and multi-field equations. An expressive API allows users to define PDEs in weak form by a syntax close to the mathematical notation. While this is also available in previous frameworks, the main novelty of Gridap is that it implements this API without introducing a domain-specific language plus a compiler of variational forms. Instead, it leverages the Julia just-in-time compiler to build efficient code, specialized for the concrete problem at hand. As a result, there is no need to use different languages for the computational back-end and the user front-end anymore, thus eliminating the so-called two-language problem. Gridap also provides a low-level API that is modular and extensible via the multiple-dispatch paradigm of Julia and provides easy access to the main building blocks of the library if required. The main contribution of this paper is the detailed presentation of the novel software abstractions behind the Gridap design that leverages the new software possibilities provided by the Julia language. The second main contribution of the article is a performance comparison against FEniCS. We measure CPU times needed to assemble discrete systems of linear equations for different problem types and show that the performance of Gridap is comparable to FEniCS, demonstrating that the new software design does not compromise performance. Gridap is freely available at Github (github.com/gridap/Gridap.jl) and distributed under an MIT license.", "venue": "ArXiv", "authors": ["Francesc  Verdugo", "Santiago  Badia"], "year": 2021, "n_citations": 0}
{"id": 4708993, "s2_id": "5e6360e10efd920919584e61774d0e67bf6f3658", "title": "The MOMMS Family of Matrix Multiplication Algorithms", "abstract": "As the ratio between the rate of computation and rate with which data can be retrieved from various layers of memory continues to deteriorate, a question arises: Will the current best algorithms for computing matrix-matrix multiplication on future CPUs continue to be (near) optimal? This paper provides compelling analytical and empirical evidence that the answer is \"no\". The analytical results guide us to a new family of algorithms of which the current state-of-the-art \"Goto's algorithm\" is but one member. The empirical results, on architectures that were custom built to reduce the amount of bandwidth to main memory, show that under different circumstances, different and particular members of the family become more superior. Thus, this family will likely start playing a prominent role going forward.", "venue": "ArXiv", "authors": ["Tyler M. Smith", "Robert A. van de Geijn"], "year": 2019, "n_citations": 1}
{"id": 4710378, "s2_id": "802b6a63948d603831bf65662f4277d4a119003e", "title": "A Flexible, Parallel, Adaptive Geometric Multigrid Method for FEM", "abstract": "We present the design and implementation details of a geometric multigrid method on adaptively refined meshes for massively parallel computations. The method uses local smoothing on the refined part of the mesh. Partitioning is achieved by using a space filling curve for the leaf mesh and distributing ancestors in the hierarchy based on the leaves. We present a model of the efficiency of mesh hierarchy distribution and compare its predictions to runtime measurements. The algorithm is implemented as part of the deal.II finite-element library and as such available to the public.", "venue": "ACM Trans. Math. Softw.", "authors": ["Thomas C. Clevenger", "Timo  Heister", "Guido  Kanschat", "Martin  Kronbichler"], "year": 2021, "n_citations": 19}
{"id": 4714256, "s2_id": "75e441f2afecbb09f094f1c3c7a6e9978c552fd2", "title": "Solving sequences of generalized least-squares problems on multi-threaded architectures", "abstract": "Generalized linear mixed-effects models in the context of genome-wide association studies (GWAS) represent a formidable computational challenge: the solution of millions of correlated generalized least-squares problems, and the processing of terabytes of data. We present high performance in-core and out-of-core shared-memory algorithms for GWAS: By taking advantage of domain-specific knowledge, exploiting multi-core parallelism, and handling data efficiently, our algorithms attain unequalled performance. When compared to GenABEL, one of the most widely used libraries for GWAS, on a 12-core processor we obtain 50-fold speedups. As a consequence, our routines enable genome studies of unprecedented size.", "venue": "Appl. Math. Comput.", "authors": ["Diego  Fabregat-Traver", "Yurii S. Aulchenko", "Paolo  Bientinesi"], "year": 2014, "n_citations": 12}
{"id": 4719153, "s2_id": "8aea737a9847c5045aa057cc2e1200a1abeaae28", "title": "Programming the Adapteva Epiphany 64-core network-on-chip coprocessor", "abstract": "Energy efficiency is the primary impediment in the path to exascale computing. Consequently, the high-performance computing community is increasingly interested in low-power high-performance embedded systems as building blocks for large-scale high-performance systems. The Adapteva Epiphany architecture integrates low-power RISC cores on a 2D mesh network and promises up to 70 GFLOPS/Watt of theoretical performance. However, with just 32\u2009KB of memory per eCore for storing both data and code, programming the Epiphany system presents significant challenges. In this paper we evaluate the performance of a 64-core Epiphany system with a variety of basic compute and communication micro-benchmarks. Further, we implemented two well known application kernels, 5-point star-shaped heat stencil with a peak performance of 65.2 GFLOPS and matrix multiplication with 65.3 GFLOPS in single precision across 64 Epiphany cores. We discuss strategies for implementing high-performance computing application kernels on such memory constrained low-power devices and compare the Epiphany with competing low-power systems. With future Epiphany revisions expected to house thousands of cores on a single chip, understanding the merits of such an architecture is of prime importance to the exascale initiative.", "venue": "2014 IEEE International Parallel & Distributed Processing Symposium Workshops", "authors": ["Anish  Varghese", "Bob  Edwards", "Gaurav  Mitra", "Alistair P. Rendell"], "year": 2014, "n_citations": 34}
{"id": 4720430, "s2_id": "0e0de12e738035b2cf91611301ee33351593d2c7", "title": "Generalized Polylogarithms in Maple", "abstract": "This paper describes generalized polylogarithms, multiple polylogarithms, and multiple zeta values, along with their implementation in Maple 2018. This set of related functions is of interest in high energy physics as well as in number theory. Algorithms for the analytical manipulation and numerical evaluation of these functions are described, along with the way these features are implemented in Maple.", "venue": "ArXiv", "authors": ["Hjalte  Frellesvig"], "year": 2018, "n_citations": 7}
{"id": 4723344, "s2_id": "9fe86e014f3b4b67287fb06dbff9f2133aaa4c40", "title": "Representing numeric data in 32 bits while preserving 64-bit precision", "abstract": "Data files often consist of numbers having only a few significant decimal digits, whose information content would allow storage in only 32 bits. However, we may require that arithmetic operations involving these numbers be done with 64-bit floating-point precision, which precludes simply representing the data as 32-bit floating-point values. Decimal floating point gives a compact and exact representation, but requires conversion with a slow division operation before it can be used. Here, I show that interesting subsets of 64-bit floating-point values can be compactly and exactly represented by the 32 bits consisting of the sign, exponent, and high-order part of the mantissa, with the lower-order 32 bits of the mantissa filled in by table lookup, indexed by bits from the part of the mantissa retained, and possibly from the exponent. For example, decimal data with 4 or fewer digits to the left of the decimal point and 2 or fewer digits to the right of the decimal point can be represented in this way using the lower-order 5 bits of the retained part of the mantissa as the index. Data consisting of 6 decimal digits with the decimal point in any of the 7 positions before or after one of the digits can also be represented this way, and decoded using 19 bits from the mantissa and exponent as the index. Encoding with such a scheme is a simple copy of half the 64-bit value, followed if necessary by verification that the value can be represented, by checking that it decodes correctly. Decoding requires only extraction of index bits and a table lookup. Lookup in a small table will usually reference cache; even with larger tables, decoding is still faster than conversion from decimal floating point with a division operation. I discuss how such schemes perform on recent computer systems, and how they might be used to automatically compress large arrays in interpretive languages such as R.", "venue": "ArXiv", "authors": ["Radford M. Neal"], "year": 2015, "n_citations": 0}
{"id": 4727085, "s2_id": "29bd5b1725a039da1a8127f24e786a15e36d1efd", "title": "GPU-Accelerated Generation of Correctly Rounded Elementary Functions", "abstract": "The IEEE 754-2008 standard recommends the correct rounding of some elementary functions. This requires solving the Table Maker\u2019s Dilemma (TMD), which implies a huge amount of CPU computation time. In this article, we consider accelerating such computations, namely the Lef\u00e8vre algorithm on graphics processing units (GPUs), which are massively parallel architectures with a partial single instruction, multiple data execution. We first propose an analysis of the Lef\u00e8vre hard-to-round argument search using the concept of continued fractions. We then propose a new parallel search algorithm that is much more efficient on GPUs thanks to its more regular control flow. We also present an efficient hybrid CPU-GPU deployment of the generation of the polynomial approximations required in the Lef\u00e8vre algorithm. In the end, we manage to obtain overall speedups up to 53.4 \u00d7 on one GPU over a sequential CPU execution and up to 7.1 \u00d7 over a hex-core CPU, which enable a much faster solution of the TMD for the double-precision format.", "venue": "ACM Trans. Math. Softw.", "authors": ["Pierre  Fortin", "Mourad  Gouicem", "Stef  Graillat"], "year": 2017, "n_citations": 2}
{"id": 4727629, "s2_id": "74b09e07aa6f2cdef47a9e3ab8f6d1351876f7df", "title": "High Performance Block Incomplete LU Factorization", "abstract": "Many application problems that lead to solving linear systems make use of preconditioned Krylov subspace solvers to compute their solution. Among the most popular preconditioning approaches are incomplete factorization methods either as single-level approaches or within a multilevel framework. We will present a block incomplete factorization that is based on skillfully blocking the system initially and throughout the factorization. This approach allows for the use of cache-optimized dense matrix kernels such as level-3 BLAS or LAPACK. We will demonstrate how this block approach outperforms the scalar method often by orders of magnitude on modern architectures, paving the way for its prospective use inside various multilevel incomplete factorization approaches or other applications where the core part relies on an incomplete factorization.", "venue": "ArXiv", "authors": ["Matthias  Bollh\u00f6fer", "Olaf  Schenk", "Fabio  Verbosio"], "year": 2019, "n_citations": 5}
{"id": 4745658, "s2_id": "9780d56345be8e9845af4cfba6affc2c7b9eef9d", "title": "An Elimination Method for Solving Bivariate Polynomial Systems: Eliminating the Usual Drawbacks", "abstract": "We present an exact and complete algorithm to isolate the real solutions of a zero-dimensional bivariate polynomial system. The proposed algorithm constitutes an elimination method which improves upon existing approaches in a number of points. First, the amount of purely symbolic operations is significantly reduced, that is, only resultant computation and square-free factorization is still needed. Second, our algorithm neither assumes generic position of the input system nor demands for any change of the coordinate system. The latter is due to a novel inclusion predicate to certify that a certain region is isolating for a solution. Our implementation exploits graphics hardware to expedite the resultant computation. Furthermore, we integrate a number of filtering techniques to improve the overall performance. Efficiency of the proposed method is proven by a comparison of our implementation with two state-of-the-art implementations, that is, Lgp and Maple's Isolate. For a series of challenging benchmark instances, experiments show that our implementation outperforms both contestants.", "venue": "ALENEX", "authors": ["Eric  Berberich", "Pavel  Emeliyanenko", "Michael  Sagraloff"], "year": 2011, "n_citations": 36}
{"id": 4750841, "s2_id": "c0e77fe5018adbb38769e00fb9b39182dac221c6", "title": "Recursive blocked algorithms for linear systems with Kronecker product structure", "abstract": "Recursive blocked algorithms have proven to be highly efficient at the numerical solution of the Sylvester matrix equation and its generalizations. In this work, we show that these algorithms extend in a seamless fashion to higher-dimensional variants of generalized Sylvester matrix equations, as they arise from the discretization of PDEs with separable coefficients or the approximation of certain models in macroeconomics. By combining recursions with a mechanism for merging dimensions, an efficient algorithm is derived that outperforms existing approaches based on Sylvester solvers.", "venue": "Numerical Algorithms", "authors": ["Minhong  Chen", "Daniel  Kressner"], "year": 2019, "n_citations": 7}
{"id": 4758545, "s2_id": "5a354f6e44441e188562c3e65afe3e91da79b593", "title": "Matrix Distributed Processing: A set of C++ Tools for implementing generic lattice computations on parallel systems", "abstract": "Abstract We present a set of programming tools (classes and functions written in C++ and based on Message Passing Interface) for fast development of generic parallel (and non-parallel) lattice simulations. They are collectively called MDP\u00a01.2 . These programming tools include classes and algorithms for matrices, random number generators, distributed lattices (with arbitrary topology), fields and parallel iterations. No previous knowledge of MPI is required in order to use them. Some applications in electromagnetism, electronics, condensed matter and lattice QCD are presented.", "venue": "ArXiv", "authors": ["Massimo Di Pierro"], "year": 2000, "n_citations": 9}
{"id": 4759953, "s2_id": "c43f9f34b0c26d82e2e4c0dddcac462d7db37ae7", "title": "Ranking and Unranking of Hereditarily Finite Functions and Permutations", "abstract": "Prolog's ability to return multiple answers on backtracking provides an elegant mechanism to derive reversible encodings of combinatorial objects as Natural Numbers i.e. {\\em ranking} and {\\em unranking} functions. Starting from a generalization of Ackerman's encoding of Hereditarily Finite Sets with Urelements and a novel tupling/untupling operation, we derive encodings for Finite Functions and use them as building blocks for an executable theory of {\\em Hereditarily Finite Functions}. The more difficult problem of {\\em ranking} and {\\em unranking} {\\em Hereditarily Finite Permutations} is then tackled using Lehmer codes and factoradics. \nThe paper is organized as a self-contained literate Prolog program available at \\url{this http URL}", "venue": "ArXiv", "authors": ["Paul  Tarau"], "year": 2008, "n_citations": 1}
{"id": 4763252, "s2_id": "b21d9d3f4d4f26f722a875817914ecfdaea1d375", "title": "MathGR: a tensor and GR computation package to keep it simple", "abstract": "We introduce the MathGR package, written in Mathematica. The package can manipulate tensor and GR calculations with either abstract or explicit indices, simplify tensors with permutational symmetries, decompose tensors from abstract indices to partially or completely explicit indices and convert partial derivatives into total derivatives. Frequently used GR tensors and a model of FRW universe with ADM type perturbations are predefined. The package is built around the philosophy to \"keep it simple\", and makes use of latest tensor technologies of Mathematica.", "venue": "ArXiv", "authors": ["Yi  Wang"], "year": 2013, "n_citations": 8}
{"id": 4769814, "s2_id": "4af18cffb650e15ca2c9164fe5548e866148ba08", "title": "Sapporo2: a versatile direct N-body library", "abstract": "Astrophysical direct N-body methods have been one of the first production algorithms to be implemented using NVIDIA\u2019s CUDA architecture. Now, almost seven years later, the GPU is the most used accelerator device in astronomy for simulating stellar systems. In this paper we present the implementation of the Sapporo2N-body library, which allows researchers to use the GPU for N-body simulations with little to no effort. The first version, released five years ago, is actively used, but lacks advanced features and versatility in numerical precision and support for higher order integrators. In this updated version we have rebuilt the code from scratch and added support for OpenCL, multi-precision and higher order integrators. We show how to tune these codes for different GPU architectures and present how to continue utilizing the GPU optimal even when only a small number of particles (N<100$N < 100$) is integrated. This careful tuning allows Sapporo2 to be faster than Sapporo1 even with the added options and double precision data loads. The code runs on a range of NVIDIA and AMD GPUs in single and double precision accuracy. With the addition of OpenCL support the library is also able to run on CPUs and other accelerators that support OpenCL.", "venue": "ArXiv", "authors": ["Jeroen  B\u00e9dorf", "Evghenii  Gaburov", "Simon Portegies Zwart"], "year": 2015, "n_citations": 7}
{"id": 4770415, "s2_id": "5a51638a554e9fe76e0577a98d85a30accb3cc2b", "title": "Multi-dimensional interpolations in C++", "abstract": "A C++ software design is presented that can be used to interpolate data in any number of dimensions. The design is based on a combination of templates of functional collections of elements and so-called type lists. The design allows for different search methodologies and interpolation techniques in each dimension. It is also possible to expand and reduce the number of dimensions, to interpolate composite data types and to produce on-the-fly additional values such as derivatives of the interpolating function.", "venue": "ArXiv", "authors": ["Maarten de Jong"], "year": 2019, "n_citations": 3}
{"id": 4771451, "s2_id": "63a69819d3c94524e10f13ce99e70eae2ab3fd4d", "title": "Convex Optimization in Julia", "abstract": "This paper describes Convex1, a convex optimization modeling framework in Julia. Convex translates problems from a user-friendly functional language into an abstract syntax tree describing the problem. This concise representation of the global structure of the problem allows Convex to infer whether the problem complies with the rules of disciplined convex programming (DCP), and to pass the problem to a suitable solver. These operations are carried out in Julia using multiple dispatch, which dramatically reduces the time required to verify DCP compliance and to parse a problem into conic form. Convex then automatically chooses an appropriate backend solver to solve the conic form problem.", "venue": "2014 First Workshop for High Performance Technical Computing in Dynamic Languages", "authors": ["Madeleine  Udell", "Karanveer  Mohan", "David  Zeng", "Jenny  Hong", "Steven  Diamond", "Stephen P. Boyd"], "year": 2014, "n_citations": 111}
{"id": 4771715, "s2_id": "3137a131ac846f106b499c38241e646b02f3bdb7", "title": "A quantitative performance analysis for Stokes solvers at the extreme scale", "abstract": "This article presents a systematic quantitative performance analysis for large finite element computations on extreme scale computing systems. Three parallel iterative solvers for the Stokes system, discretized by low order tetrahedral elements, are compared with respect to their numerical efficiency and their scalability running on up to $786\\,432$ parallel threads. A genuine multigrid method for the saddle point system using an Uzawa-type smoother provides the best overall performance with respect to memory consumption and time-to-solution. The largest system solved on a Blue Gene/Q system has more than ten trillion ($1.1 \\cdot 10 ^{13}$) unknowns and requires about 13 minutes compute time. Despite the matrix free and highly optimized implementation, the memory requirement for the solution vector and the auxiliary vectors is about 200 TByte. Brandt's notion of \"textbook multigrid efficiency\" is employed to study the algorithmic performance of iterative solvers. A recent extension of this paradigm to \"parallel textbook multigrid efficiency\" makes it possible to assess also the efficiency of parallel iterative solvers for a given hardware architecture in absolute terms. The efficiency of the method is demonstrated for simulating incompressible fluid flow in a pipe filled with spherical obstacles.", "venue": "ArXiv", "authors": ["Bj\u00f6rn  Gmeiner", "Markus  Huber", "Lorenz  John", "Ulrich  R\u00fcde", "Barbara I. Wohlmuth"], "year": 2015, "n_citations": 6}
{"id": 4772545, "s2_id": "9f703c13ecedef344b8d824ca448d5c9f1edae1f", "title": "AD in Fortran, Part 1: Design", "abstract": "We propose extensions to Fortran which integrate forward and reverse Automatic Differentiation (AD) directly into the programming model. Irrespective of implementation technology, embedding AD constructs directly into the language extends the reach and convenience of AD while allowing abstraction of concepts of interest to scientific-computing practice, such as root finding, optimization, and finding equilibria of continuous games. Multiple different subprograms for these tasks can share common interfaces, regardless of whether and how they use AD internally. A programmer can maximize a function F by calling a library maximizer, XSTAR=ARGMAX(F,X0), which internally constructs derivatives of F by AD, without having to learn how to use any particular AD tool. We illustrate the utility of these extensions by example: programs become much more concise and closer to traditional mathematical notation. A companion paper describes how these extensions can be implemented by a program that generates input to existing Fortran-based AD tools.", "venue": "ArXiv", "authors": ["Alexey  Radul", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "year": 2012, "n_citations": 4}
{"id": 4784547, "s2_id": "58b8391dc9782639521f38be36432db16a2c6ca6", "title": "A new indexed approach to render the attractors of Kleinian groups", "abstract": "One widespread procedure to render the attractor of Kleinian groups, published in the renown book \"Indra's Pearls\" and based upon a combinatorial tree model, wants huge memory resources to compute and store all the words required. We will present here a new faster and lighter version which drops the original words array and pulls out words from integer numbers.", "venue": "ArXiv", "authors": ["Alessandro  Rosa"], "year": 2017, "n_citations": 0}
{"id": 4785436, "s2_id": "8009dc0da4dd549b96f26e33f266f90b2e730ceb", "title": "A Faster, More Intuitive RooFit", "abstract": "RooFit and RooStats, the toolkits for statistical modelling in ROOT, are used in most searches and measurements at the Large Hadron Collider as well as atBfactories. Larger datasets to be collected at e.g. the HighLuminosity LHC will enable measurements with higher precision, but will require faster data processing to keep fitting times stable. In this work, a simplification of RooFit\u2019s interfaces and a redesign of its internal dataflow is presented. Interfaces are being extended to look and feel more STL-like to be more accessible both from C++ and Python to improve interoperability and ease of use, while maintaining compatibility with old code. The redesign of the dataflow improves cache locality and data loading, and can be used to process batches of data with vectorised SIMD computations. This reduces the time for computing unbinned likelihoods by a factor four to 16. This will allow to fit larger datasets of the future in the same time or faster than today\u2019s fits.", "venue": "EPJ Web of Conferences", "authors": ["Stephan  Hageboeck"], "year": 2020, "n_citations": 3}
{"id": 4798235, "s2_id": "93cf3e1431b8d4160c95e1718b400923bf5e5ab7", "title": "Resilience for Multigrid Software at the Extreme Scale", "abstract": "Fault tolerant algorithms for the numerical approximation of elliptic partial differential equations on modern supercomputers play a more and more important role in the future design of exa-scale enabled iterative solvers. Here, we combine domain partitioning with highly scalable geometric multigrid schemes to obtain fast and fault-robust solvers in three dimensions. The recovery strategy is based on a hierarchical hybrid concept where the values on lower dimensional primitives such as faces are stored redundantly and thus can be recovered easily in case of a failure. The lost volume unknowns in the faulty region are re-computed approximately with multigrid cycles by solving a local Dirichlet problem on the faulty subdomain. Different strategies are compared and evaluated with respect to performance, computational cost, and speed up. Especially effective are strategies in which the local recovery in the faulty region is executed in parallel with global solves and when the local recovery is additionally accelerated. This results in an asynchronous multigrid iteration that can fully compensate faults. Excellent parallel performance on a current peta-scale system is demonstrated.", "venue": "ArXiv", "authors": ["Markus  Huber", "Bj\u00f6rn  Gmeiner", "Ulrich  R\u00fcde", "Barbara I. Wohlmuth"], "year": 2015, "n_citations": 10}
{"id": 4801450, "s2_id": "c59152dc2ab848e93710317a52cb77a3c5535b4e", "title": "The Research and Optimization of Parallel Finite Element Algorithm based on MiniFE", "abstract": "Finite element method (FEM) is one of the most important numerical methods in modern engineering design and analysis. Since traditional serial FEM is difficult to solve large FE problems efficiently and accurately, high-performance parallel FEM has become one of the essential way to solve practical engineering problems. Based on MiniFE program, which is released by National Energy Research Scientific Computing Center(NERSC), this work analyzes concrete steps, key computing pattern and parallel mechanism of parallel FEM. According to experimental results, this work analyzes the proportion of calculation amount of each module and concludes the main performance bottleneck of the program. Based on that, we optimize the MiniFE program on a server platform. The optimization focuses on the bottleneck of the program - SpMV kernel, and uses an efficient storage format named BCRS. Moreover, an improving plan of hybrid MPI+OpenMP programming is provided. Experimental results show that the optimized program performs better in both SpMV kernel and synchronization. It can increase the performance of the program, on average, by 8.31%. Keywords : finite element, parallel, MiniFE, SpMV, performance optimization", "venue": "ArXiv", "authors": ["Meng  Wu", "Can  Yang", "Taoran  Xiang", "Daning  Cheng"], "year": 2015, "n_citations": 0}
{"id": 4801979, "s2_id": "0461e391cb2ef8c1f3deeb864efb30e7a1416622", "title": "Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation", "abstract": "Optimization on manifolds is a class of methods for optimization of an objective function, subject to constraints which are smooth, in the sense that the set of points which satisfy the constraints admits the structure of a differentiable manifold. While many optimization problems are of the described form, technicalities of differential geometry and the laborious calculation of derivatives pose a significant barrier for experimenting with these methods. \nWe introduce Pymanopt (available at this https URL), a toolbox for optimization on manifolds, implemented in Python, that---similarly to the Manopt Matlab toolbox---implements several manifold geometries and optimization algorithms. Moreover, we lower the barriers to users further by using automated differentiation for calculating derivative information, saving users time and saving them from potential calculation and implementation errors.", "venue": "J. Mach. Learn. Res.", "authors": ["James  Townsend", "Niklas  Koep", "Sebastian  Weichwald"], "year": 2016, "n_citations": 113}
{"id": 4802315, "s2_id": "523811f83fac73e6a67cbd3a2298e8e83cdc2dd4", "title": "A Heuristic Prover for Real Inequalities", "abstract": "We describe a general method for verifying inequalities between real-valued expressions, especially the kinds of straightforward inferences that arise in interactive theorem proving. In contrast to approaches that aim to be complete with respect to a particular language or class of formulas, our method establishes claims that require heterogeneous forms of reasoning, relying on a Nelson-Oppen-style architecture in which special-purpose modules collaborate and share information. The framework is thus modular and extensible. A prototype implementation shows that the method works well on a variety of examples, and complements techniques that are used by contemporary interactive provers.", "venue": "Journal of Automated Reasoning", "authors": ["Jeremy  Avigad", "Robert Y. Lewis", "Cody  Roux"], "year": 2015, "n_citations": 7}
{"id": 4802704, "s2_id": "e15133cfd09c2a2eb98c38468a897866a904d125", "title": "NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations", "abstract": "Physics-informed neural networks (PINNs) are an increasingly powerful way to solve partial differential equations, generate digital twins, and create neural surrogates of physical models. In this manuscript we detail the various methodologies of PINNs and showcase the various types of problems a PINN software can solve. We then detail the inner workings of NeuralPDE.jl and show how a formulation structured around numerical quadrature gives rise to new loss functions which allow for adaptivity towards bounded error tolerances. We describe the various ways one can use the tool, detailing mathematical techniques like using extended loss functions for parameter estimation and operator discovery, to help potential users adopt these PINN-based techniques into their workflow. We showcase how NeuralPDE uses a purely symbolic formulation so that all of the underlying training code is generated from an abstract formulation, and show how to make use of GPUs and solve systems of PDEs. Afterwards we give a detailed performance analysis which showcases the trade-off between training techniques on a large set of PDEs. From these comprehensive performance benchmarks we provide guidelines for users of PINN software, such as mixing ADAM optimizers with techniques like BFGS and mixing robust quadrature techniques with faster quasi-random samplers. We end by focusing on a complex multiphysics example, the \u2217Massachusetts Institute of Technology \u2020Julia Computing \u2021University of Turin \u00a7University of Ottawa \u00b6Carnegie Mellon University \u2016Indian Institute of Technology, Roorkee \u2217\u2217Ford Motor Company", "venue": "ArXiv", "authors": ["Kirill  Zubov", "Zoe  McCarthy", "Yingbo  Ma", "Francesco  Calisto", "Valerio  Pagliarino", "Simone  Azeglio", "Luca  Bottero", "Emmanuel  Luj'an", "Valentin  Sulzer", "Ashutosh  Bharambe", "Nand  Vinchhi", "Kaushik  Balakrishnan", "Devesh  Upadhyay", "Chris  Rackauckas"], "year": 2021, "n_citations": 0}
{"id": 4813460, "s2_id": "ebdd2f6a888984ff6a02249cf5775b2d015188ed", "title": "Automatic Differentiation in ROOT", "abstract": "In mathematics and computer algebra, automatic differentiation (AD) is a set of techniques to evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.), elementary functions (exp, log, sin, cos, etc.) and control flow statements. AD takes source code of a function as input and produces source code of the derived function. By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program. This paper presents AD techniques available in ROOT, supported by Cling, to produce derivatives of arbitrary C/C++ functions through implementing source code transformation and employing the chain rule of differential calculus in both forward mode and reverse mode. We explain its current integration for gradient computation in TFormula. We demonstrate the correctness and performance improvements in ROOT\u2019s fitting algorithms.", "venue": "EPJ Web of Conferences", "authors": ["Vassil  Vassilev", "Aleksandr  Efremov", "Oksana  Shadura"], "year": 2020, "n_citations": 3}
{"id": 4821002, "s2_id": "27045c99c8702d73dc44128877ef002945ae8c72", "title": "Comparing Python, Go, and C++ on the N-Queens Problem", "abstract": "Python currently is the dominant language in the field of Machine Learning but is often criticized for being slow to perform certain tasks. In this report, we use the well-known $N$-queens puzzle as a benchmark to show that once compiled using the Numba compiler it becomes competitive with C++ and Go in terms of execution speed while still allowing for very fast prototyping. This is true of both sequential and parallel programs. In most cases that arise in an academic environment, it therefore makes sense to develop in ordinary Python, identify computational bottlenecks, and use Numba to remove them.", "venue": "ArXiv", "authors": ["Pascal  Fua", "Krzysztof  Lis"], "year": 2020, "n_citations": 3}
{"id": 4821930, "s2_id": "7cdf209b7ae00c3da0e9c2fa01201711ae36add7", "title": "Deriving Correct High-Performance Algorithms", "abstract": "Dijkstra observed that verifying correctness of a program is difficult and conjectured that derivation of a program hand-in-hand with its proof of correctness was the answer. We illustrate this goal-oriented approach by applying it to the domain of dense linear algebra libraries for distributed memory parallel computers. We show that algorithms that underlie the implementation of most functionality for this domain can be systematically derived to be correct. The benefit is that an entire family of algorithms for an operation is discovered so that the best algorithm for a given architecture can be chosen. This approach is very practical: Ideas inspired by it have been used to rewrite the dense linear algebra software stack starting below the Basic Linear Algebra Subprograms (BLAS) and reaching up through the Elemental distributed memory library, and every level in between. The paper demonstrates how formal methods and rigorous mathematical techniques for correctness impact HPC.", "venue": "ArXiv", "authors": ["Devangi N. Parikh", "Maggie E. Myers", "Robert A. van de Geijn"], "year": 2017, "n_citations": 2}
{"id": 4822223, "s2_id": "7bed9415b0c9cd4bfdb05df6873777e970e6e1a8", "title": "MPLAPACK version 1.0.0 user manual", "abstract": "The MPLAPACK (formerly MPACK) is a multiple-precision version of LAPACK (https://www.netlib.org/lapack/). MPLAPACK version 1.0.0 is based on LAPACK version 3.9.1 and translated from Fortran 90 to C++ using FABLE, a Fortran to C++ source-to-source conversion tool (https://github.com/cctbx/cctbx_project/tree/master/fable/). MPLAPACK version 1.0.0 provides the real and complex version of MPBLAS, and the real version of MPLAPACK supports all LAPACK features: solvers for systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue problems, and singular value problems, and related matrix factorizations except for rectangular complete packed matrix form and mixed-precision routines. Besides, MPLAPACK supports a part of the complex routines of LAPACK, such as diagonalization of Hermite matrices, Cholesky factorization, and matrix inversion. The MPLAPACK defines an API for numerical linear algebra, similar to LAPACK. It is easy to port legacy C/C++ numerical codes using MPLAPACK. MPLAPACK supports binary64, binary128, FP80 (extended double), MPFR, GMP, and QD libraries (double-double and quad-double). Users can choose MPFR or GMP for arbitrary accurate calculations, double-double or quad-double for fast 32 or 64 decimal calculations. We can consider the binary64 version as the C++ version of LAPACK. MPLAPACK is available at GitHub (https://github.com/nakatamaho/mplapack/) under the 2-clause BSD license.", "venue": "ArXiv", "authors": ["Maho  Nakata"], "year": 2021, "n_citations": 0}
{"id": 4823509, "s2_id": "f75fbc807e077699390a2100ae20584856ab9a0b", "title": "Extending the statistical software package Engine for Likelihood-Free Inference", "abstract": "Bayesian inference is a principled framework for dealing with uncertainty. The practitioner can perform an initial assumption for the physical phenomenon they want to model (prior belief), collect some data and then adjust the initial assumption in the light of the new evidence (posterior belief). Approximate Bayesian Computation (ABC) methods, also known as likelihood-free inference techniques, are a class of models used for performing inference when the likelihood is intractable. The unique requirement of these models is a black-box sampling machine. Due to the modelling-freedom they provide these approaches are particularly captivating. Robust Optimisation Monte Carlo (ROMC) is one of the most recent techniques of the specific domain. It approximates the posterior distribution by solving independent optimisation problems. This dissertation focuses on the implementation of the ROMC method in the software package Engine for Likelihood-Free Inference (ELFI). In the first chapters, we provide the mathematical formulation and the algorithmic description of the ROMC approach. In the following chapters, we describe our implementation; (a) we present all the functionalities provided to the user and (b) we demonstrate how to perform inference on some real examples. Our implementation provides a robust and efficient solution to a practitioner who wants to perform inference on a simulator-based model. Furthermore, it exploits parallel processing for accelerating the inference wherever it is possible. Finally, it has been designed to serve extensibility; the user can easily replace specific subparts of the method without significant overhead on the development side. Therefore, it can be used by a researcher for further experimentation.", "venue": "ArXiv", "authors": ["Vasileios  Gkolemis", "Michael  Gutmann"], "year": 2020, "n_citations": 0}
{"id": 4824246, "s2_id": "a400dd28f4185325e9faa5d15b56a81c5ce5cb2e", "title": "xTras: A field-theory inspired xAct package for mathematica", "abstract": "Abstract We present the tensor computer algebra package xTras , which provides functions and methods frequently needed when doing (classical) field theory. Amongst others, it can compute contractions, make Ansatze, and solve tensorial equations. It is built upon the tensor computer algebra system xAct , a collection of packages for Mathematica. Program summary Program title: xTras Catalogue identifier: AESH_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AESH_v1_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 155\u00a0879 No. of bytes in distributed program, including test data, etc.: 565\u00a0389 Distribution format: tar.gz Programming language: Mathematica. Computer: Any computer running Mathematica 6 or newer. Operating system: Linux, Unix, Windows, OS X. RAM: 100 Mb Classification: 5. External routines: xACT ( www.xact.es ) Subprograms used: Cat Id Title Reference AEBH_v1_0 xPerm CPC 179 (2008) 597 ADZK_v2_0 Invar Tensor Package 2.0 CPC 179 (2008) 586 Nature of problem: Common problems in classical field theory: making Ansatze, computing contractions, solving tensorial equations, etc. Solution method: Various (group theory, brute-force, built-in Mathematica functions, etc.) Running time: 1\u201360\u00a0s", "venue": "Comput. Phys. Commun.", "authors": ["Teake  Nutma"], "year": 2014, "n_citations": 132}
{"id": 4830040, "s2_id": "ddae0bc49ddaa713ea7e3a7c2a68f048fe91bf81", "title": "Orbital-Free Density Functional Theory Implementation with the Projector Augmented-Wave Method", "abstract": "We present a computational scheme for orbital-free density functional theory (OFDFT) that simultaneously provides access to all-electron values and preserves the OFDFT linear scaling as a function of the system size. Using the projector augmented-wave method (PAW) in combination with real-space methods, we overcome some obstacles faced by other available implementation schemes. Specifically, the advantages of using the PAW method are twofold. First, PAW reproduces all-electron values offering freedom in adjusting the convergence parameters and the atomic setups allow tuning the numerical accuracy per element. Second, PAW can provide a solution to some of the convergence problems exhibited in other OFDFT implementations based on Kohn-Sham (KS) codes. Using PAW and real-space methods, our orbital-free results agree with the reference all-electron values with a mean absolute error of 10 meV and the number of iterations required by the self-consistent cycle is comparable to the KS method. The comparison of all-electron and pseudopotential bulk modulus and lattice constant reveal an enormous difference, demonstrating that in order to assess the performance of OFDFT functionals it is necessary to use implementations that obtain all-electron values. The proposed combination of methods is the most promising route currently available. We finally show that a parametrized kinetic energy functional can give lattice constants and bulk moduli comparable in accuracy to those obtained by the KS PBE method, exemplified with the case of diamond.", "venue": "The Journal of chemical physics", "authors": ["Jouko  Lehtom\u00e4ki", "Ilja  Makkonen", "Miguel A. Caro", "Ari  Harju", "Olga  Lopez-Acevedo"], "year": 2014, "n_citations": 66}
{"id": 4830933, "s2_id": "2c18adafbf8c08edc16b3a0ecad9d9bc025cf3b0", "title": "Parallel Software to Offset the Cost of Higher Precision", "abstract": "Hardware double precision is often insufficient to solve large scientific problems accurately. Computing in higher precision defined by software causes significant computational overhead. The application of parallel algorithms compensates for this overhead. Newton's method to develop power series expansions of algebraic space curves is the use case for this application.", "venue": "ArXiv", "authors": ["Jan  Verschelde"], "year": 2020, "n_citations": 2}
{"id": 4834027, "s2_id": "fa3d34c7066592dc6007271e1d116de2a136f812", "title": "GPU Algorithms for Efficient Exascale Discretizations", "abstract": "In this paper we describe the research and development activities in the Center for Efficient Exascale Discretization within the US Exascale Computing Project, targeting state-of-the-art high-order finite-element algorithms for high-order applications on GPU-accelerated platforms. We discuss the GPU developments in several components of the CEED software stack, including the libCEED, MAGMA, MFEM, libParanumal, and Nek projects. We report performance and capability improvements in several CEED-enabled applications on both NVIDIA and AMD GPU systems.", "venue": "Parallel Comput.", "authors": ["Ahmad  Abdelfattah", "Valeria  Barra", "Natalie  Beams", "Ryan  Bleile", "Jed  Brown", "Sylvain  Camier", "Robert  Carson", "Noel  Chalmers", "Veselin  Dobrev", "Yohann  Dudouit", "Paul  Fischer", "Ali  Karakus", "Stefan  Kerkemeier", "Tzanio V. Kolev", "Yu-Hsiang  Lan", "Elia  Merzari", "Misun  Min", "Malachi  Phillips", "Thilina  Rathnayake", "Robert N. Rieben", "Thomas  Stitt", "Ananias  Tomboulides", "Stanimire  Tomov", "Vladimir Z. Tomov", "Arturo  Vargas", "Timothy C. Warburton", "Kenneth  Weiss"], "year": 2021, "n_citations": 2}
{"id": 4836154, "s2_id": "e91a9d1c7707f81b3d1d1af171875ce7c0538d83", "title": "DistStat.jl: Towards Unified Programming for High-Performance Statistical Computing Environments in Julia", "abstract": "The demand for high-performance computing (HPC) is ever-increasing for everyday statistical computing purposes. The downside is that we need to write specialized code for each HPC environment. CPU-level parallelization needs to be explicitly coded for effective use of multiple nodes in cluster supercomputing environments. Acceleration via graphics processing units (GPUs) requires to write kernel code. The Julia software package DistStat.jl implements a data structure for distributed arrays that work on both multi-node CPU clusters and multi-GPU environments transparently. This package paves a way to developing high-performance statistical software in various HPC environments simultaneously. As a demonstration of the transparency and scalability of the package, we provide applications to large-scale nonnegative matrix factorization, multidimensional scaling, and $\\ell_1$-regularized Cox proportional hazards model on an 8-GPU workstation and a 720-CPU-core virtual cluster in Amazon Web Services (AWS) cloud. As a case in point, we analyze the on-set of type-2 diabetes from the UK Biobank with 400,000 subjects and 500,000 single nucleotide polymorphisms using the $\\ell_1$-regularized Cox proportional hazards model. Fitting a half-million-variate regression model took less than 50 minutes on AWS.", "venue": "ArXiv", "authors": ["Seyoon  Ko", "Hua  Zhou", "Jin  Zhou", "Joong-Ho  Won"], "year": 2020, "n_citations": 2}
{"id": 4837484, "s2_id": "34a64cdb8eec6571e40ede1f0f2cc0cd869ff204", "title": "H2Opus: A distributed-memory multi-GPU software package for non-local operators", "abstract": "Hierarchical H2-matrices are asymptotically optimal representations for the discretizations of non-local operators such as those arising in integral equations or from kernel functions. Their O(N) complexity in both memory and operator application makes them particularly suited for largescale problems. As a result, there is a need for software that provides support for distributed operations on these matrices to allow large-scale problems to be represented. In this paper, we present high-performance, distributed-memory GPU-accelerated algorithms and implementations for matrix-vector multiplication and matrix recompression of hierarchical matrices in the H2 format. The algorithms are a new module of H2Opus, a performance-oriented package that supports a broad variety of H2 matrix operations on CPUs and GPUs. Performance in the distributed GPU setting is achieved by marshaling the tree data of the hierarchical matrix representation to allow batched kernels to be executed on the individual GPUs. MPI is used for inter-process communication. We optimize the communication data volume and hide much of the communication cost with local compute phases of the algorithms. Results show near-ideal scalability up to 1024 NVIDIA V100 GPUs on Summit, with performance exceeding 2.3 Tflop/s/GPU for the matrix-vector multiplication, and 670 Gflops/s/GPU for matrix compression, which involves batched QR and SVD operations. We illustrate the flexibility and efficiency of the library by solving a 2D variable diffusivity integral fractional diffusion problem with an algebraic multigridpreconditioned Krylov solver and demonstrate scalability up to 16M degrees of freedom problems on 64 GPUs. Extreme Computing Research Center, Computer, Electrical and Mathematical Science and Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia E-mail addresses: stefano.zampini@kaust.edu.sa, wajih.boukaram@kaust.edu.sa, george.turkiyyah@kaust.edu.sa, omar.knio@kaust.edu.sa, david.keyes@kaust.edu.sa. 1 ar X iv :2 10 9. 05 45 1v 1 [ cs .D C ] 1 2 Se p 20 21", "venue": "ArXiv", "authors": ["Stefano  Zampini", "Wajih  Boukaram", "George  Turkiyyah", "Omar  Knio", "David E. Keyes"], "year": 2021, "n_citations": 0}
{"id": 4848382, "s2_id": "da68db75cc5dfeb39c2ba9f88028a5290996e7f3", "title": "A Cross-Platform Benchmark for Interval Computation Libraries", "abstract": "Interval computation is widely used to certify computations that use floating point operations to avoid pitfalls related to rounding error introduced by inaccurate operations. Despite its popularity and practical benefits, support for interval arithmetic is not standardized nor available in mainstream programming languages. We propose the first benchmark for interval computations, coupled with reference solutions computed with exact arithmetic, and compare popular C and C++ libraries over different architectures, operating systems, and compilers. The benchmark allows identifying limitations in existing implementations, and provides a reliable guide on which library to use on each system. We believe that our benchmark will be useful for developers of future interval libraries, as a way to test the correctness and performance of their algorithms.", "venue": "ArXiv", "authors": ["Xuan  Tang", "Zachary  Ferguson", "Teseo  Schneider", "Denis  Zorin", "Shoaib  Kamil", "Daniele  Panozzo"], "year": 2021, "n_citations": 0}
{"id": 4851713, "s2_id": "56e8215d410f575561778299765eab5227a9bfd6", "title": "Generating Families of Practical Fast Matrix Multiplication Algorithms", "abstract": "Matrix multiplication (GEMM) is a core operation to numerous scientific applications. Traditional implementations of Strassen-like fast matrix multiplication (FMM) algorithms often do not perform well except for very large matrix sizes, due to the increased cost of memory movement, which is particularly noticeable for non-square matrices. Such implementations also require considerable workspace and modifications to the standard BLAS interface. We propose a code generator framework to automatically implement a large family of FMM algorithms suitable for multiplications of arbitrary matrix sizes and shapes. By representing FMM with a triple of matrices [U, V, W] that capture the linear combinations of submatrices that are formed, we can use the Kronecker product to define a multi-level representation of Strassen-like algorithms. Incorporating the matrix additions that must be performed for Strassen-like algorithms into the inherent packing and micro-kernel operations inside GEMM avoids extra workspace and reduces the cost of memory movement. Adopting the same loop structures as high-performance GEMM implementations allows parallelization of all FMM algorithms with simple but efficient data parallelism without the overhead of task parallelism. We present a simple performance model for general FMM algorithms and compare actual performance of 20+ FMM algorithms to modeled predictions. Our implementations demonstrate a performance benefit over conventional GEMM on single core and multi-core systems. This study shows that Strassen-like fast matrix multiplication can be incorporated into libraries for practical use.", "venue": "2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "authors": ["Jianyu  Huang", "Leslie  Rice", "Devin A. Matthews", "Robert A. van de Geijn"], "year": 2017, "n_citations": 27}
{"id": 4852033, "s2_id": "92499d794fbbf0b88e18250df4a604c3a9389298", "title": "The virtual reality framework for engineering objects", "abstract": "A framework for virtual reality of engineering objects has been developed. This framework may simulate different equipment related to virtual reality. Framework supports 6D dynamics, ordinary differential equations, finite formulas, vector and matrix operations. The framework also supports embedding of external software.", "venue": "ArXiv", "authors": ["Petr R. Ivankov", "Nikolay P. Ivankov"], "year": 2006, "n_citations": 1}
{"id": 4853490, "s2_id": "2fa9f901a2c74660bc7ba5c08138003cfee1c175", "title": "An example of Clifford algebras calculations with GiNaC", "abstract": "Abstract.This is an example of C++ code of Clifford algebra calculations with the GiNaC computer algebra system. This code makes both symbolic and numeric computations. It was used to produce illustrations for paper [14, 12].Described features of GiNaC are already available at PyGiNaC [3] and due to course should propagate into other software like GNU Octave [7] and gTybalt [18] which use GiNaC library as their back-end.", "venue": "ArXiv", "authors": ["Vladimir V. Kisil"], "year": 2004, "n_citations": 6}
{"id": 4859944, "s2_id": "ae0341160339d793909db546c2f17fd1296a0fe0", "title": "Conformal Computing: Algebraically connecting the hardware/software boundary using a uniform approach to high-performance computation for software and hardware applications", "abstract": "We present a systematic, algebraically based, design methodology for efficient implementation of computer programs optimized over multiple levels of the processor/memory and network hierarchy. Using a common formalism to describe the problem and the partitioning of data over processors and memory levels allows one to mathematically prove the efficiency and correctness of a given algorithm as measured in terms of a set of metrics (such as processor/network speeds, etc.). The approach allows the average programmer to achieve high-level optimizations similar to those used by compiler writers (e.g. the notion of \"tiling\"). \nThe approach presented in this monograph makes use of A Mathematics of Arrays (MoA, Mullin 1988) and an indexing calculus (i.e. the psi-calculus) to enable the programmer to develop algorithms using high-level compiler-like optimizations through the ability to algebraically compose and reduce sequences of array operations. Extensive discussion and benchmark results are presented for the Fast Fourier Transform and other important algorithms.", "venue": "ArXiv", "authors": ["Lenore R. Mullin", "James E. Raynolds"], "year": 2008, "n_citations": 5}
{"id": 4860869, "s2_id": "2037b803f239437452177be1e2f38a6059f78005", "title": "Computer Assisted Parallel Program Generation", "abstract": "Parallel computation is widely employed in scientific researches, engineering activities and product development. Parallel program writing itself is not always a simple task depending on problems solved. Large-scale scientific computing, huge data analyses and precise visualizations, for example, would require parallel computations, and the parallel computing needs the parallelization techniques. In this Chapter a parallel program generation support is discussed, and a computer-assisted parallel program generation system P-NCAS is introduced. Computer assisted problem solving is one of key methods to promote innovations in science and engineering, and contributes to enrich our society and our life toward a programming-free environment in computing science. Problem solving environments (PSE) research activities had started to enhance the programming power in 1970's. The P-NCAS is one of the PSEs; The PSE concept provides an integrated human-friendly computational software and hardware system to solve a target class of problems", "venue": "ArXiv", "authors": ["Shigeo  Kawata"], "year": 2015, "n_citations": 1}
{"id": 4868280, "s2_id": "2370d4033e8296445f27dbfc086401e0e271aaab", "title": "Computational and applied topology, tutorial", "abstract": "This is a tutorial in applied and computational topology and topological data analysis. It is illustrated with numerous computational examples that utilize Gudhi library. It is under constant development, so please do not consider this version as final.", "venue": "ArXiv", "authors": ["Pawel  Dlotko"], "year": 2018, "n_citations": 0}
{"id": 4868581, "s2_id": "1c731e7d31a04f12c541ec40babc468237aecdf6", "title": "A new highly parallel non-Hermitian eigensolver", "abstract": "Calculating portions of eigenvalues and eigenvectors of matrices or matrix pencils has many applications. An approach to this calculation for Hermitian problems based on a density matrix has been proposed in 2009 and a software package called FEAST has been developed. The density-matrix approach allows FEAST's implementation to exploit a key strength of modern computer architectures, namely, multiple levels of parallelism. Consequently, the software package has been well received and subsequently commercialized. A detailed theoretical analysis of Hermitian FEAST has also been established very recently. This paper generalizes the FEAST algorithm and theory, for the first time, to tackle non-Hermitian problems. Fundamentally, the new algorithm is basic subspace iteration or Bauer bi-iteration, except applied with a novel accelerator based on Cauchy integrals. The resulting algorithm retains the multi-level parallelism of Hermitian FEAST, making it a valuable new tool for large-scale computational science and engineering problems on leading-edge computing platforms.", "venue": "SpringSim", "authors": ["Ping Tak Peter Tang", "James  Kestyn", "Eric  Polizzi"], "year": 2014, "n_citations": 12}
{"id": 4875337, "s2_id": "94897918980b3951ff6c492a8c5c30cd2c13acd6", "title": "Automatic generation of efficient sparse tensor format conversion routines", "abstract": "This paper shows how to generate code that efficiently converts sparse tensors between disparate storage formats (data layouts) such as CSR, DIA, ELL, and many others. We decompose sparse tensor conversion into three logical phases: coordinate remapping, analysis, and assembly. We then develop a language that precisely describes how different formats group together and order a tensor\u2019s nonzeros in memory. This lets a compiler emit code that performs complex remappings of nonzeros when converting between formats. We also develop a query language that can extract statistics about sparse tensors, and we show how to emit efficient analysis code that computes such queries. Finally, we define an abstract interface that captures how data structures for storing a tensor can be efficiently assembled given specific statistics about the tensor. Disparate formats can implement this common interface, thus letting a compiler emit optimized sparse tensor conversion code for arbitrary combinations of many formats without hard-coding for any specific combination. Our evaluation shows that the technique generates sparse tensor conversion routines with performance between 1.00 and 2.01\u00d7 that of hand-optimized versions in SPARSKIT and Intel MKL, two popular sparse linear algebra libraries. And by emitting code that avoids materializing temporaries, which both libraries need for many combinations of source and target formats, our technique outperforms those libraries by 1.78 to 4.01\u00d7 for CSC/COO to DIA/ELL conversion.", "venue": "PLDI", "authors": ["Stephen  Chou", "Fredrik  Kjolstad", "Saman  Amarasinghe"], "year": 2020, "n_citations": 7}
{"id": 4875497, "s2_id": "88f008846c31cb5e750d666a925cfacffcabea87", "title": "Scaling Structured Multigrid to 500K+ Cores through Coarse-Grid Redistribution", "abstract": "The efficient solution of sparse, linear systems resulting from the discretization of partial differential equations is crucial to the performance of many physics-based simulations. The algorithmic optimality of multilevel approaches for common discretizations makes them a good candidate for an efficient parallel solver. Yet, modern architectures for high-performance computing systems continue to challenge the parallel scalability of multilevel solvers. While algebraic multigrid methods are robust for solving a variety of problems, the increasing importance of data locality and cost of data movement in modern architectures motivates the need to carefully exploit structure in the problem. \nRobust logically structured variational multigrid methods, such as Black Box Multigrid (BoxMG), maintain structure throughout the multigrid hierarchy. This avoids indirection and increased coarse-grid communication costs typical in parallel algebraic multigrid. Nevertheless, the parallel scalability of structured multigrid is challenged by coarse-grid problems where the overhead in communication dominates computation. In this paper, an algorithm is introduced for redistributing coarse-grid problems through incremental agglomeration. Guided by a predictive performance model, this algorithm provides robust redistribution decisions for structured multilevel solvers. \nA two-dimensional diffusion problem is used to demonstrate the significant gain in performance of this algorithm over the previous approach that used agglomeration to one processor. In addition, the parallel scalability of this approach is demonstrated on two large-scale computing systems, with solves on up to 500K+ cores.", "venue": "SIAM J. Sci. Comput.", "authors": ["Andrew  Reisner", "Luke N. Olson", "J. David Moulton"], "year": 2018, "n_citations": 5}
{"id": 4878210, "s2_id": "5cd880e020f51a01061bd31a09452c10b2038b58", "title": "Fatgraph Algorithms and the Homology of the Kontsevich Complex", "abstract": "Fatgraphs are multigraphs enriched with a cyclic order of the edges incident to a vertex. This paper presents algorithms to: (1) generate the set of all fatgraphs having a given genus and number of boundary cycles; (2) compute automorphisms of any given fatgraph; (3) compute the homology of the fatgraph complex. The algorithms are suitable for effective computer implementation. \nIn particular, this allows us to compute the rational homology of the moduli space of Riemann surfaces with marked points. We thus compute the Betti numbers of $M_{g,n}$ with $(2g + n) \\leq 6$, corroborating known results.", "venue": "ArXiv", "authors": ["Riccardo  Murri"], "year": 2012, "n_citations": 4}
{"id": 4878344, "s2_id": "80d182294923223397b8717324a8f61aba1b4de7", "title": "DiffSharp: An AD Library for .NET Languages", "abstract": "DiffSharp is an algorithmic differentiation or automatic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind, allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives---scalars, vectors, and matrices, with a generalization to tensors underway---that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library. DiffSharp currently uses operator overloading, but we are developing a transformation-based version of the library using F#'s \"code quotation\" metaprogramming facility. Work on a CUDA-based GPU backend is also underway.", "venue": "ArXiv", "authors": ["Atilim G\u00fcnes Baydin", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "year": 2016, "n_citations": 11}
{"id": 4881637, "s2_id": "f58841132238a4e41f20958485d0ed2c3390026c", "title": "Making simple proofs simpler", "abstract": "An open partition \\pi{} [Cod09a, Cod09b] of a tree T is a partition of the vertices of T with the property that, for each block B of \\pi, the upset of B is a union of blocks of \\pi. This paper deals with the number, NP(n), of open partitions of the tree, V_n, made of two chains with n points each, that share the root.", "venue": "ArXiv", "authors": ["Pietro  Codara", "Ottavio M. D'Antona", "Francesco  Marigo", "Corrado  Monti"], "year": 2013, "n_citations": 0}
{"id": 4882750, "s2_id": "007a91c4369f5f54869b08641d2f34089fa12b9f", "title": "Blends in Maple", "abstract": "A blend of two Taylor series for the same smooth real- or complex-valued function of a single variable can be useful for approximation. We use an explicit formula for a two-point Hermite interpolational polynomial to construct such blends. We show a robust Maple implementation that can stably and efficiently evaluate blends using linear-cost Horner form, evaluate their derivatives to arbitrary order at the same time, or integrate a blend exactly. The implementation is suited for use with evalhf. We provide a top-level user interface and efficient module exports for programmatic use. \nThis work intended for presentation at the Maple Conference 2020. See this http URL", "venue": "MC", "authors": ["Robert M. Corless", "Erik  Postma"], "year": 2020, "n_citations": 3}
{"id": 4883987, "s2_id": "a0f0d005e337ca80123201572a69823021c17ad2", "title": "UFL Dual Spaces, a proposal", "abstract": "This white paper highlights current limitations in the algebraic closure Unified Form Language (UFL). UFL currently represents forms over finite element spaces, however finite element problems naturally result in objects in the dual to a finite element space, and operators mapping between primal and dual finite element spaces. This document sketches the relevant mathematical areas and proposes changes to the UFL language to support dual spaces as first class types in UFL.", "venue": "ArXiv", "authors": ["David A. Ham"], "year": 2021, "n_citations": 0}
{"id": 4884716, "s2_id": "632dfd80359dcbdf2de73ce81cc9bab867408c04", "title": "Automatic Generation of Efficient Linear Algebra Programs", "abstract": "The level of abstraction at which application experts reason about linear algebra computations and the level of abstraction used by developers of high-performance numerical linear algebra libraries do not match. The former is conveniently captured by high-level languages and libraries such as Matlab and Eigen, while the latter expresses the kernels included in the BLAS and LAPACK libraries. Unfortunately, the translation from a high-level computation to an efficient sequence of kernels is a task, far from trivial, that requires extensive knowledge of both linear algebra and high-performance computing. Internally, almost all high-level languages and libraries use efficient kernels; however, the translation algorithms are too simplistic and thus lead to a suboptimal use of said kernels, with significant performance losses. In order to both achieve the productivity that comes with high-level languages, and make use of the efficiency of low level kernels, we are developing Linnea, a code generator for linear algebra problems. As input, Linnea takes a high-level description of a linear algebra problem and produces as output an efficient sequence of calls to high-performance kernels. In 25 application problems, the code generated by Linnea always outperforms Matlab, Julia, Eigen and Armadillo, with speedups up to and exceeding 10\u00d7.", "venue": "PASC", "authors": ["Henrik  Barthels", "Christos  Psarras", "Paolo  Bientinesi"], "year": 2020, "n_citations": 10}
{"id": 4888754, "s2_id": "8249a9c1999d9b5f6766e0bdc0338223dc6ba089", "title": "A scalable H-matrix approach for the solution of boundary integral equations on multi-GPU clusters", "abstract": "In this work, we consider the solution of boundary integral equations by means of a scalable hierarchical matrix approach on clusters equipped with graphics hardware, i.e. graphics processing units (GPUs). To this end, we extend our existing single-GPU hierarchical matrix library hmglib such that it is able to scale on many GPUs and such that it can be coupled to arbitrary application codes. Using a model GPU implementation of a boundary element method (BEM) solver, we are able to achieve more than 67 percent relative parallel speed-up going from 128 to 1024 GPUs for a model geometry test case with 1.5 million unknowns and a real-world geometry test case with almost 1.2 million unknowns. On 1024 GPUs of the cluster Titan, it takes less than 6 minutes to solve the 1.5 million unknowns problem, with 5.7 minutes for the setup phase and 20 seconds for the iterative solver. To the best of the authors\u2019 knowledge, we here discuss the first fully GPU-based distributed-memory parallel hierarchical matrix Open Source library using the traditional H-matrix format and adaptive cross approximation with an application to BEM problems.", "venue": "ArXiv", "authors": ["Helmut  Harbrecht", "Peter  Zaspel"], "year": 2018, "n_citations": 2}
{"id": 4894708, "s2_id": "baf8330497b23aa817c17e2db0625694127772e8", "title": "SMAT: An Input Adaptive Sparse Matrix-Vector Multiplication Auto-Tuner", "abstract": "Sparse matrix vector multiplication (SpMV) is an important kernel in scientific and engineering applications. The previous optimizations are sparse matrix format specific and expose the choice of the best format to application programmers. In this work we develop an auto-tuning framework to bridge gap between the specific optimized kernels and their general-purpose use. We propose an SpMV autotuner (SMAT) that provides an unified interface based on compressed sparse row (CSR) to programmers by implicitly choosing the best format and the fastest implementation of any input sparse matrix in runtime. SMAT leverage a data mining model, which is formulated based on a set of performance parameters extracted from 2373 matrices in UF sparse matrix collection, to fast search the best combination. The experiments show that SMAT achieves the maximum performance of 75 GFLOP/s in single-precision and 33 GFLOP/s in double-precision on Intel, and 41 GFLOP/s in single-precision and 34 GFLOP/s in double-precision on AMD. Compared with the sparse functions in MKL library, SMAT runs faster by more than 3 times.", "venue": "ArXiv", "authors": ["Jiajia  Li", "Xiuxia  Zhang", "Guangming  Tan", "Mingyu  Chen"], "year": 2012, "n_citations": 4}
{"id": 4899579, "s2_id": "fb319291bcabb4212a2c816b01151cef075bbe84", "title": "Topology of 2D and 3D rational curves", "abstract": "In this paper we present algorithms for computing the topology of planar and space rational curves defined by a parametrization. The algorithms given here work directly with the parametrization of the curve, and do not require to compute or use the implicit equation of the curve (in the case of planar curves) or of any projection (in the case of space curves). Moreover, these algorithms have been implemented in Maple; the examples considered and the timings obtained show good performance skills.", "venue": "Comput. Aided Geom. Des.", "authors": ["Juan Gerardo Alc\u00e1zar", "Gema Mar\u00eda D\u00edaz-Toca"], "year": 2010, "n_citations": 15}
{"id": 4899910, "s2_id": "afafb2405bdfd30933e3e7f0f7ee0a8b9d45a75a", "title": "The Power of Vocabulary: The Case of Cyclotomic Polynomials", "abstract": "We observe that the vocabulary used to construct the \"answer\" to problems in computer algebra can have a dramatic effect on the computational complexity of solving that problem. We recall a formalization of this observation and explain the classic example of sparse polynomial arithmetic. For this case, we show that it is possible to extend the vocabulary so as reap the benefits of conciseness whilst avoiding the obvious pitfall of repeating the problem statement as the \"solution\". \nIt is possible to extend the vocabulary either by irreducible cyclotomics or by $x^n-1$: we look at the options and suggest that the pragmatist might opt for both.", "venue": "ArXiv", "authors": ["Jacques  Carette", "James H. Davenport"], "year": 2010, "n_citations": 0}
{"id": 4900375, "s2_id": "91727a54098fe13d2de16115ba76d076e4a9484c", "title": "CLBlast: A Tuned OpenCL BLAS Library", "abstract": "This work introduces CLBlast, an open-source BLAS library providing optimized OpenCL routines to accelerate dense linear algebra for a wide variety of devices. It is targeted at machine learning and HPC applications and thus provides a fast matrix-multiplication routine (GEMM) to accelerate the core of many applications (e.g. deep learning, iterative solvers, astrophysics, computational fluid dynamics, quantum chemistry). CLBlast has five main advantages over other OpenCL BLAS libraries: 1) it is optimized for and tested on a large variety of OpenCL devices including less commonly used devices such as embedded and low-power GPUs, 2) it can be explicitly tuned for specific problem-sizes on specific hardware platforms, 3) it can perform operations in half-precision floating-point FP16 saving bandwidth, time and energy, 4) it has an optional CUDA back-end, 5) and it can combine multiple operations in a single batched routine, accelerating smaller problems significantly. This paper describes the library and demonstrates the advantages of CLBlast experimentally for different use-cases on a wide variety of OpenCL hardware.", "venue": "IWOCL", "authors": ["Cedric  Nugteren"], "year": 2018, "n_citations": 49}
{"id": 4908336, "s2_id": "185f515a51fd21c11c801b1a09c4f311cc86667a", "title": "Computational complexity of iterated maps on the interval", "abstract": "The correct computation of orbits of discrete dynamical systems on the interval is considered. Therefore, an arbitrary-precision floating-point approach based on automatic error analysis is chosen and a general algorithm is presented. The correctness of the algorithm is shown and the computational complexity is analyzed. There are two main results. First, the computational complexity measure considered here is related to the Lyapunov exponent of the dynamical system under consideration. Second, the presented algorithm is optimal with regard to that complexity measure.", "venue": "Math. Comput. Simul.", "authors": ["Christoph  Spandl"], "year": 2012, "n_citations": 4}
{"id": 4910284, "s2_id": "fabc456a0180f8f501ebbdec35045c66b554c6a3", "title": "Asynchronous Processing of Coq Documents: From the Kernel up to the User Interface", "abstract": "The work described in this paper improves the reactivity of the Coq system by completely redesigning the way it processes a formal document. By subdividing such work into independent tasks the system can give precedence to the ones of immediate interest for the user and postpones the others. On the user side, a modern interface based on the PIDE middleware aggregates and present in a consistent way the output of the prover. Finally postponed tasks are processed exploiting modern, parallel, hardware to offer better scalability.", "venue": "ITP", "authors": ["Bruno  Barras", "Carst  Tankink", "Enrico  Tassi"], "year": 2015, "n_citations": 18}
{"id": 4912531, "s2_id": "72bc3dfb0060558936347c84aef7d9becb32dd2b", "title": "Program Verification of Numerical Computation - Part 2", "abstract": "These notes present some extensions of a formal method introduced in an earlier paper. The formal method is designed as a tool for program verification of numerical computation and forms the basis of the software package VPC. Included in the extensions that are presented here are disjunctions and methods for detecting non-computable programs. A more comprehensive list of the construction rules as higher order constructs is also presented.", "venue": "ArXiv", "authors": ["Garry  Pantelis"], "year": 2014, "n_citations": 0}
{"id": 4917077, "s2_id": "0974e493bed26bb1de815f7e3590526ed652b38f", "title": "Local shape of generalized offsets to algebraic curves", "abstract": "Offsetting is an important operation in computer aided design, with applications also in other contexts like robot path planning or tolerance analysis. In this paper we study the local behavior of an algebraic curve under a variation of the usual offsetting construction, namely the generalized offsetting process (Sendra and Sendra, 2000a). More precisely, here we discuss when and how this geometric construction may cause local changes in the shape of an algebraic curve, and we compare our results with those obtained for the case of classical offsets (Alcazar and Sendra, 2007). For these purposes, we use well-known notions of Differential Geometry, and also the notion of local shape introduced in Alcazar and Sendra (2007). Our analysis shows important differences between the topological properties of classical and generalized offsets, both at regular and singular points.", "venue": "J. Symb. Comput.", "authors": ["Juan Gerardo Alc\u00e1zar"], "year": 2012, "n_citations": 1}
{"id": 4923797, "s2_id": "dc62935d1db08a638f40cfab5ed7b985aa094112", "title": "Large-scale paralleled sparse principal component analysis", "abstract": "Principal component analysis (PCA) is a statistical technique commonly used in multivariate data analysis. However, PCA can be difficult to interpret and explain since the principal components (PCs) are linear combinations of the original variables. Sparse PCA (SPCA) aims to balance statistical fidelity and interpretability by approximating sparse PCs whose projections capture the maximal variance of original data. In this paper we present an efficient and paralleled method of SPCA using graphics processing units (GPUs), which can process large blocks of data in parallel. Specifically, we construct parallel implementations of the four optimization formulations of the generalized power method of SPCA (GP-SPCA), one of the most efficient and effective SPCA approaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS) is up to eleven times faster than the corresponding CPU implementation (using CBLAS), and up to 107 times faster than a MatLab implementation. Extensive comparative experiments in several real-world datasets confirm that SPCA offers a practical advantage.", "venue": "Multimedia Tools and Applications", "authors": ["Weifeng  Liu", "H.  Zhang", "Dapeng  Tao", "Y.  Wang", "K.  Lu"], "year": 2014, "n_citations": 63}
{"id": 4925875, "s2_id": "581df1a91aea7d78d514b82c8adb1de76d6425f4", "title": "A Julia implementation of Algorithm NCL for constrained optimization", "abstract": "Algorithm NCL is designed for general smooth optimization problems where first and second derivatives are available, including problems whose constraints may not be linearly independent at a solution (i.e., do not satisfy the LICQ). It is equivalent to the LANCELOT augmented Lagrangian method, reformulated as a short sequence of nonlinearly constrained subproblems that can be solved efficiently by IPOPT and KNITRO, with warm starts on each subproblem. We give numerical results from a Julia implementation of Algorithm NCL on tax policy models that do not satisfy the LICQ, and on nonlinear least-squares problems and general problems from the CUTEst test set.", "venue": "ArXiv", "authors": ["Ding  Ma", "Dominique  Orban", "Michael A. Saunders"], "year": 2021, "n_citations": 1}
{"id": 4927413, "s2_id": "1aef3ea69edc59bd864a4718c9ade6affdff3b75", "title": "Chemora: A PDE Solving Framework for Modern HPC Architectures", "abstract": "Modern HPC architectures consist of heterogeneous multi-core, many-node systems with deep memory hierarchies. Modern applications employ ever more advanced discretisation methods to study multi-physics problems. Developing such applications that explore cutting-edge physics on cutting-edge HPC systems has become a complex task that requires significant HPC knowledge and experience. Unfortunately, this combined knowledge is currently out of reach for all but a few groups of application developers. \nChemora is a framework for solving systems of Partial Differential Equations (PDEs) that targets modern HPC architectures. Chemora is based on Cactus, which sees prominent usage in the computational relativistic astrophysics community. In Chemora, PDEs are expressed either in a high-level \\LaTeX-like language or in Mathematica. Discretisation stencils are defined separately from equations, and can include Finite Differences, Discontinuous Galerkin Finite Elements (DGFE), Adaptive Mesh Refinement (AMR), and multi-block systems. \nWe use Chemora in the Einstein Toolkit to implement the Einstein Equations on CPUs and on accelerators, and study astrophysical systems such as black hole binaries, neutron stars, and core-collapse supernovae.", "venue": "ArXiv", "authors": ["Erik  Schnetter", "Marek  Blazewicz", "Steven R. Brandt", "David M. Koppelman", "Frank  L\u00f6ffler"], "year": 2014, "n_citations": 1}
{"id": 4933902, "s2_id": "ff560bf04314f127f65e47ef2f53c851aefbf879", "title": "Changing computing paradigms towards power efficiency", "abstract": "Power awareness is fast becoming immensely important in computing, ranging from the traditional high-performance computing applications to the new generation of data centric workloads. In this work, we describe our efforts towards a power-efficient computing paradigm that combines low- and high-precision arithmetic. We showcase our ideas for the widely used kernel of solving systems of linear equations that finds numerous applications in scientific and engineering disciplines as well as in large-scale data analytics, statistics and machine learning. Towards this goal, we developed tools for the seamless power profiling of applications at a fine-grain level. In addition, we verify here previous work on post-FLOPS/W metrics and show that these can shed much more light in the power/energy profile of important applications.", "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences", "authors": ["Pavel  Klav\u00edk", "A. Cristiano I. Malossi", "Constantine  Bekas", "Alessandro  Curioni"], "year": 2014, "n_citations": 18}
{"id": 4949213, "s2_id": "c9460524c970c61c8e3920dd4ad80bbb46b45f64", "title": "Scaling betweenness centrality using communication-efficient sparse matrix multiplication", "abstract": "Betweenness centrality (BC) is a crucial graph problem that measures the significance of a vertex by the number of shortest paths leading through it. We propose Maximal Frontier Betweenness Centrality (MFBC): a succinct BC algorithm based on novel sparse matrix multiplication routines that performs a factor of p1/3 less communication on p processors than the best known alternatives, for graphs with n vertices and average degree k = n/p2/3. We formulate, implement, and prove the correctness of MFBC for weighted graphs by leveraging monoids instead of semirings, which enables a surprisingly succinct formulation. MFBC scales well for both extremely sparse and relatively dense graphs. It automatically searches a space of distributed data decompositions and sparse matrix multiplication algorithms for the most advantageous configuration. The MFBC implementation outperforms the well-known CombBLAS library by up to 8x and shows more robust performance. Our design methodology is readily extensible to other graph problems.", "venue": "SC", "authors": ["Edgar  Solomonik", "Maciej  Besta", "Flavio  Vella", "Torsten  Hoefler"], "year": 2017, "n_citations": 59}
{"id": 4952543, "s2_id": "87f8432e350d05cce221381d4326693c735679b2", "title": "Particle-based simulations of reaction-diffusion processes with Aboria", "abstract": "Mathematical models of transport and reactions in biological systems have been traditionally written in terms of partial differential equations (PDEs) that describe the time evolution of population-level variables. In recent years, the use of stochastic particle-based models, which keep track of the evolution of each organism in the system, has become widespread. These models provide a lot more detail than the population-based PDE models, for example by explicitly modelling particle-particle interactions, but bring with them many computational challenges. In this paper we overview Aboria, a powerful and flexible C++ library for the implementation of numerical methods for particle-based models. We demonstrate the use of Aboria with a commonly used model in mathematical biology, namely cell chemotaxis. Cells interact with each other and diffuse, biased by extracellular chemicals, that can be altered by the cells themselves. We use a hybrid approach where particle-based models of cells are coupled with a PDE for the concentration of the extracellular chemical.", "venue": "ArXiv", "authors": ["Maria  Bruna", "Philip K. Maini", "Martin  Robinson"], "year": 2018, "n_citations": 0}
{"id": 4957580, "s2_id": "5991381e2a746709a4c4bb46f43f780953754ec4", "title": "GraphCombEx: a software tool for exploration of combinatorial optimisation properties of large graphs", "abstract": "We present a prototype of a software tool for exploration of multiple combinatorial optimisation problems in large real-world and synthetic complex networks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial Explorer), provides a unified framework for scalable computation of high-quality suboptimal solutions and bounds for a number of widely studied combinatorial optimisation problems in large graphs. The problems currently supported include: maximum clique, graph colouring, maximum independent set, minimum vertex clique covering, minimum dominating set, as well as the longest simple cycle problem. Suboptimal solutions and intervals for optimal objective values are estimated using scalable heuristics. GraphCombEx has previously or currently been tested in scenarios of exploring synthetic graph models, as well as real-world networks ranging from social network samples, biological networks, to very large networks from the SNAP network data repository. The tool has already been successfully used to support a number of recent studies and is particularly beneficial in exploring the combinatorial properties of previously unseen network data, before applying more sophisticated custom optimisation algorithms.", "venue": "Soft Comput.", "authors": ["David  Chalupa", "Kenneth A. Hawick"], "year": 2019, "n_citations": 0}
{"id": 4978028, "s2_id": "dc0ea0bfbe41fc6e22997b48ed13f31b3f89a99d", "title": "The RAppArmor Package: Enforcing Security Policies in R Using Dynamic Sandboxing on Linux", "abstract": "The increasing availability of cloud computing and scientific super computers brings great potential for making R accessible through public or shared resources. This allows us to efficiently run code requiring lots of cycles and memory, or embed R functionality into, e.g., systems and web services. However some important security concerns need to be addressed before this can be put in production. The prime use case in the design of R has always been a single statistician running R on the local machine through the interactive console. Therefore the execution environment of R is entirely unrestricted, which could result in malicious behavior or excessive use of hardware resources in a shared environment. Properly securing an R process turns out to be a complex problem. We describe various approaches and illustrate potential issues using some of our personal experiences in hosting public web services. Finally we introduce the RAppArmor package: a Linux based reference implementation for dynamic sandboxing in R on the level of the operating system.", "venue": "ArXiv", "authors": ["Jeroen  Ooms"], "year": 2013, "n_citations": 4}
{"id": 4979353, "s2_id": "626b1a48e0ee7f5714693c5dea3fe9f5eab07cd7", "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis", "abstract": "The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data.", "venue": "J. Mach. Learn. Res.", "authors": ["Eemeli  Lepp\u00e4aho", "Muhammad  Ammad-ud-din", "Samuel  Kaski"], "year": 2017, "n_citations": 12}
{"id": 4981176, "s2_id": "bf776b2584f60ccae1bb87cd5f9731eb1606a99e", "title": "PSelInv - A Distributed Memory Parallel Algorithm for Selected Inversion: the non-symmetric Case", "abstract": "Abstract This paper generalizes the parallel selected inversion algorithm called PSelInv to sparse non-symmetric matrices. We assume a general sparse matrix A has been decomposed as P A Q = L U on a distributed memory parallel machine, where L, U are lower and upper triangular matrices, and P, Q are permutation matrices, respectively. The PSelInv method computes selected elements of A \u2212 1 . The selection is confined by the sparsity pattern of the matrix AT. Our algorithm does not assume any symmetry properties of A, and our parallel implementation is memory efficient, in the sense that the computed elements of A \u2212 T overwrites the sparse matrix L + U in situ. PSelInv involves a large number of collective data communication activities within different processor groups of various sizes. In order to minimize idle time and improve load balancing, tree-based asynchronous communication is used to coordinate all such collective communication. Numerical results demonstrate that PSelInv can scale efficiently to 6,400 cores for a variety of matrices.", "venue": "Parallel Comput.", "authors": ["Mathias  Jacquelin", "Lin  Lin", "Chao  Yang"], "year": 2018, "n_citations": 11}
{"id": 4985215, "s2_id": "4a29d4c34fd22f0b4b1c6d688f501e956ebefaa6", "title": "Pipelined Iterative Solvers with Kernel Fusion for Graphics Processing Units", "abstract": "We revisit the implementation of iterative solvers on discrete graphics processing units and demonstrate the benefit of implementations using extensive kernel fusion for pipelined formulations over conventional implementations of classical formulations. The proposed implementations with both CUDA and OpenCL are freely available in ViennaCL and are shown to be competitive with or even superior to other solver packages for graphics processing units. The highest-performance gains are obtained for small to medium-sized systems, while our implementations are on par with vendor-tuned implementations for very large systems. Our results are especially beneficial for transient problems, where many small to medium-sized systems instead of a single big system need to be solved.", "venue": "ACM Trans. Math. Softw.", "authors": ["Karl  Rupp", "Josef  Weinbub", "Ansgar  J\u00fcngel", "Tibor  Grasser"], "year": 2016, "n_citations": 17}
{"id": 4989211, "s2_id": "3520f6e0d3a0341b606793ef7c36a9c7e6c2997b", "title": "Clustering Complex Zeros of Triangular Systems of Polynomials", "abstract": "This paper gives the first algorithm for finding a set of natural $$\\epsilon $$ \u03f5 -clusters of complex zeros of a regular triangular system of polynomials within a given polybox in $${{\\mathbb {C}}}^n$$ C n , for any given $$\\epsilon >0$$ \u03f5 > 0 . Our algorithm is based on a recent near-optimal algorithm of Becker et al. (Proceedings of the ACM on international symposium on symbolic and algebraic computation, 2016) for clustering the complex roots of a univariate polynomial where the coefficients are represented by number oracles. Our algorithm is based on recursive subdivision. It is local, numeric, certified and handles solutions with multiplicity. Our implementation is compared to with well-known homotopy solvers on various triangular systems. Our solver always gives correct answers, is often faster than the homotopy solvers that often give correct answers, and sometimes faster than the ones that give sometimes correct results.", "venue": "Math. Comput. Sci.", "authors": ["R'emi  Imbach", "Marc  Pouget", "Chee  Yap"], "year": 2021, "n_citations": 0}
{"id": 4996657, "s2_id": "47fdb8ade52ab086c20cc7d4989821eaf0c707b2", "title": "A Scala prototype to generate multigrid solver implementations for different problems and target multi-core platforms", "abstract": "Many problems in computational science and engineering involve partial differential equations and thus require the numerical solution of large, sparse (non)linear systems of equations. Multigrid is known to be one of the most efficient methods for this purpose. However, the concrete multigrid algorithm and its implementation depend highly on the underlying problem and hardware. Therefore, changes in the code or many different variants are necessary to cover all relevant cases. We report on a prototype implementation in Scala for a framework that enables abstract descriptions of PDEs, their discretisation, and their numerical solution via multigrid algorithms. From these, data structures and implementations of multigrid components required to solve elliptic PDEs on structured grids can be generated automatically. Two different test problems illustrate the potential of our approach for both CPU and GPU target platforms.", "venue": "Int. J. Comput. Sci. Eng.", "authors": ["Harald  K\u00f6stler", "Christian  Schmitt", "Sebastian  Kuckuk", "Stefan  Kronawitter", "Frank  Hannig", "J\u00fcrgen  Teich", "Ulrich  R\u00fcde", "Christian  Lengauer"], "year": 2017, "n_citations": 4}
{"id": 4997468, "s2_id": "73c7df7d986e7de65a614032aa43babb6e5d5133", "title": "Solving Wave Equations on Unstructured Geometries", "abstract": "Every wave solver serving the computational study of waves meets a trade-off of two figures of merit\u2014its computational speed and its accuracy. The use of Discontinuous Galerkin (DG) methods on graphical processing units (GPUs) significantly lowers the cost of obtaining accurate solutions. DG methods for the numerical solution of partial differential equations have enjoyed considerable success because they are both flexible and robust. They allow arbitrary unstructured geometries and easy control of accuracy without compromising simulation stability. The resulting locality in memory access is one of the factors that enables DG to run on off-the-shelf, massively parallel graphics processors (GPUs). In addition, DG's high-order nature lets it require fewer data points per represented wavelength and hence fewer memory accesses, in exchange for higher arithmetic intensity. Both of these factors work significantly in favor of a GPU implementation of DG. Discontinuous Galerkin methods are most often used to solve hyperbolic systems of conservation laws in the time domain. Parabolic and elliptic equations can also be solved using DG methods.", "venue": "ArXiv", "authors": ["Andreas  Kl\u00f6ckner", "Timothy C. Warburton", "Jan S. Hesthaven"], "year": 2013, "n_citations": 9}
{"id": 5005487, "s2_id": "f93a20fc2d1f2ebc5368e158aec6d7623576d1a1", "title": "SymPKF: a symbolic and computational toolbox for the design of parametric Kalman filter dynamics", "abstract": "Abstract. Recent research in data assimilation has led to the introduction of the parametric\nKalman filter (PKF): an implementation of the Kalman filter, whereby the\ncovariance matrices are approximated by a parameterized covariance model.\nIn the PKF, the dynamics of the covariance during the forecast step rely on\nthe prediction of the covariance parameters. Hence, the design of the parameter\ndynamics is crucial, while it can be tedious to do this by hand.\nThis contribution introduces a Python package, SymPKF, able to compute PKF dynamics\nfor univariate statistics and when the covariance model is parameterized from the\nvariance and the local anisotropy of the correlations. The ability of SymPKF to\nproduce the PKF dynamics is shown on a nonlinear diffusive advection (the Burgers equation)\nover a 1D domain and the linear advection over a 2D domain. The computation of the PKF\ndynamics is performed at a symbolic level, but an automatic code generator is also\nintroduced to perform numerical simulations. A final multivariate example\nillustrates the potential of SymPKF to go beyond the univariate case.\n", "venue": "Geoscientific Model Development", "authors": ["Olivier  Pannekoucke", "Philippe  Arbogast"], "year": 2021, "n_citations": 1}
{"id": 5006663, "s2_id": "6a6eeb2e8c7e356975769da71693672d68cb0e06", "title": "Performance Modeling for Dense Linear Algebra", "abstract": "It is well known that the behavior of dense linear algebra algorithms is greatly influenced by factors like target architecture, underlying libraries and even problem size; because of this, the accurate prediction of their performance is a real challenge. In this article, we are not interested in creating accurate models for a given algorithm, but in correctly ranking a set of equivalent algorithms according to their performance. Aware of the hierarchical structure of dense linear algebra routines, we approach the problem by developing a framework for the automatic generation of statistical performance models for BLAS and LAPACK libraries. This allows us to obtain predictions through evaluating and combining such models. We demonstrate that our approach is successful in both single- and multi-core environments, not only in the ranking of algorithms but also in tuning their parameters.", "venue": "2012 SC Companion: High Performance Computing, Networking Storage and Analysis", "authors": ["Elmar  Peise", "Paolo  Bientinesi"], "year": 2012, "n_citations": 31}
{"id": 5019605, "s2_id": "515e0823a52ec78deb357f32f1a3426269a269dc", "title": "Streaming Generalized Canonical Polyadic Tensor Decompositions", "abstract": "In this paper, we develop a method which we call OnlineGCP for computing the Generalized Canonical Polyadic (GCP) tensor decomposition of streaming data. GCP differs from traditional canonical polyadic (CP) tensor decompositions as it allows for arbitrary objective functions which the CP model attempts to minimize. This approach can provide better fits and more interpretable models when the observed tensor data is strongly non-Gaussian. In the streaming case, tensor data is gradually observed over time and the algorithm must incrementally update a GCP factorization with limited access to prior data. In this work, we extend the GCP formalism to the streaming context by deriving a GCP optimization problem to be solved as new tensor data is observed, formulate a tunable history term to balance reconstruction of recently observed data with data observed in the past, develop a scalable solution strategy based on segregated solves using stochastic gradient descent methods, describe a software implementation that provides performance and portability to contemporary CPU and GPU architectures and integrates with Matlab for enhanced useability, and demonstrate the utility and performance of the approach and software on several synthetic and real tensor data sets.", "venue": "ArXiv", "authors": ["Eric  Phipps", "Nick  Johnson", "Tamara G. Kolda"], "year": 2021, "n_citations": 0}
{"id": 5022218, "s2_id": "bab5de5764111c429aab4de547e7ffb6764d71c4", "title": "An implementation of the relational k-means algorithm", "abstract": "A C# implementation of a generalized k-means variant called relational k-means is described here. Relational k-means is a generalization of the well-known k-means clustering method which works for non-Euclidean scenarios as well. The input is an arbitrary distance matrix, as opposed to the traditional k-means method, where the clustered objects need to be identified with vectors.", "venue": "ArXiv", "authors": ["Bal\u00e1zs  Szalkai"], "year": 2013, "n_citations": 10}
{"id": 5022497, "s2_id": "5d686e81555dc3f1939df55145c76ac58907850d", "title": "Various Ways to Quantify BDMPs", "abstract": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to the size of the system it models, whereas the size of the CTMC specified by this BDMP suffers from exponential growth. Thus quantifying large BDMPs can be a challenging task. The most general method to quantify them is Monte Carlo simulation, but this may be intractable for highly reliable systems. On the other hand, some subcategories of BDMPs can be processed with much more efficient methods. For example, BDMPs without repairs can be translated into dynamic fault trees, a formalism accepted as an input of the STORM model checker, that performs numerical calculations on sparse matrices, or they can be processed with the tool FIGSEQ that explores paths going to a failure state and calculates their probabilities. BDMPs with repairs can be quantified by FIGSEQ (BDMPs capturing quickly and completely repairable behaviors are solved by a different algorithm), and by the I&AB (Initiator and All Barriers) method, recently published and implemented in a prototype version of RISKSPECTRUM PSA. This tool, based exclusively on Boolean representations looks for and quantifies minimal cut sets of the system, i.e., minimal combinations of component failures that induce the loss of the system. This allows a quick quantification of large models with repairable components, standby redundancies and some other types of dependencies between omponents. All these quantification methods have been tried on a benchmark whose definition was published at the MARS 2017 workshop: the model of emergency power supplies of a nuclear power plant. In this paper, after a recall of the theoretical principles of the various quantification methods, we compare their performances on that benchmark.", "venue": "MARS@ETAPS", "authors": ["Marc  Bouissou", "Shahid  Khan", "Joost-Pieter  Katoen", "Pavel  Krcal"], "year": 2020, "n_citations": 1}
{"id": 5028006, "s2_id": "efb9e1eb0d4e000150f9b34153d2e2dd85e94037", "title": "DiffSharp: Automatic Differentiation Library", "abstract": "In this paper we introduce DiffSharp, an automatic differentiation (AD) library designed with machine learning in mind. AD is a family of techniques that evaluate derivatives at machine precision with only a small constant factor of overhead, by systematically applying the chain rule of calculus at the elementary operator level. DiffSharp aims to make an extensive array of AD techniques available, in convenient form, to the machine learning community. These including arbitrary nesting of forward/reverse AD operations, AD with linear algebra primitives, and a functional API that emphasizes the use of higher-order functions and composition. The library exposes this functionality through an API that provides gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products. Bearing the performance requirements of the latest machine learning techniques in mind, the underlying computations are run through a high-performance BLAS/LAPACK backend, using OpenBLAS by default. GPU support is currently being implemented.", "venue": "ArXiv", "authors": ["Atilim Gunes Baydin", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "year": 2015, "n_citations": 8}
{"id": 5031870, "s2_id": "0e9383e2e127474c84614fe184a86af7c8540a49", "title": "The Stan Math Library: Reverse-Mode Automatic Differentiation in C++", "abstract": "As computational challenges in optimization and statistical inference grow ever harder, algorithms that utilize derivatives are becoming increasingly more important. The implementation of the derivatives that make these algorithms so powerful, however, is a substantial user burden and the practicality of these algorithms depends critically on tools like automatic differentiation that remove the implementation burden entirely. The Stan Math Library is a C++, reverse-mode automatic differentiation library designed to be usable, extensive and extensible, efficient, scalable, stable, portable, and redistributable in order to facilitate the construction and utilization of such algorithms. \nUsability is achieved through a simple direct interface and a cleanly abstracted functional interface. The extensive built-in library includes functions for matrix operations, linear algebra, differential equation solving, and most common probability functions. Extensibility derives from a straightforward object-oriented framework for expressions, allowing users to easily create custom functions. Efficiency is achieved through a combination of custom memory management, subexpression caching, traits-based metaprogramming, and expression templates. Partial derivatives for compound functions are evaluated lazily for improved scalability. Stability is achieved by taking care with arithmetic precision in algebraic expressions and providing stable, compound functions where possible. For portability, the library is standards-compliant C++ (03) and has been tested for all major compilers for Windows, Mac OS X, and Linux.", "venue": "ArXiv", "authors": ["Bob  Carpenter", "Matthew D. Hoffman", "Marcus  Brubaker", "Daniel  Lee", "Peter  Li", "Michael  Betancourt"], "year": 2015, "n_citations": 87}
{"id": 5034559, "s2_id": "53c7360ea6abba57886b5bb06a6976195a89aa97", "title": "Stochastic Formal Methods: An Application to Accuracy of Numeric Software", "abstract": "This paper provides a bound on the number of numeric operations (fixed or floating point) that can safely be performed before accuracy is lost. This work has important implications for control systems with safety-critical software, as these systems are now running fast enough and long enough for their errors to impact on their functionality. Furthermore, worst-case analysis would blindly advise the replacement of existing systems that have been successfully running for years. We present here a set of formal theorems validated by the PVS proof assistant. These theorems allow code analyzing tools to produce formal certificates of accurate behavior. For example, FAA regulations for aircraft require that the probability of an error be below 10-9  for a 10 hour flight", "venue": "2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07)", "authors": ["Marc  Daumas", "David R. Lester"], "year": 2007, "n_citations": 9}
{"id": 5038768, "s2_id": "93755690b4a01f25eeaf9e6220173922658dea94", "title": "Resilience in Numerical Methods: A Position on Fault Models and Methodologies", "abstract": "Future extreme-scale computer systems may expose silent data corruption (SDC) to applications, in order to save energy or increase performance. However, resilience research struggles to come up with useful abstract programming models for reasoning about SDC. Existing work randomly flips bits in running applications, but this only shows average-case behavior for a low-level, artificial hardware model. Algorithm developers need to understand worst-case behavior with the higher-level data types they actually use, in order to make their algorithms more resilient. Also, we know so little about how SDC may manifest in future hardware, that it seems premature to draw conclusions about the average case. We argue instead that numerical algorithms can benefit from a numerical unreliability fault model, where faults manifest as unbounded perturbations to floating-point data. Algorithms can use inexpensive \"sanity\" checks that bound or exclude error in the results of computations. Given a selective reliability programming model that requires reliability only when and where needed, such checks can make algorithms reliable despite unbounded faults. Sanity checks, and in general a healthy skepticism about the correctness of subroutines, are wise even if hardware is perfectly reliable.", "venue": "ArXiv", "authors": ["James  Elliott", "Mark  Hoemmen", "Frank  Mueller"], "year": 2014, "n_citations": 13}
{"id": 5044127, "s2_id": "947bff94d02ef74bca20bd7064009fbb7c2ecf77", "title": "Ginkgo - A Math Library designed for Platform Portability", "abstract": "The first associations to software sustainability might be the existence of a continuous integration (CI) framework; the existence of a testing framework composed of unit tests, integration tests, and end-to-end tests; and also the existence of software documentation. However, when asking what is a common deathblow for a scientific software product, it is often the lack of platform and performance portability. Against this background, we designed the Ginkgo library with the primary focus on platform portability and the ability to not only port to new hardware architectures, but also achieve good performance. In this paper we present the Ginkgo library design, radically separating algorithms from hardware-specific kernels forming the distinct hardware executors, and report our experience when adding execution backends for NVIDIA, AMD, and Intel GPUs. We also comment on the different levels of performance portability, and the performance we achieved on the distinct hardware backends.", "venue": "ArXiv", "authors": ["Terry  Cojean", "Yu-Hsiang Mike Tsai", "Hartwig  Anzt"], "year": 2020, "n_citations": 1}
{"id": 5049186, "s2_id": "6d9c9b7b78a418403656698698564249e8e93c9d", "title": "ViDaExpert: user-friendly tool for nonlinear visualization and analysis of multidimensional vectorial data", "abstract": "ViDaExpert is a tool for visualization and analysis of multidimensional vectorial data. ViDaExpert is able to work with data tables of \"object-feature\" type that might contain numerical feature values as well as textual labels for rows (objects) and columns (features). ViDaExpert implements several statistical methods such as standard and weighted Principal Component Analysis (PCA) and the method of elastic maps (non-linear version of PCA), Linear Discriminant Analysis (LDA), multilinear regression, K-Means clustering, a variant of decision tree construction algorithm. Equipped with several user-friendly dialogs for configuring data point representations (size, shape, color) and fast 3D viewer, ViDaExpert is a handy tool allowing to construct an interactive 3D-scene representing a table of data in multidimensional space and perform its quick and insightfull statistical analysis, from basic to advanced methods.", "venue": "ArXiv", "authors": ["Alexander N. Gorban", "Alexander  Pitenko", "Andrei Yu. Zinovyev"], "year": 2014, "n_citations": 18}
{"id": 5073230, "s2_id": "1be95c75e09f293797831f1d6140f32e1b49e569", "title": "Boosting Memory Access Locality of the Spectral Element Method with Hilbert Space-Filling Curves", "abstract": "We propose an algorithm based on Hilbert spacefilling curves to reorder mesh elements in memory for use with the Spectral Element Method, aiming to attain fewer cache misses, better locality of data reference and faster execution. We present a technique to numerically simulate acoustic wave propagation in 2D domains using the Spectral Element Method, and discuss computational performance aspects of this procedure. We reorder mesh-related data via Hilbert curves to achieve sizable reductions in execution time under several mesh configurations in shared-memory systems. Our experiments show that the Hilbert curve approach works well with meshes of several granularities and also with small and large variations in element sizes, achieving reductions between 9% and 25% in execution time when compared to three other ordering schemes.", "venue": "Computers & Geosciences", "authors": ["Roger R. F. Ara\u00fajo", "Lutz  Gross", "Samuel Xavier de Souza"], "year": 2021, "n_citations": 1}
{"id": 5078604, "s2_id": "5e665c654eef78a15cda7a01c66175e268cdf417", "title": "Computable Hilbert Schemes", "abstract": "In this PhD thesis we propose an algorithmic approach to the study of the Hilbert scheme. Developing algorithmic methods, we also obtain general results about Hilbert schemes. In Chapter 1 we discuss the equations defining the Hilbert scheme as subscheme of a suitable Grassmannian and in Chapter 5 we determine a new set of equations of degree lower than the degree of equations known so far. In Chapter 2 we study the most important objects used to project algorithmic techniques, namely Borel-fixed ideals. We determine an algorithm computing all the saturated Borel-fixed ideals with Hilbert polynomial assigned and we investigate their combinatorial properties. In Chapter 3 we show a new type of flat deformations of Borel-fixed ideals which lead us to give a new proof of the connectedness of the Hilbert scheme. In Chapter 4 we construct families of ideals that generalize the notion of family of ideals sharing the same initial ideal with respect to a fixed term ordering. Some of these families correspond to open subsets of the Hilbert scheme and can be used to a local study of the Hilbert scheme. In Chapter 6 we deal with the problem of the connectedness of the Hilbert scheme of locally Cohen-Macaulay curves in the projective 3-space. We show that one of the Hilbert scheme considered a \"good\" candidate to be non-connected, is instead connected. Moreover there are three appendices that present and explain how to use the implementations of the algorithms proposed.", "venue": "ArXiv", "authors": ["Paolo  Lella"], "year": 2012, "n_citations": 1}
{"id": 5080405, "s2_id": "b071b4970cacf0a3e37074d72da4e3d269ff731c", "title": "Enhancements to the DIDO Optimal Control Toolbox", "abstract": "In 2020, DIDO$^\u00a9$ turned 20! The software package emerged in 2001 as a basic, user-friendly MATLAB$^\\circledR$ teaching-tool to illustrate the various nuances of Pontryagin's Principle but quickly rose to prominence in 2007 after NASA announced it had executed a globally optimal maneuver using DIDO. Since then, the toolbox has grown in applications well beyond its aerospace roots: from solving problems in quantum control to ushering rapid, nonlinear sensitivity-analysis in designing high-performance automobiles. Most recently, it has been used to solve continuous-time traveling-salesman problems. Over the last two decades, DIDO's algorithms have evolved from their simple use of generic nonlinear programming solvers to a multifaceted engagement of fast spectral Hamiltonian programming techniques. A description of the internal enhancements to DIDO that define its mathematics and algorithms are described in this paper. A challenge example problem from robotics is included to showcase how the latest version of DIDO is capable of escaping the trappings of a ``local minimum'' that ensnare many other trajectory optimization methods.", "venue": "ArXiv", "authors": ["I. M. Ross"], "year": 2020, "n_citations": 2}
{"id": 5080822, "s2_id": "c0710434956fd83dc51a1edeea87419a614c38e8", "title": "ArborX: A Performance Portable Geometric Search Library", "abstract": "Searching for geometric objects that are close in space is a fundamental component of many applications. The performance of search algorithms comes to the forefront as the size of a problem increases both in terms of total object count as well as in the total number of search queries performed. Scientific applications requiring modern leadership-class supercomputers also pose an additional requirement of performance portability, i.e. being able to efficiently utilize a variety of hardware architectures. In this paper, we introduce a new open-source C++ search library, ArborX, which we have designed for modern supercomputing architectures. We examine scalable search algorithms with a focus on performance, including a highly efficient parallel bounding volume hierarchy implementation, and propose a flexible interface making it easy to integrate with existing applications. We demonstrate the performance portability of ArborX on multi-core CPUs and GPUs, and compare it to the state-of-the-art libraries such as Boost.Geometry.Index and nanoflann.", "venue": "ACM Trans. Math. Softw.", "authors": ["Damien  Lebrun-Grandi\u00e9", "Andrey  Prokopenko", "Bruno  Turcksin", "Stuart R. Slattery"], "year": 2021, "n_citations": 2}
{"id": 5082380, "s2_id": "d4674caadfa55da8ed6c6315081a47fddc91ad71", "title": "Tiled Algorithms for Matrix Computations on Multicore Architectures", "abstract": "The current computer architecture has moved towards the multi/many-core structure. However, the algorithms in the current sequential dense numerical linear algebra libraries (e.g. LAPACK) do not parallelize well on multi/many-core architectures. A new family of algorithms, the tile algorithms, has recently been introduced to circumvent this problem. Previous research has shown that it is possible to write efficient and scalable tile algorithms for performing a Cholesky factorization, a (pseudo) LU factorization, and a QR factorization. The goal of this thesis is to study tiled algorithms in a multi/many-core setting and to provide new algorithms which exploit the current architecture to improve performance relative to current state-of-the-art libraries while maintaining the stability and robustness of these libraries.", "venue": "ArXiv", "authors": ["Henricus  Bouwmeester"], "year": 2013, "n_citations": 6}
{"id": 5088261, "s2_id": "56c7547a487cb2565e42d3a84ea7ebd877b1d955", "title": "The Kernel Quantum Probabilities (KQP) Library", "abstract": "In this document, we show how the different quantities necessary to compute kernel quantum probabilities can be computed. This document form the basis of the implementation of the Kernel Quantum Probability (KQP) open source project", "venue": "ArXiv", "authors": ["Benjamin  Piwowarski"], "year": 2012, "n_citations": 1}
{"id": 5088571, "s2_id": "86c7f07deec5a2a52ade8be133c2f1e1a6c6c947", "title": "Architecture-aware configuration and scheduling of matrix multiplication on asymmetric multicore processors", "abstract": "Asymmetric multicore processors have recently emerged as an appealing technology for severely energy-constrained environments, especially in mobile appliances where heterogeneity in applications is mainstream. In addition, given the growing interest for low-power high performance computing, this type of architectures is also being investigated as a means to improve the throughput-per-Watt of complex scientific applications on clusters of commodity systems-on-chip. In this paper, we design and embed several architecture-aware optimizations into a multi-threaded general matrix multiplication (gemm), a key operation of the BLAS, in order to obtain a high performance implementation for ARM big.LITTLE AMPs. Our solution is based on the reference implementation of gemm in the BLIS library, and integrates a cache-aware configuration as well as asymmetric-static and dynamic scheduling strategies that carefully tune and distribute the operation\u2019s micro-kernels among the big and LITTLE cores of the target processor. The experimental results on a Samsung Exynos 5422, a system-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the big.LITTLE model, expose that our cache-aware versions of gemm with asymmetric scheduling attain important gains in performance with respect to its architecture-oblivious counterparts while exploiting all the resources of the AMP to deliver considerable energy efficiency.", "venue": "Cluster Computing", "authors": ["Sandra  Catal\u00e1n", "Francisco D. Igual", "Rafael  Mayo", "Rafael  Rodr\u00edguez-S\u00e1nchez", "Enrique S. Quintana-Ort\u00ed"], "year": 2016, "n_citations": 14}
{"id": 5091963, "s2_id": "4edaf7e69f06dd21b1e9a2196fd4e580b7d97558", "title": "Efficient Expression Templates for Operator Overloading-based Automatic Differentiation", "abstract": "Expression templates are a well-known set of techniques for improving the efficiency of operator overloading-based forward mode automatic differentiation schemes in the C\\(++\\) programming language by translating the differentiation from individual operators to whole expressions. However standard expression template approaches result in a large amount of duplicate computation, particularly for large expression trees, degrading their performance. In this paper we describe several techniques for improving the efficiency of expression templates and their implementation in the automatic differentiation package Sacado (Phipps et al., Advances in automatic differentiation, Lecture notes in computational science and engineering, Springer, Berlin, 2008; Phipps and Gay, Sacado automatic differentiation package. http://trilinos.sandia.gov/packages/sacado/, 2011). We demonstrate their improved efficiency through test functions as well as their application to differentiation of a large-scale fluid dynamics simulation code.", "venue": "ArXiv", "authors": ["Eric T. Phipps", "Roger P. Pawlowski"], "year": 2012, "n_citations": 52}
{"id": 5096414, "s2_id": "066e817d512bc77109ecd2e2230bfed364ff2ef6", "title": "DOLFIN: Automated finite element computing", "abstract": "We describe here a library aimed at automating the solution of partial differential equations using the finite element method. By employing novel techniques for automated code generation, the library combines a high level of expressiveness with efficient computation. Finite element variational forms may be expressed in near mathematical notation, from which low-level code is automatically generated, compiled, and seamlessly integrated with efficient implementations of computational meshes and high-performance linear algebra. Easy-to-use object-oriented interfaces to the library are provided in the form of a C++ library and a Python module. This article discusses the mathematical abstractions and methods used in the design of the library and its implementation. A number of examples are presented to demonstrate the use of the library in application code.", "venue": "TOMS", "authors": ["Anders  Logg", "Garth N. Wells"], "year": 2010, "n_citations": 553}
{"id": 5099522, "s2_id": "89b01a1538cffb6e3b64e1e62412d6136d0bbab1", "title": "The deal.II library, version 8.5", "abstract": "Abstract This paper provides an overview of the new features of the finite element library deal.II version 8.5.", "venue": "J. Num. Math.", "authors": ["Daniel  Arndt", "Wolfgang  Bangerth", "Denis  Davydov", "Timo  Heister", "Luca  Heltai", "Martin  Kronbichler", "Matthias  Maier", "Jean-Paul  Pelteret", "Bruno  Turcksin", "David  Wells"], "year": 2017, "n_citations": 144}
{"id": 5099548, "s2_id": "90dd5911d0836b2f2ed0fe815512e499a34acd61", "title": "Discovery of Non-gaussian Linear Causal Models using ICA", "abstract": "In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000). Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis (ICA), and does not require any pre-specified time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data.", "venue": "UAI", "authors": ["Shohei  Shimizu", "Aapo  Hyv\u00e4rinen", "Yutaka  Kano", "Patrik O. Hoyer"], "year": 2005, "n_citations": 41}
{"id": 5101102, "s2_id": "cb3ee9880c252c23f2b473d4055f26df2a6f75a9", "title": "MEIGO: an open-source software suite based on metaheuristics for global optimization in systems biology and bioinformatics", "abstract": "BackgroundOptimization is the key to solving many problems in computational biology. Global optimization methods, which provide a robust methodology, and metaheuristics in particular have proven to be the most efficient methods for many applications. Despite their utility, there is a limited availability of metaheuristic tools.ResultsWe present MEIGO, an R and Matlab optimization toolbox (also available in Python via a wrapper of the R version), that implements metaheuristics capable of solving diverse problems arising in systems biology and bioinformatics. The toolbox includes the enhanced scatter search method (eSS) for continuous nonlinear programming (cNLP) and mixed-integer programming (MINLP) problems, and variable neighborhood search (VNS) for Integer Programming (IP) problems. Additionally, the R version includes BayesFit for parameter estimation by Bayesian inference. The eSS and VNS methods can be run on a single-thread or in parallel using a cooperative strategy. The code is supplied under GPLv3 and is available at http://www.iim.csic.es/~gingproc/meigo.html. Documentation and examples are included. The R package has been submitted to BioConductor. We evaluate MEIGO against optimization benchmarks, and illustrate its applicability to a series of case studies in bioinformatics and systems biology where it outperforms other state-of-the-art methods.ConclusionsMEIGO provides a free, open-source platform for optimization that can be applied to multiple domains of systems biology and bioinformatics. It includes efficient state of the art metaheuristics, and its open and modular structure allows the addition of further methods.", "venue": "BMC Bioinformatics", "authors": ["Jose A Egea", "David  Henriques", "Thomas  Cokelaer", "Alejandro F Villaverde", "Julio R Banga", "Julio  Saez-Rodriguez"], "year": 2013, "n_citations": 123}
{"id": 5106606, "s2_id": "331116a59568755f6fb693c091a2c9c54dda92ba", "title": "Accelerating jackknife resampling for the Canonical Polyadic Decomposition", "abstract": "The Canonical Polyadic (CP) tensor decomposition is frequently used as a model in applications in a variety of different fields. Using jackknife resampling to estimate parameter uncertainties is often desirable but results in an increase of the already high computational cost. Upon observation that the resampled tensors, though different, are nearly identical, we show that it is possible to extend the recently proposed Concurrent ALS (CALS) technique to a jackknife resampling scenario. This extension gives access to the computational efficiency advantage of CALS for the price of a modest increase (typically a few percent) in the number of floating point operations. Numerical experiments on both synthetic and real-world datasets demonstrate that the new workflow based on a CALS extension can be several times faster than a straightforward workflow where the jackknife submodels are processed individually.", "venue": "ArXiv", "authors": ["Christos  Psarras", "Lars  Karlsson", "Rasmus  Bro", "Paolo  Bientinesi"], "year": 2021, "n_citations": 0}
{"id": 5108401, "s2_id": "ef538fd12da7c03b9bd3de71ebc16727773ce0e1", "title": "Parallelizing multiple precision Taylor series method for integrating the Lorenz system", "abstract": "A hybrid MPI+OpenMP strategy for parallelizing multiple precision Taylor series method is proposed, realized and tested. To parallelize the algorithm we combine MPI and OpenMP parallel technologies together with GMP library (GNU miltiple precision libary) and the tiny MPIGMP library. The details of the parallelization are explained on the paradigmatic model of the Lorenz system. We succeed to obtain a correct reference solution in the rather long time interval - [0,7000]. The solution is verified by comparing the results for 2700-th order Taylor series method and precision of ~ 3374 decimal digits, and those with 2800-th order and precision of ~ 3510 decimal digits. With 192 CPU cores in Nestum cluster, Sofia, Bulgaria, the 2800-th order computation was ~ 148 hours with speedup ~ 103.", "venue": "ArXiv", "authors": ["I.  Hristov", "R.  Hristova", "S.  Dimova", "P.  Armyanov", "N.  Shegunov", "I.  Puzynin", "T.  Puzynina", "Z.  Sharipov", "Z.  Tukhliev"], "year": 2020, "n_citations": 1}
{"id": 5109018, "s2_id": "e134501a63a4a254ee5a84ba7a90d0956e64c4b6", "title": "A Scalable and Modular Software Architecture for Finite Elements on Hierarchical Hybrid Grids", "abstract": "In this article, a new generic higher-order finite-element framework for massively parallel simulations is presented. The modular software architecture is carefully designed to exploit the resources of modern and future supercomputers. Combining an unstructured topology with structured grid refinement facilitates high geometric adaptability and matrix-free multigrid implementations with excellent performance. Different abstraction levels and fully distributed data structures additionally ensure high flexibility, extensibility, and scalability. The software concepts support sophisticated load balancing and flexibly combining finite element spaces. Example scenarios with coupled systems of PDEs show the applicability of the concepts to performing geophysical simulations.", "venue": "From Parallel to Emergent Computing", "authors": ["Nils  Kohl", "Dominik  Th\u00f6nnes", "Daniel  Drzisga", "Dominik  Bartuschat", "Ulrich  R\u00fcde"], "year": 2019, "n_citations": 0}
{"id": 5115311, "s2_id": "6350f9e88a42db4335869577f86b8357a2123a6d", "title": "A note on solving nonlinear optimization problems in variable precision", "abstract": "This short note considers an efficient variant of the trust-region algorithm with dynamic accuracy proposed by Carter (SIAM J Sci Stat Comput 14(2):368\u2013388, 1993) and by Conn et al. (Trust-region methods. MPS-SIAM series on optimization, SIAM, Philadelphia, 2000) as a tool for very high-performance computing, an area where it is critical to allow multi-precision computations for keeping the energy dissipation under control. Numerical experiments are presented indicating that the use of the considered method can bring substantial savings in objective function\u2019s and gradient\u2019s evaluation \u201cenergy costs\u201d by efficiently exploiting multi-precision computations.", "venue": "Comput. Optim. Appl.", "authors": ["Serge  Gratton", "Philippe L. Toint"], "year": 2020, "n_citations": 6}
{"id": 5120218, "s2_id": "aa669a995e0e3d83fd8c87197eadc1111a004c7e", "title": "Recent software developments for special functions in the Santander-Amsterdam project", "abstract": "We give an overview of published algorithms by our group and of current activities and future plans. In particular, we give details on methods for computing special functions and discuss in detail two current lines of research. Firstly, we describe the recent developments for the computation of central and non-central ?-square cumulative distributions (also called Marcum Q-functions), and we present a new quadrature method for computing them. Secondly, we describe the fourth-order methods for computing zeros of special functions recently developed, and we provide an explicit example for the computation of complex zeros of Bessel functions. We end with an overview of published software by our group for computing special functions.", "venue": "Sci. Comput. Program.", "authors": ["Amparo  Gil", "Javier  Segura", "Nico M. Temme"], "year": 2014, "n_citations": 3}
{"id": 5126227, "s2_id": "52dfd06d5c98ab0ba48d5b45ef8497911e784218", "title": "Knowledge-based automatic generation of linear algebra algorithms and code", "abstract": "This dissertation focuses on the design and the implementation of domain-specific compilers for linear algebra matrix equations. The development of efficient libraries for such equations, which lie at the heart of most software for scientific computing, is a complex process that requires expertise in a variety of areas, including the application domain, algorithms, numerical analysis and high-performance computing. Moreover, the process involves the collaboration of several people for a considerable amount of time. With our compilers, we aim to relieve the developers from both designing algorithms and writing code, and to generate routines that match or even surpass the performance of those written by human experts.", "venue": "ArXiv", "authors": ["Diego  Fabregat-Traver"], "year": 2013, "n_citations": 0}
{"id": 5128534, "s2_id": "bb98fd5c2567b4ab462d98e47ef2ebe27a38c83e", "title": "Robust level-3 BLAS Inverse Iteration from the Hessenberg Matrix", "abstract": "Inverse iteration is known to be an effective method for computing eigenvectors corresponding to simple and well-separated eigenvalues. In the non-symmetric case, the solution of shifted Hessenberg systems is a central step. Existing inverse iteration solvers approach the solution of the shifted Hessenberg systems with either RQ or LU factorizations and, once factored, solve the corresponding systems. This approach has limited level-3 BLAS potential since distinct shifts have distinct factorizations. This paper rearranges the RQ approach such that data shared between distinct shifts is exposed. Thereby the backward substitution with the triangular R factor can be expressed mostly with matrix\u2013matrix multiplications (level-3 BLAS). The resulting algorithm computes eigenvectors in a tiled, overflow-free, and task-parallel fashion. The numerical experiments show that the new algorithm outperforms existing inverse iteration solvers for the computation of both real and complex eigenvectors.", "venue": "ArXiv", "authors": ["Angelika  Schwarz"], "year": 2021, "n_citations": 0}
{"id": 5142273, "s2_id": "56c716b8c51a3acf307bcc9f2c2cf445360d452a", "title": "ModelingToolkit: A Composable Graph Transformation System For Equation-Based Modeling", "abstract": "Getting good performance out of numerical equation solvers requires that the user has provided stable and efficient functions representing their model. However, users should not be trusted to write good code. In this manuscript we describe ModelingToolkit (MTK), a symbolic equationbased modeling system which allows for composable transformations to generate stable, efficient, and parallelized model implementations. MTK blurs the lines of traditional symbolic computing by acting directly on a user\u2019s numerical code. We show the ability to apply graph algorithms for automatically parallelizing and performing index reduction on code written for differential-algebraic equation (DAE) solvers, \u201cfixing\u201d the performance and stability of the model without requiring any changes to on the user\u2019s part. We demonstrate how composable model transformations can be combined with automated data-driven surrogate generation techniques, allowing machine learning methods to generate accelerated approximate models within an acausal modeling framework. These reduced models are shown to outperform the Dymola Modelica compiler on an HVAC model by 590x at 3% error. Together, this demonstrates MTK as a system for bringing the latest research in graph transformations directly to modeling applications.", "venue": "ArXiv", "authors": ["Yingbo  Ma", "Shashi  Gowda", "Ranjan  Anantharaman", "Chris  Laughman", "Viral  Shah", "Chris  Rackauckas"], "year": 2021, "n_citations": 13}
{"id": 5145945, "s2_id": "d6ea8efc962795a0864fb07b2ab965dda36f73a7", "title": "Swarm-NG: a CUDA Library for Parallel n-body Integrations with focus on Simulations of Planetary Systems", "abstract": "Abstract We present Swarm-NG, a C++ library for the efficient direct integration of many n-body systems using a Graphics Processing Unit (GPU), such as NVIDIA\u2019s Tesla T10 and M2070 GPUs. While previous studies have demonstrated the benefit of GPUs for n-body simulations with thousands to millions of bodies, Swarm-NG focuses on many few-body systems, e.g., thousands of systems with 3\u202615 bodies each, as is typical for the study of planetary systems. Swarm-NG parallelizes the simulation, including both the numerical integration of the equations of motion and the evaluation of forces using NVIDIA\u2019s \u201cCompute Unified Device Architecture\u201d (CUDA) on the GPU. Swarm-NG includes optimized implementations of 4th order time-symmetrized Hermite integration and mixed variable symplectic integration, as well as several sample codes for other algorithms to illustrate how non-CUDA-savvy users may themselves introduce customized integrators into the Swarm-NG framework. To optimize performance, we analyze the effect of GPU-specific parameters on performance under double precision. For an ensemble of 131072 planetary systems, each containing three bodies, the NVIDIA\u00a0Tesla M2070 GPU outperforms a 6-core Intel Xeon X5675 CPU by a factor of \u223c 2.75 . Thus, we conclude that modern GPUs offer an attractive alternative to a cluster of CPUs for the integration of an ensemble of many few-body systems. Applications of Swarm-NG include studying the late stages of planet formation, testing the stability of planetary systems and evaluating the goodness-of-fit between many planetary system models and observations of extrasolar planet host stars (e.g., radial velocity, astrometry, transit timing). While Swarm-NG focuses on the parallel integration of many planetary systems, the underlying integrators could be applied to a wide variety of problems that require repeatedly integrating a set of ordinary differential equations many times using different initial conditions and/or parameter values.", "venue": "ArXiv", "authors": ["Saleh  Dindar", "Eric B. Ford", "Mario  Juric", "Young In Yeo", "Jianwei  Gao", "Aaron C. Boley", "Benjamin  Nelson", "J\u00f6rg  Peters"], "year": 2012, "n_citations": 18}
{"id": 5161587, "s2_id": "32a476258b4403affd6ebbb5e8dfc5f0bc44210d", "title": "MLDev: Data Science Experiment Automation and Reproducibility Software", "abstract": "In this paper we explore the challenges of automating experiments in data science. We propose an extensible experiment model as a foundation for integration of different open source tools for running research experiments. We implement our approach in a prototype open source MLDev software package and evaluate it in a series of experiments yielding promising results. Comparison with other state-of-the-art tools signifies novelty of our approach.", "venue": "ArXiv", "authors": ["Anton  Khritankov", "Nikita  Pershin", "Nikita  Ukhov", "Artem  Ukhov"], "year": 2021, "n_citations": 1}
{"id": 5171157, "s2_id": "eada2be5e687161e87757b79722702a9e26d1341", "title": "Strategies for the vectorized Block Conjugate Gradients method", "abstract": "Block Krylov methods have recently gained a lot of attraction. Due to their increased arithmetic intensity they offer a promising way to improve performance on modern hardware. Recently Frommer et al. presented a block Krylov framework that combines the advantages of block Krylov methods and data parallel methods. We review this framework and apply it on the Block Conjugate Gradients method,to solve linear systems with multiple right hand sides. In this course we consider challenges that occur on modern hardware, like a limited memory bandwidth, the use of SIMD instructions and the communication overhead. We present a performance model to predict the efficiency of different Block CG variants and compare these with experimental numerical results.", "venue": "ENUMATH", "authors": ["Nils-Arne  Dreier", "Christian  Engwer"], "year": 2019, "n_citations": 0}
{"id": 5172111, "s2_id": "9ec5a0a3d53c488c2eeb89c9f3d7f1b4bb06ad6c", "title": "Topology optimization using the unsmooth variational topology optimization (UNVARTOP) method: an educational implementation in MATLAB", "abstract": "This paper presents an efficient and comprehensive MATLAB code to solve two-dimensional structural topology optimization problems, including minimum mean compliance, compliant mechanism synthesis, and multi-load compliance problems. The unsmooth variational topology optimization (UNVARTOP) method, developed by Oliver et al. (Comput Methods Appl Mech Eng 355:779\u2013819, 2019 ), is used in the topology optimization code, based on the finite element method (FEM), to compute the sensitivity and update the topology. The paper also includes instructions to improve the bisection algorithm , modify the computation of the Lagrangian multiplier by using an Augmented Lagrangian to impose the constraint, implement heat conduction problems, and extend the code to three-dimensional topology optimization problems. The code, intended for students and newcomers in topology optimization, is included as an appendix (Appendix\u00a0 A ) and it can be downloaded from https://github.com/DanielYago/UNVARTOP together with supplementary material.", "venue": "ArXiv", "authors": ["Daniel  Yago", "Juan  Cante", "Oriol  Lloberas-Valls", "Javier  Oliver"], "year": 2021, "n_citations": 4}
{"id": 5177932, "s2_id": "d8becdb07dcb380164092023acb4a1df6d5a9dcc", "title": "JBotSim: a tool for fast prototyping of distributed algorithms in dynamic networks", "abstract": "JBotSim is a java library that offers basic primitives for prototyping, running, and visualizing distributed algorithms in dynamic networks. With JBotSim, one can implement an idea in minutes and interact with it ({\\it e.g.}, add, move, or delete nodes) while it is running. JBotSim is well suited to prepare live demonstrations of your algorithms to colleagues or students; it can also be used to evaluate performance at the algorithmic level (number of messages, number of rounds, etc.). Unlike most tools, JBotSim is not an integrated environment. It is a lightweight library to be used in your program. In this paper, we present an overview of its distinctive features and architecture.", "venue": "SimuTools", "authors": ["Arnaud  Casteigts"], "year": 2015, "n_citations": 7}
{"id": 5186176, "s2_id": "71b26815104cd41a13b5a16c4abec70cf96a8af8", "title": "The Planetary System: Executable Science, Technology, Engineering and Math Papers", "abstract": "Executable scientific papers contain not just layouted text for reading. They contain, or link to, machine-comprehensible representations of the scientific findings or experiments they describe. Client-side players can thus enable readers to \"check, manipulate and explore the result space\" [1]. We have realized executable papers in the STEM domain with the Planetary system. Semantic annotations associate the papers with a content commons holding the background ontology, the annotations are exposed as Linked Data, and a frontend player application hooks modular interactive services into the semantic annotations.", "venue": "ESWC", "authors": ["Christoph  Lange", "Michael  Kohlhase", "Catalin  David", "Deyan  Ginev", "Andrea  Kohlhase", "Bogdan  Matican", "Stefan  Mirea", "Vyacheslav  Zholudev"], "year": 2011, "n_citations": 2}
{"id": 5198675, "s2_id": "15748c4c09ba5a8e31b143e96086a274619609ef", "title": "Robust Task-Parallel Solution of the Triangular Sylvester Equation", "abstract": "The Bartels-Stewart algorithm is a standard approach to solving the dense Sylvester equation. It reduces the problem to the solution of the triangular Sylvester equation. The triangular Sylvester equation is solved with a variant of backward substitution. Backward substitution is prone to overflow. Overflow can be avoided by dynamic scaling of the solution matrix. An algorithm which prevents overflow is said to be robust. The standard library LAPACK contains the robust scalar sequential solver dtrsyl. This paper derives a robust, level-3 BLAS-based task-parallel solver. By adding overflow protection, our robust solver closes the gap between problems solvable by LAPACK and problems solvable by existing non-robust task-parallel solvers. We demonstrate that our robust solver achieves a similar performance as non-robust solvers.", "venue": "PPAM", "authors": ["Angelika  Schwarz", "Carl Christian Kjelgaard Mikkelsen"], "year": 2019, "n_citations": 1}
{"id": 5198977, "s2_id": "887f0218ddaa3693a3e20af247cf25a53d1f26e7", "title": "Nzmath 1.0", "abstract": "This is an announcement of the first official release (version 1.0) of the system NZMATH for number theory by Python [18]. We review all functions in NZMATH 1.0, show its main properties added after the report [11] about NZMATH 0.5.0, and describe new features for stable development. The most important point of the release is that we can now treat number fields. The second major change is that new types of polynomial programs are provided. Elliptic curve primality proving and its related programs are also available, where we partly use a library outside NZMATH as an advantage of writing the system only by Python. A new feature is that NZMATH is registered on SourceForge [19] as an open source project in order to ensure continuous development of the project. This is a unique among existing systems for number theory.", "venue": "ICMS", "authors": ["Satoru  Tanaka", "Naoki  Ogura", "Ken  Nakamula", "Tetsushi  Matsui", "Shigenori  Uchiyama"], "year": 2010, "n_citations": 0}
{"id": 5202932, "s2_id": "e81b66410499215be0149beb58d7b65335656cf5", "title": "TensorTrace: an application to contract tensor networks", "abstract": "Tensor network methods are a conceptually elegant framework for encoding complicated datasets, where high-order tensors are approximated as networks of low-order tensors. In practice, however, the numeric implementation of tensor network algorithms is often a labor-intensive and error-prone task, even for experienced researchers in this area. \\emph{TensorTrace} is application designed to alleviate the burden of contracting tensor networks: it provides a graphic drawing interface specifically tailored for the construction of tensor network diagrams, from which the code for their optimal contraction can then be automatically generated (in the users choice of the MATLAB, Python or Julia languages). \\emph{TensorTrace} is freely available at \\url{this https URL} with versions for Windows, Mac and Ubuntu.", "venue": "ArXiv", "authors": ["Glen  Evenbly"], "year": 2019, "n_citations": 5}
{"id": 5218476, "s2_id": "396bd5b9ca656800f52ccb087ce7d6afcfd667e7", "title": "C Language Extensions for Hybrid CPU/GPU Programming with StarPU", "abstract": "Modern platforms used for high-performance computing (HPC) include machines with both general-purpose CPUs, and \"accelerators\", often in the form of graphical processing units (GPUs). StarPU is a C library to exploit such platforms. It provides users with ways to define \"tasks\" to be executed on CPUs or GPUs, along with the dependencies among them, and by automatically scheduling them over all the available processing units. In doing so, it also relieves programmers from the need to know the underlying architecture details: it adapts to the available CPUs and GPUs, and automatically transfers data between main memory and GPUs as needed. While StarPU's approach is successful at addressing run-time scheduling issues, being a C library makes for a poor and error-prone programming interface. This paper presents an effort started in 2011 to promote some of the concepts exported by the library as C language constructs, by means of an extension of the GCC compiler suite. Our main contribution is the design and implementation of language extensions that map to StarPU's task programming paradigm. We argue that the proposed extensions make it easier to get started with StarPU,eliminate errors that can occur when using the C library, and help diagnose possible mistakes. We conclude on future work.", "venue": "ArXiv", "authors": ["Ludovic  Court\u00e8s"], "year": 2013, "n_citations": 10}
{"id": 5219112, "s2_id": "3c1dc0582786956d2083abc32ab387cd5359e7c3", "title": "Programming Languages for Scientific Computing", "abstract": "The Hartree\u2013Fock (HF) method is one of the simplest theory to approximate the ground-state wave-function and the ground-state energy of a many-body fermionic quantum system.", "venue": "ArXiv", "authors": ["Matthew G. Knepley"], "year": 2012, "n_citations": 4}
{"id": 5220664, "s2_id": "68f1f70ed188ca4d791188a43e157d0bbc018331", "title": "OpenMP parallelization of multiple precision Taylor series method", "abstract": "OpenMP parallelization of multiple precision Taylor series method is proposed. A very good parallel performance scalability and parallel efficiency inside one computation node of a CPU-cluster is observed. We explain the details of the parallelization on the classical example of the Lorentz equations. The same approach can be applied straightforwardly to a large class of chaotic dynamical systems.", "venue": "ArXiv", "authors": ["Stefka  Dimova", "Ivan  Hristov", "Radoslava  Hristova", "Igor V. Puzynin", "Taisia P. Puzynina", "Zarif  Sharipov", "N.  Shegunov", "Zafar  Tukhliev"], "year": 2019, "n_citations": 0}
{"id": 5224566, "s2_id": "e6ffdba48af4e48a54cadd5fb27bc9244e04b027", "title": "SnapVX: A Network-Based Convex Optimization Solver", "abstract": "SnapVX is a high-performance solver for convex optimization problems defined on networks. For problems of this form, SnapVX provides a fast and scalable solution with guaranteed global convergence. It combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use Python interface with \u201cout-of-the-box\u201d functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, parallelize, and solve large optimization problems from a variety of different applications. Documentation, examples, and more can be found on the SnapVX website at http://snap.stanford.edu/snapvx.", "venue": "J. Mach. Learn. Res.", "authors": ["David  Hallac", "Christopher  Wong", "Steven  Diamond", "Abhijit  Sharang", "Rok  Sosic", "Stephen  Boyd", "Jure  Leskovec"], "year": 2017, "n_citations": 17}
{"id": 5225113, "s2_id": "d2c543478039854ad16a764a24f1d6a9e317861a", "title": "Parallel Robust Computation of Generalized Eigenvectors of Matrix Pencils", "abstract": "In this paper we consider the problem of computing generalized eigenvectors of a matrix pencil in real Schur form. In exact arithmetic, this problem can be solved using substitution. In practice, substitution is vulnerable to floating-point overflow. The robust solvers xTGEVC in LAPACK prevent overflow by dynamically scaling the eigenvectors. These subroutines are sequential scalar codes which compute the eigenvectors one by one. In this paper we discuss how to derive robust blocked algorithms. The new StarNEig library contains a robust task-parallel solver Zazamoukh which runs on top of StarPU. Our numerical experiments show that Zazamoukh achieves a super-linear speedup compared with DTGEVC for sufficiently large matrices.", "venue": "PPAM", "authors": ["Carl Christian Kjelgaard Mikkelsen", "Mirko  Myllykoski"], "year": 2019, "n_citations": 3}
{"id": 5231097, "s2_id": "6c8c3e3613ea8bf6aa71181f43922e5c434aeb47", "title": "Analysis of a Splitting Approach for the Parallel Solution of Linear Systems on GPU Cards", "abstract": "We discuss an approach for solving sparse or dense banded linear systems ${\\bf A} {\\bf x} = {\\bf b}$ on a Graphics Processing Unit (GPU) card. The matrix ${\\bf A} \\in {\\mathbb{R}}^{N \\times N}$ is possibly nonsymmetric and moderately large; i.e., $10000 \\leq N \\leq 500000$. The ${\\it split\\ and\\ parallelize}$ (${\\tt SaP}$) approach seeks to partition the matrix ${\\bf A}$ into diagonal sub-blocks ${\\bf A}_i$, $i=1,\\ldots,P$, which are independently factored in parallel. The solution may choose to consider or to ignore the matrices that couple the diagonal sub-blocks ${\\bf A}_i$. This approach, along with the Krylov subspace-based iterative method that it preconditions, are implemented in a solver called ${\\tt SaP::GPU}$, which is compared in terms of efficiency with three commonly used sparse direct solvers: ${\\tt PARDISO}$, ${\\tt SuperLU}$, and ${\\tt MUMPS}$. ${\\tt SaP::GPU}$, which runs entirely on the GPU except several stages involved in preliminary row-column permutations, is robust and compares well in terms of efficiency with the aforementioned direct solvers. In a comparison against Intel's ${\\tt MKL}$, ${\\tt SaP::GPU}$ also fares well when used to solve dense banded systems that are close to being diagonally dominant. ${\\tt SaP::GPU}$ is publicly available and distributed as open source under a permissive BSD3 license.", "venue": "SIAM J. Sci. Comput.", "authors": ["Ang  Li", "Radu  Serban", "Dan  Negrut"], "year": 2017, "n_citations": 6}
{"id": 5232609, "s2_id": "a1ac302a7a49afa7c37f735e483d067d564cd50c", "title": "Python bindings for libcloudph++", "abstract": "This technical note introduces the Python bindings for libcloudph++. The libcloudph++ is a C++ library of algorithms for representing atmospheric cloud microphysics in numerical models. The bindings expose the complete functionality of the library to the Python users. The bindings are implemented using the Boost.Python C++ library and use NumPy arrays. This note includes listings with Python scripts exemplifying the use of selected library components. An example solution for using the Python bindings to access libcloudph++ from Fortran is presented.", "venue": "ArXiv", "authors": ["Dorota  Jarecka", "Sylwester  Arabas", "Davide Del Vento"], "year": 2015, "n_citations": 1}
{"id": 5234900, "s2_id": "bb2d08029ede64c7244889bd45fbd45ed821e328", "title": "Efficient Mesh Management in Firedrake Using PETSc DMPlex", "abstract": "The use of composable abstractions allows the application of new and established algorithms to a wide range of problems while automatically inheriting the benefits of well-known performance optimisations. This work highlights the composition of the PETSc DMPlex domain topology abstraction with the Firedrake automated finite element system to create a PDE solving environment that combines expressiveness, flexibility and high performance. We describe how Firedrake utilises DMPlex to provide the indirection maps required for finite element assembly, while supporting various mesh input formats and runtime domain decomposition. In particular, we describe how DMPlex and its accompanying data structures allow the generic creation of user-defined discretisations, while utilising data layout optimisations that improve cache coherency and ensure overlapped communication during assembly computation.", "venue": "SIAM J. Sci. Comput.", "authors": ["Michael  Lange", "Lawrence  Mitchell", "Matthew G. Knepley", "Gerard  Gorman"], "year": 2016, "n_citations": 28}
{"id": 5234995, "s2_id": "74ec688ff7b60a8f5926b108f2609276a9a76e08", "title": "Automated derivation of the adjoint of high-level transient finite element programs", "abstract": "In this paper we demonstrate a new technique for deriving discrete adjoint and tangent linear models of finite element models. The technique is significantly more efficient and automatic than standard algorithmic differentiation techniques. The approach relies on a high-level symbolic representation of the forward problem. In contrast to developing a model directly in Fortran or C++, high-level systems allow the developer to express the variational problems to be solved in near-mathematical notation. As such, these systems have a key advantage: since the mathematical structure of the problem is preserved, they are more amenable to automated analysis and manipulation. The framework introduced here is implemented in a freely available software package named dolfin-adjoint, based on the FEniCS Project. Our approach to automated adjoint derivation relies on run-time annotation of the temporal structure of the model, and employs the FEniCS finite element form compiler to automatically generate the low-level code for the derived models. The approach requires only trivial changes to a large class of forward models, including complicated time-dependent nonlinear models. The adjoint model automatically employs optimal checkpointing schemes to mitigate storage requirements for nonlinear models, without any user management or intervention. Furthermore, both the tangent linear and adjoint models naturally work in parallel, without any need to differentiate through calls to MPI or to parse OpenMP directives. The generality, applicability and efficiency of the approach are demonstrated with examples from a wide range of scientific applications.", "venue": "ArXiv", "authors": ["Patrick E. Farrell", "David A. Ham", "Simon W. Funke", "Marie E. Rognes"], "year": 2012, "n_citations": 86}
{"id": 5243799, "s2_id": "c8ba99cb08399d51427e14a6f579bd6399ef6422", "title": "An Optimized Sparse Approximate Matrix Multiply for Matrices with Decay", "abstract": "We present an optimized single-precision implementation of the Sparse Approximate Matrix Multiply (\\SpAMM{}) [M. Challacombe and N. Bock, arXiv {\\bf 1011.3534} (2010)], a fast algorithm for matrix-matrix multiplication for matrices with decay that achieves an $\\mathcal{O} (n \\log n)$ computational complexity with respect to matrix dimension $n$. We find that the max norm of the error achieved with a \\SpAMM{} tolerance below $2 \\times 10^{-8}$ is lower than that of the single-precision {\\tt SGEMM} for dense quantum chemical matrices, while outperforming {\\tt SGEMM} with a cross-over already for small matrices ($n \\sim 1000$). Relative to naive implementations of \\SpAMM{} using Intel's Math Kernel Library ({\\tt MKL}) or AMD's Core Math Library ({\\tt ACML}), our optimized version is found to be significantly faster. Detailed performance comparisons are made for quantum chemical matrices with differently structured sub-blocks. Finally, we discuss the potential of improved hardware prefetch to yield 2--3x speedups.", "venue": "SIAM J. Sci. Comput.", "authors": ["Nicolas  Bock", "Matt  Challacombe"], "year": 2013, "n_citations": 28}
{"id": 5245700, "s2_id": "29a932080d93deefb9bcd97ffd025e7cf8bd2a0c", "title": "Adaptive Modular Exponentiation Methods v.s. Python's Power Function", "abstract": "In this paper we use Python to implement two efficient modular exponentiation methods: the adaptive m-ary method and the adaptive sliding-window method of window size k, where both m's are adaptively chosen based on the length of exponent. We also conduct the benchmark for both methods. Evaluation results show that compared to the industry-standard efficient implementations of modular power function in CPython and Pypy, our algorithms can reduce 1-5% computing time for exponents with more than 3072 bits.", "venue": "ArXiv", "authors": ["Shiyu  Ji", "Kun  Wan"], "year": 2017, "n_citations": 0}
{"id": 5254903, "s2_id": "b6185c64f23a02513f0ca86d2e7f73bd2fa1134e", "title": "The DUNE-ALUGrid Module", "abstract": "In this paper we present the new DUNE-ALUGrid module. This module contains a major overhaul of the sources from the ALUgrid library and the binding to the DUNE software framework. The main changes include user defined load balancing, parallel grid construction, and an redesign of the 2d grid which can now also be used for parallel computations. In addition many improvements have been introduced into the code to increase the parallel efficiency and to decrease the memory footprint. \nThe original ALUGrid library is widely used within the DUNE community due to its good parallel performance for problems requiring local adaptivity and dynamic load balancing. Therefore, this new model will benefit a number of DUNE users. In addition we have added features to increase the range of problems for which the grid manager can be used, for example, introducing a 3d tetrahedral grid using a parallel newest vertex bisection algorithm for conforming grid refinement. In this paper we will discuss the new features, extensions to the DUNE interface, and explain for various examples how the code is used in parallel environments.", "venue": "ArXiv", "authors": ["Andreas  Dedner", "Robert  Kl\u00f6fkorn", "Martin  Nolte"], "year": 2014, "n_citations": 48}
{"id": 5255844, "s2_id": "f4c2e5d6960ecdc7ce78659fed8444f551657efc", "title": "Firedrake: automating the finite element method by composing abstractions", "abstract": "Firedrake is a new tool for automating the numerical solution of partial differential equations. Firedrake adopts the domain-specific language for the finite element method of the FEniCS project, but with a pure Python runtime-only implementation centred on the composition of several existing and new abstractions for particular aspects of scientific computing. The result is a more complete separation of concerns which eases the incorporation of separate contributions from computer scientists, numerical analysts and application specialists. These contributions may add functionality, or improve performance. \nFiredrake benefits from automatically applying new optimisations. This includes factorising mixed function spaces, transforming and vectorising inner loops, and intrinsically supporting block matrix operations. Importantly, Firedrake presents a simple public API for escaping the UFL abstraction. This allows users to implement common operations that fall outside pure variational formulations, such as flux-limiters.", "venue": "ACM Trans. Math. Softw.", "authors": ["Florian  Rathgeber", "David A. Ham", "Lawrence  Mitchell", "Michael  Lange", "Fabio  Luporini", "Andrew T. T. McRae", "Gheorghe-Teodor  Bercea", "Graham R. Markall", "Paul H. J. Kelly"], "year": 2017, "n_citations": 325}
{"id": 5259685, "s2_id": "59317501d1e5b12cba91f4b8019c27f28d08a884", "title": "Javelin: A Scalable Implementation for Sparse Incomplete LU Factorization", "abstract": "In this work, we present a new scalable incomplete LU factorization framework called Javelin to be used as a preconditioner for solving sparse linear systems with iterative methods. Javelin allows for improved parallel factorization on shared-memory many-core systems by packaging the coefficient matrix into a format that allows for high performance sparse matrix-vector multiplication and sparse triangular solves with minimal overheads. The framework achieves these goals by using a collection of traditional permutations, point-to-point thread synchronizations, tasking, and segmented prefix scans in a conventional compressed sparse row format. Moreover, this framework stresses the importance of co-designing dependent tasks, such as sparse factorization and triangular solves, on highly-threaded architectures. Using these changes, traditional fill-in and drop tolerance methods can be used, while still being able to have observed speedups of up to ~ 42\u00d7 on 68 Intel Knights Landing cores and ~ 12\u00d7 on 14 Intel Haswell cores", "venue": "2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Joshua Dennis Booth", "Gregory  Bolet"], "year": 2019, "n_citations": 1}
{"id": 5263331, "s2_id": "e6db99fd95b7cd98f218bb44235aece8051cfcf0", "title": "The flare package for high dimensional linear regression and precision matrix estimation in R", "abstract": "This paper describes an R package named flare, which implements a family of new high dimensional regression methods (LAD Lasso, SQRT Lasso, \u2113 q Lasso, and Dantzig selector) and their extensions to sparse precision matrix estimation (TIGER and CLIME). These methods exploit different nonsmooth loss functions to gain modeling exibility, estimation robustness, and tuning insensitiveness. The developed solver is based on the alternating direction method of multipliers (ADMM), which is further accelerated by the multistage screening approach. The package flare is coded in double precision C, and called from R by a user-friendly interface. The memory usage is optimized by using the sparse matrix output. The experiments show that flare is efficient and can scale up to large problems.", "venue": "J. Mach. Learn. Res.", "authors": ["Xingguo  Li", "Tuo  Zhao", "Xiaoming  Yuan", "Han  Liu"], "year": 2015, "n_citations": 59}
{"id": 5270009, "s2_id": "2e00cd9ebf0a8fd5b938491d13b9f6fe67a58c99", "title": "A Representation of Binary Matrices", "abstract": "In this article we discuss the presentation of a random binary matrix using sequence of whole nonnegative numbers. We examine some advantages and disadvantages of this presentation as an alternative of the standard presentation using two-dimensional array. It is shown that the presentation of binary matrices using ordered n-tuples of natural numbers makes the algorithms faster and saves a lot of memory. In this work we use object-oriented programming using the syntax and the semantic of C++ programming language.", "venue": "ArXiv", "authors": ["Hristina  Kostadinova", "Krasimir  Yordzhev"], "year": 2012, "n_citations": 6}
{"id": 5287040, "s2_id": "fd61733fbce1a73cfde422df5a95a9f61610febd", "title": "VECMAtk: a scalable verification, validation and uncertainty quantification toolkit for scientific simulations", "abstract": "We present the VECMA toolkit (VECMAtk), a flexible software environment for single and multiscale simulations that introduces directly applicable and reusable procedures for verification, validation (V&V), sensitivity analysis (SA) and uncertainty quantication (UQ). It enables users to verify key aspects of their applications, systematically compare and validate the simulation outputs against observational or benchmark data, and run simulations conveniently on any platform from the desktop to current multi-petascale computers. In this sequel to our paper on VECMAtk which we presented last year [1] we focus on a range of functional and performance improvements that we have introduced, cover newly introduced components, and applications examples from seven different domains such as conflict modelling and environmental sciences. We also present several implemented patterns for UQ/SA and V&V, and guide the reader through one example concerning COVID-19 modelling in detail. This article is part of the theme issue \u2018Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico\u2019.", "venue": "Philosophical Transactions of the Royal Society A", "authors": ["D.  Groen", "H.  Arabnejad", "V.  Jancauskas", "W. N. Edeling", "F.  Jansson", "R. A. Richardson", "J.  Lakhlili", "L.  Veen", "B.  Bosak", "P.  Kopta", "D. W. Wright", "N.  Monnier", "P.  Karlshoefer", "D.  Suleimenova", "R.  Sinclair", "M.  Vassaux", "A.  Nikishova", "M.  Bieniek", "Onnie O. Luk", "M.  Kulczewski", "E.  Raffin", "D.  Crommelin", "O.  Hoenen", "D. P. Coster", "T.  Piontek", "P. V. Coveney"], "year": 2021, "n_citations": 10}
{"id": 5291276, "s2_id": "d3fb604ca2d035bb6338638e4791534d1cf4d4a4", "title": "Alsvinn: A Fast multi-GPGPU finite volume solver with a strong emphasis on reproducibility", "abstract": "We present the Alsvinn simulator, a fast multi general purpose graphical processing unit (GPGPU) finite volume solver for hyperbolic conservation laws in multiple space dimensions. Alsvinn has native support for uncertainty quantifications, and exhibits excellent scaling on top tier compute clusters.", "venue": "ArXiv", "authors": ["Kjetil  Lye"], "year": 2019, "n_citations": 1}
{"id": 5293747, "s2_id": "9c66c91a027a064d91160a36f8bd6f153616aa1e", "title": "Diversification improves interpolation", "abstract": "We consider the problem of interpolating an unknown multivariate polynomial with coefficients taken from a finite field or as numerical approximations of complex numbers. Building on the recent work of Garg and Schost, we improve on the best-known algorithm for interpolation over large finite fields by presenting a Las Vegas randomized algorithm that uses fewer black box evaluations. Using related techniques, we also address numerical interpolation of sparse polynomials with complex coefficients, and provide the first provably stable algorithm (in the sense of relative error) for this problem, at the cost of modestly more evaluations. A key new technique is a randomization which makes all coefficients of the unknown polynomial distinguishable, producing what we call a diverse polynomial. Another departure from most previous approaches is that our algorithms do not rely on root finding as a subroutine. We show how these improvements affect the practical performance with trial implementations.", "venue": "ISSAC '11", "authors": ["Mark  Giesbrecht", "Daniel S. Roche"], "year": 2011, "n_citations": 31}
{"id": 5299496, "s2_id": "2429293e61a33b31a5fe28011206ffcecfc40117", "title": "Computing Tropical Prevarieties in Parallel", "abstract": "The computation of the tropical prevariety is the first step in the application of polyhedral methods to compute positive dimensional solution sets of polynomial systems. In particular, pretropisms are candidate leading exponents for the power series developments of the solutions. The computation of the power series may start as soon as one pretropism is available, so our parallel computation of the tropical prevariety has an application in a pipelined solver. We present a parallel implementation of dynamic enumeration. Our first distributed memory implementation with forked processes achieved good speedups, but quite often resulted in large variations in the execution times of the processes. The shared memory multithreaded version applies work stealing to reduce the variability of the run time. Our implementation applies the thread safe Parma Polyhedral Library (PPL), in exact arithmetic with the GNU Multiprecision Arithmetic Library (GMP), aided by the fast memory allocations of TCMalloc. Our parallel implementation is capable of computing the tropical prevariety of the cyclic 16-roots problem. We also report on computational experiments on the n-body and n-vortex problems; our computational results compare favorably with Gfan.", "venue": "PASCO@ISSAC", "authors": ["Anders Nedergaard Jensen", "Jeff  Sommars", "Jan  Verschelde"], "year": 2017, "n_citations": 3}
{"id": 5303262, "s2_id": "f75fdd206ba5bbcc2248b2b6a6ff3f4d07624576", "title": "ACORNS: An Easy-To-Use Code Generator for Gradients and Hessians", "abstract": "The computation of first and second-order derivatives is a staple in many computing applications, ranging from machine learning to scientific computing. We propose an algorithm to automatically differentiate algorithms written in a subset of C99 code and its efficient implementation as a Python script. We demonstrate that our algorithm enables automatic, reliable, and efficient differentiation of common algorithms used in physical simulation and geometry processing.", "venue": "ArXiv", "authors": ["Deshana  Desai", "Etai  Shuchatowitz", "Zhongshi  Jiang", "Teseo  Schneider", "Daniele  Panozzo"], "year": 2020, "n_citations": 1}
{"id": 5305319, "s2_id": "f1182798ab59b1f6099359f3a812895cb48d3716", "title": "BioSig - An application of Octave", "abstract": "BioSig is an open source software library for biomedical signal processing. Most users in the field are using Matlab; however, significant effort was undertaken to provide compatibility to Octave, too. This effort has been widely successful, only some non-critical components relying on a graphical user interface are missing. Now, installing BioSig on Octave is as easy as on Matlab. Moreover, a benchmark test based on BioSig has been developed and the benchmark results of several platforms are presented.", "venue": "ArXiv", "authors": ["Alois  Schl\u00f6gl"], "year": 2006, "n_citations": 1}
{"id": 5318786, "s2_id": "a117e84ef2f5acc4a222729775491f47a6bc6d82", "title": "Verified Optimization", "abstract": "Optimization is used extensively in engineering, industry, and finance, and various methods are used to transform problems to the point where they are amenable to solution by numerical methods. We describe progress towards developing a framework, based on the Lean interactive proof assistant, for designing and applying such reductions in reliable and flexible ways.", "venue": "ArXiv", "authors": ["Alexander  Bentkamp", "Jeremy  Avigad"], "year": 2021, "n_citations": 0}
{"id": 5319793, "s2_id": "83b950df259914480198a20499688656060b5796", "title": "A Representation of Changes of Images and its Application for Developmental Biolology", "abstract": "In this paper, we consider a series of events observed at spaced time intervals and present a method of representation of the series. To explain an idea, by dealing with a set of gene expression data, which could be obtained from developmental biology, the procedures are sketched with comments in some details. We mean representation by choosing a proper function, which fits well with observed data of a series, and turning its characteristics into numbers, which extract the intrinsic properties of fluctuating data. With help of a machine learning techniques, this method will give a classification of developmental biological data as well as any varying data during a certain period and the classification can be applied for diagnosis of a disease.", "venue": "ArXiv", "authors": ["Gene  Kim", "MyungHo  Kim"], "year": 2003, "n_citations": 0}
{"id": 5322371, "s2_id": "aaffd81ded735afcde73b18e2cb35dfa0adcf42a", "title": "Parallel random variates generator for GPUs based on normal numbers", "abstract": "Pseudorandom number generators are required for many computational tasks, such as stochastic modelling and simulation. This paper investigates the serial CPU and parallel GPU implementation of a Linear Congruential Generator based on the binary representation of the normal number $\\alpha_{2,3}$. We adapted two methods of modular reduction which allowed us to perform most operations in 64-bit integer arithmetic, improving on the original implementation based on 106-bit double-double operations. We found that our implementation is faster than existing methods in literature, and our generation rate is close to the limiting rate imposed by the efficiency of writing to a GPU's global memory.", "venue": "ArXiv", "authors": ["Gleb  Beliakov", "Michael  Johnstone", "Douglas C. Creighton", "Tim  Wilkin"], "year": 2012, "n_citations": 0}
{"id": 5326858, "s2_id": "02f7b7def3a66b1c0dd73bff0abb8183368c39a6", "title": "Python Implementation and Construction of Finite Abelian Groups", "abstract": "Here we present a working framework to establish finite abelian groups in python. The primary aim is to allow new A-level students to work with examples of finite abelian groups using open source software. We include the code used in the implementation of the framework. We also prove some useful results regarding finite abelian groups which are used to establish the functions and help show how number theoretic results can blend with computational power when studying algebra. The groups established are based modular multiplication and addition. We include direct products of cyclic groups meaning the user has access to all finite abelian groups.", "venue": "ArXiv", "authors": ["Paul  Bradley", "John  Smethurst"], "year": 2017, "n_citations": 0}
{"id": 5328328, "s2_id": "ee5a23d4855a3d854395924cb3d06a3a839373f7", "title": "GAIL - Guaranteed Automatic Integration Library in MATLAB: Documentation for Version 2.1", "abstract": "Automatic and adaptive approximation, optimization, or integration of functions in a cone with guarantee of accuracy is a relatively new paradigm. Our purpose is to create an open-source MATLAB package, Guaranteed Automatic Integration Library (GAIL), following the philosophy of reproducible research and sustainable practices of robust scientific software development. For our conviction that true scholarship in computational sciences are characterized by reliable reproducibility, we employ the best practices in mathematical research and software engineering known to us and available in MATLAB. This document describes the key features of functions in GAIL, which includes one-dimensional function approximation and minimization using linear splines, one-dimensional numerical integration using trapezoidal rule, and last but not least, mean estimation and multidimensional integration by Monte Carlo methods or Quasi Monte Carlo methods.", "venue": "ArXiv", "authors": ["Sou-Cheng T. Choi", "Yuhan  Ding", "Fred J. Hickernell", "Lan  Jiang", "Llu\u00eds Antoni Jim\u00e9nez Rugama", "Xin  Tong", "Yizhi  Zhang", "Xuan  Zhou"], "year": 2015, "n_citations": 5}
{"id": 5333723, "s2_id": "262f5ce0a35d2a6a4905b3a0b668823da3a0e4a9", "title": "Enhancing speed and scalability of the ParFlow simulation code", "abstract": "Regional hydrology studies are often supported by high-resolution simulations of subsurface flow that require expensive and extensive computations. Efficient usage of the latest high performance parallel computing systems becomes a necessity. The simulation software ParFlow has been demonstrated to meet this requirement and shown to have excellent solver scalability for up to 16,384 processes. In the present work, we show that the code requires further enhancements in order to fully take advantage of current petascale machines. We identify ParFlow\u2019s way of parallelization of the computational mesh as a central bottleneck. We propose to reorganize this subsystem using fast mesh partition algorithms provided by the parallel adaptive mesh refinement library p4est. We realize this in a minimally invasive manner by modifying selected parts of the code to reinterpret the existing mesh data structures. We evaluate the scaling performance of the modified version of ParFlow, demonstrating good weak and strong scaling up to 458k cores of the Juqueen supercomputer, and test an example application at large scale.", "venue": "Computational Geosciences", "authors": ["Carsten  Burstedde", "Jose A. Fonseca", "Stefan  Kollet"], "year": 2017, "n_citations": 11}
{"id": 5339889, "s2_id": "2a4ead3d05ace018734fe04738b3b55c3be49e75", "title": "Analysis of heterogeneous computing approaches to simulating heat transfer in heterogeneous material", "abstract": "The simulation of heat flow through heterogeneous material is important for the design of structural and electronic components. Classical analytical solutions to the heat equation PDE are not known for many such domains, even those having simple geometries. The finite element method can provide approximations to a weak form continuum solution, with increasing accuracy as the number of degrees of freedom in the model increases. This comes at a cost of increased memory usage and computation time; even when taking advantage of sparse matrix techniques for the finite element system matrix. We summarize recent approaches in solving problems in structural mechanics and steady state heat conduction which do not require the explicit assembly of any system matrices, and adapt them to a method for solving the time-depended flow of heat. These approaches are highly parallelizable, and can be performed on graphical processing units (GPUs). Furthermore, they lend themselves to the simulation of heterogeneous material, with a minimum of added complexity. We present the mathematical framework of assembly-free FEM approaches, through which we summarize the benefits of GPU computation. We discuss our implementation using the OpenCL computing framework, and show how it is further adapted for use on multiple GPUs. We compare the performance of single and dual GPUs implementations of our method with previous GPU computing strategies from the literature and a CPU sparse matrix approach. The utility of the novel method is demonstrated through the solution of a real-world coefficient inverse problem that requires thousands of transient heat flow simulations, each of which involves solving a 1 million degree of freedom linear system over hundreds of time steps.", "venue": "J. Parallel Distributed Comput.", "authors": ["Andrew  Loeb", "Christopher  Earls"], "year": 2019, "n_citations": 3}
{"id": 5339905, "s2_id": "199f5582324d79d433c512bfff5bed64fce1284b", "title": "On Formal Specification of Maple Programs", "abstract": "This paper is an example-based demonstration of our initial results on the formal specification of programs written in the computer algebra language MiniMaple (a substantial subset of Maple with slight extensions). The main goal of this work is to define a verification framework for MiniMaple. Formal specification of MiniMaple programs is rather complex task as it supports non-standard types of objects, e.g. symbols and unevaluated expressions, and additional functions and predicates, e.g. runtime type tests etc. We have used the specification language to specify various computer algebra concepts respective objects of the Maple package DifferenceDifferential developed at our institute.", "venue": "AISC/MKM/Calculemus", "authors": ["Muhammad Taimoor Khan", "Wolfgang  Schreiner"], "year": 2012, "n_citations": 2}
{"id": 5341918, "s2_id": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library", "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.", "venue": "NeurIPS", "authors": ["Adam  Paszke", "Sam  Gross", "Francisco  Massa", "Adam  Lerer", "James  Bradbury", "Gregory  Chanan", "Trevor  Killeen", "Zeming  Lin", "Natalia  Gimelshein", "Luca  Antiga", "Alban  Desmaison", "Andreas  K\u00f6pf", "Edward  Yang", "Zach  DeVito", "Martin  Raison", "Alykhan  Tejani", "Sasank  Chilamkurthy", "Benoit  Steiner", "Lu  Fang", "Junjie  Bai", "Soumith  Chintala"], "year": 2019, "n_citations": 9774}
{"id": 5343823, "s2_id": "5f2f078375c725905c2c78da087c73f6c39a3a26", "title": "Isabelle/jEdit - A Prover IDE within the PIDE Framework", "abstract": "PIDE is a general framework for document-oriented prover interaction and integration, based on a bilingual architecture that combines ML and Scala [2]. The overall aim is to connect LCF-style provers like Isabelle [5, \u00a76] (or Coq [5, \u00a74] or HOL [5, \u00a71]) with sophisticated front-end technology on the JVM platform, overcoming command-line interaction at last.", "venue": "AISC/MKM/Calculemus", "authors": ["Markus  Wenzel"], "year": 2012, "n_citations": 67}
{"id": 5356838, "s2_id": "f2c2a85298c9bda554dc060f508fdb09422f8b1c", "title": "Complementary approaches to understanding the plant circadian clock", "abstract": "Circadian clocks are oscillatory genetic networks that help organisms adapt to the 24-hour day/night cycle. The clock of the green alga Ostreococcus tauri is the simplest plant clock discovered so far. Its many advantages as an experimental system facilitate the testing of computational predictions. We present a model of the Ostreococcus clock in the stochastic process algebra Bio-PEPA and exploit its mapping to di erent analysis techniques, such as ordinary di erential equations, stochastic simulation algorithms and model-checking. The small number of molecules reported for this system tests the limits of the continuous approximation underlying di erential equations. We investigate the di erence between continuous-deterministic and discrete-stochastic approaches. Stochastic simulation and model-checking allow us to formulate new hypotheses on the system behaviour, such as the presence of self-sustained oscillations in single cells under constant light conditions. We investigate how to model the timing of dawn and dusk in the context of model-checking, which we use to compute how the probability distributions of key biochemical species change over time. These show that the relative variation in expression level is smallest at the time of peak expression, making peak time an optimal experimental phase marker. Building on these analyses, we use approaches from evolutionary systems biology to investigate how changes in the rate of mRNA degradation impacts the phase of a key protein likely to a ect fitness. We explore how robust this circadian clock is towards such potential mutational changes in its underlying biochemistry. Our work shows that multiple approaches lead to a more complete understanding of the clock.", "venue": "FBTC", "authors": ["Ozgur E. Akman", "Maria Luisa Guerriero", "Laurence  Loewe", "Carl  Troein"], "year": 2010, "n_citations": 25}
{"id": 5361028, "s2_id": "48b316ba2570a90bd0de553b9eecf7a63e3f1c77", "title": "The deal.II finite element library: Design, features, and insights", "abstract": "Abstract deal.II is a state-of-the-art finite element library focused on generality, dimension-independent programming, parallelism, and extensibility. Herein, we outline its primary design considerations and its sophisticated features such as distributed meshes, h p -adaptivity, support for complex geometries, and matrix-free algorithms. But deal.II is more than just a software library: It is also a diverse and worldwide community of developers and users, as well as an educational platform. We therefore also discuss some of the technical and social challenges and lessons learned in running a large community software project over the course of two decades.", "venue": "Comput. Math. Appl.", "authors": ["Daniel  Arndt", "Wolfgang  Bangerth", "Denis  Davydov", "Timo  Heister", "Luca  Heltai", "Martin  Kronbichler", "Matthias  Maier", "Jean-Paul  Pelteret", "Bruno  Turcksin", "David  Wells"], "year": 2021, "n_citations": 46}
{"id": 5363843, "s2_id": "61af0c2230d1ffd7a01126138cd0d4b02413b0cf", "title": "PETSc TSAdjoint: a discrete adjoint ODE solver for first-order and second-order sensitivity analysis", "abstract": "We present a new software system PETSc TSAdjoint for first-order and second-order adjoint sensitivity analysis of time-dependent nonlinear differential equations. The derivative calculation in PETSc TSAdjoint is essentially a high-level algorithmic differentiation process. The adjoint models are derived by differentiating the timestepping algorithms and implemented based on the parallel infrastructure in PETSc. Full differentiation of the library code including MPI routines thus is avoided, and users do not need to derive their own adjoint models for their specific applications. PETSc TSAdjoint can compute the first-order derivative, that is, the gradient of a scalar functional, and the Hessian-vector product that carries second-order derivative information, while requiring minimal input (a few callbacks) from the users. Optimal checkpointing schemes are employed by the adjoint model in a manner that is transparent to users. Usability, efficiency, and scalability are demonstrated through examples from a variety of applications.", "venue": "ArXiv", "authors": ["Hong  Zhang", "Emil M. Constantinescu", "Barry F. Smith"], "year": 2019, "n_citations": 5}
{"id": 5364696, "s2_id": "6fb0ba56bf7bb009d3555ec68264b76747fcdf48", "title": "Branch cuts in maple 17", "abstract": "Accurate and comprehensible knowledge about the position of branch cuts is essential for correctly working with multi-valued functions, such as the square root and logarithm. We discuss the new tools in Maple 17 for calculating and visualising the branch cuts of such functions, and others built up from them. The cuts are described in an intuitive and accurate form, offering substantial improvement on the descriptions previously available.", "venue": "ACCA", "authors": ["Matthew  England", "Edgardo S. Cheb-Terrab", "Russell J. Bradford", "James H. Davenport", "David J. Wilson"], "year": 2014, "n_citations": 8}
{"id": 5370700, "s2_id": "bf3dc1105c3675d4155b7b0c10850d05ee452e3d", "title": "Modular SIMD arithmetic in Mathemagix", "abstract": "Modular integer arithmetic occurs in many algorithms for computer algebra, cryptography, and error correcting codes. Although recent microprocessors typically offer a wide range of highly optimized arithmetic functions, modular integer operations still require dedicated implementations. In this article, we survey existing algorithms for modular integer arithmetic and present detailed vectorized counterparts. We also describe several applications, such as fast modular Fourier transforms and multiplication of integer polynomials and matrices. The vectorized algorithms have been implemented in C++ inside the free computer algebra and analysis system Mathemagix. The performance of our implementation is illustrated by various benchmarks.", "venue": "ACM Trans. Math. Softw.", "authors": ["Joris van der Hoeven", "Gr\u00e9goire  Lecerf", "Guillaume  Quintin"], "year": 2016, "n_citations": 23}
{"id": 5380290, "s2_id": "566c4fe4036934e1d4374220a3db0b2bb3106ab3", "title": "A Blackbox Polynomial System Solver on Parallel Shared Memory Computers", "abstract": "A numerical irreducible decomposition for a polynomial system provides representations for the irreducible factors of all positive dimensional solution sets of the system, separated from its isolated solutions. Homotopy continuation methods are applied to compute a numerical irreducible decomposition. Load balancing and pipelining are techniques in a parallel implementation on a computer with multicore processors. The application of the parallel algorithms is illustrated on solving the cyclic $n$-roots problems, in particular for $n = 8, 9$, and~12.", "venue": "CASC", "authors": ["Jan  Verschelde"], "year": 2018, "n_citations": 1}
{"id": 5383649, "s2_id": "037bead381158bcdf9a1d81df39b7656fbc74a18", "title": "Towards an efficient use of the BLAS library for multilinear tensor contractions", "abstract": "Abstract Mathematical operators whose transformation rules constitute the building blocks of a multi-linear algebra are widely used in physics and engineering applications where they are very often represented as tensors. In the last century, thanks to the advances in tensor calculus, it was possible to uncover new research fields and make remarkable progress in the existing ones, from electromagnetism to the dynamics of fluids and from the mechanics of rigid bodies to quantum mechanics of many atoms. By now, the formal mathematical and geometrical properties of tensors are well defined and understood; conversely, in the context of scientific and high-performance computing, many tensor-related problems are still open. In this paper, we address the problem of efficiently computing contractions among two tensors of arbitrary dimension by using kernels from the highly optimized BLAS library. In particular, we establish precise conditions to determine if and when GEMM, the kernel for matrix products, can be used. Such conditions take into consideration both the nature of the operation and the storage scheme of the tensors, and induce a classification of the contractions into three groups. For each group, we provide a recipe to guide the users towards the most effective use of BLAS.", "venue": "Appl. Math. Comput.", "authors": ["Edoardo Di Napoli", "Diego  Fabregat-Traver", "Gregorio  Quintana-Ort\u00ed", "Paolo  Bientinesi"], "year": 2014, "n_citations": 38}
{"id": 5393621, "s2_id": "c31359f4a4d9d95bfa991dacee09335e1cba2b5d", "title": "Computing huge Groebner basis like cyclic10 over $\\Q$ with Giac", "abstract": "We present a short description on how to fine-tune the modular algorithm implemented in the Giac computer algebra system to reconstruct huge Groebner basis over $\\Q$. \nThe classical cyclic10 benchmark will serve as example.", "venue": "ArXiv", "authors": ["Bernard  Parisse"], "year": 2019, "n_citations": 0}
{"id": 5395106, "s2_id": "8e3bd9d304376062f76a9114b4c1c19c05d24c05", "title": "The Multiple Zeta Value data mine", "abstract": "Abstract We provide a data mine of proven results for Multiple Zeta Values (MZVs) of the form \u03b6 ( s 1 , s 2 , \u2026 , s k ) = \u2211 n 1 > n 2 > \u22ef > n k > 0 \u221e { 1 / ( n 1 s 1 \u22ef n k s k ) } with weight w = \u2211 i = 1 k s i and depth k and for Euler sums of the form \u2211 n 1 > n 2 > \u22ef > n k > 0 \u221e { ( \u03f5 1 n 1 \u22ef \u03f5 1 n k ) / ( n 1 s 1 \u22ef n k s k ) } with signs \u03f5 i = \u00b1 1 . Notably, we achieve explicit proven reductions of all MZVs with weights w \u2a7d 22 , and all Euler sums with weights w \u2a7d 12 , to bases whose dimensions, bigraded by weight and depth, have sizes in precise agreement with the Broadhurst\u2013Kreimer and Broadhurst conjectures. Moreover, we lend further support to these conjectures by studying even greater weights ( w \u2a7d 30 ), using modular arithmetic. To obtain these results we derive a new type of relation for Euler sums, the Generalized Doubling Relations. We elucidate the \u201cpushdown\u201d mechanism, whereby the ornate enumeration of primitive MZVs, by weight and depth, is reconciled with the far simpler enumeration of primitive Euler sums. There is some evidence that this pushdown mechanism finds its origin in doubling relations. We hope that our data mine, obtained by exploiting the unique power of the computer algebra language form , will enable the study of many more such consequences of the double-shuffle algebra of MZVs, and their Euler cousins, which are already the subject of keen interest, to practitioners of Quantum Field Theory, and to mathematicians alike.", "venue": "Comput. Phys. Commun.", "authors": ["Johannes  Bl\u00fcmlein", "D. J. Broadhurst", "J. A. M. Vermaseren"], "year": 2010, "n_citations": 264}
{"id": 5397781, "s2_id": "09a4f8f40f69f902961ba96a7d1e55826135bc62", "title": "QRkit: Sparse, Composable QR Decompositions for Efficient and Stable Solutions to Problems in Computer Vision", "abstract": "Embedded computer vision applications increasingly require the speed and power benefits of single-precision (32 bit) floating point. However, applications which make use of Levenberg-like optimization can lose significant accuracy when reducing to single precision, sometimes unrecoverably so. This accuracy can be regained using solvers based on QR rather than Cholesky decomposition, but the absence of sparse QR solvers for common sparsity patterns found in computer vision means that many applications cannot benefit. We introduce an open-source suite of solvers for Eigen, which efficiently compute the QR decomposition for matrices with some common sparsity patterns (block diagonal, horizontal and vertical concatenation, and banded). For problems with very particular sparsity structures, these elements can be composed together in 'kit' form, hence the name QRkit. We apply our methods to several computer vision problems, showing competitive performance and suitability especially in single precision arithmetic.", "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)", "authors": ["Jan  Svoboda", "Thomas  Cashman", "Andrew W. Fitzgibbon"], "year": 2018, "n_citations": 3}
{"id": 5398596, "s2_id": "e987fbf92a2befd1ab7de411a4fd3fd0c7ea5d2e", "title": "Number parsing at a gigabyte per second", "abstract": "With disks and networks providing gigabytes per second, parsing decimal numbers from strings becomes a bottleneck. We consider the problem of parsing decimal numbers to the nearest binary floating\u2010point value. The general problem requires variable\u2010precision arithmetic. However, we need at most 17 digits to represent 64\u2010bit standard floating\u2010point numbers (IEEE 754). Thus, we can represent the decimal significand with a single 64\u2010bit word. By combining the significand and precomputed tables, we can compute the nearest floating\u2010point number using as few as one or two 64\u2010bit multiplications. Our implementation can be several times faster than conventional functions present in standard C libraries on modern 64\u2010bit systems (Intel, AMD, ARM, and POWER9). Our work is available as open source software used by major systems such as Apache Arrow and Yandex ClickHouse. The Go standard library has adopted a version of our approach.", "venue": "Softw. Pract. Exp.", "authors": ["Daniel  Lemire"], "year": 2021, "n_citations": 1}
{"id": 5404014, "s2_id": "6829981c813c122dbe13f9294f115b79278ad82f", "title": "A scikit-based Python environment for performing multi-label classification", "abstract": "scikit-multilearn is a Python library for performing multi-label classification. The library is compatible with the scikit/scipy ecosystem and uses sparse matrices for all internal operations. It provides native Python implementations of popular multi-label classification methods alongside a novel framework for label space partitioning and division. It includes modern algorithm adaptation methods, network-based label space division approaches, which extracts label dependency information and multi-label embedding classifiers. It provides python wrapped access to the extensive multi-label method stack from Java libraries and makes it possible to extend deep learning single-label methods for multi-label tasks. The library allows multi-label stratification and data set management. The implementation is more efficient in problem transformation than other established libraries, has good test coverage and follows PEP8. Source code and documentation can be downloaded from this http URL and also via pip. The library follows BSD licensing scheme.", "venue": "ArXiv", "authors": ["Piotr  Szymanski", "Tomasz  Kajdanowicz"], "year": 2017, "n_citations": 67}
{"id": 5419204, "s2_id": "e5cf893d0434e8b396edf844ced024259d8e5532", "title": "Improved Accuracy and Parallelism for MRRR-Based Eigensolvers - A Mixed Precision Approach", "abstract": "The real symmetric tridiagonal eigenproblem is of outstanding importance in numerical computations; it arises frequently as part of eigensolvers for standard and generalized dense Hermitian eigenproblems that are based on a reduction to real tridiagonal form. For its solution, the algorithm of multiple relatively robust representations (MRRR) is among the fastest methods. Although fast, the solvers based on MRRR do not deliver the same accuracy as competing methods like Divide & Conquer or the QR algorithm. In this paper, we demonstrate that the use of mixed precisions leads to improved accuracy of MRRR-based eigensolvers with limited or no performance penalty. As a result, we obtain eigensolvers that are not only as accurate as or more accurate than the best available methods, but also---under most circumstances---faster and more scalable than the competition.", "venue": "SIAM J. Sci. Comput.", "authors": ["Matthias  Petschow", "Enrique S. Quintana-Ort\u00ed", "Paolo  Bientinesi"], "year": 2014, "n_citations": 6}
{"id": 5420073, "s2_id": "072097508a2c149f512403a896c87ce4b11bb980", "title": "Experiences with Automated Build and Test for Geodynamics Simulation Codes", "abstract": "The Computational Infrastructure for Geodynamics (CIG) is an NSF funded project that develops, supports, and disseminates community-accessible software for the geodynamics research community. CIG software supports a variety of computational geodynamic research from mantle and core dynamics, to crustal and earthquake dynamics, to magma migration and seismology. To support this type of project a backend computational infrastructure is necessary. \nPart of this backend infrastructure is an automated build and testing system to ensure codes and changes to them are compatible with multiple platforms and that the changes do not significantly affect the scientific results. In this paper we describe the build and test infrastructure for CIG based on the BaTLab system, how it is organized, and how it assists in operations. We demonstrate the use of this type of testing for a suite of geophysics codes, show why codes may compile on one platform but not on another, and demonstrate how minor changes may alter the computed results in unexpected ways that can influence the scientific interpretation. Finally, we examine result comparison between platforms and show how the compiler or operating system may affect results.", "venue": "ArXiv", "authors": ["Eric M. Heien", "Todd L. Miller", "Becky  Gietzel", "Louise H. Kellogg"], "year": 2013, "n_citations": 3}
{"id": 5423175, "s2_id": "d07ada06f7b33fc9e13a2361ddf3781f1f93438b", "title": "Improved QFT algorithm for power-of-two FFT", "abstract": "This paper shows that it is possible to improve the computational cost, the memory requirements and the accuracy of Quick Fourier Transform (QFT) algorithm for power-of-two FFT (Fast Fourier Transform) just introducing a slight modification in this algorithm. The new algorithm requires the same number of additions and multiplications of split-radix 3add/3mul, one of the most appreciated FFT algorithms appeared in the literature, but employing only half of the trigonometric constants. These results can elevate the QFT approach to the level of most used FFT procedures. A new quite general way to describe FFT algorithms, based on signal types and on a particular notation, is also proposed and used, highligting its advantages.", "venue": "ArXiv", "authors": ["Lorenzo  Pasquini"], "year": 2013, "n_citations": 1}
{"id": 5430179, "s2_id": "ef701b758c6877e2b857ed181fd4bc238cb71863", "title": "Multiscale finite element calculations in Python using SfePy", "abstract": "SfePy (simple finite elements in Python) is a software for solving various kinds of problems described by partial differential equations in one, two, or three spatial dimensions by the finite element method. Its source code is mostly (85%) Python and relies on fast vectorized operations provided by the NumPy package. For a particular problem, two interfaces can be used: a declarative application programming interface (API), where problem description/definition files (Python modules) are used to define a calculation, and an imperative API, that can be used for interactive commands, or in scripts and libraries. After outlining the SfePy package development, the paper introduces its implementation, structure, and general features. The components for defining a partial differential equation are described using an example of a simple heat conduction problem. Specifically, the declarative API of SfePy is presented in the example. To illustrate one of SfePy\u2019s main assets, the framework for implementing complex multiscale models based on the theory of homogenization, an example of a two-scale piezoelastic model is presented, showing both the mathematical description of the problem and the corresponding code.", "venue": "Adv. Comput. Math.", "authors": ["Robert  Cimrman", "Vladim\u00edr  Lukes", "Eduard  Rohan"], "year": 2019, "n_citations": 26}
{"id": 5432251, "s2_id": "9dbf3d6f4e161576a29b4060c8444b6892c458d5", "title": "Computing rank-revealing factorizations of matrices stored out-of-core", "abstract": "This paper describes efficient algorithms for computing rank-revealing factorizations of matrices that are too large to fit in RAM, and must instead be stored on slow external memory devices such as solid-state or spinning disk hard drives (out-of-core or out-of-memory). Traditional algorithms for computing rank revealing factorizations, such as the column pivoted QR factorization, or techniques for computing a full singular value decomposition of a matrix, are very communication intensive. They are naturally expressed as a sequence of matrix-vector operations, which become prohibitively expensive when data is not available in main memory. Randomization allows these methods to be reformulated so that large contiguous blocks of the matrix can be processed in bulk. The paper describes two distinct methods. The first is a blocked version of column pivoted Householder QR, organized as a \"left-looking\" method to minimize the number of write operations (which are more expensive than read operations on a spinning disk drive). The second method results in a so called UTV factorization which expresses a matrix $A$ as $A = U T V^*$ where $U$ and $V$ are unitary, and $T$ is triangular. This method is organized as an algorithm-by-blocks, in which floating point operations overlap read and write operations. The second method incorporates power iterations, and is exceptionally good at revealing the numerical rank; it can often be used as a substitute for a full singular value decomposition. Numerical experiments demonstrate that the new algorithms are almost as fast when processing data stored on a hard drive as traditional algorithms are for data stored in main memory. To be precise, the computational time for fully factorizing an $n\\times n$ matrix scales as $cn^{3}$, with a scaling constant $c$ that is only marginally larger when the matrix is stored out of core.", "venue": "ArXiv", "authors": ["Nathan  Heavner", "Per-Gunnar  Martinsson", "Gregorio  Quintana-Ort'i"], "year": 2020, "n_citations": 1}
{"id": 5433244, "s2_id": "5ecf983b9c08d5b3ea3421a41f4b35b3da982781", "title": "Algorithmic differentiation of hyperbolic flow problems", "abstract": "We are interested in the development of an algorithmic differentiation framework for computing approximations to tangent vectors to scalar and systems of hyperbolic partial differential equations. The main difficulty of such a numerical method is the presence of shock waves that are resolved by proposing a numerical discretization of the calculus introduced in Bressan and Marson [Rend. Sem. Mat. Univ. Padova, 94:79-94, 1995]. Numerical results are presented for the one-dimensional Burgers equation and the Euler equations. Using the essential routines of a state-of-the-art code for computational fluid dynamics (CFD) as a starting point, three modifications are required to apply the introduced calculus. First, the CFD code is modified to solve an additional equation for the shock location. Second, we customize the computation of the corresponding tangent to the shock location. Finally, the modified method is enhanced by algorithmic differentiation. Applying the introduced calculus to problems of the Burgers equation and the Euler equations, it is found that correct sensitivities can be computed, whereas the application of black-box algorithmic differentiation fails.", "venue": "J. Comput. Phys.", "authors": ["Michael  Herty", "Jonathan  H\u00fcser", "Uwe  Naumann", "Thomas  Schilden", "Wolfgang  Schr\u00f6der"], "year": 2021, "n_citations": 0}
{"id": 5435869, "s2_id": "e4d75ef7dad1e4174ec5b16db461450eda07bda8", "title": "Extending DUNE: The dune-xt modules", "abstract": "Abstract: We present our effort to extend and complement the core modules of the Dis- tributed and Unified Numerics Environment DUNE (http://dune-project.org) by a well tested and structured collection of utilities and concepts. We describe key elements of our four modules dune-xt-common, dune-xt-grid, dune-xt-la and dune-xt-functions, which aim add further enabling the programming of generic algorithms within DUNE as well as adding an extra layer of usability and convenience.", "venue": "ArXiv", "authors": ["Tobias  Leibner", "Ren\u00e9  Milk", "Felix  Schindler"], "year": 2016, "n_citations": 7}
{"id": 5438163, "s2_id": "9a876e64f8d848643ec3aab3d19a4913e00caa7d", "title": "Characteristics-based Simulink implementation of first-order quasilinear partial differential equations", "abstract": "The paper deals with solving first-order quasilinear partial differential equations in an online simulation environment, such as Simulink, utilizing the well-known and well-recommended method of characteristics. Compared to the commonly applied space discretization methods on static grids, the characteristics-based approach provides better numerical stability. Simulink subsystem implementing the method of characteristics is developed. It employs Simulink's built-in solver and its zero-crossing detection algorithm to perform simultaneous integration of a pool of characteristics as well as to create new characteristics dynamically and discard the old ones. Numerical accuracy of the solution thus obtained is established. The subsystem has been tested on a full-state feedback example and produced better results than the space discretization-based \"method of lines\". The implementation is available for download and can be used in a wide range of models.", "venue": "SIMULTECH", "authors": ["Anton  Ponomarev", "Julian  Hofmann", "Lutz  Gr\u00f6ll"], "year": 2020, "n_citations": 2}
{"id": 5445917, "s2_id": "dd7bbf62becfb82eb9d4fb23a2c28b3d7158e22b", "title": "Software for enumerative and analytic combinatorics", "abstract": "We survey some general-purpose symbolic software packages that implement algorithms from enumerative and analytic combinatorics. Software for the following areas is covered: basic combinatorial objects, symbolic combinatorics, P\\'olya theory, combinatorial species, and asymptotics. We describe the capabilities that the packages offer as well as some of the algorithms used, and provide links to original documentation. Most of the packages are freely downloadable from the web.", "venue": "ArXiv", "authors": ["Andrew  MacFie"], "year": 2016, "n_citations": 1}
{"id": 5450977, "s2_id": "f7f72bdb78ebe14b601414cfc8e01829cffb57f7", "title": "DATeS: a highly extensible data assimilation testing suite v1.0", "abstract": "Abstract. A flexible and highly extensible data assimilation testing suite, named\nDATeS, is described in this paper. DATeS aims to offer a unified testing\nenvironment that allows researchers to compare different data assimilation\nmethodologies and understand their performance in various settings. The core\nof DATeS is implemented in Python and takes advantage of its object-oriented\ncapabilities. The main components of the package (the numerical models, the\ndata assimilation algorithms, the linear algebra solvers, and the time\ndiscretization routines) are independent of each other, which offers great\nflexibility to configure data assimilation applications. DATeS can interface\neasily with large third-party numerical models written in Fortran or in C,\nand with a plethora of external solvers.\n", "venue": "Geoscientific Model Development", "authors": ["Ahmed  Attia", "Adrian  Sandu"], "year": 2019, "n_citations": 7}
{"id": 5452119, "s2_id": "e2e92c0114fc1cc4d47a8d9b390b724c09a99ca3", "title": "A Scalable Shared-Memory Parallel Simplex for Large-Scale Linear Programming", "abstract": "We present a shared-memory parallel implementation of the Simplex tableau algorithm for dense large-scale Linear Programming (LP) problems. We present the general scheme and explain each parallelization step of the standard simplex algorithm, emphasizing important solutions for solving performance bottlenecks. We analyzed the speedup and the parallel efficiency for the proposed implementation relative to the standard Simplex algorithm using a shared-memory system with 64 processing cores. The experiments were performed for several different problems, with up to 8192 variables and constraints, in their primal and dual formulations. The results show that the performance is mostly much better when we use the formulation with more variables than inequality constraints. Also, they show that the parallelization strategies applied to avoid bottlenecks caused the implementation to scale well with the problem size and the core count up to a certain limit of problem size. Further analysis showed that this was an effect of resource limitation. Even though, our implementation was able to reach speedups in the order of 19x.", "venue": "ArXiv", "authors": ["Demetrios  Coutinho", "Samuel Xavier de Souza", "Daniel  Aloise"], "year": 2018, "n_citations": 1}
{"id": 5453961, "s2_id": "673817503e0c4d12cec1c339bda3e752dc6c47c9", "title": "Assessment Of The Wind Farm Impact On The Radar", "abstract": "This study shows the means to evaluate the wind farm impact on the radar. It proposes the set of tools, which can be used to realise this objective. The big part of report covers the study of complex pattern propagation factor as the critical issue of the Advanced Propagation Model (APM). Finally, the reader can find here the implementation of this algorithm - the real scenario in Inverness airport (the United Kingdom), where the ATC radar STAR 2000, developed by Thales Air Systems, operates in the presence of several wind farms. Basically, the project is based on terms of the department \"Strategy Technology & Innovation\", where it has been done. Also you can find here how the radar industry can act with the problem engendered by wind farms. The current strategies in this area are presented, such as a wind turbine production, improvements of air traffic handling procedures and the collaboration between developers of radars and wind turbines. The possible strategy for Thales as a main pioneer was given as well.", "venue": "ArXiv", "authors": ["Evgeny D. Norman"], "year": 2010, "n_citations": 0}
{"id": 5462896, "s2_id": "7cdb06d703fce6eb7eeb190831396bada905fbce", "title": "Achieving High Performance with Unified Residual Evaluation", "abstract": "We examine residual evaluation, perhaps the most basic operation in numerical simulation. By raising the level of abstraction in this operation, we can eliminate specialized code, enable optimization, and greatly increase the extensibility of existing code.", "venue": "ArXiv", "authors": ["Matthew G. Knepley", "Jed  Brown", "Karl  Rupp", "Barry  Smith"], "year": 2013, "n_citations": 7}
{"id": 5468382, "s2_id": "935869cba2a3d637afbca1130506cb6c44ce50c7", "title": "Throughput-Distortion Computation of Generic Matrix Multiplication: Toward a Computation Channel for Digital Signal Processing Systems", "abstract": "The generic matrix multiply (GEMM) function is the core element of high-performance linear algebra libraries used in many computationally demanding digital signal processing (DSP) systems. We propose an acceleration technique for GEMM based on dynamically adjusting the imprecision (distortion) of computation. Our technique employs adaptive scalar companding and rounding to input matrix blocks followed by two forms of packing in floating-point that allow for concurrent calculation of multiple results. Since the adaptive companding process controls the increase of concurrency (via packing), the increase in processing throughput (and the corresponding increase in distortion) depends on the input data statistics. To demonstrate this, we derive the optimal throughput-distortion control framework for GEMM for the broad class of zero-mean, independent identically distributed, input sources. Our approach converts matrix multiplication in programmable processors into a computation channel: when increasing the processing throughput, the output noise (error) increases due to: (i) coarser quantization; and (ii) computational errors caused by exceeding the machine-precision limitations. We show that, under certain distortion in the GEMM computation, the proposed framework can significantly surpass 100% of the peak performance of a given processor. The practical benefits of our proposal are shown in a face recognition system and a multilayer perceptron system trained for metadata learning from a large music feature database.", "venue": "IEEE Transactions on Signal Processing", "authors": ["Davide  Anastasia", "Yiannis  Andreopoulos"], "year": 2012, "n_citations": 16}
{"id": 5468390, "s2_id": "c78b69cb40d749f0011fbe47f3a0cdf2fcc56592", "title": "Divide-And-Conquer Computation of Cylindrical Algebraic Decomposition", "abstract": "We present a divide-and-conquer version of the Cylindrical Algebraic Decomposition (CAD) algorithm. The algorithm represents the input as a Boolean combination of subformulas, computes cylindrical algebraic decompositions of solution sets of the subformulas, and combines the results. We propose a graph-based heuristic to find a suitable partitioning of the input and present empirical comparison with direct CAD computation.", "venue": "ArXiv", "authors": ["Adam W. Strzebonski"], "year": 2014, "n_citations": 2}
{"id": 5468802, "s2_id": "d334cdd5ad417216eba053f1f1d75c109458c2f0", "title": "Owl: A General-Purpose Numerical Library in OCaml", "abstract": "Owl is a new numerical library developed in the OCaml language. It focuses on providing a comprehensive set of high-level numerical functions so that developers can quickly build up data analytical applications. In this abstract, we will present Owl's design, core components, and its key functionality.", "venue": "ArXiv", "authors": ["Liang  Wang"], "year": 2017, "n_citations": 18}
{"id": 5471365, "s2_id": "809f1a177fa81be59b58afb6fef1beaf32133f21", "title": "Locally Feasibly Projected Sequential Quadratic Programming for Nonlinear Programming on Arbitrary Smooth Constraint Manifolds", "abstract": "High-dimensional nonlinear optimization problems subject to nonlinear constraints can appear in several contexts including constrained physical and dynamical systems, statistical estimation, and other numerical models. Feasible optimization routines can sometimes be valuable if the objective function is only defined on the feasible set or if numerical difficulties associated with merit functions or infeasible termination arise during the use of infeasible optimization routines. Drawing on the Riemannian optimization and sequential quadratic programming literature, a practical algorithm is constructed to conduct feasible optimization on arbitrary implicitly defined constraint manifolds. Specifically, with n (potentially bound-constrained) variables and m < n nonlinear constraints, each outer optimization loop iteration involves a single O ( nm2 ) -flop factorization, and computationally efficient retractions are constructed that involve O(nm)-flop inner loop iterations. A package, LFPSQP.jl, is created using the Julia language that takes advantage of automatic differentiation and projected conjugate gradient methods for use in inexact/truncated Newton steps.", "venue": "ArXiv", "authors": ["Kevin S. Silmore", "James W. Swan"], "year": 2021, "n_citations": 0}
{"id": 5480490, "s2_id": "b03b6f101528d813d5f5cec7d208439932937e8d", "title": "Tangent: Automatic Differentiation Using Source Code Transformation in Python", "abstract": "Automatic differentiation (AD) is an essential primitive for machine learning programming systems. Tangent is a new library that performs AD using source code transformation (SCT) in Python. It takes numeric functions written in a syntactic subset of Python and NumPy as input, and generates new Python functions which calculate a derivative. This approach to automatic differentiation is different from existing packages popular in machine learning, such as TensorFlow and Autograd. Advantages are that Tangent generates gradient code in Python which is readable by the user, easy to understand and debug, and has no runtime overhead. Tangent also introduces abstractions for easily injecting logic into the generated gradient code, further improving usability.", "venue": "ArXiv", "authors": ["Bart van Merrienboer", "Alexander B. Wiltschko", "Dan  Moldovan"], "year": 2017, "n_citations": 18}
{"id": 5488631, "s2_id": "48ce83a235f725706b09e67f9ad82aa4673c75ac", "title": "An initial investigation of the performance of GPU-based swept time-space decomposition", "abstract": "Simulations of physical phenomena are essential to the expedient design of precision components in aerospace and other high-tech industries. These phenomena are often described by mathematical models involving partial differential equations (PDEs) without exact solutions. Modern design problems require simulations with a level of resolution that is difficult to achieve in a reasonable amount of time even in effectively parallelized solvers. Though the scale of the problem relative to available computing power is the greatest impediment to accelerating these applications, significant performance gains can be achieved through careful attention to the details of memory accesses. Parallelized PDE solvers are subject to a trade-off in memory management: store the solution for each timestep in abundant, global memory with high access costs or in a limited, private memory with low access costs that must be passed between nodes. The GPU implementation of swept time-space decomposition presented here mitigates this dilemma by using private (shared) memory, avoiding internode communication, and overwriting unnecessary values. It shows significant improvement in the execution time of the PDE solvers in one dimension achieving speedups of 6-2x for large and small problem sizes respectively compared to naive GPU versions and 7-300x compared to parallel CPU versions.", "venue": "ArXiv", "authors": ["Kyle E. Niemeyer", "Daniel  Magee"], "year": 2016, "n_citations": 0}
{"id": 5494406, "s2_id": "eb85edbddb0075b51c3918f608168f2fa17d598f", "title": "Experimental Evaluation of Multi-Round Matrix Multiplication on MapReduce", "abstract": "A common approach in the design of MapReduce algorithms is to minimize the number of rounds. Indeed, there are many examples in the literature of monolithic MapReduce algorithms, which are algorithms requiring just one or two rounds. However, we claim that the design of monolithic algorithms may not be the best approach in cloud systems. Indeed, multi-round algorithms may exploit some features of cloud platforms by suitably setting the round number according to the execution context. In this paper we carry out an experimental study of multi-round MapReduce algorithms aiming at investigating the performance of the multi-round approach. We use matrix multiplication as a case study. We first propose a scalable Hadoop library, named M$_3$, for matrix multiplication in the dense and sparse cases which allows to tradeoff round number with the amount of data shuffled in each round and the amount of memory required by reduce functions. Then, we present an extensive study of this library on an in-house cluster and on Amazon Web Services aiming at showing its performance and at comparing monolithic and multi-round approaches. The experiments show that, even without a low level optimization, it is possible to design multi-round algorithms with a small running time overhead.", "venue": "ALENEX", "authors": ["Matteo  Ceccarello", "Francesco  Silvestri"], "year": 2015, "n_citations": 8}
{"id": 5494763, "s2_id": "73afba1ff355a6e39f3811ad91198fafac55f3b7", "title": "Variants of Mersenne Twister Suitable for Graphic Processors", "abstract": "This article proposes a type of pseudorandom number generator, Mersenne Twister for Graphic Processor (MTGP), for efficient generation on graphic processessing units (GPUs). MTGP supports large state sizes such as 11213 bits, and uses the high parallelism of GPUs in computing many steps of the recursion in parallel. The second proposal is a parameter-set generator for MTGP, named MTGP Dynamic Creator (MTGPDC). MTGPDC creates up to 232 distinct parameter sets which generate sequences with high-dimensional uniformity. This facility is suitable for a large grid of GPUs where each GPU requires separate random number streams. MTGP is based on linear recursion over the two-element field, and has better high-dimensional equidistribution than the Mersenne Twister pseudorandom number generator.", "venue": "TOMS", "authors": ["Mutsuo  Saito", "Makoto  Matsumoto"], "year": 2013, "n_citations": 68}
{"id": 5495982, "s2_id": "a9496089892f58fd95925bd80a47a3fc158365dc", "title": "Solving dense generalized eigenproblems on multi-threaded architectures", "abstract": "We compare two approaches to compute a fraction of the spectrum of dense symmetric definite generalized eigenproblems: one is based on the reduction to tridiagonal form, and the other on the Krylov-subspace iteration. Two large-scale applications, arising in molecular dynamics and material science, are employed to investigate the contributions of the application, architecture, and parallelism of the method to the performance of the solvers. The experimental results on a state-of-the-art 8-core platform, equipped with a graphics processing unit (GPU), reveal that in realistic applications, iterative Krylov-subspace methods can be a competitive approach also for the solution of dense problems.", "venue": "Appl. Math. Comput.", "authors": ["Jos\u00e9 Ignacio Aliaga", "Paolo  Bientinesi", "Davor  Davidovic", "Edoardo Di Napoli", "Francisco D. Igual", "Enrique S. Quintana-Ort\u00ed"], "year": 2012, "n_citations": 10}
{"id": 5500471, "s2_id": "62c8ca1700138331a49dab894721662eda79d0c1", "title": "Tensor computations in computer algebra systems", "abstract": "This paper considers three types of tensor computations. On their basis, we attempt to formulate criteria that must be satisfied by a computer algebra system dealing with tensors. We briefly overview the current state of tensor computations in different computer algebra systems. The tensor computations are illustrated with appropriate examples implemented in specific systems: Cadabra and Maxima.", "venue": "Programming and Computer Software", "authors": ["Anna V. Korolkova", "Dmitry S. Kulyabov", "Leonid A. Sevastyanov"], "year": 2013, "n_citations": 14}
{"id": 5501670, "s2_id": "409676c2d9d5577db31333b97f0cdce8cf6eb2e3", "title": "Extending C++ for Heterogeneous Quantum-Classical Computing", "abstract": "We present qcor\u2014a language extension to C++ and compiler implementation that enables heterogeneous quantum-classical programming, compilation, and execution in a single-source context. Our work provides a first-of-its-kind C++ compiler enabling high-level quantum kernel (function) expression in a quantum-language agnostic manner, as well as a hardware-agnostic, retargetable compiler workflow targeting a number of physical and virtual quantum computing backends. qcor leverages novel Clang plugin interfaces and builds upon the XACC system-level quantum programming framework to provide a state-of-the-art integration mechanism for quantum-classical compilation that leverages the best from the community at-large. qcor translates quantum kernels ultimately to the XACC intermediate representation, and provides user-extensible hooks for quantum compilation routines like circuit optimization, analysis, and placement. This work details the overall architecture and compiler workflow for qcor, and provides a number of illuminating programming examples demonstrating its utility for near-term variational tasks, quantum algorithm expression, and feed-forward error correction schemes.", "venue": "ACM Transactions on Quantum Computing", "authors": ["Thien  Nguyen", "Anthony  Santana", "Tyler  Kharazi", "Daniel  Claudino", "Hal  Finkel", "Alexander  McCaskey"], "year": 2021, "n_citations": 9}
{"id": 5508701, "s2_id": "480d545ac4a4ffff5b1bc291c2de613192e35d91", "title": "DyNet: The Dynamic Neural Network Toolkit", "abstract": "We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at this http URL.", "venue": "ArXiv", "authors": ["Graham  Neubig", "Chris  Dyer", "Yoav  Goldberg", "Austin  Matthews", "Waleed  Ammar", "Antonios  Anastasopoulos", "Miguel  Ballesteros", "David  Chiang", "Daniel  Clothiaux", "Trevor  Cohn", "Kevin  Duh", "Manaal  Faruqui", "Cynthia  Gan", "Dan  Garrette", "Yangfeng  Ji", "Lingpeng  Kong", "Adhiguna  Kuncoro", "Gaurav  Kumar", "Chaitanya  Malaviya", "Paul  Michel", "Yusuke  Oda", "Matthew  Richardson", "Naomi  Saphra", "Swabha  Swayamdipta", "Pengcheng  Yin"], "year": 2017, "n_citations": 337}
{"id": 5513212, "s2_id": "ed6167f5aba1357fd7799015063e1f0396c3ea50", "title": "HONEI: A collection of libraries for numerical computations targeting multiple processor architectures", "abstract": "Abstract We present HONEI, an open-source collection of libraries offering a hardware oriented approach to numerical calculations. HONEI abstracts the hardware, and applications written on top of HONEI can be executed on a wide range of computer architectures such as CPUs, GPUs and the Cell processor. We demonstrate the flexibility and performance of our approach with two test applications, a Finite Element multigrid solver for the Poisson problem and a robust and fast simulation of shallow water waves. By linking against HONEI's libraries, we achieve a two-fold speedup over straight forward C++ code using HONEI's SSE backend, and additional 3\u20134 and 4\u201316 times faster execution on the Cell and a GPU. A second important aspect of our approach is that the full performance capabilities of the hardware under consideration can be exploited by adding optimised application-specific operations to the HONEI libraries. HONEI provides all necessary infrastructure for development and evaluation of such kernels, significantly simplifying their development. Program summary Program title: HONEI Catalogue identifier: AEDW_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEDW_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GPLv2 No. of lines in distributed program, including test data, etc.: 216\u2009180 No. of bytes in distributed program, including test data, etc.: 1\u2009270\u2009140 Distribution format: tar.gz Programming language: C++ Computer: x86, x86_64, NVIDIA CUDA GPUs, Cell blades and PlayStation 3 Operating system: Linux RAM: at least 500 MB free Classification: 4.8, 4.3, 6.1 External routines: SSE: none; [1] for GPU, [2] for Cell backend Nature of problem: Computational science in general and numerical simulation in particular have reached a turning point. The revolution developers are facing is not primarily driven by a change in (problem-specific) methodology, but rather by the fundamental paradigm shift of the underlying hardware towards heterogeneity and parallelism. This is particularly relevant for data-intensive problems stemming from discretisations with local support, such as finite differences, volumes and elements. Solution method: To address these issues, we present a hardware aware collection of libraries combining the advantages of modern software techniques and hardware oriented programming. Applications built on top of these libraries can be configured trivially to execute on CPUs, GPUs or the Cell processor. In order to evaluate the performance and accuracy of our approach, we provide two domain specific applications; a multigrid solver for the Poisson problem and a fully explicit solver for 2D shallow water equations. Restrictions: HONEI is actively being developed, and its feature list is continuously expanded. Not all combinations of operations and architectures might be supported in earlier versions of the code. Obtaining snapshots from http://www.honei.org is recommended. Unusual features: The considered applications as well as all library operations can be run on NVIDIA GPUs and the Cell BE. Running time: Depending on the application, and the input sizes. The Poisson solver executes in few seconds, while the SWE solver requires up to 5 minutes for large spatial discretisations or small timesteps. References: [1] http://www.nvidia.com/cuda . [2] http://www.ibm.com/developerworks/power/cell .", "venue": "Comput. Phys. Commun.", "authors": ["Danny van Dyk", "Markus  Geveler", "Sven  Mallach", "Dirk  Ribbrock", "Dominik  G\u00f6ddeke", "Carsten  Gutwenger"], "year": 2009, "n_citations": 18}
{"id": 5521863, "s2_id": "1772744792c35ddf3c2d70b2455c16049f75e2d2", "title": "Comparing OpenMP Implementations With Applications Across A64FX Platforms", "abstract": "The development of the A64FX processor by Fujitsu has created a massive innovation in High-Performance Computing and the birth of Fugaku: the current world\u2019s fastest supercomputer. A variety of tools are used to analyze the run-times and performances of several applications, and in particular, how these applications scale on the A64FX processor. We examine the performance and behavior of applications through OpenMP scaling and how their performance differs across different compilers both on the new Ookami cluster at Stony Brook University as well as the Fugaku supercomputer at RIKEN in Japan.", "venue": "IWOMP", "authors": ["Benjamin  Michalowicz", "Eric  Raut", "Yan  Kang", "Tony  Curtis", "Barbara  Chapman", "Dossay  Oryspayev"], "year": 2021, "n_citations": 0}
{"id": 5525670, "s2_id": "48a8b5ba454e616aee61000793b782042b8f9a32", "title": "Efficiently and easily integrating differential equations with JiTCODE, JiTCDDE, and JiTCSDE", "abstract": "We present a family of Python modules for the numerical integration of ordinary, delay, or stochastic differential equations. The key features are that the user enters the derivative symbolically and it is just-in-time-compiled, allowing the user to efficiently integrate differential equations from a higher-level interpreted language. The presented modules are particularly suited for large systems of differential equations such as those used to describe dynamics on complex networks. Through the selected method of input, the presented modules also allow almost complete automatization of the process of estimating regular as well as transversal Lyapunov exponents for ordinary and delay differential equations. We conceptually discuss the modules' design, analyze their performance, and demonstrate their capabilities by application to timely problems.", "venue": "Chaos", "authors": ["Gerrit  Ansmann"], "year": 2018, "n_citations": 32}
{"id": 5527964, "s2_id": "47ac10e4e9ddabaecec4bbc10bd77a4e60c7e941", "title": "A 55-line code for large-scale parallel topology optimization in 2D and 3D", "abstract": "This paper presents a 55-line code written in python for 2D and 3D topology optimization (TO) based on the open-source finite element computing software (FEniCS), equipped with various finite element tools and solvers. PETSc is used as the linear algebra back-end, which results in significantly less computational time than standard python libraries. The code is designed based on the popular solid isotropic material with penalization (SIMP) methodology. Extensions to multiple load cases, different boundary conditions, and incorporation of passive elements are also presented. Thus, this implementation is the most compact implementation of SIMP based topology optimization for 3D as well as 2D problems. \nUtilizing the concept of Euclidean distance matrix to vectorize the computation of the weight matrix for the filter, we have achieved a substantial reduction in the computational time and have also made it possible for the code to work with complex ground structure configurations. We have also presented the code's extension to large-scale topology optimization problems with support for parallel computations on complex structural configuration, which could help students and researchers explore novel insights into the TO problem with dense meshes. Appendix-A contains the complete code, and the website: \\url{this https URL} also contains the complete code.", "venue": "ArXiv", "authors": ["Abhinav  Gupta", "Rajib  Chowdhury", "Anupam  Chakrabarti", "Timon  Rabczuk"], "year": 2020, "n_citations": 0}
{"id": 5533703, "s2_id": "8811fed70ced554d95f3f870d6f13615d1df7a91", "title": "Some Notes on a Method for Proving Inequalities by Computer", "abstract": "In this article we consider mathematical fundamentals of one method for proving inequalities by computer, based on the Remez algorithm. Using the well-known results of undecidability of the existence of zeros of real elementary functions, we demonstrate that the considered method generally in practice becomes one heuristic for the verification of inequalities. We give some improvements of the inequalities considered in the theorems for which the existing proofs have been based on the numerical verifications of Remez algorithm.", "venue": "ArXiv", "authors": ["Branko J. Malesevic"], "year": 2007, "n_citations": 27}
{"id": 5542712, "s2_id": "24aa2f1004aab06000f24d5b650b891d6dc68818", "title": "Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning", "abstract": "Deep learning can achieve outstanding results in various fields. However, it requires so significant computational power that graphics processing units (GPUs) and/or numerous computers are often required for the practical application. We have developed a new distributed calculation framework called \"Sashimi\" that allows any computer to be used as a distribution node only by accessing a website. We have also developed a new JavaScript neural network framework called \"Sukiyaki\" that uses general purpose GPUs with web browsers. Sukiyaki performs 30 times faster than a conventional JavaScript library for deep convolutional neural networks (deep CNNs) learning. The combination of Sashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the distributed deep learning of deep CNNs only with web browsers on various devices. The libraries that comprise the proposed methods are available under MIT license at this http URL", "venue": "ArXiv", "authors": ["Ken  Miura", "Tatsuya  Harada"], "year": 2015, "n_citations": 11}
{"id": 5550864, "s2_id": "9ab7f55292033b592972c58f35fff63dc765fa76", "title": "Form Follows Function - Do algorithms and applications challenge or drag behind the hardware evolution?", "abstract": "We summarise some of the key statements made at the workshop Form Follows Function at ISC High Performance 2016. The summary highlights what type of co-design the presented projects experience; often in the absence of an explicit co-design agenda. Their software development picks up hardware trends but it also influences the hardware development. Observations illustrate that this cycle not always is optimal for both sides as it is not proactively steered. Key statements characterise ideas how it might be possible to integrate both hardware and software creation closer to the best of both worlds---again even without classic co-design in mind where new pieces of hardware are created. The workshop finally identified three development idioms that might help to improve software and system design with respect to emerging hardware.", "venue": "ArXiv", "authors": ["Tobias  Weinzierl"], "year": 2016, "n_citations": 0}
{"id": 5560458, "s2_id": "d85cdd48c8b321a1ed720ffdd05d5c99bb08a49b", "title": "PLANC: Parallel Low Rank Approximation with Non-negativity Constraints", "abstract": "We consider the problem of low-rank approximation of massive dense non-negative tensor data, for example to discover latent patterns in video and imaging applications. As the size of data sets grows, single workstations are hitting bottlenecks in both computation time and available memory. We propose a distributed-memory parallel computing solution to handle massive data sets, loading the input data across the memories of multiple nodes and performing efficient and scalable parallel algorithms to compute the low-rank approximation. We present a software package called PLANC (Parallel Low Rank Approximation with Non-negativity Constraints), which implements our solution and allows for extension in terms of data (dense or sparse, matrices or tensors of any order), algorithm (e.g., from multiplicative updating techniques to alternating direction method of multipliers), and architecture (we exploit GPUs to accelerate the computation in this work).We describe our parallel distributions and algorithms, which are careful to avoid unnecessary communication and computation, show how to extend the software to include new algorithms and/or constraints, and report efficiency and scalability results for both synthetic and real-world data sets.", "venue": "ACM Trans. Math. Softw.", "authors": ["Srinivas  Eswar", "Koby  Hayashi", "Grey  Ballard", "Ramakrishnan  Kannan", "Michael A. Matheson", "Haesun  Park"], "year": 2021, "n_citations": 6}
{"id": 5560724, "s2_id": "17afbdc3bf09d3ffcd11a5a8bfe076b8fe6fd47c", "title": "GHOST: Building Blocks for High Performance Sparse Linear Algebra on Heterogeneous Systems", "abstract": "While many of the architectural details of future exascale-class high performance computer systems are still a matter of intense research, there appears to be a general consensus that they will be strongly heterogeneous, featuring \u201cstandard\u201d as well as \u201caccelerated\u201d resources. Today, such resources are available as multicore processors, graphics processing units (GPUs), and other accelerators such as the Intel Xeon Phi. Any software infrastructure that claims usefulness for such environments must be able to meet their inherent challenges: massive multi-level parallelism, topology, asynchronicity, and abstraction. The \u201cGeneral, Hybrid, and Optimized Sparse Toolkit\u201d (GHOST) is a collection of building blocks that targets algorithms dealing with sparse matrix representations on current and future large-scale systems. It implements the \u201cMPI+X\u201d paradigm, has a pure C interface, and provides hybrid-parallel numerical kernels, intelligent resource management, and truly heterogeneous parallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We describe the details of its design with respect to the challenges posed by modern heterogeneous supercomputers and recent algorithmic developments. Implementation details which are indispensable for achieving high efficiency are pointed out and their necessity is justified by performance measurements or predictions based on performance models. We also provide instructions on how to make use of GHOST in existing software packages, together with a case study which demonstrates the applicability and performance of GHOST as a component within a larger software stack. The library code and several applications are available as open source.", "venue": "International Journal of Parallel Programming", "authors": ["Moritz  Kreutzer", "Jonas  Thies", "Melven  R\u00f6hrig-Z\u00f6llner", "Andreas  Pieper", "Faisal  Shahzad", "Martin  Galgon", "Achim  Basermann", "Holger  Fehske", "Georg  Hager", "Gerhard  Wellein"], "year": 2016, "n_citations": 38}
{"id": 5561087, "s2_id": "92d1b468a295a70c5044bec50f120819c2651707", "title": "SimpleTensor - a user-friendly Mathematica package for elementary tensor and differential-geometric calculations", "abstract": "In this paper we present a short overview of the new Wolfram Mathematica package intended for elementary \u201din-basis\u201d tensor and differential-geometric calculations. In contrast to alternatives our package is designed to be easy-to-use, short, all-purpose, and hackable. It supports tensor contractions using Einstein notation, transformations between different bases, tensor derivative operator, expansion in basis vectors and forms, exterior derivative, and interior product.", "venue": "ArXiv", "authors": ["D. O. Rybalka"], "year": 2021, "n_citations": 0}
{"id": 5561614, "s2_id": "1a33c3b39d2173f198897307ae8eb5629f0cad03", "title": "AbstractDifferentiation.jl: Backend-Agnostic Differentiable Programming in Julia", "abstract": "Differentiation.jl: Backend-Agnostic Differentiable Programming in Julia Frank Sch\u00e4fer Department of Physics University of Basel, Switzerland frank.schaefer@unibas.ch Mohamed Tarek Pumas-AI Inc., USA UNSW Canberra, Australia mohamed@pumas.ai Lyndon White Invenia Labs Cambridge, UK lyndon.white@invenialabs.co.uk Chris Rackauckas Massachusetts Institute of Technology, USA Julia Computing Inc., USA Pumas-AI Inc., USA crackauc@mit.edu", "venue": "ArXiv", "authors": ["Frank  Sch\u00e4fer", "Mohamed  Tarek", "Lyndon  White", "Christopher  Rackauckas"], "year": 2021, "n_citations": 0}
{"id": 5561760, "s2_id": "9e71770da4904e3c5c52fe3c9edc2e7e259c5256", "title": "MetaFEM: A Generic FEM Solver By Meta-expressions", "abstract": "Current multi-physics Finite Element Method (FEM) solvers are complex systems in terms of both their mathematical complexity and lines of code. This paper proposes a skeleton generic FEM solver, named MetaFEM, in total about 5,000 lines of Julia code, which translates generic input Partial Differential Equation (PDE) weak forms into corresponding GPU-accelerated simulations with a grammar similar to FEniCS or FreeFEM. Two novel approaches differentiate MetaFEM from the common solvers: (1) the FEM kernel is based on an original theory/algorithm which explicitly processes meta-expressions, as the name suggests, and (2) the symbolic engine is a rule-based Computer Algebra System (CAS), i.e., the equations are rewritten/derived according to a set of rewriting rules instead of going through completely fixed routines, supporting easy customization by developers. Example cases in thermal conduction, linear elasticity and incompressible flow are presented to demonstrate utility.", "venue": "ArXiv", "authors": ["Jiaxi  Xie", "Kornel  Ehmann", "Jian  Cao"], "year": 2021, "n_citations": 0}
{"id": 5562364, "s2_id": "fd577a059af091b00bd179e6c307dc3df0b2a33d", "title": "A Succinct Multivariate Lazy Multivariate Tower AD for Weil Algebra Computation", "abstract": "We propose a functional implementation of Multivariate Tower Automatic Differentiation. Our implementation is intended to be used in implementing C\u221e-structure computation of an arbitrary Weil algebra, which we discussed in [5].", "venue": "ArXiv", "authors": ["Hiromi  Ishii"], "year": 2021, "n_citations": 1}
{"id": 5564407, "s2_id": "88d3bf466e1947b6ec21502b0d32c4f8017fb57c", "title": "Glyph: Symbolic Regression Tools", "abstract": "We present Glyph - a Python package for genetic programming based symbolic regression. Glyph is designed for usage let by numerical simulations let by real world experiments. For experimentalists, glyph-remote provides a separation of tasks: a ZeroMQ interface splits the genetic programming optimization task from the evaluation of an experimental (or numerical) run. Glyph can be accessed at this http URL . Domain experts are be able to employ symbolic regression in their experiments with ease, even if they are not expert programmers. The reuse potential is kept high by a generic interface design. Glyph is available on PyPI and Github.", "venue": "Journal of Open Research Software", "authors": ["Markus  Quade", "Julien  Gout", "Markus  Abel"], "year": 2019, "n_citations": 5}
{"id": 5572923, "s2_id": "de3af797da2b3da8dafa14ec08beb4fd58315da1", "title": "Bayesian Paired-Comparison with the bpcs Package", "abstract": "This article introduces the bpcs R package (Bayesian Paired Comparison in Stan) and the statistical models implemented in the package. The goal of this package is to facilitate the use of Bayesian models for paired comparison data in behavioral research. Historically, studies on preferences have relied on Likert scale assessments and the frequentist approach to analyze the data. As an alternative, this article proposes the use of Bayesian models for forced choices assessments. The advantages of forced-choice assessments are that they minimize common bias that Likert scales are susceptible to, they can increase criterion validity, control for faking responses, and are quite useful to assess preferences in studies with children and non-humans primates. Bayesian data analyses are less sensitive to optional stopping, have better control of type I error, have stronger evidence towards the null hypothesis, allows propagation of uncertainties, includes prior information, and perform well when handling models with many parameters and latent variables. The bpcs package provides a consistent interface for R users and several functions to evaluate the posterior distribution of all parameters, to estimate the posterior distribution of any contest between items, and to obtain the posterior distribution of the ranks. Three reanalyses of recent studies that used the frequentist Bradley-Terry model are presented. These reanalyses are conducted with the Bayesian models of the bpcs package and all the code used to fit the models, generate the figures and the tables are available in the online appendix. Funding: This work was partially supported by the Wallenberg Artificial Intelligence, Autonomous Systems and Software Program (WASP) funded by Knut and Alice Wallenberg Foundation. The authors would like to thank L. Hopper for revising the results of the second reanalysis and G. Marton for revising the results of the third reanalysis. ar X iv :2 10 1. 11 22 7v 2 [ st at .M E ] 2 9 Ja n 20 21 BAYESIAN PAIRED-COMPARISON WITH THE BPCS PACKAGE 2", "venue": "ArXiv", "authors": ["David Issa Mattos", "\u00c9rika Martins Silva Ramos"], "year": 2021, "n_citations": 0}
{"id": 5573005, "s2_id": "8b0813d711ec5f8c92a4d8b9e8295b3900c709f9", "title": "Fast fully-reproducible serial/parallel Monte Carlo and MCMC simulations and visualizations via ParaMonte: : Python library", "abstract": "ParaMonte::Python (standing for Parallel Monte Carlo in Python) is a serial and MPI-parallelized library of (Markov Chain) Monte Carlo (MCMC) routines for sampling mathematical objective functions, in particular, the posterior distributions of parameters in Bayesian modeling and analysis in data science, Machine Learning, and scientific inference in general. In addition to providing access to fast high-performance serial/parallel Monte Carlo and MCMC sampling routines, the ParaMonte::Python library provides extensive post-processing and visualization tools that aim to automate and streamline the process of model calibration and uncertainty quantification in Bayesian data analysis. Furthermore, the automatically-enabled restart functionality of ParaMonte::Python samplers ensure seamless fully-deterministic into-the-future restart of Monte Carlo simulations, should any interruptions happen. The ParaMonte::Python library is MIT-licensed and is permanently maintained on GitHub at this https URL.", "venue": "ArXiv", "authors": ["Amir  Shahmoradi", "Fatemeh  Bagheri", "Joshua Alexander Osborne"], "year": 2020, "n_citations": 4}
{"id": 5573204, "s2_id": "42d433f64803f7af730b63753df8149aa19ce0a7", "title": "Arbitrary-precision computation of the gamma function", "abstract": "We discuss the best methods available for computing the gamma function \u0393(z) in arbitrary-precision arithmetic with rigorous error bounds. We address different cases: rational, algebraic, real or complex arguments; large or small arguments; low or high precision; with or without precomputation. The methods also cover the log-gamma function log \u0393(z), the digamma function \u03c8(z), and derivatives \u0393(n)(z) and \u03c8(n)(z). Besides attempting to summarize the existing state of the art, we present some new formulas, estimates, bounds and algorithmic improvements and discuss implementation results.", "venue": "ArXiv", "authors": ["Fredrik  Johansson"], "year": 2021, "n_citations": 2}
{"id": 5577768, "s2_id": "bd61d123125d624d0613c469d923de238b8ee4fb", "title": "P3DFFT: A Framework for Parallel Computations of Fourier Transforms in Three Dimensions", "abstract": "Fourier and related transforms are a family of algorithms widely employed in diverse areas of computational science, notoriously difficult to scale on high-performance parallel computers with a large number of processing elements (cores). This paper introduces a popular software package called P3DFFT which implements fast Fourier transforms (FFTs) in three dimensions in a highly efficient and scalable way. It overcomes a well-known scalability bottleneck of three-dimensional (3D) FFT implementations by using two-dimensional domain decomposition. Designed for portable performance, P3DFFT achieves excellent timings for a number of systems and problem sizes. On a Cray XT5 system P3DFFT attains 45% efficiency in weak scaling from 128 to 65,536 computational cores. Library features include Fourier and Chebyshev transforms, Fortran and C interfaces, in- and out-of-place transforms, uneven data grids, and single and double precision. P3DFFT is available as open source at http://code.google.com/p/p3dfft/. This pa...", "venue": "SIAM J. Sci. Comput.", "authors": ["Dmitry  Pekurovsky"], "year": 2012, "n_citations": 188}
{"id": 5587485, "s2_id": "f9b19dda75df93aad2188b2b92900212f4aa4c4c", "title": "Optimized Automatic Code Generation for Geometric Algebra Based Algorithms with Ray Tracing Application", "abstract": "Automatic code generation for low-dimensional geometric algorithms is capable of producing efficient low-level software code through a high-level geometric domain specific language. Geometric Algebra (GA) is one of the most suitable algebraic systems for being the base for such code generator. This work presents an attempt at realizing such idea in practice. A novel GA-based geometric code generator, called GMac, is proposed. Comparisons to similar GA-based code generators are provided. The possibility of fully benefiting from the symbolic power of GA while obtaining good performance and maintainability of software implementations is illustrated through a ray tracing application.", "venue": "ArXiv", "authors": ["Ahmad Hosney Awad Eid"], "year": 2016, "n_citations": 9}
{"id": 5588040, "s2_id": "c7e5b8d30a758d00ea17d5b2c0d89ec3538edc63", "title": "Matrix inversion using Cholesky decomposition", "abstract": "In this paper we present a method for matrix inversion based on Cholesky decomposition with reduced number of operations by avoiding computation of intermediate results; further, we use fixed point simulations to compare the numerical accuracy of the method.", "venue": "2013 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)", "authors": ["Aravindh  Krishnamoorthy", "Deepak  Menon"], "year": 2013, "n_citations": 115}
{"id": 5591504, "s2_id": "324abfe87bc349379a767dd8e177572da3e4a7f9", "title": "The NIFTY way of Bayesian signal inference", "abstract": "We introduce NIFTY, \"Numerical Information Field Theory\", a software package for the development of Bayesian signal inference algorithms that operate independently from any underlying spatial grid and its resolution. A large number of Bayesian and Maximum Entropy methods for 1D signal reconstruction, 2D imaging, as well as 3D tomography, appear formally similar, but one often finds individualized implementations that are neither flexible nor easily transferable. Signal inference in the framework of NIFTY can be done in an abstract way, such that algorithms, prototyped in 1D, can be applied to real world problems in higher-dimensional settings. NIFTY as a versatile library is applicable and already has been applied in 1D, 2D, 3D and spherical settings. A recent application is the D3PO algorithm targeting the non-trivial task of denoising, deconvolving, and decomposing photon observations in high energy astronomy.", "venue": "ArXiv", "authors": ["Marco  Selig"], "year": 2014, "n_citations": 1}
{"id": 5601856, "s2_id": "6c93216cef527875076339822bc087b6f3341538", "title": "Electronic geometry textbook: a geometric textbook knowledge management system", "abstract": "Electronic Geometry Textbook is a knowledge management system that manages geometric textbook knowledge to enable users to construct and share dynamic geometry textbooks interactively and efficiently. Based on a knowledge base organizing and storing the knowledge represented in specific languages, the system implements interfaces for maintaining the data representing that knowledge as well as relations among those data, for automatically generating readable documents for viewing or printing, and for automatically discovering the relations among knowledge data. An interface has been developed for users to create geometry textbooks with automatic checking, in real time, of the consistency of the structure of each resulting textbook. By integrating an external geometric theorem prover and an external dynamic geometry software package, the system offers the facilities for automatically proving theorems and generating dynamic figures in the created textbooks. This paper provides a comprehensive account of the current version of Electronic Geometry Textbook.", "venue": "AISC'10/MKM'10/Calculemus'10", "authors": ["Xiaoyu  Chen"], "year": 2010, "n_citations": 7}
{"id": 5601912, "s2_id": "080d7b8441a9429237981134a1d3d3e9de3232ac", "title": "Generalized Sampling in Julia", "abstract": "Generalized sampling is a numerically stable framework for obtaining reconstructions of signals in different bases and frames from their samples. For example, one can use wavelet bases for reconstruction given frequency measurements. In this paper, we will introduce a carefully documented toolbox for performing generalized sampling in Julia. Julia is a new language for technical computing with focus on performance, which is ideally suited to handle the large size problems often encountered in generalized sampling. The toolbox provides specialized solutions for the setup of Fourier bases and wavelets. The performance of the toolbox is compared to existing implementations of generalized sampling in MATLAB.", "venue": "ArXiv", "authors": ["Robert Dahl Jacobsen", "Morten  Nielsen", "Morten Grud Rasmussen"], "year": 2016, "n_citations": 3}
{"id": 5603548, "s2_id": "646f13f4c876ead591a02b6a6786234b98ca08dc", "title": "Optimisation of an FPGA Credit Default Swap engine by embracing dataflow techniques", "abstract": "Quantitative finance is the use of mathematical models to analyse financial markets and securities. Typically requiring significant amounts of computation, an important question is the role that novel architectures can play in accelerating these models in the future on HPC machines. In this paper we explore the optimisation of an existing, open source, FPGA based Credit Default Swap (CDS) engine using High Level Synthesis (HLS). Developed by Xilinx, and part of their open source Vitis libraries, the implementation of this engine currently favours flexibility and ease of integration over performance.We explore redesigning the engine to fully embrace the dataflow approach, ultimately resulting in an engine which is around eight times faster on an Alveo U280 FPGA than the original Xilinx library version. We then compare five of our engines on the U280 against a 24-core Xeon Platinum Cascade Lake CPU, outperforming the CPU by around 1.55 times, with the FPGA consuming 4.7 times less power and delivering around seven times the power efficiency of the CPU.", "venue": "2021 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Nick  Brown", "Mark  Klaisoongnoen", "Oliver Thomson Brown"], "year": 2021, "n_citations": 0}
{"id": 5606577, "s2_id": "6db557239b7a028faa953f43bfca42704152b682", "title": "Random Numbers in Scientific Computing: An Introduction", "abstract": "Random numbers play a crucial role in science and industry. Many numerical methods require the use of random numbers, in particular the Monte Carlo method. Therefore it is of paramount importance to have efficient random number generators. The differences, advantages and disadvantages of true and pseudo random number generators are discussed with an emphasis on the intrinsic details of modern and fast pseudo random number generators. Furthermore, standard tests to verify the quality of the random numbers produced by a given generator are outlined. Finally, standard scientific libraries with built-in generators are presented, as well as different approaches to generate nonuniform random numbers. Potential problems that one might encounter when using large parallel machines are discussed.", "venue": "ArXiv", "authors": ["Helmut G. Katzgraber"], "year": 2010, "n_citations": 14}
{"id": 5609386, "s2_id": "05725da13f94450436969c814459a425b628191b", "title": "SWiM -- A Semantic Wiki for Mathematical Knowledge Management", "abstract": "SWiM is a semantic wiki for collaboratively building, editing and browsing mathematical knowledge represented in the domain-specific structural semantic markup language OMDoc. It motivates users to contribute to collections of mathematical knowledge by instantly sharing the benefits of knowledge-powered services with them. SWiM is currently being used for authoring content dictionaries, i. e. collections of uniquely identified mathematical symbols, and prepared for managing a large-scale proof formalisation effort.", "venue": "ESWC", "authors": ["Christoph  Lange"], "year": 2008, "n_citations": 65}
{"id": 5614846, "s2_id": "71dc8f062ce2a5b8b10e6ffc188802193418a058", "title": "PyRDM: A Python-based library for automating the management and online publication of scientific software and data", "abstract": "The recomputability and reproducibility of results from scientific software requires access to both the source code and all associated input and output data. However, the full collection of these resources often does not accompany the key findings published in journal articles, thereby making it difficult or impossible for the wider scientific community to verify the correctness of a result or to build further research on it. This paper presents a new Python-based library, PyRDM, whose functionality aims to automate the process of sharing the software and data via online, citable repositories such as Figshare. The library is integrated into the workflow of an open-source computational fluid dynamics package, Fluidity, to demonstrate an example of its usage.", "venue": "ArXiv", "authors": ["Christian T. Jacobs", "Alexandros  Avdis", "Gerard  Gorman", "Matthew D. Piggott"], "year": 2014, "n_citations": 11}
{"id": 5617250, "s2_id": "46b45c868d6e5c795efcf82105d59f5066da06a4", "title": "FDApy: a Python package for functional data", "abstract": "We introduce the Python package, FDApy, as an implementation of functional data. This package provide modules for the analysis of such data. It includes classes for different dimensional data as well as irregularly sampled functional data. A simulation toolbox is also provided. It might be used to simulate different clusters of functional data. Some methodologies to handle these data are implemented, such as dimension reduction and clustering. New methods can be easily added. The package is publicly available on the Python Package Index and Github.", "venue": "ArXiv", "authors": ["Steven  Golovkine"], "year": 2021, "n_citations": 0}
{"id": 5617394, "s2_id": "fccbf3655c61c97c490e3fc94f9c3b18caa9f784", "title": "GPU-Based Parallel Integration of Large Numbers of Independent ODE Systems", "abstract": "The task of integrating a large number of independent ODE systems arises in various scientific and engineering areas. For nonstiff systems, common explicit integration algorithms can be used on GPUs, where individual GPU threads concurrently integrate independent ODEs with different initial conditions or parameters. One example is the fifth-order adaptive Runge\u2013Kutta\u2013Cash\u2013Karp (RKCK) algorithm. In the case of stiff ODEs, standard explicit algorithms require impractically small time-step sizes for stability reasons, and implicit algorithms are therefore commonly used instead to allow larger time steps and reduce the computational expense. However, typical high-order implicit algorithms based on backwards differentiation formulae (e.g., VODE, LSODE) involve complex logical flow that causes severe thread divergence when implemented on GPUs, limiting the performance. Therefore, alternate algorithms are needed. A GPU-based Runge\u2013Kutta\u2013Chebyshev (RKC) algorithm can handle moderate levels of stiffness and performs significantly faster than not only an equivalent CPU version but also a CPU-based implicit algorithm (VODE) based on results shown in the literature. In this chapter, we present the mathematical background, implementation details, and source code for the RKCK and RKC algorithms for use integrating large numbers of independent systems of ODEs on GPUs. In addition, brief performance comparisons are shown for each algorithm, demonstrating the potential benefit of moving to GPU-based ODE integrators.", "venue": "Numerical Computations with GPUs", "authors": ["Kyle E. Niemeyer", "Chih-Jen  Sung"], "year": 2014, "n_citations": 5}
{"id": 5622835, "s2_id": "a4f1aa0d44200982776186deacb39347dbc51995", "title": "TTC: A high-performance Compiler for Tensor Transpositions", "abstract": "We present TTC, an open-source parallel compiler for multidimensional tensor transpositions. In order to generate high-performance C++ code, TTC explores a number of optimizations, including software prefetching, blocking, loop-reordering, and explicit vectorization. To evaluate the performance of multidimensional transpositions across a range of possible use-cases, we also release a benchmark covering arbitrary transpositions of up to six dimensions. Performance results show that the routines generated by TTC achieve close to peak memory bandwidth on both the Intel Haswell and the AMD Steamroller architectures, and yield significant performance gains over modern compilers. By implementing a set of pruning heuristics, TTC allows users to limit the number of potential solutions; this option is especially useful when dealing with high-dimensional tensors, as the search space might become prohibitively large. Experiments indicate that when only 100 potential solutions are considered, the resulting performance is about 99% of that achieved with exhaustive search.", "venue": "ACM Trans. Math. Softw.", "authors": ["Paul  Springer", "Jeff R. Hammond", "Paolo  Bientinesi"], "year": 2017, "n_citations": 21}
{"id": 5623008, "s2_id": "43f01d002dab4377b944e0fd42ea6ec02587758e", "title": "Sparse Tensor Algebra as a Parallel Programming Model", "abstract": "Dense and sparse tensors allow the representation of most bulk data structures in computational science applications. We show that sparse tensor algebra can also be used to express many of the transformations on these datasets, especially those which are parallelizable. Tensor computations are a natural generalization of matrix and graph computations. We extend the usual basic operations of tensor summation and contraction to arbitrary functions, and further operations such as reductions and mapping. The expression of these transformations in a high-level sparse linear algebra domain specific language allows our framework to understand their properties at runtime to select the preferred communication-avoiding algorithm. To demonstrate the efficacy of our approach, we show how key graph algorithms as well as common numerical kernels can be succinctly expressed using our interface and provide performance results of a general library implementation.", "venue": "ArXiv", "authors": ["Edgar  Solomonik", "Torsten  Hoefler"], "year": 2015, "n_citations": 25}
{"id": 5623402, "s2_id": "cffbd23b5ae0b813bcd5f0890fd14aad6f38875e", "title": "The deal.II library, Version 9.2", "abstract": "Abstract This paper provides an overview of the new features of the finite element library deal.II, version 9.2.", "venue": "J. Num. Math.", "authors": ["Daniel  Arndt", "Wolfgang  Bangerth", "Bruno  Blais", "Thomas C. Clevenger", "Marc  Fehling", "Alexander V. Grayver", "Timo  Heister", "Luca  Heltai", "Martin  Kronbichler", "Matthias  Maier", "Peter  Munch", "Jean-Paul  Pelteret", "Reza  Rastak", "Ignacio  Tomas", "Bruno  Turcksin", "Zhuoran  Wang", "David  Wells"], "year": 2020, "n_citations": 57}
{"id": 5623839, "s2_id": "61dea99d49979962044b6b4eac1ff2ffeb352354", "title": "Testing performance with and without Block Low Rank Compression in MUMPS and the new PaStiX 6.0 for JOREK nonlinear MHD simulations", "abstract": "The interface to the MUMPS solver was updated in the JOREK MHD code to support Block Low Rank (BLR) compression and an interface to the new PaStiX solver version 6 has been implemented supporting BLR as well. First tests were carried out with JOREK, which solves a large sparse matrix system iteratively in each time step. For the preconditioning, a direct solver is applied in the code to sub-matrices, and at this point BLR was applied with the results being summarized in this report. For a simple case with a linearly growing mode, results with both solvers look promising with a considerable reduction of the memory consumption by several ten percent was obtained. A direct increase in performance was seen in particular configurations already. \nThe choice of the BLR accuracy parameter $\\epsilon$ proves to be critical in this simple test and also in more realistic simulations, which were carried out only with MUMPS due to the limited time available. The more realistic test showed an increase in run time when using BLR, which was mitigated when using larger values of $\\epsilon$. However, the GMRes iterative solver does not reach convergence anymore when $\\epsilon$ is too large, since the preconditioner becomes too inaccurate in that case. It is thus critical to use an $\\epsilon$ as large as possible, while still reaching convergence. More tests regarding this optimum will be necessary in the future. BLR can also lead to an indirect speed-up in particular cases, when the simulation can be run on a smaller number of compute nodes due to the reduced memory consumption.", "venue": "ArXiv", "authors": ["Richard  Nies", "Matthias  Hoelzl"], "year": 2019, "n_citations": 3}
{"id": 5632893, "s2_id": "f0c07b46fd7e3df74a9960f30665e17d62b32b52", "title": "Towards an Intelligent Tutor for Mathematical Proofs", "abstract": "Computer-supported learning is an increasingly important form of study since it allows for independent learning and individualized instruction. In this paper, we discuss a novel approach to developing an intelligent tutoring system for teaching textbook-style mathematical proofs. We characterize the particularities of the domain and discuss common ITS design models. Our approach is motivated by phenomena found in a corpus of tutorial dialogs that were collected in a Wizard-of-Oz experiment. We show how an intelligent tutor for textbook-style mathematical proofs can be built on top of an adapted assertion-level proof assistant by reusing representations and proof search strategies originally developed for automated and interactive theorem proving. The resulting prototype was successfully evaluated on a corpus of tutorial dialogs and yields good results.", "venue": "ThEdu", "authors": ["Serge  Autexier", "Dominik  Dietrich", "Marvin R. G. Schiller"], "year": 2011, "n_citations": 6}
{"id": 5633779, "s2_id": "932a46c9925e30b36cb13630c0c9bb1651681dca", "title": "Search Interfaces for Mathematicians", "abstract": "Access to mathematical knowledge has changed dramatically in recent years, therefore changing mathematical search practices. Our aim with this study is to scrutinize professional mathematicians\u2019 search behavior. With this understanding we want to be able to reason why mathematicians use which tool for what search problem in what phase of the search process. To gain these insights we conducted 24 repertory grid interviews with mathematically inclined people (ranging from senior professional mathematicians to non-mathematicians). From the interview data we elicited patterns for the user group \u201cmathematicians\u201d that can be applied when understanding design issues or creating new designs for mathematical search interfaces.", "venue": "CICM", "authors": ["Andrea  Kohlhase"], "year": 2014, "n_citations": 12}
{"id": 5635727, "s2_id": "2acef35aeaa784b76cd0f8e1fb87808c0f9e9204", "title": "Parallel Algorithms for Masked Sparse Matrix-Matrix Products", "abstract": "Computing the product of two sparse matrices (SpGEMM) is a fundamental operation in various combinatorial and graph algorithms as well as various bioinformatics and data analytics applications for computing inner-product similarities. For an important class of algorithms, only a subset of the output entries are needed, and the resulting operation is known as Masked SpGEMM since a subset of the output entries is considered to be \u201cmasked out\u201d. Existing algorithms for Masked SpGEMM usually do not consider mask as part of multiplication and either first compute a regular SpGEMM followed by masking, or perform a sparse inner product only for output elements that are not masked out. In this work, we investigate various novel algorithms and data structures for this rather challenging and important computation, and provide guidelines on how to design a fast Masked-SpGEMM for shared-memory architectures. Our evaluations show that factors such as matrix and mask density, mask structure and cache behavior play a vital role in attaining high performance for Masked SpGEMM. We evaluate our algorithms on a large number of real-world and synthetic matrices using several real-world benchmarks and show that our algorithms in most cases significantly outperform the state of the art for Masked SpGEMM implementations.", "venue": "ArXiv", "authors": ["Srdjan  Milakovi'c", "Oguz  Selvitopi", "Israt  Nisa", "Zoran  Budimli'c", "Aydin  Buluc"], "year": 2021, "n_citations": 0}
{"id": 5637439, "s2_id": "8916a932004a3ca720faf1445f4623d5d7f153db", "title": "The ensmallen library for flexible numerical optimization", "abstract": "We overview the ensmallen numerical optimization library, which provides a flexible C++ framework for mathematical optimization of user-supplied objective functions. Many types of objective functions are supported, including general, differentiable, separable, constrained, and categorical. A diverse set of pre-built optimizers is provided, including QuasiNewton optimizers and many variants of Stochastic Gradient Descent. The underlying framework facilitates the implementation of new optimizers. Optimization of an objective function typically requires supplying only one or two C++ functions. Custom behavior can be easily specified via callback functions. Empirical comparisons show that ensmallen outperforms other frameworks while providing more functionality. The library is available at https://ensmallen.org and is distributed under the permissive BSD license.", "venue": "ArXiv", "authors": ["Ryan R. Curtin", "Marcus  Edel", "Rahul Ganesh Prabhu", "Suryoday  Basak", "Zhihao  Lou", "Conrad  Sanderson"], "year": 2021, "n_citations": 2}
{"id": 5638689, "s2_id": "48a04268b08911b0dbe6165298d6a5c176b1f583", "title": "An in-place truncated fourier transform and applications to polynomial multiplication", "abstract": "The truncated Fourier transform (TFT) was introduced by van der Hoeven in 2004 as a means of smoothing the \"jumps\" in running time of the ordinary FFT algorithm that occur at power-of-two input sizes. However, the TFT still introduces these jumps in memory usage. We describe in-place variants of the forward and inverse TFT algorithms, achieving time complexity O(n log n) with only O(1) auxiliary space. As an application, we extend the second author's results on space-restricted FFT-based polynomial multiplication to polynomials of arbitrary degree.", "venue": "ISSAC", "authors": ["David  Harvey", "Daniel S. Roche"], "year": 2010, "n_citations": 27}
{"id": 5639281, "s2_id": "b7c7f92c4c4b08be3d8714222cd29879120c272c", "title": "Auto-Differentiating Linear Algebra", "abstract": "Development systems for deep learning, such as Theano, Torch, TensorFlow, or MXNet, are easy-to-use tools for creating complex neural network models. Since gradient computations are automatically baked in, and execution is mapped to high performance hardware, these models can be trained end-to-end on large amounts of data. However, it is currently not easy to implement many basic machine learning primitives in these systems (such as Gaussian processes, least squares estimation, principal components analysis, Kalman smoothing), mainly because they lack efficient support of linear algebra primitives as differentiable operators. We detail how a number of matrix decompositions (Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators. We have implemented these primitives in MXNet, running on CPU and GPU in single and double precision. We sketch use cases of these new operators, learning Gaussian process and Bayesian linear regression models. Our implementation is based on BLAS/LAPACK APIs, for which highly tuned implementations are available on all major CPUs and GPUs.", "venue": "ArXiv", "authors": ["Matthias W. Seeger", "Asmus  Hetzel", "Zhenwen  Dai", "Neil D. Lawrence"], "year": 2017, "n_citations": 19}
{"id": 5639647, "s2_id": "3826b200944e449c4f49d771288dc1e5c9a10355", "title": "An Implementation of the Bestvina\u2013Handel Algorithm for Surface Homeomorphisms", "abstract": "Bestvinaand Handel have introduced an effective algorithm that determines whether a given homeomorphism of an orientable, possibly'punctured surface is pseudo-Anosov. We present a Java software package that realizes this algorithm for surfaces with one puncture. It allows the user to define homeomorphisms in terms of Dehn twists, and in the pseudo-Anosov case it generates images of train tracks in the senseof Bestvina\u2013Han", "venue": "Exp. Math.", "authors": ["Peter  Brinkmann"], "year": 2000, "n_citations": 21}
{"id": 5642520, "s2_id": "cf4729eb4d1c7115060542b5743ea2b4cc95f44a", "title": "Developing numerical libraries in Java", "abstract": "The rapid and widespread adoption of Java has created a demand for reliable and reusable mathematical software components to support the growing number of compute-intensive applications now under development, particularly in science and engineering. In this paper we address practical issues of the Java language and environment which have an effect on numerical library design and development. Benchmarks which illustrate the current levels of performance of key numerical kernels on a variety of Java platforms are presented. Finally, a strategy for the development of a fundamental numerical toolkit for Java is proposed and its current status is described.", "venue": "Concurr. Pract. Exp.", "authors": ["Ronald F. Boisvert", "Jack J. Dongarra", "Roldan  Pozo", "Karin A. Remington", "G. W. Stewart"], "year": 1998, "n_citations": 43}
{"id": 5659941, "s2_id": "40be625643ad10d5db14dd1c9aa83ebddcd19195", "title": "ReLie: a Reduce program for Lie group analysis of differential equations", "abstract": "Lie symmetry analysis provides a general theoretical framework for investigating ordinary and partial differential equations. The theory is completely algorithmic even if it usually involves lengthy computations. For this reason, along the years many computer algebra packages have been developed to automate the computation. In this paper, we describe the program ReLie, written in the Computer Algebra System Reduce, since 2008 an open source program for all platforms. ReLie is able to perform almost automatically the needed computations for Lie symmetry analysis of differential equations. Its source code is freely available too. The use of the program is illustrated by means of some examples; nevertheless, it is to be underlined that it proves effective also for more complex computations where one has to deal with very large expressions.", "venue": "Symmetry", "authors": ["Francesco  Oliveri"], "year": 2021, "n_citations": 2}
{"id": 5661420, "s2_id": "34e552d576bbef99ee6834bbbed0a0b5159a2f8a", "title": "Fast computing of velocity field for flows in industrial burners and pumps", "abstract": "In this work we present a technique of fast numerical computation for solutions of Navier-Stokes equations in the case of flows of industrial interest. At first the partial differential equations are translated into a set of nonlinear ordinary differential equations using the geometrical shape of the domain where the flow is developing, then these ODEs are numerically resolved using a set of computations distributed among the available processors. We present some results from simulations on a parallel hardware architecture using native multithreads software and simulating a shared-memory or a distributed-memory environment.", "venue": "ArXiv", "authors": ["Gianluca  Argentini"], "year": 2007, "n_citations": 0}
{"id": 5670290, "s2_id": "ae64adf6fc9df5a3b24bd2c5152cda68323deb81", "title": "Modeling Deep Learning Accelerator Enabled GPUs", "abstract": "The efficacy of deep learning has resulted in its use in a growing number of applications. The Volta graphics processor unit (GPU) architecture from NVIDIA introduced a specialized functional unit, the \u201ctensor core\u201d, that helps meet the growing demand for higher performance for deep learning. In this paper we study the design of the tensor cores in NVIDIA's Volta and Turing architectures. We further propose an architectural model for the tensor cores in Volta. When implemented a GPU simulator, GPGPU-Sim, our tensor core model achieves 99.6% correlation versus an NVIDIA Titan V GPU in terms of average instructions per cycle when running tensor core enabled GEMM workloads. We also describe support added to enable GPGPU-Sim to run CUTLASS, an open-source CUDA C++ template library providing customizable GEMM templates that utilize tensor cores.", "venue": "2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)", "authors": ["Md Aamir Raihan", "Negar  Goli", "Tor M. Aamodt"], "year": 2019, "n_citations": 31}
{"id": 5671461, "s2_id": "4cd09415491b4f7942d3e7e2878d8b47d5081517", "title": "A fast exact simulation method for a class of Markov jump processes.", "abstract": "A new method of the stochastic simulation algorithm (SSA), named the Hashing-Leaping method (HLM), for exact simulations of a class of Markov jump processes, is presented in this paper. The HLM has a conditional constant computational cost per event, which is independent of the number of exponential clocks in the Markov process. The main idea of the HLM is to repeatedly implement a hash-table-like bucket sort algorithm for all times of occurrence covered by a time step with length \u03c4. This paper serves as an introduction to this new SSA method. We introduce the method, demonstrate its implementation, analyze its properties, and compare its performance with three other commonly used SSA methods in four examples. Our performance tests and CPU operation statistics show certain advantages of the HLM for large scale problems.", "venue": "The Journal of chemical physics", "authors": ["Yao  Li", "Lili  Hu"], "year": 2015, "n_citations": 8}
{"id": 5676131, "s2_id": "8004d970b52b13f92d646dfdaf310503ae538282", "title": "Softmax Optimizations for Intel Xeon Processor-based Platforms", "abstract": "Softmax is popular normalization method used in machine learning. Deep learning solutions like Transformer or BERT use the softmax function intensively, so it is worthwhile to optimize its performance. This article presents our methodology of optimization and its results applied to softmax. By presenting this methodology, we hope to increase an interest in deep learning optimizations for CPUs. We believe that the optimization process presented here could be transferred to other deep learning frameworks such as TensorFlow or PyTorch.", "venue": "ArXiv", "authors": ["Jacek  Czaja", "Michal  Gallus", "Tomasz  Patejko", "Jian  Tang"], "year": 2019, "n_citations": 3}
{"id": 5678493, "s2_id": "b63bdc0d33c91d0cef5da9e828fd75ed45e7c179", "title": "Isabelle/PIDE as Platform for Educational Tools", "abstract": "The Isabelle/PIDE platform addresses the question whether proof assistants of the LCF family are suitable as technological basis for educational tools. The traditionally strong logical foundations of systems like HOL, Coq, or Isabelle have so far been counter-balanced by somewhat inaccessible interaction via the TTY (or minor variations like the well-known Proof General / Emacs interface). Thus the fundamental question of math education tools with fully-formal background theories has often been answered negatively due to accidental weaknesses of existing proof engines. \nThe idea of \"PIDE\" (which means \"Prover IDE\") is to integrate existing provers like Isabelle into a larger environment, that facilitates access by end-users and other tools. We use Scala to expose the proof engine in ML to the JVM world, where many user-interfaces, editor frameworks, and educational tools already exist. This shall ultimately lead to combined mathematical assistants, where the logical engine is in the background, without obstructing the view on applications of formal methods, formalized mathematics, and math education in particular.", "venue": "ThEdu", "authors": ["Markus  Wenzel", "Burkhart  Wolff"], "year": 2011, "n_citations": 5}
{"id": 5685987, "s2_id": "cf6e343ec3512662d842ca3b31686ea4e4ace328", "title": "A compute-bound formulation of Galerkin model reduction for linear time-invariant dynamical systems", "abstract": "This work aims to advance computational methods for projection-based reduced order models (ROMs) of linear time-invariant (LTI) dynamical systems. For such systems, current practice relies on ROM formulations expressing the state as a rank-1 tensor (i.e., a vector), leading to computational kernels that are memory bandwidth bound and, therefore, ill-suited for scalable performance on modern many-core and hybrid computing nodes. This weakness can be particularly limiting when tackling many-query studies, where one needs to run a large number of simulations. This work introduces a reformulation, called rank-2 Galerkin, of the Galerkin ROM for LTI dynamical systems which converts the nature of the ROM problem from memory bandwidth to compute bound. We present the details of the formulation and its implementation, and demonstrate its utility through numerical experiments using, as a test case, the simulation of elastic seismic shear waves in an axisymmetric domain. We quantify and analyze performance and scaling results for varying numbers of threads and problem sizes. Finally, we present an end-to-end demonstration of using the rank-2 Galerkin ROM for a Monte Carlo sampling study. We show that the rank-2 Galerkin ROM is one order of magnitude more efficient than the rank-1 Galerkin ROM (the current practice) and about 970X more efficient than the full order model, while maintaining excellent accuracy in both the mean and statistics of the field.", "venue": "ArXiv", "authors": ["Francesco  Rizzi", "Eric J. Parish", "Patrick J. Blonigan", "John  Tencer"], "year": 2020, "n_citations": 0}
{"id": 5686626, "s2_id": "c3e507bc7f2040efa546f3f3ac96c8597991c998", "title": "Verifying an Algorithm Computing Discrete Vector Fields for Digital Imaging", "abstract": "In this paper, we present a formalization of an algorithm to construct admissible discrete vector fields in the Coq theorem prover taking advantage of the SSReflect library. Discrete vector fields are a tool which has been welcomed in the homological analysis of digital images since it provides a procedure to reduce the amount of information but preserving the homological properties. In particular, thanks to discrete vector fields, we are able to compute, inside Coq, homological properties of biomedical images which otherwise are out of the reach of this system.", "venue": "AISC/MKM/Calculemus", "authors": ["J\u00f3nathan  Heras", "Mar\u00eda  Poza", "Julio  Rubio"], "year": 2012, "n_citations": 9}
{"id": 5699082, "s2_id": "8c9424daf2baebb8b625815ea75a84db8847ae0b", "title": "A Parallel and Scalable Iterative Solver for Sequences of Dense Eigenproblems Arising in FLAPW", "abstract": "In one of the most important methods in Density Functional Theory \u2013 the Full-Potential Linearized Augmented Plane Wave (FLAPW) method \u2013 dense generalized eigenproblems are organized in long sequences. Moreover each eigenproblem is strongly correlated to the next one in the sequence. We propose a novel approach which exploits such correlation through the use of an eigensolver based on subspace iteration and accelerated with Chebyshev polynomials. The resulting solver, parallelized using the Elemental library framework, achieves excellent scalability and is competitive with current dense parallel eigensolvers.", "venue": "PPAM", "authors": ["Mario  Berljafa", "Edoardo Di Napoli"], "year": 2013, "n_citations": 2}
{"id": 5704450, "s2_id": "558f57f540f299d80f2e226afb572368b62e6b0f", "title": "Accelerating the computation of FLAPW methods on heterogeneous architectures", "abstract": "Legacy codes in computational science and engineering have been very successful in providing essential functionality to researchers. However, they are not capable of exploiting the massive parallelism provided by emerging heterogeneous architectures. The lack of portable performance and scalability puts them at high risk, ie, either they evolve or they are destined to be executed on older platforms and small clusters. One example of a legacy code which would heavily benefit from a modern redesign is FLEUR, a software for electronic structure calculations. In previous work, the computational bottleneck of FLEUR was partially reengineered to have a modular design that relies on standard building blocks, namely, BLAS and LAPACK libraries. In this paper, we demonstrate how the initial redesign enables the portability to heterogeneous architectures. More specifically, we study different approaches to port the code to architectures consisting of multi\u2010core CPUs equipped with one or more coprocessors such as Nvidia GPUs and Intel Xeon Phis. Our final code attains over 70% of the architectures' peak performance, and outperforms Nvidia's and Intel's libraries. On JURECA, the large tier\u20100 cluster where FLEUR is often executed, the code takes advantage of the full power of the computing nodes, attaining 5\u00d7 speedup over the sole use of the CPUs.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Davor  Davidovic", "Diego  Fabregat-Traver", "Markus  H\u00f6hnerbach", "Edoardo Di Napoli"], "year": 2018, "n_citations": 2}
{"id": 5708945, "s2_id": "51499d769963d7ec095c69b52ab8f56b2a1fbecf", "title": "DISROPT: a Python Framework for Distributed Optimization", "abstract": "In this paper we introduce DISROPT, a Python package for distributed optimization over networks. We focus on cooperative set-ups in which an optimization problem must be solved by peer-to-peer processors (without central coordinators) that have access only to partial knowledge of the entire problem. To reflect this, agents in DISROPT are modeled as entities that are initialized with their local knowledge of the problem. Agents then run local routines and communicate with each other to solve the global optimization problem. A simple syntax has been designed to allow for an easy modeling of the problems. The package comes with many distributed optimization algorithms that are already embedded. Moreover, the package provides full-fledged functionalities for communication and local computation, which can be used to design and implement new algorithms. DISROPT is available at github.com/disropt/disropt under the GPL license, with a complete documentation and many examples.", "venue": "ArXiv", "authors": ["Francesco  Farina", "Andrea  Camisa", "Andrea  Testa", "Ivano  Notarnicola", "Giuseppe  Notarstefano"], "year": 2019, "n_citations": 13}
{"id": 5710559, "s2_id": "1d9c849b369a130987431560cb8ae193adba7fed", "title": "A Low-Memory Time-Efficient Implementation of Outermorphisms for Higher-Dimensional Geometric Algebras", "abstract": "From the beginning of David Hestenes rediscovery of geometric algebra in the 1960s, outermorphisms have been a cornerstone in the mathematical development of GA. Many important mathematical formulations in GA can be expressed as outermorphisms such as versor products, linear projection operators, and mapping between related coordinate frames. Over the last two decades, GA-based mathematical models and software implementations have been developed in many fields of science and engineering. As such, efficient implementations of outermorphisms are of significant importance within this context. This work attempts to shed some light on the problem of optimizing software implementations of outermorphisms for practical prototyping applications using geometric algebra. The approach we propose here for implementing outermorphisms requires orders of magnitude less memory compared to other common approaches, while being comparable in time performance, especially for high-dimensional geometric algebras.", "venue": "ArXiv", "authors": ["Ahmad Hosny Eid"], "year": 2019, "n_citations": 0}
{"id": 5720196, "s2_id": "1c173f77379eaf08614d59b9a9deae9a6a514f6a", "title": "Using the VBARMS method in parallel computing", "abstract": "We introduce a new graph compression algorithm for block construction, which extends the method by Ashcraft and requires one simple to use parameter.We describe an improved block factorization computation in the VBARMS method.We conduct a parallel performance study of the VBARMS method using parallel graph partitioning.We solve turbulent three-dimensional Navier-Stokes equations on large realistic unstructured meshes. The paper describes an improved parallel MPI-based implementation of VBARMS, a variable block variant of the pARMS preconditioner proposed by Li et\u017aal. 200314 for solving general nonsymmetric linear systems. The parallel VBARMS solver can detect automatically exact or approximate dense structures in the linear system, and exploits this information to achieve improved reliability and increased throughput during the factorization. A novel graph compression algorithm is discussed that finds these approximate dense blocks structures and requires only one simple to use parameter. A complete study of the numerical and parallel performance of parallel VBARMS is presented for the analysis of large turbulent Navier-Stokes equations on a suite of three-dimensional test cases.", "venue": "Parallel Comput.", "authors": ["Bruno  Carpentieri", "Jia  Liao", "Masha  Sosonkina", "Aldo  Bonfiglioli", "Sven  Baars"], "year": 2016, "n_citations": 2}
{"id": 5721071, "s2_id": "42152143127a91c81b6d88280e6c638f2b332cf1", "title": "HeAT - a Distributed and GPU-accelerated Tensor Framework for Data Analytics", "abstract": "In order to cope with the exponential growth in available data, the efficiency of data analysis and machine learning libraries have recently received increased attention. Although corresponding array-based numerical kernels have been significantly improved, most are limited by the resources available on a single computational node. Consequently, kernels must exploit distributed resources, e.g., distributed memory architectures. To this end, we introduce HeAT, an array-based numerical programming framework for large-scale parallel processing with an easy-to-use NumPy-like API. HeAT utilizes PyTorch as a node-local eager execution engine and distributes the workload via MPI on arbitrarily large high-performance computing systems. It provides both low-level array-based computations, as well as assorted higher-level algorithms. With HeAT, it is possible for a NumPy user to take advantage of their available resources, significantly lowering the barrier to distributed data analysis. Compared with applications written in similar frameworks, HeAT achieves speedups of up to two orders of magnitude.", "venue": "ArXiv", "authors": ["Markus  Goetz", "Daniel  Coquelin", "Charlotte  Debus", "Kai  Krajsek", "Claudia  Comito", "Philipp  Knechtges", "Bj\u00f6rn  Hagemeier", "Michael  Tarnawa", "Simon  Hanselmann", "Martin  Siggel", "Achim  Basermann", "Achim  Streit"], "year": 2020, "n_citations": 4}
{"id": 5724071, "s2_id": "70e21808ca108d1fecf81d21266fdb88b5561947", "title": "A Novel Approach to Generate Correctly Rounded Math Libraries for New Floating Point Representations", "abstract": "Given the importance of floating-point~(FP) performance in numerous domains, several new variants of FP and its alternatives have been proposed (e.g., Bfloat16, TensorFloat32, and Posits). These representations do not have correctly rounded math libraries. Further, the use of existing FP libraries for these new representations can produce incorrect results. This paper proposes a novel methodology for generating polynomial approximations that can be used to implement correctly rounded math libraries. Existing methods produce polynomials that approximate the real value of an elementary function $f(x)$ and experience wrong results due to errors in the approximation and due to rounding errors in the implementation. In contrast, our approach generates polynomials that approximate the correctly rounded value of $f(x)$ (i.e., the value of $f(x)$ rounded to the target representation). This methodology provides more margin to identify efficient polynomials that produce correctly rounded results for all inputs. We frame the problem of generating efficient polynomials that produce correctly rounded results as a linear programming problem. Our approach guarantees that we produce the correct result even with range reduction techniques. Using our approach, we have developed correctly rounded, yet faster, implementations of elementary functions for multiple target representations. Our Bfloat16 library is 2.3$\\times$ faster than the corresponding state-of-the-art while producing correct results for all inputs.", "venue": "ArXiv", "authors": ["Jay P. Lim", "Mridul  Aanjaneya", "John  Gustafson", "Santosh  Nagarakatte"], "year": 2020, "n_citations": 9}
{"id": 5726729, "s2_id": "8b291aba1f8cd6fa7753e378e1b971869092f43f", "title": "IB2d: a Python and MATLAB implementation of the immersed boundary method", "abstract": "The development of fluid-structure interaction (FSI) software involves trade-offs between ease of use, generality, performance, and cost. Typically there are large learning curves when using low-level software to model the interaction of an elastic structure immersed in a uniform density fluid. Many existing codes are not publicly available, and the commercial software that exists usually requires expensive licenses and may not be as robust or allow the necessary flexibility that in house codes can provide. We present an open source immersed boundary software package, IB2d, with full implementations in both MATLAB and Python, that is capable of running a vast range of biomechanics models and is accessible to scientists who have experience in high-level programming environments. IB2d contains multiple options for constructing material properties of the fiber structure, as well as the advection-diffusion of a chemical gradient, muscle mechanics models, and artificial forcing to drive boundaries with a preferred motion.", "venue": "Bioinspiration & biomimetics", "authors": ["Nicholas A. Battista", "W. Christopher Strickland", "Laura A. Miller"], "year": 2017, "n_citations": 26}
{"id": 5727093, "s2_id": "785ff07930e859cf4d559ae34820f4e570e1c6b8", "title": "Efficient Distributed-Memory Parallel Matrix-Vector Multiplication with Wide or Tall Unstructured Sparse Matrices", "abstract": "This paper presents an efficient technique for matrix-vector and vector-transpose-matrix multiplication in distributed-memory parallel computing environments, where the matrices are unstructured, sparse, and have a substantially larger number of columns than rows or vice versa. Our method allows for parallel I/O, does not require extensive preprocessing, and has the same communication complexity as matrix-vector multiplies with column or row partitioning. Our implementation of the method uses MPI. We partition the matrix by individual nonzero elements, rather than by row or column, and use an \"overlapped\" vector representation that is matched to the matrix. The transpose multiplies use matrix-specific MPI communicators and reductions that we show can be set up in an efficient manner. The proposed technique achieves a good work per processor balance even if some of the columns are dense, while keeping communication costs relatively low.", "venue": "ArXiv", "authors": ["Jonathan  Eckstein", "Gyorgy  Matyasfalvi"], "year": 2018, "n_citations": 2}
{"id": 5743933, "s2_id": "5028aa53900529f1ae140a29c3a336d87cd9c71a", "title": "The Stochastic Processes Generation in OpenModelica", "abstract": "This paper studies program implementation problem of pseudo-random number generators in OpenModelica. We give an overview of generators of pseudo-random uniform distributed numbers. They are used as a basis for construction of generators of normal and Poisson distributions. The last step is the creation of Wiener and Poisson stochastic processes generators. We also describe the algorithm to call external C-functions from programs written in Modelica. This allows us to use random number generators implemented in the C language.", "venue": "ArXiv", "authors": ["M. N. Gevorkyan", "Anastasiya V. Demidova", "Anna V. Korolkova", "Dmitry S. Kulyabov", "Leonid A. Sevastianov"], "year": 2017, "n_citations": 1}
{"id": 5745217, "s2_id": "abbf9a75ef05b7cf97584f5ccd0c261ac75edf5a", "title": "Symbolic computation of weighted Moore-Penrose inverse using partitioning method", "abstract": "Abstract We propose a method and algorithm for computing the weighted Moore\u2013Penrose inverse of one-variable rational matrices. Continuing this idea, we develop an algorithm for computing the weighted Moore\u2013Penrose inverse of one-variable polynomial matrix. These methods and algorithms are generalizations of the method for computing the weighted Moore\u2013Penrose inverse for constant matrices, originated in Wang and Chen [G.R. Wang, Y.L. Chen, A recursive algorithm for computing the weighted Moore\u2013Penrose inverse A MN \u2020 , J. Comput. Math. 4 (1986) 74\u201385], and the partitioning method for computing the Moore\u2013Penrose inverse of rational and polynomial matrices introduced in Stanimirovic and Tasic [P.S. Stanimirovic, M.B. Tasic, Partitioning method for rational and polynomial matrices, Appl. Math. Comput. 155 (2004) 137\u2013163]. Algorithms are implemented in the symbolic computational package MATHEMATICA .", "venue": "Appl. Math. Comput.", "authors": ["Milan B.  Tasi\u0107", "Predrag S.  Stanimirovi\u0107", "Marko D.  Petkovi\u0107"], "year": 2007, "n_citations": 27}
{"id": 5755254, "s2_id": "336fa32c58e762ae416bbd5304eb20cda5a0eeb6", "title": "XAMG: A library for solving linear systems with multiple right-hand side vectors", "abstract": "Abstract This paper presents the XAMG library for solving large sparse systems of linear algebraic equations with multiple right-hand side vectors. The library specializes, but is not limited, to the solution of linear systems obtained from the discretization of elliptic differential equations. A corresponding set of numerical methods includes Krylov subspace, algebraic multigrid, Jacobi, Gauss\u2013Seidel, and Chebyshev iterative methods. The parallelization is implemented with MPI+POSIX shared memory hybrid programming model, which introduces a three-level hierarchical decomposition using the corresponding per-level synchronization and communication primitives. The code contains a number of optimizations, including the multilevel data segmentation, compression of indices, mixed-precision floating-point calculations, vector status flags, and others. The XAMG library uses the program code of the well-known hypre library to construct the multigrid matrix hierarchy. The XAMG\u2019s own implementation for the solve phase of the iterative methods provides up to a twofold speedup compared to hypre for the tests performed. Additionally, XAMG provides extended functionality to solve systems with multiple right-hand side vectors.", "venue": "SoftwareX", "authors": ["Boris  Krasnopolsky", "Alexey  Medvedev"], "year": 2021, "n_citations": 0}
{"id": 5762566, "s2_id": "83ceb698e399492368463985fcc886fd0d67cd35", "title": "Testing Fine-Grained Parallelism for the ADMM on a Factor-Graph", "abstract": "There is an ongoing effort to develop tools that apply distributed computational resources to tackle large problems or reduce the time to solve them. In this context, the Alternating Direction Method of Multipliers (ADMM) arises as a method that can exploit distributed resources like the dual ascent method and has the robustness and improved convergence of the augmented Lagrangian method. Traditional approaches to accelerate the ADMM using multiple cores are problem-specific and often require multi-core programming. By contrast, we propose a problem-independent scheme of accelerating the ADMM that does not require the user to write any parallel code. We show that this scheme, an interpretation of the ADMM as a message-passing algorithm on a factor-graph, can automatically exploit fine-grained parallelism both in GPUs and shared-memory multi-core computers and achieves significant speedup in such diverse application domains as combinatorial optimization, machine learning, and optimal control. Specifically, we obtain 10-18x speedup using a GPU, and 5-9x using multiple CPU cores, over a serial, optimized C-version of the ADMM, which is similar to the typical speedup reported for existing GPU-accelerated libraries, including cuFFT (19x), cuBLAS (17x), and cuRAND (8x).", "venue": "2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)", "authors": ["Ning  Hao", "AmirReza  Oghbaee", "Mohammad  Rostami", "Nate  Derbinsky", "Jos\u00e9  Bento"], "year": 2016, "n_citations": 12}
{"id": 5762619, "s2_id": "42932be0cd05635789efefd64d21138101bc28f5", "title": "New robust ScaLAPACK routine for computing the QR factorization with column pivoting", "abstract": "In this note we describe two modifications of the ScaLAPACK subroutines PxGEQPF for computing the QR factorization with the Businger-Golub column pivoting. First, we resolve a subtle numerical instability in the same way as we have done it for the LAPACK subroutines xGEQPF, xGEQP3 in 2006. [LAPACK Working Note 176 (2006); ACM Trans. Math. Softw. 2008]. The problem originates in the first release of LINPACK in the 1970's: due to severe cancellations in the down-dating of partial column norms, the pivoting procedure may be in the dark completely about the true norms of the pivot column candidates. This may cause miss-pivoting, and as a result loss of the important rank revealing structure of the computed triangular factor, with severe consequences on other solvers that rely on the rank revealing pivoting. The instability is so subtle that, e.g., inserting a WRITE statement or changing the process topology can drastically change the result. Secondly, we also correct a programming error in the complex subroutines PCGEQPF, PZGEQPF, which also causes wrong pivoting because of erroneous use of PSCNRM2, PDZNRM2 for the explicit norm computation.", "venue": "ArXiv", "authors": ["Zvonimir  Bujanovi\u0107", "Zlatko  Drma\u010d"], "year": 2019, "n_citations": 2}
{"id": 5787088, "s2_id": "212fa709e90bacc5520d675fde3d6f762ac48cc1", "title": "PCPATCH: software for the topological construction of multigrid relaxation methods", "abstract": "Effective relaxation methods are necessary for good multigrid convergence. For many equations, standard Jacobi and Gau{\\ss}-Seidel are inadequate, and more sophisticated space decompositions are required; examples include problems with semidefinite terms or saddle point structure. In this paper we present a unifying software abstraction, PCPATCH, for the topological construction of space decompositions for multigrid relaxation methods. Space decompositions are specified by collecting topological entities in a mesh (such as all vertices or facets) and applying a construction rule (such as taking all degrees of freedom in the cells around each entity). The software is implemented in PETSc and facilitates the elegant expression of a wide range of schemes merely by varying solver options at runtime. In turn, this allows for the very rapid development of fast solvers for difficult problems.", "venue": "ACM Trans. Math. Softw.", "authors": ["Patrick E. Farrell", "Matthew G. Knepley", "Florian  Wechsung", "Lawrence  Mitchell"], "year": 2021, "n_citations": 25}
{"id": 5787648, "s2_id": "62abc4b7d9751769a9a72df96829fcad7a2e3fe1", "title": "Speculative segmented sum for sparse matrix-vector multiplication on heterogeneous processors", "abstract": "Sparse matrix-vector multiplication (SpMV) is a central building block for scientific software and graph applications. Recently, heterogeneous processors composed of different types of cores attracted much attention because of their flexible core configuration and high energy efficiency. In this paper, we propose a compressed sparse row (CSR) format based SpMV algorithm utilizing both types of cores in a CPU-GPU heterogeneous processor. We first speculatively execute segmented sum operations on the GPU part of a heterogeneous processor and generate a possibly incorrect results. Then the CPU part of the same chip is triggered to re-arrange the predicted partial sums for a correct resulting vector. On three heterogeneous processors from Intel, AMD and nVidia, using 20 sparse matrices as a benchmark suite, the experimental results show that our method obtains significant performance improvement over the best existing CSR-based SpMV algorithms. The source code of this work is downloadable at this https URL", "venue": "Parallel Comput.", "authors": ["Weifeng  Liu", "Brian  Vinter"], "year": 2015, "n_citations": 47}
{"id": 5789373, "s2_id": "3a35b3fd06786e91193f2eb4b21888535d42927d", "title": "AMGCL: an Efficient, Flexible, and Extensible Algebraic Multigrid Implementation", "abstract": "The paper presents AMGCL -- an opensource C++ library implementing the algebraic multigrid method (AMG) for solution of large sparse linear systems of equations, usually arising from discretization of partial differential equations on an unstructured grid. The library supports both shared and distributed memory computation, allows to utilize modern massively parallel processors via OpenMP, OpenCL, or CUDA technologies, has minimal dependencies, and is easily extensible. The design principles behind AMGCL are discussed and it is shown that the code performance is on par with alternative implementations.", "venue": "Lobachevskii Journal of Mathematics", "authors": ["Denis  Demidov"], "year": 2019, "n_citations": 37}
{"id": 5789754, "s2_id": "124f9de8b70aad67fb8edc9e22007f567ea13e63", "title": "FlexRiLoG\u2014A SageMath Package for Motions of Graphs", "abstract": "In this paper we present the SageMath package FlexRiLoG (short for flexible and rigid labelings of graphs). Based on recent results the software generates motions of graphs using special edge colorings. The package computes and illustrates the colorings and the motions. We present the structure and usage of the package.", "venue": "ICMS", "authors": ["Georg  Grasegger", "Jan  Legersk'y"], "year": 2020, "n_citations": 3}
{"id": 5790632, "s2_id": "54e76730187f8bc7c7c3ac9ed0bd5d4ab9dcfc2e", "title": "Generalized eigen, singular value, and partial least squares decompositions: The GSVD package", "abstract": "The generalized singular value decomposition (GSVD, a.k.a. \"SVD triplet\", \"duality diagram\" approach) provides a unified strategy and basis to perform nearly all of the most common multivariate analyses (e.g., principal components, correspondence analysis, multidimensional scaling, canonical correlation, partial least squares). Though the GSVD is ubiquitous, powerful, and flexible, it has very few implementations. Here I introduce the GSVD package for R. The general goal of GSVD is to provide a small set of accessible functions to perform the GSVD and two other related decompositions (generalized eigenvalue decomposition, generalized partial least squares-singular value decomposition). Furthermore, GSVD helps provide a more unified conceptual approach and nomenclature to many techniques. I first introduce the concept of the GSVD, followed by a formal definition of the generalized decompositions. Next I provide some key decisions made during development, and then a number of examples of how to use GSVD to implement various statistical techniques. These examples also illustrate one of the goals of GSVD: how others can (or should) build analysis packages that depend on GSVD. Finally, I discuss the possible future of GSVD.", "venue": "ArXiv", "authors": ["Derek  Beaton"], "year": 2020, "n_citations": 0}
{"id": 5799319, "s2_id": "1f72199df398bb8fd090ea5c3254d2d95a3dcf4d", "title": "Technique detection software for Sparse Matrices", "abstract": "Sparse storage formats are techniques for storing and processing the sparse matrix data efficiently. The performance of these storage formats depend upon the distribution of non-zeros, within the matrix in different dimensions. In order to have better results we need a technique that suits best the organization of data in a particular matrix. So the decision of selecting a better technique is the main step towards improving the system's results otherwise the efficiency can be decreased. The purpose of this research is to help identify the best storage format in case of reduced storage size and high processing efficiency for a sparse matrix.", "venue": "ArXiv", "authors": ["Muhammad Taimoor Khan", "Anila  Usman"], "year": 2012, "n_citations": 1}
{"id": 5804305, "s2_id": "d5f3daa0e8aa1e49a7b7de667b523b74127a2923", "title": "The Implementation of the Colored Abstract Simplicial Complex and Its Application to Mesh Generation", "abstract": "We introduce the Colored Abstract Simplicial Complex library (CASC): a new, modern, and header-only C++ library that provides a data structure to represent arbitrary dimension abstract simplicial complexes (ASC) with user-defined classes stored directly on the simplices at each dimension. This is accomplished by using the latest C++ language features including variadic template parameters introduced in C++11 and automatic function return type deduction from C++14. Effectively, CASC decouples the representation of the topology from the interactions of user data. We present the innovations and design principles of the data structure and related algorithms. This includes a metadata-aware decimation algorithm, which is general for collapsing simplices of any dimension. We also present an example application of this library to represent an orientable surface mesh.", "venue": "ACM Trans. Math. Softw.", "authors": ["Christopher T. Lee", "John B. Moody", "Rommie E. Amaro", "James Andrew McCammon", "Michael J. Holst"], "year": 2019, "n_citations": 6}
{"id": 5815134, "s2_id": "0c6a7df269f895cbc5c24f7b384c1f0b78de545e", "title": "Algorithmic Differentiation of Linear Algebra Functions with Application in Optimum Experimental Design (Extended Version)", "abstract": "We derive algorithms for higher order derivative computation of the rectangular QR and eigenvalue decomposition of symmetric matrices with distinct eigenvalues in the forward and reverse mode of algorithmic differentiation (AD) using univariate Taylor propagation of matrices (UTPM). Linear algebra functions are regarded as elementary functions and not as algorithms. The presented algorithms are implemented in the BSD licensed AD tool ALGOPY. Numerical tests show that the UTPM algorithms derived in this paper produce results close to machine precision accuracy. The theory developed in this paper is applied to compute the gradient of an objective function motivated from optimum experimental design:", "venue": "ArXiv", "authors": ["Sebastian F. Walter", "Lutz  Lehmann"], "year": 2010, "n_citations": 7}
{"id": 5822535, "s2_id": "8c51cd04be72721166c44f65279f9b9bebf423d1", "title": "Parallel Sparse Matrix-Matrix Multiplication and Indexing: Implementation and Experiments", "abstract": "Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. Here we show that SpGEMM also yields efficient algorithms for general sparse-matrix indexing in distributed memory, provided that the underlying SpGEMM implementation is sufficiently flexible and scalable. We demonstrate that our parallel SpGEMM methods, which use two-dimensional block data distributions with serial hypersparse kernels, are indeed highly flexible, scalable, and memory-efficient in the general case. This algorithm is the first to yield increasing speedup on an unbounded number of processors; our experiments show scaling up to thousands of processors in a variety of test scenarios.", "venue": "SIAM J. Sci. Comput.", "authors": ["Aydin  Bulu\u00e7", "John R. Gilbert"], "year": 2012, "n_citations": 164}
{"id": 5823165, "s2_id": "2e655f2e5a781b1d077e1b92dcd6763cdd2064be", "title": "ADF95: Tool for automatic differentiation of a FORTRAN code designed for large numbers of independent variables", "abstract": "ADF95 is a tool to automatically calculate numerical first derivatives for any mathematical expression as a function of user defined independent variables. Accuracy of derivatives is achieved within machine precision. ADF95 may be applied to any FORTRAN 77/90/95 conforming code and requires minimal changes by the user. It provides a new derived data type that holds the value and derivatives and applies forward differencing by overloading all FORTRAN operators and intrinsic functions. An efficient indexing technique leads to a reduced memory usage and a substantially increased performance gain over other available tools with operator overloading. This gain is especially pronounced for sparse systems with large number of independent variables. A wide class of numerical simulations, e.g., those employing implicit solvers, can profit from ADF95.", "venue": "Comput. Phys. Commun.", "authors": ["Christian W. Straka"], "year": 2005, "n_citations": 23}
{"id": 5825690, "s2_id": "6175eb89ecf750c38bd650f295aaa10db63e6e9b", "title": "RawArray: A Simple, Fast, and Extensible Archival Format for Numeric Data", "abstract": "Raw data sizes are growing and proliferating in scientific research, driven by the success of data-hungry computational methods, such as machine learning. The preponderance of proprietary and shoehorned data formats make computations slower and make it harder to reproduce research and to port methods to new platforms. Here we present the RawArray format: a simple, fast, and extensible format for archival storage of multidimensional numeric arrays on disk. The RawArray file format is a simple concatenation of a header array and a data array. The header comprises seven or more 64-bit unsigned integers. The array data can be anything. Arbitrary user metadata can be appended to an RawArray file if desired, for example to store measurement details, color palettes, or geolocation data. We present benchmarks showing a factor of 2\u20133\u00d7 speedup over HDF5 for a range of array sizes and a speedup of up to 20\u00d7 in reading the common deep learning datasets MNIST and CIFAR10.", "venue": "ArXiv", "authors": ["David S. Smith"], "year": 2021, "n_citations": 0}
{"id": 5831017, "s2_id": "2ac01dd567cdb6397d9dc2dfd1d296bcc7a90975", "title": "A Hierarchically Blocked Jacobi SVD Algorithm for Single and Multiple Graphics Processing Units", "abstract": "We present a hierarchically blocked one-sided Jacobi algorithm for the singular value decomposition (SVD), targeting both single and multiple graphics processing units (GPUs). The blocking structure reflects the levels of the GPUs' memory hierarchy. The algorithm may outperform MAGMA's \\textttdgesvd, while retaining high relative accuracy. To this end, we developed a family of parallel pivot strategies on the GPU's shared address space, but applicable also to inter-GPU communication. Unlike common hybrid approaches, our algorithm in a single-GPU setting needs a CPU for the controlling purposes only, while utilizing the GPU's resources to the fullest extent permitted by the hardware. When required by the problem size, the algorithm, in principle, scales to an arbitrary number of GPU nodes. The scalability is demonstrated by more than twofold speedup for sufficiently large matrices on a Tesla S2050 system with four GPUs versus a single Fermi card.", "venue": "SIAM J. Sci. Comput.", "authors": ["Vedran  Novakovic"], "year": 2015, "n_citations": 9}
{"id": 5831499, "s2_id": "0f60ec196351876b8464d196fe71319b7a291315", "title": "Update report: LEO-II version 1.5", "abstract": "Recent improvements of the LEO-II theorem prover are presented. These improvements include a revised ATP interface, new translations into first-order logic, rule support for the axiom of choice, detection of defined equality, and more flexible strategy scheduling.", "venue": "ArXiv", "authors": ["Christoph  Benzm\u00fcller", "Nik  Sultana"], "year": 2013, "n_citations": 1}
{"id": 5837216, "s2_id": "beb325a509ca9fce6fa98c0cb78a1e3df0b86cfd", "title": "The Method of Gauss-Newton to Compute Power Series Solutions of Polynomial Homotopies", "abstract": "Abstract We consider the extension of the method of Gauss\u2013Newton from complex floating-point arithmetic to the field of truncated power series with complex floating-point coefficients. With linearization we formulate a linear system where the coefficient matrix is a series with matrix coefficients, and provide a characterization for when the matrix series is regular based on the algebraic variety of an augmented system. The structure of the linear system leads to a block triangular system. In the regular case, solving the linear system is equivalent to solving a Hermite interpolation problem. We show that this solution has cost cubic in the problem size. In general, at singular points, we rely on methods of tropical algebraic geometry to compute Puiseux series. With a few illustrative examples, we demonstrate the application to polynomial homotopy continuation.", "venue": "ArXiv", "authors": ["Nathan  Bliss", "Jan  Verschelde"], "year": 2016, "n_citations": 5}
{"id": 5839069, "s2_id": "8302b5ac3b0bed3eaf68bdedf39e38564ccddaaf", "title": "decimalInfinite: All Decimals In Bits, No Loss, Same Order, Simple", "abstract": "This paper introduces a binary encoding that supports arbitrarily large, small and precise decimals. It completely preserves information and order. It does not rely on any arbitrary use-case-based choice of calibration and is readily implementable and usable, as is. Finally, it is also simple to explain and understand.", "venue": "ArXiv", "authors": ["Ghislain  Fourny"], "year": 2015, "n_citations": 0}
{"id": 5854967, "s2_id": "06c9cfed1ce135d80679ea7330b760b66b6bbeff", "title": "Finite Element Integration on GPUs", "abstract": "We present a novel finite element integration method for low-order elements on GPUs. We achieve more than 100GF for element integration on first order discretizations of both the Laplacian and Elasticity operators on an NVIDIA GTX285, which has a nominal single precision peak flop rate of 1 TF/s and bandwidth of 159 GB/s, corresponding to a bandwidth limited peak of 40 GF/s.", "venue": "TOMS", "authors": ["Matthew G. Knepley", "Andy R. Terrel"], "year": 2013, "n_citations": 35}
{"id": 5858708, "s2_id": "155fac03679b2e71db877f66839a719fbb86c945", "title": "Some remarks on the performance of Matlab, Python and Octave in simulating dynamical systems", "abstract": "Matlab has been considered as a leader computational platform for many engineering fields. Well documented and reliable, Matlab presents as a great advantage its ability to increase the user productivity. However, Python and Octave are among some of the languages that have challenged Matlab. Octave and Python are well known examples of high-level scripting languages, with a great advantage of being open source software. The novelty of this paper is devoted to offer a comparison among these tree languages in the simulation of dynamical systems. We have applied the lower bound error to estimate the error of simulation. The comparison was performed with the chaotic systems Duffing-Ueda oscillator and the Chua's circuit, both identified with polynomial NARMAX. Octave presents the best reliable outcome. Nevertheless, Matlab needs the lowest time to undertake the same activity. Python has presented the worse result for the stop simulation criterion.", "venue": "Anais do 14\u00ba Simp\u00f3sio Brasileiro de Automa\u00e7\u00e3o Inteligente", "authors": ["P. F. S. Guedes", "E. G. Nepomuceno"], "year": 2019, "n_citations": 2}
{"id": 5869645, "s2_id": "cf2c88b428e60542e014a146f8babffdfae7c0d5", "title": "Model-guided performance analysis of the sparse matrix-matrix multiplication", "abstract": "Achieving high efficiency with numerical kernels for sparse matrices is of utmost importance, since they are part of many simulation codes and tend to use most of the available compute time and resources. In addition, especially in large scale simulation frameworks the readability and ease of use of mathematical expressions are essential components for the continuous maintenance, modification, and extension of software. In this context, the sparse matrix-matrix multiplication is of special interest. In this paper we thoroughly analyze the single-core performance of sparse matrix-matrix multiplication kernels in the Blaze Smart Expression Template (SET) framework. We develop simple models for estimating the achievable maximum performance, and use them to assess the efficiency of our implementations. Additionally, we compare these kernels with several commonly used SET-based C++ libraries, which, just as Blaze, aim at combining the requirements of high performance with an elegant user interface. For the different sparse matrix structures considered here, we show that our implementations are competitive or faster than those of the other SET libraries for most problem sizes on a current Intel multicore processor.", "venue": "2013 International Conference on High Performance Computing & Simulation (HPCS)", "authors": ["Tobias  Scharpff", "Klaus  Iglberger", "Georg  Hager", "Ulrich  R\u00fcde"], "year": 2013, "n_citations": 1}
{"id": 5872055, "s2_id": "a246c935b4a5c0085128a25387a4c95deec9d71d", "title": "A data porting tool for coupling models with different discretization needs", "abstract": "The presented work is part of a larger research program dealing with developing tools for coupling biogeochemical models in contaminated landscapes. The specific objective of this article is to provide researchers with a data porting tool to build hexagonal raster using information from a rectangular raster data (e.g. GIS format). This tool involves a computational algorithm and an open source software (written in C). The method of extending the reticulated functions defined on 2D networks is an essential key of this algorithm and can also be used for other purposes than data porting. The algorithm allows one to build the hexagonal raster with a cell size independent from the geometry of the rectangular raster. The extended function is a bi-cubic spline which can exactly reconstruct polynomials up to degree three in each variable. We validate the method by analyzing errors in some theoretical case studies followed by other studies with real terrain elevation data. We also introduce and briefly present an iterative water routing method and use it for validation on a case with concrete terrain data. We provide a method to resample piecewise constant functions.The method recovers cubic polynomials and doesn't introduce spurious oscillations.We construct a hexagonal cellular automaton model of flow routing.The water flows from a cell to the neighbors pointed by the water velocity.The velocity aligns to the projection of gravitational force on the water surface.", "venue": "Environ. Model. Softw.", "authors": ["Stelian  Ion", "Dorin  Marinescu", "Stefan-Gicu  Cruceanu", "Virgil  Iordache"], "year": 2014, "n_citations": 9}
{"id": 5872733, "s2_id": "04e04f0583d65cbcac3c686ef5f798afe89ec386", "title": "pySDC - Prototyping spectral deferred corrections", "abstract": "In this paper we present the Python framework pySDC for solving collocation problems with spectral deferred correction methods (SDC) and their time-parallel variant PFASST, the parallel full approximation scheme in space and time. pySDC features many implementations of SDC and PFASST, from simple implicit time-stepping to high-order implicit-explicit or multi-implicit splitting and multi-level spectral deferred corrections. It comes with many different, pre-implemented examples and has seven tutorials to help new users with their first steps. Time-parallelism is implemented either in an emulated way for debugging and prototyping as well as using MPI for benchmarking. The code is fully documented and tested using continuous integration, including most results of previous publications. Here, we describe the structure of the code by taking two different perspectives: the user's and the developer's perspective. While the first sheds light on the front-end, the examples and the tutorials, the second is used to describe the underlying implementation and the data structures. We show three different examples to highlight various aspects of the implementation, the capabilities and the usage of pySDC. Also, couplings to the FEniCS framework and PETSc, the latter including spatial parallelism with MPI, are described.", "venue": "ArXiv", "authors": ["Robert  Speck"], "year": 2018, "n_citations": 0}
{"id": 5874982, "s2_id": "b31d8f3989f4feddec5b390d2b85d8e069b1c2d1", "title": "A Left-Looking Selected Inversion Algorithm and Task Parallelism on Shared Memory Systems", "abstract": "Given a sparse matrix A, the selected inversion algorithm is an efficient method for computing certain selected elements of A-1. These selected elements correspond to all or some nonzero elements of the LU factors of A. In many ways, the types of matrix updates performed in the selected inversion algorithm are similar to those performed in the LU factorization, although the sequence of operations is different. In the context of LU factorization, it is known that the left-looking and right-looking algorithms exhibit different memory access and data communication patterns, and hence different behavior on shared memory and distributed memory parallel machines. Corresponding to right-looking and left-looking LU factorization, the selected inversion algorithm can be organized as a left-looking or a right-looking algorithm. The parallel right-looking version of the algorithm has been developed in [9]. The sequence of operations performed in this version of the selected inversion algorithm is similar to those performed in a left-looking LU factorization algorithm. In this paper, we describe the left-looking variant of the selected inversion algorithm, and present an efficient implementation of the algorithm for shared memory machines using a task parallel method. We demonstrate that with the task scheduling features provided by OpenMP 4.0, the left-looking selected inversion algorithm can scale well both on the Intel Haswell multicore architecture and on the Intel Knights Landing (KNL) manycore architecture up to 16 and 64 cores, respectively. On the KNL architecture, we observe that the maximum parallel efficiency achieved by the left-looking selected inversion algorithm can be as high as 62% even when all 64 cores are used, despite the inherent asynchronous nature of the computation and communication patterns in sparse matrix operations. Compared to the right-looking selected inversion algorithm, the left-looking formulation facilitates efficient pipelining of operations along different branches of the elimination tree, and can be a promising candidate for future development of massively parallel selected inversion algorithms on heterogeneous architectures.", "venue": "HPC Asia", "authors": ["Mathias  Jacquelin", "Lin  Lin", "Weile  Jia", "Yonghua  Zhao", "Chao  Yang"], "year": 2018, "n_citations": 2}
{"id": 5878393, "s2_id": "d0df4417d347f6d932f4e551925c7293dea675ed", "title": "Efficient computation of Laguerre polynomials", "abstract": "An efficient algorithm and a Fortran 90 module (LaguerrePol) for computing Laguerre polynomials Ln(\u03b1)(z) are presented. The standard three-term recurrence relation satisfied by the polynomials and different types of asymptotic expansions valid for n large and \u03b1 small, are used depending on the parameter region. \n \nBased on tests of contiguous relations in the parameter \u03b1 and the degree n satisfied by the polynomials, we claim that a relative accuracy close to or better than 10\u221212 can be obtained using the module LaguerrePol for computing the functions Ln(\u03b1)(z) in the parameter range z\u22650, \u22121", "venue": "Comput. Phys. Commun.", "authors": ["Amparo  Gil", "Javier  Segura", "Nico M. Temme"], "year": 2017, "n_citations": 12}
{"id": 5882230, "s2_id": "bd7610d1c4403b69ee7740e127bbdeaa4747f008", "title": "Lighthouse: A User-Centered Web Service for Linear Algebra Software", "abstract": "Various fields of science and engineering rely on linear algebra for large scale data analysis, modeling and simulation, machine learning, and other applied problems. Linear algebra computations often dominate the execution time of such applications. Meanwhile, experts in these domains typically lack the training or time required to develop efficient, high-performance implementations of linear algebra algorithms. In the Lighthouse project, we enable developers with varied backgrounds to readily discover and effectively apply the best available numerical software for their problems. We have developed a search-based expert system that combines expert knowledge, machine learningbased classification of existing numerical software collections, and automated code generation and optimization. Lighthouse provides a novel software engineering environment aimed at maximizing both developer productivity and application performance for dense and sparse linear algebra computations.", "venue": "ArXiv", "authors": ["Boyana  Norris", "Sa-Lin  Bernstein", "Ramya  Nair", "Elizabeth R. Jessup"], "year": 2014, "n_citations": 7}
{"id": 5886925, "s2_id": "b7f96e73ca78b8c0b542f0f3119c37b7aeb2eb50", "title": "Dynamic Sparse Tensor Algebra Compilation", "abstract": "This paper shows how to generate efficient tensor algebra code that compute on dynamic sparse tensors, which have sparsity structures that evolve over time. We propose a language for precisely specifying recursive, pointer-based data structures, and we show how this language can express a wide range of dynamic data structures that support efficient modification, such as linked lists, binary search trees, and B-trees. We then describe how, given high-level specifications of such data structures, a compiler can generate code to efficiently iterate over and compute with dynamic sparse tensors that are stored in the aforementioned data structures. Furthermore, we define an abstract interface that captures how nonzeros can be inserted into dynamic data structures, and we show how this abstraction guides a compiler to emit efficient code that store the results of sparse tensor algebra computations in dynamic data structures. We evaluate our technique and find that it generates efficient dynamic sparse tensor algebra kernels. Code that our technique emits to compute the main kernel of the PageRank algorithm is 1.05\u00d7 as fast as Aspen, a state-of-the-art dynamic graph processing framework. Furthermore, our technique outperforms PAM, a parallel ordered (key-value) maps library, by 7.40\u00d7 when used to implement element-wise addition of a dynamic sparse matrix to a static sparse matrix.", "venue": "ArXiv", "authors": ["Stephen  Chou", "Saman  Amarasinghe"], "year": 2021, "n_citations": 0}
{"id": 5893010, "s2_id": "915212daa317389adc6141dfef714a46e7527344", "title": "pyLLE: a Fast and User Friendly Lugiato-Lefever Equation Solver", "abstract": "The Lugiato-Lefever Equation (LLE), first developed to provide a description of\n spatial dissipative structures in optical systems, has recently made a significant\n impact in the integrated photonics community, where it has been adopted to help\n understand and predict Kerr-mediated nonlinear optical phenomena such as parametric\n frequency comb generation inside microresonators. The LLE is essentially an\n application of the nonlinear Schrodinger equation (NLSE) to a damped, driven Kerr\n nonlinear resonator, so that a periodic boundary condition is applied. Importantly,\n a slow-varying time envelope is stipulated, resulting in a mean-field solution in\n which the field does not vary within a round trip. This constraint, which\n differentiates the LLE from the more general Ikeda map, significantly simplifies\n calculations while still providing excellent physical representation for a wide\n variety of systems. In particular, simulations based on the LLE formalism have\n enabled modeling that quantitatively agrees with reported experimental results on\n microcomb generation (e.g., in terms of spectral bandwidth), and have also been\n central to theoretical studies that have provided better insight into novel\n nonlinear dynamics that can be supported by Kerr nonlinear microresonators. The\n great potential of microresonator frequency combs (microcombs) in a wide variety of\n applications suggests the need for efficient and widely accessible computational\n tools to more rapidly further their development. Although LLE simulations are\n commonly performed by research groups working in the field, to our knowledge no free\n software package for solving this equation in an easy and fast way is currently\n available. Here, we introduce pyLLE, an open-source LLE solver for microcomb\n modeling. It combines the user-friendliness of the Python programming language and\n the computational power of the Julia programming language.", "venue": "Journal of Research of the National Institute of Standards and Technology", "authors": ["Gregory  Moille", "Qing  Li", "Xiyuan  Lu", "Kartik  Srinivasan"], "year": 2019, "n_citations": 11}
{"id": 5896567, "s2_id": "18a713ac29e23a54ea8f3ba6d4b055e45b1f948e", "title": "The generalized matrix chain algorithm", "abstract": "In this paper, we present a generalized version of the matrix chain algorithm to generate efficient code for linear algebra problems, a task for which human experts often invest days or even weeks of works. The standard matrix chain problem consists in finding the parenthesization of a matrix product M := A1 A2 \u22ef An that minimizes the number of scalar operations. In practical applications, however, one frequently encounters more complicated expressions, involving transposition, inversion, and matrix properties. Indeed, the computation of such expressions relies on a set of computational kernels that offer functionality well beyond the simple matrix product. The challenge then shifts from finding an optimal parenthesization to finding an optimal mapping of the input expression to the available kernels. Furthermore, it is often the case that a solution based on the minimization of scalar operations does not result in the optimal solution in terms of execution time. In our experiments, the generated code outperforms other libraries and languages on average by a factor of about 9. The motivation for this work comes from the fact that\u2014despite great advances in the development of compilers\u2014the task of mapping linear algebra problems to optimized kernels is still to be done manually. In order to relieve the user from this complex task, new techniques for the compilation of linear algebra expressions have to be developed.", "venue": "CGO", "authors": ["Henrik  Barthels", "Marcin  Copik", "Paolo  Bientinesi"], "year": 2018, "n_citations": 7}
{"id": 5908730, "s2_id": "b8bd6a79500d95241e032c414f40f0cd8ce3d493", "title": "Enhancing SfePy with Isogeometric Analysis", "abstract": "In the paper a recent enhancement to the open source package SfePy (Simple Finite Elements in Python, this http URL) is introduced, namely the addition of another numerical discretization scheme, the isogeometric analysis, to the original implementation based on the nowadays standard and well-established numerical solution technique, the finite element method. The isogeometric removes the need of the solution domain approximation by a piece-wise polygonal domain covered by the finite element mesh, and allows approximation of unknown fields with a higher smoothness then the finite element method, which can be advantageous in many applications. Basic numerical examples illustrating the implementation and use of the isogeometric analysis in SfePy are shown.", "venue": "ArXiv", "authors": ["Robert  Cimrman"], "year": 2014, "n_citations": 8}
{"id": 5912265, "s2_id": "2cacfba386d5a3700f2666ef3fd994352a1bcee9", "title": "Introduction to the R package TDA", "abstract": "We present a short tutorial and introduction to using the R package TDA, which provides some tools for Topological Data Analysis. In particular, it includes implementations of functions that, given some data, provide topological information about the underlying space, such as the distance function, the distance to a measure, the kNN density estimator, the kernel density estimator, and the kernel distance. The salient topological features of the sublevel sets (or superlevel sets) of these functions can be quantified with persistent homology. We provide an R interface for the efficient algorithms of the C++ libraries GUDHI, Dionysus and PHAT, including a function for the persistent homology of the Rips filtration, and one for the persistent homology of sublevel sets (or superlevel sets) of arbitrary functions evaluated over a grid of points. The significance of the features in the resulting persistence diagrams can be analyzed with functions that implement recently developed statistical methods. The R package TDA also includes the implementation of an algorithm for density clustering, which allows us to identify the spatial organization of the probability mass associated to a density function and visualize it by means of a dendrogram, the cluster tree.", "venue": "ArXiv", "authors": ["Brittany Terese Fasy", "Jisu  Kim", "Fabrizio  Lecci", "Cl\u00e9ment  Maria"], "year": 2014, "n_citations": 109}
{"id": 5913596, "s2_id": "42d0357a781152c4c2151f5b44fe8d4e9720b594", "title": "Fundamental concepts in the Cyclus nuclear fuel cycle simulation framework", "abstract": "Nuclear fuel cycle modeling generality and robustness are improved by a modular, agent based modeling framework.Discrete material and facility tracking rather than fleet-based modeling improve nuclear fuel cycle simulation fidelity.A free, open source paradigm encourages technical experts to contribute software to the Cyclus modeling ecosystem.The flexibility of the Cyclus tool from the simulator user perspective is demonstrated with both open and closed fuel cycle examples. As nuclear power expands, technical, economic, political, and environmental analyses of nuclear fuel cycles by simulators increase in importance. To date, however, current tools are often fleet-based rather than discrete and restrictively licensed rather than open source. Each of these choices presents a challenge to modeling fidelity, generality, efficiency, robustness, and scientific transparency. The Cyclus nuclear fuel cycle simulator framework and its modeling ecosystem incorporate modern insights from simulation science and software architecture to solve these problems so that challenges in nuclear fuel cycle analysis can be better addressed. A summary of the Cyclus fuel cycle simulator framework and its modeling ecosystem are presented. Additionally, the implementation of each is discussed in the context of motivating challenges in nuclear fuel cycle simulation. Finally, the current capabilities of Cyclus are demonstrated for both open and closed fuel cycles.", "venue": "Adv. Eng. Softw.", "authors": ["Kathryn D. Huff", "Matthew J. Gidden", "Robert W. Carlsen", "Robert R. Flanagan", "Meghan B. McGarry", "Arrielle C. Opotowsky", "Erich A. Schneider", "Anthony M. Scopatz", "Paul P. H. Wilson"], "year": 2016, "n_citations": 32}
{"id": 5921968, "s2_id": "7c130ede57bf97cf124296a719781c9cc79a4a1d", "title": "ExaHyPE: An Engine for Parallel Dynamically Adaptive Simulations of Wave Problems", "abstract": "ExaHyPE (\u201cAn Exascale Hyperbolic PDE Engine\u201d) is a software engine for solving systems of first-order hyperbolic partial differential equations (PDEs). Hyperbolic PDEs are typically derived from the conservation laws of physics and are useful in a wide range of application areas. Applications powered by ExaHyPE can be run on a student\u2019s laptop, but are also able to exploit thousands of processor cores on state-of-the-art supercomputers. The engine is able to dynamically increase the accuracy of the simulation using adaptive mesh refinement where required. Due to the robustness and shock capturing abilities of ExaHyPE\u2019s numerical methods, users of the engine can simulate linear and non-linear hyperbolic PDEs with very high accuracy. Users can tailor the engine to their particular PDE by specifying evolved quantities, fluxes, and source terms. A complete simulation code for a new hyperbolic PDE can often be realised within a few hours \u2014 a task that, traditionally, can take weeks, months, often years for researchers starting from scratch. In this paper, we showcase ExaHyPE\u2019s workflow and capabilities through real-world scenarios from our two main application areas: seismology and astrophysics.", "venue": "Comput. Phys. Commun.", "authors": ["Anne  Reinarz", "Dominic E. Charrier", "Michael  Bader", "Luke  Bovard", "Michael  Dumbser", "Kenneth  Duru", "Francesco  Fambri", "Alice-Agnes  Gabriel", "Jean-Mathieu  Gallard", "Sven  K\u00f6ppel", "Lukas  Krenz", "Leonhard  Rannabauer", "Luciano  Rezzolla", "Philipp  Samfass", "Maurizio  Tavelli", "Tobias  Weinzierl"], "year": 2020, "n_citations": 35}
{"id": 5923817, "s2_id": "debb00854b49e4e717e80d4d9a385bda224cbd97", "title": "Industrial-Strength Documentation for ACL2", "abstract": "The ACL2 theorem prover is a complex system. Its libraries are vast. Industrial verification efforts may extend this base with hundreds of thousands of lines of additional modeling tools, specifications, and proof scripts. High quality documentation is vital for teams that are working together on projects of this scale. We have developed XDOC, a flexible, scalable documentation tool for ACL2 that can incorporate the documentation for ACL2 itself, the Community Books, and an organization\u2019s internal formal verification projects, and which has many features that help to keep the resulting manuals up to date. Using this tool, we have produced a comprehensive, publicly available ACL2+Books Manual that brings better documentation to all ACL2 users. We have also developed an extended manual for use within Centaur Technology that extends the public manual to cover Centaur\u2019s internal books. We expect that other organizations using ACL2 will wish to develop similarly extended manuals.", "venue": "ACL2", "authors": ["Jared  Davis", "Matt  Kaufmann"], "year": 2014, "n_citations": 4}
{"id": 5926790, "s2_id": "62f4242948cf14bb3602b702a65466a84c6509b9", "title": "An Attempt to Generate Code for Symmetric Tensor Computations", "abstract": "Symmetric matrices, a frequently studied topic in linear algebra, can be extended to higher dimensions through symmetric tensors, which arise in domains such as computational physics and chemistry [5, 11]. The fundamental mathematical appeal of the study of symmetry also renders these tensors useful in contexts ranging from a rather beautiful equivalence with homogenous polynomials [3] to more concrete applications, including decompositions [9] and finding eigenvalues [8]. Knowing about a tensor\u2019s symmetry tells us which of its values are guaranteed to be equal. If leveraged effectively, this can be utilized to improve both storage usage and computation time: values that are the same do not need to be redundantly stored or computed. Many existing works focus on optimizing one specific type of tensor computation involving symmetry. On the other hand, compilers such as taco [6] can produce code for a broad class of tensor algebra expressions, with support for per-dimension level formats [2]. However, taco does not presently provide a way to indicate or apply information about which and how tensors are symmetric. Our goal is thus to work towards a compiler-based approach for symmetric tensor computations, relying on taco for aspects of code generation unrelated to symmetry. The remainder of this document is outlined as follows. We start in Section 2 by reviewing standard definitions, which characterize a tensor\u2019s symmetry by a partition on its dimensions, such that coordinates along dimensions that belong to the same part can be permuted without changing the value. Then, we consider in Section 3 the problem of representing symmetric tensors in storage. Some values can be omitted, but when the remaining values are then placed in a dense array, we need new formulas to index into them. While non-symmetric tensors can be geometrically viewed as rectangular prisms, the lower half of, say, a symmetric matrix looks like a triangle. As we move into higher dimensions, the triangles become simplexes, and as a result, our formulas make heavy use of simplicial numbers. At this point, we will be able to reason about the symmetries of individual tensors, so we proceed a step further in Section 4 to provide abstractions for thinking about the symmetry of a computation, which will ultimately inform the structure of the code that we generate. Interesting cases arise when the various input tensors in the computation are symmetric along dimensions with different indices. Section 5 details how to generate a loop structure to handle these cases. Our method is to separate the for loops that iterate over a particular dimension based on whether the values in the corresponding region of the input are stored or omitted, in order to determine how to access the tensors correctly. We end by observing that the storage savings exploited in the resulting code come at the cost of poor computational performance.", "venue": "ArXiv", "authors": ["Jessica  Shi", "Stephen  Chou", "Fredrik  Kjolstad", "Saman  Amarasinghe"], "year": 2021, "n_citations": 0}
{"id": 5928091, "s2_id": "11cf35e425a42b4b68cfe5936024bbcf93bcaed8", "title": "Understanding Branch Cuts of Expressions", "abstract": "We assume some standard choices for the branch cuts of a group of functions and consider the problem of then calculating the branch cuts of expressions involving those functions. Typical examples include the addition formulae for inverse trigonometric functions. Understanding these cuts is essential for working with the single-valued counterparts, the common approach to encoding multi-valued functions in computer algebra systems. While the defining choices are usually simple (typically portions of either the real or imaginary axes) the cuts induced by the expression may be surprisingly complicated. We have made explicit and implemented techniques for calculating the cuts in the computer algebra programme Maple. We discuss the issues raised, classifying the different cuts produced. The techniques have been gathered in the BranchCuts package, along with tools for visualising the cuts. The package is included in Maple 17 as part of the FunctionAdvisor tool.", "venue": "MKM/Calculemus/DML", "authors": ["Matthew  England", "Russell J. Bradford", "James H. Davenport", "David J. Wilson"], "year": 2013, "n_citations": 16}
{"id": 5929858, "s2_id": "07bcc076a6cd0832eb9e83fa42c16efe5f2c2c89", "title": "A review of automatic differentiation and its efficient implementation", "abstract": "Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation (AD) is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of AD, however, requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region\u2010based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi\u2010analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open and practical problems include the extension of current packages to provide more specialized routines, and finding optimal methods to perform higher\u2010order differentiation.", "venue": "Wiley Interdiscip. Rev. Data Min. Knowl. Discov.", "authors": ["Charles C. Margossian"], "year": 2019, "n_citations": 72}
{"id": 5931753, "s2_id": "3b5df6e14bfd6aea2afdaa9267a69daa97eccfe5", "title": "CAPD: : DynSys: a flexible C++ toolbox for rigorous numerical analysis of dynamical systems", "abstract": "We present the CAPD::DynSys library for rigorous numerical analysis of dynamical systems. The basic interface is described together with several interesting case studies illustrating how it can be used for computer-assisted proofs in dynamics of ODEs.", "venue": "Commun. Nonlinear Sci. Numer. Simul.", "authors": ["Tomasz  Kapela", "Marian  Mrozek", "Daniel  Wilczak", "Piotr  Zgliczy'nski"], "year": 2021, "n_citations": 16}
{"id": 5932830, "s2_id": "9652253a15284e90f40484010c3e308a70741738", "title": "Boda-RTC: Productive generation of portable, efficient code for convolutional neural networks on mobile computing platforms", "abstract": "The popularity of neural networks (NNs) spans academia [1], industry [2], and popular culture [3]. In particular, convolutional neural networks (CNNs) have been applied to many image based machine learning tasks and have yielded strong results [4]. The availability of hardware/software systems for efficient training and deployment of large and/or deep CNN models is critical for the continued success of the field [5] [1]. Early systems for NN computation focused on leveraging existing dense linear algebra techniques and libraries [6] [7]. Current approaches use low-level machine specific programming [8] and/or closed-source, purpose-built vendor libraries [9]. In this work, we present an open source system that, compared to existing approaches, achieves competitive computational speed while achieving significantly greater portability. We achieve this by targeting the vendor-neutral OpenCL platform [10] using a code-generation approach. We argue that our approach allows for both: (1) the rapid development of new computational kernels for existing hardware targets, and (2) the rapid tuning of existing computational kernels for new hardware targets. Results are presented for a case study of targeting the Qualcomm Snapdragon 820 mobile computing platform [11] for CNN deployment.", "venue": "2016 IEEE 12th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)", "authors": ["Matthew W. Moskewicz", "Forrest N. Iandola", "Kurt  Keutzer"], "year": 2016, "n_citations": 8}
{"id": 5936231, "s2_id": "f99315609bd8f5beed1b77295ddda79633b6596e", "title": "Exact Acceleration of K-Means++ and K-Means\u2225", "abstract": "K-Means++ and its distributed variant K-Means\u2016 have become de facto tools for selecting the initial seeds of K-means. While alternatives have been developed, the effectiveness, ease of implementation, and theoretical grounding of the K-means++ and \u2016 methods have made them difficult to \u201cbest\u201d from a holistic perspective. By considering the limited opportunities within seed selection to perform pruning, we develop specialized triangle inequality pruning strategies and a dynamic priority queue to show the first acceleration of K-Means++ and KMeans\u2016 that is faster in run-time while being algorithmicly equivalent. For both algorithms we are able to reduce distance computations by over 500\u00d7. For K-means++ this results in up to a 17\u00d7 speedup in run-time and a 551\u00d7 speedup for K-means\u2016. We achieve this with simple, but carefully chosen, modifications to known techniques which makes it easy to integrate our approach into existing implementations of these algorithms.", "venue": "IJCAI", "authors": ["Edward  Raff"], "year": 2021, "n_citations": 0}
{"id": 5936758, "s2_id": "8bf4e287eae8c9539a5f1f2fc285c91f497a33a7", "title": "High performance solutions for big-data GWAS", "abstract": "We consider mixed-models based genome-wide association studies of large scale.We address GWAS with only a single trait and with many traits.GWAS with arbitrarily large populations, and of arbitrarily many SNPs and traits are enabled.Distributed memory architectures, such as Cloud, clusters, and supercomputers are used.Scalability with respect to problem size and resources is demonstrated. In order to associate complex traits with genetic polymorphisms, genome-wide association studies process huge datasets involving tens of thousands of individuals genotyped for millions of polymorphisms. When handling these datasets, which exceed the main memory of contemporary computers, one faces two distinct challenges: (1) millions of polymorphisms and thousands of phenotypes come at the cost of hundreds of gigabytes of data, which can only be kept in secondary storage; (2) the relatedness of the test population is represented by a relationship matrix, which, for large populations, can only fit in the combined main memory of a distributed architecture. In this paper, by using distributed resources such as Cloud or clusters, we address both challenges: the genotype and phenotype data is streamed from secondary storage using the double-buffering technique, while the relationship matrix is kept across the main memory of a distributed memory system. With the help of these solutions, we develop separate algorithms for studies involving only one or a multitude of traits. We show that these algorithms sustain high-performance and allow the analysis of enormous datasets.", "venue": "Parallel Comput.", "authors": ["Elmar  Peise", "Diego  Fabregat-Traver", "Paolo  Bientinesi"], "year": 2015, "n_citations": 11}
{"id": 5944472, "s2_id": "6b570069f14c7588e066f7138e1f21af59d62e61", "title": "Theano: A Python framework for fast computation of mathematical expressions", "abstract": "Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. \nThe present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.", "venue": "ArXiv", "authors": ["Rami  Al-Rfou'", "Guillaume  Alain", "Amjad  Almahairi", "Christof  Angerm\u00fcller", "Dzmitry  Bahdanau", "Nicolas  Ballas", "Fr\u00e9d\u00e9ric  Bastien", "Justin  Bayer", "Anatoly  Belikov", "Alexander  Belopolsky", "Yoshua  Bengio", "Arnaud  Bergeron", "James  Bergstra", "Valentin  Bisson", "Josh Bleecher Snyder", "Nicolas  Bouchard", "Nicolas  Boulanger-Lewandowski", "Xavier  Bouthillier", "Alexandre de Br\u00e9bisson", "Olivier  Breuleux", "Pierre Luc Carrier", "Kyunghyun  Cho", "Jan  Chorowski", "Paul F. Christiano", "Tim  Cooijmans", "Marc-Alexandre  C\u00f4t\u00e9", "Myriam  C\u00f4t\u00e9", "Aaron C. Courville", "Yann  Dauphin", "Olivier  Delalleau", "Julien  Demouth", "Guillaume  Desjardins", "Sander  Dieleman", "Laurent  Dinh", "Melanie  Ducoffe", "Vincent  Dumoulin", "Samira Ebrahimi Kahou", "Dumitru  Erhan", "Ziye  Fan", "Orhan  Firat", "Mathieu  Germain", "Xavier  Glorot", "Ian J. Goodfellow", "Matthew  Graham", "\u00c7aglar  G\u00fcl\u00e7ehre", "Philippe  Hamel", "Iban  Harlouchet", "Jean-Philippe  Heng", "Bal\u00e1zs  Hidasi", "Sina  Honari", "Arjun  Jain", "S\u00e9bastien  Jean", "Kai  Jia", "Mikhail  Korobov", "Vivek  Kulkarni", "Alex  Lamb", "Pascal  Lamblin", "Eric  Larsen", "C\u00e9sar  Laurent", "Sean  Lee", "Simon  Lefran\u00e7ois", "Simon  Lemieux", "Nicholas  L\u00e9onard", "Zhouhan  Lin", "Jesse A. Livezey", "Cory  Lorenz", "Jeremiah  Lowin", "Qianli  Ma", "Pierre-Antoine  Manzagol", "Olivier  Mastropietro", "Robert  McGibbon", "Roland  Memisevic", "Bart van Merrienboer", "Vincent  Michalski", "Mehdi  Mirza", "Alberto  Orlandi", "Christopher Joseph Pal", "Razvan  Pascanu", "Mohammad  Pezeshki", "Colin  Raffel", "Daniel  Renshaw", "Matthew  Rocklin", "Adriana  Romero", "Markus  Roth", "Peter  Sadowski", "John  Salvatier", "Fran\u00e7ois  Savard", "Jan  Schl\u00fcter", "John  Schulman", "Gabriel  Schwartz", "Iulian  Serban", "Dmitriy  Serdyuk", "Samira  Shabanian", "\u00c9tienne  Simon", "Sigurd  Spieckermann", "S. Ramana Subramanyam", "Jakub  Sygnowski", "J\u00e9r\u00e9mie  Tanguay", "Gijs van Tulder", "Joseph P. Turian", "Sebastian  Urban", "Pascal  Vincent", "Francesco  Visin", "Harm de Vries", "David  Warde-Farley", "Dustin J. Webb", "Matthew  Willson", "Kelvin  Xu", "Lijun  Xue", "Li  Yao", "Saizheng  Zhang", "Ying  Zhang"], "year": 2016, "n_citations": 2033}
{"id": 5952858, "s2_id": "958144ef9138422a721976e9a60136bc20b7f464", "title": "EXA-DUNE: Flexible PDE Solvers, Numerical Methods and Applications", "abstract": "In the EXA-DUNE project we strive to (i) develop and implement numerical algorithms for solving PDE problems efficiently on heterogeneous architectures, (ii) provide corresponding domain-specific abstractions that allow application scientists to effectively use these methods, and (iii) demonstrate performance on porous media flow problems. In this paper, we present first results on the hybrid parallelisation of sparse linear algebra, system and RHS assembly, the implementation of multiscale finite element methods and the SIMD performance of high-order discontinuous Galerkin methods within an application scenario.", "venue": "Euro-Par Workshops", "authors": ["Peter  Bastian", "Christian  Engwer", "Dominik  G\u00f6ddeke", "Oleg P. Iliev", "Olaf  Ippisch", "Mario  Ohlberger", "Stefan  Turek", "Jorrit  Fahlke", "Sven  Kaulmann", "Steffen  M\u00fcthing", "Dirk  Ribbrock"], "year": 2014, "n_citations": 21}
{"id": 5955803, "s2_id": "31f99204bbef6d4e8edfa962993c6989b4c6b0ac", "title": "Nonconforming Mesh Refinement for High-Order Finite Elements", "abstract": "We propose a general algorithm for nonconforming adaptive mesh refinement (AMR) of unstructured meshes in high-order finite element codes. Our focus is on $h$-refinement with a fixed polynomial ord...", "venue": "SIAM J. Sci. Comput.", "authors": ["Jakub  Cerven\u00fd", "Veselin  Dobrev", "Tzanio V. Kolev"], "year": 2019, "n_citations": 14}
{"id": 5963994, "s2_id": "4538fbfff219b46a4afe4c3ef5bba2c018699f95", "title": "Parametric model order reduction using pyMOR", "abstract": "pyMOR is a free software library for model order reduction that includes both reduced basis and system-theoretic methods. All methods are implemented in terms of abstract vector and operator interfaces, which allows direct integration of pyMOR's algorithms with a wide array of external PDE solvers. In this contribution, we give a brief overview of the available methods and experimentally compare them for the parametric instationary thermal-block benchmark defined in arXiv:2003.00846.", "venue": "ArXiv", "authors": ["Petar  Mlinari\u0107", "Stephan  Rave", "Jens  Saak"], "year": 2020, "n_citations": 1}
{"id": 5970379, "s2_id": "a68110a8d84fcab7114bd649d6f3677aa54181d5", "title": "Accelerated Multiple Precision Direct Method and Mixed Precision Iterative Refinement on Python Programming Environment", "abstract": "Current Python programming environment does not have any reliable and efficient multiple precision floating-point (MPF) arithmetic except \u201cmpmath\u201d and \u201cgmpy2\u201d packages based on GNU MP(GMP) and MPFR libraries. Although it is well known that multi-componenttype MPF library can be utilized for middle length precision arithmetic under 200 bits, they are not widely used on Python environment. In this paper, we describe our accelerated MPF direct method with AVX2 techniques and its application to mixed precision iterative refinement combined with mpmath, and demonstrate their efficiency on x86 64 computational environments.", "venue": "ArXiv", "authors": ["Tomonori  Kouya"], "year": 2021, "n_citations": 0}
{"id": 5972317, "s2_id": "b4bd64229bddaee1acb68a44b4b80c4e31382d55", "title": "GNA: new framework for statistical data analysis", "abstract": "We report on the status of GNA \u2014 a new framework for fitting large-scale physical models. GNA utilizes the data flow concept within which a model is represented by a directed acyclic graph. Each node is an operation on an array (matrix multiplication, derivative or cross section calculation, etc). The framework enables the user to create flexible and efficient large-scale lazily evaluated models, handle large numbers of parameters, propagate parameters\u2019 uncertainties while taking into account possible correlations between them, fit models, and perform statistical analysis.\nThe main goal of the paper is to give an overview of the main concepts and methods as well as reasons behind their design. Detailed technical information is to be published in further works.", "venue": "EPJ Web of Conferences", "authors": ["Anna  Fatkina", "Maxim  Gonchar", "Anastasia  Kalitkina", "Liudmila  Kolupaeva", "Dmitry  Naumov", "Dmitry  Selivanov", "Konstantin  Treskov"], "year": 2019, "n_citations": 2}
{"id": 5973517, "s2_id": "86216613dd11e1f6d0bf6ab7d0c056b1b2c4b9d6", "title": "Power Consumption Analysis of Parallel Algorithms on GPUs", "abstract": "Due to their highly parallel multi-cores architecture, GPUs are being increasingly used in a wide range of computationally intensive applications. Compared to CPUs, GPUs can achieve higher performances at accelerating the programs' execution in an energy-efficient way. Therefore GPGPU computing is useful for high performance computing applications and in many scientific research fields. In order to bring further performance improvements, GPU clusters are increasingly adopted. The energy consumed by GPUs cannot be neglected. Therefore, an energy-efficient time scheduling of the programs that are going to be executed by the parallel GPUs based on their deadline as well as the assigned priorities could be deployed to face their energetic avidity. For this reason, we present in this paper a model enabling the measure of the power consumption and the time execution of some elementary operations running on a single GPU using a new developed energy measurement protocol. Consequently, using our methodology, energy needs of a program could be predicted, allowing a better task scheduling.", "venue": "2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)", "authors": ["Fr\u00e9d\u00e9ric  Magoul\u00e8s", "Abal-Kassim Cheik Ahamed", "Alban  Desmaison", "Jean-Christophe  L\u00e9chenet", "Francois  Mayer", "Haifa Ben Salem", "Thomas  Zhu"], "year": 2014, "n_citations": 2}
{"id": 5974388, "s2_id": "2fed84c74f8fa683057eb9968200d53d1b45428f", "title": "When Lift-and-Project Cuts are Different", "abstract": "In this paper, we present a method to determine if a lift-and-project cut for a mixed-integer linear program is irregular, in which case the cut is not equivalent to any intersection cut from the bases of the linear relaxation. This is an important question due to the intense research activity for the past decade on cuts from multiple rows of simplex tableau as well as on lift-and-project cuts from non-split disjunctions. While it is known since Balas and Perregaard (2003) that lift-and-project cuts from split disjunctions are always equivalent to intersection cuts and consequently to such multi-row cuts, Balas and Kis (2016) have recently shown that there is a necessary and sufficient condition in the case of arbitrary disjunctions: a lift-and-project cut is regular if, and only if, it corresponds to a regular basic solution of the Cut Generating Linear Program (CGLP). This paper has four contributions. First, we state a result that simplifies the verification of regularity for basic CGLP solutions from Balas and Kis (2016). Second, we provide a mixed-integer formulation that checks whether there is a regular CGLP solution for a given cut that is regular in a broader sense, which also encompasses irregular cuts that are implied by the regular cut closure. Third, we describe a numerical procedure based on such formulation that identifies irregular lift-and-project cuts. Finally, we use this method to evaluate how often lift-and-project cuts from simple $t$-branch split disjunctions are irregular, and thus not equivalent to multi-row cuts, on 74 instances of the MIPLIB benchmarks.", "venue": "INFORMS J. Comput.", "authors": ["Egon  Balas", "Thiago  Serra"], "year": 2020, "n_citations": 1}
{"id": 5976369, "s2_id": "8bc5c614f4e587858f29a1efe1e94f426d7d280a", "title": "Generic and typical ranks of three-way arrays", "abstract": "The concept of tensor rank, introduced in the twenties, has been popularized at the beginning of the seventies. This has allowed to carry out factor analysis on arrays with more than two indices. The generic rank may be seen as an upper bound to the number of factors that can be extracted from a given tensor, with certain uniqueness conditions. We explain how to obtain numerically the generic rank of tensors of arbitrary dimensions, and compare it with the rare algebraic results already known at order three. In particular, we examine the cases of symmetric tensors, tensors with symmetric matrix slices, or tensors with free entries. Related applications include antenna array processing.", "venue": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing", "authors": ["Pierre  Comon", "Jos M. F. ten Berge"], "year": 2008, "n_citations": 15}
{"id": 5977039, "s2_id": "1d5f05ea8d776dca5ee120a02c084c641ecbcbb3", "title": "Bitslicing and the Method of Four Russians Over Larger Finite Fields", "abstract": "We present a method of computing with matrices over very small finite fields of size larger than 2. Specifically, we show how the Method of Four Russians can be efficiently adapted to these larger fields, and introduce a row-wise matrix compression scheme that both reduces memory requirements and allows one to vectorize element operations. We also present timings which confirm the efficiency of these methods and exceed the speed of the fastest implementations the authors are aware of.", "venue": "ArXiv", "authors": ["Tomas  Boothby", "Robert W. Bradshaw"], "year": 2009, "n_citations": 14}
{"id": 5980387, "s2_id": "9a83f8ebfecacfb30c4796cccaf5d59561f081ac", "title": "Performance evaluation of explicit finite difference algorithms with varying amounts of computational and memory intensity", "abstract": "Future architectures designed to deliver exascale performance motivate the need for novel algorithmic changes in order to fully exploit their capabilities. In this paper, the performance of several numerical algorithms, characterised by varying degrees of memory and computational intensity, are evaluated in the context of finite difference methods for fluid dynamics problems. It is shown that, by storing some of the evaluated derivatives as single thread- or process-local variables in memory, or recomputing the derivatives on-the-fly, a speed-up of ~2 can be obtained compared to traditional algorithms that store all derivatives in global arrays.", "venue": "J. Comput. Sci.", "authors": ["Satya P. Jammy", "Christian T. Jacobs", "Neil D. Sandham"], "year": 2019, "n_citations": 9}
{"id": 5984379, "s2_id": "3b735cb67eec3615d8f758b0493e6691b7c04c9a", "title": "Elfun18 A collection of Matlab functions for the computation of Elliptical Integrals and Jacobian elliptic functions of real arguments", "abstract": "Abstract We outline a set of MATLAB functions that enable the computation of elliptic integrals and Jacobian elliptic functions for real arguments. The correctness, robustness, efficiency, and accuracy of the functions are discussed in detail. An example from elasticity theory is provided to illustrate the use of this collection.", "venue": "SoftwareX", "authors": ["Milan  Batista"], "year": 2019, "n_citations": 4}
{"id": 5988201, "s2_id": "e66f15954b32df82c27a16f9d4582328d8f6383d", "title": "Employing online quantum random number generators for generating truly random quantum states in Mathematica", "abstract": "Abstract The presented package for the Mathematica\u00a0computing system allows the harnessing of quantum random number generators (QRNG) for investigating the statistical properties of quantum states. The described package implements a number of functions for generating random states. The new version of the package adds the ability to use the on-line quantum random number generator service and implements new functions for retrieving lists of random numbers. Thanks to the introduced improvements, the new version provides faster access to high-quality sources of random numbers and can be used in simulations requiring large amount of random data. New version program summary Program title: TRQS Catalogue identifier: AEKA_v2_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEKA_v2_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 18\u00a0134 No. of bytes in distributed program, including test data, etc.: 2\u00a0520\u00a049 Distribution format: tar.gz Programming language: Mathematica, C. Computer: Any supporting Mathematica in version 7 or higher. Operating system: Any platform supporting Mathematica; tested with GNU/Linux (32 and 64 bit). RAM: Case-dependent Supplementary material:\u00a0 Fig.\u00a01 mentioned below can be downloaded. Classification: 4.15. External routines: Quantis software library ( http://www.idquantique.com/support/quantis-trng.html ) Catalogue identifier of previous version: AEKA_v1_0 Journal reference of previous version: Comput. Phys. Comm. 183(2012)118 Does the new version supersede the previous version?: Yes Nature of problem: Generation of random density matrices and utilization of high-quality random numbers for the purpose of computer simulation. Solution method: Use of a physical quantum random number generator and an on-line service providing access to the source of true random numbers generated by quantum real number generator. Reasons for new version: Added support for the high-speed on-line quantum random number generator and improved methods for retrieving lists of random numbers. Summary of revisions: The presented version provides two signicant improvements. The first one is the ability to use the on-line Quantum Random Number Generation service developed by PicoQuant GmbH and the Nano-Optics groups at the Department of Physics of Humboldt University. The on-line service supported in the version 2.0 of the TRQS\u00a0package provides faster access to true randomness sources constructed using the laws of quantum physics. The service is freely available at https://qrng.physik.hu-berlin.de/ . The use of this service allows using the presented package with the need of a physical quantum random number generator. The second improvement introduced in this version is the ability to retrieve arrays of random data directly for the used source. This increases the speed of the random number generation, especially in the case of an on-line service, where it reduces the time necessary to establish the connection. Thanks to the speed improvement of the presented version, the package can now be used in simulations requiring larger amounts of random data. Moreover, the functions for generating random numbers provided by the current version of the package more closely follow the pattern of functions for generating pseudo- random numbers provided in Mathematica. Additional comments: Speed comparison: The implementation of the support for the QRNG on-line service provides a noticeable improvement in the speed of random number generation. For the samples of real numbers of size 10 1 ; 10 2 , \u2026 , 10 7 the times required to generate these samples using Quantis USB device and QRNG service are compared in Fig.\u00a01 . The presented results show that the use of the on-line service provides faster access to random numbers. One should note, however, that the speed gain can increase or decrease depending on the connection speed between the computer and the server providing random numbers. Running time: Depends on the used source of randomness and the amount of random data used in the experiment. References: [1] M. Wahl, M. Leifgen, M. Berlin, T. Rohlicke, H.-J. Rahn, O. Benson., An ultrafast quantum random number generator with provably bounded output bias based on photon arrival time measurements, Applied Physics Letters, Vol. 098, 171105 (2011). http://dx.doi.org/10.1063/1.3578456 .", "venue": "Comput. Phys. Commun.", "authors": ["Jaroslaw Adam Miszczak"], "year": 2013, "n_citations": 6}
{"id": 5993625, "s2_id": "41d04aa3c25dcfbf1b44ce666c48759e03c216c7", "title": "tf.data: A Machine Learning Data Processing Framework", "abstract": "Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators which can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions allow users to focus on the application logic of data processing, while tf.data's runtime ensures that pipelines run efficiently. \nWe demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Google's fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer.", "venue": "Proc. VLDB Endow.", "authors": ["Derek G. Murray", "Jiri  Simsa", "Ana  Klimovic", "Ihor  Indyk"], "year": 2021, "n_citations": 5}
{"id": 5995043, "s2_id": "ccc22c435a121092c093faa538d9f5d0e329e5d0", "title": "Solving Polynomial Systems with phcpy", "abstract": "The solutions of a system of polynomials in several variables are often needed, e.g.: in the design of mechanical systems, and in phase-space analyses of nonlinear biological dynamics. Reliable, accurate, and comprehensive numerical solutions are available through PHCpack, a FOSS package for solving polynomial systems with homotopy continuation. This paper explores new developments in phcpy, a scripting interface for PHCpack, over the past five years. For instance, phcpy is now available online through a JupyterHub server featuring Python2, Python3, and SageMath kernels. As small systems are solved in real-time by phcpy, they are suitable for interactive exploration through the notebook interface. Meanwhile, phcpy supports GPU parallelization, improving the speed and quality of solutions to much larger polynomial systems. From various model design and analysis problems in STEM, certain classes of polynomial system frequently arise, to which phcpy is well-suited.", "venue": "Proceedings of the 18th Python in Science Conference", "authors": ["Jasmine  Otto", "Angus  Forbes", "Jan  Verschelde"], "year": 2019, "n_citations": 2}
{"id": 5995957, "s2_id": "4ced3fd73ccc79ba7f72544e6ba9b1f87127150b", "title": "Toward resilient algorithms and applications", "abstract": "Large-scale computing platforms have always dealt with unreliability coming from many sources. In contrast applications for large-scale systems have generally assumed a fairly simplistic failure model: The computer is a reliable digital machine, with consistent execution time and infrequent failures that can be handled by occasionally storing a checkpoint of application state and restarting from that saved state if the system fails. Many computing experts, and several key technology trends indicate that the current simplistic application view of a high-end system is no longer feasible. Instead, algorithms and application developers must adopt more complex models for system reliability and adapt algorithms and implementation to be more resilient in the presence of failures and increased failure detection and correction.\n In this talk we present motivation for moving away from a checkpoint-restart-only model and discuss several new models for resilience, including latency tolerance, local recovery from local failure and selective reliability. We also discuss strategies for designing new algorithms and applications, and some of the required system and programming environment features.", "venue": "FTXS '13", "authors": ["Michael A. Heroux"], "year": 2013, "n_citations": 21}
{"id": 5995973, "s2_id": "d8bd44997c6397ddec48cbf8aea24d1ad755df85", "title": "Adaptive SpMV/SpMSpV on GPUs for Input Vectors of Varied Sparsity", "abstract": "Despite numerous efforts for optimizing the performance of Sparse Matrix and Vector Multiplication (SpMV) on modern hardware architectures, few works are done to its sparse counterpart, Sparse Matrix and Sparse Vector Multiplication (SpMSpV), not to mention dealing with input vectors of varied sparsity. The key challenge is that depending on the sparsity levels, distribution of data, and compute platform, the optimal choice of SpMV/SpMSpV kernel can vary, and a static choice does not suffice. In this article, we propose an adaptive SpMV/SpMSpV framework, which can automatically select the appropriate SpMV/SpMSpV kernel on GPUs for any sparse matrix and vector at the runtime. Based on systematic analysis on key factors such as computing pattern, workload distribution and write-back strategy, eight candidate SpMV/SpMSpV kernels are encapsulated into the framework to achieve high performance in a seamless manner. A comprehensive study on machine learning-based kernel selector is performed to choose the kernel and adapt with the varieties of both the input and hardware from both accuracy and overhead perspectives. Experiments demonstrate that the adaptive framework can substantially outperform the previous state-of-the-art in real-world applications on NVIDIA Tesla K40m, P100, and V100 GPUs.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Min  Li", "Yulong  Ao", "Chao  Yang"], "year": 2021, "n_citations": 3}
{"id": 5996083, "s2_id": "d1b450d3b5b95711c7ee33020de528253ebaddc1", "title": "Univariate real root isolation in an extension field", "abstract": "We present algorithmic, complexity and implementation results for the problem of isolating the real roots of a univariate polynomial in <i>B</i><sub>\u03b1</sub> \u2208 <i>L</i>[<i>y</i>], where <i>L</i>=<i>Q</i>\u03b1 is a simple algebraic extension of the rational numbers. We revisit two approaches for the problem. In the first approach, using resultant computations, we perform a reduction to a polynomial with integer coefficients and we deduce a bound of <i>O</i><sub><i>B</i></sub>(<i>N</i><sup>10</sup>) for isolating the real roots of <i>B</i><sub>\u03b1</sub>, where <i>N</i> is an upper bound on all the quantities (degree and bitsize) of the input polynomials. In the second approach we isolate the real roots working directly on the polynomial of the input. We compute improved separation bounds for the roots and we prove that they are optimal, under mild assumptions. For isolating the real roots we consider a modified Sturm algorithm, and a modified version of Descartes' algorithm introduced by Sagraloff. For the former we prove a complexity bound of <i>O</i><sub>B</sub>(<i>N</i><sup>8</sup>) and for the latter a bound of <i>O</i><sub>B</sub>(<i>N</i><sup>7</sup>). We implemented the algorithms in C as part of the core library of Mathematica and we illustrate their efficiency over various data sets. Finally, we present complexity results for the general case of the first approach, where the coefficients belong to multiple extensions.", "venue": "ISSAC '11", "authors": ["Adam W. Strzebonski", "Elias P. Tsigaridas"], "year": 2011, "n_citations": 19}
{"id": 6004614, "s2_id": "d85a9c2da63fadb28e23ff8b2525b1d8e6bbbc3b", "title": "A C++ interface to QCDNUM", "abstract": "In this document we report on the recent development of a C++ interface to the FORTRAN-based evolution program QCDNUM. A short description of the interface is given with a few basic examples of its usage.", "venue": "ArXiv", "authors": ["Valerio  Bertone", "Michiel  Botje"], "year": 2017, "n_citations": 2}
{"id": 6006715, "s2_id": "f449adc556e1d0a914fbf300a0c77c1f6151942e", "title": "On the shape of curves that are rational in polar coordinates", "abstract": "In this paper we provide a computational approach to the shape of curves which are rational in polar coordinates, i.e. which are defined by means of a parametrization (r(t),@q(t)) where both r(t), @q(t) are rational functions. Our study includes theoretical aspects on the shape of these curves, and algorithmic results which eventually lead to an algorithm for plotting the ''interesting parts'' of the curve, i.e. the parts showing the main geometrical features.", "venue": "Comput. Aided Geom. Des.", "authors": ["Juan Gerardo Alc\u00e1zar", "Gema Mar\u00eda D\u00edaz-Toca"], "year": 2012, "n_citations": 0}
{"id": 6010153, "s2_id": "132cde4b6f6a0d84d17a4a1ef8a896d747250427", "title": "Computing Hypergeometric Functions Rigorously", "abstract": "We present an efficient implementation of hypergeometric functions in arbitrary-precision interval arithmetic. The functions 0F1, 1F1, 2F1, and 2F0 (or the Kummer U-function) are supported for unrestricted complex parameters and argument, and, by extension, we cover exponential and trigonometric integrals, error functions, Fresnel integrals, incomplete gamma and beta functions, Bessel functions, Airy functions, Legendre functions, Jacobi polynomials, complete elliptic integrals, and other special functions. The output can be used directly for interval computations or to generate provably correct floating-point approximations in any format. Performance is competitive with earlier arbitrary-precision software and sometimes orders of magnitude faster. We also partially cover the generalized hypergeometric function pFq and computation of high-order parameter derivatives.", "venue": "ACM Trans. Math. Softw.", "authors": ["Fredrik  Johansson"], "year": 2019, "n_citations": 28}
{"id": 6030016, "s2_id": "2f32db7dd62911751abd44407aa461c900740c50", "title": "A generic finite element framework on parallel tree-based adaptive meshes", "abstract": "We present highly scalable parallel distributed-memory algorithms and associated data structures for a generic finite element framework that supports $h$-adaptivity on computational domains represented as multiple connected adaptive trees, thus providing multi-scale resolution on problems governed by partial differential equations. The framework is grounded on a rich representation of the adaptive mesh suitable for generic finite elements that is built on top of a low-level, light-weight forest-of-trees data structure handled by a specialized, highly parallel adaptive meshing engine. Along the way, we have identified the requirements that the forest-of-trees layer must fulfill to be coupled into our framework. Essentially, it must be able to describe neighboring relationships between cells in the adapted mesh (apart from hierarchical relationships) across the lower-dimensional objects at the boundary of the cells. Atop this two-layered mesh representation, we build the rest of data structures required for the numerical integration and assembly of the discrete system of linear equations. We consider algorithms that are suitable for both subassembled and fully-assembled distributed data layouts of linear system matrices. The proposed framework has been implemented within the FEMPAR scientific software library, using p4est as a practical forest-of-octrees demonstrator. A comprehensive strong scaling study of this implementation when applied to Poisson and Maxwell problems reveals remarkable scalability up to 32.2K CPU cores and 482.2M degrees of freedom. Besides, the implementation in FEMPAR of the proposed approach is up to 2.6 and 3.4 times faster than the state-of-the-art deal.ii finite element software in the $h$-adaptive approximation of a Poisson problem with first- and second-order Lagrangian finite elements, respectively (excluding the linear solver step from the comparison).", "venue": "SIAM J. Sci. Comput.", "authors": ["Santiago  Badia", "Alberto F. Mart\u00edn", "Eric  Neiva", "Francesc  Verdugo"], "year": 2020, "n_citations": 10}
{"id": 6030359, "s2_id": "28265bc53df1c3af8d5d04bb61ba47b41092d350", "title": "A bi-directional extensible interface between Lean and Mathematica", "abstract": "We implement a user-extensible ad hoc connection between the Lean proof assistant and the computer algebra system Mathematica. By reflecting the syntax of each system in the other and providing a flexible interface for extending translation, our connection allows for the exchange of arbitrary information between the two systems. We show how to make use of the Lean metaprogramming framework to verify certain Mathematica computations, so that the rigor of the proof assistant is not compromised. We also use Mathematica as an untrusted oracle to guide proof search in the proof assistant and interact with a Mathematica notebook from within a Lean session. In the other direction, we import and process Lean declarations from within Mathematica. The proof assistant library serves as a database of mathematical knowledge that the CAS can display and explore.", "venue": "ArXiv", "authors": ["Robert Y. Lewis", "Minchao  Wu"], "year": 2021, "n_citations": 0}
{"id": 6030703, "s2_id": "8981a9636be19ea183532c061686a0b38f7a0c8e", "title": "A Brief Introduction to Automatic Differentiation for Machine Learning", "abstract": "Machine learning, and neural network models in particular, have been improving the state of the art performance on many artificial intelligence related tasks. Neural network models are typically implemented using frameworks that perform gradient based optimization methods to fit a model to a dataset. These frameworks use a technique of calculating derivatives called automatic differentiation (AD) which removes the burden of performing derivative calculations from the model designer. In this report we describe AD, its motivations, and different implementation approaches. We briefly describe dataflow programming as it relates to AD. Lastly, we present example programs that are implemented with Tensorflow and PyTorch, which are two commonly used AD frameworks.", "venue": "ArXiv", "authors": ["Davan  Harrison"], "year": 2021, "n_citations": 0}
{"id": 6035860, "s2_id": "628e800ce9f3958a26f98cfd371bdc4420f16836", "title": "ZKCM: A C++ library for multiprecision matrix computation with applications in quantum information", "abstract": "Abstract ZKCM is a C++ library developed for the purpose of multiprecision matrix computation, on the basis of the GNU MP and MPFR libraries. It provides an easy-to-use syntax and convenient functions for matrix manipulations including those often used in numerical simulations in quantum physics. Its extension library, ZKCM_QC, is developed for simulating quantum computing using the time-dependent matrix-product-state simulation method. This paper gives an introduction about the libraries with practical sample programs. Program Summary Program title: ZKCM Catalogue identifier: AEPI_v1_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEPI_v1_0.html Program obtainable from: CPC Program Library, Queen\u2019s University, Belfast, N. Ireland Licensing provisions: GNU Lesser General Public License, version 3 No. of lines in distributed program, including test data, etc.: 95600 No. of bytes in distributed program, including test data, etc.: 1133481 Distribution format: tar.gz Programming language: C++. Computer: General computers. Operating system: Unix-like systems, such as Linux, Free BSD, Cygwin on Windows OS, etc. RAM: Several mega bytes\u2013several giga bytes, dependent on the problem instance Classification: 4.8, 4.15. External routines: GNU MP (GMP) [1], MPFR [2] Ver. 3.0.0 or later Nature of problem: Multiprecision computation is helpful to guarantee and/or evaluate the accuracy of simulation results in numerical physics. There is a potential demand for a programming library focusing on matrix computation usable for this purpose with a user-friendly syntax. Solution method: A C++ library ZKCM has been developed for multiprecision matrix computation. It provides matrix operations useful for numerical studies of physics, e.g.,\u00a0the tensor product (Kronecker product), the tracing-out operation, the inner product, the LU decomposition, the Hermitian-matrix diagonalization, the singular-value decomposition, and the discrete Fourier transform. For basic floating-point operations, GMP and MPFR libraries are used. An extension library ZKCM QC has also been developed, which employs the time-dependent matrix-product-state method to simulate quantum computing. Restrictions: Multiprecision computation with more than a half thousand bit precision is often a thousand times slower than double-precision computation for any kind of matrix computation. Additional comments: A user\u2019s manual is placed in the directory \u201cdoc\u201d of the package. Each function is explained in a reference manual found in the directories \u201cdoc/html\u201d and \u201cdoc/latex\u201d. Sample programs are placed in the directory \u201csamples\u201d. Running time: It takes less than thirty seconds to obtain a DFT spectrum for 2 16 data points of a time evolution of a quantum system described by a 4 \u00d7 4 matrix Hamiltonian for 256-bit precision when we use recent AMD or Intel CPU with 2.5\u00a0GHz or more CPU frequency. It takes three to five minutes to diagonalize a 100 \u00d7 100 Hermitian matrix for 512-bit precision using the aforementioned CPU. References: [1] The GNU Multiple Precision Arithmetic Library, http://gmplib.org/ . [2] L. Fousse et\u00a0al., MPFR: A multiple-precision binary floating-point library with correct rounding, ACM Trans. Math. Software 33 (2007) 13, http://www.mpfr.org/ .", "venue": "Comput. Phys. Commun.", "authors": ["Akira  SaiToh"], "year": 2013, "n_citations": 10}
{"id": 6038021, "s2_id": "eac764ed4a283302ff6e7063f99853c36c9347ce", "title": "COFFEE - An MPI-parallelized Python package for the numerical evolution of differential equations", "abstract": "COFFEE (ConFormal Field Equation Evolver) is a Python package primarily developed to numerically evolve systems of partial differential equations over time using the method of lines. It includes a variety of time integrators and finite differencing stencils with the summation-by-parts property, as well as pseudo-spectral functionality for angular derivatives of spin-weighted functions. Some additional capabilities include being MPI-parallelisable on a variety of different geometries, HDF data output and post processing scripts to visualize data, and an actions class that allows users to create code for analysis after each timestep.", "venue": "SoftwareX", "authors": ["Georgios  Doulis", "J\u00f6rg  Frauendiener", "Chris  Stevens", "Ben  Whale"], "year": 2019, "n_citations": 2}
{"id": 6043636, "s2_id": "5098e83a1287bb3e0fc0e428eb32500046323f6e", "title": "FEniCS-preCICE: Coupling FEniCS to other Simulation Software", "abstract": "The new software FEniCS-preCICE is a middle software layer, sitting in between the existing finiteelement library FEniCS and the coupling library preCICE. The middle layer simplifies coupling (existing) FEniCS application codes to other simulation software via preCICE. To this end, FEniCSpreCICE converts between FEniCS and preCICE mesh and data structures, provides easy-to-use coupling conditions, and manages data checkpointing for implicit coupling. The new software is a library itself and follows a FEniCS-native style. Only a few lines of additional code are necessary to prepare a FEniCS application code for coupling. We illustrate the functionality of FEniCS-preCICE by two examples: a FEniCS heat conduction code coupled to OpenFOAM and a FEniCS linear elasticity code coupled to SU2. The results of both scenarios are compared with other simulation software showing good agreement.", "venue": "SoftwareX", "authors": ["Benjamin  Rodenberg", "Ishaan  Desai", "Richard  Hertrich", "Alexander  Jaust", "Benjamin  Uekermann"], "year": 2021, "n_citations": 2}
{"id": 6048478, "s2_id": "a6cce69af13af400f60690a0ebe80c62c40257fe", "title": "A comparison of techniques for solving the Poisson equation in CFD", "abstract": "CFD is a ubiquitous technique central to much of computational simulation such as that required by aircraft design. Solving of the Poisson equation occurs frequently in CFD and there are a number of possible approaches one may leverage. The dynamical core of the MONC atmospheric model is one example of CFD which requires the solving of the Poisson equation to determine pressure terms. Traditionally this aspect of the model has been very time consuming and-so it is important to consider how we might reduce the runtime cost. In this paper we survey the different approaches implemented in MONC to perform the pressure solve. Designed to take advantage of large scale, modern, HPC machines, we are concerned with the computation and communication behaviour of the available techniques and in this text we focus on direct FFT and indirect iterative methods. In addition to describing the implementation of these techniques we illustrate on up to 32768 processor cores of a Cray XC30 both the performance and scalability of our approaches. Raw runtime is not the only measure so we also make some comments around the stability and accuracy of solution. The result of this work are a number of techniques, optimised for large scale HPC systems, and an understanding of which is most appropriate in different situations.", "venue": "ArXiv", "authors": ["N. A. P. Brown"], "year": 2020, "n_citations": 1}
{"id": 6054770, "s2_id": "f2614cd5a8e4df34bb9c111cd75205e2500acc2b", "title": "Review of theory and implementation of hyper-dual numbers for first and second order automatic differentiation", "abstract": "In this review we present hyper-dual numbers as a tool for the automatic differentiation of computer programs via operator overloading. \nWe start with a motivational introduction into the ideas of algorithmic differentiation. Then we illuminate the concepts behind operator overloading and dual numbers. \nAfterwards, we present hyper-dual numbers (and vectors) as an extension of dual numbers for the computation of the Jacobian and the Hessian matrices of a computer program. We review a mathematical theorem that proves the correctness of the derivative information that is obtained from hyper-dual numbers. \nFinally, we refer to a freely available implementation of a hyper-dual number class in Matlab. We explain an interface that can be called with a function as argument such that the Jacobian and Hessian of this function are returned.", "venue": "ArXiv", "authors": ["Martin  Neuenhofen"], "year": 2018, "n_citations": 4}
{"id": 6063906, "s2_id": "1e7bbe997d366901f4bb16b08f1a3add15f8cec9", "title": "The QWalk simulator of quantum walks", "abstract": "Several research groups are giving special attention to quantum walks recently, because this research area have been used with success in the development of new efficient quantum algorithms. A general simulator of quantum walks is very important for the development of this area, since it allows the researchers to focus on the mathematical and physical aspects of the research instead of deviating the efforts to the implementation of specific numerical simulations. In this paper we present QWalk, a quantum walk simulator for one- and two-dimensional lattices. Finite two-dimensional lattices with generic topologies can be used. Decoherence can be simulated by performing measurements or by breaking links of the lattice. We use examples to explain the usage of the software and to show some recent results of the literature that are easily reproduced by the simulator.", "venue": "Comput. Phys. Commun.", "authors": ["Franklin L. Marquezino", "Renato  Portugal"], "year": 2008, "n_citations": 15}
{"id": 6070407, "s2_id": "1554887c6bd76c443a477b27dbcab35877787b27", "title": "LightSeq: A High Performance Inference Library for Transformers", "abstract": "Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow. Experimental results on standard machine translation benchmarks show that achieves up to 14x speedup compared with TensorFlow and 1.4x speedup compared with , a concurrent CUDA implementation. The code will be released publicly after the review.", "venue": "NAACL", "authors": ["Xiaohui  Wang", "Ying  Xiong", "Yang  Wei", "Mingxuan  Wang", "Lei  Li"], "year": 2021, "n_citations": 6}
{"id": 6071769, "s2_id": "de344cf29ba32b452ec6cf99727895b7eccbe0e0", "title": "pyFFS: A Python Library for Fast Fourier Series Computation", "abstract": "Fourier transforms are an often necessary component in many computational tasks, and can be computed efficiently through the fast Fourier transform (FFT) algorithm. However, many applications involve an underlying continuous signal, and a more natural choice would be to work with e.g. the Fourier series (FS) coefficients in order to avoid the additional overhead of translating between the analog and discrete domains. Unfortunately, there exists very little literature and tools for the manipulation of FS coefficients from discrete samples. This paper introduces a Python library called pyFFS for efficient FS coefficient computation, convolution, and interpolation. While the libraries SciPy and NumPy provide efficient functionality for discrete Fourier transform coefficients via the FFT algorithm, pyFFS addresses the computation of FS coefficients through what we call the fast Fourier series (FFS). Moreover, pyFFS includes an FS interpolation method based on the chirp Z-transform that can make it more than an order of magnitude faster than the SciPy equivalent when one wishes to perform interpolation. GPU support through the CuPy library allows for further acceleration, e.g. an order of magnitude faster for computing the 2-D FS coefficients of 1000\u00d71000 samples and nearly two orders of magnitude faster for 2-D interpolation. As an application, we discuss the use of pyFFS in Fourier optics. pyFFS is available as an open source package at https://github.com/imagingofthings/pyFFS, with documentation at https://pyffs.readthedocs.io.", "venue": "ArXiv", "authors": ["Eric  Bezzam", "Sepand  Kashani", "Paul  Hurley", "Matthieu  Simeoni"], "year": 2021, "n_citations": 0}
{"id": 6076280, "s2_id": "3a37dab6c5287844669fa1061d69757d95e3a0b7", "title": "Computing the sparse matrix vector product using block-based kernels without zero padding on processors with AVX-512 instructions", "abstract": "The sparse matrix-vector product (SpMV) is a fundamental operation in many scientific applications from various fields. The High Performance Computing (HPC) community has therefore continuously invested a lot of effort to provide an efficient SpMV kernel on modern CPU architectures. Although it has been shown that block-based kernels help to achieve high performance, they are difficult to use in practice because of the zero padding they require. In the current paper, we propose new kernels using the AVX-512 instruction set, which makes it possible to use a blocking scheme without any zero padding in the matrix memory storage. We describe mask-based sparse matrix formats and their corresponding SpMV kernels highly optimized in assembly language. Considering that the optimal blocking size depends on the matrix, we also provide a method to predict the best kernel to be used utilizing a simple interpolation of results from previous executions. We compare the performance of our approach to that of the Intel MKL CSR kernel and the CSR5 open-source package on a set of standard benchmark matrices. We show that we can achieve significant improvements in many cases, both for sequential and for parallel executions. Finally, we provide the corresponding code in an open source library, called SPC5.", "venue": "PeerJ Comput. Sci.", "authors": ["B\u00e9renger  Bramas", "Pavel  Kus"], "year": 2018, "n_citations": 5}
{"id": 6078594, "s2_id": "3a0d1781768a86690f2ffe49d3c29c9b1d457a94", "title": "Optimizing Xeon Phi for Interactive Data Analysis", "abstract": "The Intel Xeon Phi manycore processor is designed to provide high performance matrix computations of the type often performed in data analysis. Common data analysis environments include Matlab, GNU Octave, Julia, Python, and R. Achieving optimal performance of matrix operations within data analysis environments requires tuning the Xeon Phi OpenMP settings, process pinning, and memory modes. This paper describes matrix multiplication performance results for Matlab and GNU Octave over a variety of combinations of process counts and OpenMP threads and Xeon Phi memory modes. These results indicate that using KMP_AFFINITY=granlarity=fine, taskset pinning, and all2all cache memory mode allows both Matlab and GNU Octave to achieve 66% of the practical peak performance for process counts ranging from 1 to 64 and OpenMP threads ranging from 1 to 64. These settings have resulted in generally improved performance across a range of applications and has enabled our Xeon Phi system to deliver significant results in a number of real-world applications.", "venue": "2019 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Chansup  Byun", "Jeremy  Kepner", "William  Arcand", "David  Bestor", "William  Bergeron", "Matthew  Hubbell", "Vijay  Gadepally", "Michael  Houle", "Michael  Jones", "Anne  Klein", "Lauren  Milechin", "Peter  Michaleas", "Julie  Mullen", "Andrew  Prout", "Antonio  Rosa", "Siddharth  Samsi", "Charles  Yee", "Albert  Reuther"], "year": 2019, "n_citations": 5}
{"id": 6087737, "s2_id": "ae7162cac65d75af15c4ca61c721204a6a33ae42", "title": "Using Premia and Nsp for constructing a risk management benchmark for testing parallel architecture", "abstract": "Financial institutions have massive computations to carry out overnight which are very demanding in terms of the consumed CPU. The challenge is to price many different products on a cluster-like architecture. We have used the Premia software to valuate the financial derivatives. In this work, we explain how Premia can be embedded into Nsp, a scientific software like Matlab, to provide a powerful tool to valuate a whole portfolio. Finally, we have integrated an MPI toolbox into Nsp to enable to use Premia to solve a bunch of pricing problems on a cluster. This unified framework can then be used to test different parallel architectures.", "venue": "2009 IEEE International Symposium on Parallel & Distributed Processing", "authors": ["Jean-Philippe  Chancelier", "Bernard  Lapeyre", "J\u00e9r\u00f4me  Lelong"], "year": 2009, "n_citations": 4}
{"id": 6095254, "s2_id": "306510f3e899d212329f7e5fb71d7da1b08cbfd5", "title": "OGRe: An Object-Oriented General Relativity Package for Mathematica", "abstract": "OGRe is a modern Mathematica package for differential geometry and tensor calculus. It can be used in a variety of contexts where tensor calculations are needed, in both mathematics and physics, but it is especially suitable for general relativity \u2014 the field of physics where tensors are most commonly and ubiquitously used. Whether the user is doing cutting-edge research in general relativity or just making first steps in learning the theory, the ability to manipulate tensors and perform tensor calculations quickly, easily, and intuitively will greatly simplify and accelerate their work.", "venue": "Journal of Open Source Software", "authors": ["Barak  Shoshany"], "year": 2021, "n_citations": 0}
{"id": 6097139, "s2_id": "3d8aef7b08b50b0e6ac345621e67cbb55cdc1250", "title": "A review of error estimation in adaptive quadrature", "abstract": "The most critical component of any adaptive numerical quadrature routine is the estimation of the integration error. Since the publication of the first algorithms in the 1960s, many error estimation schemes have been presented, evaluated, and discussed. This article presents a review of existing error estimation techniques and discusses their differences and their common features. Some common shortcomings of these algorithms are discussed, and a new general error estimation technique is presented.", "venue": "CSUR", "authors": ["Pedro  Gonnet"], "year": 2012, "n_citations": 32}
{"id": 6101542, "s2_id": "fe4c269558f569ce89dcdff31c88f79b5b30804d", "title": "On the effects of scaling on the performance of Ipopt", "abstract": "The open-source nonlinear solver Ipopt (this https URL) is a widely-used software package for the solution of large-scale non-linear optimization problems. At its heart, it employs a third-party linear solver to solve a series of sparse symmetric indefinite systems. The speed, accuracy and robustness of the chosen linear solver is critical to the overall performance of Ipopt. In some instances, it can be beneficial to scale the linear system before it is solved. \nIn this paper, different scaling algorithms are employed within Ipopt with a new linear solver HSL_MA97 from the HSL mathematical software library (this http URL). An extensive collection of problems from the CUTEr test set (this http URL) is used to illustrate the effects of scaling.", "venue": "ArXiv", "authors": ["Jonathan D. Hogg", "Jennifer A. Scott"], "year": 2013, "n_citations": 4}
{"id": 6103377, "s2_id": "c4ee0a9124c5cb289fcf604844d10bc8f9c2bae3", "title": "Matlab vs. OpenCV: A Comparative Study of Different Machine Learning Algorithms", "abstract": "Scientific Computing relies on executing computer algorithms coded in some programming languages. Given a particular available hardware, algorithms speed is a crucial factor. There are many scientific computing environments used to code such algorithms. Matlab is one of the most tremendously successful and widespread scientific computing environments that is rich of toolboxes, libraries, and data visualization tools. OpenCV is a (C++)-based library written primarily for Computer Vision and its related areas. This paper presents a comparative study using 20 different real datasets to compare the speed of Matlab and OpenCV for some Machine Learning algorithms. Although Matlab is more convenient in developing and data presentation, OpenCV is much faster in execution, where the speed ratio reaches more than 80 in some cases. The best of two worlds can be achieved by exploring using Matlab or similar environments to select the most successful algorithm; then, implementing the selected algorithm using OpenCV or similar environments to gain a speed factor.", "venue": "ArXiv", "authors": ["Ahmed A. Elsayed", "Waleed A. Yousef"], "year": 2019, "n_citations": 2}
{"id": 6119980, "s2_id": "88daa04777f3ce028fa1d68b222e06a0449912a8", "title": "Research on the fast Fourier transform of image based on GPU", "abstract": "Study of general purpose computation by GPU (Graphics Processing Unit) can improve the image processing capability of micro-computer system. This paper studies the parallelism of the different stages of decimation in time radix 2 FFT algorithm, designs the butterfly and scramble kernels and implements 2D FFT on GPU. The experiment result demonstrates the validity and advantage over general CPU, especially in the condition of large input size. The approach can also be generalized to other transforms alike.", "venue": "ArXiv", "authors": ["Feifei  Shen", "Zhenjian  Song", "Congrui  Wu", "Jiaqi  Geng", "Qingyun  Wang"], "year": 2015, "n_citations": 0}
{"id": 6121385, "s2_id": "5612748b26fccd0df77e5c091b15fe0bce969022", "title": "Optimally Tuned Iterative Reconstruction Algorithms for Compressed Sensing", "abstract": "We conducted an extensive computational experiment, lasting multiple CPU-years, to optimally select parameters for two important classes of algorithms for finding sparse solutions of underdetermined systems of linear equations. We make the optimally tuned implementations available at sparselab.stanford.edu; they run \u00bfout of the box\u00bf with no user tuning: it is not necessary to select thresholds or know the likely degree of sparsity. Our class of algorithms includes iterative hard and soft thresholding with or without relaxation, as well as CoSaMP, subspace pursuit and some natural extensions. As a result, our optimally tuned algorithms dominate such proposals. Our notion of optimality is defined in terms of phase transitions, i.e., we maximize the number of nonzeros at which the algorithm can successfully operate. We show that the phase transition is a well-defined quantity with our suite of random underdetermined linear systems. Our tuning gives the highest transition possible within each class of algorithms. We verify by extensive computation the robustness of our recommendations to the amplitude distribution of the nonzero coefficients as well as the matrix ensemble defining the underdetermined system. Our findings include the following. 1) For all algorithms, the worst amplitude distribution for nonzeros is generally the constant-amplitude random-sign distribution, where all nonzeros are the same amplitude. 2) Various random matrix ensembles give the same phase transitions; random partial isometries may give different transitions and require different tuning. 3) Optimally tuned subspace pursuit dominates optimally tuned CoSaMP, particularly so when the system is almost square.", "venue": "IEEE Journal of Selected Topics in Signal Processing", "authors": ["Arian  Maleki", "David L. Donoho"], "year": 2010, "n_citations": 277}
{"id": 6121905, "s2_id": "6e8dd3369a48444a4d836dac53b58568b20addd7", "title": "Conjugate gradient solvers on Intel Xeon Phi and NVIDIA GPUs", "abstract": "Lattice Quantum Chromodynamics simulations typically spend most of the runtime in inversions of the Fermion Matrix. This part is therefore frequently optimized for various HPC architectures. Here we compare the performance of the Intel Xeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate gradient solver. By exposing more parallelism to the accelerator through inverting multiple vectors at the same time, we obtain a performance greater than 300 GFlop/s on both architectures. This more than doubles the performance of the inversions. We also give a short overview of the Knights Corner architecture, discuss some details of the implementation and the effort required to obtain the achieved performance.", "venue": "ArXiv", "authors": ["O.  Kaczmarek", "Christian  Schmidt", "P.  Steinbrecher", "M.  Wagner"], "year": 2014, "n_citations": 7}
{"id": 6123968, "s2_id": "11ab9c067e0658ebe93b5609176204061329a0cc", "title": "Role-Oriented Code Generation in an Engine for Solving Hyperbolic PDE Systems", "abstract": "The development of a high performance PDE solver requires the combined expertise of interdisciplinary teams with respect to application domain, numerical scheme and low-level optimization. In this paper, we present how the ExaHyPE engine facilitates the collaboration of such teams by isolating three roles: application, algorithms, and optimization expert. We thus support team members in letting them focus on their own area of expertise while integrating their contributions into an HPC production code. Inspired by web application development practices, ExaHyPE relies on two custom code generation modules, the Toolkit and the Kernel Generator, which follow a Model-View-Controller architectural pattern on top of the Jinja2 template engine library. Using Jinja2's templates to abstract the critical components of the engine and generated glue code, we isolate the application development from the engine. The template language also allows us to define and use custom template macros that isolate low-level optimizations from the numerical scheme described in the templates. We present three use cases, each focusing on one of our user roles, showcasing how the design of the code generation modules allows to easily expand the solver schemes to support novel demands from applications, to add optimized algorithmic schemes (with reduced memory footprint, e.g.), or provide improved low-level SIMD vectorization support.", "venue": "HUST/SE-HER/WIHPC@SC", "authors": ["Jean-Matthieu  Gallard", "Lukas  Krenz", "Leonhard  Rannabauer", "Anne  Reinarz", "Michael  Bader"], "year": 2019, "n_citations": 1}
{"id": 6129998, "s2_id": "88cc17a72958b578de8bee5c2588b74964f21907", "title": "On the Performance Prediction of BLAS-based Tensor Contractions", "abstract": "Tensor operations are surging as the computational building blocks for a variety of scientific simulations and the development of high-performance kernels for such operations is known to be a challenging task. While for operations on one- and two-dimensional tensors there exist standardized interfaces and highly-optimized libraries (BLAS), for higher dimensional tensors neither standards nor highly-tuned implementations exist yet. In this paper, we consider contractions between two tensors of arbitrary dimensionality and take on the challenge of generating high-performance implementations by resorting to sequences of BLAS kernels. The approach consists in breaking the contraction down into operations that only involve matrices or vectors. Since in general there are many alternative ways of decomposing a contraction, we are able to methodically derive a large family of algorithms. The main contribution of this paper is a systematic methodology to accurately identify the fastest algorithms in the bunch, without executing them. The goal is instead accomplished with the help of a set of cache-aware micro-benchmarks for the underlying BLAS kernels. The predictions we construct from such benchmarks allow us to reliably single out the best-performing algorithms in a tiny fraction of the time taken by the direct execution of the algorithms.", "venue": "PMBS@SC", "authors": ["Elmar  Peise", "Diego  Fabregat-Traver", "Paolo  Bientinesi"], "year": 2014, "n_citations": 15}
{"id": 6131099, "s2_id": "e4805fb4ca47212c5bc6c642d550a47276dbe000", "title": "How to Run Mathematica Batch-files in Background ?", "abstract": "Mathematica is a versatile equipment for doing numeric and symbolic computations and it has wide spread applications in all branches of science. Mathematica has a complete consistency to design it at every stage that gives it multilevel capability and helps advanced usage evolve naturally. Mathematica functions work for any precision of number and it can be easily computed with symbols, represented graphically to get the best answer. Mathematica is a robust software development that can be used in any popular operating systems and it can be communicated with external programs by using proper mathlink commands. \nSometimes it is quite desirable to run jobs in background of a computer which can take considerable amount of time to finish, and this allows us to do work on other tasks, while keeping the jobs running. Most of us are very familiar to run jobs in background for the programs written in the languages like C, C++, F77, F90, F95, etc. But the way of running jobs, written in a mathematica notebook, in background is quite different from the conventional method. In this article, we explore how to create a mathematica batch-file from a mathematica notebook and run it in background. Here we concentrate our study only for the Unix version, but one can run mathematica programs in background for the Windows version as well by using proper mathematica batch-file.", "venue": "ArXiv", "authors": ["Santanu K. Maiti"], "year": 2006, "n_citations": 2}
{"id": 6131110, "s2_id": "036116307e40e7b249c9a1443dcced8a9b76f44b", "title": "PQser: a Matlab package for spectral seriation", "abstract": "Seriation is an important ordering problem which consists of finding the best ordering of a set of units whose interrelationship is defined by a bipartite graph. It has important applications in, e.g., archaeology, anthropology, psychology, and biology. This paper presents a Matlab implementation of an algorithm for spectral seriation by Atkins et al., based on the use of the Fiedler vector of the Laplacian matrix associated to the problem, which encodes the set of admissible solutions into a PQ-tree. We introduce some numerical technicalities in the original algorithm to improve its performance, and point out that the presence of a multiple Fiedler value may have a substantial influence on the computation of an approximated solution, in the presence of inconsistent data sets. Practical examples and numerical experiments show how to use the toolbox to process data sets deriving from real-world applications.", "venue": "Numerical Algorithms", "authors": ["Anna  Concas", "Caterina  Fenu", "Giuseppe  Rodriguez"], "year": 2018, "n_citations": 7}
{"id": 6142206, "s2_id": "1d3510ac773933380be94be2032502e741ba003d", "title": "A Practical Guide to Randomized Matrix Computations with MATLAB Implementations", "abstract": "Matrix operations such as matrix inversion, eigenvalue decomposition, singular value decomposition are ubiquitous in real-world applications. Unfortunately, many of these matrix operations so time and memory expensive that they are prohibitive when the scale of data is large. In real-world applications, since the data themselves are noisy, machine-precision matrix operations are not necessary at all, and one can sacrice a reasonable amount of accuracy for", "venue": "ArXiv", "authors": ["Shusen  Wang"], "year": 2015, "n_citations": 25}
{"id": 6143648, "s2_id": "427826ee0aabc9c4fbd6946be1eb3fb2ecff1599", "title": "Building pattern recognition applications with the SPARE library", "abstract": "This paper presents the SPARE C++ library, an open source software tool conceived to build pattern recognition and soft computing systems. The library follows the requirement of the generality: most of the implemented algorithms are able to process user-defined input data types transparently, such as labeled graphs and sequences of objects, as well as standard numeric vectors. Here we present a high-level picture of the SPARE library characteristics, focusing instead on the specific practical possibility of constructing pattern recognition systems for different input data types. In particular, as a proof of concept, we discuss two application instances involving clustering of real-valued multidimensional sequences and classification of labeled graphs.", "venue": "ArXiv", "authors": ["Lorenzo  Livi", "Guido Del Vescovo", "Antonello  Rizzi", "Fabio Massimo Frattale Mascioli"], "year": 2014, "n_citations": 12}
{"id": 6149998, "s2_id": "c1f3fb4a7698a72575a6b445c1cc9927689b3cb3", "title": "High-Performance Derivative Computations using CoDiPack", "abstract": "There are several AD tools available that all implement different strategies for the reverse mode of AD. The most common strategies are primal value taping (implemented e.g. by ADOL-C) and Jacobian taping (implemented e.g. by Adept and dco/c++). Particulary for Jacobian taping, recent advances using expression templates make it very attractive for large scale software. However, the current implementations are either closed source or miss essential features and flexibility. Therefore, we present the new AD tool CoDiPack (Code Differentiation Package) in this paper. It is specifically designed for minimal memory consumption and optimal runtime, such that it can be used for the differentiation of large scale software. An essential part of the design of CoDiPack is the modular layout and the recursive data structures which not only allow the efficient implementation of the Jacobian taping approach but will also enable other approaches like the primal value taping or new research ideas. We will finally present the performance values of CoDiPack on a generic PDE example and on the SU2 code.", "venue": "ACM Trans. Math. Softw.", "authors": ["Max  Sagebaum", "Tim  Albring", "Nicolas R. Gauger"], "year": 2019, "n_citations": 53}
{"id": 6150872, "s2_id": "119d65ac264e64835fd9f602d9f23041b8c8132e", "title": "General Semiparametric Shared Frailty Model Estimation and Simulation with frailtySurv", "abstract": "The R package frailtySurv for simulating and fitting semi-parametric shared frailty models is introduced. Package frailtySurv implements semi-parametric consistent estimators for a variety of frailty distributions, including gamma, log-normal, inverse Gaussian and power variance function, and provides consistent estimators of the standard errors of the parameters' estimators. The parameters' estimators are asymptotically normally distributed, and therefore statistical inference based on the results of this package, such as hypothesis testing and confidence intervals, can be performed using the normal distribution. Extensive simulations demonstrate the flexibility and correct implementation of the estimator. Two case studies performed with publicly available datasets demonstrate applicability of the package. In the Diabetic Retinopathy Study, the onset of blindness is clustered by patient, and in a large hard drive failure dataset, failure times are thought to be clustered by the hard drive manufacturer and model.", "venue": "Journal of statistical software", "authors": ["John V. Monaco", "Malka  Gorfine", "Li  Hsu"], "year": 2018, "n_citations": 5}
{"id": 6152439, "s2_id": "38fb6b42f4c1cd1aa9ba2c54821c3fd42e6235f7", "title": "LSMR: An iterative algorithm for sparse least-squares problems", "abstract": "An iterative method LSMR is presented for solving linear systems Ax = b and leastsquares problems min \u2016Ax\u2212b\u20162, with A being sparse or a fast linear operator. LSMR is based on the Golub-Kahan bidiagonalization process. It is analytically equivalent to the MINRES method applied to the normal equation ATAx = ATb, so that the quantities \u2016Ark\u2016 are monotonically decreasing (where rk = b\u2212Axk is the residual for the current iterate xk). We observe in practice that \u2016rk\u2016 also decreases monotonically, so that compared to LSQR (for which only \u2016rk\u2016 is monotonic) it is safer to terminate LSMR early. We also report some experiments with reorthogonalization.", "venue": "ArXiv", "authors": ["David Chin-Lung Fong", "Michael A. Saunders"], "year": 2010, "n_citations": 226}
{"id": 6153011, "s2_id": "7033e39136dd66d63b34f493e1d60ee0b652b831", "title": "juSFEM: A Julia-based Open-source Package of Parallel Smoothed Finite Element Method (S-FEM) for Elastic Problems", "abstract": "Abstract The Smoothed Finite Element Method (S-FEM) proposed by Liu G.R. can achieve more accurate results than the conventional FEM. Currently, much commercial software and many open-source packages have been developed to analyze various science and engineering problems using the FEM. However, there is little work focusing on designing and developing software or packages for the S-FEM. In this paper, we design and implement an open-source package of the parallel S-FEM for elastic problems by utilizing the Julia language on multi-core CPU. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing. We term our package as juSFEM. To the best of the authors\u2019 knowledge, juSFEM is the first package of parallel S-FEM developed with the Julia language. To verify the correctness and evaluate the efficiency of juSFEM, two groups of benchmark tests are conducted. The benchmark results show that (1) juSFEM can achieve accurate results when compared to commercial FEM software ABAQUS, and (2) juSFEM only requires 543\u00a0s to calculate the displacements of a 3D elastic cantilever beam model which is composed of approximately 2 million tetrahedral elements, while in contrast the commercial FEM software needs 930\u00a0s for the same calculation model; (3) the parallel juSFEM executed on the 24-core CPU is approximately 20 \u00d7 faster than the corresponding serial version. Moreover, the structure and function of juSFEM are easily modularized, and the code in juSFEM is clear and readable, which is convenient for further development.", "venue": "Comput. Math. Appl.", "authors": ["Zenan  Huo", "Gang  Mei", "Nengxiong  Xu"], "year": 2021, "n_citations": 3}
{"id": 6157887, "s2_id": "b14b472fa38e89991c4c501d1684b3936644cf87", "title": "Polcovar: Software for Computing the Mean and Variance of Subgraph Counts in Random Graphs", "abstract": "The mean and variance of the number of appearances of a given subgraph $H$ in an Erd\\H{o}s--R\\'enyi random graph over $n$ nodes are rational polynomials in $n$. We present a piece of software named Polcovar (from \"polynomial\" and \"covariance\") that computes the exact rational coefficients of these polynomials in function of $H$.", "venue": "ArXiv", "authors": ["J\u00e9r\u00f4me  Kunegis"], "year": 2014, "n_citations": 0}
{"id": 6162460, "s2_id": "a060d2eaac67dabd0064885d3a3933fa0ba6b787", "title": "Making big steps in trajectories", "abstract": "We consider the solution of initial value problems within the context of hybrid systems and emphasise use of high precision approximations (in software for exact real arithmetic). We propose a novel algorithm for the computation of trajectories up to the area where discontinuous jumps appear, applicable for holomorphic flow functions. Examples with a prototypical implementation illustrate that the algorithm might provide results with higher precision than well-known ODE solvers at a\nsimilar computation time.", "venue": "CCA", "authors": ["Norbert Th. M\u00fcller", "Margarita V. Korovina"], "year": 2010, "n_citations": 7}
{"id": 6163100, "s2_id": "697b2bc7925a51568d2d558491c44dcb2efd22bd", "title": "MADNESS: A Multiresolution, Adaptive Numerical Environment for Scientific Simulation", "abstract": "MADNESS (multiresolution adaptive numerical environment for scientific simulation) is a high-level software environment for solving integral and differential equations in many dimensions that uses adaptive and fast harmonic analysis methods with guaranteed precision that are based on multiresolution analysis and separated representations. Underpinning the numerical capabilities is a powerful petascale parallel programming environment that aims to increase both programmer productivity and code scalability. This paper describes the features and capabilities of MADNESS and briefly discusses some current applications in chemistry and several areas of physics.", "venue": "SIAM J. Sci. Comput.", "authors": ["Robert J. Harrison", "Gregory  Beylkin", "Florian A. Bischoff", "Justus A. Calvin", "George I. Fann", "Jacob  Fosso-Tande", "Diego  Galindo", "Jeff R. Hammond", "Rebecca  Hartman-Baker", "Judith C. Hill", "Jun  Jia", "Jakob S. Kottmann", "M.-J. Yvonne Ou", "Junchen  Pei", "Laura E. Ratcliff", "Matthew G. Reuter", "Adam C. Richie-Halford", "Nichols A. Romero", "Hideo  Sekino", "William A. Shelton", "Bryan E. Sundahl", "W. Scott Thornton", "Edward F. Valeev", "\u00c1lvaro  V\u00e1zquez-Mayagoitia", "Nicholas  Vence", "Takeshi  Yanai", "Yukina  Yokoi"], "year": 2016, "n_citations": 58}
{"id": 6165546, "s2_id": "5a3613a7682069dc4d275b51fa7384e2576fc641", "title": "Verifying Integer Programming Results", "abstract": "Software for mixed-integer linear programming can return incorrect results for a number of reasons, one being the use of inexact floating-point arithmetic. Even solvers that employ exact arithmetic may suffer from programming or algorithmic errors, motivating the desire for a way to produce independently verifiable certificates of claimed results. Due to the complex nature of state-of-the-art MIP solution algorithms, the ideal form of such a certificate is not entirely clear. This paper proposes such a certificate format designed with simplicity in mind, which is composed of a list of statements that can be sequentially verified using a limited number of inference rules. We present a supplementary verification tool for compressing and checking these certificates independently of how they were created. We report computational results on a selection of MIP instances from the literature. To this end, we have extended the exact rational version of the MIP solver SCIP to produce such certificates.", "venue": "IPCO", "authors": ["Kevin K. H. Cheung", "Ambros M. Gleixner", "Daniel E. Steffy"], "year": 2017, "n_citations": 11}
{"id": 6168526, "s2_id": "3cbc28b2be6057340fec5da4a270c5de94ff39fe", "title": "Hardware-Oriented Krylov Methods for High-Performance Computing", "abstract": "Krylov subspace methods are an essential building block in numerical simulation software. The efficient utilization of modern hardware is a challenging problem in the development of these methods. In this work, we develop Krylov subspace methods to solve linear systems with multiple right-hand sides, tailored to modern hardware in high-performance computing. To this end, we analyze an innovative block Krylov subspace framework that allows to balance the computational and data-transfer costs to the hardware. Based on the framework, we formulate commonly used Krylov methods. For the CG and BiCGStab methods, we introduce a novel stabilization approach as an alternative to a deflation strategy. This helps us to retain the block size, thus leading to a simpler and more efficient implementation. In addition, we optimize the methods further for distributed memory systems and the communication overhead. For the CG method, we analyze approaches to overlap the communication and computation and present multiple variants of the CG method, which differ in their communication properties. Furthermore, we present optimizations of the orthogonalization procedure in the GMRes method. Beside introducing a pipelined Gram-Schmidt variant that overlaps the global communication with the computation of inner products, we present a novel orthonormalization method based on the TSQR algorithm, which is communication-optimal and stable. For all optimized method, we present tests that show their superiority in a distributed setting.", "venue": "ArXiv", "authors": ["Nils-Arne  Dreier"], "year": 2021, "n_citations": 0}
{"id": 6175439, "s2_id": "ac03394078e9018224119914177fcd14d0e8d632", "title": "OpenGM: A C++ Library for Discrete Graphical Models", "abstract": "OpenGM is a C++ template library for defining discrete graphical models and performing inference on these models, using a wide range of state-of-the-art algorithms. No restrictions are imposed on the factor graph to allow for higher-order factors and arbitrary neighborhood structures. Large models with repetitive structure are handled efficiently because (i) functions that occur repeatedly need to be stored only once, and (ii) distinct functions can be implemented differently, using different encodings alongside each other in the same model. Several parametric functions (e.g. metrics), sparse and dense value tables are provided and so is an interface for custom C++ code. Algorithms are separated by design from the representation of graphical models and are easily exchangeable. OpenGM, its algorithms, HDF5 file format and command line tools are modular and extendible.", "venue": "ArXiv", "authors": ["Bj\u00f6rn  Andres", "Thorsten  Beier", "J\u00f6rg H. Kappes"], "year": 2012, "n_citations": 100}
{"id": 6177165, "s2_id": "8e76bb806667804cc2a1ecea7ed083cc80c5c83f", "title": "GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU", "abstract": "Author(s): Yang, Carl; Buluc, Aydin; Owens, John D | Abstract: High-performance implementations of graph algorithms are challenging to implement on new parallel hardware such as GPUs, because of three challenges: (1) difficulty of coming up with graph building blocks, (2) load imbalance on parallel hardware, and (3) graph problems having low arithmetic intensity. To address these challenges, GraphBLAS is an innovative, on-going effort by the graph analytics community to propose building blocks based in sparse linear algebra, which will allow graph algorithms to be expressed in a performant, succinct, composable and portable manner. In this paper, we examine the performance challenges of a linear algebra-based approach to building graph frameworks and describe new design principles for overcoming these bottlenecks. Among the new design principles is exploiting input sparsity, which allows users to write graph algorithms without specifying push and pull direction. Exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed. Load-balancing is an important feature for balancing work amongst parallel workers. We describe the important load-balancing features for handling graphs with different characteristics. The design principles described in this paper have been implemented in \"GraphBLAST\", the first open-source linear algebra-based graph framework on GPU targeting high-performance computing. The results show that on a single GPU, GraphBLAST has on average at least an order of magnitude speedup over previous GraphBLAS implementations SuiteSparse and GBTL, comparable performance to the fastest GPU hardwired primitives and shared-memory graph frameworks Ligra and Gunrock, and better performance than any other GPU graph framework, while offering a simpler and more concise programming model.", "venue": "ArXiv", "authors": ["Carl  Yang", "Aydin  Bulu\u00e7", "John D. Owens"], "year": 2019, "n_citations": 27}
{"id": 6178303, "s2_id": "0d082603565d38d3b47658ffb04e127f8a622382", "title": "AD in Fortran, Part 2: Implementation via Prepreprocessor", "abstract": "We describe an implementation of the Farfel Fortran77 AD extensions (Radul et al. AD in Fortran, Part 1: Design (2012), http://arxiv.org/abs/1203.1448). These extensions integrate forward and reverse AD directly into the programming model, with attendant benefits to flexibility, modularity, and ease of use. The implementation we describe is a \u201cprepreprocessor\u201d that generates input to existing Fortran-based AD tools. In essence, blocks of code which are targeted for AD by Farfel constructs are put into subprograms which capture their lexical variable context, and these are closure-converted into top-level subprograms and specialized to eliminate Open image in new window arguments, rendering them amenable to existing AD preprocessors, which are then invoked, possibly repeatedly if the AD is nested.", "venue": "ArXiv", "authors": ["Alexey  Radul", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "year": 2012, "n_citations": 3}
{"id": 6179097, "s2_id": "be1214061323bb8b986dfacb7fcd0545f4e0f164", "title": "ParMooN - A modernized program package based on mapped finite elements", "abstract": "ParMooN is a program package for the numerical solution of elliptic and parabolic partial differential equations. It inherits the distinct features of its predecessor MooNMD (John and Matthies, 2004): strict decoupling of geometry and finite element spaces, implementation of mapped finite elements as their definition can be found in textbooks, and a geometric multigrid preconditioner with the option to use different finite element spaces on different levels of the multigrid hierarchy. After having presented some thoughts about in-house research codes, this paper focuses on aspects of the parallelization for a distributed memory environment, which is the main novelty of ParMooN. Numerical studies, performed on compute servers, assess the efficiency of the parallelized geometric multigrid preconditioner in comparison with some parallel solvers that are available in the library PETSc. The results of these studies give a first indication whether the cumbersome implementation of the parallelized geometric multigrid method was worthwhile or not.", "venue": "Comput. Math. Appl.", "authors": ["Ulrich  Wilbrandt", "Clemens  Bartsch", "Naveed  Ahmed", "Najib  Alia", "Felix  Anker", "Laura  Blank", "Alfonso  Caiazzo", "Sashikumaar  Ganesan", "Swetlana  Giere", "Gunar  Matthies", "Raviteja  Meesala", "Abdus  Shamim", "Jagannath  Venkatesan", "Volker  John"], "year": 2017, "n_citations": 37}
{"id": 6180309, "s2_id": "9d939383dcf303a67568efc552f89b6362eb8484", "title": "A Study on the Influence of Caching: Sequences of Dense Linear Algebra Kernels", "abstract": "It is universally known that caching is critical to attain high-performance implementations: In many situations, data locality (in space and time) plays a bigger role than optimizing the (number of) arithmetic floating point operations. In this paper, we show evidence that at least for linear algebra algorithms, caching is also a crucial factor for accurate performance modeling and performance prediction.", "venue": "VECPAR", "authors": ["Elmar  Peise", "Paolo  Bientinesi"], "year": 2014, "n_citations": 9}
{"id": 6185318, "s2_id": "2de26997917b3dbffa2cea6b71c157ecf7e207cc", "title": "Geometric Sparsification of Closeness Relations: Eigenvalue Clustering for Computing Matrix Functions", "abstract": "We show how to efficiently solve a clustering problem that arises in a method to evaluate functions of matrices. The problem requires finding the connected components of a graph whose vertices are eigenvalues of a real or complex matrix and whose edges are pairs of eigenvalues that are at most \\delta away from each other. Davies and Higham proposed solving this problem by enumerating the edges of the graph, which requires at least $\\Omega(n^{2})$ work. We show that the problem can be solved by computing the Delaunay triangulation of the eigenvalues, removing from it long edges, and computing the connected components of the remaining edges in the triangulation. This leads to an $O(n\\log n)$ algorithm. We have implemented both algorithms using CGAL, a mature and sophisticated computational-geometry software library, and we demonstrate that the new algorithm is much faster in practice than the naive algorithm. We also present a tight analysis of the naive algorithm, showing that it performs $\\Theta(n^{2})$ work, and correct a misrepresentation in the original statement of the problem. To the best of our knowledge, this is the first application of computational geometry to solve a real-world problem in numerical linear algebra.", "venue": "ArXiv", "authors": ["Nir  Goren", "Dan  Halperin", "Sivan  Toledo"], "year": 2020, "n_citations": 0}
{"id": 6190152, "s2_id": "b1fe0269ea4c24f34fd68d77ebee93b1bf70be06", "title": "A parallel non-uniform fast Fourier transform library based on an \"exponential of semicircle\" kernel", "abstract": "The nonuniform fast Fourier transform (NUFFT) generalizes the FFT to off-grid data. Its many applications include image reconstruction, data analysis, and the numerical solution of differential equations. We present FINUFFT, an efficient parallel library for type 1 (nonuiform to uniform), type 2 (uniform to nonuniform), or type 3 (nonuniform to nonuniform) transforms, in dimensions 1, 2, or 3. It uses minimal RAM, requires no precomputation or plan steps, and has a simple interface to several languages. We perform the expensive spreading/interpolation between nonuniform points and the fine grid via a simple new kernel---the `exponential of semicircle' $e^{\\beta \\sqrt{1-x^2}}$ in $x\\in[-1,1]$---in a cache-aware load-balanced multithreaded implementation. The deconvolution step requires the Fourier transform of the kernel, for which we propose efficient numerical quadrature. For types 1 and 2, rigorous error bounds asymptotic in the kernel width approach the fastest known exponential rate, namely that of the Kaiser--Bessel kernel. We benchmark against several popular CPU-based libraries, showing favorable speed and memory footprint, especially in three dimensions when high accuracy and/or clustered point distributions are desired.", "venue": "SIAM J. Sci. Comput.", "authors": ["Alex H. Barnett", "Jeremy F. Magland", "Ludvig af Klinteberg"], "year": 2019, "n_citations": 49}
{"id": 6190513, "s2_id": "c53688c56347ab97b8e5caa582a4d1138a2e344c", "title": "Implicit Hari\u2013Zimmermann algorithm for the generalized SVD on the GPUs", "abstract": "A parallel, blocked, one-sided Hari\u2013Zimmermann algorithm for the generalized singular value decomposition (GSVD) of a real or a complex matrix pair ( F , G ) is here proposed, where F and G have the same number of columns, and are both of the full column rank. The algorithm targets either a single graphics processing unit (GPU), or a cluster of those, performs all non-trivial computation exclusively on the GPUs, requires the minimal amount of memory to be reasonably expected, scales acceptably with the increase of the number of GPUs available, and guarantees the reproducible, bitwise identical output of the runs repeated over the same input and with the same number of GPUs.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Vedran  Novakovi\u0107", "Sanja  Singer"], "year": 2021, "n_citations": 3}
{"id": 6194109, "s2_id": "b4b7c4cb71a4b3ae5ffcb20d417eb872f8539b55", "title": "Computing Integer Powers in Floating-Point Arithmetic", "abstract": "We introduce two algorithms for accurately evaluating powers to a positive integer in floating-point arithmetic, assuming a fused multiply-add (fma) instruction is available. We show that our log-time algorithm always produce faithfully-rounded results, discuss the possibility of getting correctly rounded results, and show that results correctly rounded in double precision can be obtained if extended precision is available with the possibility to round into double precision (with a single rounding).", "venue": "2007 Conference Record of the Forty-First Asilomar Conference on Signals, Systems and Computers", "authors": ["Peter  Kornerup", "Vincent  Lef\u00e8vre", "Jean-Michel  Muller"], "year": 2007, "n_citations": 4}
{"id": 6197102, "s2_id": "4e722c352a97356a928aea98be1e22efc228f58f", "title": "Automating the Last-Mile for High Performance Dense Linear Algebra", "abstract": "High performance dense linear algebra (DLA) libraries often rely on a general matrix multiply (Gemm) kernel that is implemented using assembly or with vector intrinsics. In particular, the real-valued Gemm kernels provide the overwhelming fraction of performance for the complex-valued Gemm kernels, along with the entire level-3 BLAS and many of the real and complex LAPACK routines. Thus,achieving high performance for the Gemm kernel translates into a high performance linear algebra stack above this kernel. However, it is a monumental task for a domain expert to manually implement the kernel for every library-supported architecture. This leads to the belief that the craft of a Gemm kernel is more dark art than science. It is this premise that drives the popularity of autotuning with code generation in the domain of DLA. \nThis paper, instead, focuses on an analytical approach to code generation of the Gemm kernel for different architecture, in order to shed light on the details or voo-doo required for implementing a high performance Gemm kernel. We distill the implementation of the kernel into an even smaller kernel, an outer-product, and analytically determine how available SIMD instructions can be used to compute the outer-product efficiently. We codify this approach into a system to automatically generate a high performance SIMD implementation of the Gemm kernel. Experimental results demonstrate that our approach yields generated kernels with performance that is competitive with kernels implemented manually or using empirical search.", "venue": "ArXiv", "authors": ["Richard Michael Veras", "Tze Meng Low", "Tyler Michael Smith", "Robert A. van de Geijn", "Franz  Franchetti"], "year": 2016, "n_citations": 5}
{"id": 6201360, "s2_id": "49576f5000ba3d09bb82bad72b10626bf7d091c7", "title": "calculus: High Dimensional Numerical and Symbolic Calculus in R", "abstract": "The R package calculus implements C++ optimized functions for numerical and symbolic calculus, such as the Einstein summing convention, fast computation of the LeviCivita symbol and generalized Kronecker delta, Taylor series expansion, multivariate Hermite polynomials, high-order derivatives, ordinary differential equations, differential operators and numerical integration in arbitrary orthogonal coordinate systems. The library applies numerical methods when working with functions or symbolic programming when working with characters or expressions. The package handles multivariate numerical calculus in arbitrary dimensions and coordinates and implements the symbolic counterpart of the numerical methods whenever possible, without depending on external computer algebra systems. Except for Rcpp, the package has no strict dependencies in order to provide a stable self-contained toolbox that invites re-use.", "venue": "ArXiv", "authors": ["Emanuele  Guidotti"], "year": 2021, "n_citations": 0}
{"id": 6207616, "s2_id": "13e7f6ed105bcee2bc83dd5330e040cf51b28af9", "title": "A General-Purpose Hierarchical Mesh Partitioning Method with Node Balancing Strategies for Large-Scale Numerical Simulations", "abstract": "Large-scale parallel numerical simulations are essential for a wide range of engineering problems that involve complex, coupled physical processes interacting across a broad range of spatial and temporal scales. The data structures involved in such simulations (meshes, sparse matrices, etc.) are frequently represented as graphs, and these graphs must be optimally partitioned across the available computational resources in order for the underlying calculations to scale efficiently. Partitions which minimize the number of graph edges that are cut (edge-cuts) while simultaneously maintaining a balance in the amount of work (i.e. graph nodes) assigned to each processor core are desirable, and the performance of most existing partitioning software begins to degrade in this metric for partitions with more than than $O(10^3)$ processor cores. In this work, we consider a general-purpose hierarchical partitioner which takes into account the existence of multiple processor cores and shared memory in a compute node while partitioning a graph into an arbitrary number of subgraphs. We demonstrate that our algorithms significantly improve the preconditioning efficiency and overall performance of realistic numerical simulations running on up to 32,768 processor cores with nearly $10^9$ unknowns.", "venue": "2018 IEEE/ACM 9th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (scalA)", "authors": ["Fande  Kong", "Roy H. Stogner", "Derek  Gaston", "John W. Peterson", "Cody J. Permann", "Andrew E. Slaughter", "Richard C. Martineau"], "year": 2018, "n_citations": 11}
{"id": 6209172, "s2_id": "7bfe9b201b0f4e383d82431cbcb18ec9f300614c", "title": "Forward-Mode Automatic Differentiation in Julia", "abstract": "We present ForwardDiff, a Julia package for forward-mode automatic differentiation (AD) featuring performance competitive with low-level languages like C++. Unlike recently developed AD tools in other popular high-level languages such as Python and MATLAB, ForwardDiff takes advantage of just-in-time (JIT) compilation to transparently recompile AD-unaware user code, enabling efficient support for higher-order differentiation and differentiation using custom number types (including complex numbers). For gradient and Jacobian calculations, ForwardDiff provides a variant of vector-forward mode that avoids expensive heap allocation and makes better use of memory bandwidth than traditional vector mode. In our numerical experiments, we demonstrate that for nontrivially large dimensions, ForwardDiff's gradient computations can be faster than a reverse-mode implementation from the Python-based autograd package. We also illustrate how ForwardDiff is used effectively within JuMP, a modeling language for optimization. According to our usage statistics, 41 unique repositories on GitHub depend on ForwardDiff, with users from diverse fields such as astronomy, optimization, finite element analysis, and statistics.", "venue": "ArXiv", "authors": ["Jarrett  Revels", "Miles  Lubin", "Theodore  Papamarkou"], "year": 2016, "n_citations": 163}
{"id": 6213618, "s2_id": "c90e5f88a977f587831e0d32f0e2f99e1bc87628", "title": "Precision\u2013Energy\u2013Throughput Scaling of Generic Matrix Multiplication and Convolution Kernels via Linear Projections", "abstract": "Generic matrix multiplication (GEMM) and convolution (CONV)/cross-correlation kernels often constitute the bulk of the compute- and memory-intensive processing within image/audio recognition and matching systems. We propose a novel method to scale the energy and processing throughput of GEMM and CONV kernels for such error-tolerant multimedia applications by adjusting the precision of computation. Our technique employs linear projections to the input matrix or signal data during the top-level GEMM and CONV blocking and reordering. The GEMM and CONV kernel processing then uses the projected inputs and the results are accumulated to form the final outputs. Throughput and energy scaling takes place by changing the number of projections computed by each kernel, which in turn produces approximate results, i.e., changes the precision of the performed computation. Results derived from a voltage- and frequency-scaled ARM Cortex A15 processor running face recognition and music-matching algorithms demonstrate that the proposed approach allows for a 280%-440% increase of processing throughput and a 75%-80% decrease of energy consumption against the optimized GEMM and CONV kernels without any impact on the obtained recognition or matching accuracy. Even higher gains can be obtained, if one is willing to tolerate some reduction in the accuracy of the recognition and matching applications.", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "authors": ["Mohammad Ashraful Anam", "Paul N. Whatmough", "Yiannis  Andreopoulos"], "year": 2014, "n_citations": 4}
{"id": 6219224, "s2_id": "86279580317bf70210914698cde683b2393fcc4b", "title": "MP users guide", "abstract": "MP is a package of ANSI Standard Fortran (ANS X3.9-1966) subroutines for performing multiple-precision floating-point arithmetic and evaluating elementary and special functions. The subroutines are machine independent and the precision is arbitrary, subject to storage limitations. The User's Guide describes the routines and their calling sequences, example and test programs, use of the Augment precompiler, and gives installation instructions for the package.", "venue": "ArXiv", "authors": ["Richard P. Brent"], "year": 2010, "n_citations": 17}
{"id": 6223997, "s2_id": "8a51acace4f9be17d7ec6ed47c9cdbc829381869", "title": "Parameter identification in large kinetic networks with BioPARKIN", "abstract": "Modelling, parameter identification, and simulation play an important role in systems biology. Usually, the goal is to determine parameter values that minimise the difference between experimental measurement values and model predictions in a least-squares sense. Large-scale biological networks, however, often suffer from missing data for parameter identification. Thus, the least-squares problems are rank-deficient and solutions are not unique. Many common optimisation methods ignore this detail because they do not take into account the structure of the underlying inverse problem. These algorithms simply return a \"solution\" without additional information on identifiability or uniqueness. This can yield misleading results, especially if parameters are co-regulated and data are noisy.", "venue": "ArXiv", "authors": ["Thomas  Dierkes", "Susanna  R\u00f6blitz", "Moritz  Wade", "Peter  Deuflhard"], "year": 2013, "n_citations": 11}
{"id": 6224416, "s2_id": "1bbcf06899f1d053b2fba248e69da7886674288b", "title": "Accelerating BLAS on Custom Architecture through Algorithm-Architecture Co-design", "abstract": "Basic Linear Algebra Subprograms (BLAS) play key role in high performance and scientific computing applications. Experimentally, yesteryear multicore and General Purpose Graphics Processing Units (GPGPUs) are capable of achieving up to 15 to 57% of the theoretical peak performance at 65W to 240W respectively for compute bound operations like Double/Single Precision General Matrix Multiplication (XGEMM). For bandwidth bound operations like Single/Double precision Matrix-vector Multiplication (XGEMV) the performance is merely 5 to 7% of the theoretical peak performance in multicores and GPGPUs respectively. Achieving performance in BLAS requires moving away from conventional wisdom and evolving towards customized accelerator tailored for BLAS through algorithm-architecture co-design. In this paper, we present acceleration of Level-1 (vector operations), Level-2 (matrix-vector operations), and Level-3 (matrix-matrix operations) BLAS through algorithm architecture co-design on a Coarse-grained Reconfigurable Architecture (CGRA). We choose REDEFINE CGRA as a platform for our experiments since REDEFINE can be adapted to support domain of interest through tailor-made Custom Function Units (CFUs). For efficient sequential realization of BLAS, we present design of a Processing Element (PE) and perform micro-architectural enhancements in the PE to achieve up-to 74% of the theoretical peak performance of PE in DGEMM, 40% in DGEMV and 20% in double precision inner product (DDOT). We attach this PE to REDEFINE CGRA as a CFU and show the scalability of our solution. Finally, we show performance improvement of 3-140x in PE over commercially available Intel micro-architectures, ClearSpeed CSX700, FPGA, and Nvidia GPGPUs.", "venue": "ArXiv", "authors": ["Farhad  Merchant", "Tarun  Vatwani", "Anupam  Chattopadhyay", "Soumyendu  Raha", "S. K. Nandy", "Ranjani  Narayan"], "year": 2016, "n_citations": 3}
{"id": 6225898, "s2_id": "065b3b5a5f9e89bb1569c27545a637c81dcad335", "title": "UniCalc.LIN: a linear constraint solver for the UniCalc system", "abstract": "In this short paper we present a linear constraint solver for the UniCalc system, an environment for reliable solution of mathematical modeling problems.", "venue": "ArXiv", "authors": ["E.  Petrov", "Yu.  Kostov", "E.  Botoeva"], "year": 2006, "n_citations": 0}
{"id": 6238216, "s2_id": "70f3a3caeb56b042493e748d3b6bf8d91727dc00", "title": "A task-driven implementation of a simple numerical solver for hyperbolic conservation laws", "abstract": "This article describes the implementation of an all-in-one numerical procedure within the runtime StarPU. In order to limit the complexity of the method, for the sake of clarity of the presentation of the non-classical task-driven programming environnement, we have limited the numerics to first order in space and time. Results show that the task distribution is efficient if the tasks are numerous and individually large enough so that the task heap can be saturated by tasks which computational time covers the task management overhead. Next, we also see that even though they are mostly faster on graphic cards, not all the tasks are suitable for GPUs, which brings forward the importance of the task scheduler. Finally, we look at a more realistic system of conservation laws with an expensive source term, what allows us to conclude and open on future works involving higher local arithmetic intensity, by increasing the order of the numerical method or by enriching the model (increased number of parameters and therefore equations).", "venue": "ArXiv", "authors": ["Mohamed  Essadki", "Jonathan  Jung", "Adam  Larat", "Milan  Pelletier", "Vincent  Perrier"], "year": 2017, "n_citations": 3}
{"id": 6239553, "s2_id": "d3b08034d1c17776c698d680f9547d3c5f0bdf84", "title": "A Unified Iteration Space Transformation Framework for Sparse and Dense Tensor Algebra", "abstract": "We address the problem of optimizing mixed sparse and dense tensor algebra in a compiler. We show that standard loop transformations, such as strip-mining, tiling, collapsing, parallelization and vectorization, can be applied to irregular loops over sparse iteration spaces. We also show how these transformations can be applied to the contiguous value arrays of sparse tensor data structures, which we call their position space, to unlock load-balanced tiling and parallelism. \nWe have prototyped these concepts in the open-source TACO system, where they are exposed as a scheduling API similar to the Halide domain-specific language for dense computations. Using this scheduling API, we show how to optimize mixed sparse/dense tensor algebra expressions, how to generate load-balanced code by scheduling sparse tensor algebra in position space, and how to generate sparse tensor algebra GPU code. Our evaluation shows that our transformations let us generate good code that is competitive with many hand-optimized implementations from the literature.", "venue": "ArXiv", "authors": ["Ryan  Senanayake", "Fredrik  Kjolstad", "Changwan  Hong", "Shoaib  Kamil", "Saman  Amarasinghe"], "year": 2020, "n_citations": 3}
{"id": 6244382, "s2_id": "e8fb4b5703ba0d57bf39b5a43d4c585f585b76d9", "title": "KBLAS: An Optimized Library for Dense Matrix-Vector Multiplication on GPU Accelerators", "abstract": "KBLAS is an open-source, high-performance library that provides optimized kernels for a subset of Level 2 BLAS functionalities on CUDA-enabled GPUs. Since performance of dense matrix-vector multiplication is hindered by the overhead of memory accesses, a double-buffering optimization technique is employed to overlap data motion with computation. After identifying a proper set of tuning parameters, KBLAS efficiently runs on various GPU architectures while avoiding code rewriting and retaining compliance with the standard BLAS API. Another optimization technique allows ensuring coalesced memory access when dealing with submatrices, especially for high-level dense linear algebra algorithms. All KBLAS kernels have been leveraged to a multi-GPU environment, which requires the introduction of new APIs. Considering general matrices, KBLAS is very competitive with existing state-of-the-art kernels and provides a smoother performance across a wide range of matrix dimensions. Considering symmetric and Hermitian matrices, the KBLAS performance outperforms existing state-of-the-art implementations on all matrix sizes and achieves asymptotically up to 50p and 60p speedup against the best competitor on single GPU and multi-GPUs systems, respectively. Performance results also validate our performance model. A subset of KBLAS high-performance kernels have been integrated into NVIDIA's standard BLAS implementation (cuBLAS) for larger dissemination, starting from version 6.0.", "venue": "ACM Trans. Math. Softw.", "authors": ["Ahmad  Abdelfattah", "David E. Keyes", "Hatem  Ltaief"], "year": 2016, "n_citations": 32}
{"id": 6246176, "s2_id": "22c2c3020c6379e3d72e541783b99ff9355a0625", "title": "Encog: library of interchangeable machine learning models for Java and C#", "abstract": "This paper introduces the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was 1st released in 2008. Encog allows a variety of machine learning models to be applied to datasets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from this http URL", "venue": "J. Mach. Learn. Res.", "authors": ["Jeff  Heaton"], "year": 2015, "n_citations": 75}
{"id": 6250231, "s2_id": "8105a0832859d5507a5051ed7ae31569300e3fba", "title": "Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing", "abstract": "In this paper, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo's design principle abstracts all functionality as \"linear operators\", motivating the notation of a \"linear operator algebra library\". Ginkgo's current focus is oriented towards providing sparse linear algebra functionality for high performance GPU architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific back ends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo's usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo's high performance on state-of-the-art GPU architectures.", "venue": "ArXiv", "authors": ["Hartwig  Anzt", "Terry  Cojean", "Goran  Flegar", "Fritz  Goebel", "Thomas  Gr\u00fctzmacher", "Pratik  Nayak", "Tobias  Ribizel", "Yu-Hsiang  Tsai", "Enrique S. Quintana-Ort\u00ed"], "year": 2020, "n_citations": 9}
{"id": 6252922, "s2_id": "7f6c8f67053eb21856e7c8dfc835a5342a69d024", "title": "PRIMME_SVDS: A High-Performance Preconditioned SVD Solver for Accurate Large-Scale Computations", "abstract": "The increasing number of applications requiring the solution of large scale singular value problems have rekindled interest in iterative methods for the SVD. Some promising recent ad- vances in large scale iterative methods are still plagued by slow convergence and accuracy limitations for computing smallest singular triplets. Furthermore, their current implementations in MATLAB cannot address the required large problems. Recently, we presented a preconditioned, two-stage method to effectively and accurately compute a small number of extreme singular triplets. In this research, we present a high-performance software, PRIMME SVDS, that implements our hybrid method based on the state-of-the-art eigensolver package PRIMME for both largest and smallest singular values. PRIMME SVDS fills a gap in production level software for computing the partial SVD, especially with preconditioning. The numerical experiments demonstrate its superior performance compared to other state-of-the-art software and its good parallel performance under strong and weak scaling.", "venue": "SIAM J. Sci. Comput.", "authors": ["Lingfei  Wu", "Eloy  Romero", "Andreas  Stathopoulos"], "year": 2017, "n_citations": 38}
{"id": 6258881, "s2_id": "df3f7501eced0bb4fb76f6cd11b7ae9b5f14a262", "title": "A Unified Optimization Approach for Sparse Tensor Operations on GPUs", "abstract": "Sparse tensors appear in many large-scale applications with multidimensional and sparse data. While multidimensional sparse data often need to be processed on manycore processors, attempts to develop highly-optimized GPU-based implementations of sparse tensor operations are rare. The irregular computation patterns and sparsity structures as well as the large memory footprints of sparse tensor operations make such implementations challenging. We leverage the fact that sparse tensor operations share similar computation patterns to propose a unified tensor representation called F-COO. Combined with GPU-specific optimizations, F-COO provides highly-optimized implementations of sparse tensor computations on GPUs. The performance of the proposed unified approach is demonstrated for tensor-based kernels such as the Sparse Matricized Tensor-Times-Khatri-Rao Product (SpMTTKRP) and the Sparse Tensor-Times-Matrix Multiply (SpTTM) and is used in tensor decomposition algorithms. Compared to state-of-the-art work we improve the performance of SpTTM and SpMTTKRP up to 3.7 and 30.6 times respectively on NVIDIA Titan-X GPUs. We implement a CANDECOMP/PARAFAC (CP) decomposition and achieve up to 14.9 times speedup using the unified method over state-of-the-art libraries on NVIDIA Titan-X GPUs.", "venue": "2017 IEEE International Conference on Cluster Computing (CLUSTER)", "authors": ["Bangtian  Liu", "Chengyao  Wen", "Anand D. Sarwate", "Maryam Mehri Dehnavi"], "year": 2017, "n_citations": 40}
{"id": 6261575, "s2_id": "c7d395105accf7994cbe5b7f576c6f5577cc85dd", "title": "Kokkos Kernels: Performance Portable Sparse/Dense Linear Algebra and Graph Kernels", "abstract": "As hardware architectures are evolving in the push towards exascale, developing Computational Science and Engineering (CSE) applications depend on performance portable approaches for sustainable software development. This paper describes one aspect of performance portability with respect to developing a portable library of kernels that serve the needs of several CSE applications and software frameworks. We describe Kokkos Kernels, a library of kernels for sparse linear algebra, dense linear algebra and graph kernels. We describe the design principles of such a library and demonstrate portable performance of the library using some selected kernels. Specifically, we demonstrate the performance of four sparse kernels, three dense batched kernels, two graph kernels and one team level algorithm.", "venue": "ArXiv", "authors": ["Sivasankaran  Rajamanickam", "Seher  Acer", "Luc  Berger-Vergiat", "Vinh  Dang", "Nathan  Ellingwood", "Evan  Harvey", "Brian  Kelley", "Christian R. Trott", "Jeremiah  Wilke", "Ichitaro  Yamazaki"], "year": 2021, "n_citations": 3}
{"id": 6261856, "s2_id": "eda0c7f699e4987bc1ab999c747cc686073c96fb", "title": "Synergistic CPU-FPGA Acceleration of Sparse Linear Algebra", "abstract": "This paper describes REAP, a software-hardware approach that enables high performance sparse linear algebra computations on a cooperative CPU-FPGA platform. REAP carefully separates the task of organizing the matrix elements from the computation phase. It uses the CPU to provide a first-pass re-organization of the matrix elements, allowing the FPGA to focus on the computation. We introduce a new intermediate representation that allows the CPU to communicate the sparse data and the scheduling decisions to the FPGA. The computation is optimized on the FPGA for effective resource utilization with pipelining. REAP improves the performance of Sparse General Matrix Multiplication (SpGEMM) and Sparse Cholesky Factorization by 3.2X and 1.85X compared to widely used sparse libraries for them on the CPU, respectively.", "venue": "ArXiv", "authors": ["Mohammadreza  Soltaniyeh", "Richard P. Martin", "Santosh  Nagarakatte"], "year": 2020, "n_citations": 3}
{"id": 6262821, "s2_id": "00e288dc0c29d61fd8c344c2ba4483bbe9ca1b6d", "title": "Faster Math Functions, Soundly", "abstract": "Standard library implementations of functions like sin and exp optimize for accuracy, not speed, because they are intended for general-purpose use. But applications tolerate inaccuracy from cancellation, rounding error, and singularities\u2014sometimes even very high error\u2014and many application could tolerate error in function implementations as well. This raises an intriguing possibility: speeding up numerical code by tuning standard function implementations. This paper thus introduces OpTuner, an automatic method for selecting the best implementation of mathematical functions at each use site. OpTuner assembles dozens of implementations for the standard mathematical functions from across the speed-accuracy spectrum. OpTuner then uses error Taylor series and integer linear programming to compute optimal assignments of function implementation to use site and presents the user with a speed-accuracy Pareto curve they can use to speed up their code. In a case study on the POV-Ray ray tracer, OpTuner speeds up a critical computation, leading to a whole program speedup of 9% with no change in the program output (whereas human efforts result in slower code and lower-quality output). On a broader study of 37 standard benchmarks, OpTuner matches 216 implementations to 89 use sites and demonstrates speed-ups of 107% for negligible decreases in accuracy and of up to 438% for error-tolerant applications.", "venue": "ArXiv", "authors": ["Ian  Briggs", "Pavel  Panchekha"], "year": 2021, "n_citations": 0}
{"id": 6265857, "s2_id": "fb27ed1e9937f435a121d933980f36d4764ba0b8", "title": "Numerical application and Turbo C program using the Gauss-Jordan Method", "abstract": "The article presents the general notions and algorithm about the Gauss-Jordan method. An eloquent example is given and the Turbo C program illustrated this method. We conclude that we can obtain by this method the determinant, by simple calculations and reducing the rounding errors", "venue": "ArXiv", "authors": ["Anghel  Drugarin", "Cornelia  Victoria"], "year": 2014, "n_citations": 0}
{"id": 6278509, "s2_id": "c628c47de3e6dec4075b373280fa53eaa66e7039", "title": "An optimized and scalable eigensolver for sequences of eigenvalue problems", "abstract": "In many scientific applications, the solution of nonlinear differential equations are obtained through the setup and solution of a number of successive eigenproblems. These eigenproblems can be regarded as a sequence whenever the solution of one problem fosters the initialization of the next. In addition, in some eigenproblem sequences, there is a connection between the solutions of adjacent eigenproblems. Whenever it is possible to unravel the existence of such a connection, the eigenproblem sequence is said to be correlated. When facing with a sequence of correlated eigenproblems, the current strategy amounts to solving each eigenproblem in isolation. We propose an alternative approach that exploits such correlation through the use of an eigensolver based on subspace iteration and accelerated with Chebyshev polynomials (Chebyshev filtered subspace iteration (ChFSI)). The resulting eigensolver is optimized by minimizing the number of matrix\u2013vector multiplications and parallelized using the Elemental library framework. Numerical results show that ChFSI achieves excellent scalability and is competitive with current dense linear algebra parallel eigensolvers. Copyright \u00a9 2014 John Wiley & Sons, Ltd.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Mario  Berljafa", "Daniel  Wortmann", "Edoardo Di Napoli"], "year": 2015, "n_citations": 6}
{"id": 6280074, "s2_id": "443fcb28de3a4773b0fac2c4badae156bf898230", "title": "Preparing Ginkgo for AMD GPUs \u2013 A Testimonial on Porting CUDA Code to HIP", "abstract": "With AMD reinforcing their ambition in the scientific high performance computing ecosystem, we extend the hardware scope of the Ginkgo linear algebra package to feature a HIP backend for AMD GPUs. In this paper, we report and discuss the porting effort from CUDA, the extension of the HIP framework to add missing features such as cooperative groups, the performance price of compiling HIP code for AMD architectures, and the design of a library providing native backends for NVIDIA and AMD GPUs while minimizing code duplication by using a shared code base.", "venue": "Euro-Par Workshops", "authors": ["Yuhsiang M. Tsai", "Terry  Cojean", "Tobias  Ribizel", "Hartwig  Anzt"], "year": 2020, "n_citations": 3}
{"id": 6282488, "s2_id": "c31407e89e0b01ed29ce33de655a61627aeac8b0", "title": "Batched QR and SVD Algorithms on GPUs with Applications in Hierarchical Matrix Compression", "abstract": "High performance GPU hosted batched QR decomposition kernels are developed and outperform current implementations for small and rectangular matrices.Various GPU hosted batched singular value decomposition kernels are developed and used as building blocks of a batched randomized SVD kernel for numerically low rank matrix blocks.Batched QR, SVD, and GEMM kernels are used to compress hierarchical matrices entirely on the GPU. We present high performance implementations of the QR and the singular value decomposition of a batch of small matrices hosted on the GPU with applications in the compression of hierarchical matrices. The one-sided Jacobi algorithm is used for its simplicity and inherent parallelism as a building block for the SVD of low rank blocks using randomized methods. We implement multiple kernels based on the level of the GPU memory hierarchy in which the matrices can reside and show substantial speedups against streamed cuSOLVER SVDs. The resulting batched routine is a key component of hierarchical matrix compression, opening up opportunities to perform H-matrix arithmetic efficiently on GPUs.", "venue": "Parallel Comput.", "authors": ["Wajih Halim Boukaram", "George M. Turkiyyah", "Hatem  Ltaief", "David E. Keyes"], "year": 2018, "n_citations": 31}
{"id": 6282938, "s2_id": "e808e1cb3877445a1fa3505b48b05125c0cbec63", "title": "MonteCarloMeasurements.jl: Nonlinear Propagation of Arbitrary Multivariate Distributions by means of Method Overloading", "abstract": "This manuscript outlines a software package that facilitates working with probability distributions by means of Monte-Carlo methods, in a way that allows for propagation of multivariate probability distributions through arbitrary functions. We provide a \\emph{type} that represents probability distributions by an internal vector of unweighted samples, \\texttt{Particles}, which is a subtype of a \\texttt{Real} number and behaves just like a regular real number in calculations by means of method overloading. This makes the software easy to work with and presents minimal friction for the user. We highlight how this design facilitates optimal usage of SIMD instructions and showcase the package for uncertainty propagation through an off-the-shelf ODE solver, as well as for robust probabilistic optimization with automatic differentiation.", "venue": "ArXiv", "authors": ["Fredrik Bagge Carlson"], "year": 2020, "n_citations": 5}
{"id": 6285778, "s2_id": "628ba5084a9870d74719bf84b1385ec58e28b436", "title": "A New Test for Hamming-Weight Dependencies", "abstract": "Pseudorandom number generators (PRNGs) are algorithms that generate a seemingly random output using a deterministic algorithm. A F-bit PRNG is defined by a state space ( , a transition (or next-state) computable function C : ( \u2192 ( , and a computable output function 5 : ( \u2192 { 0, 1 } that maps the state space into F-bit words. One then considers an initial state, or seed B \u2208 ( , and computes the sequence ofF-bit outputs 5 (B), 5 (C (B)), 5 ( C (B) ) , 5 ( C (B) ) , . . .", "venue": "ArXiv", "authors": ["David  Blackman", "Sebastiano  Vigna"], "year": 2021, "n_citations": 1}
{"id": 6286750, "s2_id": "bfb6f88c0987f17fdb9360b478479ee4ed11cb48", "title": "libRoadRunner: A High Performance SBML Simulation and Analysis Library", "abstract": "MOTIVATION\nThis article presents libRoadRunner, an extensible, high-performance, cross-platform, open-source software library for the simulation and analysis of models expressed using Systems Biology Markup Language (SBML). SBML is the most widely used standard for representing dynamic networks, especially biochemical networks. libRoadRunner is fast enough to support large-scale problems such as tissue models, studies that require large numbers of repeated runs and interactive simulations.\n\n\nRESULTS\nlibRoadRunner is a self-contained library, able to run both as a component inside other tools via its C++ and C bindings, and interactively through its Python interface. Its Python Application Programming Interface (API) is similar to the APIs of MATLAB ( WWWMATHWORKSCOM: ) and SciPy ( HTTP//WWWSCIPYORG/: ), making it fast and easy to learn. libRoadRunner uses a custom Just-In-Time (JIT) compiler built on the widely used LLVM JIT compiler framework. It compiles SBML-specified models directly into native machine code for a variety of processors, making it appropriate for solving extremely large models or repeated runs. libRoadRunner is flexible, supporting the bulk of the SBML specification (except for delay and non-linear algebraic equations) including several SBML extensions (composition and distributions). It offers multiple deterministic and stochastic integrators, as well as tools for steady-state analysis, stability analysis and structural analysis of the stoichiometric matrix.\n\n\nAVAILABILITY AND IMPLEMENTATION\nlibRoadRunner binary distributions are available for Mac OS X, Linux and Windows. The library is licensed under Apache License Version 2.0. libRoadRunner is also available for ARM-based computers such as the Raspberry Pi. http://www.libroadrunner.org provides online documentation, full build instructions, binaries and a git source repository.\n\n\nCONTACTS\nhsauro@u.washington.edu or somogyie@indiana.edu\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.", "venue": "Bioinform.", "authors": ["Endre T. Somogyi", "Jean-Marie  Bouteiller", "James A. Glazier", "Matthias  K\u00f6nig", "J. Kyle Medley", "Maciej  Swat", "Herbert M. Sauro"], "year": 2015, "n_citations": 105}
{"id": 6287960, "s2_id": "b951550f8f9830048214a3ee4057f9892cd36947", "title": "Efficient FFT mapping on GPU for radar processing application: modeling and implementation", "abstract": "General-purpose multiprocessors (as, in our case, Intel IvyBridge and Intel Haswell) increasingly add GPU computing power to the former multicore architectures. When used for embedded applications (for us, Synthetic aperture radar) with intensive signal processing requirements, they must constantly compute convolution algorithms, such as the famous Fast Fourier Transform. Due to its \"fractal\" nature (the typical buttery shape, with larger FFTs dened as combination of smaller ones with auxiliary data array transpose functions), one can hope to compute analytically the size of the largest FFT that can be performed locally on an elementary GPU compute block. Then, the full application must be organized around this given building block size. Now, due to phenomena involved in the data transfers between various memory levels across CPUs and GPUs, the optimality of such a scheme is only loosely predictable (as communications tend to overcome in time the complexity of computations). Therefore a mix of (theoretical) analytic approach and (practical) runtime validation is here needed. As we shall illustrate, this occurs at both stage,", "venue": "ArXiv", "authors": ["Mohamed Amine Bergach", "Emilien  Kofman", "Robert de Simone", "Serge  Tissot", "Michel  Syska"], "year": 2015, "n_citations": 1}
{"id": 6296498, "s2_id": "0c736b6f2be7ba4986bd68d2479ebb43856271c8", "title": "Performance Analysis and Optimization of Sparse Matrix-Vector Multiplication on Modern Multi- and Many-Core Processors", "abstract": "This paper presents a low-overhead optimizer for the ubiquitous sparse matrix-vector multiplication (SpMV) kernel. Architectural diversity among different processors together with structural diversity among different sparse matrices lead to bottleneck diversity. This justifies an SpMV optimizer that is both matrix- and architecture-adaptive through runtime specialization. To this direction, we present an approach that first identifies the performance bottlenecks of SpMV for a given sparse matrix on the target platform either through profiling or by matrix property inspection, and then selects suitable optimizations to tackle those bottlenecks. Our optimization pool is based on the widely used Compressed Sparse Row (CSR) sparse matrix storage format and has low preprocessing overheads, making our overall approach practical even in cases where fast decision making and optimization setup is required. We evaluate our optimizer on three x86-based computing platforms and demonstrate that it is able to distinguish and appropriately optimize SpMV for the majority of matrices in a representative test suite, leading to significant speedups over the CSR and Inspector-Executor CSR SpMV kernels available in the latest release of the Intel MKL library.", "venue": "2017 46th International Conference on Parallel Processing (ICPP)", "authors": ["Athena  Elafrou", "Georgios I. Goumas", "Nectarios  Koziris"], "year": 2017, "n_citations": 14}
{"id": 6297147, "s2_id": "5c15c2864bb995bff0453bd10e6727790a2c6490", "title": "The ELAPS framework: Experimental Linear Algebra Performance Studies", "abstract": "In scientific computing, optimal use of computing resources comes at the cost of extensive coding, tuning, and benchmarking. While the classic approach of \u201cfeatures first, performance later\u201d is supported by a variety of tools such as Tau, Vampir, and Scalasca, the emerging performance-centric approach, in which both features and performance are primary objectives, is still lacking suitable development tools. For dense linear algebra applications, we fill this gap with the Experimental Linear Algebra Performance Studies (ELAPS) framework, a multi-platform open-source environment for easy, fast, and yet powerful performance experimentation and prototyping. In contrast to many existing tools, ELAPS targets the beginning of the development process, assisting application developers in both algorithmic and optimization decisions. With ELAPS, users construct experiments to investigate how performance and efficiency depend on factors such as caching, algorithmic parameters, problem size, and parallelism. Experiments are designed either through Python scripts or a specialized Graphical User Interface (GUI), and run on a spectrum of architectures, ranging from laptops to accelerators and clusters. The resulting reports provide various metrics and statistics that can be analyzed both numerically and visually. In this article, we introduce ELAPS and illustrate its practical value in guiding critical performance decisions already in early development stages.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Elmar  Peise", "Paolo  Bientinesi"], "year": 2019, "n_citations": 7}
{"id": 6298863, "s2_id": "57a40440be2c8f38ebde172cddca3925d2827c62", "title": "Achieving 100,000,000 database inserts per second using Accumulo and D4M", "abstract": "The Apache Accumulo database is an open source relaxed consistency database that is widely used for government applications. Accumulo is designed to deliver high performance on unstructured data such as graphs of network data. This paper tests the performance of Accumulo using data from the Graph500 benchmark. The Dynamic Distributed Dimensional Data Model (D4M) software is used to implement the benchmark on a 216-node cluster running the MIT SuperCloud software stack. A peak performance of over 100,000,000 database inserts per second was achieved which is 100\u00d7 larger than the highest previously published value for any other database. The performance scales linearly with the number of ingest clients, number of database servers, and data size. The performance was achieved by adapting several supercomputing techniques to this application: distributed arrays, domain decomposition, adaptive load balancing, and single-program-multiple-data programming.", "venue": "2014 IEEE High Performance Extreme Computing Conference (HPEC)", "authors": ["Jeremy  Kepner", "William  Arcand", "David  Bestor", "Bill  Bergeron", "Chansup  Byun", "Vijay  Gadepally", "Matthew  Hubbell", "Peter  Michaleas", "Julie  Mullen", "Andrew  Prout", "Albert  Reuther", "Antonio  Rosa", "Charles  Yee"], "year": 2014, "n_citations": 65}
{"id": 6302389, "s2_id": "76e516d15d92d28c62d6df4063024ca323f45453", "title": "3D acoustic-elastic coupling with gravity: the dynamics of the 2018 Palu, Sulawesi earthquake and tsunami", "abstract": "We present a highly scalable 3D fully-coupled Earth & ocean model of earthquake rupture and tsunami generation and perform the first fully coupled simulation of an actual earthquake-tsunami event and a 3D benchmark problem of tsunami generation by a megathrust dynamic earthquake rupture. Multi-petascale simulations, with excellent performance demonstrated on three different platforms, allow high-resolution forward modeling. Our largest mesh has \u2248261 billion degrees of freedom, resolving at least 15 Hz of the acoustic wave field. We self-consistently model seismic, acoustic and surface gravity wave propagation in elastic (Earth) and acoustic (ocean) materials sourced by physics-based non-linear earthquake dynamic rupture, thereby gaining insight into the tsunami generation process without relying on approximations that have previously been applied to permit solution of this challenging problem. Complicated geometries, including high-resolution bathymetry, coastlines and segmented earthquake faults are discretized by adaptive unstructured tetrahedral meshes. This inevitably leads to large differences in element sizes and wave speeds which can be mitigated by ADER local time-stepping and a Discontinuous Galerkin discretization yielding high-order accuracy in time and space.", "venue": "SC", "authors": ["Lukas  Krenz", "Carsten  Uphoff", "Thomas  Ulrich", "Alice-Agnes  Gabriel", "Lauren S. Abrahams", "Eric M. Dunham", "Michael  Bader"], "year": 2021, "n_citations": 1}
{"id": 6303304, "s2_id": "ef2bfc0dd193113b8508e0a1563e7a4673674a30", "title": "Complex Additive Geometric Multilevel Solvers for Helmholtz Equations on Spacetrees", "abstract": "We introduce a family of implementations of low-order, additive, geometric multilevel solvers for systems of Helmholtz equations arising from Schr\u00f6dinger equations. Both grid spacing and arithmetics may comprise complex numbers, and we thus can apply complex scaling to the indefinite Helmholtz operator. Our implementations are based on the notion of a spacetree and work exclusively with a finite number of precomputed local element matrices. They are globally matrix-free. Combining various relaxation factors with two grid transfer operators allows us to switch from additive multigrid over a hierarchical basis method into a Bramble-Pasciak-Xu (BPX)-type solver, with several multiscale smoothing variants within one code base. Pipelining allows us to realize full approximation storage (FAS) within the additive environment where, amortized, each grid vertex carrying degrees of freedom is read/written only once per iteration. The codes realize a single-touch policy. Among the features facilitated by matrix-free FAS is arbitrary dynamic mesh refinement (AMR) for all solver variants. AMR as an enabler for full multigrid (FMG) cycling\u2014the grid unfolds throughout the computation\u2014allows us to reduce the cost per unknown. The present work primary contributes toward software realization and design questions. Our experiments show that the consolidation of single-touch FAS, dynamic AMR, and vectorization-friendly, complex scaled, matrix-free FMG cycles delivers a mature implementation blueprint for solvers of Helmholtz equations in general. For this blueprint, we put particular emphasis on a strict implementation formalism as well as some implementation correctness proofs.", "venue": "ACM Trans. Math. Softw.", "authors": ["Bram  Reps", "Tobias  Weinzierl"], "year": 2017, "n_citations": 14}
{"id": 6311557, "s2_id": "19ada5f9f34d5c44e592d6eba3de952bc27b58b6", "title": "Fast Matrix-Free Evaluation of Discontinuous Galerkin Finite Element Operators", "abstract": "We present an algorithmic framework for matrix-free evaluation of discontinuous Galerkin finite element operators. It relies on fast quadrature with sum factorization on quadrilateral and hexahedral meshes, targeting general weak forms of linear and nonlinear partial differential equations. Different algorithms and data structures are compared in an in-depth performance analysis. The implementations of the local integrals are optimized by vectorization over several cells and faces and an even-odd decomposition of the one-dimensional interpolations. Up to 60% of the arithmetic peak on Intel Haswell, Broadwell, and Knights Landing processors is reached when running from caches and up to 40% of peak when also considering the access to vectors from main memory. On 2\u00d714 Broadwell cores, the throughput is up to 2.2 billion unknowns per second for the 3D Laplacian and up to 4 billion unknowns per second for the 3D advection on affine geometries, close to a simple copy operation at 4.7 billion unknowns per second. Our experiments show that MPI ghost exchange has a considerable impact on performance and we present strategies to mitigate this effect. Finally, various options for evaluating geometry terms and their performance are discussed. Our implementations are publicly available through the deal.II finite element library.", "venue": "ACM Trans. Math. Softw.", "authors": ["Martin  Kronbichler", "Katharina  Kormann"], "year": 2019, "n_citations": 57}
{"id": 6313771, "s2_id": "58b21000e4245da1af8f39f11f823ab5363b5962", "title": "Optimization of Generalized Jacobian Chain Products without Memory Constraints", "abstract": "The efficient computation of Jacobians represents a fundamental challenge in computational science and engineering. Large-scale modular numerical simulation programs can be regarded as sequences of evaluations of in our case differentiable modules with corresponding local Jacobians. The latter are typically not available. Tangent and adjoint versions of the individual modules are assumed to be given as results of algorithmic differentiation instead. The classical (Jacobian) matrix chain product formulation is extended with the optional evaluation of matrix-free Jacobian-matrix and matrix-Jacobian products as tangents and adjoints. We propose a dynamic programming algorithm for the minimization of the computational cost of such generalized Jacobian chain products without considering constraints on the available persistent system memory. In other words, the naive evaluation of an adjoint of the entire simulation program is assumed to be a feasible option. No checkpointing is required. Under the given assumptions we obtain optimal solutions which improve the best state of the art methods by factors of up to seven on a set of randomly generated problem instances of growing size.", "venue": "ArXiv", "authors": ["UWE  NAUMANN"], "year": 2020, "n_citations": 2}
{"id": 6328048, "s2_id": "3304fd3c938c03747ef72e09fcf2a1d9fddf356c", "title": "Computing the coefficients for the power series solution of the Lane-Emden equation with the Python library SymPy", "abstract": "It is shown how the Python library Sympy can be used to compute symbolically the coefficients of the power series solution of the Lane-Emden equation (LEE). Sympy is an open source Python library for symbolic mathematics. The power series solutions are compared to the numerically computed solutions using matplotlib. The results of a run time measurement of the implemented algorithm are discussed at the end.", "venue": "ArXiv", "authors": ["Klaus  Rohe"], "year": 2014, "n_citations": 0}
{"id": 6329743, "s2_id": "a57ec9d1bcb64cac7d769aad3a37a723d84ae933", "title": "RLibm-Prog: Progressive Polynomial Approximations for Fast Correctly Rounded Math Libraries", "abstract": "This paper presents a novel method for generating a single polynomial approximation that produces correctly rounded results for all inputs of an elementary function for multiple representations. The generated polynomial approximation has the nice property that the first few lower degree terms produce correctly rounded results for specific representations of smaller bitwidths, which we call progressive performance. To generate such progressive polynomial approximations, we approximate the correctly rounded result and formulate the computation of correctly rounded polynomial approximations as a linear program inspired by our RLibm project. In contrast to our prior work, we avoid storing large lookup tables for the polynomial coefficients. We observe that the problem of computing polynomial approximations to elementary functions is a linear programming problem in low dimensions, i.e., with a small number of unknowns. We design a fast randomized algorithm for efficiently computing polynomial approximations with progressive performance. Our method produces polynomial approximations that are faster than the RLibm project and other mainstream libraries while also having progressive performance.", "venue": "ArXiv", "authors": ["Mridul  Aanjaneya", "Jay P. Lim", "Santosh  Nagarakatte"], "year": 2021, "n_citations": 0}
{"id": 6343200, "s2_id": "9eee5bbad46122f995c0d6e1424fa66ef855aaf2", "title": "Introducing Geometric Algebra to Geometric Computing Software Developers: A Computational Thinking Approach", "abstract": "Designing software systems for Geometric Computing applications can be a challenging task. Software engineers typically use software abstractions to hide and manage the high complexity of such systems. Without the presence of a unifying algebraic system to describe geometric models, the use of software abstractions alone can result in many design and maintenance problems. Geometric Algebra (GA) can be a universal abstract algebraic language for software engineering geometric computing applications. Few sources, however, provide enough information about GA-based software implementations targeting the software engineering community. In particular, successfully introducing GA to software engineers requires quite different approaches from introducing GA to mathematicians or physicists. This article provides a high-level introduction to the abstract concepts and algebraic representations behind the elegant GA mathematical structure. The article focuses on the conceptual and representational abstraction levels behind GA mathematics with sufficient references for more details. In addition, the article strongly recommends applying the methods of Computational Thinking in both introducing GA to software engineers, and in using GA as a mathematical language for developing Geometric Computing software systems.", "venue": "ArXiv", "authors": ["Ahmad Hosny Eid"], "year": 2017, "n_citations": 0}
{"id": 6345908, "s2_id": "7f98578c03e6ad89c1f9a3962b0a43321d1b19aa", "title": "Sound Approximation of Programs with Elementary Functions", "abstract": "Elementary function calls are a common feature in numerical programs. While their implementions in library functions are highly optimized, their computation is nonetheless very expensive compared to plain arithmetic. Full accuracy is, however, not always needed. Unlike arithmetic, where the performance difference between for example single and double precision floating-point arithmetic is relatively small, elementary function calls provide a much richer tradeoff space between accuracy and efficiency. Navigating this space is challenging. First, generating approximations of elementary function calls which are guaranteed to satisfy accuracy error bounds is highly nontrivial. Second, the performance of such approximations generally depends on several parameters which are unintuitive to choose manually, especially for non-experts. \nWe present a fully automated approach and tool which approximates elementary function calls inside small programs while guaranteeing overall user provided error bounds. Our tool leverages existing techniques for roundoff error computation and approximation of individual elementary function calls, and provides automated selection of many parameters. Our experiments show that significant efficiency improvements are possible in exchange for reduced, but guaranteed, accuracy.", "venue": "CAV", "authors": ["Eva  Darulova", "Anastasia  Volkova"], "year": 2019, "n_citations": 8}
{"id": 6346063, "s2_id": "8d0295eff7fa463f8579a68de86e5c91970fc0bc", "title": "Event-Based Automatic Differentiation of OpenMP with OpDiLib", "abstract": "We present the new software OpDiLib, a universal add-on for classical operator overloading AD tools that enables the automatic differentiation (AD) of OpenMP parallelized code. With it, we establish support for OpenMP features in a reverse mode operator overloading AD tool to an extent that was previously only reported on in source transformation tools. We achieve this with an event-based implementation ansatz that is unprecedented in AD. Combined with modern OpenMP features around OMPT, we demonstrate how it can be used to achieve differentiation without any additional modifications of the source code; neither do we impose a priori restrictions on the data access patterns, which makes OpDiLib highly applicable. For further performance optimizations, restrictions like atomic updates on the adjoint variables can be lifted in a fine-grained manner for any parts of the code. OpDiLib can also be applied in a semi-automatic fashion via a macro interface, which supports compilers that do not implement OMPT. In a detailed performance study, we demonstrate the applicability of OpDiLib for a pure operator overloading approach in a hybrid parallel environment. We quantify the cost of atomic updates on the adjoint vector and showcase the speedup and scaling that can be achieved with the different configurations of OpDiLib in both the forward and the reverse pass.", "venue": "ArXiv", "authors": ["Johannes  Bl\u00fchdorn", "Max  Sagebaum", "Nicolas R. Gauger"], "year": 2021, "n_citations": 0}
{"id": 6351060, "s2_id": "17c43fc67536edb3e3e8b125dcfe9b6bf25a1f2a", "title": "CheasePy", "abstract": "CheasePy is code written in Python to run the CHEASE (Cubic Hermite Element Axisymmetric Static Equilibrium) code, which solves the Grad-Shafranov equation for toroidal MHD equilibria using pressure and current profiles and fixed plasma boundaries that is defined by a set of experimental data points (R,Z). The CheasePy code allows an iterative running of the CHEASE code either to check the preservation of MHD equilibria or converging to an experimentally defined total toroidal plasma current by modifying any input quantity.", "venue": "ArXiv", "authors": ["Ehab  Hassan"], "year": 2019, "n_citations": 0}
{"id": 6353674, "s2_id": "5698b3c51723d37c28b27a96cc737674c5363db5", "title": "Higher-order reverse automatic differentiation with emphasis on the third-order", "abstract": "It is commonly assumed that calculating third order information is too expensive for most applications. But we show that the directional derivative of the Hessian ($$D^3 f(x)\\cdot d$$D3f(x)\u00b7d) can be calculated at a cost proportional to that of a state-of-the-art method for calculating the Hessian matrix. We do this by first presenting a simple procedure for designing high order reverse methods and applying it to deduce several methods including a reverse method that calculates $$D^3f(x)\\cdot d$$D3f(x)\u00b7d. We have implemented this method taking into account symmetry and sparsity, and successfully calculated this derivative for functions with a million variables. These results indicate that the use of third order information in a general nonlinear solver, such as Halley\u2013Chebyshev methods, could be a practical alternative to Newton\u2019s method. Furthermore, high-order sensitivity information is used in methods for robust aerodynamic design. An efficient high-order differentiation tool could facilitate the use of similar methods in the design of other mechanical structures.", "venue": "Math. Program.", "authors": ["Robert Mansel Gower", "Artur L. Gower"], "year": 2016, "n_citations": 6}
{"id": 6355210, "s2_id": "e9c0ea227d97e039392348034335debfbbd983d3", "title": "Computing isomorphisms and embeddings of finite fields", "abstract": "Let $\\mathbb{F}_q$ be a finite field. Given two irreducible polynomials $f,g$ over $\\mathbb{F}_q$, with $\\mathrm{deg} f$ dividing $\\mathrm{deg} g$, the finite field embedding problem asks to compute an explicit description of a field embedding of $\\mathbb{F}_q[X]/f(X)$ into $\\mathbb{F}_q[Y]/g(Y)$. When $\\mathrm{deg} f = \\mathrm{deg} g$, this is also known as the isomorphism problem. \nThis problem, a special instance of polynomial factorization, plays a central role in computer algebra software. We review previous algorithms, due to Lenstra, Allombert, Rains, and Narayanan, and propose improvements and generalizations. Our detailed complexity analysis shows that our newly proposed variants are at least as efficient as previously known algorithms, and in many cases significantly better. \nWe also implement most of the presented algorithms, compare them with the state of the art computer algebra software, and make the code available as open source. Our experiments show that our new variants consistently outperform available software.", "venue": "Math. Comput.", "authors": ["Ludovic  Brieulle", "Luca De Feo", "Javad  Doliskani", "Jean-Pierre  Flori", "\u00c9ric  Schost"], "year": 2019, "n_citations": 7}
{"id": 6356336, "s2_id": "4984ac1a2dd4652d7407a02fc9791350d0ec662e", "title": "Introduction to Medical Image Registration with DeepReg, Between Old and New", "abstract": "This document outlines a tutorial to get started with medical image registration using the open-source package DeepReg. The basic concepts of medical image registration are discussed, linking classical methods to newer methods using deep learning. Two iterative, classical algorithms using optimisation and one learning-based algorithm using deep learning are coded step-by-step using DeepReg utilities, all with real, open-accessible, medical data.", "venue": "ArXiv", "authors": ["N. Montana Brown", "Y.  Fu", "S. U. Saeed", "A.  Casamitjana", "Z. M. C. Baum", "R.  Delaunay", "Q.  Yang", "A.  Grimwood", "Z.  Min", "E.  Bonmati", "T.  Vercauteren", "M. J. Clarkson", "Y.  Hu"], "year": 2020, "n_citations": 2}
{"id": 6358384, "s2_id": "bc90bb9a865db462d65ef0bcf3e6e77886df3d55", "title": "A Novel Parallel Algorithm for Gaussian Elimination of Sparse Unsymmetric Matrices", "abstract": "We describe a new algorithm for Gaussian Elimination suitable for general (unsymmetric and possibly singular) sparse matrices of any entry type, which has a natural parallel and distributed-memory formulation but degrades gracefully to sequential execution. \n \nWe present a sample MPI implementation of a program computing the rank of a sparse integer matrix using the proposed algorithm. Some preliminary performance measurements are presented and discussed, and the performance of the algorithm is compared to corresponding state-of-the-art algorithms for floating-point and integer matrices.", "venue": "PPAM", "authors": ["Riccardo  Murri"], "year": 2011, "n_citations": 4}
{"id": 6363229, "s2_id": "f790a213580853cb3634621530aefdf8ab615435", "title": "Large-scale linear regression: Development of high-performance routines", "abstract": "In statistics, series of ordinary least squares problems (OLS) are used to study the linear correlation among sets of variables of interest; in many studies, the number of such variables is at least in the millions, and the corresponding datasets occupy terabytes of disk space. As the availability of large-scale datasets increases regularly, so does the challenge in dealing with them. Indeed, traditional solvers-which rely on the use of \"black-box\" routines optimized for one single OLS-are highly inefficient and fail to provide a viable solution for big-data analyses. As a case study, in this paper we consider a linear regression consisting of two-dimensional grids of related OLS problems that arise in the context of genome-wide association analyses, and give a careful walkthrough for the development of ols-grid, a high-performance routine for shared-memory architectures; analogous steps are relevant for tailoring OLS solvers to other applications. In particular, we first illustrate the design of efficient algorithms that exploit the structure of the OLS problems and eliminate redundant computations; then, we show how to effectively deal with datasets that do not fit in main memory; finally, we discuss how to cast the computation in terms of efficient kernels and how to achieve scalability. Importantly, each design decision along the way is justified by simple performance models. ols-grid enables the solution of 1011 correlated OLS problems operating on terabytes of data in a matter of hours.", "venue": "Appl. Math. Comput.", "authors": ["Alvaro  Frank", "Diego  Fabregat-Traver", "Paolo  Bientinesi"], "year": 2016, "n_citations": 8}
{"id": 6370937, "s2_id": "8ae54f6346b08c6748e45a7055ad8621321c0559", "title": "HERMES: Towards an Integrated Toolbox to Characterize Functional and Effective Brain Connectivity", "abstract": "The analysis of the interdependence between time series has become an important field of research in the last years, mainly as a result of advances in the characterization of dynamical systems from the signals they produce, the introduction of concepts such as generalized and phase synchronization and the application of information theory to time series analysis. In neurophysiology, different analytical tools stemming from these concepts have added to the \u2018traditional\u2019 set of linear methods, which includes the cross-correlation and the coherency function in the time and frequency domain, respectively, or more elaborated tools such as Granger Causality.This increase in the number of approaches to tackle the existence of functional (FC) or effective connectivity (EC) between two (or among many) neural networks, along with the mathematical complexity of the corresponding time series analysis tools, makes it desirable to arrange them into a unified-easy-to-use software package. The goal is to allow neuroscientists, neurophysiologists and researchers from related fields to easily access and make use of these analysis methods from a single integrated toolbox.Here we present HERMES (http://hermes.ctb.upm.es), a toolbox for the Matlab\u00ae environment (The Mathworks, Inc), which is designed to study functional and effective brain connectivity from neurophysiological data such as multivariate EEG and/or MEG records. It includes also visualization tools and statistical methods to address the problem of multiple comparisons. We believe that this toolbox will be very helpful to all the researchers working in the emerging field of brain connectivity analysis.", "venue": "Neuroinformatics", "authors": ["Guiomar  Niso", "Ricardo  Bru\u00f1a", "Ernesto  Pereda", "Ricardo  Guti\u00e9rrez", "Ricardo  Bajo", "Fernando  Maest\u00fa", "Francisco del Pozo"], "year": 2013, "n_citations": 161}
{"id": 6380191, "s2_id": "526787a2a5e149db340b41b038304ccab53edae8", "title": "Optimizing Sparse Matrix-Vector Multiplication on Emerging Many-Core Architectures", "abstract": "Sparse matrix vector multiplication (SpMV) is one of the most common operations in scientific and high-performance applications, and is often responsible for the application performance bottleneck. While the sparse matrix representation has a significant impact on the resulting application performance, choosing the right representation typically relies on expert knowledge and trial and error. This paper provides the first comprehensive study on the impact of sparse matrix representations on two emerging many-core architectures: the Intel's Knights Landing (KNL) XeonPhi and the ARM-based FT-2000Plus (FTP). Our large-scale experiments involved over 9,500 distinct profiling runs performed on 956 sparse datasets and five mainstream SpMV representations. We show that the best sparse matrix representation depends on the underlying architecture and the program input. To help developers to choose the optimal matrix representation, we employ machine learning to develop a predictive model. Our model is first trained offline using a set of training examples. The learned model can be used to predict the best matrix representation for any unseen input for a given architecture. We show that our model delivers on average 95% and 91% of the best available performance on KNL and FTP respectively, and it achieves this with no runtime profiling overhead.", "venue": "ArXiv", "authors": ["Shizhao  Chen", "Jianbin  Fang", "Donglin  Chen", "Chuanfu  Xu", "Zheng  Wang"], "year": 2018, "n_citations": 5}
{"id": 6384348, "s2_id": "0b4e82c71f3c34dd394f28cacfcf9bb2c165eea8", "title": "Hierarchical N-body Simulations with Autotuning for Heterogeneous Systems", "abstract": "Algorithms designed to efficiently solve the classical N-body problem of mechanics fit well on GPU hardware and exhibit excellent scalability on many GPUs. Their computational intensity makes them a promising approach for other applications amenable to an N-body formulation. Adding features such as autotuning makes multipole-type algorithms ideal for heterogeneous computing environments.", "venue": "Computing in Science & Engineering", "authors": ["Rio  Yokota", "Lorena A. Barba"], "year": 2012, "n_citations": 41}
{"id": 6388634, "s2_id": "01c8f02b49e82de6e56bcd289bc1d02054e03e22", "title": "Subtotal ordering -- a pedagogically advantageous algorithm for computing total degree reverse lexicographic order", "abstract": "Total degree reverse lexicographic order is currently generally regarded as most often fastest for computing Grobner bases. This article describes an alternate less mysterious algorithm for computing this order using exponent subtotals and describes why it should be very nearly the same speed the traditional algorithm, all other things being equal. However, experimental evidence suggests that subtotal order is actually slightly faster for the Mathematica R Grobner basis implementation more often than not. This is probably because the weight vectors associated with the natural subtotal weight matrix and with the usual total degree reverse lexicographic weight matrix are different, and Mathematica also uses those the corresponding weight vectors to help select successive S polynomials and divisor polynomials: Those selection heuristics appear to work slightly better more often with subtotal weight vectors. However, the most important advantage of exponent subtotals is pedagogical. It is easier to understand than the total degree reverse lexicographic algorithm, and it is more evident why the resulting order is often the fastest known order for computing Grobner bases.", "venue": "ArXiv", "authors": ["David R. Stoutemyer"], "year": 2012, "n_citations": 0}
{"id": 6389252, "s2_id": "e6004d5a3da169680c0b82696ff1f75b8a389620", "title": "FusionStitching: Deep Fusion and Code Generation for Tensorflow Computations on GPUs", "abstract": "In recent years, there is a surge on machine learning applications in industry. Many of them are based on popular AI frameworks like Tensorflow, Torch, Caffe, or MxNet, etc, and are enpowered by accelerator platforms such as GPUs. One important challenge of running Tensorflow computations on GPUs is the fine granularity problem, namely, FLOPS of individual ops are far from enough to fully exploit the computing power of underlying accelerators. The XLA framework provides a solid foundation to explore this problem further. In this paper, we propose FusionStitching, a novel, comprehensive Op fusion and code generation system to stitch computations into large GPU kernels. Experimental results on four public models and two of our large inhouse applications show another 55% (geometric mean) reduction of GPU kernel launches, compared to the XLA fusion baseline. This increases the E2E performance of both of our latency critical inhouse applications up to 20%.", "venue": "ArXiv", "authors": ["Guoping  Long", "Jun  Yang", "Kai  Zhu", "Wei  Lin"], "year": 2018, "n_citations": 6}
{"id": 6390354, "s2_id": "4bfd216bf471d624603de82da46de5bc77fc027a", "title": "micompr: An R Package for Multivariate Independent Comparison of Observations", "abstract": "The R package micompr implements a procedure for assessing if two or more multivariate samples are drawn from the same distribution. The procedure uses principal component analysis to convert multivariate observations into a set of linearly uncorrelated statistical measures, which are then compared using a number of statistical methods. This technique is independent of the distributional properties of samples and automatically selects features that best explain their differences. The procedure is appropriate for comparing samples of time series, images, spectrometric measures or similar high-dimension multivariate observations.", "venue": "R J.", "authors": ["Nuno  Fachada", "Vitor V. Lopes", "Rui Costa Martins", "Agostinho C. Rosa"], "year": 2016, "n_citations": 8}
{"id": 6403972, "s2_id": "4de9ba48ad6d02aca030558472e2320e0642e90a", "title": "An R Package for generating covariance matrices for maximum-entropy sampling from precipitation chemistry data", "abstract": "We present an open-source R package (MESgenCov v 0.1.0) for temporally fitting multivariate precipitation chemistry data and extracting a covariance matrix for use in the MESP (maximum-entropy sampling problem). We provide multiple functionalities for modeling and model assessment. The package is tightly coupled with NADP/NTN (National Atmospheric Deposition Program / National Trends Network) data from their set of 379 monitoring sites, 1978--present. The user specifies the sites, chemicals, and time period desired, fits an appropriate user-specified univariate model for each site and chemical selected, and the package produces a covariance matrix for use by MESP algorithms.", "venue": "ArXiv", "authors": ["Hessa  Al-Thani", "Jon  Lee"], "year": 2020, "n_citations": 2}
{"id": 6405294, "s2_id": "37bd8e950cd99b854c553170c73f554fa1e5ccbd", "title": "Software for Distributed Computation on Medical Databases: A Demonstration Project", "abstract": "Bringing together the information latent in distributed medical databases promises to personalize medical care by enabling reliable, stable modeling of outcomes with rich feature sets (including patient characteristics and treatments received). However, there are barriers to aggregation of medical data, due to lack of standardization of ontologies, privacy concerns, proprietary attitudes toward data, and a reluctance to give up control over end use. Aggregation of data is not always necessary for model fitting. In models based on maximizing a likelihood, the computations can be distributed, with aggregation limited to the intermediate results of calculations on local data, rather than raw data. Distributed fitting is also possible for singular value decomposition. There has been work on the technical aspects of shared computation for particular applications, but little has been published on the software needed to support the \"social networking\" aspect of shared computing, to reduce the barriers to collaboration. We describe a set of software tools that allow the rapid assembly of a collaborative computational project, based on the flexible and extensible R statistical software and other open source packages, that can work across a heterogeneous collection of database environments, with full transparency to allow local officials concerned with privacy protections to validate the safety of the method. We describe the principles, architecture, and successful test results for the site-stratified Cox model and rank-k Singular Value Decomposition (SVD).", "venue": "ArXiv", "authors": ["Balasubramanian  Narasimhan", "Daniel L. Rubin", "Samuel M. Gross", "Marina  Bendersky", "Philip W. Lavori"], "year": 2014, "n_citations": 9}
{"id": 6409355, "s2_id": "fcb3e8106ecd041b57d65cbda692c0ff1ff227e7", "title": "jInv-a Flexible Julia Package for PDE Parameter Estimation", "abstract": "Estimating parameters of Partial Differential Equations (PDEs) from noisy and indirect measurements often requires solving ill-posed inverse problems. These so called parameter estimation or inverse medium problems arise in a variety of applications such as geophysical, medical imaging, and nondestructive testing. Their solution is computationally intense since the underlying PDEs need to be solved numerous times until the reconstruction of the parameters is sufficiently accurate. Typically, the computational demand grows significantly when more measurements are available, which poses severe challenges to inversion algorithms as measurement devices become more powerful. \nIn this paper we present jInv, a flexible framework and open source software that provides parallel algorithms for solving parameter estimation problems with many measurements. Being written in the expressive programming language Julia, jInv is portable, easy to understand and extend, cross-platform tested, and well-documented. It provides novel parallelization schemes that exploit the inherent structure of many parameter estimation problems and can be used to solve multiphysics inversion problems as is demonstrated using numerical experiments motivated by geophysical imaging.", "venue": "SIAM J. Sci. Comput.", "authors": ["Lars  Ruthotto", "Eran  Treister", "Eldad  Haber"], "year": 2017, "n_citations": 32}
{"id": 6409442, "s2_id": "0464c3a2091a601eba1703d8427fe33357c6b5b9", "title": "Analytical Nonlocal Electrostatics Using Eigenfunction Expansions of Boundary-Integral Operators", "abstract": "In this paper, we present an analytical solution to nonlocal continuum electrostatics for an arbitrary charge distribution in a spherical solute. Our approach relies on two key steps: (1) re-formulating the PDE problem using boundary-integral equations, and (2) diagonalizing the boundary-integral operators using the fact their eigenfunctions are the surface spherical harmonics. To introduce this uncommon approach for analytical calculations in separable geometries, we rederive Kirkwood\u2019s classic results for a protein surrounded concentrically by a pure-water ion-exclusion layer and then a dilute electrolyte (modeled with the linearized Poisson\u2013Boltzmann equation). Our main result, however, is an analytical method for calculating the reaction potential in a protein embedded in a nonlocal-dielectric solvent, the Lorentz model studied by Dogonadze and Kornyshev. The analytical method enables biophysicists to study the new nonlocal theory in a simple, computationally fast way; an open-source MATLAB implementation is included as supplemental information.", "venue": "ArXiv", "authors": ["Jaydeep P. Bardhan", "Matthew G. Knepley", "Peter R. Brune"], "year": 2012, "n_citations": 1}
{"id": 6415498, "s2_id": "30bfec22a8fd87aa7505c0442260242fd74fefcf", "title": "FunGrim: A Symbolic Library for Special Functions", "abstract": "We present the Mathematical Functions Grimoire (FunGrim), a website and database of formulas and theorems for special functions. We also discuss the symbolic computation library used as the backend and main development tool for FunGrim, and the Grim formula language used in these projects to represent mathematical content semantically.", "venue": "ICMS", "authors": ["Fredrik  Johansson"], "year": 2020, "n_citations": 1}
{"id": 6416459, "s2_id": "22b57f72325e395488d0d189821d05d4707954dd", "title": "Nauticle: a general-purpose particle-based simulation tool", "abstract": "Nauticle is a general-purpose numerical solver pursuing the easy adoption and application of arbitrary particle-based numerical schemes. Phenomena seemingly different from both physical and length-scale point of views, such as the motion of celestial objects or individuals in a flowing crowd are consid- ered as processes governed by internodal (interplanetary, interpersonal etc.) laws. Furthermore, a skeleton comprised of a system of spatially distributed particles is separated from the physical laws, which is therefore arbitrarily exchangeable and combineable. Motivated by this generalization, Nauticle considers interactions as mathematical functions operating on field variables over a particle system. Thus the governing equations are moved to the users level, facilitating the handling of almost arbitrary equations using the inter- action operators of the adopted interaction laws.", "venue": "ArXiv", "authors": ["Bal\u00e1zs  T\u00f3th"], "year": 2017, "n_citations": 4}
{"id": 6417356, "s2_id": "5a8a8f38da2cbb81c8a5c3f38ee8b12ff86c88eb", "title": "Arrangement computation for planar algebraic curves", "abstract": "We present a new certified and complete algorithm to compute arrangements of real planar algebraic curves. Our algorithm provides a geometric-topological analysis of the decomposition of the plane induced by a finite number of algebraic curves in terms of a cylindrical algebraic decomposition of the plane. Compared to previous approaches, we improve in two main aspects: Firstly, we significantly limit the types of involved exact operations, that is, our algorithms only use resultant and gcd computations as purely symbolic operations. Secondly, we introduce a new hybrid method in the lifting step of our algorithm which combines the use of a certified numerical complex root solver and information derived from the resultant computation. Additionally, we never consider any coordinate transformation and the output is also given with respect to the initial coordinate system.\n We implemented our algorithm as a prototypical package of the C++-library Cgal. Our implementation exploits graphics hardware to expedite the resultant and gcd computation. We also compared our implementation with the current reference implementation, that is, Cgal's curve analysis and arrangement for algebraic curves. For various series of challenging instances, our experiments show that the new implementation outperforms the existing one.", "venue": "SNC '11", "authors": ["Eric  Berberich", "Pavel  Emeliyanenko", "Alexander  Kobel", "Michael  Sagraloff"], "year": 2012, "n_citations": 17}
{"id": 6429289, "s2_id": "6dc11bd4108950ef398458cf791e1c20d2aeaaaf", "title": "MOOSE: Enabling Massively Parallel Multiphysics Simulation", "abstract": "Harnessing modern parallel computing resources to achieve complex multi-physics simulations is a daunting task. The Multiphysics Object Oriented Simulation Environment (MOOSE) aims to enable such development by providing simplified interfaces for specification of partial differential equations, boundary conditions, material properties, and all aspects of a simulation without the need to consider the parallel, adaptive, nonlinear, finite-element solve that is handled internally. Through the use of interfaces and inheritance, each portion of a simulation becomes reusable and composable in a manner that allows disparate research groups to share code and create an ecosystem of growing capability that lowers the barrier for the creation of multiphysics simulation codes. Included within the framework is a unique capability for building multiscale, multiphysics simulations through simultaneous execution of multiple sub-applications with data transfers between the scales. Other capabilities include automatic differentiation, scaling to a large number of processors, hybrid parallelism, and mesh adaptivity. To date, MOOSE-based applications have been created in areas of science and engineering such as nuclear physics, geothermal science, magneto-hydrodynamics, seismic events, compressible and incompressible fluid flow, microstructure evolution, and advanced manufacturing processes.", "venue": "SoftwareX", "authors": ["Cody J. Permann", "Derek R. Gaston", "David  Andrs", "Robert W. Carlsen", "Fande  Kong", "Alexander D. Lindsay", "Jason M. Miller", "John W. Peterson", "Andrew E. Slaughter", "Roy H. Stogner", "Richard C. Martineau"], "year": 2020, "n_citations": 117}
{"id": 6430159, "s2_id": "252577efc96902ed2422bc4333bdc85a2cc24d88", "title": "Flexible numerical optimization with ensmallen", "abstract": "This report provides an introduction to the ensmallen numerical optimization library, as well as a deep dive into the technical details of how it works. The library provides a fast and flexible C++ framework for mathematical optimization of arbitrary user-supplied functions. A large set of pre-built optimizers is provided, including many variants of Stochastic Gradient Descent and Quasi-Newton optimizers. Several types of objective functions are supported, including differentiable, separable, constrained, and categorical objective functions. Implementation of a new optimizer requires only one method, while a new objective function requires typically only one or two C++ methods. Through internal use of C++ template metaprogramming, ensmallen provides support for arbitrary user-supplied callbacks and automatic inference of unsupplied methods without any runtime overhead. Empirical comparisons show that ensmallen outperforms other optimization frameworks (such as Julia and SciPy), sometimes by large margins. The library is available at this https URL and is distributed under the permissive BSD license.", "venue": "ArXiv", "authors": ["Ryan R. Curtin", "Marcus  Edel", "Rahul Ganesh Prabhu", "Suryoday  Basak", "Zhihao  Lou", "Conrad  Sanderson"], "year": 2020, "n_citations": 1}
{"id": 6436773, "s2_id": "3f33390e951699ba44c29029a7344fc47766f2b6", "title": "Leveraging the bfloat16 Artificial Intelligence Datatype For Higher-Precision Computations", "abstract": "In recent years fused-multiply-add (FMA) units with lower-precision multiplications and higher-precision accumulation have proven useful in machine learning/artificial intelligence applications, most notably in training deep neural networks due to their extreme computational intensity. Compared to classical IEEE-754 32 bit (FP32) and 64 bit (FP64) arithmetic, these reduced precision arithmetic can naturally be sped up disproportional to their shortened width. The common strategy of all major hardware vendors is to aggressively further enhance their performance disproportionately. One particular FMA operation that multiplies two BF16 numbers while accumulating in FP32 has been found useful in deep learning, where BF16 is the 16-bit floating point datatype with IEEE FP32 numerical range but 8 significant bits of precision. In this paper, we examine the use this FMA unit to implement higher-precision matrix routines in terms of potential performance gain and implications on accuracy. We demonstrate how a decomposition into multiple smaller datatypes can be used to assemble a high-precision result, leveraging the higher precision accumulation of the FMA unit. We first demonstrate that computations of vector inner products and by natural extension, matrix-matrix products can be achieved by decomposing FP32 numbers in several BF16 numbers followed by appropriate computations that can accommodate the dynamic range and preserve accuracy compared to standard FP32 computations, while projecting up to 5.2x speed-up. Furthermore, we examine solution of linear equations formulated in the residual form that allows for iterative refinement. We demonstrate that the solution obtained to be comparable to those offered by FP64 under a large range of linear system condition numbers.", "venue": "2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)", "authors": ["Greg  Henry", "Ping Tak Peter Tang", "Alexander  Heinecke"], "year": 2019, "n_citations": 17}
{"id": 6444988, "s2_id": "01fd68e4f11c974fc797bb124dff7ef48d415a3f", "title": "Efficient algorithms for computing a rank-revealing UTV factorization on parallel computing architectures", "abstract": "The randomized singular value decomposition (RSVD) is by now a well established technique for efficiently computing an approximate singular value decomposition of a matrix. Building on the ideas that underpin the RSVD, the recently proposed algorithm \u201crandUTV\u201d computes a full factorization of a given matrix that provides low-rank approximations with near-optimal error. Because the bulk of randUTV is cast in terms of communication-efficient operations like matrixmatrix multiplication and unpivoted QR factorizations, it is faster than competing rank-revealing factorization methods like column pivoted QR in most high performance computational settings. In this article, optimized randUTV implementations are presented for both shared memory and distributed memory computing environments. For shared memory, randUTV is redesigned in terms of an algorithm-by-blocks that, together with a runtime task scheduler, eliminates bottlenecks from data synchronization points to achieve acceleration over the standard blocked algorithm, based on a purely fork-join approach. The distributed memory implementation is based on the ScaLAPACK library. The performances of our new codes compare favorably with competing factorizations available on both shared memory and distributed memory architectures.", "venue": "ArXiv", "authors": ["N.  Heavner", "F. D. Igual", "G.  Quintana-Ort'i", "P. G. Martinsson"], "year": 2021, "n_citations": 0}
{"id": 6466140, "s2_id": "27e1caff568315abcb36b4949011be040ada6168", "title": "Series misdemeanors", "abstract": "Puiseux series are power series in which the exponents can be fractional and/or negative rational numbers. Several computer algebra systems have one or more built-in or loadable functions for computing truncated Puiseux series -- perhaps generalized to allow coefficients containing functions of the series variable that are dominated by any power of that variable, such as logarithms and nested logarithms of the series variable. Some computer-algebra systems also offer functions that can compute more-general truncated recursive hierarchical series. However, for all of these kinds of truncated series there are important implementation details that haven't been addressed before in the published literature and in current implementations.\n For implementers this article contains ideas for designing more convenient, correct, and efficient implementations or improving existing ones. For users, this article is a warning about some of these limitations. More specifically, this article discusses issues such as avoiding unnecessary restrictions such as prohibiting negative or fractional requested orders, the pros and cons of displaying results with explicit infectious error terms of the form o (...), O (...), and/or \u03b8(...), efficient data structures, and algorithms that efficiently give users exactly the order or number of nonzero terms they request..\n Most of the ideas in this article have been implemented in the computer-algebra within the TI-Nspire calculator, Windows and Macintosh products.", "venue": "ACCA", "authors": ["David R. Stoutemyer"], "year": 2013, "n_citations": 1}
{"id": 6466936, "s2_id": "7b6bc33abe23ee86e86078081c97a38bbd1d204c", "title": "A Search for Good Pseudo-random Number Generators : Survey and Empirical Studies", "abstract": "In today's world, several applications demand numbers which appear random but are generated by a background algorithm; that is, pseudo-random numbers. Since late $19^{th}$ century, researchers have been working on pseudo-random number generators (PRNGs). Several PRNGs continue to develop, each one demanding to be better than the previous ones. In this scenario, this paper targets to verify the claim of so-called good generators and rank the existing generators based on strong empirical tests in same platforms. To do this, the genre of PRNGs developed so far has been explored and classified into three groups -- linear congruential generator based, linear feedback shift register based and cellular automata based. From each group, well-known generators have been chosen for empirical testing. Two types of empirical testing has been done on each PRNG -- blind statistical tests with Diehard battery of tests, TestU01 library and NIST statistical test-suite and graphical tests (lattice test and space-time diagram test). Finally, the selected $29$ PRNGs are divided into $24$ groups and are ranked according to their overall performance in all empirical tests.", "venue": "ArXiv", "authors": ["Kamalika  Bhattacharjee", "Krishnendu  Maity", "Sukanta  Das"], "year": 2018, "n_citations": 8}
{"id": 6472434, "s2_id": "560e6228f51158f4a59cdb96c4773bedb308a4fc", "title": "Hierarchical Jacobi Iteration for Structured Matrices on GPUs using Shared Memory", "abstract": "High fidelity scientific simulations modeling physical phenomena typically require solving large linear systems of equations which result from discretization of a partial differential equation (PDE) by some numerical method. This step often takes a vast amount of computational time to complete, and therefore presents a bottleneck in simulation work. Solving these linear systems efficiently requires the use of massively parallel hardware with high computational throughput, as well as the development of algorithms which respect the memory hierarchy of these hardware architectures to achieve high memory bandwidth. \nIn this paper, we present an algorithm to accelerate Jacobi iteration for solving structured problems on graphics processing units (GPUs) using a hierarchical approach in which multiple iterations are performed within on-chip shared memory every cycle. A domain decomposition style procedure is adopted in which the problem domain is partitioned into subdomains whose data is copied to the shared memory of each GPU block. Jacobi iterations are performed internally within each block's shared memory, avoiding the need to perform expensive global memory accesses every step. We test our algorithm on the linear systems arising from discretization of Poisson's equation in 1D and 2D, and observe speedup in convergence using our shared memory approach compared to a traditional Jacobi implementation which only uses global memory on the GPU. We observe a x8 speedup in convergence in the 1D problem and a nearly x6 speedup in the 2D case from the use of shared memory compared to a conventional GPU approach.", "venue": "ArXiv", "authors": ["Mohammad Shafaet Islam", "Qiqi  Wang"], "year": 2020, "n_citations": 1}
{"id": 6476508, "s2_id": "15d7ca8b3810ac4a11c29b61b2ceedcaa9ca96bc", "title": "The deal.II library, Version 9.3", "abstract": "Abstract This paper provides an overview of the new features of the finite element library deal.II, version 9.3.", "venue": "J. Num. Math.", "authors": ["Daniel  Arndt", "Wolfgang  Bangerth", "Bruno  Blais", "Marc  Fehling", "Rene  Gassm\u00f6ller", "Timo  Heister", "Luca  Heltai", "Uwe  K\u00f6cher", "Martin  Kronbichler", "Matthias  Maier", "Peter  Munch", "Jean-Paul  Pelteret", "Sebastian  Proell", "Konrad  Simon", "Bruno  Turcksin", "David  Wells", "Jiaqi  Zhang"], "year": 2021, "n_citations": 7}
{"id": 6478074, "s2_id": "7be191cb470f4e15d8b214a5d00eea09ddf04cef", "title": "The Optimal Uncertainty Algorithm in the Mystic Framework", "abstract": "We have recently proposed a rigorous framework for Uncertainty Quantification (UQ) in which UQ objectives and assumption/information set are brought into \nthe forefront, providing a framework for the communication and comparison of UQ \nresults. In particular, this framework does not implicitly impose inappropriate assumptions nor does it repudiate relevant information. \nThis framework, which we call Optimal Uncertainty Quantification (OUQ), is \nbased on the observation that given a set of assumptions and information, there \nexist bounds on uncertainties obtained as values of optimization problems and that \nthese bounds are optimal. It provides a uniform environment for the optimal solution of the problems of validation, certification, experimental design, reduced order \nmodeling, prediction, extrapolation, all under aleatoric and epistemic uncertainties. \nOUQ optimization problems are extremely large, and even though under general \nconditions they have finite-dimensional reductions, they must often be solved numerically. This general algorithmic framework for OUQ has been implemented in the \nmystic optimization framework. We describe this implementation, and demonstrate \nits use in the context of the Caltech surrogate model for hypervelocity impact.", "venue": "ArXiv", "authors": ["Mike  McKerns", "Houman  Owhadi", "Clint  Scovel", "Timothy John Sullivan", "Michael  Ortiz"], "year": 2012, "n_citations": 12}
{"id": 6479218, "s2_id": "1c8716888df008870c209e5682ccea9fe771f17d", "title": "A Highly Efficient Parallel Algorithm for Computing the Fiedler Vector", "abstract": "The eigenvector corresponding to the second smallest eigen value of the Laplacian of a graph, known as the Fiedler vector, has a number of applications in areas t hat include matrix reordering, graph partitioning, protein analysis, data mining, machine learning, and web search. The computation of the Fiedler vector has been regarded as an expensive process as it involv es s l ing a large eigenvalue problem. We present a novel and efficient parallel algorithm for computi ng he Fiedler vector of large graphs based on the Trace Minimization algorithm (Sameh, et.al). We compar e the parallel performance of our method with a multilevel scheme, designed specifically for computi ng he Fiedler vector, which is implemented in routine MC73Fiedler of the Harwell Subroutine Library (HSL). In additio n, we compare the quality of the Fiedler vector for the application of weighted matrix reordering and provide a metric for measuring the quality of reordering.", "venue": "ArXiv", "authors": ["Murat  Manguoglu"], "year": 2010, "n_citations": 7}
{"id": 6480332, "s2_id": "77c48e4b63105359a0a33802c2df54e1a963330f", "title": "Example Setups of Navier-Stokes Equations with Control and Observation: Spatial Discretization and Representation via Linear-quadratic Matrix Coefficients", "abstract": "We provide spatial discretizations of nonlinear incompressible Navier-Stokes equations with inputs and outputs in the form of matrices ready to use in any numerical linear algebra package. We discuss the assembling of the system operators and the realization of boundary conditions and inputs and outputs. We describe the two benchmark problems - the driven cavity and the cylinder wake - and provide the corresponding data. The use of the data is illustrated by numerous example setups. The test cases are provided as plain PYTHON or OCTAVE/MATLAB script files for immediate replication.", "venue": "ArXiv", "authors": ["Maximilian  Behr", "Peter  Benner", "Jan  Heiland"], "year": 2017, "n_citations": 8}
{"id": 6487520, "s2_id": "2b66fa4a01b07bdd618ddd01b183007d3480afff", "title": "Singular Values using Cholesky Decomposition", "abstract": "In this paper two ways to compute singular values are presented which use Cholesky decomposition as their basic operation.", "venue": "ArXiv", "authors": ["Aravindh  Krishnamoorthy", "Kenan  Kocagoez"], "year": 2012, "n_citations": 2}
{"id": 6487742, "s2_id": "7fc662d50be02b0c1d6673666ea300eaf30ee0df", "title": "PittPack: An Open-Source Poisson's Equation Solver for Extreme-Scale Computing with Accelerators", "abstract": "We present a parallel implementation of a direct solver for the Poisson's equation on extreme-scale supercomputers with accelerators. We introduce a chunked-pencil decomposition as the domain-decomposition strategy to distribute work among processing elements to achieve superior scalability at large number of accelerators. Chunked-pencil decomposition enables overlapping nodal communication and data transfer between the central processing units (CPUs) and the graphics processing units (GPUs). Second, it improves data locality by keeping neighboring elements in adjacent memory locations. Third, it allows usage of shared-memory for certain segments of the algorithm when possible, and last but not least, it enables contiguous message transfer among the nodes. Two different communication patterns are designed. The fist pattern aims to fully overlap the communication with data transfer and designed for speedup of overall turnaround time, whereas the second method concentrates on low memory usage and is more network friendly for computations at extreme scale. To ensure software portability, we interleave OpenACC with MPI in the software. The numerical solution and its formal second order of accuracy is verified using method of manufactured solutions for various combinations of boundary conditions. Weak scaling analysis is performed using up to 1.1 trillion Cartesian mesh points using 16384 GPUs on a petascale leadership class supercomputer.", "venue": "Comput. Phys. Commun.", "authors": ["Jaber J. Hasbestan", "Inanc  Senocak"], "year": 2020, "n_citations": 1}
{"id": 6489397, "s2_id": "cef52bea3f13b20d6a31391034b0fccec71c6781", "title": "The NumPy Array: A Structure for Efficient Numerical Computation", "abstract": "In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.", "venue": "Computing in Science & Engineering", "authors": ["St\u00e9fan van der Walt", "S. Chris Colbert", "Ga\u00ebl  Varoquaux"], "year": 2011, "n_citations": 6031}
{"id": 6490773, "s2_id": "be329ad64e94eb9364170cf34dba805cc60c6c59", "title": "Raising the Performance of the Tinker-HP Molecular Modeling Package [Article v1.0]", "abstract": "This living paper reviews the present High Performance Computing (HPC) capabilities of the Tinker-HP molecular modeling package. We focus here on the reference, double precision, massively parallel molecular dynamics engine present in Tinker-HP and dedicated to perform large scale simulations. We show how it can be adapted to recent Intel Central Processing Unit (CPU) petascale architectures. First, we discuss the new set of Intel Advanced Vector Extensions 512 (Intel AVX-512) instructions present in recent Intel processors (e.g., the Intel Xeon Scalable and Intel Xeon Phi 2nd generation processors) allowing for larger vectorization enhancements. These instructions constitute the central source of potential computational gains when using the latest processors, justifying important vectorization efforts for developers. We then briefly review the organization of the Tinker-HP code and identify the computational hotspots which require Intel AVX-512 optimization and we propose a general and optimal strategy to vectorize those particular parts of the code. We intended to present our optimization strategy in a pedagogical way so it could benefit to other researchers and students interested in gaining performances in their own software. Finally we present the performance enhancements obtained compared to the unoptimized code both sequentially and at the scaling limit in parallel for classical non-polarizable (CHARMM) and polarizable force fields (AMOEBA). This paper never ceases to be updated as we accumulate new data on the associated Github repository between new versions of this living paper.", "venue": "Living Journal of Computational Molecular Science", "authors": ["Luc-Henri  Jolly", "Alejandro  Duran", "Louis  Lagardere", "Jay W. Ponder", "Pengyu  Ren", "Jean-Philip  Piquemal"], "year": 2019, "n_citations": 7}
{"id": 6491228, "s2_id": "07c53171617771fa90ef90002a4f8d7e8461a2fb", "title": "Far-HO: A Bilevel Programming Package for Hyperparameter Optimization and Meta-Learning", "abstract": "In (Franceschi et al., 2018) we proposed a unified mathematical framework, grounded on bilevel programming, that encompasses gradient-based hyperparameter optimization and meta-learning. We formulated an approximate version of the problem where the inner objective is solved iteratively, and gave sufficient conditions ensuring convergence to the exact problem. In this work we show how to optimize learning rates, automatically weight the loss of single examples and learn hyper-representations with Far-HO, a software package based on the popular deep learning framework TensorFlow that allows to seamlessly tackle both HO and ML problems.", "venue": "ArXiv", "authors": ["Luca  Franceschi", "Riccardo  Grazzi", "Massimiliano  Pontil", "Saverio  Salzo", "Paolo  Frasconi"], "year": 2018, "n_citations": 1}
{"id": 6492298, "s2_id": "3e20e89c2dc544c4f4e05dc6e51e78cebf41e296", "title": "The Alternative Operad Is Not Koszul", "abstract": "Using computer calculations, we prove the statement in the title.", "venue": "Exp. Math.", "authors": ["Askar  Dzhumadil'daev", "Pasha  Zusmanovich"], "year": 2011, "n_citations": 8}
{"id": 6494696, "s2_id": "291626890e54e8c4f5c70808759ea57371b2d388", "title": "Software realization of the complex spectra analysis algorithm in R", "abstract": "Software realization of the complex spectra decomposition on unknown number of similarcomponents is proposed.The algorithm is based on non-linear minimizing the sum of squared residuals of the spectrum model. For the adequacy checking the complex of criteria is used.It tests the model residuals correspondence with the normal distribution, equality to zero of their mean value and autocorrelation. Also the closeness of residuals and experimental data variances is checked.", "venue": "ArXiv", "authors": ["Vladimir  Bakhrushin"], "year": 2015, "n_citations": 0}
{"id": 6496667, "s2_id": "202a842b87196a513c48254ef6f8c99a984482f1", "title": "Sparse Matrix Multiplication On An Associative Processor", "abstract": "Sparse matrix multiplication is an important component of linear algebra computations. Implementing sparse matrix multiplication on an associative processor (AP) enables high level of parallelism, where a row of one matrix is multiplied in parallel with the entire second matrix, and where the execution time of vector dot product does not depend on the vector size. Four sparse matrix multiplication algorithms are explored in this paper, combining AP and baseline CPU processing to various levels. They are evaluated by simulation on a large set of sparse matrices. The computational complexity of sparse matrix multiplication on AP is shown to be an O(nnz) where nnz is the number of nonzero elements. The AP is found to be especially efficient in binary sparse matrix multiplication. AP outperforms conventional solutions in power efficiency.", "venue": "IEEE Transactions on Parallel and Distributed Systems", "authors": ["Leonid  Yavits", "Amir  Morad", "Ran  Ginosar"], "year": 2015, "n_citations": 18}
{"id": 6502759, "s2_id": "4e216f78cdb041170856d9b4d4b9b0f7de329c0a", "title": "Exact symbolic-numeric computation of planar algebraic curves", "abstract": "We present a certified and complete algorithm to compute arrangements of real planar algebraic curves. It computes the decomposition of the plane induced by a finite number of algebraic curves in terms of a cylindrical algebraic decomposition. From a high-level perspective, the overall method splits into two main subroutines, namely an algorithm denoted Bisolve to isolate the real solutions of a zero-dimensional bivariate system, and an algorithm denoted GeoTop to compute the topology of a single algebraic curve. \n \nCompared to existing approaches based on elimination techniques, we considerably improve the corresponding lifting steps in both subroutines. As a result, generic position of the input system is never assumed, and thus our algorithm never demands for any change of coordinates. In addition, we significantly limit the types of symbolic operations involved, that is, we only use resultant and gcd computations as purely symbolic operations. The latter results are achieved by combining techniques from different fields such as (modular) symbolic computation, numerical analysis and algebraic geometry. \n \nWe have implemented our algorithms as prototypical contributions to the C++-project Cgal. We exploit graphics hardware to expedite the remaining symbolic computations. We have also compared our implementation with the current reference implementations, that is, Lgp and Maple\u2019s Isolate for polynomial system solving, and Cgal\u2019s bivariate algebraic kernel for analyses and arrangement computations of algebraic curves. For various series of challenging instances, our exhaustive experiments show that the new implementations outperform the existing ones.", "venue": "Theor. Comput. Sci.", "authors": ["Eric  Berberich", "Pavel  Emeliyanenko", "Alexander  Kobel", "Michael  Sagraloff"], "year": 2013, "n_citations": 21}
{"id": 6506043, "s2_id": "f222ae64c00a2c3bf1294d9acf89a3cf38b74982", "title": "Accelerating scientific computations with mixed precision algorithms", "abstract": "On modern architectures, the performance of 32-bit operations is often at least twice as fast as the performance of 64-bit operations. By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. The approach presented here can apply not only to conventional processors but also to other technologies such as Field Programmable Gate Arrays (FPGA), Graphical Processing Units (GPU), and the STI Cell BE processor. Results on modern processor architectures and the STI Cell BE are presented.", "venue": "Comput. Phys. Commun.", "authors": ["Marc  Baboulin", "Alfredo  Buttari", "Jack J. Dongarra", "Jakub  Kurzak", "Julie  Langou", "Julien  Langou", "Piotr  Luszczek", "Stanimire  Tomov"], "year": 2009, "n_citations": 160}
{"id": 6507541, "s2_id": "dd318d672a50757f15b614236b6d7b0e8a3fd524", "title": "Fast Sparse Matrix-Vector Multiplication on GPUs: Implications for Graph Mining", "abstract": "Scaling up the sparse matrix-vector multiplication kernel on modern Graphics Processing Units (GPU) has been at the heart of numerous studies in both academia and industry. In this article we present a novel non-parametric, self-tunable, approach to data representation for computing this kernel, particularly targeting sparse matrices representing power-law graphs. Using real web graph data, we show how our representation scheme, coupled with a novel tiling algorithm, can yield significant benefits over the current state of the art GPU efforts on a number of core data mining algorithms such as PageRank, HITS and Random Walk with Restart.", "venue": "Proc. VLDB Endow.", "authors": ["Xintian  Yang", "Srinivasan  Parthasarathy", "P.  Sadayappan"], "year": 2011, "n_citations": 125}
{"id": 6513146, "s2_id": "8fa1a7d8a7c2f6dc9f86a114ddf091f11747c0b2", "title": "Enclave Tasking for Discontinuous Galerkin Methods on Dynamically Adaptive Meshes", "abstract": "High-order discontinuous Galerkin (DG) methods promise to be an excellent discretization paradigm for hyperbolic differential equation solvers running on supercomputers, since they combine high ari...", "venue": "SIAM J. Sci. Comput.", "authors": ["Dominic E. Charrier", "Benjamin  Hazelwood", "Tobias  Weinzierl"], "year": 2020, "n_citations": 11}
{"id": 6520249, "s2_id": "5945c017b7abe56686795d3ed030e042b5c823a9", "title": "Triangular decomposition of semi-algebraic systems", "abstract": "Regular chains and triangular decompositions are fundamental and well-developed tools for describing the complex solutions of polynomial systems. This paper proposes adaptations of these tools focusing on solutions of the real analogue: semi-algebraic systems.\n We show that any such system can be decomposed into finitely many regular semi-algebraic systems. We propose two specifications of such a decomposition and present corresponding algorithms. Under some assumptions, one type of decomposition can be computed in singly exponential time w.r.t. the number of variables. We implement our algorithms and the experimental results illustrate their effectiveness.", "venue": "ISSAC", "authors": ["Changbo  Chen", "James H. Davenport", "John P. May", "Marc Moreno Maza", "Bican  Xia", "Rong  Xiao"], "year": 2010, "n_citations": 11}
{"id": 6520525, "s2_id": "f7e634a2824c53e9ad22ffd9578b9887965e28f0", "title": "Scalar Arithmetic Multiple Data: Customizable Precision for Deep Neural Networks", "abstract": "Quantization of weights and activations in Deep Neural Networks (DNNs) is a powerful technique for network compression, and has enjoyed significant attention and success. However, much of the inference-time benefit of quantization is accessible only through customized hardware accelerators or with an FPGA implementation of quantized arithmetic. Building on prior work, we show how to construct very fast implementations of arbitrary bit-precise signed and unsigned integer operations using a software technique which logically embeds a vector architecture with custom bit-width lanes in fixed-width scalar arithmetic. At the strongest level of quantization, our approach yields a maximum speedup of ~ 6\u00d7 on an x86 platform, and ~ 10\u00d7 on an ARM platform versus quantization to native 8-bit integers.", "venue": "2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)", "authors": ["Andrew  Anderson", "David  Gregg"], "year": 2019, "n_citations": 1}
{"id": 6521299, "s2_id": "4741887a0915ebe0326010d366f7a33bc945b87f", "title": "Fast Flexible Function Dispatch in Julia", "abstract": "Technical computing is a challenging application area for programming languages to address. This is evinced by the unusually large number of specialized languages in the area (e.g. MATLAB, R), and the complexity of common software stacks, often involving multiple languages and custom code generators. We believe this is ultimately due to key characteristics of the domain: highly complex operators, a need for extensive code specialization for performance, and a desire for permissive high-level programming styles allowing productive experimentation. The Julia language attempts to provide a more effective structure for this kind of programming by allowing programmers to express complex polymorphic behaviors using dynamic multiple dispatch over parametric types. The forms of extension and reuse permitted by this paradigm have proven valuable for technical computing. We report on how this approach has allowed domain experts to express useful abstractions while simultaneously providing a natural path to better performance for high-level technical code.", "venue": "ArXiv", "authors": ["Jeff  Bezanson", "Jake  Bolewski", "Jiahao  Chen"], "year": 2018, "n_citations": 1}
{"id": 6523879, "s2_id": "51de42a1e8870d11bc07f6d0a4b8ab2d369f7138", "title": "DSLib: An open source library for the dominant set clustering method", "abstract": "DSLib is an open-source implementation of the Dominant Set (DS) clustering algorithm written entirely in Matlab. The DS method is a graph-based clustering technique rooted in the evolutionary game theory that starts gaining lots of interest in the computer science community. Thanks to its duality with game theory and its strict relation to the notion of maximal clique, has been explored in several directions not only related to clustering problems. Applications in graph matching, segmentation, classification and medical imaging are common in literature. This package provides an implementation of the original DS clustering algorithm since no code has been officially released yet, together with a still growing collection of methods and variants related to it. Our library is integrable into a Matlab pipeline without dependencies, it is simple to use and easily extendable for upcoming works. The latest source code, the documentation and some examples can be downloaded from https://xwasco.github.io/DominantSetLibrary.", "venue": "ArXiv", "authors": ["Sebastiano  Vascon", "Samuel Rota Bulo", "Vittorio  Murino", "Marcello  Pelillo"], "year": 2020, "n_citations": 1}
{"id": 6528817, "s2_id": "a4d7e0399e723eb0aacd369249a65720c2ef884c", "title": "Possible Directions for Improving Dependency Versioning in R", "abstract": "One of the most powerful features of R is its infrastructure for contributed code. The built-in package manager and complementary repositories provide a great system for development and exchange of code, and have played an important role in the growth of the platform towards the de-facto standard in statistical computing that it is today. However, the number of packages on CRAN and other repositories has increased beyond what might have been foreseen, and is revealing some limitations of the current design. One such problem is the general lack of dependency versioning in the infrastructure. This paper explores this problem in greater detail, and suggests approaches taken by other open source communities that might work for R as well. Three use cases are defined that exemplify the issue, and illustrate how improving this aspect of package management could increase reliability while supporting further growth of the R community.", "venue": "R J.", "authors": ["Jeroen  Ooms"], "year": 2013, "n_citations": 17}
{"id": 6531826, "s2_id": "a48f4506d57b6533584f3e0e3f8e1e491b138841", "title": "Accelerating an Iterative Eigensolver for Nuclear Structure Configuration Interaction Calculations on GPUs using OpenACC", "abstract": "To accelerate the solution of large eigenvalue problems arising from many-body calculations in nuclear physics on distributed-memory parallel systems equipped with general-purpose Graphic Processing Units (GPUs), we modified a previously developed hybrid MPI/OpenMP implementation of an eigensolver written in FORTRAN 90 by using an OpenACC directives based programming model. Such an approach requires making minimal changes to the original code and enables a smooth migration of large-scale nuclear structure simulations from a distributed-memory many-core CPU system to a distributed GPU system. However, in order to make the OpenACC based eigensolver run efficiently on GPUs, we need to take into account the architectural differences between a many-core CPU and a GPU device. Consequently, the optimal way to insert OpenACC directives may be different from the original way of inserting OpenMP directives. We point out these differences in the implementation of sparse matrix-matrix multiplications (SpMM), which constitutes the main cost of the eigensolver, as well as other differences in the preconditioning step and dense linear algebra operations. We compare the performance of the OpenACC based implementation executed on multiple GPUs with the performance on distributed-memory many-core CPUs, and demonstrate significant speedup achieved on GPUs compared to the on-node performance of a many-core CPU. We also show that the overall performance improvement of the eigensolver on multiple GPUs is more modest due to the communication overhead among different MPI ranks.", "venue": "ArXiv", "authors": ["Pieter  Maris", "Chao  Yang", "Dossay  Oryspayev", "Brandon  Cook"], "year": 2021, "n_citations": 1}
{"id": 6532707, "s2_id": "a7149c8944350cbfe41e7aa0c23a2af43206ffa4", "title": "Memory efficient scheduling of Strassen-Winograd's matrix multiplication algorithm", "abstract": "We propose several new schedules for Strassen-Winograd's matrix multiplication algorithm, they reduce the extra memory allocation requirements by three different means: by introducing a few pre-additions, by overwriting the input matrices, or by using a first recursive level of classical multiplication. In particular, we show two fully in-place schedules: one having the same number of operations, if the input matrices can be overwritten; the other one, slightly increasing the constant of the leading term of the complexity, if the input matrices are read-only. Many of these schedules have been found by an implementation of an exhaustive search algorithm based on a pebble game.", "venue": "ISSAC '09", "authors": ["Brice  Boyer", "Jean-Guillaume  Dumas", "Cl\u00e9ment  Pernet", "Wei  Zhou"], "year": 2009, "n_citations": 38}
{"id": 6541121, "s2_id": "7f18f9b3d3b86a64f04e33d5cd0cbd93ef958649", "title": "Adapting mathematical domain reasoners", "abstract": "Mathematical learning environments help students in mastering mathematical knowledge. Mature environments typically offer thousands of interactive exercises. Providing feedback to students solving interactive exercises requires domain reasoners for doing the exercisespecific calculations. Since a domain reasoner has to solve an exercise in the same way a student should solve it, the structure of domain reasoners should follow the layered structure of the mathematical domains. Furthermore, learners, teachers, and environment builders have different requirements for adapting domain reasoners, such as providing more details, disallowing or enforcing certain solutions, and combining multiple mathematical domains in a new domain. In previous work we have shown how domain reasoners for solving interactive exercises can be expressed in terms of rewrite strategies, rewrite rules, and views. This paper shows how users can adapt and configure such domain reasoners to their own needs. This is achieved by enabling users to explicitly communicate the components that are used for solving an exercise.", "venue": "AISC'10/MKM'10/Calculemus'10", "authors": ["Bastiaan  Heeren", "Johan  Jeuring"], "year": 2010, "n_citations": 7}
{"id": 6541261, "s2_id": "300e2ad1611d0be3c3a435d74d0aa17def776b9a", "title": "Bayesian Network Constraint-Based Structure Learning Algorithms: Parallel and Optimised Implementations in the bnlearn R Package", "abstract": "It is well known in the literature that the problem of learning the structure of Bayesian networks is very hard to tackle: its computational complexity is super-exponential in the number of nodes in the worst case and polynomial in most real-world scenarios. \nEfficient implementations of score-based structure learning benefit from past and current research in optimisation theory, which can be adapted to the task by using the network score as the objective function to maximise. This is not true for approaches based on conditional independence tests, called constraint-based learning algorithms. The only optimisation in widespread use, backtracking, leverages the symmetries implied by the definitions of neighbourhood and Markov blanket. \nIn this paper we illustrate how backtracking is implemented in recent versions of the bnlearn R package, and how it degrades the stability of Bayesian network structure learning for little gain in terms of speed. As an alternative, we describe a software architecture and framework that can be used to parallelise constraint-based structure learning algorithms (also implemented in bnlearn) and we demonstrate its performance using four reference networks and two real-world data sets from genetics and systems biology. We show that on modern multi-core or multiprocessor hardware parallel implementations are preferable over backtracking, which was developed when single-processor machines were the norm.", "venue": "ArXiv", "authors": ["Marco  Scutari"], "year": 2014, "n_citations": 91}
{"id": 6544746, "s2_id": "09dfbd52611188b2a649e62b2f30430907027ff5", "title": "Cactus: Issues for Sustainable Simulation Software", "abstract": "The Cactus Framework is an open-source, modular, portable programming environment for the collaborative development and deployment of scientific applications using high-performance computing. Its roots reach back to 1996 at the National Center for Supercomputer Applications and the Albert Einstein Institute in Germany, where its development jumpstarted. Since then, the Cactus framework has witnessed major changes in hardware infrastructure as well as its own community. This paper describes its endurance through these past changes and, drawing upon lessons from its past, also discusses future", "venue": "ArXiv", "authors": ["Frank  L\u00f6ffler", "Steven R. Brandt", "Gabrielle  Allen", "Erik  Schnetter"], "year": 2013, "n_citations": 8}
{"id": 6550413, "s2_id": "92330d250d2081b479877520ead17b42f10b6ee4", "title": "Improved Time Warp Edit Distance - A Parallel Dynamic Program in Linear Memory", "abstract": "Edit Distance is a classic family of dynamic programming problems, among which Time Warp Edit Distance refines the problem with the notion of a metric and temporal elasticity. A novel Improved Time Warp Edit Distance algorithm that is both massively parallelizable and requiring only linear storage is presented. This method uses the procession of a three diagonal band to cover the original dynamic program space. Every element of the diagonal update can be computed in parallel. The core method is a feature of the TWED Longest Common Subsequence data dependence and is applicable to dynamic programs that share similar band subproblem structure. The algorithm has been implemented as a CUDA C library with Python bindings. Speedups for challenging problems are phenomenal.", "venue": "ArXiv", "authors": ["Garrett  Wright"], "year": 2020, "n_citations": 0}
{"id": 6566031, "s2_id": "c293716cd0f30809294869a68b0b52a2b42bfcc0", "title": "Silent error detection in numerical time-stepping schemes", "abstract": "Errors due to hardware or low-level software problems, if detected, can be fixed by various schemes, such as recomputation from a checkpoint. Silent errors are errors in application state that have escaped low-level error detection. At extreme scale, where machines can perform astronomically many operations per second, silent errors threaten the validity of computed results. We propose a new paradigm for detecting silent errors at the application level. Our central idea is to frequently compare computed values to those provided by a cheap checking computation, and to build error detectors based on the difference between the two output sequences. Numerical analysis provides us with usable checking computations for the solution of initial-value problems in ODEs and PDEs, arguably the most common problems in computational science. Here, we provide, optimize, and test methods based on Runge\u2013Kutta and linear multistep methods for ODEs, and on implicit and explicit finite difference schemes for PDEs. We take the heat equation and Navier\u2013Stokes equations as examples. In tests with artificially injected errors, this approach effectively detects almost all meaningful errors, without significant slowdown.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Austin R. Benson", "Sven  Schmit", "Robert  Schreiber"], "year": 2015, "n_citations": 73}
{"id": 6566324, "s2_id": "30e48f492a0ff0af20cf109ce3fe02ac76c04f57", "title": "Algorithms for large-scale whole genome association analysis", "abstract": "In order to associate complex traits with genetic polymorphisms, genome-wide association studies process huge datasets involving tens of thousands of individuals genotyped for millions of polymorphisms. When handling these datasets, which exceed the main memory of contemporary computers, one faces two distinct challenges: 1) Millions of polymorphisms come at the cost of hundreds of Gigabytes of genotype data, which can only be kept in secondary storage; 2) the relatedness of the test population is represented by a covariance matrix, which, for large populations, can only fit in the combined main memory of a distributed architecture. In this paper, we present solutions for both challenges: The genotype data is streamed from and to secondary storage using a double buffering technique, while the covariance matrix is kept across the main memory of a distributed memory system. We show that these methods sustain high-performance and allow the analysis of enormous datasets.", "venue": "EuroMPI", "authors": ["Elmar  Peise", "Diego  Fabregat-Traver", "Yurii S. Aulchenko", "Paolo  Bientinesi"], "year": 2013, "n_citations": 4}
{"id": 6571514, "s2_id": "b0ae4214acd33d9ee82f3c5b1fd7c337e6b6a414", "title": "NOMAD version 4: Nonlinear optimization with the MADS algorithm", "abstract": "NOMAD is software for optimizing blackbox problems. In continuous development since 2001, it constantly evolved with the integration of new algorithmic features published in scientific publications. These features are motivated by real applications encountered by industrial partners. The latest major release of NOMAD, version 3, dates from 2008. Minor releases are produced as new features are incorporated. The present work describes NOMAD 4, a complete redesign of the previous version, with a new architecture providing more flexible code, added functionalities and reusable code. We introduce algorithmic components, which are building blocks for more complex algorithms, and can initiate other components, launch nested algorithms, or perform specialized tasks. They facilitate the implementation of new ideas, including the MegaSearchPoll component, warm and hot restarts, and a revised version of the PSDMADS algorithm. Another main improvement of NOMAD 4 is the usage of parallelism, to simultaneously compute multiple blackbox evaluations, and to maximize usage of available cores. Running different algorithms, tuning their parameters, and comparing their performance for optimization is simpler than before, while overall optimization performance is maintained between versions 3 and 4. NOMAD is freely available at www.gerad.ca/nomad and the whole project is visible at github.com/bbopt/nomad.", "venue": "ArXiv", "authors": ["Charles  Audet", "S'ebastien Le Digabel", "Viviane Rochon Montplaisir", "Christophe  Tribes"], "year": 2021, "n_citations": 4}
{"id": 6573353, "s2_id": "6819084ce5c5a9973c865edd69f395daf64d08e2", "title": "Non-invasive multigrid for semi-structured grids", "abstract": "Multigrid solvers for hierarchical hybrid grids (HHG) have been proposed to promote the efficient utilization of high performance computer architectures. These HHG meshes are constructed by uniformly refining a relatively coarse fully unstructured mesh. While HHG meshes provide some flexibility for unstructured applications, most multigrid calculations can be accomplished using efficient structured grid ideas and kernels. This paper focuses on generalizing the HHG idea so that it is applicable to a broader community of computational scientists, and so that it is easier for existing applications to leverage structured multigrid components. Specifically, we adapt the structured multigrid methodology to significantly more complex semi-structured meshes. Further, we illustrate how mature applications might adopt a semi-structured solver in a relatively non-invasive fashion. To do this, we propose a formal mathematical framework for describing the semistructured solver. This formalism allows us to precisely define the associated multigrid method and to show its relationship to a more traditional multigrid solver. Additionally, the mathematical framework clarifies the associated software design and implementation. Numerical experiments highlight the relationship of the new solver with classical multigrid. We also demonstrate the generality and potential performance gains associated with this type of semi-structured multigrid.", "venue": "ArXiv", "authors": ["Matthias  Mayr", "Luc  Berger-Vergiat", "Peter  Ohm", "Raymond S. Tuminaro"], "year": 2021, "n_citations": 0}
{"id": 6582957, "s2_id": "62ab96ae5c688413b1275816938e841ee3d3c6c0", "title": "FISLAB - the Fuzzy Inference Tool-box for SCILAB", "abstract": "The present study represents \"The Fislab package of programs meant to develop the fuzzy regulators in the Scilab environment\" in which we present some general issues, usage requirements and the working mode of the Fislab environment. In the second part of the article some features of the Scilab functions from the Fislab package are described.", "venue": "ArXiv", "authors": ["Simona  Apostol"], "year": 2009, "n_citations": 1}
{"id": 6592226, "s2_id": "b159f4022bd998fa504cfc5f9be6af9d34318414", "title": "An exact, cache-localized algorithm for the sub-quadratic convolution of hypercubes", "abstract": "Fast multidimensional convolution can be performed naively in quadratic time and can often be performed more efficiently via the Fourier transform; however, when the dimensionality is large, these algorithms become more challenging. A method is proposed for performing exact hypercube convolution in sub-quadratic time. The method outperforms FFTPACK, called via numpy, and FFTW, called via pyfftw) for hypercube convolution. Embeddings in hypercubes can be paired with sub-quadratic hypercube convolution method to construct sub-quadratic algorithms for variants of vector convolution.", "venue": "ArXiv", "authors": ["Oliver  Serang"], "year": 2016, "n_citations": 0}
{"id": 6608307, "s2_id": "e23e7248f8ec0046b4aac598cc4d9f644976ce74", "title": "Open source software in quantum computing", "abstract": "Open source software is becoming crucial in the design and testing of quantum algorithms. Many of the tools are backed by major commercial vendors with the goal to make it easier to develop quantum software: this mirrors how well-funded open machine learning frameworks enabled the development of complex models and their execution on equally complex hardware. We review a wide range of open source software for quantum computing, covering all stages of the quantum toolchain from quantum hardware interfaces through quantum compilers to implementations of quantum algorithms, as well as all quantum computing paradigms, including quantum annealing, and discrete and continuous-variable gate-model quantum computing. The evaluation of each project covers characteristics such as documentation, licence, the choice of programming language, compliance with norms of software engineering, and the culture of the project. We find that while the diversity of projects is mesmerizing, only a few attract external developers and even many commercially backed frameworks have shortcomings in software engineering. Based on these observations, we highlight the best practices that could foster a more active community around quantum computing software that welcomes newcomers to the field, but also ensures high-quality, well-documented code.", "venue": "PloS one", "authors": ["Mark  Fingerhuth", "Tom\u00e1s  Babej", "Peter  Wittek"], "year": 2018, "n_citations": 49}
{"id": 6608715, "s2_id": "4e3df2fd9b61aaf53165679f3ef33a2d94f65cc7", "title": "Computational topology with Regina: Algorithms, heuristics and implementations", "abstract": "Regina is a software package for studying 3-manifold triangulations and normal surfaces. It includes a graphical user interface and Python bindings, and also supports angle structures, census enumeration, combinatorial recognition of triangulations, and high-level functions such as 3-sphere recognition, unknot recognition and connected sum decomposition. \nThis paper brings 3-manifold topologists up-to-date with Regina as it appears today, and documents for the first time in the literature some of the key algorithms, heuristics and implementations that are central to Regina's performance. These include the all-important simplification heuristics, key choices of data structures and algorithms to alleviate bottlenecks in normal surface enumeration, modern implementations of 3-sphere recognition and connected sum decomposition, and more. We also give some historical background for the project, including the key role played by Rubinstein in its genesis 15 years ago, and discuss current directions for future development.", "venue": "ArXiv", "authors": ["Benjamin A. Burton"], "year": 2012, "n_citations": 21}
{"id": 6609044, "s2_id": "b95b98282509b64d956492279b752e61053ebe48", "title": "Rings: an efficient Java/Scala library for polynomial rings", "abstract": "Abstract In this paper we briefly discuss Rings \u2014\u00a0an efficient lightweight library for commutative algebra. Polynomial arithmetic, GCDs, polynomial factorization and Grobner bases are implemented with the use of modern asymptotically fast algorithms. Rings can be easily interacted or embedded in applications in high-energy physics and other research areas via a simple API with fully typed hierarchy of algebraic structures and algorithms for commutative algebra. The use of the Scala language brings a quite novel powerful, strongly typed functional programming model allowing to write short, expressive, and fast code for applications. At the same time Rings shows one of the best performances among existing software for algebraic calculations. Program summary Program Title: Rings Program Files doi: http://dx.doi.org/10.17632/2k79hftjy9.1 Licensing provisions: Apache 2.0 Programming language: Java, Scala Nature of problem: Fast methods for rational function arithmetic, simplification of polynomial expressions, Grobner bases and other related computer algebra methods naturally arising in physical applications Solution method: Efficient implementation of modern asymptotically fast algorithms in Java language External routines: Java 8 and higher, Scala 2.11 or 2.12 Additional comments: project page: https://github.com/PoslavskySV/rings , \u00a0documentation: http://rings.readthedocs.io/en/latest/", "venue": "Comput. Phys. Commun.", "authors": ["Stanislav  Poslavsky"], "year": 2019, "n_citations": 6}
{"id": 6609164, "s2_id": "028f8d10001e8fb14e7b0b8577b022cb2a01543f", "title": "Delayed approximate matrix assembly in multigrid with dynamic precisions", "abstract": "The accurate assembly of the system matrix is an important step in any code that solves partial differential equations on a mesh. We either explicitly set up a matrix, or we work in a matrix\u2010free environment where we have to be able to quickly return matrix entries upon demand. Either way, the construction can become costly due to nontrivial material parameters entering the equations, multigrid codes requiring cascades of matrices that depend upon each other, or dynamic adaptive mesh refinement that necessitates the recomputation of matrix entries or the whole equation system throughout the solve. We propose that these constructions can be performed concurrently with the multigrid cycles. Initial geometric matrices and low accuracy integrations kickstart the multigrid iterations, while improved assembly data is fed to the solver as and when it becomes available. The time to solution is improved as we eliminate an expensive preparation phase traditionally delaying the actual computation. We eliminate algorithmic latency. Furthermore, we desynchronize the assembly from the solution process. This anarchic increase in the concurrency level improves the scalability. Assembly routines are notoriously memory\u2010 and bandwidth\u2010demanding. As we work with iteratively improving operator accuracies, we finally propose the use of a hierarchical, lossy compression scheme such that the memory footprint is brought down aggressively where the system matrix entries carry little information or are not yet available with high accuracy.", "venue": "Concurr. Comput. Pract. Exp.", "authors": ["Charles D. Murray", "Tobias  Weinzierl"], "year": 2021, "n_citations": 1}
{"id": 6639375, "s2_id": "6053f083165cc67802344fef6aa9003b5459ac92", "title": "Utilizing Static Analysis and Code Generation to Accelerate Neural Networks", "abstract": "As datasets continue to grow, neural network (NN) applications are becoming increasingly limited by both the amount of available computational power and the ease of developing high-performance applications. Researchers often must have expert systems knowledge to make their algorithms run efficiently. Although available computing power increases rapidly each year, algorithm efficiency is not able to keep pace due to the use of general purpose compilers, which are not able to fully optimize specialized application domains. Within the domain of NNs, we have the added knowledge that network architecture remains constant during training, meaning the architecture's data structure can be statically optimized by a compiler. In this paper, we present SONNC, a compiler for NNs that utilizes static analysis to generate optimized parallel code. We show that SONNC's use of static optimizations make it able to outperform hand-optimized C++ code by up to 7.8X, and MATLAB code by up to 24X. Additionally, we show that use of SONNC significantly reduces code complexity when using structurally sparse networks.", "venue": "ICML", "authors": ["Lawrence C. McAfee", "Kunle  Olukotun"], "year": 2012, "n_citations": 1}
{"id": 6640074, "s2_id": "e7f4cf58c9a578ff0fa419d09619efc0e1c3b63a", "title": "A Framework for Lattice QCD Calculations on GPUs", "abstract": "Computing platforms equipped with accelerators like GPUs have proven to provide great computational power. However, exploiting such platforms for existing scientific applications is not a trivial task. Current GPU programming frameworks such as CUDA C/C++ require low-level programming from the developer in order to achieve high performance code. As a result porting of applications to GPUs is typically limited to time-dominant algorithms and routines, leaving the remainder not accelerated which can open a serious Amdahl's law issue. The Lattice QCD application Chroma allows us to explore a different porting strategy. The layered structure of the software architecture logically separates the data-parallel from the application layer. The QCD Data-Parallel software layer provides data types and expressions with stencil-like operations suitable for lattice field theory. Chroma implements algorithms in terms of this high-level interface. Thus by porting the low-level layer one effectively ports the whole application layer in one swing. The QDP-JIT/PTX library, our reimplementation of the low-level layer, provides a framework for Lattice QCD calculations for the CUDA architecture. The complete software interface is supported and thus applications can be run unaltered on GPU-based parallel computers. This reimplementation was possible due to the availability of a JIT compiler which translates an assembly language (PTX) to GPU code. The existing expression templates enabled us to employ compile-time computations in order to build code generators and to automate the memory management for CUDA. Our implementation has allowed us to deploy the full Chroma gauge-generation program on large scale GPU-based machines such as Titan and Blue Waters and accelerate the calculation by more than an order of magnitude.", "venue": "2014 IEEE 28th International Parallel and Distributed Processing Symposium", "authors": ["F. T. Winter", "Mike A. Clark", "R. G. Edwards", "B\u00e1lint  Jo\u00f3"], "year": 2014, "n_citations": 26}
{"id": 6642028, "s2_id": "b2d1d5b78dfb83468dbc1dcd535ad03cc7e72277", "title": "LeoPARD - A Generic Platform for the Implementation of Higher-Order Reasoners", "abstract": "LeoPARD supports the implementation of knowledge representation and reasoning tools for higher-order logic(s). It combines a sophisticated data structure layer (polymorphically typed {\\lambda}-calculus with nameless spine notation, explicit substitutions, and perfect term sharing) with an ambitious multi-agent blackboard architecture (supporting prover parallelism at the term, clause, and search level). Further features of LeoPARD include a parser for all TPTP dialects, a command line interpreter, and generic means for the integration of external reasoners.", "venue": "CICM", "authors": ["Max  Wisniewski", "Alexander  Steen", "Christoph  Benzm\u00fcller"], "year": 2015, "n_citations": 19}
{"id": 6642284, "s2_id": "8b9a526b357664433de2a944d2d10f7f471d56df", "title": "RProtoBuf: Efficient Cross-Language Data Serialization in R", "abstract": "Modern data collection and analysis pipelines often involve a sophisticated mix of applications written in general purpose and specialized programming languages. Many formats commonly used to import and export data between different programs or systems, such as CSV or JSON, are verbose, inefficient, not type-safe, or tied to a specific programming language. Protocol Buffers are a popular method of serializing structured data between applications - while remaining independent of programming languages or operating systems. They offer a unique combination of features, performance, and maturity that seems particularly well suited for data-driven applications and numerical computing. The RProtoBuf package provides a complete interface to Protocol Buffers from the R environment for statistical computing. This paper outlines the general class of data serialization requirements for statistical computing, describes the implementation of the RProtoBuf package, and illustrates its use with example applications in large-scale data collection pipelines and web services.", "venue": "ArXiv", "authors": ["Dirk  Eddelbuettel", "Murray  Stokely", "Jeroen  Ooms"], "year": 2014, "n_citations": 5}
{"id": 6643616, "s2_id": "3b663c889b8d4e25eb27d770ac27939c0efcae1d", "title": "Software for Sparse Tensor Decomposition on Emerging Computing Architectures", "abstract": "In this paper, we develop software for decomposing sparse tensors that is portable to and performant on a variety of multicore, manycore, and GPU computing architectures. The result is a single cod...", "venue": "SIAM J. Sci. Comput.", "authors": ["Eric T. Phipps", "Tamara G. Kolda"], "year": 2019, "n_citations": 18}
{"id": 6645914, "s2_id": "8f5520285c750074b58f4e01be917d3c38c95717", "title": "Refactoring the MPS/University of Chicago Radiative MHD (MURaM) model for GPU/CPU performance portability using OpenACC directives", "abstract": "The MURaM (Max Planck University of Chicago Radiative MHD) code is a solar atmosphere radiative MHD model that has been broadly applied to solar phenomena ranging from quiet to active sun, including eruptive events such as flares and coronal mass ejections. The treatment of physics is sufficiently realistic to allow for the synthesis of emission from visible light to extreme UV and X-rays, which is critical for a detailed comparison with available and future multi-wavelength observations. This component relies critically on the radiation transport solver (RTS) of MURaM; the most computationally intensive component of the code. The benefits of accelerating RTS are multiple fold: A faster RTS allows for the regular use of the more expensive multi-band radiation transport needed for comparison with observations, and this will pave the way for the acceleration of ongoing improvements in RTS that are critical for simulations of the solar chromosphere. We present challenges and strategies to accelerate a multi-physics, multi-band MURaM using a directive-based programming model, OpenACC in order to maintain a single source code across CPUs and GPUs. Results for a 2883 test problem show that MURaM with the optimized RTS routine achieves 1.73x speedup using a single NVIDIA V100 GPU over a fully subscribed 40-core Intel Skylake CPU node and with respect to the number of simulation points (in millions) per second, a single NVIDIA V100 GPU is equivalent to 69 Skylake cores. We also measure parallel performance on up to 96 GPUs and present weak and strong scaling results.", "venue": "PASC", "authors": ["Eric  Wright", "Damien  Przybylski", "Matthias  Rempel", "Cena  Miller", "Supreeth  Suresh", "Shiquan  Su", "Richard  Loft", "Sunita  Chandrasekaran"], "year": 2021, "n_citations": 0}
{"id": 6647909, "s2_id": "5da33ea29af9a8e2d5fdd5f9ccdfe4a2d517887a", "title": "Probabilistic Inference on Noisy Time Series (PINTS)", "abstract": "Time series models are ubiquitous in science, arising in any situation where researchers seek to understand how a system's behaviour changes over time. A key problem in time series modelling is \\emph{inference}; determining properties of the underlying system based on observed time series. For both statistical and mechanistic models, inference involves finding parameter values, or distributions of parameters values, for which model outputs are consistent with observations. A wide variety of inference techniques are available and different approaches are suitable for different classes of problems. This variety presents a challenge for researchers, who may not have the resources or expertise to implement and experiment with these methods. PINTS (Probabilistic Inference on Noisy Time Series - this https URL is an open-source (BSD 3-clause license) Python library that provides researchers with a broad suite of non-linear optimisation and sampling methods. It allows users to wrap a model and data in a transparent and straightforward interface, which can then be used with custom or pre-defined error measures for optimisation, or with likelihood functions for Bayesian inference or maximum-likelihood estimation. Derivative-free optimisation algorithms - which work without harder-to-obtain gradient information - are included, as well as inference algorithms such as adaptive Markov chain Monte Carlo and nested sampling which estimate distributions over parameter values. By making these statistical techniques available in an open and easy-to-use framework, PINTS brings the power of modern statistical techniques to a wider scientific audience.", "venue": "Journal of Open Research Software", "authors": ["Michael  Clerx", "Martin  Robinson", "Ben  Lambert", "Chon Lok Lei", "Sanmitra  Ghosh", "Gary R. Mirams", "David  Gavaghan"], "year": 2019, "n_citations": 22}
{"id": 6649951, "s2_id": "d8be23b1c4b7130e3deea6168cfb1c3bd70a44c9", "title": "TeXmacs-Reduce interface", "abstract": "This tutorial (based on the talk at the TeXmacs workshop in Faro, Portugal, February 26 - March 2, 2012) describes the new and improved Reduce plugin in GNU TeXmacs.", "venue": "ArXiv", "authors": ["Andrey  Grozin"], "year": 2012, "n_citations": 0}
{"id": 6653276, "s2_id": "167ab2674b97797f238d1939d1bf448afa64715f", "title": "Veamy: an extensible object-oriented C++ library for the virtual element method", "abstract": "This paper summarizes the development of Veamy, an object-oriented C++ library for the virtual element method (VEM) on general polygonal meshes, whose modular design is focused on its extensibility. The linear elastostatic and Poisson problems in two dimensions have been chosen as the starting stage for the development of this library. The theory of the VEM, upon which Veamy is built, is presented using a notation and a terminology that resemble the language of the finite element method (FEM) in engineering analysis. Several examples are provided to demonstrate the usage of Veamy, and in particular, one of them features the interaction between Veamy and the polygonal mesh generator PolyMesher. A computational performance comparison between VEM and FEM is also conducted. Veamy is free and open source software.", "venue": "Numerical Algorithms", "authors": ["A.  Ortiz-Bernardin", "C.  Alvarez", "N.  Hitschfeld-Kahler", "A.  Russo", "R.  Silva-Valenzuela", "E.  Olate-Sanzana"], "year": 2019, "n_citations": 5}
{"id": 6657384, "s2_id": "7635f8954842b09a65c22858fa0f5f520832409f", "title": "Next generation input-output data format for HEP using Google's protocol buffers", "abstract": "We propose a data format for Monte Carlo (MC) events, or any structural data, including experimental data, in a compact binary form using variable-size integer encoding as implemented in the Google's Protocol Buffers package. This approach is implemented in the so-called ProMC library which produces smaller file sizes for MC records compared to the existing input-output libraries used in high-energy physics (HEP). Other important features are a separation of abstract data layouts from concrete programming implementations, self-description and random access. Data stored in ProMC files can be written, read and manipulated in a number of programming languages, such C++, Java and Python.", "venue": "ArXiv", "authors": ["Sergei V. Chekanov"], "year": 2013, "n_citations": 8}
{"id": 6662834, "s2_id": "db4ded9429243a3ce1d8d565ab41f16eb3f4028a", "title": "Fast arithmetics in artin-schreier towers over finite fields", "abstract": "An Artin-Schreier tower over the finite field <b>F</b><sub><i>p</i></sub> is a tower of field extensions generated by polynomials of the form <i>X<sup>p</sup></i>-<i>X</i>-\u03b1. Following Cantor and Couveignes, we give algorithms with quasi-linear time complexity for arithmetic operations in such towers. As an application, we present an implementation of Couveignes' algorithm for computing isogenies between elliptic curves using the <i>p</i>-torsion.", "venue": "ISSAC '09", "authors": ["Luca De Feo", "\u00c9ric  Schost"], "year": 2009, "n_citations": 16}
{"id": 6667531, "s2_id": "d047b34db8e8c9867752d2e0e73044830164ca49", "title": "A toolbox of Equation-Free functions in Matlab\\Octave for efficient system level simulation", "abstract": "The `equation-free toolbox' empowers the computer-assisted analysis of complex, multiscale systems. Its aim is to enable you to immediately use microscopic simulators to perform macro-scale system level tasks and analysis, because micro-scale simulations are often the best available description of a system. The methodology bypasses the derivation of macroscopic evolution equations by computing the micro-scale simulator only over short bursts in time on small patches in space, with bursts and patches well-separated in time and space respectively. We introduce the suite of coded equation-free functions in an accessible way, link to more detailed descriptions, discuss their mathematical support, and introduce a novel and efficient algorithm for Projective Integration. Some facets of toolbox development of equation-free functions are then detailed. Download the toolbox functions (this https URL) and use to empower efficient and accurate simulation in a wide range of your science and engineering problems.", "venue": "Numer. Algorithms", "authors": ["John  Maclean", "J. E. Bunder", "A. J. Roberts"], "year": 2021, "n_citations": 3}
{"id": 6669224, "s2_id": "396d1eeac78cb7f7a206cad44e9c24899791a453", "title": "Wilson and Domainwall Kernels on Oakforest-PACS", "abstract": "We report the performance of Wilson and Domainwall Kernels on a new Intel Xeon Phi Knights Landing based machine named Oakforest-PACS, which is co-hosted by University of Tokyo and Tsukuba University and is currently fastest in Japan. This machine uses Intel Omni-Path for the internode network. We compare performance with several types of implementation including that makes use of the Grid library. The code is incorporated with the code set Bridge++.", "venue": "ArXiv", "authors": ["Issaku  Kanamori", "Hideo  Matsufuru"], "year": 2017, "n_citations": 3}
{"id": 6671490, "s2_id": "9f9f74402a7cc5197339321baa35762beaf2c29b", "title": "Reducing communication in algebraic multigrid with multi-step node aware communication", "abstract": "Algebraic multigrid (AMG) is often viewed as a scalable O ( n ) solver for sparse linear systems. Yet, AMG lacks parallel scalability due to increasingly large costs associated with communication, both in the initial construction of a multigrid hierarchy and in the iterative solve phase. This work introduces a parallel implementation of AMG that reduces the cost of communication, yielding improved parallel scalability. It is common in Message Passing Interface (MPI), particularly in the MPI-everywhere approach, to arrange inter-process communication, so that communication is transported regardless of the location of the send and receive processes. Performance tests show notable differences in the cost of intra- and internode communication, motivating a restructuring of communication. In this case, the communication schedule takes advantage of the less costly intra-node communication, reducing both the number and the size of internode messages. Node-centric communication extends to the range of components in both the setup and solve phase of AMG, yielding an increase in the weak and strong scaling of the entire method.", "venue": "Int. J. High Perform. Comput. Appl.", "authors": ["Amanda  Bienz", "William D Gropp", "Luke N Olson"], "year": 2020, "n_citations": 4}
{"id": 6674622, "s2_id": "15980b2721ceab1c7f16a2bcf51febc7ffc75422", "title": "A Branch and Cut Algorithm for the Halfspace Depth Problem", "abstract": "The concept of \\emph{data depth} in non-parametric multivariate descriptive statistics is the generalization of the univariate rank method to multivariate data. \\emph{Halfspace depth} is a measure of data depth. Given a set $S$ of points and a point $p$, the halfspace depth (or rank) of $p$ is defined as the minimum number of points of $S$ contained in any closed halfspace with $p$ on its boundary. Computing halfspace depth is NP-hard, and it is equivalent to the Maximum Feasible Subsystem problem. In this paper a mixed integer program is formulated with the big-$M$ method for the halfspace depth problem. We suggest a branch and cut algorithm for these integer programs. In this algorithm, Chinneck's heuristic algorithm is used to find an upper bound and a related technique based on sensitivity analysis is used for branching. Irreducible Infeasible Subsystem (IIS) hitting set cuts are applied. We also suggest a binary search algorithm which may be more numerically stable. The algorithms are implemented with the BCP framework from the \\textbf{COIN-OR} project.", "venue": "ArXiv", "authors": ["Dan  Chen"], "year": 2007, "n_citations": 4}
{"id": 6681892, "s2_id": "5c09797bb61d0bbf01d671c5c605e347f1807f47", "title": "Least Squares on GPUs in Multiple Double Precision", "abstract": "This paper describes the application of the code generated by the CAMPARY software to accelerate the solving of linear systems in the least squares sense on Graphics Processing Units (GPUs), in double double, quad double, and octo double precision. The goal is to use accelerators to offset the cost overhead caused by multiple double precision arithmetic. For the blocked Householder QR and the back substitution, of interest are those dimensions at which teraflop performance is attained. The other interesting question is the cost overhead factor that appears each time the precision is doubled. Experimental results are reported on five different NVIDIA GPUs, with a particular focus on the P100 and the V100, both capable of teraflop performance. Thanks to the high Compute to Global Memory Access (CGMA) ratios of multiple double arithmetic, teraflop performance is already attained running the double double QR on 1,024-by-1,024 matrices, both on the P100 and the V100. For the back substitution, the dimension of the upper triangular system must be as high as 17,920 to reach one teraflops on the V100, in quad double precision, and then taking only the times spent by the kernels into account. The lower performance of the back substitution in small dimensions does not prevent teraflop performance of the solver at dimension 1,024, as the time for the QR decomposition dominates. In doubling the precision from double double to quad double and from quad double to octo double, the observed cost overhead factors are lower than the factors predicted by the arithmetical operation counts. This observation correlates with the increased performance for increased precision, which can again be explained by the high CGMA ratios.", "venue": "ArXiv", "authors": ["Jan  Verschelde"], "year": 2021, "n_citations": 0}
{"id": 6683926, "s2_id": "13d4ff3e14b0e497d8dd351083494156ff5e3897", "title": "Parallelized Discrete Exterior Calculus for Three-Dimensional Elliptic Problems", "abstract": "A formulation of elliptic boundary value problems is used to develop the first discrete exterior calculus (DEC) library for massively parallel computations with 3D domains. This can be used for steady-state analysis of any physical process driven by the gradient of a scalar quantity, e.g. temperature, concentration, pressure or electric potential, and is easily extendable to transient analysis. In addition to offering this library to the community, we demonstrate one important benefit from the DEC formulation: effortless introduction of strong heterogeneities and discontinuities. These are typical for real materials, but challenging for widely used domain discretization schemes, such as finite elements. Specifically, we demonstrate the efficiency of the method for calculating the evolution of thermal conductivity of a solid with a growing crack population. Future development of the library will deal with transient problems, and more importantly with processes driven by gradients of vector quantities.", "venue": "ArXiv", "authors": ["Pieter D. Boom", "Ashley  Seepujak", "Odysseas  Kosmas", "Lee  Margetts", "Andrey  Jivkov"], "year": 2021, "n_citations": 0}
{"id": 6687817, "s2_id": "39bcbab9d8ec44221f6e8865d322c41de5c60a53", "title": "Construction and implementation of asymptotic expansions for Jacobi\u2013type orthogonal polynomials", "abstract": "AbstractWe are interested in the asymptotic behavior of orthogonal polynomials of the generalized Jacobi type as their degree n goes to \u221e$\\infty $. These are defined on the interval [\u22121, 1] with weight function\nw(x)=(1\u2212x)\u03b1(1+x)\u03b2h(x),\u03b1,\u03b2>\u22121$$w(x)=(1-x)^{\\alpha}(1+x)^{\\beta}h(x), \\quad \\alpha,\\beta>-1 $$ and h(x) a real, analytic and strictly positive function on [\u22121, 1]. This information is available in the work of Kuijlaars et al. (Adv. Math. 188, 337\u2013398 2004), where the authors use the Riemann\u2013Hilbert formulation and the Deift\u2013Zhou non-linear steepest descent method. We show that computing higher-order terms can be simplified, leading to their efficient construction. The resulting asymptotic expansions in every region of the complex plane are implemented both symbolically and numerically, and the code is made publicly available. The main advantage of these expansions is that they lead to increasing accuracy for increasing degree of the polynomials, at a computational cost that is actually independent of the degree. In contrast, the typical use of the recurrence relation for orthogonal polynomials in computations leads to a cost that is at least linear in the degree. Furthermore, the expansions may be used to compute Gaussian quadrature rules in O(n)$\\mathcal {O}(n)$ operations, rather than O(n2)$\\mathcal {O}(n^{2})$ based on the recurrence relation.", "venue": "Adv. Comput. Math.", "authors": ["Alfredo  Dea\u00f1o", "Daan  Huybrechs", "Peter  Opsomer"], "year": 2016, "n_citations": 6}
{"id": 6691621, "s2_id": "52bddc0bc4fa3f64766707042890648c83709f57", "title": "Machine Learning at Scale", "abstract": "It takes skill to build a meaningful predictive model even with the abundance of implementations of modern machine learning algorithms and readily available computing resources. Building a model becomes challenging if hundreds of terabytes of data need to be processed to produce the training data set. In a digital advertising technology setting, we are faced with the need to build thousands of such models that predict user behavior and power advertising campaigns in a 24/7 chaotic real-time production environment. As data scientists, we also have to convince other internal departments critical to implementation success, our management, and our customers that our machine learning system works. In this paper, we present the details of the design and implementation of an automated, robust machine learning platform that impacts billions of advertising impressions monthly. This platform enables us to continuously optimize thousands of campaigns over hundreds of millions of users, on multiple continents, against varying performance objectives.", "venue": "ArXiv", "authors": ["Sergei  Izrailev", "Jeremy M. Stanley"], "year": 2014, "n_citations": 3}
{"id": 6692823, "s2_id": "0b3cb7e45e09fe91d8c0ec137a98bfce62e8ab25", "title": "Verified AIG Algorithms in ACL2", "abstract": "And-Inverter Graphs (AIGs) are a popular way to represent Boolean functions (like circuits). AIG simplification algorithms can dramatically reduce an AIG, and play an important role in modern hardware verification tools like equivalence checkers. In practice, these tricky algorithms are implemented with optimized C or C++ routines with no guarantee of correctness. Meanwhile, many interactive theorem provers can now employ SAT or SMT solvers to automatically solve finite goals, but no theorem prover makes use of these advanced, AIG-based approaches. \nWe have developed two ways to represent AIGs within the ACL2 theorem prover. One representation, Hons-AIGs, is especially convenient to use and reason about. The other, Aignet, is the opposite; it is styled after modern AIG packages and allows for efficient algorithms. We have implemented functions for converting between these representations, random vector simulation, conversion to CNF, etc., and developed reasoning strategies for verifying these algorithms. \nAside from these contributions towards verifying AIG algorithms, this work has an immediate, practical benefit for ACL2 users who are using GL to bit-blast finite ACL2 theorems: they can now optionally trust an off-the-shelf SAT solver to carry out the proof, instead of using the built-in BDD package. Looking to the future, it is a first step toward implementing verified AIG simplification algorithms that might further improve GL performance.", "venue": "ACL2", "authors": ["Jared  Davis", "Sol  Swords"], "year": 2013, "n_citations": 7}